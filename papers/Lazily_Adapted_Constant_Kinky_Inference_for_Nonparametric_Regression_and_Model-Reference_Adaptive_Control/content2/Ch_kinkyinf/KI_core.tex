



\subsection{Kinky Inference}
\label{sec:KI_core}
In this section, we will introduce the class of learning rules we refer to as \emph{Kinky Inference}. They encompass a host of other methods such as Lipschitz Interpolation and Nonlinear Set Interpolation.  The approach also fits into the more general framework of inference over group mappings considered in Sec. \ref{sec:group_learners}. 
%
%We will then establish some theoretical guarantees for certain parameter settings, in particular of the sequence of Lipschitz parameters $L(n)$. In subsequent parts of this section we will then consider a method for adapting these parameters to the data yielding a rule we will refer to as \emph{Lazily Adapted Constant Kinky Inference (LACKI)}. The section culminates in establishing universal approximation guarantees for our LACKI rule.

The rules possess a parameter $L(n)$ that needs to be specified by any KI algorithm. In this paper, we are most concerned with studying LACKI, a KI rule algorithm where $L(n)$ coincides with a noise-robust and multi-variate generalisation of Strongin's estimate \cite{Strongin1973} of a H\"older constant computed from the data set $\data_n$ available at time step $n$. 

%We will consider the special case $\inspace = \Real^d, \outspace \subseteq \Real^m$ with pseudo-metric $\metric_\inspace$ on the input space and output-space metric defined by $\metric_\outspace(x,x') := \norm{x-x'}_\infty , \forall x,x' \in \inspace$. 

%\subsubsection{Conservative learning and inference in a nutshell}
%
%The general approach we will consider is as follows. We desire to learn a function $f$ based prior knowledge of the form that $f \in \mathcal K_{prior}$ and some (noisy) function sample $\data$. 
%The task of conservative inference over functions (i.e. learning) then is the step of forming a posterior belief of the form $f \in \mathcal K_{post}= \mathcal K_{prior} \cap \mathcal K(\data)$. Here,
%$\mathcal K (\data)$ is the set of all functions that could have generated the observed data. 
%The conservatism lies in the fact that we have not ruled out any candidate functions that are consistent with our information. 
%To perform inference over a function value at input $x$ in a conservative manner, we merely infer that $f \in \prederrbox(x) =[\boden(x), \decke(x)]$ 
%where \textit{floor function} $\boden(x) =\inf_{\phi \in \mathcal K_{post}} \phi(x)$ and  \textit{ceiling function } $\decke(x) = \sup_{\phi \in \mathcal K_{post}} \phi(x)$ delimit the values that any function in the posterior class $\mathcal K_{post}$ could give rise to. If a point prediction of a function value is required, we could choose any function value between the values given by the floor and ceiling which also give rise to the bounds on the point prediction. 
%Of course, the type of inference we get from this general procedure depends on the chosen prior class $\mathcal K_{prior}$ and the specifics of the data. We will develop \textit{kinky inference} which is a method for considering prior classes of multi-output functions over metric input spaces that are optionally constrained by boundedness and by knowledge of H\"older regularity as well as knowledge on uncertainty bounds on the observations and function inputs. For this class of inference mechanisms we prove conservatism and convergence guarantees to the learning target. What is more, we will consider classes where the imposed regularity assumptions are uncertain or adapted lazily to adjust for overly restrictive prior assumptions that do not fit the observations.
%
%
%
%This capability of imposing boundedness assumptions, accommodating for noise and being able to adjust the regularity assumptions will prove of great importance in the 
%control applications of our method.
%
%\subsection{The framework of conservative inference -- definitions, setting and desiderata }
%\label{sec:prob_def}

%As a basic example, consider the ordinary differential equation $\frac{\d x}{\d t} = (\xi - x)$ where $\xi: I \subset \Real \to \Real $ is an absolutely continuous function superimposed by a countable number of steps. Suppose, the ODE describes the trajectory $x: I \to \Real$ of a controlled particle in one-dimensional state space. Based on $N$ time-state observations $D_N=(t_1,x_1 ),..., (t_N,x_N )$ we desire to estimate the total distance travelled $S$ in time interval $I$, $S = \int_I x(t) \d t$. Solving


%%\textbf{Setting.}
%Let $(\inspace, \metric_\inspace)$, $(\outspace ,\metric_\outspace)$ be two metric spaces and $f: \inspace \to \outspace$ be a function. Spaces $\inspace, \outspace$ will be referred to as \textit{input space} and \textit{output space}, respectively.
%Assume, we have access to a \textit{sample set} $\data_n:= \{\bigl( s_i, \tilde f_i, \obserr(s_i) \bigr) \vert i=1,\ldots, N_n \} $ containing $N_n \in \nat$ sample vectors $\tilde f_i \in \outspace$ of function $f$ at sample input $s_i \in \inspace$. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the \textit{grid} of sample inputs contained in $\data_n$.
%
%
%The sampled function values are allowed to have interval-bounded \textit{observational error} given by $\obserr : \inspace \to \Real^m_{\geq 0}$. That is, all we know is that $f(s_i) \in \obserrhyprec_i := [\underline f_1(s_i), \overline f_1(s_i) ] \times \ldots \times [\underline f_d(s_i), \overline f_d(s_i) ]$ where $\underline f_j(s_i) := \tilde f_{i,j} - \obserr_j(s_i)$, $\overline f(s_i) := \tilde f_{i,j} + \obserr_j(s_i)$ and $\tilde f_{i,j}$ denotes the $j$th component of vector $\tilde f_{i}$.
%
%The interpretation of these errors depends on the given application. For instance, in the context of system identification, the sample might be based on noisy measurements of velocities. The noise may be due to sensor noise or may represent numerical approximation errors. As we will see below, $\obserr$ may also quantify \textit{input} uncertainty (that is when predicting $f(x)$, $x$ is uncertain). This is an important case to consider, especially in the context of control applications where the ground truth might represent a vector field or an observer model.
%
%
%It is our aim to learn function $f$ in the sense that, folding in knowledge about $\data_n$, we infer \textit{predictions} $\predf(x)$ of $f(x)$ at unobserved \textit{query inputs} $x \notin G_n$. Being the target of learning, we will refer to $f$ as the \emph{target} or \emph{ground-truth} function. 
%
%In addition to the predictions themselves, we are also interested in \textit{conservative bounds} on the error of the inferred predictions. Depending on the hypothesis space under consideration these bounds can be of a probabilistic or deterministic nature. In this chapter, for the time being, we are interested in the latter.
%
%That is, we desire to provide a computable \textit{prediction uncertainty} function $\prederr: \inspace \to \outspace$ such that we can guarantee that for any query $x$, our prediction $\predf(x)$ is within the cube of error given by $\prederr(x)$ as follows 
%
%
%Being interested in conservative worst-case guarantees, we are mostly interested 
%
%
%
%In order to be able to deduce the error bounds around the predictions $\predf$, it is important to impose a priori restrictions on the class of possible targets (the hypothesis space). 

%\textbf{Definitions.}
%
\textbf{Setting.}  Let $\inspace$, $\outspace$ be two spaces endowed with (pseudo-) metrics $\metric_{\inspace}: \inspace^2 \to \Real_{\geq 0}, \metric_\outspace:\outspace^2 \to \Real_{\geq 0}$, respectively. Spaces $\inspace, \outspace$ will be referred to as \textit{input space} and \textit{output space}, respectively. 
It will be convenient to restrict our attention to input and output spaces that are additive abelian groups and which are \emph{translation-invariant} with respect to their (pseudo) metrics. That is, for the input space $\inspace$, we assume $ \metric_\inspace(x+x',x''+x') = \metric(x,x''),\forall x,x',x'' \in \inspace$. 

For simplicity, throughout the remainder of this work, we will assume the output space is the canonical Hilbert space $\outspace = \Real^m$ $(m \in \nat)$ endowed with the $\metric_{\outspace}(y,y') = \norm{y-y'}_\infty, \forall y,y' \in \outspace$. 

Let $f: \inspace \to \outspace$ be a \emph{target} or \emph{ground-truth} function we desire to learn. For our purposes, learning means regression. That is, we utilise the data to construct a computable function that allows us predict values of the target function at any given input.

%\footnote{Extensions to more general output spaces seem very possible, albeit, instead of learning a function, we would then learn a discrepancy from a nominal reference in output space. We will leave this to future work.}
%In contrast, we do not impose restrictions on the input space (pseudo-) metric. 
%While our simulations will consider the deterministic case, it would be entirely possible to consider input spaces that are index sets of stochastic processes endowed with suitable norm metrics. Since the output metric is defined via a norm, we will often drop the subscript of the input metric. That is we may on occasion write $\metric$ instead of $\metric_\inspace$.





Assume that, at time step $n$, we have access to a \textit{sample} or \textit{data set} $\data_n:= \{\bigl( s_i, \tilde f_i \bigr) \, \vert \, i=1,\ldots, N_n \} $ containing $N_n \in \nat$ (possibly erroneous) sample vectors $\tilde f_i \in \outspace$ of \emph{target function} $f$ at sample input $s_i \in \inspace$. 
The sampled function values are allowed to have interval-bounded \textit{observational error} or \emph{noise} given by a bound $\obserr : \inspace \to \Real^m_{\geq 0}$. That is, all we know is that $f(s_i) \in \obserrhyprec_i := [\underline f_1(s_i), \overline f_1(s_i) ] \times \ldots \times [\underline f_d(s_i), \overline f_d(s_i) ]$ where $\underline f_j(s_i) := \tilde f_{i,j} - \obserr_j(s_i)$, $\overline f(s_i) := \tilde f_{i,j} + \obserr_j(s_i)$ and $\tilde f_{i,j}$ denotes the $j$th component of vector $\tilde f_{i}$. 
The interpretation of these errors depends on the given application and this ``noise'' may be deterministic or stochastic. For instance, in the context of system identification, the sample might be based on noisy measurements of velocities and it may be due to sensor noise or may capture numerical approximation error. 

Furthermore, $\obserr$ may also accommodate \textit{input} uncertainty (that is when predicting $f(x)$, $x$ is uncertain) (for details refer to \cite{calliess2014_thesis}). In the course of our theoretical considerations below the error will also serve to absorb the discrepancy between a H\"older and a non-H\"older function.
%
%This is an important case to consider, especially in the context of control applications where the ground truth might represent a vector field or an observer model. Finally, the observational error can also model error around the computation of the metric. This might be of interest if the metric has to be approximated numerically or estimated with statistical methods that provide confidence intervals (in the latter case our predictions also only hold within pertaining confidence bounds). 

\textbf{Learning.}
It is our aim to learn target function $f$ in the sense that, combining prior knowledge about $f$ with the observed data $\data_n$, we infer \textit{predictions} $\predfn(x)$ of $f(x)$ at unobserved \textit{query inputs} $x \notin G_n$. Here, $G_n =\{s_i | i =1\ldots,N_n\}$ refers to the (not necessarily regular) \emph{grid} of sample inputs. In our context, the evaluation of $\predf_n$ is what we refer to as \textit{(inductive) inference}. 
%
%For a discussion of the competing definitions of non-deductive inference in a philosophical context, the reader is referred to \cite{Flach2000}. 
%
The entire function $\predfn$ that is learned to facilitate predictions is referred to as the \textit{predictor}. 

A typical \textit{desideratum} of a good predictor is that it is efficiently \textit{computable} and \textit{converges to the target} (up to the observational error given by $\obserr$) in the limit of increasingly dense data sets.
%Typically,
%the predictor lies in the space that coincides with (or is at least dense in) the a priori hypothesis space $\mathcal K_{prior}$ of conceivable target functions. In this case the predictor can be referred to as a \textit{candidate function} or \textit{hypothesis} \cite{mitchellbook:97}.

%In addition to the predictions themselves, we are also interested in \textit{uncertainty bounds} on the error of the inferred predictions. 
%%Depending on the hypothesis space under consideration, these bounds can be of a probabilistic or deterministic nature. In this chapter, we  restrict ourselves to the latter.
%
%That is, we desire to provide a computable \textit{prediction uncertainty function} $\prederr: \inspace \to \outspace$ such that we \textit{believe} that for any query $x \in \inspace$, the ground truth value $f(x)$ lies somewhere within the \textit{uncertainty hyperrectangle }
%\begin{equation}
%	\prederrbox_n(x) := \Bigl\{ y \in \outspace \, | \, \forall j \in \{1,...,\dim{\outspace}\} : y_j \in \prederrbox_{n,j}(x)\Bigr\}
%\end{equation} around the prediction $\predfn (x)$ where 
%\begin{equation}\label{eq:prederrint}
%	\prederrbox_{n,j}(x) : = [ \predfnj(x) - \prederrnj(x), \predfnj(x) + \prederrnj(x)] 
%\end{equation}
%is referred to as the $j$th \textit{prediction uncertainty} interval.
%Here, $\predfnj(x)$, $\prederrnj(x)$ denote the $j$th components of vectors $\predfn(x), \prederrn(x)$, respectively.

 %As a word of caution, we remark that using the last statement as a definition still seems very broad. 

%\textbf{Desiderata.}
In our context, we will understand a \textit{machine learning algorithm} to implement a computable function that maps a data set $\data_n $ to a 
 $\predf_n$ (and possibly an uncertainty estimate function $\prederr_n$). 

%The resulting learning or inference rule is called \textit{conservative} if the uncertainty quantifications never underestimate the error. That is, based on the an a priori assumption $f \in \mathcal K_{prior}$, we can guarantee $f(x) \in \prederrbox_n(x), \forall x$, regardless of the observed data set $\data_n$.

In this work we will expand on the basis of the following class of predictors to perform learning as inference over unobserved function values:

\begin{defn}[Kinky Inference (KI) rule ] \label{def:KIL}
Let  $\Real_\infty := \Real \cup\{- \infty, \infty\}$ and $\inspace$ be some space endowed with a pseudo-metric $\metric_\inspace$. Let $\lbf,\ubf: \inspace \to \outspace \subseteq \Real_\infty^m$ denote \textit{lower- and upper bound functions}, that can be specified in advance and assume $\lbf(x) \leq \ubf(x), \forall x \in I \subset \inspace$ component-wise. 
Furthermore, let $\obserrpar$ denote a parameter that specifies a deterministic belief about the true observational error bound $\obserr$.
Given sample set $\data_n$, we define the predictive functions $\predfn: \inspace \to \outspace, \prederrn: \inspace \to \Real^m_{\geq 0}$ to perform inference over function values.
For $j=1,\ldots,m$, their $j$th output components are given by:
%Furthermore, define
% $\coeff1,\coeff2 \in \Real_{\geq0}$ such that $\coeff1+\coeff2 =1$ (unless explicitly stated otherwise, we assume $\coeff1=\coeff2 =\frac 1 2)$.
	\begin{align*}
   \predfnj(x) :=& \frac{1}{2} \min\{ \ubf_j(x), \decke_{n,j}(x;L(n)\bigr)\} \\
    &+ \frac{1}{2} \max\{ \lbf_j(x), \boden_{n,j}(x;L(n)\bigr) \},\\
	\prederrnj(x) :=& \frac{1}{2} \min\{ \ubf_j(x), \decke_{n,j}\bigl(x;L(n)\bigr)\} \\
	&- \frac{1}{2} \max\{ \lbf_j, \boden_{n,j}\bigl(x;L(n)\bigr) \}.
	\end{align*}
	Here, $\decke_n\bigl(\cdot;L(n)\bigr), \boden_n\bigl(\cdot;L(n)\bigr): \inspace \to \Real^m$ are called ceiling and floor functions, respectively. Their $j$th component functions are given by
	\[\decke_{n,j}\bigl(x; L(n)\bigr) := \min_{i=1,\ldots,N_n}   \tilde f_{i,j} + L_j(n) \metric^\hexp(x,s_i) + \obserrpar_j(x)\] and 
	\[\boden_{n,j}\bigl(x; L(n)\bigr) := \max_{i=1,\ldots,N_n}   \tilde f_{i,j} - L_j(n) \metric^\hexp(x,s_i) - \obserrpar_j(x),\] respectively.
  Here $\hexp \in \Real, L(n) \in \Real^m, \forall n \in \nat_0$ and functions $\lbf,\ubf,\obserrpar:  \inspace \to \Real_\infty^m$ are parameters that have to be specified in advance. To disable restrictions of boundedness, it is allowed to specify the upper and lower bound functions to constants $\infty$ or $-\infty$, respectively.	
	Function $\predfn$ is the predictor that is to be utilised for predicting/inferring function values at unseen inputs. Function $\prederrn(x)$ is meant to quantify the uncertainty of prediction $\predfn(x)$. 
	%Note, if the data set is fixed (i.e. $n$ does not change), we may sometimes drop the subscript $n$ and write $\predf$ instead of $\predf_n$.
\end{defn}

To provide an intuition, consider the following special case where we have access to a noise-free sample $\data_n$ and suppose 
the target $f$ is a real-valued $L^*-p$ H\"older continuous function. Observing the noise-free sample point $(s_i,f_i)$ constrains the set of function values $f(x)$ to the set $\mathbb S_i(x) =\{ \phi \in \outspace | \metric_\outspace(\phi, f_i) \leq L^* \metric_\inspace(s_i,x) \}$. Considering a set of sample points $\data_n$, target value $f(x)$ is constrained to lie in the intersection $\mathbb S(x)=\cap_{i=1}^{N_n} \mathbb S_i(x)$. It is easy to see that the floor and ceiling functions are tight lower and upper bounds of $\mathbb S(x)$ with $\mathbb S(x) := \{ \phi \in \outspace | \boden_n(x; L^*) \leq \phi \leq \decke_n(x;L^*)\}$. In other words, setting parameter $L(n)$ to the best H\"older constant $L^*$ and bounds $\lbf =-\infty,\ubf=+\infty$ yields a predictor $\predfn(x)$ that for every query $x$ chooses the mid-point of the set $\mathbb S(x)$ of those function values that can possibly be assumed by a H\"older continuous function that interpolates the observed sample. Prediction error $\prederrn(x)$ simply is the radius of the set.

For the case of $p=1$, this approach is known as \emph{Lipschitz interpolation} \cite{Beliakov2006,Zabinsky2003}. Since a set is utilised for interpolation, the approach is also known as \emph{Nonlinear Set Interpolation} \cite{Milanese2004}. Specification of $\ubf,\lbf$ allows us to incorporate additional knowledge and constrain our set $\mathbb S(x)$ further. 
For instance, when estimating densities we might incorporate the knowledge of dealing with nonnegative functions. In this case, it makes sense to set $\lbf$ to a constant value of zero yielding $\mathbb S(x) = \{\phi | \phi \geq 0 \} \cap_{i=1}^{N_n} \mathbb S_i(x) $.

%\begin{figure*}
        %\centering
				  %\subfigure[Initial prediction $\predf_1$.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/akt2learneddrift}
    %\label{fig:KIcollavoidopenlooppredmodels1}
  %} 	 
					  %\subfigure[Improved prediction $\predf_2$.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/akt3learneddrift}
    %\label{fig:KIcollavoidopenlooppredmodels2}
  %} 	
							  %\subfigure[Social cost after each experiment.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm, clip, trim = 3.1cm 9.5cm 4.5cm 9cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/soccost}
    %\label{fig:KIsoccost}
  %} 
	%\label{fig:KIcollavoidopenlooppredmodels}
   %\caption{Belief over drift models based on $\data_1$ and $\data_2 \supset \data_1$ with $\abs{\data_1} = 2, \abs{\data_2} = 100$. Here, the top figures show the predictions of $\predf_{n,3}(s)$ and the bottom plots depict the predictions of $\predf_{n,4}(s)$ for $n =1$ and $n=2$, respectively. The ground-truth drift model was $f(x) = \bigl(0,0, - \sin(0.5 \, x_1), - \sin( 0.5 \, x_2) \bigr)^\top$. Rightmost plot: Social cost after each experiment. Note how the reduced uncertainty translated to reduced social cost of the last experiment (bar plot 3) vs the cost in the second (bar plot 2). While the first experiment (bar plot 1) also accumulated low social cost, a collision occurred.}
%\end{figure*}	 
%
%\begin{figure}
%        \centering
%				  \subfigure[KI prediction with two examples.]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.1cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/Ch_kinkyinf/figs/ex1hfe1.pdf}
%    \label{fig:hfe1}
%  } 	
%	 \subfigure[Refined prediction.]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.1cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/Ch_kinkyinf/figs/ex1hfe2.pdf}
%    \label{fig:hfe2}
%  } 	%\hspace{2cm}
%	  \subfigure[Using a periodic pseudo-metric.]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.1cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/Ch_kinkyinf/figs/ex1hfeperiodic.pdf}
%    \label{fig:hfe3}
%  } 	
%   \caption{Examples of different target functions and full kinky inference predictions. \textbf{Fig. \ref{fig:hfe1} and  Fig. \ref{fig:hfe2}} show kinky inference performed for two and ten noisy sample inputs of target function $x\mapsto \sqrt{\abs{\sin(x)}}$ with observational noise level $\varepsilon =0.3$. The floor function folds in knowledge of the non-negativity of the target function ($\lbf = 0$). The predictions were made based on the assumption of $p=\frac 1 2$ and $L = 1$ and employed the standard metric $\metric_\inspace(x,x') = \abs{x-x'}$. 
%	As can be seen from the plots, the function is well interpolated between sample points with small perturbations. Furthermore, the target is within the bounds predicted by the ceiling and floor functions.
%		\textbf{Fig. \ref{fig:hfe3}} depicts a prediction folding in knowledge about the periodicity of the target function by choosing a pseudo-metric $\metric_\inspace = \Bigl|\sin(\pi \abs{x-x'}\Bigr|$. The prediction performs well even in unexplored parts of input space taking advantage of the periodicity of the target. This time, there was no observational error and the target function was $x \mapsto \abs{\sin(\pi x)} +1$.   }
%			\label{fig:exHfe1}
%\end{figure}	  

%
%\begin{figure}
%        \centering
%							  %\subfigure[Wingrock dynamics.]{
%    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    %\includegraphics[width = 5.5cm]
%								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								%{content/Ch_kinkyinf/figs/hfewingrock_dyn_groundtruth}
%    %\label{fig:KIvs2NN}
%  %} 	
%				  \subfigure[Wingrock dynamics.]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 5.5cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/Ch_kinkyinf/figs/hfewingrock_dyn_groundtruth}
%    \label{fig:hfe5}
%  } 	
%	 \subfigure[Inferred model.]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 5.5cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/Ch_kinkyinf/figs/hfewingrock_dyn_learned}
%    \label{fig:hfe4}
%  } 	%\hspace{2cm}
%	 
%%
%   \caption{Example of a prediction of the KI rule on two-dimensional input space. \textit{Left plot:} the target function being a patch of the wingrock dynamics we will study in Sec. \ref{sec:KIMRAC}. \textit{Right plot:} the predictions inferred on the basis of 100 sample points using metric $\metric_\inspace(x,x') = \norm{x-x'}_\infty$ and $p=1, L=2.5$.}
%		\label{fig:exHfe2}
%\end{figure}	 

%Note the inference rule is nonparametric in that, that its complexity grows with the sample size. However, as any inference method one has to a priori specify prior knowledge, some of which is encoded in hyper-parmeters. In the KI rule, these parameters include the choice of pseudo-metric (or its parameters), parameter $\hexp$, the sequence of constants $\seq{L(n)}{n \in \nat}$ as well as the presupposed level of observational error $\obserr$.

%To develop a first feel for this kinky inference rule, we plotted some examples of target functions and pertaining predictions in Fig. \ref{fig:exHfe1}. %and Fig. \ref{fig:exHfe2}.
%Firstly, we can see that the predictions involve kinks. This is due to the minimisations and maximisations that occur in the computation of the ceiling and floor functions which introduce kinks into the prediction signal. This property provided the motivation behind the term ``kinky inference'' \cite{calliess2014_thesis}.

When choosing $L(n)$ to coincide with the best H\"older constant, one can give strong guarantees of convergence to the target as on the tightness of the prediction bounds  \cite{calliess2014_thesis}. 
Unfortunately, this requires us to know at least an upper bound of $L^*$ and therefore, several authours have proposed different approaches of how to estimate the constant from the data (e.g. \cite{Strongin1973,Wood1996}). However, it appears to be largely unknown how to do so in the presence of bounded observational noise $\obserr >0$ in a principled manner. Furthermore, when we replace $L(n)$ by the empirical estimates, nothing seems to be known about the convergence properties of the resulting kinky inference rule that is based on these estimates.  

In the remainder of the paper, we shall address this gap. Firstly, we propose a choice a lazy estimator for to be utilised in place of $L(n)$ that can be set to be robust to noise (i.e. does not grow unbounded). We then prove universal approximation properties of the LACKI rule before examining its performance in a control application.

%\textbf{Properties.}
%%
%%For now, we assume we are given constant $$
%Given H\"older constant $L^* \in \Real^m_+$ and exponent $p \in [0,1]$ we define  $\hoelset {L^*} {\metric_\inspace} p =\{ f: \inspace \to \outspace \subset \Real^m \, |  \forall j \in \{1,\ldots,m\} \forall x,x' \in \inspace: \abs{f_j(x) - f_j(x')}  \leq \, L_j^* \metric^p_{\inspace}(x,x') \}$ to be the class of $L_j^*-p$- H\"older continuous functions with respect to (pseudo-) metric $\metric_\inspace$. Furthermore, let $\bfset :=\{\phi : \inspace \to \outspace \, | \, \forall x \in \inspace, j \in\{1,...,m\}: \lbf_j(x) \leq \phi_j(x) \leq \ubf_j(x)\} $
%denote the set of all functions bounded component-wise between functions $\lbf,\ubf: \inspace \to \Real_\infty^m$.
%
%The next theorem tells us that for targets contained in the intersection of these two function classes, our kinky inference rule as per Def. \ref{def:KIL} is monotonically convergent (in the limit of dense sample grids) and provides conservative yet optimally tight uncertainty bounds, provided that parameters are set appropriately.
%
%\begin{thm} \label{thm:KIdesiderata}
%
%Let the target $f: \inspace \to \outspace$ be contained in the class   
%%\begin{equation}
%	$\mathcal K_{prior}= \bigl\{ f : \inspace \to \outspace \subseteq \Real^m |  f \in \hoelset L {\metric_{\inspace}} \rho \cap \bfset \bigr\}$. Furthermore assume the parameters of the KI rule as per as per Def. \ref{def:KIL} are chosen to be 
%	$L(n) := L^*$  ($n \in \nat_0$).
%%\end{equation}
%Then the inference rule exhibits the following properties:
%\begin{enumerate}
%	\item \textit{Conservatism:} $\forall x \in \inspace: f(x) \in \prederrbox_n(x)$.
%	\item \textit{Monotonicity:} Additional data cannot increase the uncertainty. 
%	%
%	That is, $\data_n \subseteq \data_{n+1} $ implies 
%		$\prederrbox_n(x) \supseteq \prederrbox_{n+1}(x)$ or equivalently, $\prederr_n(x) \geq \prederr_{n+1}(x), \forall x \in \inspace$ (where the inequality holds for each component).
%	%\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I \subset \inspace$ and for each input $x \in I$ let the sample error function $\obserr: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
%%\begin{itemize}
%%\item (a) The uncertainty converges point-wise: $\forall x \in I: 0 \leq \lim_{n \to \infty} \prederrn(x) \leq m_x$.
%%In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
%		%
%		%\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
%%\end{itemize}	
%	\item \textit{Convergence}: If $(G_n )_{n \in \nat }$ is a sample grid sequence of inputs converging to a dense subset of $I \subset \inspace$ (as $n \to \infty$) then  $\prederrn(x) \stackrel{n \to \infty}{\longrightarrow} \obserr(x), \forall x \in I$. If convergence of the grid to the dense subset is uniform and the observational error is zero everywhere ($\obserr(x) = 0, \,\forall x$) then the convergence  $\prederrn \stackrel{n \to \infty}{\longrightarrow} 0$ is uniform on $I$. 
%	   \item \textit{Minimality}: There is no conservative uncertainty bound that is tighter than $\prederr_n$.
%%If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    
%\end{enumerate}
%%\begin{enumerate}
%%\item  
%
%%\item If in addition, for some $d \in \nat$, $\inspace \subseteq \Real^d$ and $\metric_{\inspace}(x,x') = \norm{x-x'}$ for some norm that is equivalent to the Euclidean norm then the inference mechanism is also optimal (as per Desideratum 4 in Def. \ref{def:KIdesiderata}). That is, there is no other conservative inference mechanism with tighter uncertainty bound function $\prederrn$. 
%%\end{enumerate}
%\end{thm}
%\begin{proof}
%The proof is contained in \cite{calliess2014_thesis}.
%\end{proof}
%
%%We have assumed that the H\"older exponent $p \in(0,1)$ is a free parameter. It is well known that The following Lemmata tell us that our inference rule is typically conservative and appropriate if we choose $p$ as the assumed H\"older exponent, as long as it does not exceed the (largest) H\"older exponent of the target function.
%
%
%
%
%\textbf{Computational effort.} 
%We can see from Def. \ref{def:KIL} that the kinky inference method can be classified as a supervised nonparametric learning method with prediction effort growing linearly with the data set size $\abs{\data_n} = N_n$. If the computational effort for computing the metric for a given input is in the class $\mathcal O(\delta)$ and the computational effort for computing the bounding functions $\lbf$ and $\ubf$ is in $\mathcal O(\beta)$ then the computational effort for computing the prediction functions is generally in $\mathcal O\bigl(m (N_n \delta+\beta) \bigr)$. On the other hand, batch learning simply requires storing the $N_n$ data points, which generally can be done in the order of $\mathcal O\bigl((m+d) N_n \bigr)$ where $d$ denotes the input space dimensionality. In incremental learning, the computation required to add a new data point requires a computation in $\mathcal O(m+d)$.
%
%For the special case of $d=m=1$, with $p=1$, no error $\obserr = 0$, metric $\metric_\inspace(x,x') = \abs{x-x'}$ and bounds $\lbf=-\infty, \ubf =\infty$, our inference rule reduces to the interpolation rule considered in \cite{Zabinsky2003}. Here, the prediction only depends on the two most adjacent data points. Finding those can be accomplished by binary search in $\mathcal O(\log N_n)$, provided the inputs are stored in an ordered list.



%For example, in case the simple metric $\metric_\inspace(x,x') = \norm{x-x'}$ is utilised and the bounding functions are constant we have $\mathcal O(\delta) = \mathcal O(d)$ and $\mathcal O(\beta) = \mathcal O(1)$, yielding a total computational effort of $\mathcal O(d \, N_n)$ for performing inference. Finally, as we have derived in Sec. \ref{sec:onedimlipfloorceil}, the effort reduces to logarithmic asymptotic complexity $\mathcal O(\log N_n)$ in the one-dimensional case where the metric $\metric(x,x') = \abs{x-x'} $ is used and if $p=1$ (cf. Rem. \ref{rem:lipencl1d4KI}). This can make a significant difference in terms of computational speed of the inference in the presence of large data sets.

%
%\begin{defn}[Desiderata] \label{def:KIdesiderata} With definitions as above, we desire a machine learning algorithm to implement a mapping $\data_n \mapsto (\predfn,\prederrn)$ such that, for all data sets $\data_n$, the resulting inductive inference satisfies the following \textbf{desiderata}:
%\begin{enumerate}
	%\item \textit{Conservatism:} We can guarantee $\forall x \in \inspace \, \forall \phi \in \mathcal K_{prior} \cap \mathcal K(\data_n): \phi(x) \in \prederrbox_n(x)$. In particular, the ground-truth $f$ is always contained in the posterior set and its function values always are contained in the corresponding uncertainty hyperrectangles.
	%\item \textit{Monotonicity:} Additional data cannot increase the uncertainty. 
	%
	%That is, $\data_n \subseteq \data_{n+1} $ implies 
		%$\prederrbox_n(x) \supseteq \prederrbox_{n+1}(x)$ or equivalently, $\prederr_n(x) \geq \prederr_{n+1}(x), \forall x \in \inspace$ (where the inequality holds for each component).
	%%\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I \subset \inspace$ and for each input $x \in I$ let the sample error function $\obserr: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
%%\begin{itemize}
%%\item (a) The uncertainty converges point-wise: $\forall x \in I: 0 \leq \lim_{n \to \infty} \prederrn(x) \leq m_x$.
%%In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
		%%
		%%\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
%%\end{itemize}	
	%\item \textit{Convergence}: If $(G_n )_{n \in \nat }$ is a sample grid sequence of inputs converging to a dense subset of $\inspace$ (as $n \to \infty$) then  $\prederrn(x) \stackrel{n \to \infty}{\longrightarrow} \obserr(x), \forall x \in \inspace$. If convergence of the grid to the dense subset is uniform (cf. Def. \ref{def:uniform_setconvergence}) and the observational error is zero everywhere ($\obserr(x) = 0, \,\forall x$) then the convergence  $\prederrn \stackrel{n \to \infty}{\longrightarrow} 0$ is uniform. 
	   %\item \textit{Minimality (optimality)}: There is no conservative uncertainty bound that is tighter than $\prederr_n$. That is, if for some hyperrectangle $H$, $x \in \inspace$ we have $H \subsetneq \prederrbox_n(x)$ then we have: $\exists x \in \inspace, \phi \in \mathcal K_{prior} \cap \mathcal K(\data_n) : \phi(x) \notin H$. In particular, it might be possible that the function value of the ground-truth function at $x$ is not contained in $H$.
%%If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    
%\end{enumerate}
%\end{defn}
%
%\begin{rem}
%Note the statement of uniform convergence in the absence of observational error is interesting from a machine learning theoretic point of view. It tells us that if we can construct  a grid sequence $(G_n)_{n \in \nat}$ that uniformly converges to a dense set of the domain $\inspace$ then the worst-case approximation error  $\sup_{x \in \inspace} \norm{ \predfn(x) - f(x) }_{\infty} $ vanishes as $n \to \infty$.
%This can also be of interest in quadrature: if we construct a closed-form expression of the integral of the functions $\decke_n: x \mapsto \predfn(x) + \prederrn(x), \boden_n: x \mapsto \predfn(x) - \prederrn(x)$ then due to uniform convergence, $\int_\inspace f(x) \d x= \int_{\inspace} \lim_{n \to \infty} \decke_n(x) \d x = \lim_{n \to \infty} \int_{\inspace}  \decke_n(x) \d x $ assuming the integrals exists.
%Past work, not included in the thesis, has utilised this result for conservative quadrature of H\"older continuous functions. 
%\end{rem}
%
%%Note, Desiderta 1-3 combined imply that in the absence of observational errors ($\obserr \equiv 0$), the prediction $\predfn$ converges monotonically to the target when data sets become increasingly dense.
%
%\subsection{The kinky inference and nonparametric machine learning mechanism}
%Having specified general desirable properties on conservative inference we will now define an inference rule which satisfies these desiderata under certain conditions.
%This inference rule, which we will call \textit{kinky inference} is defined as follows:
%%We are now in a position to define our kinky inference rule:
%
%\begin{defn}[Kinky inference rule (KIR)] \label{def:KIL}
%Let  $\Real_\infty := \Real \cup\{- \infty, \infty\}$ and $\inspace$ be some space endowed with a pseudo-metric $\metric_\inspace$. Let $\lbf,\ubf: \inspace \to \outspace \subseteq \Real_\infty^m$ denote \textit{lower- and upper bound functions}, that can be specified in advance and assume $\lbf(x) \leq \ubf(x), \forall x \in I \subset \inspace$ component-wise. 
%Given sample set $\data_n$, we define the predictive functions $\predfn: \inspace \to \outspace, \prederrn: \inspace \to \Real^m_{\geq 0}$ to perform inference over function values.
%For $j=1,\ldots,m$, their $j$th output components are given by:
%%Furthermore, define
%% $\coeff1,\coeff2 \in \Real_{\geq0}$ such that $\coeff1+\coeff2 =1$ (unless explicitly stated otherwise, we assume $\coeff1=\coeff2 =\frac 1 2)$.
	%\begin{align}
   %\predfnj(x) &:= \frac{1}{2} \min\{ \ubf_j(x), \decke_{n,j}(x)\} + \frac{1}{2} \max\{ \lbf_j, \boden_{n,j}(x) \} \label{eq:KIpred}\\  
	%\prederrnj(x) &:= \frac{1}{2} \bigl(\min\{ \ubf_j(x), \decke_{n,j}(x)\} - \max\{ \lbf_j, \boden_{n,j}(x) \} \bigr)   \label{eq:KIprederr}
	%\end{align}
	%Here, $\decke_n, \boden_n: \inspace \to \Real^m$ are called ceiling and floor functions, respectively. Their $j$th component functions are given by
	%$\decke_{n,j}(x) := \min_{i=1,\ldots,N_n}   \tilde f_{i,j} + L_j \metric^p(x,s_i) + \obserr_j(x)$ and 
	%$\boden_{n,j}(x) := \max_{i=1,\ldots,N_n}   \tilde f_{i,j} - L_j \metric^p(x,s_i) - \obserr_j(x)$, respectively.
  %Here $p \in \Real, L \in \Real^m$ and functions $\lbf,\ubf,\obserr$ are parameters that have to be specified in advance. To disable restrictions of boundedness, it is allowed to specify the upper and lower bound functions to constant $\infty$ or $-\infty$, respectively.	
	%Function $\predfn$ is the predictor that is to be utilised for predicting/inferring function values at unseen inputs. Function $\prederrn(x)$ quantifies the uncertainty of prediction $\predfn(x)$. 
	%%Note, if the data set is fixed (i.e. $n$ does not change), we may sometimes drop the subscript $n$ and write $\predf$ instead of $\predf_n$.
%\end{defn}
%
%%
%%\begin{defn}[Kinky inference rule (KIR)] \label{def:KIL}
%%Define $\lbf,\ubf: \inspace \to \Real \cup\{- \infty, \infty\}$ to be the \textit{lower- and upper bound function}, and assume $\lbf(x) \leq \ubf(x), \forall x \in I \subset \inspace$. 
%%Given sample set $D_n$ we define the following predictive functions to perform inductive inference:
%%%Furthermore, define
%%% $\coeff1,\coeff2 \in \Real_{\geq0}$ such that $\coeff1+\coeff2 =1$ (unless explicitly stated otherwise, we assume $\coeff1=\coeff2 =\frac 1 2)$.
	%%\begin{align}
   %%\predf_n(x) &:= \frac{1}{2} \min\{ \ubf(x), \decke_n(x)\} + \frac{1}{2} \max\{ \lbf, \boden_n(x) \} \\  
	%%\prederr_n(x) &:= \frac{1}{2} \bigl(\min\{ \ubf(x), \decke_n(x)\} - \max\{ \lbf, \boden_n(x) \} \bigr) \label{eq:KIprederr}\\  
	%%\end{align}
	%%where $\decke_n(x) := \min_{i=1,\ldots,N_n}  \tilde f_i + L \metric(x,s_i)+\obserr(x)$
	%%is called the ceiling 
	%%and $\boden_n(x) := \max_{i=1,\ldots,N_n}   \tilde f_i - L \metric(x,s_i) - \obserr(x)$
	%%is called the floor function.
	%%Function $\predf$ is the predictor that is to be utilised for predicting/inferring function values at unseen inputs. Function $\prederr_n(x)$ quantifies the uncertainty of prediction $\predf_n(x)$. Note, if the data set is fixed (i.e. $n$ does not change), we will drop the subscript $n$ and write $\predf$ instead of $\predf_n$.
%%\end{defn}
 %
%
%\begin{figure}
        %\centering
				  %\subfigure[KI prediction with two examples.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/ex1Hfe1}
    %\label{fig:hfe1}
  %} 	
	 %\subfigure[Refined prediction.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/ex1Hfe2}
    %\label{fig:hfe2}
  %} 	%\hspace{2cm}
	  %\subfigure[Using a periodic pseudo-metric.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/ex1hfeperiodic}
    %\label{fig:hfe3}
  %} 	
	%\label{fig:exHfe1}
   %\caption{Examples of different target functions and kinky inference predictions. \textbf{Fig. \ref{fig:hfe1} and  Fig. \ref{fig:hfe2}} show kinky inference performed for two and ten noisy sample inputs of target function $x\mapsto \sqrt{\abs{\sin(x)}}$ with observational noise level $\obserr =0.3$. The floor function folds in knowledge of the non-negativity of the target function ($\lbf = 0$). The predictions were made based on the assumption of $p=\frac 1 2$ and $L = 1$ and employed the standard metric $\metric(x,x') = \abs{x-x'}$. 
	%As can be seen from the plots, the function is well interpolated between sample points with small perturbations. Furthermore, the target is within the bounds predicted by the ceiling and floor functions.
		%\textbf{Fig. \ref{fig:hfe3}} depicts a prediction folding in knowledge about the periodicity of the target function by choosing a pseudo-metric $\metric = \Bigl|\sin(\pi \abs{x-x'}\Bigr|$. The prediction performs well even in unexplored parts of input space taking advantage of the periodicity of the target. This time, there was no observational error and the target function was $x \mapsto \abs{\sin(\pi x)} +1$.   }
%\end{figure}	  
%
%
%\begin{figure}
        %\centering
				  %\subfigure[Wingrock dynamics.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5.5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/hfewingrock_dyn_groundtruth}
    %\label{fig:hfe5}
  %} 	
	 %\subfigure[Inferred model.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5.5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/hfewingrock_dyn_learned}
    %\label{fig:hfe4}
  %} 	%\hspace{2cm}
	 %
	%\label{fig:exHfe2}
   %\caption{Example of a prediction on two-dimensional input space. \textit{Left plot:} the target function being a patch of the wingrock dynamics we will study in Sec. \ref{sec:KIMRAC}. \textit{Right plot:} the predictions inferred on the basis of 100 sample points using metric $\metric(x,x') = \norm{x-x'}_\infty$ and $p=1, L=2.5$.}
%\end{figure}	 
%
%To develop a first feel for the kinky inference rule, we have plotted some examples of ground-truth functions and pertaining predictions in Fig. \ref{fig:exHfe1} and Fig. \ref{fig:exHfe2}. Firstly, we can see that the predictions involve kinks. This is due to the minimisations and maximisations that occur in the computation of the ceiling and floor functions which introduce breakpoints into the prediction signal. This property provided the motivation behind the term ``kinky inference''.
%
%Secondly, we note that in all the examples, the conservative bounds were accurate. That is, the target remained contained in them. An example where we see how the bounds shrink to the ground-truth (up to the observational error tube) will be given in Sec. \ref{sec:agentexploration}, where we consider an agent that utilises kinky inference to learn a drift vector field it is immersed in. Also, note from Fig. \ref{fig:hfe3} how the choice of distance metric $\metric$ can affect the predictions. In this example, a metric was chosen that took into account a priori knowledge about periodicity of the target function. This allowed predictions to be accurate even in unexplored parts of the input space. 
%
%%\jcom{probably move remark below where the experiments are and we can show a periodic kernel.}
%\begin{rem}[Remarks on the relationship to kernels and distance metric learning]
%Unless altered by $\ubf,\lbf$, the kinky inference predictor assigns similar function values to inputs that are similar (i.e. close) with respect to the chosen input metric. Therefore, the choice of metric is important. Metrics are measures of dissimilarity and therefore, are closely related to similarity measures such as kernels.
%%For instance, we have seen that  $\metric(x,y) := \normtop{x-y}$ defines a proper metric and (uniform) convergence with respect to this metric is equivalent to (uniform) convergence with respect to metric $ D: (x,y) \mapsto \norm{x-y}$ (see Sec. \ref{sec:unifconvhoeldermetricequivmetricunifconv}). 
%In much of the recent machine learning literature, kernel functions are utilised to perform inference.
%Note, a kernel function is a measure of similarity often based on quantities inversely related to metrics (which are dissimilarity measures). In order to gain inspiration about metrics or pseudo-metrics suitable for an application at hand, we can draw on the parts of the kernel learning literature that is concerned with the tailoring of kernels to specific applications  (for a recent overview of \textit{kernel engineering}, including periodic ones, refer to\cite{duvenaud2014}). 
%As we have seen in the example depicted in Fig. \ref{fig:hfe3}, we might consider folding in knowledge of periodicity of the underlying function class. However, as a future direction, one might consider exploring to harness more sophisticated automated kernel construction techniques (e.g. \cite{argyriou2005,Kloft2010,duvenaud2014}) and distance metric learning methods (see \cite{Kulis2012}) that have been developed throughout the machine learning community in recent years.
%\end{rem}
%
%
%
%
%\subsection{Theoretical guarantees}
%
%The examples depicted in the plots of Fig. Fig. \ref{fig:exHfe1} and Fig. \ref{fig:exHfe2} seem to be consistent with what we would expect from an inference mechanism that is conservative and monotonically convergent and hence, might satisfy the desiderata we stated in Def. \ref{def:KIdesiderata}.
%We will now give conditions under which we can guarantee the desiderata to be fulfilled. 
%
%Of course, the validity of our guarantees depends on the a priori hypothesis space $\mathcal K_{prior}$. Inspection of the prediction rules reveals that the minimisation and maximisations eliminate all those function values that do not adhere to boundedness conditions (imposed by $\lbf,\ubf$) and conditions on H\"older continuity up to a margin of error given by $\obserr$. 
%
%That this is exactly the right class for which our desiderata hold is asserted by the following theorem:
%
%\begin{thm} \label{thm:KIdesiderata}
%Assuming all (a priori) possible targets $f: \inspace \to \outspace$ are given by the class   
%\begin{equation}
	%\mathcal K_{prior}= \bigl\{ f : \inspace \to \outspace \subseteq \Real^m |  f \in \hoelset L {\metric_{\inspace}} p \cap \bfset \bigr\}
%\end{equation}
%where $\hoelset L {\metric_\inspace} p =\{ f: \inspace \to \outspace \subset \Real^m \, |  \forall j \in \{1,\ldots,m\} \forall x,x' \in \inspace: \abs{f_j(x) - f_j(x')}  \leq \, L_j \metric^p_{\inspace}(x,x') \}$ denotes the class of $L-p$- H\"older continuous functions with respect to metric $\metric_\inspace$ and $\bfset :=\{\phi : \inspace \to \outspace \, | \, \forall x \in \inspace, j \in\{1,...,m\}: \lbf_j(x) \leq \phi_j(x) \leq \ubf_j(x)\} $
%is the set of all functions bounded component-wise between functions $\lbf,\ubf: \Real^d \to \Real \cup \{- \infty, \infty\}$, where we will always define $\Real_\infty := \Real \cup\{- \infty, \infty\}$. 
%
%Then we have:
%%\begin{enumerate}
%%\item  
%The inference rule as per Def. \ref{def:KIL} is conservative, monotonically convergent (in the limit of dense sample grids) and optimal in the sense of Def. \ref{def:KIdesiderata}. That is, it satisfies Desiderata 1-4 as per Def. \ref{def:KIdesiderata}. 
%%\item If in addition, for some $d \in \nat$, $\inspace \subseteq \Real^d$ and $\metric_{\inspace}(x,x') = \norm{x-x'}$ for some norm that is equivalent to the Euclidean norm then the inference mechanism is also optimal (as per Desideratum 4 in Def. \ref{def:KIdesiderata}). That is, there is no other conservative inference mechanism with tighter uncertainty bound function $\prederrn$. 
%%\end{enumerate}
%\end{thm}
%\begin{proof}
%The statement is derived in the appendix, Sec. \ref{sec:proofKIdesiderata}.
%\end{proof}
%
%%\jcom{Requirements on the regularity of the bound functions or error ? Can metric be a pseudo-metric?}
%
   %%
%%%=================
%%
%%Let  $\Real_\infty := \Real \cup\{- \infty, \infty\}$.
%%In this work, we are primarily interested in functions restricted to be H\"older continuous and bounded component-wise by some known bounding functions $\lbf,\ubf: \inspace \to \Real^m_\infty$. That is, we know the target is contained in 
%%the a priori hypothesis space 
%%\begin{equation}
	%%\mathcal K_{prior}= \bigl\{ f : \inspace \to \Real |  f \in \hoelset L \metric p \cap \bfset \bigr\}
%%\end{equation}
%%where $\hoelset L \metric p =\{ f: \inspace \to \outspace \subset \Real^m \, |  \norm{}  \leq \, \}$ denotes the class of $L-p$- H\"older continuous functions with respect to metric $\metric$ and $\bfset :=\{\phi : \inspace \to \outspace \, | \, \forall x \in \inspace, j \in\{1,...,m\}: \lbf_j(x) \leq \phi_j(x) \leq \ubf_j(x)\} $
%%is the set of all functions bounded component-wise between functions $\lbf,\ubf: \Real^d \to \Real \cup \{- \infty, \infty\}$, where we will always define $\Real_\infty := \Real \cup\{- \infty, \infty\}$. 
%%
%%For ease of notation, we will follow the convention of dropping the subscripts from inequalities and interpret an inequality component-wise. Similarly, if we omit the argument from an inequality, the inequality is understood to hold point-wise. For example, we interpret write $\lbf \leq \phi$ in place of  $\lbf_j(x) \leq \phi_j(x), \forall j,x$. 
%
%%
%%%=======================
%%
%%
%%
%%From the perspective of topological inference, the estimator is a data inference method for function estimation:
%%$B(\cdot) = M_{dinf}(\mathcal K_{prior}, \cdot, \gamma ) $ (for some sufficiently large $\gamma$) mapping sample data into a hypothesis interval of possible functions. 
%%Here,  our a priori knowledge is that we know, for each $J \subset I$ a nonnegative number $L_J$ such that the underlying target $f \in \mathcal F$ is $L_J-$Lipschitz on $J$. That is, $\mathcal K_{prior} =\{\phi \in \mathcal F \,|\, \forall J \subset I \forall x,x' \in J: \abs(\phi(x) - \phi(x') \leq L_J \norm{x-x'}\}$. 
%%Note, we will not explicitly compute the prior knowledge set. Instead will will fold in the Lipschitz continuity to define an optimal estimator output defining an enclosure that coincides with $\mathcal K_{post} \subset \mathcal K_{prior}$ and hence, with $T (\mathcal K_{post} \cup \mathcal K_{prior})$. Therefore, the resulting inference mechanism will be optimal. 
%%
%%Note, how our desiderata are equivalent to the corresponding notions of the inference mechanism that is to be designed.
%
%
%%Our method will 
%%
%%We 
%%
%%In this section, we consider hypotheses spaces comprising H\"older continuous functions. This is a very general class of uniformly continuous functions that can exhibit infinitely many non-differentiable points.
%%
%%
 %%In addition to a learned hypothesis, our approach also provides error bounds around each prediction that are conservative and tight. That is, the bounds contain exactly those function within the assumed H\"older class that could have generated the sample. 
%%
%%The rest of the section is structured as follows: First we 
%%
%%
%%
%%Our conservative function estimation procedure constructed upper and lower bound functions that are sample-consistent. The enclosure encompasses all possible candidate functions with an assumed H\"older property that could have generated the sample. Therefore, any H\"older function within the enclosure is a valid hypothesis that could have generated the data and hence, a suitable generalisation model for non-deductive inference. 
%%
%%
%%We assume the dimensionality of the target's input domain $I$ is $d \geq 1$. Remember, we defined the set of function samples as $D_n= \{\bigl( s_i, \tilde f_i, \obserr(s_i) \bigr) \vert i=1,\ldots, N_n \} $. 
%%Let $f$ denote the \textit{ground-truth} or \textit{target function} generating the data. $\tilde f_i$ is a noisy function value such that we know $f(s_i) \in [\tilde f_i - \obserr(s_i) , \tilde f_i + \obserr(s_i)], \, (i=1,\ldots, N_n)$. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the grid of sample inputs contained in $D_n$.  
%%
%%A robust learning rule is a computable mapping $\mathcal R : D_n \mapsto (\predf, \prederr) $.
%%Here $\predf$ is a predictor allowing to make inductive inference and $\prederr(x)$ quantifies the uncertainty or error around the prediction $\predf(x)$.
%%
 %%
%%Let $\metric: \inspace^2 \to \Real$ be a metric. We propose the following nonparametric inference rule:
%
%\subsubsection{Computational effort}
%We can see from Def. \ref{def:KIL} that the kinky inference method can be classified as a supervised nonparametric learning method. That is, the computational effort for predicting grows with the data set size $N_n$. If the computational effort for computing the metric for a given input is in the class $\mathcal O(\delta)$ and the computational effort for computing the bounding functions $\lbf$ and $\ubf$ is in $\mathcal O(\beta)$ then the computational effort for computing the prediction functions is in  $\mathcal O(N_n \delta+\beta)$. 
%For example, in case the simple metric $\metric_\inspace(x,x') = \norm{x-x'}$ is utilised and the bounding functions are constant we have $\mathcal O(\delta) = \mathcal O(d)$ and $\mathcal O(\beta) = \mathcal O(1)$, yielding a total computational effort of $\mathcal O(d \, N_n)$ for performing inference. Finally, as we have derived in Sec. \ref{sec:onedimlipfloorceil}, the effort reduces to logarithmic asymptotic complexity $\mathcal O(\log N_n)$ in the one-dimensional case where the metric $\metric(x,x') = \abs{x-x'} $ is used and if $p=1$ (cf. Rem. \ref{rem:lipencl1d4KI}). This can make a significant difference in terms of computational speed of the inference in the presence of large data sets.
%%
%%
%%
%%\subsection{Uncertain inputs}
%%So far, we have considered output uncertainty but not input uncertainty. That is, we have assumed that the function values $f(s_i)$ at the sample inputs are uncertain but that query inputs $x$ as well as sample 
%%inputs $s_i$ are known exactly. However, in many applications this assumption typically is not met. For example, in Sec. \ref{sec:hfeandcontrol}, we will employ KI for learning and controlling dynamic systems.
%%Here the target functions are state-dependent vector fields. Here, the state measurements often are corrupted by noise (e.g. due to sensor noise or delays) which can affect both training data and query inputs. In the latter case, the controller might query the KI method to predict 
%%a function value $f(x)$ based on the assumption of the plant being in state $x$ while the plant may in fact be in a different state $x+\delta_x$. We would like to guarantee that our inference over the actual function value still is within the predicted bounds provided the input estimation error $\delta_x$ is known or known to be bounded. 
%%Fortunately, the H\"older property of the target can help to address the problem of quantifying the prediction error due to input uncertainty. For instance, consider $\metric(x,x') = \norm{x-x'}$. By H\"older continuity, the error of the prediction of the $j$th output component of the target function $f$ is bounded by $\abs{f_j(x+\delta_x) - f_j(x)} \leq \norm{\delta_x}^p$. 
%%
%%Firstly, assume the training inputs $s_i$ were certain but the query inputs $x$ during prediction are uncertain.
%%Therefore, assuming we know an upper bound $\errinp(x)$ on the input uncertainty, i.e. 
%%$\norm{\delta_x}^p\leq \errinp(x), \forall x$, we can add this term to the uncertainty estimate $\prederr(x)$ as per Eq. \ref{eq:KIprederr}.
%%%This means that the additional uncertainty only affects the prediction of this particular query point. 
%%
%%Secondly, consider the case were both the sample inputs $s_i$ and the query inputs $x$ are uncertain with the uncertainty being quantified by the function $\errinp(\cdot)$. In this case, the uncertainty of the sample inputs
 %%affects the posterior hypothesis (i.e. the enclosure) over $f$ and hence, the inferences over function values at all inputs. 
%%By the same argument as for the case of query noise, we accommodate for this fact leveraging the H\"older property. Instead of adjusting just the prediction uncertainty, we simply add $ \errinp(\cdot)$ to the error function $\obserr(\cdot)$ (that we have previously used to model observational 
%%error of the function values). That is, the observational uncertainty model has absorbed the input uncertainty.  Predictions with uncertain query points can then be done as per Def. \ref{def:KIL} (but with $ \errinp(\cdot)+ \obserr(\cdot)$ in place of $\obserr(\cdot) )$.
%%
%%
%%\subsection{A smoothed inference rule} \label{sec:SKI}
%%The inferred predictor of kinky inference as per Eq. \ref{eq:KIpred} contains kinks. In certain applications, this can be disadvantageous. For instance, below we will feed the predictor into a control signal. However, in applications such as robotics, smooth controllers are typically preferred over non-smooth ones since the latter give rise to increase wear and tear of the actuators. With this in mind we will consider a modification of the predictor which filters the prediction output through a smoothing filter as known from computer vision \cite{forsyth2002,Baessmann2004}. The idea is to convolve the predictor with a function that implements a low-pass filter and abates high frequencies thereby smoothing out rough transitions in the signal. 
%%For now, we will restrict the exposition to one-dimensional input and output spaces. However, extensions to multiple output dimensions result from following our steps through for each component function. Extensions to multi-dimensional input spaces follow by taking advantage of the corresponding filters developed for multi-dimensional signal processing (see e.g \cite{young1995,Prorakis2006}). 
%%As an example, for some $n \in \nat, w_i \geq 0 \, (i=1,...,n)$ and (typically small) step size $\sigma \in \Real \geq 0$, consider the modified prediction rule (\textit{smoothed kinky inference (SKI) rule} ):
%%%
%%\begin{equation}
%%f_n^* = \frac{1}{\sum_{i=-q}^q w_i} 
%%\sum_{i=-q}^q w_i \predfn (x+i \sigma) .
%%\end{equation}
%%%
%%Either $w_0 = 2$ and $w_{-1}=w_2 =1$ or $w_{-2} = w_2 = 7, w_{-1}=w_1 = 26, w_0 = 41$  are common choices for discrete approximations of a Gaussian filter \cite{Baessmann2004}. 
%%
%%A comparison of the predictions made by the {smoothed kinky inference rule (SKI) } against the prediction function of the standard kinky inference rule is depicted in Fig. \ref{fig:exSKI1}. As can be seen from the plot, in the SKI prediction the kinks have been smoothed out while all the predictions still are contained within the prediction bounds given by the floor and ceiling functions.
%%
%%\begin{figure}
        %%\centering
				  %%%\subfigure[KI on a sine curve.]{
    %%%%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %%%\includegraphics[width = 5cm]
								%%%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%%%{content/Ch_kinkyinf/figs/sinKI1}
    %%%\label{fig:KI1}
  %%%} 	
	 %%\subfigure[KI on a sine curve.]{
    %%%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %%\includegraphics[width = 5cm]
								%%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%%{content/Ch_kinkyinf/figs/sinKI3}
    %%\label{fig:KI2}
  %%} 	%\hspace{2cm}
	  %%%\subfigure[SKI on a sine curve.]{
    %%%%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %%%\includegraphics[width = 5cm]
								%%%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%%%{content/Ch_kinkyinf/figs/sinSKI1}
    %%%\label{fig:SKI1}
		%%%} 	
	  %%\subfigure[SKI on a sine curve.]{
    %%%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %%\includegraphics[width = 5cm]
								%%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%%{content/Ch_kinkyinf/figs/sinSKI3}
    %%\label{fig:SKI2}
  %%} 	
	%%\label{fig:exSKI1}
   %%\caption{Comparison of the kinky inference rule (left) versus the smoothed kinky inference rule (right) when predicting a sine curve. Note the smoothed out kinks of the SKI rule due to the application of the smoothing filter. Here we chose $\sigma =0.1$. }
%%\end{figure}	  
%%
%%The question arises how the smoothing might affect the theoretical properties. Firstly, we note that due to Lem. \ref{lem:Hoeldarithmetic}, the H\"older exponent and constant remain unchanged. For the constant, we apply point 4 and 9 of the lemma to see that $L(f_n^*) = \frac{1}{\sum_{i=-q}^q w_i} 
%%\sum_{i=-q}^q w_i L(\predfn (x+i \sigma)) = L(\predf_n) \frac{1}{\sum_{i=-q}^q w_i}  \sum_{i=-q}^q w_i = L(\predfn)$.
%%
%%Secondly, the question arises what the maximal discrepancy between $\predfn$ and $f_n^*$ is. This question is of importance to make sure that the prediction still is within the bounds given by the ceiling and floor functions. To this end, assume $\predfn$ is H\"older with constant $L$ and exponent $p$. For simplicity assume we utilise Gaussian smoothing for the SKI rule with $n=1$, $w_{-1} = w_1 =1, w_0 =2$. The deviation of this resulting smoothed predictor $f_n^* $ from the kinky inference predictor $\predfn$ can be bounded as follows
%%\begin{align}
%%\abs{\predfn(x) - f_n^*(x)} &= \abs{\predfn(x) - \frac 1 {w_{-1}+w_0+w_1} (w_{-1} \predfn(x-\sigma) + w_0 \predfn(x)+ w_1 \predfn(x+\sigma)} \\
%%&= \frac 1 4 \abs{ 2 \predfn(x) -   \predfn(x-\sigma) - \predfn(x+\sigma)} \\
%%&\leq  \frac 1 4 \abs{  \predfn(x) -   \predfn(x-\sigma)} + \frac 1 4 \abs{ \predfn(x) - \predfn(x+\sigma)}\\
%%&\leq  \frac L 4  \metric^p(x,x-\sigma) + \frac L 4 \metric^p(x,x+\sigma).
%%\end{align}
%%If the metric is induced by a norm $\norm{\cdot}$ the last expression simplifies to $\frac L 2 \norm{\sigma}^p$.
%%In order to ensure that uncertainty bound takes this error into account, we can add this error to the uncertainty quantification $\prederrn$. This guarantees that the ground truth is contained within the error bounds around predictions $f_n^*$. Furthermore, since the bound is valid for all inputs $x \in \inspace$, the convergence guarantees established for kinky inference also hold for the smoothed version up to supremum norm error of at most $\sup_{x \in \inspace} \frac L 4  \metric^p(x,x-\sigma) + \frac L 4 \metric^p(x,x+\sigma)$ (which in the norm-based metric case equals $\frac L 2 \norm{\sigma}^p$).
%%
%%%\subsection{Discussion}
%%%As we observed, our posterior knowledge coincides with the set of sample-consistent functions. Therefore, an exhaustive enclosure is a superset of $\mathcal K_{prior} \cap \mathcal K_{post}$. 
%%%Remembering our discussion of topological inference, we realize that all we need to do to solve Task 1 (as specified in Sec. \ref{sec:problemdef_task1}), is to define our data inference mechanism $M_{\text{dinf}}$ to output the optimal enclosure. A prescription for computing the bounds of this enclosure was given in the definitions above. From these, we see that $M_{dinf}$computes in $\mathcal O(N_n d)$. That is, its computational effort (translating to the necessary inference investment $\gamma_{\text{dinf}})$ is linear in the number of samples and dimensionality of the problem. If we deal with maximum-norms, norm computation can be done with computation that is logarithmic in $d$, reducing the overall evaluation effort to $\mathcal O(N_n \log d)$.   