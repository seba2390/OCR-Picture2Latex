\subsubsection{A note on H\"older continuity for optimisation bounds}
%\jcom{Nat suggests to cite no regret optim paper}
%\subsection{A simple bounded optimisation procedure for Hoelder continuous functions on multi-dimensional domains} 
\label{sec:Hoelder_opt_basic_brief}
In the presence of a known Lipschitz constant, optimisation of a Lipschitz continuous function can be done with Shupert's algorithm \cite{Shubert:72}. Of importance to us at various parts of the thesis is, that it also provides non-asymptotic error bounds.  Unfortunately, it is limited to minimising functions on one-dimensional input domains. 
For multi-dimensional domains DIRECT \cite{direct:93} is a global optimiser that is popular, not least because no Lipschitz constant is required. On the flip side, it does not provide the error bounds afforded by Shupert's methods. Furthermore, we will sometimes be interested in bounds on the maxima and minima for the more general case, where the functions to be optimised are H\"older continuous with respect to normed spaces.  
%This is fine when we attempt to find $\bar v$ for a GP. However, for random fields, i.e. multi-input GPs, we require an optimisation algorithm capable of finding a conservative upper bound on the maximum of the kernels $k(\cdot,x_i)$ even for vector inputs. Fortunately, a naive method one could use, even for H\"older-continuous functions, is as follows:
We are currently working on such a method, as well as on a corresponding cubature algorithm. 
As it is work in progress and its exposition is beyond the scope of this work, we will merely sketch a naive version that provides some desired bounded optimisation for H\"older continuous functions:

Let $f: I \subset \Real^d \to \Real$ be a H\"older continuous function with H\"older constant $L$ and exponent $p$ given with respect to the maximum-norm $\norm{\cdot}_\infty$.\footnote{Due to norm-equivalence relationships we can bound each norm by each other norm by multiplication with a well-known constant (see e.g. \cite{koenigsberger:2000}). Therefore, knowing a H\"older-constant with respect to one norm immediately yields a constant with respect to another, e.g. $\norm{\cdot}_\infty \leq \norm{\cdot}_2 \leq \sqrt{d} \norm{\cdot}_\infty$ where $d$ is the dimension of the input space.}  
$S = \{(s_i, f_i) | i =1,...,N\} $ be a sample of target function $f$ with $f_i = f(s_i)$, $s_i \in I, \forall i$. 

Assume $I$ is a hyper-rectangle and  let  $J_1,...,J_N \subset I$ be a partition of the domain $I$ such that each sub-domain $J_j$ is a hyperrectangle containing $s_j$.
(More generally, to avoid $I$ having to be a hyperrectangle itself, one could alternatively assume that the $J_1,\ldots,J_N$ are a covering of $I$. That is, $I \subset \cup_i J_i$.)
 
By H\"older continuity $\forall x \in J_i: \abs{f_i - f(x)} \leq L \norm{x - s_i}_\infty^p$for some $p \in (0,1], L \geq 0$. 
Let  $a_1,\ldots,a_d >0$, $b_1,\ldots,b_d>0$ be defined such that $J_i =  s_i + ([-a_1,b_1] \times...\times [-a_d,b_d])$ where the addition is defined point-wise. Furthermore, let 
\begin{equation}
	D_j:= \max\{a_1,\ldots,a_d, b_1,\ldots,b_d\}
\end{equation}
 be the maximal distance of $s_i$ to the boundary of $J_j$.

Then, $\forall x \in J_j: f(x)  \leq f_j + L \max_{x \in J_j} \norm{x - s_j}_\infty^p =f_j + L D_j^p =: q_i$. 

Since, the hyperrectangles formed a covering of domain $i$, the desired upper bound on the maximum is $\max_{j=1}^N q_j \geq \max_{x \in I} f(x)$.
By the same argument, a lower bound on the maximum based on the finite sample can be obtained as:
$\min_{j=1}^N f_j - L D_j^p  \leq \min_{x \in I} f(x)$.

In conclusion, we have 
\begin{align}
\underline M := \min_{j=1\,...,N} f_j - L \, D_j^p  \leq \min_{x \in I} f(x) \label{eq:minhoeldineqfinsample}\\
\overline M := \max_{j=1,...,N} f_j + L \, D_j^p  \geq \max_{x \in I} f(x). \label{eq:maxhoeldineqfinsample}
\end{align}
%
A \textit{batch} algorithm would get a sample, construct a partition $J_1,...,J_N$ for that sample and then compute the minimum and maximum bounds $\underline M, \overline M$ given above.


An \textit{adaptive} algorithm for bounded optimisation would be able to incrementally expand the given sample. That is, it would incrementally partition domain $I$ into increasing small sub-hyperrectangles until the computational budget is expended or the bounds on the maxima and minima have shrunk satisfactorily. 

The exploration criterion of where to obtain the next function sample could be tailored to finding $\overline M$ or  $\underline M$, depending on the task at hand.

For instance, in a minimisation problem one might choose to obtain a new sample point in the hyperrectangle $J_{j_*}$ where  $ j_* = \arg\min_{j=1\,...,N} f_j - L \, D_j^p $ is the current rectangle where our present bounds allow for the lowest value to be.


%based on reduction of the error $E=\overline M - \underline M$. That is, we desire to add a sample such that $E$ is minimal after the new sample is inserted. 

%
%As an illustration, consider the following situation: We are given the covariance function
%$k(t,t') := \exp(- \abs{t-t'})$. One a priori upper bound on $\bar v$ would be to choose $k(t,t) = 1$.
%If we have training data $\data$, we can compute $\bar v$ as per  Eq. \ref{eq:barvQ}.
%
%To this end, we notice that $k(\cdot,t') = f \circ g (\cdot)$ with $f(t) = \exp(- t)$ and $g(t) = \abs{t-t'}$. Both functions are Lipschitz. That is, they are H\"older with exponent $p_f=p_g = 1$ and constants $L_f=L_g = 1$.

