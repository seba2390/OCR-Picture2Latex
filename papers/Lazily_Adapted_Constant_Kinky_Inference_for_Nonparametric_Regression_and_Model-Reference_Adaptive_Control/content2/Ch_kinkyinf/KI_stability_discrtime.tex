\section{Stability guarantees in discrete-time systems}
\label{sec:KIMRACstabbounds}

In the previous section, we gave an illustration LACKI-MRAC -- a combination of a feedback-linearising controller with our KI learning method. The results are encouraging -- our adaptive control law managed to learn a dynamic system online and to track a reference in the presence of wingrock dynamics where other state of the art methods failed. 

To simulate the dynamics we relied on a first-order Euler approximation resulting in a discrete-time dynamical system.

In this section, we study the convergence of LACKI-MRAC in such discrete-time systems.
In these we will consider both the offline and the online learning setting. In the former, the LACKI-learner receives a sample set once and builds a controller that remains unaltered during execution time.
For this case, we will provide robustness bounds on the control success (as quantified by a bound on the norm of the error dynamics) as a function of the remaining uncertainty of the trained LACKI model. 

In the online learning setting, which we considered in the wing-rock control simulations, the LACKI-learner gets updated with the most recent observation after each time step. Provided the initial uncertainty is bounded on the given state space, we will be able to guarantee that LACKI-MRAC leads to a closed-loop system that is globally asymptotically stable.

\subsection{Tracking error bounds for the offline learning setting} 
In the offline learning setting, the predictor sequence $\seq{\predfn}{n \in \nat}$ of adaptive elements is based on only one fixed data set $\data_0$ at time $0$ which is not updated subsequently.
That is $\data_n = \data_0, \forall n \in \nat$.

Given some data set $\data_n =\data_0$ at time $n$ and the resulting predictor $\predfn(\cdot)$, the model error is given by $F_n(\cdot) := f(\cdot) - \predfn(\cdot)$. Since we assume constant data, the error does not change either. That is, we have $F_n \equiv F_0 \equiv: F, \forall n$.
 
In our analysis, we consider discrete-time dynamical systems. For example the dynamics might be first-order Euler approximations of the control-affine dynamics of Sec. \ref{sec:KIMRAC}.
Consequently, the error dynamics as per Eq. \ref{eq:errordynmrac} translate to the recurrence relation:
\begin{equation}\label{eq:errordynmrac_nIMRACdiscrete}
	e_{n+1}  = M e_n + \tinc F_n(x_n)
\end{equation}
where $\tinc \in \Real_+$ is a positive time increment and  $n$ is the time step index. Furthermore,
%
\begin{equation}
	F_n(x_n) = f(x_n) - \predfn(x_n) = B \bigl(\nu_{ad}(x_n) -  \tilde a(x_n)\bigr) 
\end{equation}
is an uncertain increment due to the model error of the learner (cf. Eq. \ref{eq:errordynmrac}), 						$B = \left(\begin{array}[h]{c}
				O_m \\ \tinc I_m
						\end{array}\right)$ and 

\begin{equation}
	M = \left(\begin{array}[h]{cc}
				I_m &  \, \tinc I_{m}\\
				-\tinc K_1 & I_m- \tinc K_2 
						\end{array}\right) 
\end{equation}
					is the (error state) transition matrix. 	
Here, $m = \frac d 2$ is half the dimensionality of the state space, $I_m$ denotes the $m \times m$ identity matrix and $K_1,K_2$ are gain matrices that can be freely chosen by the designer of the linear pseudo controller.		
%This is completely consistent with the uncertain recurrence considered in Eq. \ref{eq:dynextstatediscrete1}, with the exception, that the uncertainty of increment $F_n$ is not probabilistic. Instead it is an uncertain quantity reflecting the prediction error of the trained KI learner. Assuming its assumptions are met, Thm. \ref{thm:KIdesiderata} tells us that $F_n$ is interval-bounded. Employing the same type of argument as in Lem. \ref{lem:origprocdiscrfacts} we see that the solution of the recurrence is given by:
By induction, it is easy to show that the recurrence can be converted into the closed-form expression:
\begin{align*}
	\vc e_n &= M \vc e_{n-1} + \tinc \vc F(x_{n+1})  \\ 
	&= \dots= M^k \, \vc e_0 + \tinc \sum_{i=0}^{n-1} M^{n-1-i} \, \vc F(x_i)	.	
\end{align*}	
For vector norm $\norm{\cdot}$, let $\matnorm{\cdot}$ denote a matrix norm such that $\norm{Mv} \leq \matnorm{M} \norm{v}$ for all suitable matrices $M$ and vectors $v$.
We desire to bound the  norm of the error. To this end, we leverage that the norms are sub-additive and sub-multiplicative to deduce: 
\begin{align*}
	\norm{\vc e_n} &\leq   \matnorm{M^{n}} \, \norm{\vc e_0} + \tinc \sum_{i=0}^{n-1}  \matnorm{M^{n-1-i}} \, \norm{\vc F(x_i)}		\\
	&\leq \matnorm{M^{n}} \, \norm{\vc e_0} + \tinc \maxerrn_n	 \sum_{i=0}^{n-1}  \matnorm{M^{n-1-i}} =: \varrho[n]
\end{align*}
where  $\maxerrn_n$ is chosen such that we can guarantee that $\maxerrn_n	\geq \max_{i=1,...,n-1} \norm{F(x_i)}$.
For instance, we could choose $\maxerrn_n := \sup_{s \in \iaspace_n} \norm{F(s)} $
	where $\iaspace_n = \bigcup_{t< k} \{x \in \iaspace | \norm{x - \xi[t]} \leq \varrho[t] \}$ is the union of the possible states around the reference trajectory $\xi[\cdot]$ at previous time steps.
\begin{remark}	[Bounded error innovations] 
We assume there exists a maximum model error norm $\maxerrn$, i.e. $\forall i: \norm{F_i(x_i)} \leq \maxerrn$ for some bound $\maxerrn \in \Real$.
This is a realistic assumption in any physically plausible systems where the drift forces $a(x)$ are inevitably bounded. Since also, for any finite data, our KI learner has a bounded prediction function, the discrepancy given by $F$ is bounded as well. 
\label{rem:bnd_err_innovations}
\end{remark}

Given the boundedness of $F$ we have: \vspace{-1em}
\begin{equation}
	\norm{\vc e_n} \leq   \matnorm{M^{n}} \, \norm{\vc e_0} + \tinc \maxerrn \sum_{i=0}^{n-1}  \matnorm{M^{n-1-i}}.
\end{equation}

The right-hand side is convergent provided the gains $K_1, K_2$ are chosen such that $M$ is stable, i.e. $\specrad(M) <1$, and provided $\maxerrn_n$ is is bounded (see e.g. \cite{calliess2014_thesis}). 

Whether or not $M$ is stable, in low dimensions, the sums can be computed offline and in advance. This is of great benefit if 
 the controller that is building on the error bounds is utilising optimisation-based control with a finite time-horizon.   

To obtain a (conservative) closed-form bound on the error norms \cite{calliess2014_thesis}
contains a derivation and discussion of the following result:

\begin{thm}\label{thm:normboundsboundeddisturbmainbody}
Let $(F_n)_{k \in \nat_0}$ be a norm-bounded sequence of vectors with $\maxerrn_n :=\max_{i \in \{0,\ldots,n-1\}} \norm{F_i(x_i)} \leq \maxerrn \in \Real$. 
For error sequence $(e_n)_{n \in \nat_0}$ defined by the linear recurrence 
	$e_{n+1}  = M e_n + \tinc F_n(x_n) \,\,\,\, (k \in \nat_0)$, we can define the following bounds:
	
	\begin{enumerate}
	\item $\norm{\vc e_n} \leq \matnorm{M^{n}} \, \norm{\vc e_0} + \tinc \maxerrn_{n} \sum_{i=0}^{n-1}  \matnorm{M^{i}}$. If $\specrad(M) <1$ and $\exists \maxerrn$ $:\maxerrn\geq \maxerrn_{n-1} \geq 0, \forall n$, the right-hand side converges as $n \to \infty$.
	\item Let $k_0 \in \nat, k_0 >1$ such that $\matnorm{M^n} <1, \forall n \geq k_0$, let $\varphi := \matnorm{M^{k_0}} <1$ and let $\delta_n := \floor{n/k_0}$. If $r:=\specrad(M) < 1$, for $n > k_0$,  we also have:
	\begin{align*} \norm{\vc e_n} &\leq c \, \varphi^{\delta_n} \, \norm{\vc e_0} + \tinc \maxerrn_{n} \Bigl(\sum_{j=0}^{n_0-1} \matnorm{M^j} + c \, k_0 \frac{\varphi-\varphi^{ \floor{\frac{n}{k_0}}+1 }}{1-\varphi} \Bigr)\\
	& \stackrel{n \to \infty}{\longrightarrow} C \leq \tinc \maxerrn \sum_{j=0}^{n_0-1} \matnorm{M^j} 
	+ \frac{\tinc \maxerrn\,  c\, k_0 \,\varphi}{1-\varphi} 
	\end{align*} 
	for some constants $C,c \in \Real$.
	Here, possible choices for $c \in \Real$ are: 
	
(i) $c= \max\{\matnorm{M^i} | i=1,\ldots,k_0-1 \}$ 

or (ii) $c = \frac{1}{(d-1)!} \Bigl(\frac{1 - d}{\log r}\Bigr)^{d-1}	\matnorm{M}^{d-1}\, \, \, r^{\frac{1 - d}{\log r}-d+1}  $.
%
Since $\matnorm{M} \neq 1$, one can also choose (iii) $c:= \matnorm{M}^{n_0}$. %In this case, $\matnorm{M^k} \leq  \, \matnorm{M}^{n_0}^{(k \text{ div } k_0) +1} $.

\item If $\matnorm{M} \neq 1$, we have: \newline
$\norm{\vc e_n} \leq \matnorm{M}^{n} \, \norm{\vc e_0} + \tinc \maxerrn_{n}   \frac{1-\matnorm{M}^n}{1-\matnorm{M}}. $
	\end{enumerate}
%\begin{proof}
%The theorem is proven in \cite{calliess2014_thesis}.
%
%\end{proof}	
\end{thm}


\subsection{Global asymptotic stability of KI-MRAC in the online learning setting}
In this subsection, we lift the assumption that the available sample is static. Instead we assume that at time step $k+1$ we get to see an additional sample of the uncertain drift at the state visited in the previous time step $k$. 
That is, the predictor $\predf_{n+1}(\cdot)$ is based on $\data_{n+1} = \data_n \cup \{ \bigl(\state_n, f(\state_n) \bigr)\}, \forall n $.
Therefore, we also need to index the innovations vector field by time. That is, $F_n := f - \predfn$ denotes the prediction error function (or innovation) due to the data available at time $n \in \nat$. 
As pointed out in Rem. \ref{rem:bnd_err_innovations}, the error innovations $F_n$ are assumed to be bounded for all finite sample sets $\data_n$. 

Above, we have seen that any continuous function can be approximated by some H\"older continuous LACKI predictor up to an arbitrarily small error. 
For convenience, we will establish the following definition:

\begin{defn}
We say that a continuous function $f$ is $L^*-p$-H\"older \emph{up to} error $\bar E_h \in \Real$ on domain $\inspace$ iff there is an $L^*-p$-H\"older function $\phi \in \hoelset {L^*} { } p$ and a function $\psi$ such that $\forall x: f(x) = \phi(x)+\psi(x), \, \sup_{x \in \inspace} \metric_\outspace\bigl(0,\psi(x)\bigr) \leq \bar E_h$.
\end{defn}


%\begin{lem} Assume the target $f$ is bounded on sub-domain $I \subseteq \inspace$ and that the data increases between time steps, i.e. $\data_n \subseteq \data_{n+1}, \forall n \in \nat_0$. Then we have:
%\[\forall x \in I \subset \inspace : \norm{F_n (x)}_\infty \geq \norm{F_{n+1}(x)}_\infty.\]
%\jcom{true? perhaps not needed anyway..}
%\end{lem}
 
%Above theorems consider the case where the data becomes dense in the domain and we are interested in the worst-case prediction error. Relevant to our online-learning and control setting below is the situation where we are only interested in the error on a specific trajectory of inputs. If the prediction error on such a trajectory vanishes then we will be able to establish stability of the trajectory.
%In preparation of these considerations, we will establish the following facts:

%
%\begin{lem}
%Assume we are given a trajectory $\seq{x_n}{n \in \nat}$ of inputs that is bounded, i.e. where 
%$\metric_\inspace(x_n,0) \leq \beta$ for some $\beta \in \Real_+$ and all $n \in \nat$.
%Define $G_n = \{ x_i | i \in \nat, i < n\}$.
%
%Then we have \[ \dist(G_n,x_n) = \min\{\metric_\inspace(g,x_n) | \, g \in G_n\} \stackrel{n \to \infty}{\longrightarrow} 0.\]
%\begin{proof}
%The intuition of the following proof is that if the distances were not to converge there would be an infinite number of disjoint balls around the input points that summed up to infinite volume. This however, would be a contradiction to the presupposed boundedness of the sequence.
%We formalise this intuition as follows:
%For contradiction assume there is an $\epsilon >0$ such that for all $n \in \nat$ there exists an $N_n \geq n $ such that 
%\begin{equation}
%\dist(x_{N_n}, G_{N_n}) > \epsilon.
%\end{equation} 
%Hence, by definition of $G_{N_n} =\{ x_i | i < N_n\} $,
%\eqn{eq:i34kjjk3}{\forall i < N_n : \metric_\inspace(x_{N_n},x_i) > \epsilon.}
%
%Let $C_n := \bigcup_{i < n} \ball{\frac \epsilon 2}{x_i} $ be the union of all $\frac \epsilon 2$-balls around each point in $G_n$ and define $\bar I = \bigcup_{n \in \nat} C_n$.
%By definition, each $x_n$ is contained in $\bar I$.
%Since sequence $(x_n)_{n \in \nat}$ is bounded, $\bar I $ has a finite volume relative to some positive, shift-invariant measure $\mu$. I.e. $\mu(\bar I) < \infty$. Furthermore, $\mu(C_n) \leq \sum_{i <n} \mu(B_i) \leq \mu(\bar I)< \infty$ where $B_i := \ball{\frac \epsilon 2}{x_i}$. Since $M:=\mu(B_1) = \mu(B_n)\forall n \in \nat$ we have $\mu(C_n) \leq (n-1) M$.
%Define $m:= \ceil{\frac{\mu(\bar I)}{M}}$.
%Now, choose $n > m+1$. For $i=1,...,n$ $\exists N_i \geq i \forall j \leq N_i: \metric_\inspace(x_{N_i},x_j) > \epsilon$.  Define a permutation $\pi$ such that $\pi(N_1) \leq \ldots \leq \pi(N_n)$. Hence, we have $\metric_\inspace(x_{\pi(N_i)},x_{\pi(N_j)}) > \epsilon , \forall i,j =1,...,n, i < j$. Thus, $B_{\pi(N_i)} \cap B_{\pi(N_j)} = \emptyset , \forall i,j =1,...,n, i \neq j$. Hence, $\mu(\bar I) \geq \mu(C_{\pi(N_n)+1}) = \mu(C_{\pi(N_1)}) + \sum_{i=1}^n \mu(B_{\pi(N_i)}) = \mu(C_{\pi(N_1)}) +  n M > \mu(C_{\pi(N_1)}) + (m+1) M >\mu(C_{\pi(N_1)}) + \mu(\bar I)$ by definition of $m= \ceil{\frac{\mu(\bar I)}{M}}$. But since $\mu(C_{\pi(N_1)}) \geq 0$, we conclude the false statement $\mu(\bar I) > \mu(\bar I)$.
%\end{proof}
%\label{lem:bndseq_entails_distgridvanish}
%\end{lem} 
%
%
%\begin{lem}
%As before, let the observational noise level be bounded by $\obserr$ and assume the output- space pseudo-metric to be translation-invariant.
%Let target $f$ be $L^*-p$-H\"older up to error $E$. That is, there is an $L^*-p$-H\"older function $\psi$ and a function $\phi$ such that $\forall x: f(x) = \phi(x)+\psi(x), \, \metric_\outspace\bigl(0,\psi(x)\bigr) \leq E $.
%
%Assume we are given a trajectory $\seq{x_n}{n \in \nat}$ of inputs that is bounded, i.e. where 
%$\metric_\inspace(x_n,0) \leq \beta$ for some $\beta \in \Real_+$ and all $n \in \nat$.
%Furthermore, assume $\data_{n+1} = \data_n \cup \{ \bigl(x_n, \tilde f(x_n)\bigr) \}$ and thus, $G_n = \{ x_i | i \in \nat, i < n\}$.
%Then the prediction error on the sequence vanishes, i.e. \[\metric_\outspace\bigl(\predfn(x_n),f(x_n) \bigr) \stackrel{n \to \infty}{\longrightarrow} \metric_\outspace(0,\obserr) + 2E.\]
%\begin{proof}
%We have established that $\predfn$ is $L(n)-p$- H\"older with $L(n) \leq \bar L$ for some $\bar L \in \Real_{\geq 0}$.
%
%Since sequence $(x_n)$ is bounded, Lem. \ref{lem:bndseq_entails_distgridvanish} is applicable. 
%Let $\xi_n := \argmin_{g \in G_n} \metric_\inspace(x_n,g)$ denote the nearest neighbour of $x_n$ in $G_n = \{x_1,...,x_{n-1}\}$.
%Hence, $\forall n \in \nat: \metric_\inspace (x_n,\xi_n) \to 0$ as $n \to \infty$.
%
%We have:
%$\metric_\outspace\bigl(\predfn(x_n) , f(x_n)\bigr) \leq \metric_\outspace\bigl(\predfn(x_n) ,  f(\xi_n)  \bigr) + \metric_\outspace\bigl(f(\xi_n) , f(x_n)\bigr) \stackrel{(i)}{=}\metric_\outspace\bigl(\predfn(x_n) , \predfn(\xi_n) \bigr) +\metric_\outspace\bigl(0 , \obserr\bigr) + \metric_\outspace\bigl(f(\xi_n) , f(x_n)\bigr) \leq \metric_\outspace\bigl(\predfn(x_n) , \predfn(\xi_n) \bigr) +\metric_\outspace\bigl(0 , \obserr\bigr) + \metric_\outspace\bigl(\phi(\xi_n) , \phi(x_n)\bigr) + 2E \stackrel{(ii)}{\leq} 2 \max\{\bar L, L^* \} \metric_\inspace(x,\xi_n^x )^p + \metric_\outspace(0,\obserr) + 2E \longrightarrow \metric_\outspace(0,\obserr) + 2E$.
%where steps denoted by (i) follow from Lem. \ref{lem:bilinaddtransinvgroup} and (ii) follows from the assumed H\"older properties.
%\end{proof}
%\label{lem:vanisishingseqprederr_groups}
%\end{lem} 
 
 
\begin{lem} For some constant $L^*$ let the target $f$ be $L^*-p$-H\"older continuous up to level $\bar E_h$. 
%Furthermore, 
%assume that the observational error is zero, i.e. $\obserrbnd =0$.

If $F_n(x_n) = f(x_n) - \predfn(x_n)$ is bounded for all $ n \in \nat_0$ then we have 
\[\forall k \in \nat_0: \norm{F_n(\state_{n+k})}_\infty \stackrel{n \to \infty}{\longrightarrow} [0, \frac \hestthresh 2 +  \obserrbnd +2\bar E_h]. \] 


\begin{proof}
If sequence $\seq{F_n(x_n)}{n\in \nat} $ is bounded then by Thm. \ref{thm:normboundsboundeddisturbmainbody}, sequence \seq{x_n}{n\in \nat} is bounded. 
Furthermore,
by Lem. \ref{lem:LACKIsampleconsistency}, we know that our predictors $\predfn(\cdot)$ are sample-consistent up to $\frac \lambda 2$ and hence, $\norm{ F_n(x)}_\infty \leq \frac \hestthresh 2 +  \obserrbnd=: E_s ,\forall x \in \data_n=\{x_1,...,x_{n-1}\}$.

The rest follows directly from Lem. \ref{lem:vanisishingseqprederr_groups} assuming $\metric_\outspace (f,f') = \norm{f-f'}_\infty$.
\end{proof}
\label{lem:innovnorm_convergence}
\end{lem}


\begin{remark}
In particular, in case the target is H\"older continuous $(\bar E_h =0)$ and there is no observational error ($\obserrbnd =0$), parameterising our LACKI rule with $\hestthresh =0$ yields vanishing innovations, i.e. 
\[\forall k \in \nat_0: \norm{F_n(\state_{n+k})}_\infty \stackrel{n \to \infty}{\longrightarrow} 0. \]
\end{remark}

The next theorem gives a stability guarantee for the tracking error of our LACKI-MRAC controller:

\begin{thm}[Tracking error convergence]
Assume the preconditions and definitions of Lem. \ref{lem:innovnorm_convergence} hold.
If the initial error innovation function is bounded, i.e. if $\exists b \in \Real \forall x: \norm{F_0(x)} \leq b $, and, if $M$ is a stable matrix, i.e. if $\specrad(M) <1$, then the tracking error converges to the interval $[0,\frac \hestthresh 2 +  \obserrbnd + 2 \bar E_h]$. That is,
\[\norm{e_n} \stackrel{n \to \infty}{\longrightarrow} [0,\frac \hestthresh 2 +  \obserrbnd + 2 \bar E_h]. \]

\end{thm}
\begin{proof}
We show 
\begin{equation}
\forall \epsilon > 0 \exists N \in \nat \forall n\geq N: \norm{e_n} \leq \epsilon.
\end{equation}
Referring to (i) in Thm. \ref{thm:normboundsboundeddisturbmainbody} we see that for all $k \in \nat$ we have: 
\begin{equation}
\norm{\vc e_{n+k}} \leq \matnorm{M^{k}} \, \norm{\vc e_n} + \sigma \, \tinc \, Q_{n:n+k} 
\label{eq:errdyn320894}
\end{equation}
where $\sigma$ is some nonnegative number and $Q_{n:n+k}:=\max\{\norm{F_n(x_n)},\ldots,\norm{F_{k+n-1}(x_{k+n-1})} \}$.


Let $\epsilon >0$. By Lem. \ref{lem:innovnorm_convergence}, the sequence $\seq{\norm{F_n(x_n)}_\infty}{n \in \nat}$ converges to the interval $[0, \frac \hestthresh 2 +  \obserrbnd +2\bar E_h]$. Hence, we can infer the following two statements:


(I) There exists an index $n_0 \in \nat$ such that for all $k$: $Q_{n_0:n_0+k} \leq \frac{\epsilon}{2 \Delta \sigma}+\frac \hestthresh 2 +  \obserrbnd +2\bar E_h$.

On the other hand,
by Thm. \ref{thm:normboundsboundeddisturbmainbody}, boundedness of $F_0$ entails boundedness of the error dynamics. Thus, $\exists b \in \Real \forall n: \norm{e_n} \leq \beta$.
Knowing that the error dynamics are bounded by some $\beta \geq 0$ we see that $\matnorm{M^{k}} \, \norm{\vc e_n} \leq \matnorm{M^{k}} \beta \stackrel{k \to \infty}{\longrightarrow} 0$. Here, the convergence to zero follows from the assumption that $M$ is a stable matrix. Hence,
we have:

(II) For any $n$ we can choose $k_0(n) \in \nat$ such that for all $k \geq k_0(n): \matnorm{M^{k}} \, \norm{\vc e_n} \leq \frac\epsilon 2 $.
%
Combining (I) and (II) with Eq. \ref{eq:errdyn320894} allows us to conclude that for any $n \geq N:= n_0 +k_0(n_0)$  we have 
\begin{equation*}
\norm{\vc e_{n}} \leq \frac \epsilon 2 + \sigma \, \tinc \,\frac{\epsilon}{2 \Delta \sigma} +\frac \hestthresh 2 +  \obserrbnd +2\bar E_h= \epsilon +\frac \hestthresh 2 +  \obserrbnd +2\bar E_h. 
\label{eq:errdyn3208942}
\end{equation*}
Hence, we have shown that $\forall \epsilon >0 \exists N \geq 0 \forall n\geq 0: \norm{\vc e_{n}} \leq \epsilon +\frac \hestthresh 2 +  \obserrbnd +2\bar E_h$ which is the desired convergence result.

\end{proof}
Note, since the error converges to a bounded set the state will converge to the target trajectory. So, if the target trajectory is bounded, the continuity of the control law (as a function of state) implies that the control is bounded as well.

%
%
\begin{cor}
In the special case of error-free observations of a H\"older continuous target function, choosing a parameter $\hestthresh =0$ implies:
\[\norm{e_n} \stackrel{n \to \infty}{\longrightarrow} 0. \]
That is, the error dynamics are globally asymptotically stable with equilibrium point 0.
Furthermore, the control action sequence $\seq{u(x_n)}{n \in \nat}$ converges, provided the target trajectory $\seq{\xi_n}{n \in \nat}$ does and $\inspace$ is a Hilbert space.
\end{cor}

\begin{proof} The stability statement is an immediate consequence of the preceding theorem. 
Remember the control action at time $n$ is of the form $u_n := u(x_n) = -\predfn(x_n) - A x_n $ for some Matrix $A$. We show that $(u_n)$ is a Cauchy sequence, provided that the target sequence $\xi_n$ is. Due to the Hilbert space assumption this implies the desired convergence result.

So, let $\epsilon >0$. Since $(e_n),(\xi_n)$ converge the state sequence $(x_n)$ converges and all three are Cauchy sequence. Hence, there is $N$ such that for all $n,m >N$: $\norm{e_n-e_m}< \frac{\epsilon} {2 \matnorm{K}}$ and $\norm{x_n-x_m}< \frac{\epsilon} {2\bar L}$. 
Hence, by definition of the control law and H\"older continuity of the predictors with constant $\bar L$, for all $m,n >N:$ $\norm{u_n-u_m} \leq \matnorm{K} \norm{e_n -e_m} + \norm{\predfn(x_n) - \predf_m(x_m) } \leq \frac{\epsilon}{2} + \bar L \norm{x_n -x_m} \leq \epsilon $.

\end{proof}


