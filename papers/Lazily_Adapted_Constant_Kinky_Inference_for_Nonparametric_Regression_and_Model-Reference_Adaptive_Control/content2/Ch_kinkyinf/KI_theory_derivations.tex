
\chapter{Supplementary derivations for kinky inference}
\label{ch:KIderivations}
%\section{H\"older continuity and metric property of the exponentiated map \jcom{Check -- don't think this subsection needed anymore.. (?)} }
%
 %
%We endeavour to show that $x \mapsto L \norm{ x -s}^p$ is Hoelder continuous with coefficient $L$ and exponent $p$. This will become important since these functions will serve as essential building blocks of our conservative Hoelder function estimates. 
%
%To this end, we will show that $(x,y) \mapsto \norm{x-y}^p$, for $p \in (0,1] $, is a metric.  This can be utilized to show that
%$x \mapsto \norm{ x -s}^p$ is a Hoelder continuous function.
%
%To establish the metric property, we note that any positive definite function that is subadditive gives rise to a canonical metric. Formally :
%
%
%To prove $(x,y) \mapsto \norm{x-y}^p$ is a metric, we need to establish a few facts. 
%Firstly, we remind ourselves of the following well-known fact:
%\begin{lem} \label{lem:pd_n_concave_subadditive}
 %A nonnegative, concave function $g:\Real_{\geq 0} \to \Real$ with $g(0) = 0$ is subadditive. 
 %That is, $\forall x,y \in \Real_{\geq 0}: g(x+y) \leq g(x) + g(y)$. 
 %
 %\begin{proof}
%If $x = y = 0$ then subadditivity trivally holds:  $0=g(x+y) \leq g(x) + g(y) = 0$.
%So, let $x, y \in \Real_+$ such that $x >0 \vee y >0$.
%We have, $g( x +y) = \frac{x}{x+y} g(x+y) + \frac{y}{x+y} g(x+y) \leq g(\frac{x}{x+y} (x+y) ) +  g(\frac{y}{x+y}(x+y)) = g(x) + g(y)$.
%Taking into account that $\frac{x}{x+y}, \frac{x}{x+y} \in [0,1]$, the last inequality can be seen as follows:
%
 %Since $g$ is concave we have 
%$\forall p \in [0,1], x \in \Real: g(p x) =  g(px + (1-p) 0) \geq p g(x) + (1-p) g(0) \geq p g(x)  $. The last inequality follows from $g(0) \geq 0$.
%\end{proof}
%\end{lem}
%
%\begin{lem} \label{lem:x2p_pdNsubadd}
 %Let $h: \begin{cases} \Real_{\geq 0} \to \Real_{\geq 0},\\ \, x \mapsto x^p\end{cases}$, for $p \in (0,1] $. $h$ is positive definite and subadditive. 
 %That is, (i) $h(0) = 0 $ and  $h(x) > 0, \forall x \neq 0$ and (ii) $\forall x,y \in \Real_{\geq 0}: h(x+y) \leq h(x) + h(y)  $.
 %Moreover, $h$ is concave. If $p \in (0,1)$, h is strictly concave. 
 %
%\begin{proof}
%\textit{Pos. def. :} $h(0) = 0$.  Also $\lim_{x \to 0_+} h(x) =0$. Since $\nabla h (x) = p h^{p-1}(x) >0 $ for $x >0$, $h$ is strictly monotonically increasing on $\Real_+$. Hence, $h(x) > 0, \forall x >0$. 
%
%\textit{Concavity:} If $p =1$, $h$ is linear and hence, concave. If $p \in (0,1)$, $\nabla^2 h(x) = p (p-1) h(x)^{p-2} > 0$ regardless of $x$.
%
%\textit{Subadditivity:} Follows directly with Lem. \ref{lem:pd_n_concave_subadditive} on the basis of established positive definiteness and concavity.
  %
%\end{proof}
%\end{lem}
%
%
%
%\begin{lem}\label{lem:hoeldererror_metric}
%Let $p \in (0,1]$.
%With definitions as in Thm. \ref{thm:subaddmetric1}, we assume that set $G$ is endowed with a quasi- metric $d$. Function
%$\metric: \begin{cases} \inspace \times \inspace \to \Real_{\geq 0} \\ (x,y) \mapsto \Bigl(d(x,y)\Bigr)^p \end{cases}$ is a quasi-metric on $G$.
%If $d$ is a metric then so is $\metric$.
%\begin{proof}
 %By Lem. \ref{lem:x2p_pdNsubadd}, $x\mapsto x^p$ is pos. def. and subadditive. Therefore, positive definiteness and the triangle inequality of $d$ readily extend to $\metric$ as follows: 
%
%\textit{Pos. def.:}
%Let $x=0$. $\metric(x,x) = d(x,x)^p = 0^p = 0$. If $x \neq 0$ then $k :=d(x,x) \neq 0$. Hence   $\metric(x,x) = d(x,x)^p = k^p \neq 0$.
%
%\textit{Triangle inequality:}
%Choose arbitrary $x,y,z \in \inspace $. We have $\metric(x,z) = d(x,z)^p \leq (d(x,y) + d(y,z) \bigr)^p \leq d(x,y)^p + d(y,z)^p = \metric(x,y) + \metric(y,z)$. Here, the first inequality followed from the triangle inequality of quasi-metric $d$ and the second from subadditivity properties established in Lem. \ref{lem:x2p_pdNsubadd}.
%
%\textit{Symmetry:} If $d$ is a metric it is symmetric. Hence, $\metric(x,y) = d(x,y)^p = d(y,x)^p = \metric(y,x), \forall x,y \in \inspace $ in which case $\metric$ also is symmetric.
%\end{proof}
%\end{lem}
%
%Before proceeding we establish a slight generalization of the well-known \textit{reverse triangle inequality}:
%
%\begin{lem}[Reverse Triangle Inequality] \label{thm:revtriangle}
%Let  $\inspace$ be a set and $\metric : \inspace^2 \to \Real$ a symmetric function that fulfills the triangle inequality. 
%That is,  $\forall x,y,z \in \inspace: \metric (x,y) = \metric(y,x) \wedge \metric(x,z) \leq \metric(x,y) + \metric(y,z)$.
%
%Then \[\forall x,y,z \in \inspace: \abs{\metric(x,y) - \metric(z,y)} \leq \metric(x,z).\]
%\begin{proof}
%
%For contradiction, assume 
%$\abs{\metric(x,y) - \metric(z,y)}>\metric(x,z)$ for some $x,y,z \in \inspace$.
%This is implies  
%$
%(i)\,\,\, \metric(x,y) - \metric(z,y)>\metric(x,z) 
%\, \, $  or  $
  %\,\,  (ii) \,\,\,\, \metric(z,y)-\metric(x,y) >\metric(x,z)$.
%Both inequalities contradict the triangle inequality:
%$(i) \Leftrightarrow \metric(x,y)  >\metric(x,z) \metric(z,y) $ and 
%$(ii) \Leftrightarrow  \metric(z,y) > \metric(x,z) + \metric(x,y) =  \metric(z,x) + \metric(x,y)$.
%
%\end{proof}
%\end{lem}
%
%
%\begin{thm}
%Let $\metric$ be a metric on set $G$. For arbitrary $s \in \inspace $ we define $\phi_s: \inspace \to \Real $ as $\phi_s: x \mapsto \metric (x,s) $.
%$\phi_s $ is Lipschitz with respect to metric $\metric$. That is, \[\forall x,y \in \inspace : \abs{\phi_s(x) - \phi_s(y) } \leq \metric (x,y). \]
%\begin{proof}
%$\abs{\phi_s(x) - \phi_s(y)} = \abs{\metric(x,s) - \metric(y,s)} \stackrel{Thm. \ref{thm:revtriangle}} {\leq} \metric(x,y), \forall x,y,s \in \inspace $.
%\end{proof}
%\end{thm}
%Finally, combining this theorem with Lem. \ref{lem:hoeldererror_metric} immediately establishes that mappings of the form $\metric(\cdot,s)^p$ and hence, of the form
%$\norm{\cdot -s}^p$ are Hoelder continuous with exponent $p$ ($\in (0,1]$):
%
%\begin{thm}
%Let $\metric$ be a metric on set $G$ and let $p \in (0,1], L \geq 0$. For arbitrary $s \in \inspace $ we define $\phi_s:\begin{cases}  \inspace \to \Real \\ x \mapsto L \, \bigl(\metric (x,s) \bigr)^p\end{cases}$.
%$\phi_s $ is Hoelder with exponent $p$. That is, \[\forall x,y \in \inspace : \abs{\phi_s(x) - \phi_s(y) } \leq L \, \metric (x,y)^p. \]
%In particular, for any norm $\norm \cdot$ on $G$ and $s \in \inspace $, mapping $x \mapsto L \norm{x-s}^p$ is Hoelder with constant $L$ and exponent $p$.
%\begin{proof}
%By Lem. \ref{lem:hoeldererror_metric}, $\metric(\cdot,\cdot)$ is a metric on $G$. Hence, Lem. \ref{thm:revtriangle} is applicable yielding:
%$\abs{\phi_s(x) - \phi_s(y)} = L \, \abs{\metric(x,s)^p - \metric(y,s)^p} \stackrel{Lem. \ref{thm:revtriangle} } {\leq} \metric(x,y)^p, \forall x,y,s \in \inspace$. The last sentence concerning the norms follows from the fact that a mapping $(x,y) \mapsto \norm{x-y}$ defines a metric.
%\end{proof}
%\end{thm}
%
%
%
 %At first glance, the result may seem intuitive. However, it should be noted that, for $p >1$, $x \mapsto \norm{x -s}^p$ no longer is Hoelder with exponent $p$. This is consistent with the well-known fact that Hoelder functions with exponent $>1$ are constant functions.
%
%We conclude this section by the following theorem stating that any concatenation of Hoelder continuous functions is Hoelder continuous:
%
%\begin{thm} \label{thm:concathoelder}
%Let $(\statespace, \metric)$ be a metric space and $f,g : \statespace \to \statespace$ be two Hoelder continuous mappings with Hoelder constants $L_f, L_g$ and Hoelder exponents $p_f,p_g$, respectively.
%Then, the concatenation $h=f \circ g: \statespace \to \statespace $ is also Hoelder continuous with Hoelder constant $L_h:= L_f L_g^{p_f}$ and exponent $p_h:=p_g \, p_f$.
%That is, 
%\[\forall \state,\state' \in \statespace: \metric\bigl(h(\state),h(\state')\bigr) \leq L_h \bigl(\metric(\state,\state')\bigr)^{p_h}.\]
%\begin{proof}
%%\begin{align}
%$\metric\bigl(f \circ g(\state),f\circ g(\state')\bigr) \leq L_f  \Bigl(\metric(g(\state),g(\state'))\Bigr)^{p_f}$
%$\leq L_f  \Bigl(L_g \metric(\state,\state')^{p_g}\Bigr)^{p_f}$ $= L_f L_g^{p_f}   \Bigl(\metric(\state,\state')\Bigr)^{p_g\, p_f} $ where in the first step we were using Hoelder-continuity of $f$ and in the second, we were using Hoelder continuity of $g$ combined with the fact that $(\cdot)^{p_f}$ is a monotonically increasing  function. 
%
%\end{proof}
%\end{thm} 
%In conjunction with Hoelder properties of the square root function established in Ex. \ref{ex:sqrtfctHoelder}, Thm. 
%\ref{thm:concathoelder} immediately yields the following result:
%\begin{cor}
%If $f: \statespace \to \statespace $ is Hoelder continuous with constant $L_f$ and exponent $p_f$ then $\sqrt{f}$ also is Hoelder, having  Hoelder constant $\sqrt{L_f}$ and exponent $p_f$.
%\end{cor}

\section{Convergence properties }
We will construct function estimates that converge to the ground truth in the limit of large sample sets, provided the samples fulfil certain convergence properties themselves. To establish this in a rigorous manner, we need to review a few core convergence concepts. 
\begin{defn}[Uniform convergence of function sequences] Let $(\inspace, \metric_\inspace), (\outspace,\metric_\outspace)$ be two metric spaces.
For $n \in \nat$, let $f_n : I \to \Real$. $f_n \stackrel{n \to \infty}{\longrightarrow} f$ \emph{uniformly} (with respect to $\metric_\outspace$) if
\[  \forall \epsilon >0  \exists N_0 \in \nat \forall n\geq N_0 \forall x \in I: \metric_\outspace(f_n(x), f(x)) < \epsilon. \] 
\end{defn}

The key distinction between both variants of convergence is that in point-wise convergence, the convergence rate can depend on $x$. By contrast, in uniform convergence the rate is independent of function input $x$.


For our purpose the following definition of a dense set suffices:
\begin{defn}[Dense] Let $(\mathcal X,\metric)$ be a metric space.  
A set $D \subset \mathcal X$ is ($\metric$-) dense in a set $I \subset \mathcal X$ if every  $\metric-$ open subset of $I$ contains at least one point of $D$. That is:
$\forall x \in I, \epsilon > 0 \exists \xi \in D: \metric(\xi,x) < \epsilon$.     
\end{defn}


\begin{defn}[Uniform convergence of sequences of sets]\label{def:uniform_setconvergence}
Let $\bigl(\mathcal X,\metric\bigr)$ be a metric space.  
A sequence of sets $(Q_n)_{n \in \nat}$, $Q_n \subset \mathcal X$, $\abs{Q_n} < \infty$ converges to set $Q \subset \mathcal X$ \textit{uniformly} if we have:
\[\forall \epsilon > 0 \exists N_0 \in \nat \forall n \geq N_0 \forall q \in Q \exists q_n \in Q_n : \metric(q_n,q) \leq \epsilon. \]

If the sequence consists of finite sets, we can rewrite the condition as 
\[\forall \epsilon > 0 \exists N_0 \in \nat \forall n \geq N_0 \forall q \in Q : \min_{q_n \in Q_n} \metric(q_n,q) \leq \epsilon. \]

\end{defn}

In analogy to the intuition of uniform convergence of function sequences, uniform convergence of sets means that the rate of convergence does not depend on the location within the set.


In preparation for the derivations below, it will be useful to establish that uniform convergence to a dense subset of set $I$ implies uniform convergence to $I$ itself. Formally we express this as follows:

\begin{lem}
\label{lem:unifdenseset1}
Let $Q$ be a dense subset of $I$ and let $Q_n$ be a sequence of sets uniformly converging to $Q$ in the limit of $n \to \infty$. We have \[\forall e > 0 \exists N_0 \forall n \geq N_0 \forall x \in I \exists s \in Q_n: \metric(x,s) \leq e. \]


\begin{proof}
Let $e>0$, $N_0 \in \nat$ such that for all $n \geq N_0$ we have $\forall q \in Q \exists s_n(q) \in Q_n: \metric(s_n(q),q) \leq \frac{e}{2}$. 
Now, choose any $x \in I, n \geq N_0$. Since $Q$ is dense in $I$ we can choose $q \in Q$ such that $\metric(q,x) \leq \frac{e}{2}$. 
Hence, utilising the triangle inequality, we conclude $\metric(x,s_n(q)) \leq \metric(x,q) +\metric(q,s_n(q)) \leq \frac{e}{2} + \frac{e}{2} = e$.
\end{proof}
\end{lem}
An alternative statement of the lemma is that, under the lemma's premise, we have 

\[\forall e > 0 \exists N_0 \forall n \geq N_0 \forall x \in I : \inf_{s \in Q_n} \metric(x,s) \leq e. \]


\begin{defn} [``Eventually dense partition '']
For each $n \in \nat$ let $\mathcal P_n :=\{P_{n,1},...P_{n,n}\}$ be a partition of $I$ into subsets, each being a connected set and having the property that $P_{n,i}$ contains exactly one sample input $s_i$. Furthermore, let $r_{n,i} = \sup_{x,x' \in P_{n,i}} \frac{1}{2} \norm{x-x'}$ be $ P_{n,i}$'s ``radius'' w.r.t norm $\norm{\cdot}$. 
We call $(\mathcal P_n)_{n \in \nat}$ an \emph{eventually dense} partition sequence if the sequence $(r^*_n)$ of maximum radii, $r^*_n : = \max \{ r_{n,i} \}$, converges to zero as $n \to \infty$.

A sequence of inputs $(G_n)_{n \in \nat}$ will be called ``eventually dense'' if it is impossible to define a partition sequence separating the points in $G_n$ for each $n$ such that $G_n$ is not eventually dense.
\end{defn}
It should be clear that a sample grid that converges to a dense subset in $I$ is eventually dense (and vice versa).


%\jcom{dont think I need the next 2 results statements anymore:}
It will become useful to remind ourselves of the standard fact that monotone sequences converge if they are bounded:
\begin{thm}[Monotone convergence theorem]
Let $(a_n)_{n \in \nat}$ be a monotone sequence of real numbers. That is, $a_n \geq a_{n+1}, \forall n \in \nat$ or $a_n \leq a_{n+1}, \forall n \in \nat$. The sequence converges to a finite real number if and only if the sequence is bounded. In particular, if the sequence is monotonically increasing then $\lim_{n \to \infty} a_n = \sup a_n$. If it is decreasing, we have $\lim_{n \to \infty} a_n = \inf a_n$.
\end{thm}
This theorem can be directly applied to prove the following result:
\begin{cor}
\label{cor:monseqfctsencl}
Let $(\phi_n)_{n \in \nat}$, $\phi_n : I \to \Real$ be a point-wise monotonically increasing sequence of functions and, let $(\gamma_n)_{n \in \nat}$, $\gamma_n : I \to \Real$ be a point-wise monotonously decreasing sequence of functions. That is, $\forall x \in I \forall n \in \nat: \phi_n(x) \leq \phi_{n+1}(x) \wedge \gamma_n(x) \geq \gamma_{n+1}(x) $. Furthermore, we assume $\forall x \in I \forall n \in \nat: \phi_n(x) \leq \gamma_{n}(x)$. 

Then, both sequences are point-wise convergent with point-wise limits 
$\lim_{n \to \infty} \phi_n (x) = \sup_{n \in \nat} \phi_n \leq \inf_{n \in \nat} = \lim_{n \to \infty} \gamma_n (x) $.
\end{cor}
The corollary will be needed below when we establish convergence of our function estimate bounds.

\subsection{Convergence with respect to the exponentiated metric}
\label{sec:unifconvhoeldermetricequivmetricunifconv}
The purpose of the next two results is to show that it does not matter whether one establishes a convergence property with respect to a metric $\metric$ or whether one does so with respect to $\metric^p$.
\begin{lem}\label{lem:hoeldermetricconv}
Let $p \in (0,1]$, $\mathcal X$ be a metric space and $(x_n)_{n \in \nat}$ be a sequence in $\mathcal X$ converging to $x \in \mathcal X$ with respect to metric $\metric$ iff the sequence converges to $x$ with respect to $\metric^p$.

\begin{proof}
$\Rightarrow:$
Define the sequence $d_n :=  \metric(x_n,x) $.
Convergence with respect to $\metric$ means that $d_n \to 0$ as $n \to \infty$. Since $d_n \geq$ mapping $ \psi: \mathfrak x \mapsto \mathfrak x^p$ is well-defined for sequence inputs $d_n$. Furthermore, $\psi$ is a continuous function and thus, 
$\lim_{n \to \infty} \metric^p(x_n,x) = \lim_{n \to \infty} \psi(d_n) =  \psi( \lim_{n \to \infty} d_n) = \psi (0)  =0$.
Hence, $x_n$ converges to $x$ with respect to $\metric^p$ as well.
$\Leftarrow:$ Completely analogous.

\end{proof}  
\end{lem}

\begin{lem}\label{lem:hoeldermetricconv}
Let $p \in  (0,1]$, $\mathcal F$ be a metric space of functions. Sequence of functions $(f_n)_{n \in \nat}$ converges uniformly to $f$ with respect to metric $\metric$ iff the sequence also converges uniformly to $f$ with respect to $\metric^p$.


\begin{proof} $\Rightarrow:$
Since $f_n \to f$ uniformly, we have 
$\forall e >0 \exists N_e \geq 0 \forall n \geq N_e, x \in I : \metric(f_n , f) < e$. We show that also $\forall e >0 \exists N_e \geq 0 \forall n \geq N_e, x \in I : \metric(f_n , f)^p < e$.
Mapping $\psi: x \mapsto x^p$ invectively maps positive numbers onto the set of positive real numbers. Hence, $\metric(f_n, f) < e \Leftrightarrow \metric(f_n , f)^p < e^p$.
Let $e >0$ and $ \varepsilon := \sqrt[p]{e}$. Due to uniform convergence, we can choose $ N_\varepsilon$ such that $\forall x \in I, n \geq N_\varepsilon: \metric(f_n , f) <\varepsilon$ which is equivalent to stating $\forall x \in I, n \geq N_\varepsilon: \metric(f_n , f)^p <\varepsilon^p = e$. $\Leftarrow:$ Completely analogous.
\end{proof}  
\end{lem}




\section{Theoretical guarantees of the enclosure}

%\jcom{Talks about Lipschitz instead of Hoelder .. needs to be fixed}
%We assume the dimensionality of the target's input domain $I \subseteq \inspace$ is $d \geq 1$. 
 Remember, we defined the set of samples as $D_n= \{\bigl( s_i, \tilde f_i, \varepsilon(s_i) \bigr) \vert i=1,\ldots, N_n \} $. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the grid of sample inputs contained in $D_n$.
Our exposition of this subsection focusses on the case of one-dimensional output space. That is, $\outspace = \Real$ and we assume to be the target function $f$ to be a mapping $f: I \subseteq \inspace \to \Real$.


In this subsection, we assume $\metric_\inspace$ to be any suitable metric on the target function domain and  $\metricp(x,x') := \metric_\inspace^p(x,x')$. We have seen that (uniform) convergence with respect to $\metricp$ is equivalent to (uniform) convergence with respect to metric $\metric$ (see Sec. \ref{sec:unifconvhoeldermetricequivmetricunifconv}). Also note, that Lipschitz continuity with respect to $\metricp$ is H\"older continuity with respect to metric $\metric_\inspace$. %Therefore, we may use the terms Lipschitz continuity and H\"older continuity interchangeably if it is clear which metrics the terms refer to.
Furthermore, remember we are primarily interested in functions restricted to be H\"older with respect to $\metric$ and bounded by functions $\lbf,\ubf$. That is, we know the target is contained in $\mathcal K_{prior} = \lipset L \metricp  \cap \bfset$ where $\lipset L \metric$ denotes the $L$- Lipschitz functions with respect to  $\metricp$ and $\bfset :=\{\phi : \inspace \to \Real \, | \, \forall x \in \inspace : \lbf(x) \leq \phi(x) \leq \ubf(x)\} $. 

As a final note, at no point will we utilise the definiteness property of the metric. Therefore, our definitions and results extend from metrics to the more general class of semi-metrics.

%We will now define a data inference mechanism $M_{dinf}$ that maps the sample $D_n$ into a subset 
%
%\subsubsection{A priori knowledge} We assume, our  a priori knowledge is that we know, for each $J \subset I$ a nonnegative number $L[J]$ such that the underlying target $f \in \mathcal F$ is $L[J]-$Lipschitz on $J$. That is, $\mathcal K_{prior} =\{\phi \in \mathcal F \,|\, \forall J \subset I \forall x,x' \in J: \abs(\phi(x) - \phi(x') \leq L[J] \norm{x-x'}\}$.

%\subsubsection{Computing a posteriori knowledge}

\begin{defn}[Sample-consistent functions]
A function $\kappa: I \to \Real$ is called \textit{consistent with sample} $D_n$ (shorthand: \emph{sample-consistent}) if \[ \forall i \in \{1,\ldots,N_n\}: \kappa(s_i) \in [\tilde f_i - \varepsilon(s_i), \tilde f_i + \varepsilon(s_i)  ] =[\underline f(s_i),\overline f(s_i) ].\] The set of all sample-consistent functions on $I$ will be denoted by $\mathcal K(D_n)$.
\end{defn}
Informally speaking, $\mathcal K(D_n)$ contains all functions that could have created the sample set. Conversely, sample $D_n$ rules out all functions that are not sample-consistent as candidates for being the target. 

%In the lingo of our consideration on topological inference, $\mathcal K(D_n)$ embodies part of our posterior knowledge: $\mathcal K_{\text{post}} = \mathcal K(D_n)$. 





\begin{defn}[Enclosure]
Let $\decke,\boden: I \to \Real$ be two functions, such that $\decke
\geq \boden$ pointwise on $I$. We define their enclosure on $I$ as
the compact interval $\mathcal E^\decke_\boden (I)$ of all functions
between them. That is, \[\mathcal E^\decke_\boden (I) := \{\phi: I
\to \Real \vert \forall t \in I: \decke(t) \geq \phi(t) \geq
\boden(t) \}.\]
\end{defn}
For ease of notation we omit the domain argument whenever we refer
to $I$. That is, we write  $\mathcal E^\decke_\boden :=\mathcal
E^\decke_\boden (I)$.


An \emph{enclosure} is sample-consistent if it is contained in the set $\mathcal K(D_n)$ of sample-consistent functions. 

Based on the sample, we desire to find two
\emph{enclosing functions} $\decke_n: I \to \Real$, $\mathfrak
\boden_n: I \to \Real$ bounding the target from above and below that
are as tight as possible and converge to the target in the limit of
infinite, sample set size $N$. 
More formally, we would ideally like it to satisfy the following desiderata:

\begin{defn}[Desiderata] \label{defn:desiderata}
 Based on an observed, erroneous data set $D_n$, we desire to define enclosing functions $\decke_n, \boden_n$ that give rise to an enclosure $\einschluss$ with the following properties:

\begin{enumerate} 

    \item \textbf{Sample-Consistency}: The enclosure is sample-consistent, i.e. $\einschluss \subseteq \mathcal K(D_n)$.
		%The enclosing functions are sample-consistent, i.e. $\decke_n,\boden_n \in \mathcal K(D_n)$.
    
     \item \textbf{Exhaustiveness (conservatism)}: Every $\phi, \lbf \leq \phi \leq \ubf$ coinciding with target $f$ on with grid $G_n$ and
having at most constant $L[J]$ on every subset $J \subset I$ is also contained in the enclosure: $\forall \phi: I \to \Real, \phi \in \mathcal K_{prior} \cap \mathcal K(D_n): \phi \in \einschluss $. That is, $\mathcal K_{prior} \cap \mathcal K(D_n) \subseteq \einschluss$.
    In particular, the target is always contained in the enclosure: $f \in \einschluss$.

\item \textbf{Monotonicity:} Additional data does not increase uncertainy. That is,
if $G_n \subseteq G_{n+1} $, we have $\mathcal E^{\decke_{n+1}}_{\boden_{n+1}} \subseteq \einschluss, \forall n \in \nat$.
		   
\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I$ and for each input $x \in I$ let the sample error function $\varepsilon: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
\begin{itemize}
\item (a) The sequence of enclosures converges point-wise. In particular, 

$\forall x \in I: 0 \leq \lim_{n \to \infty} \decke_n (x) -\boden_n(x) \leq 2 m_x$.
In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
		
		\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
\end{itemize}	
   \item \textbf{Minimality (optimality)}: 
If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    

\end{enumerate}

\end{defn}
 
Next, we will derive enclosing functions that give rise to such an enclosure.

\begin{defn}[Sample point ceiling and floor functions]
\label{def:sampleceilandfloormultidim}
Remember, $\overline f(s_i) = \tilde f_i + \varepsilon(s_i)$ and $\underline f(s_i) = \tilde f_i - \varepsilon(s_i)$.  For the $i$th sample ($i \in \{1,\ldots,N_n\}$, $L \geq 0$, $x \in I$, we define  
\begin{enumerate}
	\item The $i$th \emph{sample point ceiling} function: \[\decke_n^i (x; L) :x \mapsto  \overline f (s_i) +  \,L \, \metricp(x, s_i), \]
	\item and the $i$th \emph{sample point floor} function: \[\boden_n^i (x; L) :x \mapsto  \underline f (s_i) -  \,L \, \metricp(x, s_i) .\] 
\end{enumerate}
%Here $n_i$ is a norm-dependent constant ensuring that both functions are $L$-Lipschitz with repect to $\norm{\cdot}$. 
If constant $L$ is clear from the context, its explicit mention shall be omitted.
\end{defn}
%Note, for the maximum-norm $\norm{\cdot}_\infty$ it is easy to see that  $n_i =1$ renders both functions $L$-Lipschitz. For the 1-norm, one can either utilize Lipschitz arithmetic to show that for $n_i=1$ the functions would be $d L$-Lipschitz or, one can leverage the well-known inequality~\cite{koenigsberger:2000}   $\norm{\cdot}_1 \leq d \norm{\cdot}_\infty$ to determine $n_i := \frac{1}{d}$.

We show that these functions give rise to enclosures that satisfy the first three Desiderata of Def. \ref{defn:desiderata}:
\begin{lem} 
\label{lem:samplefloorceiling}
Let  $i \in \{1,...,N_n\}$.
Enclosure $\mathcal E^{\decke_n^i}_{\boden_n^i}$ is (i) consistent with the $i$th sample and (ii) conservative. 
More specifically: 

(i)  \[\forall i \in \{1,...,N_n\}, \phi \in \mathcal E^{\decke_n^i}_{\boden_n^i}:  \phi(s_i) \in [\underline f(s_i), \overline f(s_i)].\]

(ii) If target $f: I \to \Real$ is $L$-Lipschitz on $J \subset I$ with respect to $\metricp$ then the corresponding sample ceilings are upper bounds and the sample floor functions are lower bounds. That is, for $i =1,\ldots, N$, $\forall x \in J$ we have \[\decke_n^i (x; L) \geq f(x)  \geq  \boden_n^i (x; L).\]

\begin{proof}
(i) Follows directly from the definitions.
(ii) We show the first inequality, $\decke_n^i (x; L) \geq f(x)$. (The proof of the second inequality,  $f(x)  \geq  \boden_n^i (x; L)$, is completely analogous.) 

Due to the H\"older property, we have 
$\forall x,x' \in J: \abs{f(x) - f(x')} \leq L \metricp(x,x').$
In particular, 
$\forall x \in J: f(x) - f(s_i) \leq\abs{f(x) - f(s_i)} \leq L \, \metricp(x, s_i).$
Hence, 
$\forall x \in J: f(x)  \leq f(s_i) + L \, \metricp(x, s_i)$ 
$\leq \tilde f(s_i) +\varepsilon(s_i) + L \, \metricp(x, s_i) = \decke_n^i (x; L)$.

%Since $\decke_n^i (x; L)  = \tilde f_i + \varepsilon(s_i) + L_i \metricp(x, s_i)$, this entails $\decke_n^i (x; L) \geq f(x)$ for all $\varepsilon(s_i) \geq 0$. Therefore, it suffices to focus on the case for $\varepsilon(s_i) =0$.
 %So, let $\varepsilon(s_i) =0$. 

%&\leq f(c_i) + \ell_i \normtop{x-c_i}_{\infty}:= \decke^i_n(x)
%\end{align}
%where $\ell_i = \sqrt d L_i$ is a Lipschitz constant for the max-norm.
%The last inequality is due to the well-known relationship 
%$\normtop{x}_\infty \leq \normtop{x}_2 \leq \sqrt{d} \, \normtop{x}_\infty $.



\end{proof}
\end{lem}



\begin{defn}[H\"older enclosure] \label{def:optenclosingfcts} On  $J \subset I$, define the \emph{optimal ceiling} as
\[\decke^*_n (x; L) : x \mapsto  \min_{i=1,\ldots,N_n} \decke^i_n(x;L)\]
and the \emph{optimal floor function} as
 \[\boden^*_n (x; L) :x \mapsto   \max_{i=1,\ldots,N_n} \boden^i_n(x; L).\]
The enclosure $\einschluss$ with enclosing function choices $\decke_n := \decke_n^*$ and $\boden_n := \boden_n^*$ will be called ``H\"older enclosure'' and be denoted by $\opteinschluss$.
If constant $L$ is fixed and clear from the context we may omit its explicit mention from the syntax.
\end{defn}
%An example for a Lipschitz enclosure is depicted in Fig. \ref{figlipencl_nonoise_neo}.
%\begin{figure*}
%%
	%\centering
		%\includegraphics[scale = .4]{content/content_hoelderquad2/figs/enclosure.pdf}
	%\caption{Example of an enclosure (for metric $\metricp: (x,y) \mapsto \norm{x-y}$) for a given sample of function values on one-dimensional domain $I = [0,2.5]$. The Lipschitz constant was one, uniformly over the entire domain. The samples were noise-free.}
	%\label{figlipencl_nonoise_neo}
%\end{figure*}

Before we can prove that the optimal functions are indeed optimal, we need to undergo some derivative preparations.

\begin{lem} \label{lem:optenclcontainstarget}
The H\"older enclosure always contains the target. That is, $\decke^*_n(x) \geq f(x) \geq \boden^*_n(x), \forall x \in I, n \in \nat$.
\begin{proof}
This is a direct consequence of Lem. \ref{lem:samplefloorceiling}.
\end{proof}
\end{lem}


\begin{thm} \label{thm:enclmain}
Choosing $\decke_n := \min\{ \decke_n^*, \ubf\}$ and $\boden_n := \max\{\boden_n^*,\lbf\}$ yields an enclosure $\einschluss$ that satisfies Desiderata 1-5 specified in Def. \ref{defn:desiderata}.
\begin{proof}
We bear in mind that $\einschluss = \bfset \cap \opteinschluss$.

\textit{1) Consistency}. By definition of $\decke_n^*$ and $\boden_n^*$ in conjunction with Lem. \ref{lem:samplefloorceiling}.(i) we see that $\opteinschluss = \cap_i \mathcal E_{\boden_n^i}^{\decke_n^i} \subseteq \mathcal K(D_n)$. Desideratum 1 follows from $\einschluss = \bfset \cap \opteinschluss \subseteq \opteinschluss$.\\

\textit{2) Conservatism}. By definition of $\decke_n^*$ and $\boden_n^*$ in conjunction with Lem. \ref{lem:samplefloorceiling}.(ii) we have $\mathcal K(D_n) \cap \lipset L \metricp \subseteq \opteinschluss $. Hence, $\mathcal K_{prior} \cap \mathcal K(D_n) =  \bfset \cap \lipset L \metricp \cap \mathcal K(D_n) \subseteq \opteinschluss \cap \bfset = \einschluss$ proving
Desideratum 2.

\textit{3) Monotonicity.} 

This follows directly from the way the ceiling and floor functions are defined. Considering the ceiling:
We have $\decke_n (x) = \min\{\ubf,\min_{i =1,...,N_n} \decke^i_{n}(x) \} $. W.l.o.g., for $i=1,\ldots,N_n$, let $\decke_n^i = \decke_{n+1}^{i}$. 

Then 
$\decke_{n+1} (x) = \min\{\ubf(x),\min\Bigl\{ \min_{i =1,...,N_n} \decke^i_{n}(x), \min_{i= N_n +1,\ldots,N_{n+1}}  
\decke^i_{n+1}(x) \Bigr\} \} $

$\leq \min \{\ubf(x), \min_{i =1,...,N_n} \decke^i_{n}(x)  \}=  \decke_{n}(x) $. Statement $\boden_{n+1} (x) \geq  \boden_{n}(x)$ follows analogously.

\textit{4 a) Pointwise convergence.} In absence of errors, one could show pointwise convergence by utilising Cor. \ref{cor:monseqfctsencl} --which is applicable due to Lem. \ref{lem:samplefloorceiling} in conjunction with 3). We will now prove the more general statement in the presence of observational errors $\varepsilon$.

Pick an arbitrary $x \in I$ such that the error $\varepsilon(\xi) \leq m_x$ for all $\xi$ in some $\epsilon-$ball $\mathcal U_\epsilon(x) = \{\xi \in I | \metricp(\xi ,x) < \epsilon \}$ around $x$.
It remains to be shown that $\lim_{n\to \infty} \decke_n(x) - \boden_n(x) \leq 2 m_x$ for inputs $x$.
Define $L := \sup_{J \subset I} L[J] $  to be the maximum H\"older constant on the domain.
Let $e>0$ be arbitrary. We show that there exists $N_0 \in \nat$ such that for all $n \geq N_0: \decke_n(x;L) - \boden_n(x;L) \leq e + 2 m_x$ as follows:

Since we assume $G_n$ converges to a dense subset of $I$, Lem. \ref{lem:hoeldermetricconv} applies. Hence, there is $N_0 \in \nat$ such that $\forall n \geq N_0 \exists i \in \{1,...,N_n\}: s_i \in \mathcal U_{e/2L}(x) = \{\xi \in I | \metricp(x,\xi) < \frac{e}{2L}\}$.
 
So, choose $n \geq N_0$. 
Hence, $L \metricp(x,s_j)  < \frac{e}{2}, \,\varepsilon(s_q) \leq m_x \hspace{1cm} \text{ (*)}$ 

where 
$q \in \arg\min_{i \in \{1,...,N_n\} } L \, \metricp(x,s_i) $. 
We have:

 $\decke_n^*(x;L) - \boden_n^*(x;L)$ 

$= \min_{i \in \{1,...,N_n\} } L \, \metricp(x,s_i) + \tilde f(s_i) + \varepsilon(s_i) - \max_{i \in \{1,...,N_n\} }  \tilde f(s_i) - \varepsilon(s_i) - L \metricp(x,s_i) $

$= \min_{i \in \{1,...,N_n\} } \bigl\{L \, \metricp(x,s_i) + \tilde f(s_i) + \varepsilon(s_i) \bigr\}+ \min_{i \in \{1,...,N_n\} }  \bigl\{-\tilde f(s_i) + \varepsilon(s_i)+ L \, \metricp(x,s_i)\bigr\} $

$\leq \min_{i \in \{1,...,N_n\} } \bigl\{L \, \metricp(x,s_i) + \tilde f(s_i) + \varepsilon(s_i)    -\tilde f(s_i) + \varepsilon(s_i)+ L \metricp(x,s_i) \bigr\}$

$=2\min_{i \in \{1,...,N_n\} } L \, \metricp(x,s_i) + \varepsilon(s_i) $
$\leq 2 L \, \metricp(x,s_q)+ 2\varepsilon(s_q) $
$\stackrel{(*)}{<} e + 2 m_x  $.\\
As a special case, in the noise free setting with  $m_x=0$, this result implies that the enclosure shrinks to a single value as $n \to \infty$. That this value is $f(x)$ follows from Lem. \ref{lem:optenclcontainstarget}.  \\

We have shown convergence of $\decke_n^*(x;L) - \boden_n^*(x;L)$. Convergence of $\decke_n(x) - \boden_n(x) = \min\{\ubf(x), \decke_n^*(x)\} - \max\{\lbf(x),\boden_n^*(x)\}$ follows by continuity of the $\max$ and $\min$ operation.


 \textit{4 b) Uniform convergence.}
Let $G_n = \{s_1,\ldots,s_{N_n}\}$ denote the $N_n$-sample input grid which we assume to converge uniformly to a dense set of domain $I$. 


(I) Firstly, we show uniform convergence of $\decke_n^*, \boden_n^*$ to target $f$.

Assuming $m_x =0$, we desire to show that $\forall e >0 \exists N_0 \forall n \geq N_0 , x \in I : \abs{ \decke^*_n(x) - f(x)} \leq e $ and $\forall e >0 \exists N_0 \forall n \geq N_0 , x \in I : \abs{ \boden^*_n(x) - f(x)} \leq e $.
Let $e > 0$.
 %and define $L := \sup_{J \subset I} L[J] $  to be the maximum Lipschitz constant on the domain and $n$ the corresponding normalization constant making the target $L$-Lipschitz. 
Let $G_n$ converge uniformly to a dense subset  $G \subset I$. By Lem. \ref{lem:unifdenseset1}, we know that 
there exists $N_0$ such that for all $n \geq N_0$ and all $x \in I$ there is a $q \in G_n$ such that $\metricp(x,q) < \frac{e}{2  L}$. 
In particular, $ L \, \metricp(s_q ,x ) \leq \frac{e}{2}$ where we define $q \in \arg\min_{i} \metricp(s_i ,x )$. 
Hold such $N_0$ fixed, choose arbitrary $n \geq N_0$ and $x \in I$. 

For starters, we show that each $\decke^*_n$ converges uniformly to $ f$ by showing $\abs{\decke^*_n(x) - f(x) } \leq e$.

We have
$\abs{\decke^*_n(x) - f(x)} \stackrel{2)}{=} \decke^*_n(x) - f(x) $  
$\stackrel{def.}{\leq}\min_{i} \decke^i_n(x) - f(x) $
$\leq   \decke^q_n(x) - f(x) $
$\stackrel{def.}{\leq}  L \, \metricp(x,s_q)+f(s_q) - f(x) $
$\leq  L \, \metricp(x,s_q)+ \abs{f(s_q) - f(x) }$
$\stackrel{f \text{ L-Lip.}}{\leq}  L \, \metricp(x,s_q)+   L \, \metricp(x,s_q) \leq  e$ where in the last step we have leveraged $L \, \metricp(x,s_q) \leq \frac{e}{2}$.

Next, we show the analogous for the floor. 
That is we show  $\abs{\boden^*_n(x) - f(x) }\leq e$:

$\abs{  \boden^*_n(x) - f(x)}\stackrel{2)}{=} f(x) - \boden^*_n(x) $  
$\stackrel{def.}{\leq} f(x) - \max_{i} \boden^i_n(x) $
$\leq   f(x)- \boden^q_n(x)  $
$\stackrel{def.}{\leq} f(x) - (f(s_q) -  L \, \metricp(x,s_q))  $
$\leq  L \, \metricp(x,s_q)+ \abs{f(s_q) - f(x) }$
$\leq  L \, \metricp(x,s_q)+   L\, \metricp(x,s_q) \leq 2 \frac{e}{2}= e$ where, as before, in the last step we have utilised $ L \, \metricp(x,s_q) \leq \frac{e}{2}$.\\

(II) Secondly, we need to show that $\decke_n (\cdot)= \max\{\lbf(\cdot), \decke^*_n(\cdot)\}$ and $\boden_n = \min\{\lbf(\cdot), \boden^*_n(\cdot)\}$ converge uniformly to $f$. However, this presents no difficulty since we know that $f(x) \in [\lbf(x) ,\ubf(x)]$. Let $e\geq0$ be arbitrary. We need to show $\exists \tilde N_0(e) \forall n \geq \tilde N_0(e) , x \in I : \abs{ \decke_n(x) - f(x)} \leq e $. In (I), we already have established $\exists  N_0(e) \forall n \geq \tilde N_0(e) , x \in I : \abs{ \decke^*_n(x) - f(x)} \leq e $. We choose $\tilde N_0(e) := N_0(e)$. Since $f \leq \decke_n = \min\{ \ubf(x), \decke^*_n\} \leq \decke^*_n$ pointwise, for all $x \in I$, we have 
$e \geq \abs{\decke^*_n(x) - f(x) } =  \decke^*_n(x) - f(x) \geq \decke_n(x) - f(x)  = \abs{\decke_n(x) - f(x)} $. 

Uniform convergence of the floor $\boden_n$ follows completely analogously.


\textit{5) Optimality.}  By Lem. \ref{lem:Hoeldarithmetic}.(6) we know that $\decke_n^*,\boden_n^* \in \lipset L \metricp$. Hence, $\decke_n , \boden_n \in \lipset L \metricp \cap \bfset$. 
Thus, if $\lipset L \metricp \cap \bfset \subset \mathcal E^b_a $ then also $\decke_n, \boden_n \in \mathcal E^b_a$. This implies $\decke_n \leq b, \boden_n \geq a$, i.e. $\einschluss \subseteq \mathcal E^b_a$.
\end{proof} 
\end{thm}

%So far, our guarantee of optimality of the enclosure only holds for $p=1$. This is due to the fact that at present, we cannot guarantee that the minimum of two $p-L-$H\"older functions also is $p-L-$ H\"older, unless $p=1$. Intuitively, this might well be true also for $p<1$, but investigating this rigorously has to be deferred to future work.

\subsection{Proof of Thm. \ref{thm:KIdesiderata}}
\label{sec:proofKIdesiderata}
%
%
%\begin{defn}[Desiderata] \label{defn:desiderata}
 %Based on an observed, erroneous data set $D_n$, we desire to define enclosing functions $\decke_n, \boden_n$ that give rise to an enclosure $\einschluss$ with the following properties:
%
%\begin{enumerate} 
%
    %\item \textbf{Sample-Consistency}: The enclosure is sample-consistent, i.e. $\einschluss \subseteq \mathcal K(D_n)$.
		%%The enclosing functions are sample-consistent, i.e. $\decke_n,\boden_n \in \mathcal K(D_n)$.
    %
     %\item \textbf{Exhaustiveness (conservatism)}: Every $\phi, \lbf \leq \phi \leq \ubf$ coinciding with target $f$ on with grid $G_n$ and
%having at most Lipschitz
    %constant $L[J]$ on every subset $J \subset I$ is also contained in the enclosure: $\forall \phi: I \to \Real, \phi \in \mathcal K_{prior} \cap \mathcal K(D_n): \phi \in \einschluss $. That is, $\mathcal K_{prior} \cap \mathcal K(D_n) \subseteq \einschluss$.
    %In particular, the target is always contained in the enclosure: $f \in \einschluss$.
%
%\item \textbf{Monotonicity:} Additional data does not increase uncertainy. That is,
%if $G_n \subseteq G_{n+1} $, we have $\mathcal E^{\decke_{n+1}}_{\boden_{n+1}} \subseteq \einschluss, \forall n \in \nat$.
		   %
%\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I$ and for each input $x \in I$ let the sample error function $\varepsilon: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
%\begin{itemize}
%\item (a) The sequence of enclosures converges point-wise. In particular, 
%
%$\forall x \in I: 0 \leq \lim_{n \to \infty} \decke_n (x) -\boden_n(x) \leq 2 m_x$.
%In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
		%
		%\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
%\end{itemize}	
   %\item \textbf{Minimality (optimality)}: 
%If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    
%
%\end{enumerate}
%
%\end{defn}
%
%\begin{defn}[Desiderata] \label{def:KIdesiderata} With definitions as above, we desire a machine learning algorithm to implement a mapping $\data_n \mapsto (\predfn,\prederrn)$ such that, for all data sets $\data_n$, the resulting inductive inference satisfies the following \textbf{desiderata}:
%\begin{enumerate}
	%\item \textit{Conservatism:} We can guarantee $\forall x \in \inspace \, \forall \phi \in \mathcal K_{prior} \cap \mathcal K(\data_n): \phi(x) \in \prederrbox_n(x)$. In particular, the ground-truth $f$ is always contained in the posterior set and its function values always are contained in the corresponding uncertainty hyperrectangles.
	%\item \textit{Monotonicity:} Additional data cannot increase the uncertainty. 
	%
	%That is, $\data_n \subseteq \data_{n+1} $ implies 
		%$\prederrbox_n(x) \supseteq \prederrbox_{n+1}(x)$ or equivalently, $\prederr_n(x) \geq \prederr_{n+1}(x), \forall x \in \inspace$ (where the inequality holds for each component).
	%%\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I \subset \inspace$ and for each input $x \in I$ let the sample error function $\varepsilon: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
%%\begin{itemize}
%%\item (a) The uncertainty converges point-wise: $\forall x \in I: 0 \leq \lim_{n \to \infty} \prederrn(x) \leq m_x$.
%%In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
		%%
		%%\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
%%\end{itemize}	
	%\item \textit{Convergence}: If $(G_n )_{n \in \nat }$ is a sample grid sequence of inputs converging to a dense subset of $\inspace$ (as $n \to \infty$) then  $\prederrn(x) \stackrel{n \to \infty}{\longrightarrow} \varepsilon(x), \forall x \in \inspace$. If convergence of the grid to the dense subset is uniform (cf. Def. \ref{def:uniform_setconvergence}) and the observational error is constant ($\exists c \in \Real: \varepsilon \equiv c $) then the convergence  $\prederrn \stackrel{n \to \infty}{\longrightarrow} \varepsilon$ is uniform. 
	   %\item \textit{Minimality (optimality)}: There is no conservative uncertainty bound that is tighter than $\prederr_n$. That is, if for some hyperrectangle $H$, $x \in \inspace$ we have $H \subsetneq \prederrbox_n(x)$ then we have: $\exists x \in \inspace, \phi \in \mathcal K_{prior} \cap \mathcal K(\data_n) : \phi(x) \notin H$. In particular, it might be possible that the function value of the ground-truth function at $x$ is not contained in $H$.
%%If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    
%\end{enumerate}
%\end{defn}

We are now in a position to conclude the statement of Theorem \ref{thm:KIdesiderata} which is restated here for convenience:
\textbf{Theorem \ref{thm:KIdesiderata}:}

Assuming all (a priori) possible targets $f: \inspace \to \outspace$ are given by the class   
\begin{equation}
	\mathcal K_{prior}= \bigl\{ f : \inspace \to \outspace \subseteq \Real^m |  f \in \hoelset L {\metric_{\inspace}} p \cap \bfset \bigr\}
\end{equation}
where $\hoelset L {\metric_\inspace} p =\{ f: \inspace \to \outspace \subset \Real^m \, |  \forall j \in \{1,\ldots,m\} \forall x,x' \in \inspace: \abs{f_j(x) - f_j(x')}  \leq \, L_j \metric^p_{\inspace}(x,x') \}$ denotes the class of $L-p$- H\"older continuous functions with respect to metric $\metric_\inspace$ and $\bfset :=\{\phi : \inspace \to \outspace \, | \, \forall x \in \inspace, j \in\{1,...,m\}: \lbf_j(x) \leq \phi_j(x) \leq \ubf_j(x)\} $
is the set of all functions bounded component-wise between functions $\lbf,\ubf: \Real^d \to \Real \cup \{- \infty, \infty\}$, where we will always define $\Real_\infty := \Real \cup\{- \infty, \infty\}$. 

Then we have:
%\begin{enumerate}
%\item  
The inference rule as per Def. \ref{def:KIL} is conservative, monotonically convergent (in the limit of dense sample grids) and optimal in the sense of Def. \ref{def:KIdesiderata}. That is, it satisfies Desiderata 1-4 as per Def. \ref{def:KIdesiderata}. 
%\item If in addition, for some $d \in \nat$, $\inspace \subseteq \Real^d$ and $\metric_{\inspace}(x,x') = \norm{x-x'}$ for some norm that is equivalent to the Euclidean norm then the inference mechanism is also optimal (as per Desideratum 4 in Def. \ref{def:KIdesiderata}). That is, there is no other conservative inference mechanism with tighter uncertainty bound function $\prederrn$. 
%\end{enumerate}
%\end{thm}

\begin{proof}
We desire to apply Thm. \ref{thm:enclmain} to each component function.
Inspecting the definitions, this turns out to establish the desired statement for each component function of the predictors and target. 
Since the desiderata as per Def. \ref{def:KIdesiderata} are requirements for each output component, this will conclude the proof. 

In detail:
We prove the claim for each output component $j=1,...,m$. %So, let $j \in \{1,...,m\}$ be fixed. 
By assumption, the target's component function $f_j$ is $L_j-p$ H\"older. Remember from Def. \ref{def:KIL}, 
%\begin{align}
  $ \predfnj(x) := \frac{1}{2} \min\{ \ubf_j(x), \tilde \decke_{n,j}(x)\} + \frac{1}{2} \max\{ \lbf_j, \tilde \boden_{n,j}(x) \}$ and   
$	\prederrnj(x) := \frac{1}{2} \bigl(\min\{ \ubf_j(x), \tilde \decke_{n,j}(x)\} - \max\{ \lbf_j, \tilde \boden_{n,j}(x) \} \bigr)$
	%\end{align}
	with component functions given by
	$\tilde \decke_{n,j}(x) := \min_{i=1,\ldots,N_n}   \tilde f_{i,j} + L_j \metric^p(x,s_i) + \varepsilon_j(x)$ and 
	$\tilde \boden_{n,j}(x) := \max_{i=1,\ldots,N_n}   \tilde f_{i,j} - L_j \metric^p(x,s_i) - \varepsilon_j(x)$, respectively. 
Connecting to Thm. \ref{thm:enclmain}, for $x \in \inspace$, we define $\decke_n (x) := \predfnj(x) + \prederrnj(x) =  \min\{ \ubf_j(x), \tilde \decke_{n,j}(x) \}$ and $\boden_n (x) := \predfnj(x) - \prederrnj(x) =\max\{ \lbf_j, \tilde \boden_{n,j}(x) \} $. Furthermore, we note $\decke_n - \boden_n = 2 \prederrnj(x)$. So, convergence of the enclosure (i.e. convergence of $\decke_n - \boden_n$ ) is equivalent to convergence of $2 \prederrnj$. Finally, by definition, we see that $\decke_n^*(x) = \decke_{n,j}(x)$ and $\decke_n^*(x) = \boden_{n,j}(x)$ where $\decke^*_n, \boden^*_n$ match up the definitions of Def. \ref{def:optenclosingfcts}. 
Hence, Thm. \ref{thm:enclmain} is applicable. Therefore, Desideratum 1 -5 of Def. \ref{defn:desiderata} hold for the $j$th component of the prediction and target. 

Note, the $j$th prediction uncertainty interval $\prederrbox_j(x)$ just coincides with $[\boden_n(x), \decke_n(x)]$ (cf. Eq. \ref{eq:prederrint}). Thus, By Desideratum 2 of Def. \ref{defn:desiderata} we conclude:
%\begin{equation}\label{eq:prederrint}
$\forall \phi_j \in \einschluss : \phi_j(x) \in \prederrbox_{n,j}=[ \predfnj(x) - \prederrnj(x), \predfnj(x) + \prederrnj(x)] $.
%\end{equation}
%is referred to as the $j$th \textit{prediction uncertainty} interval.
In particular, this means $f_j(x)   \in \prederrbox_{n,j}$. Hence, Desideratum 1 of Def. \ref{def:KIdesiderata} (conservatism) holds.
The remaining desiderata as per Def. \ref{thm:KIdesiderata} follow from the validity of the corresponding desiderata as per Thm. \ref{thm:enclmain} on the enclosure for each component $j$. The fact that the desiderata in the former are equivalent to the validity of the desiderata for each component $j$ (and the latter can be shown by the established link 

By Desideratum 2 implies $\boden_n(x) \leq f_j(x) \leq \decke_n(x), \forall x$ which by our definition is equivalent to stating $f_j(x) \in [\boden_n(x), \decke_n(x)] = \prederrbox_j(x)$. Desiderata 3,4 of Def. \ref{defn:desiderata}, establishes the desired properties of monotonocity and convergence, which extend to the corresponding properties to $\prederrbox_j$ and hence, to $\prederrnj$. In turn, this establishes Desiderata 2-3 of Def. \ref{def:KIdesiderata}. 
Owing to the same link, $[\boden_n(x), \decke_n(x)] = \prederrbox_j(x)$, minimality of $\prederrbox_j(x)$ (i.e. of $\prederrnj$) follows from the minimality of the enclosure (Desideratum 5 of Def. \ref{defn:desiderata}). Thus, Desideratum 4 as per Def. \ref{def:KIdesiderata} holds as well.

\end{proof}

\section{Pruning of uninformative sample points}

The remaining derivations of this subsection show how the data can be pruned when new data points arrive the render old ones uninformative. They can be skipped by a reader not interested in the details.


We will focus our exposition on the ceiling functions. The derivations for the floor functions are entirely analogous (where one has to substitute $\leq$ by $\geq$ and $\min$ by $\max$).

\begin{lem}\label{lem:dominatedsamplefct}
Let $\mathcal N := \{1,...,N_n\}$, $q,i \in \mathcal N$ be the indices of a sample ceiling functions $\decke_n^q(\cdot;L_q)$ and $\decke_n^i(\cdot;L_i)$ where $L_q \leq L_i$ everywhere. We have
\[\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i) \Leftrightarrow \forall t \in I: \decke_n^q(t;L_q) \leq \decke_n^i(t;L_i).\]
\begin{proof}
$\underline \Rightarrow:$
$\decke_n^q (t;L_q) = \decke_n^q(s_q;L_q) + L_q \metricp(t, s_q)$
$\leq \decke_n^q(s_q;L_q) +L_q \metricp(s_i, s_q)+ L_q\metricp(t,s_i) $
$=\decke_n^q(s_i;L_q) +  L_q\metricp(t,s_i)$ 
$\leq \decke_n^i(s_i;L_i) + L_q\metricp(t,s_i)  $
$= \overline f(s_i) + L_q\metricp(t,s_i)    $
$\leq \overline f(s_i) + L_i\metricp(t,s_i)    = \decke_n^i(t;L)$.\\
$\underline \Leftarrow:$ Trivial.
\end{proof}
\end{lem}

The lemma implies that we can remove samples that are dominated by other samples' ceiling functions:
\begin{thm} 
\label{thm:removedominatedsamples}
Let $\mathcal N := \{1,...,N_n\}$ and let $i \in \mathcal N$ be the index of sample ceiling function $\decke_n^i(\cdot;L_i): t \mapsto \overline f(s_i) + L_i \abs{t - s_i}$. If there exists a sample ceiling function $u^q(\cdot;L_q), q \in \mathcal N \backslash \{i\}$, $L_q \leq L_i$ everywhere, having a value below $\decke_n^i$ at the $i$th sample, then the $i$th sample plays no role in computing the optimal ceiling. That is:\\  
If there is $q \in \mathcal N \backslash\{ i\}$ such that $\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i)$, then we have: \\
$\decke^*(t;L_1,\ldots,L_n) = \min_{j \in \mathcal N} \decke^j(t;L_j) =  \min_{j \in \mathcal N\backslash \{i\}} \decke^j(t;L_j)$.
\begin{proof}
The theorem is a direct consequence of Lem. \ref{lem:dominatedsamplefct}. 
\end{proof}

\end{thm}

\begin{rem}
Normally, $L_i = L_q\, \forall q,j \in \mathcal N$ where each $L_i$ is a function mapping $t$ to a H\"older constant valid in some neighbourhood of $t$.
\end{rem}

\begin{thm}
\label{thm:dominatedsamples_greatLIPconst}
Let $\mathcal N := \{1,...,N_n\}$ and let $i \in \mathcal N$ be the index of sample ceiling function $\decke_n^i(\cdot;L_i): t \mapsto \overline f(s_i) + L_i \metricp(t,s_i) $. We have: \\if $\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i)$ then 
\[  \decke^*(t;L_1,...,L_n) \geq \min_{j \in \mathcal N} \decke^j(t;L_j) =  \min \bigl\{ \min_{j \in \mathcal N\backslash \{i\}} \decke^j(t;L_j), \decke_n^q(s_i) + L_i \metricp(t,s_i) \bigr\}.\]

\begin{proof} Let $\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i)$ and $L_q > L_i $ everywhere. Since $\decke_n^q(s_i) \leq \decke_n^i(s_i)$ and $L_i < L_q$ we have $\decke_n^i(t;L_i) = \overline f(s_i) + L_i \metricp(t,s_i) $
$= \decke_n^i(s_i;L_i) + L_i \metricp(t,s_i)$
$\geq \decke_n^q(s_i;L_i) + L_i \metricp(t,s_i)$.

\end{proof}

\end{thm}

\begin{rem} \label{rem:assumptionsceil}
The theorems allow us to assume, without loss of generality, that $\decke_n^i(s_i) = \min_j \decke_n^j(s_i)$ (as a preprocessing step, Thm. 
\ref{thm:removedominatedsamples} allows us to remove samples violating this assumption, without changing the optimal ceiling). 
\end{rem}



\section{Special case: one-dimensional inputs, p=1}
\label{sec:onedimlipfloorceil}
For functions on one-dimensional domains and exponent $p=1$, matters simplify. We assume $\metric(x,y) := \abs{x-y}$. This case has been considered in the context of optimisation \cite{Shubert:72} and quadrature \cite{Baran2008} and the results therefore are not new. However, for our kinky inference method, the results rederived below have favourable practical implications (see Rem. \ref{rem:lipencl1d4KI}).
%
%gravity of the results of Thm Lem. \ref{lem:ceiling_maxvalues} and Lem. {lem:floor_minvalues} is that in the one dimensional (Lipschitz) case, the floor and ceiling functions (and hence, the prediction functions $\predfn, \prederrn$ can be evaluated based on the minimisation of two neighbouring 

%From now on, we will assume that $L_1,\ldots, L_n =: L$. 

For ease of notation we will therefore often omit explicit mention of $L$ in the statements of the ceiling and floor functions, unless it becomes necessary. Additionally, we assume, without loss of generality, that the sample inputs are ordered such that $s_1\leq \ldots \leq s_n$. Let $b := \sup I $ and $a := \inf I$.



Under these assumptions, we next show that computing the optimal ceiling function only requires taking into account the up to two most adjacent samples :

\begin{lem}\label{lem:shieldingsamples_1d}
Let the assumptions of Rem. \ref{rem:assumptionsceil} hold.

We have:

\begin{enumerate}
\item $\forall i \in \mathcal N, t \geq s_i: \decke_n^* ( t) =\min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N, j \geq i\}, $	
\item $\forall i \in \mathcal N, t \leq s_i: \decke_n^* ( t) =\min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N,j \leq i\}.$ 
\end{enumerate}

\begin{proof}
1.) Let $t \geq s_i$. 

$\text{Showing} \leq:$ This is trivial since $\decke_n^*(t) = \min_{j \in \mathcal N} \decke_n^j(t) \leq \min_{j \in \mathcal N, j \geq i}\decke_n^j(t)$. 


$\text{Showing} \geq:$ For contradiction, assume $\decke_n^* ( t) < \min_{j\geq i} \decke_n^j(t).$ 
Hence, there is a $q \in \mathcal N, q < i$ such that 

\begin{equation}
\label{eq:lala1}
\forall t \geq s_i: \decke_n^q(t) < \min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N, j \geq i\}.
\end{equation}
%Again, utilizing $s_i \leq t \leq s_{i+1}$ this is equivalent to $\exists q : \decke_n^q(t) < \overline f(s_i) + L \abs{t - s_i} \wedge \decke_n^q(t) < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$
%
%$\stackrel{def.}{\Leftrightarrow} \exists q : \overline f(s_q) + L \abs{t - s_q} < \overline f(s_i) + L \abs{t - s_i} \wedge f(s_q) + L \abs{t - s_q} < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$.
Choose such a $q \in \mathcal N, q < i$.
By assumption, we have $s_q < s_i$. Hence, Condition \ref{eq:lala1} implies in particular $\decke_n^q(t) = \overline f(s_q) + L (t - s_q) < \decke_n^i(t) = \overline f(s_i) + L (t - s_i)$. We have $\overline f(s_q) + L (t - s_q) = \overline f(s_q) + L (s_i - s_q) + L (t - s_i) = \decke_n^q(s_i) + L (t-s_i) \stackrel{!}{ <} \overline f(s_i) + L (t-s_i)=\decke_n^i(s_i) + L (t-s_i) $ which holds, if and only if $\decke_n^q(s_i) < \decke_n^i(s_i)$. However, the last inequality contradicts our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (Rem. \ref{rem:assumptionsceil}).\\

2.) The proof is analogous to 1.):

Let $t \leq s_i$. 

$\text{Showing} \leq:$ This is trivial since $\decke_n^*(t) = \min_{j \in \mathcal N} \decke_n^j(t) \leq \min_{j \in \mathcal N, j \leq i}\decke_n^j(t)$. 


$\text{Showing} \geq:$ For contradiction, assume $\decke_n^* ( t) < \min_{j\leq i} \decke_n^j(t).$ 
Hence, there is a $q \in \mathcal N, q > i$ such that 

\begin{equation}
\label{eq:lala2}
\forall t \leq s_i: \decke_n^q(t) < \min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N, j \leq i\}.
\end{equation}
%Again, utilizing $s_i \leq t \leq s_{i+1}$ this is equivalent to $\exists q : \decke_n^q(t) < \overline f(s_i) + L \abs{t - s_i} \wedge \decke_n^q(t) < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$
%
%$\stackrel{def.}{\Leftrightarrow} \exists q : \overline f(s_q) + L \abs{t - s_q} < \overline f(s_i) + L \abs{t - s_i} \wedge f(s_q) + L \abs{t - s_q} < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$.
Choose such a $q \in \mathcal N , q > i$ (thus, $s_q > s_i$).


The way we ordered the samples, $q>i$ implies $s_q > s_{i}$. Thus, Condition \ref{eq:lala2} implies in particular: $\decke_n^q(t) =\overline f(s_q) + L (s_q -t ) < \decke_n^i(t) = \overline f(s_{i}) + L ( s_{i} -t) = \decke_n^{i}(s_{i}) + L ( s_{i} -t)$. We have $\overline f(s_q) + L (s_q-t) = \overline f(s_q) + L (s_q - s_{i} ) + L (s_{i} - t) = \decke_n^q(s_{i}) + L (s_{i} - t) \stackrel{!}{ <} \decke_n^{i}(s_{i}) + L (s_{i} -t) $. However, the last inequality contradicts our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (made in Rem. \ref{rem:assumptionsceil}).



%By definition of the sample ceilings, it follows 
%$\exists q : \decke_n^q(t) < \decke_n^i(t) \wedge \decke_n^q(t) < \decke_n^{i+1} (t)$.
\end{proof}

\end{lem}


The Lemma allows us to tremendously simplify computation of the optimal ceiling-- it tells us that all samples, except the up to two neighbouring ones, can be discarded when computing the optimal ceiling at a given input:

\begin{thm}\label{thm:lipceil1d}
Let the assumptions of Rem. \ref{rem:assumptionsceil} hold.
Let $a:=s_0: = \inf I, s_{n+1} := b:=\sup I$ and for $i=0,\ldots,N_n$ define $I_i := [s_i,s_{i+1}]$.

%we define line segments via \[\alpha^\decke_n_i: t
%\mapsto \overline f(s_i) + L (t - s_i)\] and \[\beta^\decke_n_i: t \mapsto
%\overline f(s_{i+1}) -L (t - s_{i+1}).\] 

We have:

\[ \forall i, t \in I_i: \decke_n^*(t) = \begin{cases} \decke_n^{i+1}(t) &, i=0\\
															\min\{\decke_n^i(t),\decke_n^{i+1}(t) \}&, i \in \{1,\ldots,N_n-1 \} \\
															\decke_n^{i}(t)&, i=N.	\end{cases} \]


\end{thm}



The analogous statement for the H\"older floor function can be derived completely analogously and will therefore be given at this point without proof:


\begin{thm}\label{thm:lipfloor1d}
Assume the assumptions of Rem. \ref{rem:assumptionsceil} hold.
Let $a:=s_0: = \inf I, s_{n+1} := b:=\sup I$ and for $i=0,\ldots,N_n$ define $I_i := [s_i,s_{i+1}]$.

%we define line segments via \[\alpha^\boden_i: t
%\mapsto \overline f(s_i) + L (t - s_i)\] and \[\beta^\boden_i: t \mapsto
%\overline f(s_{i+1}) -L (t - s_{i+1}).\] 

We have:

\[ \forall i, t \in I_i: \boden_n^*(t) = \begin{cases} \boden_n^{i+1}(t) &, i=0\\
															\max\{\boden_n^i(t),\boden_n^{i+1}(t) \}&, i \in \{1,\ldots,N_n-1 \} \\
															\boden_n^{i}(t)&, i=N.	\end{cases} \]


\end{thm}

In addition to saving computation by discarding irrelevant samples, the the theorems will play an important role when computing a closed-form the integral of the optimal ceiling and floor functions.


%\begin{cor}\label{cor:lipceil}
%Assume the assumptions of Rem. \ref{rem:assumptionsceil} hold.
%On interval $I_i = [s_i,s_{i+1}]$ we define line segments via \[\alpha^\decke_n_i: t
%\mapsto \overline f(s_i) + L (t - s_i)\] and \[\beta^\decke_n_i: t \mapsto
%\overline f(s_{i+1}) -L (t - s_{i+1}).\] 
%
%We have:
%
%\begin{enumerate}
%\item
%For $i \in \mathcal N$ the projection of the optimal H\"older ceiling onto interval $I_i$ can be computed as the point-wise minimum of $\alpha^\decke_n_i$ and $ \beta^\decke_n_i$:
%\[\forall t \in I_i: \decke_n^* ( t) =\min\{ \alpha^\decke_n_i(t), \beta^\decke_n_i(t) \}= \min\{\decke_n^i(t),\decke_n^{i+1}(t) \} .\] 
%
%\item $\forall t \in [a,s_1], \tau \in [s_n,b] : \decke_n^*(t) = \decke_n^1(t) \wedge  \decke_n^*(\tau) = \decke_n^N(\tau)$.
%\end{enumerate}
%\begin{proof}
%1.) Since $s_i \leq t \leq s_{i+1}$, $\alpha^\decke_n_i(t) = \decke_n^i(t)$ and $\beta^\decke_n_i(t) = \decke_n^{i+1}(t)$.
%Let $t \in I_i$.
%
%$\text{Showing} \leq:$ By definition, $\decke_n^*(t) = \min_j \decke_n^j(t) = \min_j \overline f(s_j) + L \abs{t - s_j}  \leq \min\{\overline f(s_i) + L \abs{t - s_i}, \overline f(s_{i+1}) + L \abs{t - s_{i+1}} \} = \min\{\overline f(s_i) + L (t - s_i), \overline f(s_{i+1}) - L  (t-s_{i+1}) \}$ where the last equality holds owing to $s_i \leq t \leq s_{i+1}$.
%
%$\text{Showing} \geq:$ For contradiction, assume $\decke_n^* ( t) <  \min\{\overline f(s_i) + L (t - s_i), \overline f(s_{i+1}) - L  (t-s_{i+1}) \}$. This implies the existence of a $q \in \mathcal N$ such that 
%
%\begin{equation}
%\label{eq:lala1}
%\decke_n^q(t) < \overline f(s_i) + L (t - s_i)
%\end{equation}
%and 
%\begin{equation}
%\label{eq:lala2}
 %\decke_n^q(t) < \overline f(s_{i+1}) - L  (t-s_{i+1}). 
%\end{equation}
%%Again, utilizing $s_i \leq t \leq s_{i+1}$ this is equivalent to $\exists q : \decke_n^q(t) < \overline f(s_i) + L \abs{t - s_i} \wedge \decke_n^q(t) < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$
%%
%%$\stackrel{def.}{\Leftrightarrow} \exists q : \overline f(s_q) + L \abs{t - s_q} < \overline f(s_i) + L \abs{t - s_i} \wedge f(s_q) + L \abs{t - s_q} < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$.
%Choose such a $q \in \mathcal N$.
%
%Case A: Let $s_q < s_i$. Then, Condition \ref{eq:lala1} implies $\decke_n^q(t) = \overline f(s_q) + L (t - s_q) < \overline f(s_i) + L (t - s_i)$. We have $\overline f(s_q) + L (t - s_q) = \overline f(s_q) + L (s_i - s_q) + L (t - s_i) = \decke_n^q(s_i) + L (t-s_i) \stackrel{!}{ <} \overline f(s_i) + L (t-s_i)=\decke_n^i(s_i) + L (t-s_i) $ which holds, if and only if $\decke_n^q(s_i) < \decke_n^i(s_i)$. However, the last inequality contradicts our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (Rem. \ref{rem:assumptionsceil}).
%
%Case B: Let $s_q > s_{i+1}$. Then, Condition \ref{eq:lala2} implies $\overline f(s_q) + L (s_q -t ) < \overline f(s_{i+1}) + L ( s_{i+1} -t) = \decke_n^{i+1}(s_{i+1}) + L ( s_{i+1} -t)$. We have $\overline f(s_q) + L (s_q-t) = \overline f(s_q) + L (s_q - s_{i+1} ) + L (s_{i+1} - t) = \decke_n^q(s_{i+1}) + L (s_{i+1} - t) \stackrel{!}{ <} \decke_n^{i+1}(s_{i+1}) + L (s_{i+1} -t) $. However, the last inequality contradicts our our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (made in Rem. \ref{rem:assumptionsceil}).
%We have proven the first claim.\\
%
%2.) 
%
%%By definition of the sample ceilings, it follows 
%%$\exists q : \decke_n^q(t) < \decke_n^i(t) \wedge \decke_n^q(t) < \decke_n^{i+1} (t)$.
%\end{proof}
%
%\end{cor}



In preparation for the part on quadrature it will prove useful to determine the extrema of the enclosing functions on each subinterval $I_i$.


\begin{lem} \label{lem:Lipbetweensamples}
We have:
\begin{enumerate}
\item If $\decke^i(s_i)  \leq \decke^{i+1}(s_i)$ and $\decke^{i+1}(s_{i+1})  \leq \decke^i(s_{i+1})$ then 
we have 

$ \abs{\overline f(s_{i+1})- \overline f(s_i)} \leq L \abs{s_{i+1} -s_i}$.
\item If $\boden^i(s_i)  \geq \boden^{i+1}(s_i)$ and $\boden^{i+1}(s_{i+1})  \geq \boden^i(s_{i+1})$ then 
we have

$ \abs{\underline f(s_{i+1})- \underline f(s_i)} \leq L \abs{s_{i+1} -s_i}$.
\end{enumerate}

\begin{proof}
1.) 
On the one hand, we have:
$\decke^i(s_i)  \leq \decke^{i+1}(s_i)$ 
$\Leftrightarrow$
 $\decke^i(s_i) - \decke^{i+1}(s_i)  \leq 0$
%
$\Leftrightarrow$ $ \overline f(s_i) + L \abs{s_i-s_i} -\overline f(s_{i+1}) - L \abs{s_i-s_{i+1}}  \leq 0$
%
$\Leftrightarrow$ $ \overline f(s_i) -\overline f(s_{i+1}) - L \abs{s_i-s_{i+1}}  \leq 0$
%
$\Leftrightarrow$ $ \overline f(s_i) -\overline f(s_{i+1})   \leq  L \abs{s_i-s_{i+1}}$

$\Leftrightarrow$ $ \overline f(s_{i+1})-\overline f(s_i)    \geq  -L \abs{s_{i+1} - s_i}$.\\
%
On the other hand:
$\decke^{i+1}(s_{i+1})  \leq \decke^i(s_{i+1})$
%
$\Leftrightarrow$ $\decke^{i+1}(s_{i+1}) - \decke^i(s_{i+1}) \leq 0$
$\Leftrightarrow$ $\overline f(s_{i+1})  - \overline f(s_i) - L \abs{s_{i+1} -s_i} \leq 0$
$\Leftrightarrow$ $\overline f(s_{i+1})  - \overline f(s_i)  \leq  L \abs{s_{i+1} -s_i}$.

2.) The proof is completely analogous to 1.). 
\end{proof}
\end{lem}

\begin{lem} \label{lem:ceiling_maxvalues}
Let $\metric(x,y) := \abs{x-y}$. For $i \in \{1,\ldots,N_n-1\}$ let  $L_i > 0$, $I_i = [s_i,s_{i+1}]$.
Define $\xi^{\mathfrak u}_i :=  \frac{s_i+s_{i+1}}{2}+ \frac{\overline f(s_{i+1}) - \overline f(s_i)}{2 L_i}  $.\\
For $t \in I_i$ we have 
\begin{enumerate}
\item $\xi^{\mathfrak u}_i \in I_i$ and  $\decke^*_n(t) = 
 \begin{cases}  
 \decke^i_n (t), & t \leq \xi^\decke_i \\
 \decke^{i+1}_n(t), & t \geq \xi^\decke_i.
 \end{cases}$
\item $\xi^{\mathfrak u}_i \in \arg\max_{\tau \in I_i} \decke^*_n (\tau)$.
\item  $y^{\mathfrak u}_i := \decke_n^*(\xi^{\mathfrak u}_i) = \frac{\overline f(s_{i+1}) +\overline f(s_i)}{2} + L_i \frac{s_{i+1}-s_i}{2}$ .
\end{enumerate}
\begin{proof} Let $1 \leq i \leq N-1, t \in I_i$. Define $\alpha^\decke_i(t) = \overline f(a_i) + L_i (t-a_i) = \decke_n^i(t), \beta^\decke_i (t) = \overline f(b_i) + L_i (b_i - t) = \decke^{i+1}_n(t), \forall t \in I_i$.

Firstly, $\xi^\decke_i \in I_i$ is a direct consequence of Lem. \ref{lem:Lipbetweensamples}.

Secondly, we show $\decke^*_n(t) = 
 \begin{cases}  
 \decke^i_n (t), & t \leq \xi^\decke_i \\
 \decke^{i+1}_n(t), & t \geq \xi^\decke_i.
 \end{cases}$:

By definition,
$\decke^*_n (t) = \min\{ \decke^{i}_n(t), \decke^{i+1}_n(t)\} = \min\{\alpha^i_n (t),\beta^i_n(t)\}$.
We have $\dot  \alpha^\decke_i(t) = L_i > 0$ and $\dot  \beta^\decke_i(t) = - L_i < 0$. Hence, the functions are strictly monotonous. Let $\xi$ be the input where the lines intersect, i.e. $\alpha^\decke_i(\xi) = \beta^\decke_i(\xi)$. Monotonicity implies $\forall t \leq \xi : \alpha^\decke_i(t) \leq \alpha^\decke_i(\xi) \wedge \beta^\decke_i(t) \geq \beta^\decke_i(\xi) = \alpha^\decke_i(\xi)$. 
Hence,
 \[\forall t \leq \xi: \decke^*_n(t) = \min \{\alpha_i^{\mathfrak u} (t), \beta_i^{\mathfrak u} (t) \}= \alpha_i^{\mathfrak u} (t).\] 
 
Analogously, monotonicity implies $\forall t \geq \xi : \beta^\decke_i(t) \leq \beta^\decke_i(\xi) \wedge \alpha^\decke_i(t) \geq \alpha^\decke_i(\xi) = \beta^\decke_i(\xi)$. 
Thus,
 \[\forall t \geq \xi: \decke^*_n(t) = \min \{\alpha_i^{\mathfrak u} (t), \beta_i^{\mathfrak u} (t) \}= \beta_i^{\mathfrak u} (t).\] 

In conjunction, we have 
 $\decke^*_n(t) = 
 \begin{cases}  
 \alpha_i^{\mathfrak u} (t), & t \leq \xi \\
 \beta_i^{\mathfrak u} (t), & t \geq \xi 
 \end{cases}$.
 Again, using monotonicity of $\alpha_i^{\mathfrak u}$ and $\beta_i^{\mathfrak u}$ we infer that $\decke^*_n$ has a single maximum $y := \decke^*_n(\xi)$ at $\xi$.

Next, we show that $\xi = \xi_i^{\mathfrak u}$: 

$\alpha^\decke_i(\xi) = \beta^\decke_i(\xi)$

$\Leftrightarrow  \overline f(s_i) + L_i (\xi - s_i) = \overline f(s_{i+1}) -L_i (\xi - s_{i+1})$

$\Leftrightarrow  L_i \bigl( 2 \xi - s_{i+1}  - s_i\bigr) = \overline f(s_{i+1}) - \overline f(s_i) $

$\stackrel{L_i > 0}{\Leftrightarrow}   \bigl( 2 \xi - s_{i+1}  - s_i\bigr) = \frac{\overline f(s_{i+1}) - \overline f(s_i)}{L_i} $

$\Leftrightarrow   \xi  = \frac{\overline f(s_{i+1}) - \overline f(s_i)}{2 L_i}  + \frac{s_{i+1}+s_i}{2} = \xi^{\mathfrak u}_i$.



Finally, we need to evaluate $\decke^*_n(\xi^{\mathfrak u}_i)$ to prove the last claim:
We have 

 $\decke^*_n(\xi) = \alpha_i^{\mathfrak u} (\xi) =  \overline f(s_i) + L_i (\xi - s_i) $

$= \overline f(s_i) + L_i \bigl( \frac{\overline f(s_{i+1}) - \overline f(s_i)}{2 L_i}  + \frac{s_{i+1}+s_i}{2} - s_i \bigr) $ 

$=  \frac{\overline f(s_{i+1}) - \overline f(s_i) + 2\overline f(s_i)}{2 }  + L_i \frac{s_{i+1}-s_i}{2}   = y_i^{\mathfrak u}$. 



\end{proof}

\end{lem}


Completely analogously to the the proof of Lem. \ref{lem:ceiling_maxvalues} we can prove the corresponding statement for the H\"older floor:
\begin{lem} \label{lem:floor_minvalues}
Let $\metric(x,y) := \abs{x-y}$.
For $i \in \{1,\ldots,N_n-1\}$ let  $L_i > 0$, $I_i = [s_i,s_{i+1}]$.
Define $\xi^{\boden}_i :=  \frac{s_i+s_{i+1}}{2}- \frac{\underline f(s_{i+1}) - \underline f(s_i)}{2 L_i}  $.\\
For $t \in I_i$ we have 
\begin{enumerate}
\item $\xi^{\boden}_i \in I_i$ and  $\boden^*_n(t) = 
 \begin{cases}  
 \boden^i_n (t), & t \leq \xi^\boden_i \\
 \boden^{i+1}_n(t), & t \geq \xi^\boden_i.
 \end{cases}$
\item $\xi^{\boden}_i \in \arg\min_{\tau \in I_i} \boden^*_n (\tau)$.
\item  $y^{\boden}_i := \boden_n^*(\xi^{\boden}_i) = \frac{\underline f(s_{i+1}) +\underline f(s_i)}{2} - L_i \frac{s_{i+1}-s_i}{2}$ .
\end{enumerate}

\begin{proof} The proof is analogous to the one provided for Lem. \ref{lem:ceiling_maxvalues}.
%
%The case $i = N$ is trivial. Let $1 \leq i \leq N-1$. By definition,
%$\boden^i_n (t) = \min\{ \alpha^\boden_i(t), \beta^\boden_i(t)\}$.
%We have $\dot  \alpha^\boden_i(t) = - L_i < 0, \forall t$ and $\dot  \beta^\boden_i(t) =  L_i > 0, \forall t$. Hence, the functions are strictly monotonous. Let $\xi$ the input where the lines intersect, i.e. $\alpha^\boden_i(\xi) = \beta^\boden_i(\xi)$. Monotonicity implies $\forall t \leq \xi : \alpha^\boden_i(t) \geq \alpha^\boden_i(\xi) \wedge \beta^\boden_i(t) \leq \beta^\boden_i(\xi) = \alpha^\boden_i(\xi)$. 
%Hence,
 %\[\forall t \leq \xi: \boden_n^i(t) = \max \{\alpha_i^{\mathfrak l} (t), \beta_i^{\mathfrak l} (t) \}= \alpha_i^{\mathfrak l} (t).\] 
 %
%Analogously, monotonicity implies $\forall t \geq \xi : \beta^\boden_i(t) \geq \beta^\boden_i(\xi) \wedge \alpha^\boden_i(t) \leq \alpha^\boden_i(\xi) = \beta^\boden_i(\xi)$. 
%Thus,
 %\[\forall t \geq \xi: \boden_n^i(t) = \max \{\alpha_i^{\mathfrak l} (t), \beta_i^{\mathfrak l} (t) \}= \beta_i^{\mathfrak l} (t).\] 
%
%In conjunction, we have 
 %$\boden_n^i(t) = 
 %\begin{cases}  
 %\alpha_i^{\mathfrak l} (t), & t \leq \xi \\
 %\beta_i^{\mathfrak l} (t), & t \geq \xi 
 %\end{cases}$.
 %Again, utilizing monotonicity of $\alpha_i^{\mathfrak l}$ and $\beta_i^{\mathfrak l}$ we infer that $\boden_n^i$ has a single minimum $y := \boden_n^i(\xi)$ at $\xi$.
%It remains  (i) to show that $\xi = \xi_i^{\mathfrak l}$, (ii) we need to evaluate $\boden_n^i(\xi^{\mathfrak l}_i)$ showing $y = y_i^{\mathfrak l}$, and (iii) we need to show that 
%$\xi^{\mathfrak l}_i \in \bar I_i$ .
%
%(i) $\alpha^\boden_i(\xi) = \beta^\boden_i(\xi)$
%
%$\Leftrightarrow  f(a_i) - L_i (\xi - a_i) = f(b_i) +L_i (\xi - b_i)$
%
%$\Leftrightarrow  L_i \bigl( 2 \xi - b_i  - a_i\bigr) = f(a_i) - f(b_i) $
%
%$\stackrel{L_i > 0}{\Leftrightarrow}   \bigl( 2 \xi - b_i  - a_i\bigr) = \frac{f(a_i) - f(b_i)}{L_i} $
%
%$\Leftrightarrow   \xi  = \frac{f(a_i) - f(b_i)}{2 L_i}  + \frac{b_i+a_i}{2} = \xi^{\mathfrak l}_i$.
%
%(ii) $\boden_n^i(\xi) = \alpha_i^{\mathfrak l} (\xi) =  f(a_i) - L_i (\xi - a_i) $
%
%$= f(a_i) - L_i \bigl( \frac{f(a_i) - f(b_i)}{2 L_i}  + \frac{b_i+a_i}{2} - a_i \bigr) $ 
%
%$=  \frac{-f(a_i) + f(b_i) + 2f(a_i)}{2 }  - L_i \frac{b_i-a_i}{2}   = y_i^{\mathfrak l}$. 
%
%(iii) $\xi^{\mathfrak l}_i \in \bar I_i$ if and only if $a_i \leq \xi^{\mathfrak l}_i \leq b_i$. Since $\xi^{\mathfrak l}_i = -\frac{f(b_i) - f(a_i)}{2 L_i}  + \frac{b_i+a_i}{2}$ and $\frac{b_i+a_i}{2}$ is just in the middle value of interval $\bar I_i = [a_i,b_i]$, we have 
%
%$\xi^{\mathfrak l}_i \in \bar I_i$ if $\abs{\frac{f(b_i) - f(a_i)}{2 L_i}} \leq \abs{\frac{b_i-a_i}{2}}$.
%However, the latter is the case due to $L_i$-H\"older continuity of $f$ on $\bar I_i$.  
  
\end{proof}

\end{lem}

\begin{rem}[Reduction of computational effort for KI and collision detection] \label{rem:lipencl1d4KI}
While the results above are not new, their statement has practical benefits in terms of computational effort of the kinky inference rule in one-dimensions when the canonical metric $\metric(x,x') = \abs{x-x'}$ is employed and $p=1$. Then Thm. \ref{thm:lipceil1d} and Thm. \ref{thm:lipfloor1d} tell us that the ceiling and floor functions can be evaluated by only considering the two most adjacent sample ceiling and floor functions. If we maintain a search tree over the grid, we can search for those two entries in $\mathcal O(\log N_n)$. This reduces the computational effort for evaluating the prediction functions $\predfn,\prederrn$ (cf. Def. \ref{def:KIL}) from linear to logarithmic asymptotic complexity. This can make a significant difference in terms of computational speed of the inference in the presence of large data sets.

As a second benefit of our derivations, Lem. \ref{lem:floor_minvalues} and Lem. \ref{lem:ceiling_maxvalues} can be utilised in the collision detection method as presented in Sec. \ref{sec:adaptiveLipshubertstyle} where negative function values need to be proven or ruled out. The Lemmata tell us which finite number of points to check.
\end{rem}