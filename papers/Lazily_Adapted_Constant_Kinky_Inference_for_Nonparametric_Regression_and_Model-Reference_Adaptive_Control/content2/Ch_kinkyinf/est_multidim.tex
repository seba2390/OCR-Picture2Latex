
\section{Task 1 -- Conservative function estimation }
\label{sec:Hfe}
\textbf{Overview.} We desire to solve the conservative estimation problem (Task 1). As described above it can be described as a topological inference problem for which we will now design an optimal inference mechanism.  

In Sec. \ref{sec:targetest_multidim}, we derive computable H\"older ceiling and floor functions and prove that they satisfy the desiderata conservatism, convergence and optimality.  

To this end, we derive a number of upper and lower bound functions and show that these bound each function that is sample-consistent and H\"older continuous (hence, in particular, our target function). This constrains all H\"older continuous candidate functions that are sample-consistent, i.e. all H\"older functions that could have generated the (erroneous) sample set $D_n$. We then show how to convert these bounding constraints into our desired ceiling and floor functions $\decke_n, \boden_n$. Having established conservatism, we then establish optimality by realizing that $\decke_n$ and $\boden_n$ are indeed both H\"older functions that are sample consistent. Therefore, no tighter bounds can be found establishing optimality. Finally, we prove uniform convergence to the ground truth.

\jcom{--- EVTL RAUS:}
The section concludes with a discussion of the one-dimensional case in preparation of our elucidations in Sec. \ref{sec:quadr}. Among other things, we will find that in the one-dimensional case, $\decke_n$ and $\boden_n$ can be evaluated at substantially lower computation.\\

\subsection{Convergence properties}
We will construct function estimates that converge to the ground truth in the limit of large sample sets, provided the samples fulfil certain convergence properties themselves. To establish this in a rigorous manner, we need to review a few core convergence concepts. 
\begin{defn}[Uniform convergence of function sequences] Let $(\inspace, \metric_\inspace), (\outspace,\metric_\outspace)$ be two metric spaces.
For $n \in \nat$, let $f_n : I \to \Real$. $f_n \stackrel{n \to \infty}{\longrightarrow} f$ \emph{uniformly} (with respect to $\metric_\outspace$) if
\[  \forall \epsilon >0  \exists N_0 \in \nat \forall n\geq N_0 \forall x \in I: \metric_\outspace(f_n(x), f(x)) < \epsilon. \] 
\end{defn}

The key distinction between both variants of convergence is that in point-wise convergence, the convergence rate can depend on $x$. By contrast, in uniform convergence the rate is independent of function input $x$.


For our purpose the following definition of a dense set suffices:
\begin{defn}[Dense] Let $(\mathcal X,\metric)$ be a metric space.  
A set $D \subset \mathcal X$ is ($\metric$-) dense in a set $I \subset \mathcal X$ if every  $\metric-$ open subset of $I$ contains at least one point of $D$. That is:
$\forall x \in I, \epsilon > 0 \exists \xi \in D: \metric(\xi,x) < \epsilon$.     
\end{defn}


\begin{defn}[Uniform convergence of sequences of sets]\label{def:uniform_setconvergence}
Let $\bigl(\mathcal X,\metric\bigr)$ be a metric space.  
A sequence of sets $(Q_n)_{n \in \nat}$, $Q_n \subset \mathcal X$, $\abs{Q_n} < \infty$ converges to set $Q \subset \mathcal X$ \textit{uniformly} if we have:
\[\forall \epsilon > 0 \exists N_0 \in \nat \forall n \geq N_0 \forall q \in Q \exists q_n \in Q_n : \metric(q_n,q) < \epsilon. \]

If the sequence consists of finite sets, we can rewrite the condition as 
\[\forall \epsilon > 0 \exists N_0 \in \nat \forall n \geq N_0 \forall q \in Q : \min_{q_n \in Q_n} \metric(q_n,q) < \epsilon. \]

\end{defn}

In preparation for the derivations below, it will be useful to establish that uniform convergence to a dense subset of set $I$ implies uniform convergence to $I$ itself. Formally we express this as follows:

\begin{lem}
\label{lem:unifdenseset1}
Let $Q$ be a dense subset of $I$ and let $Q_n$ be a sequence of sets uniformly converging to $Q$ in the limit of $n \to \infty$. We have \[\forall e > 0 \exists N_0 \forall n \geq N_0 \forall x \in I \exists s \in Q_n: \metric(x,s) \leq e. \]


\begin{proof}
Let $e>0$, $N_0 \in \nat$ such that for all $n \geq N_0$ we have $\forall q \in Q \exists s_n(q) \in Q_n: \metric(s_n(q),q) \leq \frac{e}{2}$. 
Now, choose any $x \in I, n \geq N_0$. Since $Q$ is dense in $I$ we can choose $q \in Q$ such that $\metric(q,x) \leq \frac{e}{2}$. 
Hence, utilising the triangle inequality, we conclude $\metric(x,s_n(q)) \leq \metric(x,q) +\metric(q,s_n(q)) \leq \frac{e}{2} + \frac{e}{2} = e$.
\end{proof}
\end{lem}
An alternative statement of the lemma is that, under the lemma's premise, we have 

\[\forall e > 0 \exists N_0 \forall n \geq N_0 \forall x \in I : \inf_{s \in Q_n} \metric(x,s) \leq e. \]


\begin{defn} [``Eventually dense partition '']
For each $n \in \nat$ let $\mathcal P_n :=\{P_{n,1},...P_{n,n}\}$ be a partition of $I$ into subsets, each being a connected set and having the property that $P_{n,i}$ contains exactly one sample input $s_i$. Furthermore, let $r_{n,i} = \sup_{x,x' \in P_{n,i}} \frac{1}{2} \norm{x-x'}$ be $ P_{n,i}$'s ``radius'' w.r.t norm $\norm{\cdot}$. 
We call $(\mathcal P_n)_{n \in \nat}$ an \emph{eventually dense} partition sequence if the sequence $(r^*_n)$ of maximum radii, $r^*_n : = \max \{ r_{n,i} \}$, converges to zero as $n \to \infty$.

A sequence of inputs $(G_n)_{n \in \nat}$ will be called ``eventually dense'' if it is impossible to define a partition sequence separating the points in $G_n$ for each $n$ such that $G_n$ is not eventually dense.
\end{defn}
It should be clear that a sample grid that converges to a dense subset in $I$ is eventually dense (and vice versa).


It will become useful to remind ourselves of the standard fact that monotone sequences converge if they are bounded:
\begin{thm}[Monotone convergence theorem]
Let $(a_n)_{n \in \nat}$ be a monotone sequence of real numbers. That is, $a_n \geq a_{n+1}, \forall n \in \nat$ or $a_n \leq a_{n+1}, \forall n \in \nat$. The sequence converges to a finite real number if and only if the sequence is bounded. In particular, if the sequence is monotonically increasing then $\lim_{n \to \infty} a_n = \sup a_n$. If it is decreasing, we have $\lim_{n \to \infty} a_n = \inf a_n$.
\end{thm}
This theorem can be directly applied to prove the following result:
\begin{cor}
\label{cor:monseqfctsencl}
Let $(\phi_n)_{n \in \nat}$, $\phi_n : I \to \Real$ be a point-wise monotonically increasing sequence of functions and, let $(\gamma_n)_{n \in \nat}$, $\gamma_n : I \to \Real$ be a point-wise monotonously decreasing sequence of functions. That is, $\forall x \in I \forall n \in \nat: \phi_n(x) \leq \phi_{n+1}(x) \wedge \gamma_n(x) \geq \gamma_{n+1}(x) $. Furthermore, we assume $\forall x \in I \forall n \in \nat: \phi_n(x) \leq \gamma_{n}(x)$. 

Then, both sequences are point-wise convergent with point-wise limits 
$\lim_{n \to \infty} \phi_n (x) = \sup_{n \in \nat} \phi_n \leq \inf_{n \in \nat} = \lim_{n \to \infty} \gamma_n (x) $.
\end{cor}
The corollary will be needed below when we establish convergence of our function estimate bounds.

\subsubsection{Convergence with respect to the exponentiated metric}
\label{sec:unifconvhoeldermetricequivmetricunifconv}
The purpose of the next two results is to show that it does not matter whether one establishes a convergence property with respect to a metric $\metric$ or whether one does so with respect to the exponentiated $\metric^p$.
\begin{lem}\label{lem:hoeldermetricconv}
Let $p \in (0,1]$, $\mathcal X$ be a metric space and $(x_n)_{n \in \nat}$ be a sequence in $\mathcal X$ converging to $x \in \mathcal X$ with respect to metric $\metric$ iff the sequence converges to $x$ with reselect to metric $\metric^p$.

\begin{proof}
$\Rightarrow:$
Define the sequence $d_n :=  \metric(x_n,x) $.
Convergence with respect to $\metric$ means that $d_n \to 0$ as $n \to \infty$. Since $d_n \geq$ mapping $ \psi: \mathfrak x \mapsto \mathfrak x^p$ is well-defined for sequence inputs $d_n$. Furthermore, $\psi$ is a continuous function and thus, 
$\lim_{n \to \infty} \metric^p(x_n,x) = \lim_{n \to \infty} \psi(d_n) =  \psi( \lim_{n \to \infty} d_n) = \psi (0)  =0$.
Hence, $x_n$ converges to $x$ with respect to $\metric^p$ as well.
$\Leftarrow:$ Completely analogous.

\end{proof}  
\end{lem}

\begin{lem}\label{lem:hoeldermetricconv}
Let $p \in  (0,1]$, $\mathcal F$ be a metric space of functions. Sequence of functions $(f_n)_{n \in \nat}$ converges uniformly to $f$ with respect to metric $\metric$ iff the sequence also converges uniformly to $f$ with respect to metric $\metric^p$.


\begin{proof} $\Rightarrow:$
Since $f_n \to f$ uniformly, we have 
$\forall e >0 \exists N_e \geq 0 \forall n \geq N_e, x \in I : \metric(f_n , f) < e$. We show that also $\forall e >0 \exists N_e \geq 0 \forall n \geq N_e, x \in I : \metric(f_n , f)^p < e$.
Mapping $\psi: x \mapsto x^p$ invectively maps positive numbers onto the set of positive real numbers. Hence, $\metric(f_n, f) < e \Leftrightarrow \metric(f_n , f)^p < e^p$.
Let $e >0$ and $ \varepsilon := \sqrt[p]{e}$. Due to uniform convergence, we can choose $ N_\varepsilon$ such that $\forall x \in I, n \geq N_\varepsilon: \metric(f_n , f) <\varepsilon$ which is equivalent to stating $\forall x \in I, n \geq N_\varepsilon: \metric(f_n , f)^p <\varepsilon^p = e$. $\Leftarrow:$ Completely analogous.
\end{proof}  
\end{lem}




\subsection{Theoretical guarantees for kinky inference and learning}
\label{sec:targetest_multidim}

%\jcom{Talks about Lipschitz instead of Hoelder .. needs to be fixed}
%We assume the dimensionality of the target's input domain $I \subseteq \inspace$ is $d \geq 1$. 
 Remember, we defined the set of samples as $D_n= \{\bigl( s_i, \tilde f_i, \varepsilon(s_i) \bigr) \vert i=1,\ldots, N_n \} $. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the grid of sample inputs contained in $D_n$.
Our exposition focusses on the case of one-dimensional output space. That is, $\outspace = \Real$.


In this subsection, we assume $\metric$ to be any suitable metric on the target function domain. For instance, we have seen that  $\metric(x,y) := \normtop{x-y}$ defines a proper metric and (uniform) convergence with respect to this metric is equivalent to (uniform) convergence with respect to metric $ D: (x,y) \mapsto \norm{x-y}$ (see Sec. \ref{sec:unifconvhoeldermetricequivmetricunifconv}). Also note, that Lipschitz continuity with respect to $\metric$ is H\"older continuity with respect to metric $D$. Therefore, we may use the terms Lipschitz continuity and H\"older continuity interchangeably if it is clear which metrics the terms refer to.
Furthermore, remember we are primarily interested in functions restricted to be Lipschitz with respect to $\metric$ and bounded by functions $\lbf,\ubf$. That is, we know the target is contained in $\mathcal K_{prior} = \lipset L \metric  \cap \bfset$ where $\lipset L \metric$ denotes the $L$- Lipschitz functions with respect to metric $\metric$ and $\bfset :=\{\phi : I \to \Real \, | \, \forall x \in I : \lbf(x) \leq \phi(x) \leq \ubf(x)\} $. 

%We will now define a data inference mechanism $M_{dinf}$ that maps the sample $D_n$ into a subset 
%
%\subsubsection{A priori knowledge} We assume, our  a priori knowledge is that we know, for each $J \subset I$ a nonnegative number $L[J]$ such that the underlying target $f \in \mathcal F$ is $L[J]-$Lipschitz on $J$. That is, $\mathcal K_{prior} =\{\phi \in \mathcal F \,|\, \forall J \subset I \forall x,x' \in J: \abs(\phi(x) - \phi(x') \leq L[J] \norm{x-x'}\}$.

%\subsubsection{Computing a posteriori knowledge}

\begin{defn}[Sample-consistent functions]
A function $\kappa: I \to \Real$ is called \textit{consistent with sample} $D_n$ (shorthand: \emph{sample-consistent}) if \[ \forall i \in \{1,\ldots,N_n\}: \kappa(s_i) \in [\tilde f_i - \varepsilon(s_i), \tilde f_i + \varepsilon(s_i)  ] =[\underline f(s_i),\overline f(s_i) ].\] The set of all sample-consistent functions on $I$ will be denoted by $\mathcal K(D_n)$.
\end{defn}
Informally speaking, $\mathcal K(D_n)$ contains all functions that could have created the sample set. Conversely, sample $D_n$ rules out all functions that are not sample-consistent as candidates for being the target. 

%In the lingo of our consideration on topological inference, $\mathcal K(D_n)$ embodies part of our posterior knowledge: $\mathcal K_{\text{post}} = \mathcal K(D_n)$. 





\begin{defn}[Enclosure]
Let $\decke,\boden: I \to \Real$ be two functions, such that $\decke
\geq \boden$ pointwise on $I$. We define their enclosure on $I$ as
the compact interval $\mathcal E^\decke_\boden (I)$ of all functions
between them. That is, \[\mathcal E^\decke_\boden (I) := \{\phi: I
\to \Real \vert \forall t \in I: \decke(t) \geq \phi(t) \geq
\boden(t) \}.\]
\end{defn}
For ease of notation we omit the domain argument whenever we refer
to $I$. That is, we write  $\mathcal E^\decke_\boden :=\mathcal
E^\decke_\boden (I)$.


An \emph{enclosure} is sample-consistent if it is contained in the set $\mathcal K(D_n)$ of sample-consistent functions. 

Based on the sample, we desire to find two
\emph{enclosing functions} $\decke_n: I \to \Real$, $\mathfrak
\boden_n: I \to \Real$ bounding the target from above and below that
are as tight as possible and converge to the target in the limit of
infinite, sample set size $N$. 
More formally, we would ideally like it to satisfy the following desiderata:

\begin{defn}[Desiderata] \label{defn:desiderata}
 Based on an observed, erroneous data set $D_n$, we desire to define enclosing functions $\decke_n, \boden_n$ that give rise to an enclosure $\einschluss$ with the following properties:

\begin{enumerate} 

    \item \textbf{Sample-Consistency}: The enclosure is sample-consistent, i.e. $\einschluss \subseteq \mathcal K(D_n)$.
		%The enclosing functions are sample-consistent, i.e. $\decke_n,\boden_n \in \mathcal K(D_n)$.
    
     \item \textbf{Exhaustiveness (conservatism)}: Every $\phi, \lbf \leq \phi \leq \ubf$ coinciding with target $f$ on with grid $G_n$ and
having at most Lipschitz
    constant $L[J]$ on every subset $J \subset I$ is also contained in the enclosure: $\forall \phi: I \to \Real, \phi \in \mathcal K_{prior} \cap \mathcal K(D_n): \phi \in \einschluss $. That is, $\mathcal K_{prior} \cap \mathcal K(D_n) \subseteq \einschluss$.
    In particular, the target is always contained in the enclosure: $f \in \einschluss$.

\item \textbf{Monotonicity:} Additional data does not increase uncertainy. That is,
if $G_n \subseteq G_{n+1} $, we have $\mathcal E^{\decke_{n+1}}_{\boden_{n+1}} \subseteq \einschluss, \forall n \in \nat$.
		   
\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I$ and for each input $x \in I$ let the sample error function $\varepsilon: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
\begin{itemize}
\item (a) The sequence of enclosures converges point-wise. In particular, 

$\forall x \in I: 0 \leq \lim_{n \to \infty} \decke_n (x) -\boden_n(x) \leq 2 m_x$.
In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
		
		\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
\end{itemize}	
   \item \textbf{Minimality (optimality)}: 
If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    

\end{enumerate}

\end{defn}
 
Next, we will derive enclosing functions that give rise to such an enclosure.

\begin{defn}[Sample point ceiling and floor functions]
\label{def:sampleceilandfloormultidim}
Remember, $\overline f(s_i) = \tilde f_i + \varepsilon(s_i)$ and $\underline f(s_i) = \tilde f_i - \varepsilon(s_i)$.  For the $i$th sample ($i \in \{1,\ldots,N_n\}$, $L \geq 0$, $x \in I$, we define  
\begin{enumerate}
	\item The $i$th \emph{sample point ceiling} function: \[\decke_n^i (x; L) :x \mapsto  \overline f (s_i) +  \,L \, \metric(x, s_i), \]
	\item and the $i$th \emph{sample point floor} function: \[\boden_n^i (x; L) :x \mapsto  \underline f (s_i) -  \,L \, \metric(x, s_i) .\] 
\end{enumerate}
%Here $n_i$ is a norm-dependent constant ensuring that both functions are $L$-Lipschitz with repect to $\norm{\cdot}$. 
If constant $L$ is clear from the context, its explicit mention shall be omitted.
\end{defn}
%Note, for the maximum-norm $\norm{\cdot}_\infty$ it is easy to see that  $n_i =1$ renders both functions $L$-Lipschitz. For the 1-norm, one can either utilize Lipschitz arithmetic to show that for $n_i=1$ the functions would be $d L$-Lipschitz or, one can leverage the well-known inequality~\cite{koenigsberger:2000}   $\norm{\cdot}_1 \leq d \norm{\cdot}_\infty$ to determine $n_i := \frac{1}{d}$.

We show that these functions give rise to enclosures that satisfy the first three Desiderata of Def. \ref{defn:desiderata}:
\begin{lem} 
\label{lem:samplefloorceiling}
Let  $i \in \{1,...,N_n\}$.
Enclosure $\mathcal E^{\decke_n^i}_{\boden_n^i}$ is (i) consistent with the $i$th sample and (ii) conservative. 
More specifically: 

(i)  \[\forall i \in \{1,...,N_n\}, \phi \in \mathcal E^{\decke_n^i}_{\boden_n^i}:  \phi(s_i) \in [\underline f(s_i), \overline f(s_i)].\]

(ii) If target $f: I \to \Real$ is $L$-Lipschitz on $J \subset I$ with respect to metric $\metric$ then the corresponding sample ceilings are upper bounds and the sample floor functions are lower bounds. That is, for $i =1,\ldots, N$, $\forall x \in J$ we have \[\decke_n^i (x; L) \geq f(x)  \geq  \boden_n^i (x; L).\]

\begin{proof}
(i) Follows directly from the definitions.
(ii) We show the first inequality, $\decke_n^i (x; L) \geq f(x)$. (The proof of the second inequality,  $f(x)  \geq  \boden_n^i (x; L)$, is completely analogous.) 

Due to Lipschitz continuity, we have 
$\forall x,x' \in J: \abs{f(x) - f(x')} \leq L \metric(x,x').$
In particular, 
$\forall x \in J: f(x) - f(s_i) \leq\abs{f(x) - f(s_i)} \leq L \, \metric(x, s_i).$
Hence, 
$\forall x \in J: f(x)  \leq f(s_i) + L \, \metric(x, s_i)$ 
$\leq \tilde f(s_i) +\varepsilon(s_i) + L \, \metric(x, s_i) = \decke_n^i (x; L)$.

%Since $\decke_n^i (x; L)  = \tilde f_i + \varepsilon(s_i) + L_i \metric(x, s_i)$, this entails $\decke_n^i (x; L) \geq f(x)$ for all $\varepsilon(s_i) \geq 0$. Therefore, it suffices to focus on the case for $\varepsilon(s_i) =0$.
 %So, let $\varepsilon(s_i) =0$. 

%&\leq f(c_i) + \ell_i \normtop{x-c_i}_{\infty}:= \decke^i_n(x)
%\end{align}
%where $\ell_i = \sqrt d L_i$ is a Lipschitz constant for the max-norm.
%The last inequality is due to the well-known relationship 
%$\normtop{x}_\infty \leq \normtop{x}_2 \leq \sqrt{d} \, \normtop{x}_\infty $.



\end{proof}
\end{lem}



\begin{defn}[Lipschitz enclosure] On  $J \subset I$, define the \emph{optimal ceiling} as
\[\decke^*_n (x; L) : x \mapsto  \min_{i=1,\ldots,N_n} \decke^i_n(x;L)\]
and the \emph{optimal floor function} as
 \[\boden^*_n (x; L) :x \mapsto   \max_{i=1,\ldots,N_n} \boden^i_n(x; L).\]
The enclosure $\einschluss$ with enclosing function choices $\decke_n := \decke_n^*$ and $\boden_n := \boden_n^*$ will be called ``Lipschitz enclosure'' and be denoted by $\opteinschluss$.
If constant $L$ is fixed and clear from the context we may omit its explicit mention from the syntax.
\end{defn}
An example for a Lipschitz enclosure is depicted in Fig. \ref{figlipencl_nonoise_neo}.
\begin{figure*}
%
	\centering
		\includegraphics[scale = .4]{content/content_hoelderquad2/figs/enclosure.pdf}
	\caption{Example of an enclosure (for metric $\metric: (x,y) \mapsto \norm{x-y}$) for a given sample of function values on one-dimensional domain $I = [0,2.5]$. The Lipschitz constant was one, uniformly over the entire domain. The samples were noise-free.}
	\label{figlipencl_nonoise_neo}
\end{figure*}

Before we can prove that the optimal functions are indeed optimal, we need to undergo some derivative preparations.

\begin{lem} \label{lem:optenclcontainstarget}
The Lipschitz enclosure always contains the target. That is, $\decke^*_n(x) \geq f(x) \geq \boden^*_n(x), \forall x \in I, n \in \nat$.
\begin{proof}
This is a direct consequence of Lem. \ref{lem:samplefloorceiling}.
\end{proof}
\end{lem}


\begin{thm}
Choosing $\decke_n := \min\{ \decke_n^*, \ubf\}$ and $\boden_n := \max\{\boden_n^*,\lbf\}$ yields an enclosure $\einschluss$ that satisfies Desiderata 1-5 specified in Def. \ref{defn:desiderata}.
\begin{proof}
We bear in mind that $\einschluss = \bfset \cap \opteinschluss$.

\textit{1) Consistency}. By definition of $\decke_n^*$ and $\boden_n^*$ in conjunction with Lem. \ref{lem:samplefloorceiling}.(i) we see that $\opteinschluss = \cap_i \mathcal E_{\boden_n^i}^{\decke_n^i} \subseteq \mathcal K(D_n)$. Desideratum 1 follows from $\einschluss = \bfset \cap \opteinschluss \subseteq \opteinschluss$.\\

\textit{2) Conservatism}. By definition of $\decke_n^*$ and $\boden_n^*$ in conjunction with Lem. \ref{lem:samplefloorceiling}.(ii) we have $\mathcal K(D_n) \cap \lipset L \metric \subseteq \opteinschluss $. Hence, $\mathcal K_{prior} \cap \mathcal K(D_n) =  \bfset \cap \lipset L \metric \cap \mathcal K(D_n) \subseteq \opteinschluss \cap \bfset = \einschluss$ proving
Desideratum 2.

\textit{3) Monotonicity.} 

This follows directly from the way the ceiling and floor functions are defined. Considering the ceiling:
We have $\decke_n (x) = \min\{\ubf,\min_{i =1,...,N_n} \decke^i_{n}(x) \} $. W.l.o.g., for $i=1,\ldots,N_n$, let $\decke_n^i = \decke_{n+1}^{i}$. 

Then 
$\decke_{n+1} (x) = \min\{\ubf(x),\min\Bigl\{ \min_{i =1,...,N_n} \decke^i_{n}(x), \min_{i= N_n +1,\ldots,N_{n+1}}  
\decke^i_{n+1}(x) \Bigr\} \} $

$\leq \min \{\ubf(x), \min_{i =1,...,N_n} \decke^i_{n}(x)  \}=  \decke_{n}(x) $. Statement $\boden_{n+1} (x) \geq  \boden_{n}(x)$ follows analogously.

\textit{4 a) Pointwise convergence.} In absence of errors, one could show pointwise convergence by utilising Cor. \ref{cor:monseqfctsencl} --which is applicable due to Lem. \ref{lem:samplefloorceiling} in conjunction with 3). We will now prove the more general statement in the presence of observational errors $\varepsilon$.

Pick an arbitrary $x \in I$ such that the error $\varepsilon(\xi) \leq m_x$ for all $\xi$ in some $\epsilon-$ball $\mathcal U_\epsilon(x) = \{\xi \in I | \metric(\xi ,x) < \epsilon \}$ around $x$.
It remains to be shown that $\lim_{n\to \infty} \decke_n(x) - \boden_n(x) \leq 2 m_x$ for inputs $x$.
Define $L := \sup_{J \subset I} L[J] $  to be the maximum Lipschitz constant on the domain.
Let $e>0$ be arbitrary. We show that there exists $N_0 \in \nat$ such that for all $n \geq N_0: \decke_n(x;L) - \boden_n(x;L) \leq e + 2 m_x$ as follows:

Since we assume $G_n$ converges to a dense subset of $I$, Lem. \ref{lem:hoeldermetricconv} applies. Hence, there is $N_0 \in \nat$ such that $\forall n \geq N_0 \exists i \in \{1,...,N_n\}: s_i \in \mathcal U_{e/2L}(x) = \{\xi \in I | \metric(x,\xi) < \frac{e}{2L}\}$.
 
So, choose $n \geq N_0$. 
Hence, $L \metric(x,s_j)  < \frac{e}{2}, \,\varepsilon(s_q) \leq m_x \hspace{1cm} \text{ (*)}$ 

where 
$q \in \arg\min_{i \in \{1,...,N_n\} } L \, \metric(x,s_i) $. 
We have:

 $\decke_n^*(x;L) - \boden_n^*(x;L)$ 

$= \min_{i \in \{1,...,N_n\} } L \, \metric(x,s_i) + \tilde f(s_i) + \varepsilon(s_i) - \max_{i \in \{1,...,N_n\} }  \tilde f(s_i) - \varepsilon(s_i) - L \metric(x,s_i) $

$= \min_{i \in \{1,...,N_n\} } \bigl\{L \, \metric(x,s_i) + \tilde f(s_i) + \varepsilon(s_i) \bigr\}+ \min_{i \in \{1,...,N_n\} }  \bigl\{-\tilde f(s_i) + \varepsilon(s_i)+ L \, \metric(x,s_i)\bigr\} $

$\leq \min_{i \in \{1,...,N_n\} } \bigl\{L \, \metric(x,s_i) + \tilde f(s_i) + \varepsilon(s_i)    -\tilde f(s_i) + \varepsilon(s_i)+ L \metric(x,s_i) \bigr\}$

$=2\min_{i \in \{1,...,N_n\} } L \, \metric(x,s_i) + \varepsilon(s_i) $
$\leq 2 L \, \metric(x,s_q)+ 2\varepsilon(s_q) $
$\stackrel{(*)}{<} e + 2 m_x  $.\\
As a special case, in the noise free setting with  $m_x=0$, this result implies that the enclosure shrinks to a single value as $n \to \infty$. That this value is $f(x)$ follows from Lem. \ref{lem:optenclcontainstarget}.  \\

We have shown convergence of $\decke_n^*(x;L) - \boden_n^*(x;L)$. Convergence of $\decke_n(x) - \boden_n(x) = \min\{\ubf(x), \decke_n^*(x)\} - \max\{\lbf(x),\boden_n^*(x)\}$ follows by continuity of the $\max$ and $\min$ operation.


 \textit{4 b) Uniform convergence.}
Let $G_n = \{s_1,\ldots,s_{N_n}\}$ denote the $N_n$-sample input grid which we assume to converge uniformly to a dense set of domain $I$. 
Assuming $m_x =0$, we desire to show that $\forall e >0 \exists N_0 \forall n \geq N_0 , x \in I : \abs{ \decke_n(x) - f(x)} < e $ and $\forall e >0 \exists N_0 \forall n \geq N_0 , x \in I : \abs{ \boden_n(x) - f(x)} < e $.
Let $e > 0$.
 %and define $L := \sup_{J \subset I} L[J] $  to be the maximum Lipschitz constant on the domain and $n$ the corresponding normalization constant making the target $L$-Lipschitz. 
Let $G_n$ converge uniformly to a dense subset  $G \subset I$. By Lem. \ref{lem:unifdenseset1}, we know that 
there exists $N_0$ such that for all $n \geq N_0$ and all $x \in I$ there is a $q \in G_n$ such that $\metric(x,q) < \frac{e}{2  L}$. 
In particular, $ L \, \metric(s_q ,x ) < \frac{e}{2}$ where we define $q \in \arg\min_{i} \metric(s_i ,x )$. 
Hold such $N_0$ fixed, choose arbitrary $n \geq N_0$ and $x \in I$. 

Firstly, we show that each $\decke_n$ converges uniformly to $ f$ by showing $\abs{\decke_n(x) - f(x) }< e$.

We have
$\abs{\decke_n(x) - f(x)} \stackrel{2)}{=} \decke_n(x) - f(x) $  
$\stackrel{def.}{\leq}\min_{i} \decke^i_n(x) - f(x) $
$\leq   \decke^q_n(x) - f(x) $
$\stackrel{def.}{\leq}  L \, \metric(x,s_q)+f(s_q) - f(x) $
$\leq  L \, \metric(x,s_q)+ \abs{f(s_q) - f(x) }$
$\stackrel{f \text{ L-Lip.}}{\leq}  L \, \metric(x,s_q)+   L \, \metric(x,s_q) <  e$ where in the last step we have leveraged $L \, \metric(x,s_q) < \frac{e}{2}$.

Secondly, we show the analogous for the floor. 
That is we show  $\abs{\boden_n(x) - f(x) }< e$:


$\abs{  \boden_n(x) - f(x)}\stackrel{2)}{=} f(x) - \boden_n(x) $  
$\stackrel{def.}{\leq} f(x) - \max_{i} \boden^i_n(x) $
$\leq   f(x)- \boden^q_n(x)  $
$\stackrel{def.}{\leq} f(x) - (f(s_q) -  L \, \metric(x,s_q))  $
$\leq  L \, \metric(x,s_q)+ \abs{f(s_q) - f(x) }$
$\leq  L \, \metric(x,s_q)+   L\, \metric(x,s_q) < 2 \frac{e}{2}= e$ where, as before, in the last step we have utilised $ L \, \metric(x,s_q) < \frac{e}{2}$.\\

\textit{5) Optimality.}  By Lem. \ref{lem:Hoeldarithmetic}.(6) we know that $\decke_n^*,\boden_n^* \in \lipset L \metric$. Hence, $\decke_n , \boden_n \in \lipset L \metric \cap \bfset$. 
Thus, if $\lipset L \metric \cap \bfset \subset \mathcal E^b_a $ then also $\decke_n, \boden_n \in \mathcal E^b_a$. This implies $\decke_n \leq b, \boden_n \geq a$, i.e. $\einschluss \subseteq \mathcal E^b_a$.
\end{proof} 
\end{thm}

%So far, our guarantee of optimality of the enclosure only holds for $p=1$. This is due to the fact that at present, we cannot guarantee that the minimum of two $p-L-$H\"older functions also is $p-L-$ H\"older, unless $p=1$. Intuitively, this might well be true also for $p<1$, but investigating this rigorously has to be deferred to future work.


\subsubsection{Pruning of uninformative sample points}

The remaining derivations of this subsection show how the data can be pruned when new data points arrive the render old ones uninformative. They can be skipped by a reader not interested in the details.


We will focus our exposition on the ceiling functions. The derivations for the floor functions are entirely analogous (where one has to substitute $\leq$ by $\geq$ and $\min$ by $\max$).

\begin{lem}\label{lem:dominatedsamplefct}
Let $\mathcal N := \{1,...,N_n\}$, $q,i \in \mathcal N$ be the indices of a sample ceiling functions $\decke_n^q(\cdot;L_q)$ and $\decke_n^i(\cdot;L_i)$ where $L_q \leq L_i$ everywhere. We have
\[\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i) \Leftrightarrow \forall t \in I: \decke_n^q(t;L_q) \leq \decke_n^i(t;L_i).\]
\begin{proof}
$\underline \Rightarrow:$
$\decke_n^q (t;L_q) = \decke_n^q(s_q;L_q) + L_q \metric(t, s_q)$
$\leq \decke_n^q(s_q;L_q) +L_q \metric(s_i, s_q)+ L_q\metric(t,s_i) $
$=\decke_n^q(s_i;L_q) +  L_q\metric(t,s_i)$ 
$\leq \decke_n^i(s_i;L_i) + L_q\metric(t,s_i)  $
$= \overline f(s_i) + L_q\metric(t,s_i)    $
$\leq \overline f(s_i) + L_i\metric(t,s_i)    = \decke_n^i(t;L)$.\\
$\underline \Leftarrow:$ Trivial.
\end{proof}
\end{lem}

The lemma implies that we can remove samples that are dominated by other samples' ceiling functions:
\begin{thm} 
\label{thm:removedominatedsamples}
Let $\mathcal N := \{1,...,N_n\}$ and let $i \in \mathcal N$ be the index of sample ceiling function $\decke_n^i(\cdot;L_i): t \mapsto \overline f(s_i) + L_i \abs{t - s_i}$. If there exists a sample ceiling function $u^q(\cdot;L_q), q \in \mathcal N \backslash \{i\}$, $L_q \leq L_i$ everywhere, having a value below $\decke_n^i$ at the $i$th sample, then the $i$th sample plays no role in computing the optimal ceiling. That is:\\  
If there is $q \in \mathcal N \backslash\{ i\}$ such that $\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i)$, then we have: \\
$\decke^*(t;L_1,\ldots,L_n) = \min_{j \in \mathcal N} \decke^j(t;L_j) =  \min_{j \in \mathcal N\backslash \{i\}} \decke^j(t;L_j)$.
\begin{proof}
The theorem is a direct consequence of Lem. \ref{lem:dominatedsamplefct}. 
\end{proof}

\end{thm}

\begin{rem}
Normally, $L_i = L_q\, \forall q,j \in \mathcal N$ where each $L_i$ is a function mapping $t$ to a H\"older constant valid in some neighbourhood of $t$.
\end{rem}

\begin{thm}
\label{thm:dominatedsamples_greatLIPconst}
Let $\mathcal N := \{1,...,N_n\}$ and let $i \in \mathcal N$ be the index of sample ceiling function $\decke_n^i(\cdot;L_i): t \mapsto \overline f(s_i) + L_i \metric(t,s_i) $. We have: \\if $\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i)$ then 
\[  \decke^*(t;L_1,...,L_n) \geq \min_{j \in \mathcal N} \decke^j(t;L_j) =  \min \bigl\{ \min_{j \in \mathcal N\backslash \{i\}} \decke^j(t;L_j), \decke_n^q(s_i) + L_i \metric(t,s_i) \bigr\}.\]

\begin{proof} Let $\decke_n^q(s_i;L_q) \leq \decke_n^i(s_i;L_i)$ and $L_q > L_i $ everywhere. Since $\decke_n^q(s_i) \leq \decke_n^i(s_i)$ and $L_i < L_q$ we have $\decke_n^i(t;L_i) = \overline f(s_i) + L_i \metric(t,s_i) $
$= \decke_n^i(s_i;L_i) + L_i \metric(t,s_i)$
$\geq \decke_n^q(s_i;L_i) + L_i \metric(t,s_i)$.

\end{proof}

\end{thm}

\begin{rem} \label{rem:assumptionsceil}
The theorems allow us to assume, without loss of generality, that $\decke_n^i(s_i) = \min_j \decke_n^j(s_i)$ (as a preprocessing step, Thm. 
\ref{thm:removedominatedsamples} allows us to remove samples violating this assumption, without changing the optimal ceiling). 
\end{rem}



\subsection{Special case: one-dimensional inputs, p=1}
For functions on one-dimensional domains and exponent $p=1$, matters simplify. We assume $\metric(x,y) := \abs{x-y}$. This case has been considered in the context of optimisation \cite{Shubert:72} and quadrature \cite{Baran2008}.

%From now on, we will assume that $L_1,\ldots, L_n =: L$. 

For ease of notation we will therefore often omit explicit mention of $L$ in the statements of the ceiling and floor functions, unless it becomes necessary. Additionally, we assume, without loss of generality, that the sample inputs are ordered such that $s_1\leq \ldots \leq s_n$. Let $b := \sup I $ and $a := \inf I$.



Under these assumptions, we next show that computing the optimal ceiling function only requires taking into account the up to two most adjacent samples :

\begin{lem}\label{lem:shieldingsamples_1d}
Let the assumptions of Rem. \ref{rem:assumptionsceil} hold.

We have:

\begin{enumerate}
\item $\forall i \in \mathcal N, t \geq s_i: \decke_n^* ( t) =\min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N, j \geq i\}, $	
\item $\forall i \in \mathcal N, t \leq s_i: \decke_n^* ( t) =\min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N,j \leq i\}.$ 
\end{enumerate}

\begin{proof}
1.) Let $t \geq s_i$. 

$\text{Showing} \leq:$ This is trivial since $\decke_n^*(t) = \min_{j \in \mathcal N} \decke_n^j(t) \leq \min_{j \in \mathcal N, j \geq i}\decke_n^j(t)$. 


$\text{Showing} \geq:$ For contradiction, assume $\decke_n^* ( t) < \min_{j\geq i} \decke_n^j(t).$ 
Hence, there is a $q \in \mathcal N, q < i$ such that 

\begin{equation}
\label{eq:lala1}
\forall t \geq s_i: \decke_n^q(t) < \min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N, j \geq i\}.
\end{equation}
%Again, utilizing $s_i \leq t \leq s_{i+1}$ this is equivalent to $\exists q : \decke_n^q(t) < \overline f(s_i) + L \abs{t - s_i} \wedge \decke_n^q(t) < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$
%
%$\stackrel{def.}{\Leftrightarrow} \exists q : \overline f(s_q) + L \abs{t - s_q} < \overline f(s_i) + L \abs{t - s_i} \wedge f(s_q) + L \abs{t - s_q} < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$.
Choose such a $q \in \mathcal N, q < i$.
By assumption, we have $s_q < s_i$. Hence, Condition \ref{eq:lala1} implies in particular $\decke_n^q(t) = \overline f(s_q) + L (t - s_q) < \decke_n^i(t) = \overline f(s_i) + L (t - s_i)$. We have $\overline f(s_q) + L (t - s_q) = \overline f(s_q) + L (s_i - s_q) + L (t - s_i) = \decke_n^q(s_i) + L (t-s_i) \stackrel{!}{ <} \overline f(s_i) + L (t-s_i)=\decke_n^i(s_i) + L (t-s_i) $ which holds, if and only if $\decke_n^q(s_i) < \decke_n^i(s_i)$. However, the last inequality contradicts our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (Rem. \ref{rem:assumptionsceil}).\\

2.) The proof is analogous to 1.):

Let $t \leq s_i$. 

$\text{Showing} \leq:$ This is trivial since $\decke_n^*(t) = \min_{j \in \mathcal N} \decke_n^j(t) \leq \min_{j \in \mathcal N, j \leq i}\decke_n^j(t)$. 


$\text{Showing} \geq:$ For contradiction, assume $\decke_n^* ( t) < \min_{j\leq i} \decke_n^j(t).$ 
Hence, there is a $q \in \mathcal N, q > i$ such that 

\begin{equation}
\label{eq:lala2}
\forall t \leq s_i: \decke_n^q(t) < \min\{  \decke_n^j (t) \,\vert \, j \in \mathcal N, j \leq i\}.
\end{equation}
%Again, utilizing $s_i \leq t \leq s_{i+1}$ this is equivalent to $\exists q : \decke_n^q(t) < \overline f(s_i) + L \abs{t - s_i} \wedge \decke_n^q(t) < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$
%
%$\stackrel{def.}{\Leftrightarrow} \exists q : \overline f(s_q) + L \abs{t - s_q} < \overline f(s_i) + L \abs{t - s_i} \wedge f(s_q) + L \abs{t - s_q} < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$.
Choose such a $q \in \mathcal N , q > i$ (thus, $s_q > s_i$).


The way we ordered the samples, $q>i$ implies $s_q > s_{i}$. Thus, Condition \ref{eq:lala2} implies in particular: $\decke_n^q(t) =\overline f(s_q) + L (s_q -t ) < \decke_n^i(t) = \overline f(s_{i}) + L ( s_{i} -t) = \decke_n^{i}(s_{i}) + L ( s_{i} -t)$. We have $\overline f(s_q) + L (s_q-t) = \overline f(s_q) + L (s_q - s_{i} ) + L (s_{i} - t) = \decke_n^q(s_{i}) + L (s_{i} - t) \stackrel{!}{ <} \decke_n^{i}(s_{i}) + L (s_{i} -t) $. However, the last inequality contradicts our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (made in Rem. \ref{rem:assumptionsceil}).



%By definition of the sample ceilings, it follows 
%$\exists q : \decke_n^q(t) < \decke_n^i(t) \wedge \decke_n^q(t) < \decke_n^{i+1} (t)$.
\end{proof}

\end{lem}


The Lemma allows us to tremendously simplify computation of the optimal ceiling-- it tells us that all samples, except the up to two neighbouring ones, can be discarded when computing the optimal ceiling at a given input:

\begin{thm}\label{thm:lipceil1d}
Let the assumptions of Rem. \ref{rem:assumptionsceil} hold.
Let $a:=s_0: = \inf I, s_{n+1} := b:=\sup I$ and for $i=0,\ldots,N_n$ define $I_i := [s_i,s_{i+1}]$.

%we define line segments via \[\alpha^\decke_n_i: t
%\mapsto \overline f(s_i) + L (t - s_i)\] and \[\beta^\decke_n_i: t \mapsto
%\overline f(s_{i+1}) -L (t - s_{i+1}).\] 

We have:

\[ \forall i, t \in I_i: \decke_n^*(t) = \begin{cases} \decke_n^{i+1}(t) &, i=0\\
															\min\{\decke_n^i(t),\decke_n^{i+1}(t) \}&, i \in \{1,\ldots,N_n-1 \} \\
															\decke_n^{i}(t)&, i=N.	\end{cases} \]


\end{thm}



The analogous statement for the H\"older floor function can be derived completely analogously and will therefore be given at this point without proof:


\begin{thm}\label{thm:lipfloor1d}
Assume the assumptions of Rem. \ref{rem:assumptionsceil} hold.
Let $a:=s_0: = \inf I, s_{n+1} := b:=\sup I$ and for $i=0,\ldots,N_n$ define $I_i := [s_i,s_{i+1}]$.

%we define line segments via \[\alpha^\boden_i: t
%\mapsto \overline f(s_i) + L (t - s_i)\] and \[\beta^\boden_i: t \mapsto
%\overline f(s_{i+1}) -L (t - s_{i+1}).\] 

We have:

\[ \forall i, t \in I_i: \boden_n^*(t) = \begin{cases} \boden_n^{i+1}(t) &, i=0\\
															\max\{\boden_n^i(t),\boden_n^{i+1}(t) \}&, i \in \{1,\ldots,N_n-1 \} \\
															\boden_n^{i}(t)&, i=N.	\end{cases} \]


\end{thm}

In addition to saving computation by discarding irrelevant samples, the the theorems will play an important role when computing a closed-form the integral of the optimal ceiling and floor functions.


%\begin{cor}\label{cor:lipceil}
%Assume the assumptions of Rem. \ref{rem:assumptionsceil} hold.
%On interval $I_i = [s_i,s_{i+1}]$ we define line segments via \[\alpha^\decke_n_i: t
%\mapsto \overline f(s_i) + L (t - s_i)\] and \[\beta^\decke_n_i: t \mapsto
%\overline f(s_{i+1}) -L (t - s_{i+1}).\] 
%
%We have:
%
%\begin{enumerate}
%\item
%For $i \in \mathcal N$ the projection of the optimal H\"older ceiling onto interval $I_i$ can be computed as the point-wise minimum of $\alpha^\decke_n_i$ and $ \beta^\decke_n_i$:
%\[\forall t \in I_i: \decke_n^* ( t) =\min\{ \alpha^\decke_n_i(t), \beta^\decke_n_i(t) \}= \min\{\decke_n^i(t),\decke_n^{i+1}(t) \} .\] 
%
%\item $\forall t \in [a,s_1], \tau \in [s_n,b] : \decke_n^*(t) = \decke_n^1(t) \wedge  \decke_n^*(\tau) = \decke_n^N(\tau)$.
%\end{enumerate}
%\begin{proof}
%1.) Since $s_i \leq t \leq s_{i+1}$, $\alpha^\decke_n_i(t) = \decke_n^i(t)$ and $\beta^\decke_n_i(t) = \decke_n^{i+1}(t)$.
%Let $t \in I_i$.
%
%$\text{Showing} \leq:$ By definition, $\decke_n^*(t) = \min_j \decke_n^j(t) = \min_j \overline f(s_j) + L \abs{t - s_j}  \leq \min\{\overline f(s_i) + L \abs{t - s_i}, \overline f(s_{i+1}) + L \abs{t - s_{i+1}} \} = \min\{\overline f(s_i) + L (t - s_i), \overline f(s_{i+1}) - L  (t-s_{i+1}) \}$ where the last equality holds owing to $s_i \leq t \leq s_{i+1}$.
%
%$\text{Showing} \geq:$ For contradiction, assume $\decke_n^* ( t) <  \min\{\overline f(s_i) + L (t - s_i), \overline f(s_{i+1}) - L  (t-s_{i+1}) \}$. This implies the existence of a $q \in \mathcal N$ such that 
%
%\begin{equation}
%\label{eq:lala1}
%\decke_n^q(t) < \overline f(s_i) + L (t - s_i)
%\end{equation}
%and 
%\begin{equation}
%\label{eq:lala2}
 %\decke_n^q(t) < \overline f(s_{i+1}) - L  (t-s_{i+1}). 
%\end{equation}
%%Again, utilizing $s_i \leq t \leq s_{i+1}$ this is equivalent to $\exists q : \decke_n^q(t) < \overline f(s_i) + L \abs{t - s_i} \wedge \decke_n^q(t) < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$
%%
%%$\stackrel{def.}{\Leftrightarrow} \exists q : \overline f(s_q) + L \abs{t - s_q} < \overline f(s_i) + L \abs{t - s_i} \wedge f(s_q) + L \abs{t - s_q} < \overline f(s_{i+1}) + L  \abs{t-s_{i+1}}$.
%Choose such a $q \in \mathcal N$.
%
%Case A: Let $s_q < s_i$. Then, Condition \ref{eq:lala1} implies $\decke_n^q(t) = \overline f(s_q) + L (t - s_q) < \overline f(s_i) + L (t - s_i)$. We have $\overline f(s_q) + L (t - s_q) = \overline f(s_q) + L (s_i - s_q) + L (t - s_i) = \decke_n^q(s_i) + L (t-s_i) \stackrel{!}{ <} \overline f(s_i) + L (t-s_i)=\decke_n^i(s_i) + L (t-s_i) $ which holds, if and only if $\decke_n^q(s_i) < \decke_n^i(s_i)$. However, the last inequality contradicts our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (Rem. \ref{rem:assumptionsceil}).
%
%Case B: Let $s_q > s_{i+1}$. Then, Condition \ref{eq:lala2} implies $\overline f(s_q) + L (s_q -t ) < \overline f(s_{i+1}) + L ( s_{i+1} -t) = \decke_n^{i+1}(s_{i+1}) + L ( s_{i+1} -t)$. We have $\overline f(s_q) + L (s_q-t) = \overline f(s_q) + L (s_q - s_{i+1} ) + L (s_{i+1} - t) = \decke_n^q(s_{i+1}) + L (s_{i+1} - t) \stackrel{!}{ <} \decke_n^{i+1}(s_{i+1}) + L (s_{i+1} -t) $. However, the last inequality contradicts our our assumption of $\min_j \decke_n^j(s_i) =\decke_n^i(s_i)\,\forall i$ (made in Rem. \ref{rem:assumptionsceil}).
%We have proven the first claim.\\
%
%2.) 
%
%%By definition of the sample ceilings, it follows 
%%$\exists q : \decke_n^q(t) < \decke_n^i(t) \wedge \decke_n^q(t) < \decke_n^{i+1} (t)$.
%\end{proof}
%
%\end{cor}



In preparation for the part on quadrature it will prove useful to determine the extrema of the enclosing functions on each subinterval $I_i$.


\begin{lem} \label{lem:Lipbetweensamples}
We have:
\begin{enumerate}
\item If $\decke^i(s_i)  \leq \decke^{i+1}(s_i)$ and $\decke^{i+1}(s_{i+1})  \leq \decke^i(s_{i+1})$ then 
we have 

$ \abs{\overline f(s_{i+1})- \overline f(s_i)} \leq L \abs{s_{i+1} -s_i}$.
\item If $\boden^i(s_i)  \geq \boden^{i+1}(s_i)$ and $\boden^{i+1}(s_{i+1})  \geq \boden^i(s_{i+1})$ then 
we have

$ \abs{\underline f(s_{i+1})- \underline f(s_i)} \leq L \abs{s_{i+1} -s_i}$.
\end{enumerate}

\begin{proof}
1.) 
On the one hand, we have:
$\decke^i(s_i)  \leq \decke^{i+1}(s_i)$ 
$\Leftrightarrow$
 $\decke^i(s_i) - \decke^{i+1}(s_i)  \leq 0$
%
$\Leftrightarrow$ $ \overline f(s_i) + L \abs{s_i-s_i} -\overline f(s_{i+1}) - L \abs{s_i-s_{i+1}}  \leq 0$
%
$\Leftrightarrow$ $ \overline f(s_i) -\overline f(s_{i+1}) - L \abs{s_i-s_{i+1}}  \leq 0$
%
$\Leftrightarrow$ $ \overline f(s_i) -\overline f(s_{i+1})   \leq  L \abs{s_i-s_{i+1}}$

$\Leftrightarrow$ $ \overline f(s_{i+1})-\overline f(s_i)    \geq  -L \abs{s_{i+1} - s_i}$.\\
%
On the other hand:
$\decke^{i+1}(s_{i+1})  \leq \decke^i(s_{i+1})$
%
$\Leftrightarrow$ $\decke^{i+1}(s_{i+1}) - \decke^i(s_{i+1}) \leq 0$
$\Leftrightarrow$ $\overline f(s_{i+1})  - \overline f(s_i) - L \abs{s_{i+1} -s_i} \leq 0$
$\Leftrightarrow$ $\overline f(s_{i+1})  - \overline f(s_i)  \leq  L \abs{s_{i+1} -s_i}$.

2.) The proof is completely analogous to 1.). 
\end{proof}
\end{lem}

\begin{lem} \label{lem:ceiling_maxvalues}
Let $\metric(x,y) := \abs{x-y}$. For $i \in \{1,\ldots,N_n-1\}$ let  $L_i > 0$, $I_i = [s_i,s_{i+1}]$.
Define $\xi^{\mathfrak u}_i :=  \frac{s_i+s_{i+1}}{2}+ \frac{\overline f(s_{i+1}) - \overline f(s_i)}{2 L_i}  $.\\
For $t \in I_i$ we have 
\begin{enumerate}
\item $\xi^{\mathfrak u}_i \in I_i$ and  $\decke^*_n(t) = 
 \begin{cases}  
 \decke^i_n (t), & t \leq \xi^\decke_i \\
 \decke^{i+1}_n(t), & t \geq \xi^\decke_i.
 \end{cases}$
\item $\xi^{\mathfrak u}_i \in \arg\max_{\tau \in I_i} \decke^*_n (\tau)$.
\item  $y^{\mathfrak u}_i := \decke_n^*(\xi^{\mathfrak u}_i) = \frac{\overline f(s_{i+1}) +\overline f(s_i)}{2} + L_i \frac{s_{i+1}-s_i}{2}$ .
\end{enumerate}
\begin{proof} Let $1 \leq i \leq N-1, t \in I_i$. Define $\alpha^\decke_i(t) = \overline f(a_i) + L_i (t-a_i) = \decke_n^i(t), \beta^\decke_i (t) = \overline f(b_i) + L_i (b_i - t) = \decke^{i+1}_n(t), \forall t \in I_i$.

Firstly, $\xi^\decke_i \in I_i$ is a direct consequence of Lem. \ref{lem:Lipbetweensamples}.

Secondly, we show $\decke^*_n(t) = 
 \begin{cases}  
 \decke^i_n (t), & t \leq \xi^\decke_i \\
 \decke^{i+1}_n(t), & t \geq \xi^\decke_i.
 \end{cases}$:

By definition,
$\decke^*_n (t) = \min\{ \decke^{i}_n(t), \decke^{i+1}_n(t)\} = \min\{\alpha^i_n (t),\beta^i_n(t)\}$.
We have $\dot  \alpha^\decke_i(t) = L_i > 0$ and $\dot  \beta^\decke_i(t) = - L_i < 0$. Hence, the functions are strictly monotonous. Let $\xi$ be the input where the lines intersect, i.e. $\alpha^\decke_i(\xi) = \beta^\decke_i(\xi)$. Monotonicity implies $\forall t \leq \xi : \alpha^\decke_i(t) \leq \alpha^\decke_i(\xi) \wedge \beta^\decke_i(t) \geq \beta^\decke_i(\xi) = \alpha^\decke_i(\xi)$. 
Hence,
 \[\forall t \leq \xi: \decke^*_n(t) = \min \{\alpha_i^{\mathfrak u} (t), \beta_i^{\mathfrak u} (t) \}= \alpha_i^{\mathfrak u} (t).\] 
 
Analogously, monotonicity implies $\forall t \geq \xi : \beta^\decke_i(t) \leq \beta^\decke_i(\xi) \wedge \alpha^\decke_i(t) \geq \alpha^\decke_i(\xi) = \beta^\decke_i(\xi)$. 
Thus,
 \[\forall t \geq \xi: \decke^*_n(t) = \min \{\alpha_i^{\mathfrak u} (t), \beta_i^{\mathfrak u} (t) \}= \beta_i^{\mathfrak u} (t).\] 

In conjunction, we have 
 $\decke^*_n(t) = 
 \begin{cases}  
 \alpha_i^{\mathfrak u} (t), & t \leq \xi \\
 \beta_i^{\mathfrak u} (t), & t \geq \xi 
 \end{cases}$.
 Again, using monotonicity of $\alpha_i^{\mathfrak u}$ and $\beta_i^{\mathfrak u}$ we infer that $\decke^*_n$ has a single maximum $y := \decke^*_n(\xi)$ at $\xi$.

Next, we show that $\xi = \xi_i^{\mathfrak u}$: 

$\alpha^\decke_i(\xi) = \beta^\decke_i(\xi)$

$\Leftrightarrow  \overline f(s_i) + L_i (\xi - s_i) = \overline f(s_{i+1}) -L_i (\xi - s_{i+1})$

$\Leftrightarrow  L_i \bigl( 2 \xi - s_{i+1}  - s_i\bigr) = \overline f(s_{i+1}) - \overline f(s_i) $

$\stackrel{L_i > 0}{\Leftrightarrow}   \bigl( 2 \xi - s_{i+1}  - s_i\bigr) = \frac{\overline f(s_{i+1}) - \overline f(s_i)}{L_i} $

$\Leftrightarrow   \xi  = \frac{\overline f(s_{i+1}) - \overline f(s_i)}{2 L_i}  + \frac{s_{i+1}+s_i}{2} = \xi^{\mathfrak u}_i$.



Finally, we need to evaluate $\decke^*_n(\xi^{\mathfrak u}_i)$ to prove the last claim:
We have 

 $\decke^*_n(\xi) = \alpha_i^{\mathfrak u} (\xi) =  \overline f(s_i) + L_i (\xi - s_i) $

$= \overline f(s_i) + L_i \bigl( \frac{\overline f(s_{i+1}) - \overline f(s_i)}{2 L_i}  + \frac{s_{i+1}+s_i}{2} - s_i \bigr) $ 

$=  \frac{\overline f(s_{i+1}) - \overline f(s_i) + 2\overline f(s_i)}{2 }  + L_i \frac{s_{i+1}-s_i}{2}   = y_i^{\mathfrak u}$. 



\end{proof}

\end{lem}


Completely analogously to the the proof of Lem. \ref{lem:ceiling_maxvalues} we can prove the corresponding statement for the H\"older floor:
\begin{lem} \label{lem:floor_minvalues}
Let $\metric(x,y) := \abs{x-y}$.
For $i \in \{1,\ldots,N_n-1\}$ let  $L_i > 0$, $I_i = [s_i,s_{i+1}]$.
Define $\xi^{\boden}_i :=  \frac{s_i+s_{i+1}}{2}- \frac{\underline f(s_{i+1}) - \underline f(s_i)}{2 L_i}  $.\\
For $t \in I_i$ we have 
\begin{enumerate}
\item $\xi^{\boden}_i \in I_i$ and  $\boden^*_n(t) = 
 \begin{cases}  
 \boden^i_n (t), & t \leq \xi^\boden_i \\
 \boden^{i+1}_n(t), & t \geq \xi^\boden_i.
 \end{cases}$
\item $\xi^{\boden}_i \in \arg\min_{\tau \in I_i} \boden^*_n (\tau)$.
\item  $y^{\boden}_i := \boden_n^*(\xi^{\boden}_i) = \frac{\underline f(s_{i+1}) +\underline f(s_i)}{2} - L_i \frac{s_{i+1}-s_i}{2}$ .
\end{enumerate}

\begin{proof} The proof is analogous to the one provided for Lem. \ref{lem:ceiling_maxvalues}.
%
%The case $i = N$ is trivial. Let $1 \leq i \leq N-1$. By definition,
%$\boden^i_n (t) = \min\{ \alpha^\boden_i(t), \beta^\boden_i(t)\}$.
%We have $\dot  \alpha^\boden_i(t) = - L_i < 0, \forall t$ and $\dot  \beta^\boden_i(t) =  L_i > 0, \forall t$. Hence, the functions are strictly monotonous. Let $\xi$ the input where the lines intersect, i.e. $\alpha^\boden_i(\xi) = \beta^\boden_i(\xi)$. Monotonicity implies $\forall t \leq \xi : \alpha^\boden_i(t) \geq \alpha^\boden_i(\xi) \wedge \beta^\boden_i(t) \leq \beta^\boden_i(\xi) = \alpha^\boden_i(\xi)$. 
%Hence,
 %\[\forall t \leq \xi: \boden_n^i(t) = \max \{\alpha_i^{\mathfrak l} (t), \beta_i^{\mathfrak l} (t) \}= \alpha_i^{\mathfrak l} (t).\] 
 %
%Analogously, monotonicity implies $\forall t \geq \xi : \beta^\boden_i(t) \geq \beta^\boden_i(\xi) \wedge \alpha^\boden_i(t) \leq \alpha^\boden_i(\xi) = \beta^\boden_i(\xi)$. 
%Thus,
 %\[\forall t \geq \xi: \boden_n^i(t) = \max \{\alpha_i^{\mathfrak l} (t), \beta_i^{\mathfrak l} (t) \}= \beta_i^{\mathfrak l} (t).\] 
%
%In conjunction, we have 
 %$\boden_n^i(t) = 
 %\begin{cases}  
 %\alpha_i^{\mathfrak l} (t), & t \leq \xi \\
 %\beta_i^{\mathfrak l} (t), & t \geq \xi 
 %\end{cases}$.
 %Again, utilizing monotonicity of $\alpha_i^{\mathfrak l}$ and $\beta_i^{\mathfrak l}$ we infer that $\boden_n^i$ has a single minimum $y := \boden_n^i(\xi)$ at $\xi$.
%It remains  (i) to show that $\xi = \xi_i^{\mathfrak l}$, (ii) we need to evaluate $\boden_n^i(\xi^{\mathfrak l}_i)$ showing $y = y_i^{\mathfrak l}$, and (iii) we need to show that 
%$\xi^{\mathfrak l}_i \in \bar I_i$ .
%
%(i) $\alpha^\boden_i(\xi) = \beta^\boden_i(\xi)$
%
%$\Leftrightarrow  f(a_i) - L_i (\xi - a_i) = f(b_i) +L_i (\xi - b_i)$
%
%$\Leftrightarrow  L_i \bigl( 2 \xi - b_i  - a_i\bigr) = f(a_i) - f(b_i) $
%
%$\stackrel{L_i > 0}{\Leftrightarrow}   \bigl( 2 \xi - b_i  - a_i\bigr) = \frac{f(a_i) - f(b_i)}{L_i} $
%
%$\Leftrightarrow   \xi  = \frac{f(a_i) - f(b_i)}{2 L_i}  + \frac{b_i+a_i}{2} = \xi^{\mathfrak l}_i$.
%
%(ii) $\boden_n^i(\xi) = \alpha_i^{\mathfrak l} (\xi) =  f(a_i) - L_i (\xi - a_i) $
%
%$= f(a_i) - L_i \bigl( \frac{f(a_i) - f(b_i)}{2 L_i}  + \frac{b_i+a_i}{2} - a_i \bigr) $ 
%
%$=  \frac{-f(a_i) + f(b_i) + 2f(a_i)}{2 }  - L_i \frac{b_i-a_i}{2}   = y_i^{\mathfrak l}$. 
%
%(iii) $\xi^{\mathfrak l}_i \in \bar I_i$ if and only if $a_i \leq \xi^{\mathfrak l}_i \leq b_i$. Since $\xi^{\mathfrak l}_i = -\frac{f(b_i) - f(a_i)}{2 L_i}  + \frac{b_i+a_i}{2}$ and $\frac{b_i+a_i}{2}$ is just in the middle value of interval $\bar I_i = [a_i,b_i]$, we have 
%
%$\xi^{\mathfrak l}_i \in \bar I_i$ if $\abs{\frac{f(b_i) - f(a_i)}{2 L_i}} \leq \abs{\frac{b_i-a_i}{2}}$.
%However, the latter is the case due to $L_i$-H\"older continuity of $f$ on $\bar I_i$.  
  
\end{proof}

\end{lem}

\section{Kinky inference over function values - nonparametric machine learning}
%Non-deductive inference is the process of drawing conclusions about the general on the basis of the particular. Machine learning is about the automated construction of a mathematical model that facilitates non-deductive inference. 
%These days, supervised machine learning essentially is function estimation. That is, given a (noisy) sample of a target function, artificial learning is the process of constructing a computable model of a function that generalises beyond the observed sample. 
%While parametric machine learning subsumes all the information contained in the sample in a parameter of fixed size, nonparametric machine learning typically takes the entire sample into account when making inference. That is, it creates models for non-deductive inference whose complexity grows with the sample size. This conceptual difference typically leads to complementary advantages and disadvantages \cite{mitchellbook:97}: Parametric methods typically can take longer to aggregate all information into the finite parameter, have stronger inductive bias, but once a model is learned, inference speed is determined by the size of the parameter, not the size of the data and thus, can be very fast. For instance, in artificial neural networks, the parameter is given by the weights of the network links. Once the weights are determined from the training data, the required computation only depends on the number of weights, not on the size of the training data. 
Supervised machine learning methods are algorithms for inductive inference. On the basis of a sample, they construct (learn) a generative model of a data generating process that facilitates inference over the underlying ground truth function and aims to predict function values at unobserved inputs. If the inferences (i.e. predictions of function values) are utilised in a decision-making process whose outcome involves risk, information about the prediction uncertainty can be vital. For instance, a robot that has a poor model about its dynamics would have to keep greater distance to obstacles than one that has a good model. Having such applications in mind, we are especially interested in learning algorithms that allow for conservative inference. That is, the predictions come with uncertainty quantifications that never underestimate the true uncertainty. 

Since typically there are infinitely many explanations that could explain any finite data set, one needs to impose assumptions a priori in order to be able to generalise beyond the observed data and to establish the desired uncertainty quantifications. 

These assumptions, sometimes referred to as inductive bias \cite{mitchellbook:97}, take different forms and are an essential distinguishing factor between different machine learning algorithms. 
The inductive bias implicitly or explicitly restricts the hypothesis space. That is, the space of target functions that can be learned.
In this work, we devise a learning method for ground truth functions that are assumed to be contained in the a priori hypothesis space $\Kprior$ of H\"older continuous functions that may moreover be known to be contained within certain predefined bounds. 

In case the parameters of the H\"older class the function is contained in are known (or at least an upper bound on the H\"older constant and bounds) we can give robust error estimates that give worst-case guarantees on the predictions.

\subsubsection{Conservative learning and inference in a nutshell \jcom{perhaps an intro candidate}}

The general approach we will consider is as follows. We desire to learn a function $f$ based prior knowledge in the form that $f \in \mathcal K_{prior}$ and some (noisy) function sample $\data$. 
The task of conservative inference over functions (i.e. learning) then is the step of forming a posterior belief of the form $f \in \mathcal K_{post}= \mathcal K_{prior} \cap \mathcal K(\data)$
where data is the set of all functions that could have generated the observed data. 
The conservatism lies in the fact that we have not ruled out any candidate functions that are consistent with our information. 
To perform inference over a function value at input $x$ in a conservative manner, we merely infer that $f \in \prederrbox(x) =[\boden(x), \decke(x)]$ 
where \textit{floor function} $\boden(x) =\inf_{\phi \in \mathcal K_{post}} \phi(x)$ and  \textit{ceiling function } $\decke(x) = \sup_{\phi \in \mathcal K_{post}} \phi(x)$ delimit the values the any function in the posterior class $\mathcal K_{post}$ could give rise to. If a point prediction of a function value is required, we could choose any function value between the values given by the floor and ceiling which also give rise to the bounds on the point prediction. 
Of course, the type of inference we get from this general procedure depends on the chosen prior class $\mathcal K_{prior}$ and the specifics of the data. We will develop \textit{kinky inference} which is a method for considering prior classes of multi-output functions over metric input spaces that are optionally constrained by boundedness and by knowledge of H\"older regularity as well as knowledge on uncertainty bounds on the observations and function inputs. For this class of inference mechanisms we prove conservatism and convergence guarantees to the learning target. What is more, we will consider classes where the imposed regularity assumptions are uncertain or adapted lazily to adjust for overly restrictive prior assumptions that do not fit the observations.

The capability of imposing boundedness assumptions, accommodating for noise and being able to adjust the regularity assumptions will prove of great importance in the 
control applciations we will consider in Sec. \jcom{fill in} 
\subsection{The framework of conservative inference -- definitions, setting and desiderata }
\label{sec:prob_def}

%As a basic example, consider the ordinary differential equation $\frac{\d x}{\d t} = (\xi - x)$ where $\xi: I \subset \Real \to \Real $ is an absolutely continuous function superimposed by a countable number of steps. Suppose, the ODE describes the trajectory $x: I \to \Real$ of a controlled particle in one-dimensional state space. Based on $N$ time-state observations $D_N=(t_1,x_1 ),..., (t_N,x_N )$ we desire to estimate the total distance travelled $S$ in time interval $I$, $S = \int_I x(t) \d t$. Solving


%%\textbf{Setting.}
%Let $(\inspace, \metric_\inspace)$, $(\outspace ,\metric_\outspace)$ be two metric spaces and $f: \inspace \to \outspace$ be a function. Spaces $\inspace, \outspace$ will be referred to as \textit{input space} and \textit{output space}, respectively.
%Assume, we have access to a \textit{sample set} $\data_n:= \{\bigl( s_i, \tilde f_i, \varepsilon(s_i) \bigr) \vert i=1,\ldots, N_n \} $ containing $N_n \in \nat$ sample vectors $\tilde f_i \in \outspace$ of function $f$ at sample input $s_i \in \inspace$. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the \textit{grid} of sample inputs contained in $\data_n$.
%
%
%The sampled function values are allowed to have interval-bounded \textit{observational error} given by $\varepsilon : \inspace \to \Real^m_{\geq 0}$. That is, all we know is that $f(s_i) \in \obserrhyprec_i := [\underline f_1(s_i), \overline f_1(s_i) ] \times \ldots \times [\underline f_d(s_i), \overline f_d(s_i) ]$ where $\underline f_j(s_i) := \tilde f_{i,j} - \varepsilon_j(s_i)$, $\overline f(s_i) := \tilde f_{i,j} + \varepsilon_j(s_i)$ and $\tilde f_{i,j}$ denotes the $j$th component of vector $\tilde f_{i}$.
%
%The interpretation of these errors depends on the given application. For instance, in the context of system identification, the sample might be based on noisy measurements of velocities. The noise may be due to sensor noise or may represent numerical approximation errors. As we will see below, $\varepsilon$ may also quantify \textit{input} uncertainty (that is when predicting $f(x)$, $x$ is uncertain). This is an important case to consider, especially in the context of control applications where the ground truth might represent a vector field or an observer model.
%
%
%It is our aim to learn function $f$ in the sense that, folding in knowledge about $\data_n$, we infer \textit{predictions} $\predf(x)$ of $f(x)$ at unobserved \textit{query inputs} $x \notin G_n$. Being the target of learning, we will refer to $f$ as the \emph{target} or \emph{ground-truth} function. 
%
%In addition to the predictions themselves, we are also interested in \textit{conservative bounds} on the error of the inferred predictions. Depending on the hypothesis space under consideration these bounds can be of a probabilistic or deterministic nature. In this chapter, for the time being, we are interested in the latter.
%
%That is, we desire to provide a computable \textit{prediction uncertainty} function $\prederr: \inspace \to \outspace$ such that we can guarantee that for any query $x$, our prediction $\predf(x)$ is within the cube of error given by $\prederr(x)$ as follows 
%
%
%Being interested in conservative worst-case guarantees, we are mostly interested 
%
%
%
%In order to be able to deduce the error bounds around the predictions $\predf$, it is important to impose a priori restrictions on the class of possible targets (the hypothesis space). 

%\textbf{Definitions.}
Let $f: \inspace \to \outspace$ be a function where $\inspace$, $\outspace$ are two spaces endowed with (semi-) metrics $\metric_{\inspace}: \inspace^2 \to \Real_{\geq 0}, \metric_\outspace:\outspace^2 \to \Real_{\geq 0}$, respectively. Spaces $\inspace, \outspace$ will be referred to as \textit{input space} and \textit{output space}, respectively. 

For simplicity, in this work, we will assume $\outspace \subseteq \Real^m$ and $\metric_{\outspace}(y,y') = \norm{y-y'}$ for some standard norm that is equivalent to the maximum norm.\footnote{Extensions to more general output spaces seem very possible, albeit, instead of learning a function, we would then learn a discrepancy from a nominal reference in output space. We will leave this to future work.}
In contrast, we do not impose restrictions on the input space metric. While our simulations will consider the deterministic case, it would be entirely possible to consider inputs spaces that are index sets of stochastic processes endowed with suitable norm metrics (for further comments, refer to Sec. \ref{sec:kinkyinf_futurework}). Since the output metric is defined via a norm, we will often drop the subscript of the input metric. That is we may write $\metric$ instead of $\metric_\inspace$.


\jcom{probably move remark below where the experiments are and we can show a periodic kernel.}
\begin{rem}[Remarks on relationship to kernels]
Metrics are measures of dissimilarity and will aid our efforts to perform inference.
%For instance, we have seen that  $\metric(x,y) := \normtop{x-y}$ defines a proper metric and (uniform) convergence with respect to this metric is equivalent to (uniform) convergence with respect to metric $ D: (x,y) \mapsto \norm{x-y}$ (see Sec. \ref{sec:unifconvhoeldermetricequivmetricunifconv}). 
In much of the recent machine learning literature, kernel functions are utilised to perform inference.
Note, a kernel function is a measure of similarity often based on quantities inversely related to metrics (which are dissimilarity measures). Therefore, we can draw on the kernel learning literature to draw inspiration about metrics or pseudo-metrics suitable for an application at hand (for a recent overview of kernels, including periodic ones, refer to\cite{duvenaud2014}). 
For instance, we might consider folding in knowledge of periodicity of the underlying function class which we might borrow from the kernel learning literature \cite{duvenaud2014}.
\end{rem}


Assume, we have access to a \textit{sample} or \textit{data set} $\data_n:= \{\bigl( s_i, \tilde f_i, \varepsilon(s_i) \bigr) \vert i=1,\ldots, N_n \} $ containing $N_n \in \nat$ sample vectors $\tilde f_i \in \outspace$ of function $f$ at sample input $s_i \in \inspace$. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the \textit{grid} of sample inputs contained in $\data_n$. Subscript $n$ aides our exposition when we consider sequences of data sets indexed by $n$.

The sampled function values are allowed to have interval-bounded \textit{observational error} given by $\varepsilon : \inspace \to \Real^m_{\geq 0}$. That is, all we know is that $f(s_i) \in \obserrhyprec_i := [\underline f_1(s_i), \overline f_1(s_i) ] \times \ldots \times [\underline f_d(s_i), \overline f_d(s_i) ]$ where $\underline f_j(s_i) := \tilde f_{i,j} - \varepsilon_j(s_i)$, $\overline f(s_i) := \tilde f_{i,j} + \varepsilon_j(s_i)$ and $\tilde f_{i,j}$ denotes the $j$th component of vector $\tilde f_{i}$.

The interpretation of these errors depends on the given application. For instance, in the context of system identification, the sample might be based on noisy measurements of velocities. The noise may be due to sensor noise or may represent numerical approximation errors. As we will see below, $\varepsilon$ may also quantify \textit{input} uncertainty (that is when predicting $f(x)$, $x$ is uncertain). This is an important case to consider, especially in the context of control applications where the ground truth might represent a vector field or an observer model. Finally, the observational error can also model error around the compuation of the metric. This might be of interest if the metric has to be approximated numerically or estimated with statistical methods. 


It is our aim to learn function $f$ in the sense that, combining prior knowledge about $f$ with the observed data $\data_n$, we infer \textit{predictions} $\predfn(x)$ of $f(x)$ at unobserved \textit{query inputs} $x \notin G_n$. Being the target of learning, we will refer to $f$ as the \emph{target} or \emph{ground-truth} function. In our context, the evaluation of $\predf_n$ is what we refer to as \textit{(inductive) inference}. For a discussion of the competing definitions of non-deductive inference in a philosophical context the reader is referred to \cite{Flach2000}. The entire function $\predfn$ that is learned to facilitate predictions is referred to as the \textit{predictor}. Typically,
the predictor lies in the space that coincides or is at least dense in space $\mathcal K_{prior}$ of conceivable target functions. In this case the predictor can be referred to as a \textit{candidate function} or \textit{hypothesis} \cite{mitchellbook:97}.

In addition to the predictions themselves, we are also interested in \textit{conservative bounds} on the error of the inferred predictions. Depending on the hypothesis space under consideration these bounds can be of a probabilistic or deterministic nature. In this chapter, for the time being, we are interested in the latter.

That is, we desire to provide a computable \textit{prediction uncertainty function} $\prederr: \inspace \to \outspace$ such that we \textit{believe} that for any query $x \in \inspace$, the ground truth value $f(x)$ lies somewhere within the \textit{uncertainty hyperrectangle }
\begin{equation}
	\prederrbox_n(x) := \Bigl\{ y \in \outspace \, | \, \forall j \in \{1,...,\dim{\outspace}\} : f_j(x) \in [ \predfnj(x) - \prederrnj(x), \predfnj(x) + \prederrnj(x)] \Bigr\}
\end{equation} around the prediction $\predfn (x)$.
Here, $\predfnj(x)$, $\prederrnj(x)$ denote the $j$th components of vectors $\predfn(x), \prederrn(x)$, respectively.

 %As a word of caution, we remark that using the last statement as a definition still seems very broad. 

%\textbf{Desiderata.}
In this chapter, will understand a \textit{machine learning mechanism} to implement a computable function that maps a data set $\data_n $ to a prediction function $\predf_n$ and an uncertainty estimate function $\prederr_n$ both of which exhibit certain desirable properties, which will be discussed next. 

Mitchell \cite{mitchellbook:97} proposed to call an algorithm to be a machine learning algorithm, if it improves its performance with respect to some specified cost function with increasing data. We are interested in learning a function that \textit{converges} to the ground truth. That is, in the limit of infinite, informative data, the point-wise distance, measured by a norm, between the prediction and the ground-truth should shrink \textit{monotonically}. That is, the cost in Mitchell's definition might be quantified by this distance.

%Note, converse statement would be too general to be useful to capture behaviour that matches the intuition of learning. 

Being interested in \textit{conservative} inference, we desire to give conditions under which the uncertainty beliefs $\prederr_n(x)$ around the predictions $\predf_n(x)$ are never overoptimistic. That is, we desire to give a guarantee that the target cannot assume values outside the uncertainty hyperrectangle $\prederrbox_n$.

In order to conceive an inference mechanism that can generalise beyond the sample and to establish its desired properties, we need to make a priori assumptions.
That is, it is important to impose a priori restrictions on the class of possible targets (the \textit{hypothesis space}). As mentioned above, we will denote this space by $\mathcal K_{prior}$.


%Furthermore, for each sub-hyperrectangle $J \subset I$ we have access to a Lipschitz number $L_J \geq 0$. That is, $L_J$ is a number such that $\forall x,x' \in J: \abs{f(x) - f(x')} \leq L_J \, \norm{ x- x'}$ where $\norm \cdot$ is a fixed norm on $I$.\footnote{Note, due to norm-equivalence in finite-dimensional space, it is of no principal concern which norm is chosen.} 

%We assume $\metric_\inspace: \inspace^2 \to \Real$ to be any suitable metric (or pseudo-metric) on the target function domain that encodes a notion of dissimilarity between inputs. 
%
%%For instance, we have seen that  $\metric(x,y) := \normtop{x-y}$ defines a proper metric and (uniform) convergence with respect to this metric is equivalent to (uniform) convergence with respect to metric $ D: (x,y) \mapsto \norm{x-y}$ (see Sec. \ref{sec:unifconvhoeldermetricequivmetricunifconv}). 
%
%Note, a kernel function is a measure of similarity often based on quantities inversely related to metrics (which are dissimilarity measures). Therefore, we can draw on the kernel learning literature to draw inspiration about metrics or pseudo-metrics suitable for an application at hand (for a recent overview of kernels, including periodic ones, refer to\cite{duvenaud2014}). 
%For instance, we might consider folding in knowledge of periodicity of the underlying function class which we might borrow from the kernel learning literature \cite{duvenaud2014}.

%Also note, Lipschitz continuity with respect to $\metric$ is H\"older continuity with respect to metric $D$ (cf. Thm. \ref{thm:pmetricphoelder}). Therefore, we may use the terms Lipschitz continuity and H\"older continuity interchangeably if it is clear which metrics the terms refer to.

Once sample $\data_n$ is known, we can combine the information contained therein with the prior. At the very least, we can form a \textit{posterior hypothesis space} $\mathcal K_{post}$ by falsification. That is, we eliminate all hypotheses from $\mathcal K_{prior}$ that could not have generated the sample. 

To formalise this, we define 
\begin{equation}
	\Kdat = \Bigl\{ f: \inspace \to \outspace | \, \forall i \in \{1,...,N_n\}: \, f(s_i) \in \obserrhyprec_i  \Bigr\}
\end{equation}
to be the set of \textit{sample-consistent} functions as the set of all functions that might have generated the data.


To perform inference over functions, we construct a posterior hypothesis set $\mathcal K_{post}$. 
Normally, we would expect \textit{monotonicity} of our inference in that the additional information afforded by the data should not make us more uncertain. That is,  $\mathcal K_{post} \subset \mathcal K_{prior}$. In the light of the data, we should 
also require consistency yielding the requirement $\mathcal K_{post} \subset \mathcal K(\data_n)$. In conjunction we then have 
\begin{equation}
	\mathcal K_{post} \subseteq \mathcal K_{prior} \cap \mathcal K(\data_n)
\end{equation} or, conservatively, 
$\mathcal K_{post} = \mathcal K_{prior} \cap \mathcal K(\data_n)$.\footnote{As an aside, note that the latter choice is consistent with conservatism and would be the choice consistent with Popper's approach of falsification to epistemology.}

\begin{rem}[False prior hypotheses, lazy relaxations and search bias]
\label{rem:lazyrelaxationpriorspace}
Furthermore, note that it might happen that $\mathcal K_{prior} \cap \mathcal K_{post} =\emptyset$. Presuming there was no problem with the data set, this might be indicative of $\mathcal K_{prior}$ being too restrictive. One approach to deal with such a situation would be to relax our prior assumptions. If we do so just enough to ensure that $\mathcal K_{post} \neq \emptyset$, we will call this relaxation \emph{lazy}. Below, where we consider prior hypothesis spaces constrained by H\"older regularity under the assumption that the parameters of the H\"older condition are known. Since this is often difficult to achieve in practice, Sec. \ref{sec:uncertainconstants} contains a lazy adaptation rule of the H\"older constant. This is an example of a lazy relaxation of the prior hypothesis space. 
From a wider point of view, one could of course interpret the relaxation procedure as part of the learning algorithm. From this vantage point, the actual prior hypothesis space is the maximally relaxed space on which the algorithm imposes the search bias \cite{mitchellbook:97} preferring hypothesis in more constrained sets over relaxed ones. Since more constrained spaces have lower learning capacity, this search bias imposes a form of regularization found in many machine learning approaches. Interestingly, in the context of the relaxations of the H\"older constant consider in Sec. \ref{sec:lazylipconstupdate}, preferring lower H\"older constants over larger ones seems to establish a strong link to regularisation in kernel methods \cite{wahabarepresenter:1990}. Investigating these links is something we have not done yet, but will be undertaken in future work. \jcom{perhaps last paragraph into discussion and future work. also check wahabas book}
\end{rem}

Having learned, i.e. performed inference over functions, we can use this to predict function values conservatively by considering all the function values within the posterior class which could be assumed at a given query input $x \in \inspace$.

We are now in a position to summarize our desiderata on the joint inference mechanism as follows:

%Based on sample set $\data_n$, we desire to design an inference 
%method that satisfies the following desiderata:

%\subsection{Task 1 -- Computing a conservative \textit{function estimate}}
%\label{sec:problemdef_task1}

\begin{defn}[Desiderata] \label{def:KIdesiderata} With definitions as above, we desire a machine learning algorithm to implement a mapping $\data_n \mapsto (\predfn,\prederrn)$ such that, for all data sets $\data_n$, the resulting inductive inference satisfies the following \textbf{desiderata}:
\begin{enumerate}
	\item \textit{Conservatism:} We can guarantee  $ f(x) \in \prederrbox_n(x),  \forall x \in \inspace, \forall f \in \mathcal K_{prior} \cap \mathcal K(\data_n)$.
	\item \textit{Monotonicity:} Additional data cannot increase the uncertainty. 
	
	That is, $\data_n \subseteq \data_{n+1} $ implies 
		$\prederrbox_n(x) \supseteq \prederrbox_{n+1}(x)$ or equivalently, $\prederr_n(x) \geq \prederr_{n+1}(x), \forall x \in \inspace$ (where the inequality holds for each component).
	%\item \textbf{Convergence} to the target: Let the set of sample inputs $G_n$ in $D_n$ converge to a dense subset of domain $I \subset \inspace$ and for each input $x \in I$ let the sample error function $\varepsilon: I \to \Real_{\geq 0}$ be bounded by $m_x \geq 0$ in some neighbourhood of $x$. Then, we have 
%\begin{itemize}
%\item (a) The uncertainty converges point-wise: $\forall x \in I: 0 \leq \lim_{n \to \infty} \prederrn(x) \leq m_x$.
%In particular, the enclosure converges to the target $f(x)$ at inputs $x$ where $m_x=0$.  
		%
		%\item (b) If convergence of $G_n$ is uniform and the target observations are error-free, i.e. $m_x = 0, \forall x \in I$, then the enclosure converges to the target $f$ uniformly.
%\end{itemize}	
	\item \textit{Convergence}: If $(G_n )_{n \in \nat }$ is a sample grid sequence of inputs converging to a dense subset of $\inspace$ (as $n \to \infty$) then  $\prederrn(x) \stackrel{n \to \infty}{\longrightarrow} \varepsilon(x), \forall x \in \inspace$. If convergence of the grid to the dense subset is uniform (cf. Def. \ref{def:uniform_setconvergence}) and the observational error is constant ($\exists c \in \Real: \varepsilon \equiv c $) then the convergence  $\prederrn \stackrel{n \to \infty}{\longrightarrow} \varepsilon$ is uniform. 
	   \item \textit{Minimality (optimality)}: There is no conservative uncertainty bound that is tighter than $\prederr_n$. That is, if for some hyperrectangle $H$, $x \in \inspace$ we have $H \subsetneq \prederrbox_n(x)$ then we have: $\exists x \in \inspace, f \in \mathcal K_{prior} \cap \mathcal K(\data_n) : f(x) \notin H$. 
%If an enclosure $\mathcal E^b_a$  satisfies \emph{Desideratum 2} then it is a superset of our enclosure $\einschluss$, i.e. $b \geq \decke_n, a \leq \boden_n$, $\einschluss \subseteq \mathcal E^b_a$.    
\end{enumerate}
\end{defn}

%Note, Desiderta 1-3 combined imply that in the absence of observational errors ($\varepsilon \equiv 0$), the prediction $\predfn$ converges monotonically to the target when data sets become increasingly dense.

\subsection{The kinky inference and nonparametric machine learning mechanism}

We are now in a position to define our kinky inference rule:

\begin{defn}[Kinky inference rule (KIR)] \label{def:KIL}
Let  $\Real_\infty := \Real \cup\{- \infty, \infty\}$ and $\inspace$ be some space endowed with a semi-metric $\metric_\inspace$. Let $\lbf,\ubf: \inspace \to \outspace \subseteq \Real_\infty^m$ denote \textit{lower- and upper bound functions}, that can be specified in advance and assume $\lbf(x) \leq \ubf(x), \forall x \in I \subset \inspace$ component-wise. 
Given sample set $\data_n$ we define the predictive functions $\predfn: \inspace \to \outspace, \prederrn: \inspace \to \Real^m_{\geq 0}$ to perform inference over function values.
For $j=1,\ldots,m$, their $j$th output component is given by  
%Furthermore, define
% $\coeff1,\coeff2 \in \Real_{\geq0}$ such that $\coeff1+\coeff2 =1$ (unless explicitly stated otherwise, we assume $\coeff1=\coeff2 =\frac 1 2)$.
	\begin{align}
   \predfnj(x) &:= \frac{1}{2} \min\{ \ubf_j(x), \decke_{n,j}(x)\} + \frac{1}{2} \max\{ \lbf_j, \boden_{n,j}(x) \} \label{eq:KIpred}\\  
	\prederrnj(x) &:= \frac{1}{2} \bigl(\min\{ \ubf_j(x), \decke_{n,j}(x)\} - \max\{ \lbf_j, \boden_{n,j}(x) \} \bigr)   
	\end{align}
	where $\decke_n, \boden_n: \inspace \to \Real^m$ are called ceiling and floor functions, respectively. Their $j$ component functions are given by
	$\decke_{n,j}(x) := \min_{i=1,\ldots,N_n}   \tilde f_{i,j} + L_j \metric^p(x,s_i) + \varepsilon_j(x)$ and 
	$\boden_{n,j}(x) := \max_{i=1,\ldots,N_n}   \tilde f_{i,j} - L_j \metric^p(x,s_i) - \varepsilon_j(x)$, respectively.
  Here $p \in \Real, L \in \Real^m$ and functions $\lbf,\ubf,\varepsilon$ are parameters that have to be specified in advance. To disable restrictions of boundedness, it is allowed to specify the upper and lower bound functions to constant $\infty$ or $-\infty$, respectively.	
	Function $\predfn$ is the predictor that is to be utilised for predicting/inferring function values at unseen inputs. Function $\prederrn(x)$ quantifies the uncertainty of prediction $\predfn(x)$. 
	%Note, if the data set is fixed (i.e. $n$ does not change), we may sometimes drop the subscript $n$ and write $\predf$ instead of $\predf_n$.
\end{defn}

%
%\begin{defn}[Kinky inference rule (KIR)] \label{def:KIL}
%Define $\lbf,\ubf: \inspace \to \Real \cup\{- \infty, \infty\}$ to be the \textit{lower- and upper bound function}, and assume $\lbf(x) \leq \ubf(x), \forall x \in I \subset \inspace$. 
%Given sample set $D_n$ we define the following predictive functions to perform inductive inference:
%%Furthermore, define
%% $\coeff1,\coeff2 \in \Real_{\geq0}$ such that $\coeff1+\coeff2 =1$ (unless explicitly stated otherwise, we assume $\coeff1=\coeff2 =\frac 1 2)$.
	%\begin{align}
   %\predf_n(x) &:= \frac{1}{2} \min\{ \ubf(x), \decke_n(x)\} + \frac{1}{2} \max\{ \lbf, \boden_n(x) \} \\  
	%\prederr_n(x) &:= \frac{1}{2} \bigl(\min\{ \ubf(x), \decke_n(x)\} - \max\{ \lbf, \boden_n(x) \} \bigr) \label{eq:KIprederr}\\  
	%\end{align}
	%where $\decke_n(x) := \min_{i=1,\ldots,N_n}  \tilde f_i + L \metric(x,s_i)+\varepsilon(x)$
	%is called the ceiling 
	%and $\boden_n(x) := \max_{i=1,\ldots,N_n}   \tilde f_i - L \metric(x,s_i) - \varepsilon(x)$
	%is called the floor function.
	%Function $\predf$ is the predictor that is to be utilised for predicting/inferring function values at unseen inputs. Function $\prederr_n(x)$ quantifies the uncertainty of prediction $\predf_n(x)$. Note, if the data set is fixed (i.e. $n$ does not change), we will drop the subscript $n$ and write $\predf$ instead of $\predf_n$.
%\end{defn}
 

\begin{figure}
        \centering
				  \subfigure[KI prediction with two examples.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/ex1Hfe1}
    \label{fig:hfe1}
  } 	
	 \subfigure[Refined prediction.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/ex1Hfe2}
    \label{fig:hfe2}
  } 	%\hspace{2cm}
	  \subfigure[Using a periodic pseudo-metric.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/ex1hfeperiodic}
    \label{fig:hfe3}
  } 	
	\label{fig:exHfe1}
   \caption{Examples of different target functions and kinky inference predictions. \textbf{Fig. \ref{fig:hfe1} and  Fig. \ref{fig:hfe2}} show kinky inference performed for two and ten noisy sample inputs of target function $x\mapsto \sqrt{\abs{\sin(x)}}$ with observational noise level $\varepsilon =0.3$. The floor function folds in knowledge of the non-negativity of the target function ($\lbf = 0$). The predictions were made based on the assumption of $p=\frac 1 2$ and $L = 1$ and employed the standard metric $\metric(x,x') = \abs{x-x'}$. 
	As can be seen from the plots, the function is well interpolated between sample points with small perturbations. Furthermore, the target is within the bounds predicted by the ceiling and floor functions.
		\textbf{Fig. \ref{fig:hfe3}} depicts a prediction folding in knowledge about the periodicity of the target function by choosing a pseudo-metric $\metric = \abs{\sin(\pi \abs{x-x'}}$. The prediction performs well even in unexplored parts of input space taking advantage of the periodicity of the target. This time, there was no observational error and the target function was $x \mapsto \abs{\sin(\pi x)} +1$.   }
\end{figure}	  


\begin{figure}
        \centering
				  \subfigure[Wingrock dynamics.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5.5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/hfewingrock_dyn_groundtruth}
    \label{fig:hfe5}
  } 	
	 \subfigure[Inferred model.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5.5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/hfewingrock_dyn_learned}
    \label{fig:hfe4}
  } 	%\hspace{2cm}
	 
	\label{fig:exHfe2}
   \caption{Example of a prediction on two-dimensional input space. \textit{Left plot:} the target function being a patch of the wingrock dynamics we will study in Sec. \ref{sec:KIMRAC}. \textit{Right plot:} the predictions inferred on the basis of 100 sample points using metric $\metric(x,x') = \norm{x-x'}_\infty$ and $p=1, L=2.5$.}
\end{figure}	 

To develop a first feel for the kinky inference rule, we have plotted some examples of ground-truth functions and pertaining predictions in Fig. \ref{fig:exHfe1} and Fig. \ref{fig:exHfe2}. Firtsly, we can see that the predictions involve kinks. This is due to the minimisations and maximisations that occur in the computation of the ceiling and floor functions which introduce breakpoints into the prediction signal. This property provided the motivation behind the term ``kinky inference''.

Secondly, we note that in all the examples, the conservative bounds were accurate. That is, the target remained contained in them. An example where we see how the bounds shrink to the ground-truth (up to the observational error tube) will be given in Sec. \ref{sec:agentexploration}, where we consider an agent that utilises kinky inference guide online exploration of a drift vector field it is immersed in. Also, note from Fig. \ref{fig:hfe3} how the choice of distance metric $\metric$ can affect the predictions. In this example, a metric was chosen that took into account an a priori known periodicity of the target function. This allowed predictions to be accurate even in unexplored parts of the input space. 


\subsection{Theoretical guarantees}

The examples depicted in the plots of Fig. Fig. \ref{fig:exHfe1} and Fig. \ref{fig:exHfe2} seem to be consistent with what we would expect from an inference mechanism that is conservative and monotonically convergent and hence, might satisfy the desiderata we stated in Def. \ref{def:KIdesiderata}.
We will now give conditions under which we can guarantee the desiderata to be fulfilled. 

Of course, the validity of our guarantees depends on the a priori hypothesis space $\mathcal K_{prior}$. Inspection of the prediction rules reveals that the minimisation and maximisations eliminate all those function values that do not adhere to boundedness conditions (imposed by $\lbf,\ubf$) and conditions on H\"older continuity up to a margin of error given by $\varepsilon$. 

That this is exactly the right class for which our desiderata hold is asserted by the following theorem:

\begin{thm}
Assuming all (a priori) possible targets $f: \inspace \to \outspace$ are given by the class   
\begin{equation}
	\mathcal K_{prior}= \bigl\{ f : \inspace \to \outspace \subseteq \Real^m |  f \in \hoelset L {\metric_{\inspace}} p \cap \bfset \bigr\}
\end{equation}
where $\hoelset L {\metric_\inspace} p =\{ f: \inspace \to \outspace \subset \Real^m \, |  \forall j \in \{1,\ldots,m\} \forall x,x' \in \inspace: \abs{f_j(x) - f_j(x')}  \leq \, L_j \metric^p_{\inspace}{x,x'} \}$ denotes the class of $L-p$- H\"older continuous functions with respect to metric $\metric_\inspace$ and $\bfset :=\{\phi : \inspace \to \outspace \, | \, \forall x \in \inspace, j \in\{1,...,m\}: \lbf_j(x) \leq \phi_j(x) \leq \ubf_j(x)\} $
is the set of all functions bounded component-wise between functions $\lbf,\ubf: \Real^d \to \Real \cup \{- \infty, \infty\}$, where we will always define $\Real_\infty := \Real \cup\{- \infty, \infty\}$. 

Then we have:
%\begin{enumerate}
%\item  
The inference rule as per Def. \ref{def:KIL} is conservative, monotonically convergent (in the limit of dense sample grids) and optimal in the sense of Def. \ref{def:KIdesiderata}. That is, it satisfies Desiderata 1-4 as per Def. \ref{def:KIdesiderata}. 
%\item If in addition, for some $d \in \nat$, $\inspace \subseteq \Real^d$ and $\metric_{\inspace}(x,x') = \norm{x-x'}$ for some norm that is equivalent to the Euclidean norm then the inference mechanism is also optimal (as per Desideratum 4 in Def. \ref{def:KIdesiderata}). That is, there is no other conservative inference mechanism with tighter uncertainty bound function $\prederrn$. 
%\end{enumerate}
\end{thm}

\jcom{Requirements on the regularity of the bound functions or error ? Can metric be a pseudo-metric?}

   %
%%=================
%
%Let  $\Real_\infty := \Real \cup\{- \infty, \infty\}$.
%In this work, we are primarily interested in functions restricted to be H\"older continuous and bounded component-wise by some known bounding functions $\lbf,\ubf: \inspace \to \Real^m_\infty$. That is, we know the target is contained in 
%the a priori hypothesis space 
%\begin{equation}
	%\mathcal K_{prior}= \bigl\{ f : \inspace \to \Real |  f \in \hoelset L \metric p \cap \bfset \bigr\}
%\end{equation}
%where $\hoelset L \metric p =\{ f: \inspace \to \outspace \subset \Real^m \, |  \norm{}  \leq \, \}$ denotes the class of $L-p$- H\"older continuous functions with respect to metric $\metric$ and $\bfset :=\{\phi : \inspace \to \outspace \, | \, \forall x \in \inspace, j \in\{1,...,m\}: \lbf_j(x) \leq \phi_j(x) \leq \ubf_j(x)\} $
%is the set of all functions bounded component-wise between functions $\lbf,\ubf: \Real^d \to \Real \cup \{- \infty, \infty\}$, where we will always define $\Real_\infty := \Real \cup\{- \infty, \infty\}$. 
%
%For ease of notation, we will follow the convention of dropping the subscripts from inequalities and interpret an inequality component-wise. Similarly, if we omit the argument from an inequality, the inequality is understood to hold point-wise. For example, we interpret write $\lbf \leq \phi$ in place of  $\lbf_j(x) \leq \phi_j(x), \forall j,x$. 

%
%%=======================
%
%
%
%From the perspective of topological inference, the estimator is a data inference method for function estimation:
%$B(\cdot) = M_{dinf}(\mathcal K_{prior}, \cdot, \gamma ) $ (for some sufficiently large $\gamma$) mapping sample data into a hypothesis interval of possible functions. 
%Here,  our a priori knowledge is that we know, for each $J \subset I$ a nonnegative number $L_J$ such that the underlying target $f \in \mathcal F$ is $L_J-$Lipschitz on $J$. That is, $\mathcal K_{prior} =\{\phi \in \mathcal F \,|\, \forall J \subset I \forall x,x' \in J: \abs(\phi(x) - \phi(x') \leq L_J \norm{x-x'}\}$. 
%Note, we will not explicitly compute the prior knowledge set. Instead will will fold in the Lipschitz continuity to define an optimal estimator output defining an enclosure that coincides with $\mathcal K_{post} \subset \mathcal K_{prior}$ and hence, with $T (\mathcal K_{post} \cup \mathcal K_{prior})$. Therefore, the resulting inference mechanism will be optimal. 
%
%Note, how our desiderata are equivalent to the corresponding notions of the inference mechanism that is to be designed.


%Our method will 
%
%We 
%
%In this section, we consider hypotheses spaces comprising H\"older continuous functions. This is a very general class of uniformly continuous functions that can exhibit infinitely many non-differentiable points.
%
%
 %In addition to a learned hypothesis, our approach also provides error bounds around each prediction that are conservative and tight. That is, the bounds contain exactly those function within the assumed H\"older class that could have generated the sample. 
%
%The rest of the section is structured as follows: First we 
%
%
%
%Our conservative function estimation procedure constructed upper and lower bound functions that are sample-consistent. The enclosure encompasses all possible candidate functions with an assumed H\"older property that could have generated the sample. Therefore, any H\"older function within the enclosure is a valid hypothesis that could have generated the data and hence, a suitable generalisation model for non-deductive inference. 
%
%
%We assume the dimensionality of the target's input domain $I$ is $d \geq 1$. Remember, we defined the set of function samples as $D_n= \{\bigl( s_i, \tilde f_i, \varepsilon(s_i) \bigr) \vert i=1,\ldots, N_n \} $. 
%Let $f$ denote the \textit{ground-truth} or \textit{target function} generating the data. $\tilde f_i$ is a noisy function value such that we know $f(s_i) \in [\tilde f_i - \varepsilon(s_i) , \tilde f_i + \varepsilon(s_i)], \, (i=1,\ldots, N_n)$. To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the grid of sample inputs contained in $D_n$.  
%
%A robust learning rule is a computable mapping $\mathcal R : D_n \mapsto (\predf, \prederr) $.
%Here $\predf$ is a predictor allowing to make inductive inference and $\prederr(x)$ quantifies the uncertainty or error around the prediction $\predf(x)$.
%
 %
%Let $\metric: \inspace^2 \to \Real$ be a metric. We propose the following nonparametric inference rule:

\subsubsection{Computational effort}
We can see from Def. \ref{def:KIL} that the kinky inference method can be classified as supervised nonparametric learnign method. That is, the computational effort for predicting grows with the data set size $N_n$. if the computational effort 
As we observed, our posterior knowledge coincides with the set of sample-consistent functions. Therefore, an exhaustive enclosure is a superset of $\mathcal K_{prior} \cap \mathcal K_{post}$. 
Remembering our discussion of topological inference, we realize that all we need to do to solve Task 1 (as specified in Sec. \ref{sec:problemdef_task1}), is to define our data inference mechanism $M_{\text{dinf}}$ to output the optimal enclosure. A prescription for computing the bounds of this enclosure was given in the definitions above. From these, we see that $M_{dinf}$computes in $\mathcal O(N_n d)$. That is, its computational effort (translating to the necessary inference investment $\gamma_{\text{dinf}})$ is linear in the number of samples and dimensionality of the problem. If we deal with maximum-norms, norm computation can be done with computation that is logarithmic in $d$, reducing the overall evaluation effort to $\mathcal O(N_n \log d)$.   


\subsection{Uncertain inputs}
So far, we have considered output uncertainty but not input uncertainty. That is, we have assumed that the function values $f(s_i)$ at the sample inputs are uncertain but that query inputs $x$ as well as sample 
inputs $s_i$ are known exactly. However, in many applications this assumption typically is not met. For example, in Sec. \ref{sec:hfeandcontrol}, we will employ KI for learning and controlling dynamic systems.
Here the target functions are state-dependent vector fields. Here, the state measurements often are corrupted by noise (e.g. due to sensor noise or delays) which can affect both training data and query inputs. In the latter case, the controller might query the KI method to predict 
a function value $f(x)$ based on the assumption of the plant being in state $x$ while the plant may in fact be in a different state $x+\delta_x$. We would like to guarantee that our inference over the actual function value still is within the predicted bounds provided the input estimation error $\delta_x$ is known or known to be bounded. 
Fortunately, the H\"older property of the target can help to address the problem of quantifying the prediction error due to input uncertainty. For instance, consider $\metric(x,x') = \norm{x-x'}$. By H\"older continuity, the error of the prediction of the $j$th output component of the target function $f$ is bounded by $\abs{f_j(x+\delta_x) - f_j(x)} \leq \norm{\delta_x}^p$. 

Firstly, assume the training inputs $s_i$ were certain but the query inputs $x$ during prediction are uncertain.
Therefore, assuming we know an upper bound $\errinp(x)$ on the input uncertainty, i.e. 
$\norm{\delta_x}^p\leq \errinp(x), \forall x$, we can add this term to the uncertainty estimate $\prederr(x)$ as per Eq. \ref{eq:KIprederr}.
%This means that the additional uncertainty only affects the prediction of this particular query point. 

Secondly, consider the case were both the sample inputs $s_i$ and the query inputs $x$ are uncertain with the uncertainty being quantified by the function $\errinp(\cdot)$. In this case, the uncertainty of the sample inputs
 affects the posterior hypothesis (i.e. the enclosure) over $f$ and hence, the inferences over function values at all inputs. 
By the same argument as for the case of query noise, we accommodate for this fact leveraging the H\"older property. Instead of adjusting just the prediction uncertainty, we simply add $ \errinp(\cdot)$ to the error function $\varepsilon(\cdot)$ (that we have previously used to model observational 
error of the function values). That is, the observational uncertainty model has absorbed the input uncertainty.  Predictions with uncertain query points can then be done as per Def. \ref{def:KIL} (but with $ \errinp(\cdot)+ \varepsilon(\cdot)$ in place of $\varepsilon(\cdot) )$.


\subsection{A smoothed inference rule}
The inferred predictor of kinky inference as per Eq. \ref{eq:KIpred} contains kinks. In certain applications, this can be disadvantageous. For instance, below we will feed the predictor into a control signal. However, in applications such as robotics, smooth controllers are typically preferred over non-smooth ones since the latter give rise to increase wear and tear of the actuators. With this in mind we will consider a modification of the predictor which filters the prediction output through a smoothing filter as known from computer vision \cite{forsyth2002,Baessmann2004}. The idea is to convolve the predictor with a function that implements a low-pass filter and abates high frequencies thereby smoothing out rough transitions in the signal. 
For now, we will restrict the exposition to one-dimensional input and output spaces. However, extensions to multiple output dimensions result from following our steps through for each component function. Extensions to multi-dimensional input spaces follow by taking advantage of the corresponding filters developed for multi-dimensional signal processing. \jcom{citation??}

As an example, for some $n \in \nat, w_i \geq 0 \, (i=1,...,n)$ and (typically small) step size $\sigma \in \Real \geq 0$, consider the modified prediction rule (\textit{smoothed kinky inference (SKI) rule} ):
%
\begin{equation}
f_n^* = \frac{1}{\sum_{i=-q}^N w_i} 
\sum_{i=-q}^q w_i \predfn (x+i \sigma) .
\end{equation}
%
Either $w_0 = 2$ and $w_{-1}=w_2 =1$ or $w_{-2} = w_2 = 7, w_{-1}=w_1 = 26, w_0 = 41$  are common choices for discrete approximations of a Gaussian filter \cite{Baessmann2004}. 

A comparison of the predictions made by the {smoothed kinky inference rule (SKI) } against the prediction function of the standard kinky inference rule is depicted in Fig. \ref{fig:exSKI1}. As can be seen from the plot, in the SKI prediction the kinks have been smoothed out while all the predictions still are contained within the prediction bounds given by the floor and ceiling functions.

\begin{figure}
        \centering
				  %\subfigure[KI on a sine curve.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/sinKI1}
    %\label{fig:KI1}
  %} 	
	 \subfigure[KI on a sine curve.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/sinKI3}
    \label{fig:KI2}
  } 	%\hspace{2cm}
	  %\subfigure[SKI on a sine curve.]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/sinSKI1}
    %\label{fig:SKI1}
		%} 	
	  \subfigure[SKI on a sine curve.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/sinSKI3}
    \label{fig:SKI2}
  } 	
	\label{fig:exSKI1}
   \caption{Comparison of the kinky inference rule (left) versus the smoothed kinky inference rule (right) when predicting a sine curve. Note the smoothed out kinks of the SKI rule due to the application of the smoothing filter. Here we chose $\sigma =0.1$. }
\end{figure}	  

The question arises how the smoothing might affect the theoretical properties. Firstly, we note that due to Lem. \ref{lem:Hoeldarithmetic}, the H\"older exponent and constant remain unchanged. For the constant, we apply point 4 and 9 of the lemma to see that $L(f_n^*) = \frac{1}{\sum_{i=-q}^q w_i} 
\sum_{i=-q}^q w_i L(\predfn (x+i \sigma)) = L(\predf_n) \frac{1}{\sum_{i=-q}^q w_i}  \sum_{i=-q}^q w_i = L(\predfn)$.

Secondly, the question arises what the maximal discrepancy between $\predfn$ and $f_n^*$ is. This question is of importance to make sure that the prediction still is within the bounds given by the ceiling and floor functions. To this end, assume $\predfn$ is H\"older with constant $L$ and exponent $p$. For simplicity assume we utilise Gaussian smoothing for the SKI rule with $n=1$, $w_{-1} = w_1 =1, w_0 =2$. The deviation of this resulting smoothed predictor $f_n^* $ from the kinky inference predictor $\predfn$ can be bounded as follows
\begin{align}
\abs{\predfn(x) - f_n^*(x)} &= \abs{\predfn(x) - \frac 1 {w_{-1}+w_0+w_1} (w_{-1} \predfn(x-\sigma) + w_0 \predfn(x)+ w_1 \predfn(x+\sigma)} \\
&= \frac 1 4 \abs{ 2 \predfn(x) -   \predfn(x-\sigma) - \predfn(x+\sigma)} \\
&\leq  \frac 1 4 \abs{  \predfn(x) -   \predfn(x-\sigma)} + \frac 1 4 \abs{ \predfn(x) - \predfn(x+\sigma)}\\
&\leq  \frac L 4  \metric^p(x,x-\sigma) + \frac L 4 \metric^p(x,x+\sigma).
\end{align}
If the metric is induced by a norm $\norm{\cdot}$ the last expression simplifies to $\frac L 2 \norm{\sigma}^p$.
In order to ensure that uncertainty bound takes this error into account, we can add this error to the uncertainty quantification $\prederrn$. This guarantees that the ground truth is contained within the error bounds around predictions $f_n^*$. Furthermore, since the bound is valid for all inputs $x \in \inspace$, the convergence guarantees established for kinky inference also hold for the smoothed version up to supremum norm error of at most $\sup_{x \in \inspace} \frac L 4  \metric^p(x,x-\sigma) + \frac L 4 \metric^p(x,x+\sigma)$ (which in the norm-based metric case equals $\frac L 2 \norm{\sigma}^p$).

%\subsection{Discussion}
%As we observed, our posterior knowledge coincides with the set of sample-consistent functions. Therefore, an exhaustive enclosure is a superset of $\mathcal K_{prior} \cap \mathcal K_{post}$. 
%Remembering our discussion of topological inference, we realize that all we need to do to solve Task 1 (as specified in Sec. \ref{sec:problemdef_task1}), is to define our data inference mechanism $M_{\text{dinf}}$ to output the optimal enclosure. A prescription for computing the bounds of this enclosure was given in the definitions above. From these, we see that $M_{dinf}$computes in $\mathcal O(N_n d)$. That is, its computational effort (translating to the necessary inference investment $\gamma_{\text{dinf}})$ is linear in the number of samples and dimensionality of the problem. If we deal with maximum-norms, norm computation can be done with computation that is logarithmic in $d$, reducing the overall evaluation effort to $\mathcal O(N_n \log d)$.   