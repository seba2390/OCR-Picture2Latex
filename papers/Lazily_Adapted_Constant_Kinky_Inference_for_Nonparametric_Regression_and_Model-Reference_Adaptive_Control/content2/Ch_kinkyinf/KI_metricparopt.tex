\section{Parameter optimisation}
In order to infer function values, the KI rules had to be given a number of parameters that were assumed to be specified a priori. These parameter include the chosen H\"older number sequences $L(n)$ as well as the distance metric $\metric_\inspace$.

However, these design parameters determine much of the predictive performance of the resulting inferences. For example, in Fig. \ref{fig:exHfe1} above, we have illustrated how knowing a periodic target function's frequency allows one to predict well even in parts of input space where no sample inputs exist. 
In the given example we considered target function $f(x)= \abs{\sin(\pi \param x)} +1$ having a frequency $\param =1$. Here, the choice of pseudo-metric $\metric_\inspace(x,x';\hat \param) = \abs{\sin(\pi \hat \param \abs{x-x'})} $ with $\hat \param= \param$ proved appropriate for predicting with the right periodicity.

Obviously, if one presupposes a false frequency $\hat \param \neq \param$ then the prediction performance can suffer dramatically. For instance, consider the plot 

\begin{figure}
        \centering
				  \subfigure[KI prediction with two examples.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/ex1hfe1.pdf}
    \label{fig:metricparopt1}
  } 	
	 \subfigure[Refined prediction.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/ex1hfe2.pdf}
    \label{fig:metricparopt2}
  } 	%\hspace{2cm}
	  \subfigure[Using a periodic pseudo-metric.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/ex1hfeperiodic.pdf}
    \label{fig:metricparopt3}
  } 	
   \caption{Lalala }
			\label{fig:metricparopt}
\end{figure}	 

....

In the absence of accurate knowledge of the suitable parameter $\param$ of the pseudo-metric, it would be helpful to estimate it from the data. To this end we will modify standard ideas from 
machine learning. In particular, we will optimise the parameter based on the model assumption that the data was drawn independently from a Laplacian distribution with the mode coinciding with the posterior prediction. That is, a function value $f_s \in \Real$ at input $x \in I$ is assumed to have density $p(f_x| x,\data,\param) = \frac{1}{2\lambda}\exp(-\frac{\abs{f_x - \predfn(x;\param)}}{\lambda})$ for some parameter $\lambda$.
Given some set of \emph{objective function evaluation data} $\data = \bigl\{ ( s_i, f(s_i) ) | \, i=1,\dots, N \bigr\}$ we will determine the estimate by maximising the conditional likelihood  
$p\bigl(f(s_1),\ldots,f(s_N)  | s_1,...,s_N,\data \bigr) = \frac{1}{2\lambda}\exp \Bigl(-\prod_{i=1}^N\frac{\abs{f(s_i) - \predfn(s_i;\param)}}{\lambda} \Bigr) $.
This can be done be minimising the negative log-likelihood. Since multiplication with positive factors does not change the minimiser, we thus obtain the parameter estimate $\hat \param$ as  
\begin{equation}
\hat \param = \argmin_\param -\frac{\lambda}{N}\log p\bigl(f(s_1),\ldots,f(s_N)  | s_1,...,s_N,\data_n \bigr) = \argmin_\param \sum_{i=1}^N\frac{\abs{f(s_i) - \predfn(s_i;\param)}}{N}.
\end{equation}

The H\"older function of the objective function is just less than or equal to the maximum H\"older constant of the functions $\varphi_1,...,\varphi_N$ where 
 $\varphi_i (\param) = \predfn(s_i;\param)$.
 
 The determination of the H\"older constant thus depends on the definition of the parameter within the definition of the predictor. 
 
\subsection{$\param = p$}
By Eq. \ref{eq:hru582jsokbbbn},
\begin{align}
\predfn(x) &= \frac 1 2 \bigl( \tilde f_j + L \metric^p(s_j,x)   \bigr) + \frac 1 2 \bigl( \tilde  f_k - L \metric^p(s_k,x) \bigr)
\end{align}
for some $k,j \in \{1,...,N\}$ being the indices of the sample inputs minimising and maximising the ceiling and floor functions, respectively.

 Hence, by application of Lem. \ref{lem:Hoeldarithmetic}.8) we can examine the absolute values of the derivatives (in $p$), showing that  $L(\varphi_i) \leq\sup_{p \in (0,1)} (\abs{ \frac 1 2  \log(\metric(s_k,s_i) ) \, \metric^p(s_k,s_i)  }+ \abs{\frac 1 2 \log(\metric(s_j,s_i) ) \,  \metric^p(s_j,s_i)})$. 
 
\subsection{Learning a good Lipschitz constant and observational error bound.}
 
 The choice of H\"older constant can greatly influence predictive performance. Chosen conservatively large, it will exactly interpolate all data points. However, in the presence of noise this might cause poorer predictive performance on a test set. 
Generally, the presence of noise introduces large (and in theory, unbounded) derivatives. Therefore, any empirical estimates of H\"older constant can grow unbounded in the presence of noise. Consequently, the resulting predictor will behave akin to a nearest neighbour regression approach and hence, overfit to the noise. An example of this is depicted in Fig. \ref{fig:LACKInoise0}
\begin{figure}
        \centering
        			  \subfigure[Prediction with empirical estimate of $L(n)$.]{        \includegraphics[width = 4.1cm]
								{content/figs/LACKInoise.pdf}
    \label{fig:LACKInoise00}
  }  
			  \subfigure[Prediction with $L(n)$ minimising test error.]{
        \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/Loptnoise.pdf}
    \label{fig:LACKInoise01}
  }    
  %
  \caption{Comparison of predictions based on noise sample with parameter $L(n)$ chosen in two different manners. Left: $L(n)$ chosen to coincide with the empirical estimate of the Lipschitz constant computed based on the entire sample (light blue and green dots). We can see that the resulting prediction (black curve) overfits to the noisy signal (magenta curve) resulting in a poor alignment with the target (blue curve). Right: $L(n)$ chosen to minimise empirical prediction error on test set (light blue dots). We can see that the resulting prediction smoothes out the noise and is a much better fit to the target.}
			\label{fig:LACKInoise0}
\end{figure}	


Instead relying on empirical estimates, we will therefore follow a different approach. That is, we will determine the $L(n)$ based on their predicted performance assessed based on the available data. As we will see this approach will often result in automated smoothing and avoid .


Therefore, we choose to estimate the H\"older constant parameter $\param = L(n)$ on based on the predictive performance of the predictor $\predfn(\cdot;\param)$ on the evaluation data.
Once this is done, we then estimate the noise levels by setting them just large enough so that they can account for the discrepancy between the predictor and all the available data.
 
 \subsection{Learning the periodicity of the data.}
 
 
 
 \subsection{Automated Relevance determination (ARD) }
 Assume $p=1$.
 
 \subsubsection{One-norm}
 Let $\param =  (\param_1,\ldots,\param_d) \geq 0$ and define a weighted metric as per $\metric(x,x';\param) = \sum_{i=1}^d \param_i \abs{x_i-x_i'} = \abs{x-x'}^\top \param$. The weight $\param_i$ quantifies the degree to which the $i$th input component should contribute to the prediction. Large weights suggest a strong degree of importance: i.e. small deviations of a query $x_i$ from the closest example $s_{j,i}$ in the $i$th component will result in large uncertainty of the prediction. Conversely, if $\param_i=0$ effectively disables any influence of the $i$th input dimension in the prediction process.
 
Owing to this interpretation and in keeping with analogous definitions in the kernel literature, inferring $\param$ from the data will be referred to as \emph{automated relevance determination (ARD)}. 

Our chosen metric is differentiable with respect to parameter $\param$. Therefore, we can appeal to Lem. \ref{lem:Hoeldarithmetic}.8) to show that as a function of $\param$ a Lipschitz number of $\param \mapsto \metric(s_i,s_j;\param)$ is bounded from above by 
$\norm{s_i-s_j} _1$. By definition of the predictor we have \begin{align}
\predfn(x;\param) &= \frac 1 2 \bigl( \tilde f_j + L(n) \metric^p(s_j,x;\param)   \bigr) + \frac 1 2 \bigl( \tilde  f_k - L(n) \metric^p(s_k,x;\param) \bigr)
\end{align}
for some $k,j \in\{1,\ldots,N_n\}$.

Once again appealing to Lem. \ref{lem:Hoeldarithmetic}.8), we see that 
$L(\varphi_i) \leq \frac {L(n)} 2 \sup_{i,j} \norm{s_i-s_j} _1+ \frac {L(n)} 2 \sup_{i,j} \norm{s_i-s_j} _1 = {L(n)} \max_{i,j=1,...,N_n} \norm{s_i-s_j} _1$.

 \subsubsection{Maximum-norm}
 Let $\param =  (\param_1,\ldots,\param_d) \geq 0$ and define a weighted metric as per $\metric(x,x';\param) = \max_{i=1,\ldots,d} \param_i \abs{x_i-x_i'}$. The weight $\param_i$ quantifies the degree to which the $i$th input component should contribute to the prediction. Large weights suggest a strong degree of importance: i.e. small deviations of a query $x_i$ from the closest example $s_{j,i}$ in the $i$th component will result in large uncertainty of the prediction. Conversely, if $\param_i=0$ effectively disables any influence of the $i$th input dimension in the prediction process.
 
Owing to this interpretation and in keeping with analogous definitions in the kernel literature, inferring $\param$ from the data will be referred to as \emph{automated relevance determination (ARD)}. 

We can appeal to Lem. \ref{lem:Hoeldarithmetic} to show that as a function of $\param$ a Lipschitz number of $\param \mapsto \metric(s_i,s_j;\param)$ is bounded from above by 
$\norm{s_i-s_j}_\infty$. By definition of the predictor we have \begin{align}
\predfn(x;\param) &= \frac 1 2 \bigl( \tilde f_j + L(n) \metric^p(s_j,x;\param)   \bigr) + \frac 1 2 \bigl( \tilde  f_k - L(n) \metric^p(s_k,x;\param) \bigr)
\end{align}
for some $k,j \in\{1,\ldots,N_n\}$.

Once again appealing to Lem. \ref{lem:Hoeldarithmetic}.8), we see that 
$L(\varphi_i) \leq \frac {L(n)} 2 \sup_{i,j} \norm{s_i-s_j} _\infty+ \frac {L(n)} 2 \sup_{i,j} \norm{s_i-s_j}_\infty = {L(n)} \max_{i,j=1,...,N_n} \norm{s_i-s_j}_\infty$.

\subsubsection{Example -- Pendulum dynamics}
As a first example, we consider learning the dynamics of a friction-less pendulum based on a sample. Denoting the angle of a pendulum by $q$,  its dynamics are given by the ordinary differential equation $\ddot q = f(x)$ where [m;b;l;g] 
$x = [q;\dot q]$ is called the state and 
$f(x) = - 9.81 \sin(x_1)$. Here, we learn the dynamics by learning the function $f$ based on a sample of triplets of angles, velocities and accelerations. We can see that this target function only depends on the first component of its input, namely the angle. So, if we incorporate this in our KI method by choosing the metric $\metric(x,x';\param) = \max\{\param_1 \abs{x_1-x_1'},\param_2\abs{x_2-x_2'}$ with $\param =[1; 0]$ we should expect superior predictive power over using the standard maximum norm (where $\param =[1; 1]$). 
Therefore, we expect ARD method to automatically uncover this advantageous parameter. As an 
example affirming this intuition, refer to Fig. \ref{fig:LACKIARD_1pend}.
The example of Fig. \ref{fig:LACKIARD_1pend_target} - Fig. \ref{fig:LACKIARD_1pend_pred1} illustrate how the ARD method can uncover the fact that the target function only depends on one of the input components and how this can affect the predictive performance. Since the success of ARD depends on the composition of the data sets we, assessed its potency across a range of 1000 different randomised training data sets $\data_1,\ldots,\data_{1000}$.  We recorded the average prediction errors $\sum_{i=1}^M \frac{\abs{f(s_i) - \predf_j(s_i) }}{M}$ of each of the resulting predictors $\predf_j$ $(j=1,\ldots,1000)$ on a test set of size $M=1000$. The resulting box plots are depicted in Fig. \ref{fig:LACKIARD_1pend_bp}. 
The plots show that on the vast majority of trials, the ARD approach succeeded in uncovering the fact that only the first input component impacts target function and to take advantage in terms of improved prediction accuracy.  




\begin{figure}
        \centering
        				  \subfigure[Target function.]{
    \includegraphics[width = 6.2cm,trim = 19mm 15mm 19mm 19mm, clip]
								{content/figs/ARD1pend_target.pdf}
    \label{fig:LACKIARD_1pend_target}
  } 	
  				  \subfigure[Prediction without ARD.]{
    \includegraphics[width = 6.2cm,trim = 19mm 15mm 19mm 19mm, clip]
								{content/figs/ARD1pend_pred0.pdf}
    \label{fig:LACKIARD_1pend_pred0}
  } 
    				  \subfigure[Prediction with ARD.]{
    \includegraphics[width = 6.2cm,trim = 19mm 15mm 19mm 19mm, clip]
								{content/figs/ARD1pend_pred1.pdf}
    \label{fig:LACKIARD_1pend_pred1}
  } 
      				  \subfigure[Comparison over 1000 rnd trials.]{
    \includegraphics[width = 5.5cm]
								{content/figs/ARD1pend_bp.pdf}
    \label{fig:LACKIARD_1pend_bp}
  } 
%				  \subfigure[Target function.]{
%    \includegraphics[width = 6.2cm,trim = 1mm 1mm 1mm 1mm, clip]
%								{content/figs/ARD1pend_target_2000.pdf}
%    \label{fig:LACKIARD_1pend_target}
%  } 	
%  				  \subfigure[Prediction without ARD.]{
%    \includegraphics[width = 6.2cm,trim = 1mm 1mm 1mm 1mm, clip]
%								{content/figs/ARD1pend_pred0_2000.pdf}
%    \label{fig:LACKIARD_1pend_pred0}
%  } 
%    				  \subfigure[Prediction with ARD.]{
%    \includegraphics[width = 6.2cm,trim = 1mm 1mm 1mm 1mm, clip]
%								{content/figs/ARD1pend_pred1_2000.pdf}
%    \label{fig:LACKIARD_1pend_pred1}
%  } 
%      				  \subfigure[Comparison over 1000 rnd trials.]{
%    \includegraphics[width = 5.5cm]
%								{content/figs/ARD1pend_bp_2000.pdf}
%    \label{fig:LACKIARD_1pend_bp}
%  } 
   \caption{LACKI with automated relevance determination.  Fig. \ref{fig:LACKIARD_1pend_target} depicts the target on a test set of M=1000 sample points (plotted in blue). Fig. \ref{fig:LACKIARD_1pend_pred0} depicts the prediction of LACKI based on the maximum-norm metric and a sample $\data$ containing 14 data points (cyan) of triples of angles, angular velocities and accelerations. Fig. \ref{fig:LACKIARD_1pend_pred1} depicts the corresponding prediction $\predf$ but where the ARD-metric was employed whose parameters were automatically determined as the maximiser of predictiveness. To this end, the available data was partitioned into objective function evaluation (plotted in magenta) and conditioning data (plotted in green) as described above. In the depicted example the parameters were determined to be $\param_1 = 0.99$ and $\param_2 = 0.02$ accurately reflecting the dominance of the importance of the angle over the velocity to predict accelerations.
Fig. \ref{fig:LACKIARD_1pend_bp} depicts prediction errors and determined parameters across a range of 1000 randomised trials. 
The first and second boxplots pertain to the average prediction errors of LACKI without and with ARD, respectively.} The last two box plots depict the recorded automatically determined parameter components $\param_1$ and $\param_2$. 			\label{fig:LACKIARD_1pend}
\end{figure}	




