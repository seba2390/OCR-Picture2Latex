\subsection{Kinky inference and model reference adaptive control for online learning and tracking control in the presence of wing rock dynamics }
\label{sec:KIMRAC}
As pointed out in \cite{chowdharyacc2013}, modern fighter aircraft designs are susceptible to lightly damped oscillations in roll known as ``wing rock''. Commonly occurring during landing \cite{Saad2000}, removing wing rock from the dynamics is crucial for precision control of such aircraft.
Precision tracking control in the presence of wing rock is a nonlinear problem of practical importance and has served as a test bed for a number nonlinear adaptive control methods \cite{Chowdhary2013,Monahemi1996,chowdharyacc2013}.

For comparison, we replicate the experiments of the recent work of Chowdhary et. al. \cite{Chowdhary2013,ChowdharyCDC2013}.\footnote{We are grateful to the authors for kindly providing the code.}
Here the authors have compared their Gaussian process based approach, called \textit{GP-MRAC}, to the more established adaptive model-reference control approach based on RBF networks \cite{Sanner1992,Kim1998}, referred to as \textit{RBFN-MRAC}. Replacing the Gaussian process learner by our kinky inference learner, we readily obtain an analogous approach which we will refer to as \textit{LACKI-MRAC}. As an additional baseline, we also examine the performance of a simple P-controller.

While with the exact same parameters settings of the experiments in \cite{Chowdhary2013}, performance of our LACKI-MRAC method comes second to GP-MRAC, we also evaluate the performance of all controllers over a range of 555 random parameter settings and initial conditions. As we will see, across this range of problem instances and parameter settings,LACKI-MRAC markedly outperforms all other methods.

\subsubsection{Model reference adaptive control}
Before proceeding with the wing rock application we will commence with (i) outlining model reference adaptive control (MRAC) \cite{astroemadaptivectrlbook2013} as considered in \cite{Chowdhary2013} and (ii) describe the deployment of kinky inference to this framework. 
We will now rehearse the description of MRAC for second-order systems following \cite{Chowdhary2013}. 

Assume $m \in \nat$ to be the dimensionality of a configuration of the system in question and define $d = 2m$ to be the dimensionality of the pertaining state space $\statespace$.

Let $x = [x_1;x_2] \in \statespace$ denote the state of the plant to be controlled.
Given the control-affine system 
 
\begin{align}
\dot x_1 &= x_2 \\
\dot x_2 &= a(x) + b(x) \, u(x) \label{eq:secorddynctrlaff}
\end{align}

it is desired to find a control law $u(x)$ such that the closed-loop dynamics exhibit a desired reference behaviour:

\begin{align}
\dot \xi_1 &= \xi_2 \\
\dot \xi_2 &= f_{r}(\xi,r)
\end{align}
where $r$ is a reference command, $f_r$ some desired response and $t \mapsto \xi (t)$ is the reference trajectory.

If a priori $a$ and $b$ are believed to coincide with $\hat a_0, \hat b_0$ respectively, the inversion control 
$u = \hat b_0^{-1} (- \hat a_0 +u')$ is applied. This reduces the closed-loop dynamics to 
$\dot x_1 = x_2, \dot x_2 = u' + \tilde a(x,u) $
where $\tilde a(x,u)$ captures the modelling error of the dynamics: 
\begin{equation}
	\tilde a (x,u ) = a(x) - \hat a_0(x) + \bigl(b(x) - \hat b_0(x)\bigr) u.
\end{equation}
 Let $I_d \in \Real^{d \times d}$ denote the identity matrix.  If $b$ is perfectly known, then $b - \hat b_0^{-1} = 0$ and the model error can be written as $\tilde a (x)= a(x) - \hat a_0(x)$. In particular, $\tilde a$ has lost its dependence on the control input. 



In this situation \cite{Chowdhary2013,ChowdharyCDC2013} propose to set 
the pseudo control as follows: $u'(x) :=  \nu_{r} + \nu_{pd} - \nu_{ad}$ where $\nu_{r} = f_{r}(\xi,r)$ is a feed-forward reference term,  $\nu_{ad}$ is a yet to be defined output of a learning module \emph{adaptive element} and $\nu_{pd} = [K_1 K_2] e$ is a feedback error term designed to decrease the \textit{tracking error} $e(t) = \xi(t) - x(t)$ by defining $K_1,K_2 \in \Real^{m \times m}$ as described in what is to follow.

Inserting these components, we see that the resulting \textit{error dynamics} are:


\begin{equation}\label{eq:errordynmrac}
	\dot e = \dot \xi - [x_2; \nu_r + \nu_{pd}+ \tilde a(x) ] = M e + B \bigl(\nu_{ad}(x) -  \tilde a(x)\bigr)
\end{equation}


where $M = \left(\begin{array}[h]{cc}
			O_m &  \, I_{m}\\
			-K_1 & -K_2 
					\end{array}\right)$ and $B = \left(\begin{array}[h]{c}
			O_m \\ I_m
					\end{array}\right)$.
If the feedback gain matrices $K_1,K_2$ parametrising $\nu_{pd}$ are chosen such that $M$ is stable then the error dynamics converge to zero as desired, provided the learning error $E_\lambda$ vanishes: $E_\lambda (x(t)) = \norm{\nu_{ad}(x(t)) -  a(x(t))} \stackrel{t \to \infty} {\longrightarrow} 0$. 

It is assumed that the adaptive element is the output of a learning algorithm that is tasked to learn $\tilde a$ online. This is done by continuously feeding it training examples of the form $\bigl(x(t_i), \tilde a(x(t_i)) + \varepsilon_i\bigr)$ where $\varepsilon_i$ is observational noise.  

Intuitively, assuming the learning algorithm is suitable to learn target $\tilde a$ (i.e. $\tilde a$ is close to some element in the hypothesis space \cite{mitchellbook:97} of the learner) and that the controller manages to keep the visited state space bounded, the learning error (as a function of time $t$) should vanish.

Substituting different learning algorithms yields different adaptive controllers. \textit{RBFN-MRAC} \cite{Kim1998} utilises radial basis function neural networks for this purpose whereas \textit{GP-MRAC} 
employs Gaussian process learning \cite{GPbook:2006} to learn $\tilde a$ \cite{Chowdhary2013,ChowdharyCDC2013}. 

%\jcom{possibly the following paragraph will go into a related work section:}
%Note, this setting of GP-MRAC is a special case of the SP-SIIC approach we introduced in Ch. \ref{ch:SPSIIC}. Here $\hat a_0$ is the prior mean function of the s.p. we place over the drift. Our exposition in Sec. \ref{ch:SPSIIC} is more general in that it allows for uncertain $b$ that is learned from data, considers under-actuated systems and makes no distributional restrictions. As an additional feature, GP-MRAC assumes that the data sets can be updated continuously. While of course this is not tractable the authors maintain a data corpus of a fixed size by a range of methods including simple FIFO queues as well as sparsification methods \cite{Kingravi2014,Csato2002}. In contrast, we utilise learning criteria to only include informative data points. In addition, we employ hyper-parameter optimisation which we show to be highly beneficial. While hyper-parameter training cannot be done at high frequency, it would be possible to run it in the background on a separate CPU (in parallel to the CPU on which the controller runs). 

In what is to follow, we utilise our LACKI method as the adaptive element. Following the nomenclature of the previous methods we name the resulting adaptive controller \textit{LACKI-MRAC}.

\subsubsection{The wing rock control problem}
The wing rock dynamics control problem considers an aircraft in flight. Denoting $x_1$ to be the roll attitude (angle of the aircraft wings) and $x_2$ the roll rate (measured in angles per second), the controller can set the aileron control input $u$ to influence the state $x := [x_1;x_2]$.

Based on \cite{Monahemi1996}, Chowdhary et. al. \cite{Chowdhary2013,ChowdharyCDC2013} consider the following model of the wing rock dynamics: 

\begin{align}
\dot x_1 &= x_2 \\
\dot x_2 &= a(x) + b \, u 
\end{align}
where $b =3$ is a known constant and 
$a(x) = W_0^* + W_1^* x_1 + W_2^* x_2 + W_3^* \abs{x_1} x_2 + W_4^* \abs{x_2} x_2 + W^*_5 x_2^3$ is an priori unknown nonlinear drift. 



Note, the drift is non-smooth but, employing Lem. \ref{lem:Hoeldarithmetic}, it would be easy to derive a Lipschitz constant on any bounded subset of state space if the parameters $W := (W_0^*,\ldots, W_5^*)$ were known.

To control the system we employ LACKI as the adaptive element $\nu_{ad}$.
In the absence of the knowledge of a Lipschitz constant, we start with a guess of $\underline L=1$ (which will turn out to be too low) and update it following the procedure described in Sec. \ref{sec:lacki}.

In a first instance, we replicated the experiments conducted in \cite{Chowdhary2013,chowdharyacc2013} with the exact same parameter settings. That is, we chose $W_0^* = 0.8, W_1^* = 0.2314, W_2^* = 0.6918, W_3^* = -0.6245, W_4^* = 0.0095, W_5^* = 0.0214$. 



The simulation initialised with start state $x = (3,6)^\top$ and simulated forward with a first-order Euler approximation with time increment $\tinc = 0.005 [s]$ over a time interval $\indsett = [t_0,t_f]$ with $t_0 = 0[s]$ and $t_f = 50[s]$. Training examples and control signal were continuously updated every $\Delta_u= \Delta_o = \tinc [s]$. The RBF and GP learning algorithms were initialised with fixed length scales of 0.3 units. The GP was given a training example budget of a maximum of 100 training data points to condition the posterior model on. Our kinky inference learner was initialised with $L =p= 1$ and updated online following the lazy update method described in Sec. \ref{sec:lazylipconstupdate}.

The test runs also exemplify the working of the lazy update rule.
The initial guess $\underline L=1$ was too low. However, our lazy update rule successfully picked up on this and had ended up increasing constant to $L=2.6014$ by the end of the online learning process.



%
%
\begin{figure*}
        \centering
				  \subfigure[Tracking error (RBF-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[ width=.3\textwidth, clip, trim = 2.5cm 8cm 3cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wr2RBF}
   % \label{fig:wingrockresultsbp}
  } 
					  \subfigure[Tracking error (GP-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width=.3\textwidth, clip, trim = 2.5cm 8cm 3cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wr2GP}
   % \label{fig:wingrockresultsbp}
  } 
					  \subfigure[Tracking error (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width=.3\textwidth, clip, trim = 2.5cm 8cm 3cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wr2}
   % \label{fig:wingrockresultsbp}
  }
   \caption{Tracking error comparison of first example.}
	 \label{fig:wrtrackerrorsex1}
\end{figure*}	   

The results are plotted in Fig. \ref{fig:wrtrackerrorsex1}. 
We can see that in terms of tracking error of the reference our LACKI-MRAC outperformed RBF-MRAC and was a close runner-up to GP-MRAC which had the lowest tracking errors.  

%
To obtain an impression of the learning performance of the three learning algorithms we also recorded the prediction error histories for this example problem. The results are depicted in Fig. \ref{fig:wrprederrorsex1}. We can see that our kinky inference method and the GP method both succeeded in predicting the drift quite accurately while the RBFN method was somewhat lagging behind.
This is consistent with the observations made in \cite{Chowdhary2013,ChowdharyCDC2013}. The authors explain the relatively poor performance of the radial basis function network method by the fact that the reference trajectory on occasion led outside the region of state space where the centres of the basis function were placed in advance. By contrast, due to the non-parametric nature of the GP, GP-MRAC does not suffer from such a priori limitations. In fact, it can be seen as an RBF method that flexibly places basis functions around all observed data points \cite{GPbook:2006}. We would add that, as a non-parametric method, LACKI-MRAC shares this kind of flexibility, which might explain the fairly similar performance. 

However, being an online method, the authors of GP-MRAC explicitly avoided hyperparameter training via optimising the marginal log-likelihood. The latter is commonly done in GP learning \cite{GPbook:2006} to avoid the impact of an unadjusted prior but is often a computational bottle neck. Therefore, avoiding such hyperparameter optimisation greatly enhances learning and prediction speed in an online setting. However, we would expect the performance of the prediction to be dependent upon the hyperparameter settings. As we have noted above, the Lipschitz constant depends on the part of state space visited at runtime. Similarly, we might expect length scale changes depending on the part of state space the trajectory is in. Unfortunately, \cite{Chowdhary2013,ChowdharyCDC2013,chowdharyacc2013} provide no discussion of the length scale parameter setting and also called the choice of the maximum training corpus size ``arbitrary''. 

Since the point of learning-based and adaptive control is to be able to adapt to various settings, we test the controllers across a range of randomised problem settings, initial conditions and parameter settings.

We created 555 randomised test runs of the wingrock tracking problems and tested each algorithm on each one of them. The initial state $x(t_0)$ was drawn uniformly at random from $[0,7] \times [0,7]$, the initial kernel length scales were drawn uniformly at random from $[0.05,2]$, and used both for RBF-MRAC and GP-MRAC. The initial H\"older constant $\underline L$ for LACKI-MRAC was initialised at random from the same interval but was allowed to be adapted as part of the online learning process. Furthermore, we chose $\hestthresh =0$. The parameter weights $W$ of the system dynamics specified above were multiplied by a constant drawn uniformly at random from the interval $[0,2]$. To allow for better predictive performance of GP-MRAC we doubled the maximal budget to 200 training examples. 
The feedback gains were chosen to be $K_1=K_2=1$. 

In addition to the three adaptive controllers we also tested the performance of a simple $P$ controller with just these feedback gains (i.e. we executed x-MRAC with adaptive element $\nu_{ad}=0$). This served as a baseline comparison to highlight the benefits of the adaptive element over simple feedback control.

The performance of all controllers across these randomised trials is depicted in Fig. \ref{fig:wingrockresultsbp}. Each data point of each boxplot represent a performance measurement for one particular trial.

\begin{figure*}
        \centering
				  \subfigure[Prediction v.s. ground truth  (RBF-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = .3\textwidth, clip, trim = 3cm 9cm 3cm 10cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wr4RBF}
   % \label{fig:wingrockresultsbp}
  } 
					  \subfigure[Prediction v.s. ground truth  (GP-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width=.3\textwidth, clip, trim = 3cm 9cm 3cm 10cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wr4GP}
   % \label{fig:wingrockresultsbp}
  } 
					  \subfigure[Prediction v.s. ground truth  (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width=.3\textwidth, clip, trim = 3cm 9cm 3cm 10cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wr4}
   % \label{fig:wingrockresultsbp}
  } 
   \caption{Prediction vs ground truth comparisons for the first example. Both nonparametric methods accurately predict the true drift and clearly outperform the RBFN learner.}
	\label{fig:wrprederrorsex1}
\end{figure*}	   


\begin{figure}
        \centering
				  \subfigure[Results over 555 randomised examples.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = .5\textwidth, clip, trim = 1.5cm 8cm 1cm 7cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/wingrockbp}
   % \label{fig:wingrockresultsbp}
  } 
   \caption{Performance of the different online controllers over a range of 555 trials with randomised parameter settings and initial conditions. 1: RBF-MRAC, 2: GP-MRAC, 3:LACKI-MRAC, 4: P-Controller. LACKI-MRAC outperforms all other methods with respect to all performance measures, except for prediction runtime (where the parametric learner RBF-MRAC performs best).} 
       \label{fig:wingrockresultsbp}
\end{figure}	   
  
	
	For each method, the figures show the boxplots of the following recorded quantities: 
	\begin{itemize}
		\item \textit{log-XERR}: cummulative angular position error (log-deg), i.e. $\log(\int_{t_0}^{t_f} \norm{\xi_1(t) - x_1 (t)} \dt )$.
		\item \textit{log-XDOTERR}:  cummulative roll rate error (log-deg/sec.), i.e. $\log(\int_{t_0}^{t_f} \norm{\xi_2(t) - x_2 (t)} \dt )$.
		\item \textit{log-PREDERR}: log-prediction error, i.e. 
		
		$\log(\int_{t_0}^{t_f} \norm{\nu_{ad}(x(t)) - \tilde a(x(t))} \dt )$.
		\item \textit{log-CMD}: cummulative control magnitude (log-scale), i.e. $\log(\int_{t_0}^{t_f} \norm{u(t)} \dt )$.
		\item \textit{log-max. RT (predictions)}: the log of the maximal runtime (within time span $[t_0,t_f]$) each method took to generate a prediction $\nu_{ad}$ within the time span.
		\item \textit{log-max. RT (learning)}: the log of the maximal runtime (within time span $[t_0,t_f]$) it took each method to incorporate a new training example of the drift $\tilde a$.
	\end{itemize}
	
	As can be seen from Fig. \ref{fig:wingrockresultsbp}, all three adaptive methods outperformed the simple $P$ controller in terms of tracking error. 
	
	In terms of prediction runtime, the RBF-MRAC outperformed both GP-MRAC and LACKI-MRAC. This is hardly surprising. After all, RBF-MRAC is a parametric method with constant prediction time. By contrast, both non-parametric methods will have prediction times growing with the number of training examples.
That is, it would be the case if GP-MRAC were given an infinite training size budget. Indeed one might argue whether GP-MRAC, if operated with a finite budget, actually is a parametric approximation where the parameter consists of the hyperparameters along with the fixed-size training data matrix. When comparing the (maximum) prediction and learning runtimes one should also bear in mind that GP-MRAC predicted with up to 200 examples in the training data set. By contrast, LACKI-MRAC undiscerningly had incorporated all 10001 training points by the end of each trial.

Across the remaining metrics, LACKI-MRAC markedly outperformed all other methods.

Note, we have also attempted to test all methods across a greater range of problem settings, including larger initial states, more varied hyper-parameter settings, lower feedback gains and more varied choices of dynamics coefficients $W$. However, this resulted in GP-MRAC to often run into conditioning problems. This is a common issue in GP learning due to the necessity of matrix inversion or Cholesky decompositions of the covariance matrix. Similar behaviour ensued when setting the training size budget to large values. All these changes often resulted in long learning runtimes, spiky control outputs and thus, poor overall performance. Similarly, code execution of our RBF-MRAC implementation was frequently interrupted with error messages when the state was initialised to positions outside of the rectangle $[0,7] \times [0,7]$.

We have not investigated the root cause of these issues in greater detail yet. However, it might be worth exploring whether the great robustness of kinky inference might be an additional selling point that sets it apart from other recent adaptive control methods. Such robustness is of course important in control settings such as flight control where failure or erratic behaviour of the adaptive element may result in critical incidents. 

An example where GP-MRAC failed to track the reference occurred when repeating our first experiment  with the following modifications: The initial state was chosen to be $x(t_0) = (-90,40)^\top$ corresponding to a rapidly rotating aircraft. Furthermore, the wing rock coefficients $W$ were multiplied by a factor of $5$, amplifying the non-linearities of the drift field. 

When initialised with a length scale parameter of 0.3, the GP ran into conditioning problems and caused the output of the adaptive element in GP-MRAC to produce spikes of very large magnitude and thus, further destabilised the system. We tried the problem with various kernel length scale settings ranging from $0.3$ to $20$. Increasing the length scale parameter to length scale of at least 1 seemed to fix the conditioning problem. Nonetheless, GP-MRAC still did not manage to learn and stabilise the system in any of these settings. A record of GP-MRAC's performance in this example (for length scale of 1) is depicted in Fig.  \ref{fig:gpfailGPWR1} -  \ref{fig:gpfailGPWR3}. As the plots show, GP-MRAC starts with relatively high tracking and prediction error from which it could not recover. At about 26 seconds into the simulation the state rapidly diverged.

 
\begin{figure*}
        \centering
				  \subfigure[Position (GP-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5.5cm, clip, trim = 4cm 9cm 4cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail1}
   \label{fig:gpfailGPWR1}
  } 
					  \subfigure[Tracking error (GP-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5.5cm, clip, trim = 4cm 9cm 4cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail2}
   \label{fig:gpfailGPWR2}
  } 
							  \subfigure[Log - prediction error (GP-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[scale =.35]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail_gpprederr}
    \label{fig:gpfailGPWR3}
  } 
					  \subfigure[Position (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5.5cm, clip, trim = 3.5cm 9cm 4cm 10cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail1_hfe}
    \label{fig:gpfailKIWR1}
  } 
					  \subfigure[Tracking error (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 5.5cm, clip, trim = 3.5cm 9cm 4cm 10cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail2_hfe}
    \label{fig:gpfailKIWR2}
  } 
%
					  \subfigure[Log - prediction error (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[scale =.35,clip, trim = 0cm 0cm 0cm .1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail_hfeprederr}
    \label{fig:gpfailKIWR3}
  } 
						  %\subfigure[State path (KI-MRAC).]{
    %%\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    %\includegraphics[width = 5cm, clip, trim = 2cm 9cm 1cm 7cm]
								%%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								%{content/Ch_kinkyinf/figs/gpfail_statepathHfe}
   %% \label{fig:wingrockresultsbp}
  %} 	
	%
   \caption{Example where GP-MRAC fails. By contrast,LACKI-MRAC manages to adapt and direct the system back to the desired trajectory.}
	\label{fig:gpfail}
\end{figure*}	 


For comparison, we also tried LACKI-MRAC on the same problem, starting with initial $L=1$ as before. Starting out with a relatively large tracking and prediction error, LACKI-MRAC nonetheless managed to recover and successfully track the system (see  Fig.  \ref{fig:gpfailKIWR1} -  \ref{fig:gpfailKIWR3}). The state path and learned drift model obtained by LACKI-MRAC are depicted in Fig. \ref{fig:gpfail2}.
%
\begin{figure*}
        \centering
				  \subfigure[State path (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = .4\textwidth, clip, trim = 3cm 9cm 3cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail_statepathHfe}
   % \label{fig:wingrockresultsbp}
  } 	
	  \subfigure[Learned drift model (KI-MRAC).]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = .4\textwidth, clip, trim = 4.3cm 9cm 4cm 9cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/Ch_kinkyinf/figs/gpfail_hfelearnedmodel}
   % \label{fig:wingrockresultsbp}
  } 	
	%
   \caption{Depicted are the state path and the drift model learned online byLACKI-MRAC.}
	\label{fig:gpfail2}
\end{figure*}	 