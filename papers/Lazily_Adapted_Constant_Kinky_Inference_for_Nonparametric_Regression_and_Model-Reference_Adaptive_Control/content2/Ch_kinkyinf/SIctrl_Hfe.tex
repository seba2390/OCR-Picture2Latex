\section{Kinky inference and model reference adaptive control}
\label{sec:hfeandcontrol}
\label{sec:ctrlapplications}
In Sec. \ref{sec:KI_core}, we have presented \textit{kinky inference} (KI),  a non-parametric machine learning method that entertains assumptions about H\"older continuity and boundedness and can handle interval-bounded noise. In this section, we illustrate the utility of the approach in the context of model learning of nonlinear dynamical systems and (tracking) control.

As a first test and introduction to the control applications, Sec. \ref{sec:firstorderctrlandKI} considers an application of KI for model learning and control in a first-order system where the control can directly influence the velocity of a continuous-time plant. The bounds afforded by the KI rule are employed to provide a stability guarantee for a controlled uncertain system that meets the prior assumptions of the KI learner. 
In Sec. \ref{sec:KIMRAC}, we translate the controller to a more realistic second-order system. 
Simulations conducted in the context of wingrock tracking control demonstrate that our KI learning method is well suited for learning dynamic system models both offline and online and can outperform established machine learning methods such as Gaussian processes or radial basis function neural networks in model-reference adaptive control. 
%\jcom{If possible include SIIC and inversion model learning}


   

\subsection{A kinky inference velocity controller with stability guarantees }
\label{sec:firstorderctrlandKI}
Consider an agent that desires to control a plant with the first-order dynamics 
$\dot x = f(x) + g(x) u $
where $x \in \statespace \subset \Real^d$ is the state (``position'') and $u \in \Real^d$ a control that can be chosen by the agent. Set $\inspace$ might be some bounded space. For instance, the bounds could be imposed by the confines of a room a robot moves in or, in heating control, might be a range of temperatures. 
For simplicity, we assume that $g(x) \in \Real^{d \times d}$ is invertible and known for all $x \in \statespace$. By contrast, drift vector field $f$ is assumed to be uncertain a priori and will be identified and inverted utilising our kinky inference rule.
%We then combine the learner with a closed-loop inversion procedure quite in lieu to the procedure outlined in Ch. \ref{ch:SPSIIC}. (That is, in place of a random field the belief over $f$ will be inferred by the kinky inference learner introduced above.)


It is our aim to design a controller such that state trajectory $x(\cdot)$ follows \textit{reference} trajectory $\xref(\cdot)$. 
For instance, in the one-dimensional case, if we desire the state to reach setpoint $\xi$ we could set the reference derivative to $\dxref(t) = w (\xi - x )$, i.e. $\xref(t) = x(0) \exp( - w t ) - \xi (1- \exp(1- Kt) )$. That is, if the devised control law were successful in causing the state to follow the reference, the state  would converge to the target $\xi$ as $t \to \infty$. Control with a reference model is also referred to as \textit{tracking}, since the objective is to follow (i.e. track) the reference signal $\xref$. A special case, which we also have encountered in Ch. \ref{ch:SPSIIC}, is the task of \textit{stabilisation}. Here, the reference signal $\xref$ is set to a constant setpoint $\xi \in \statespace$. That is, the only control objective is to reach, and stay close to, the one point $\xi$. Note, in setpoint control we have $\dxref =0$.
%Alternatively, we might choose $\xref =\xi, \dxref=\dot \xi =0$ and need to make sure 

Having chosen a reference, we can utilise feedback linearisation in a model-reference adaptive control fashion \cite{Narendra1989} similar to the approach in Ch. \ref{ch:SPSIIC} and Sec. \ref{sec:KIMRAC}. That is, we define a control law:
\begin{equation}
	u:= g^{-1}(x) ( \nufb - \nu_{ad} + \nuref).
\end{equation}
Here, $\nuref := \dxref(t)$ defines the reference derivative and $\nufb$ is a \textit{feedback element} we will define below. Furthermore, intended to remove the drift $f$,
\textit{adaptive element} $\nu_{ad} := \predfn(x)$ is the output of the kinky inference rule as per Def. \ref{def:KIL} after having observed data set $\data_n=\Bigl\{\bigl(x_j, f(x_j), \varepsilon(x_j) \bigr) \,|\, j=1,...,N_n\Bigr\}$ containing a (noisy) sample of the drift at $N_n$ inputs $x_1,\ldots,x_{N_n}$. 

Closing the control-loop changes the plant dynamics to
%\begin{equation}
$\dot x = F(x) +\nufb + \nuref = F(x) +\nufb + \dxref$
%\end{equation}
where $F_n(x) = f(x) - \predfn(x)$. 
Consequently the \textit{error dynamics} are
\begin{equation}
	\dot e = F_n(x) +\nufb
\end{equation}
where the \textit{state error trajectory} $e (\cdot) = x(\cdot)- \xref(\cdot)$ represents the deviation of the state trajectory from the nominal trajectory given by the reference.

Since the drift $f$ is uncertain, $F(x)$ will generally not be zero. Assuming $f$ satisfies the prior assumptions inserted into the kinky inference learner ($f \in \mathcal K_{prior}$), Thm. \ref{thm:KIdesiderata} guarantees that the prediction bounds $\prederr$ provided by our KI rule (cf. Def. \ref{def:KIL}) hold and $F$ is bounded. In particular, $F_n(x) \in E_n := [-\prederr_{n,1}(x), \prederr_{n,1}(x)] \times [-\prederr_{n,d}(x),...,\prederr_{n,d}(x)]$ where $\prederrn$ is defined as per Def. \ref{def:KIL}.
 
One way of driving the error to zero and to supress the influence of the remaining uncertainty is to set the feedback term to $\nufb = -K e$ with $K := \text{diag}(w_1,...,w_d)$ for some gain weights $w_1,...,w_d \geq 0$.
%
This yields error dynamics given by the differential equation:
 \begin{equation}
	\dot e (t) = F_n(x(t)) - K e(t).
\end{equation}


%If we can bound the error, we can construct a robust tube about the trajectory and harness it for stability analysis and collision avoidance:
%By integration, we see that $x(\tau) = x(0) + \int_0^\tau \dxref(t) \dt + \int_0^\tau F(x(t)) - K x(t) \dt = \xref(\tau) + e(t)$.
We will now analyse the impact of both $K$ and the maximal prediction uncertainty on stability.

Remember, we can restate the differential equation integral form as follows (cf. Lem. \ref{lem:intcurve}):
 %\begin{equation}
	$e (t) = e(0) + \int_{0}^t - K e(\tau) \d \tau+ \int_{0}^t F_n(x(\tau)) \d \tau  .$
%\end{equation}
Since $F_n = f - \predfn$, by Thm. \ref{thm:KIdesiderata}, $\abs{F_n(x)} \leq \maxerrn:= \sup_{x \in \statespace}\norm{\prederrn(x)}_\infty $. Thus, 
$\norm{e (t)}_\infty \leq  \norm{ e(0) + \int_{0}^t - K e(\tau) \d \tau}_{\infty} + \norm{ \int_{0}^t F_n(x(\tau)) \d \tau}_\infty \leq \norm{ e(0) + \int_{0}^t - K e(\tau) \d \tau}_{\infty} +t \, \maxerrn.$ Observe,
 $e(0) + \int_{0}^t - K e(\tau) \d \tau $ is the solution to the ODE $\dot e = - K e$ with initial value $e(0)$. The solution is well-known to be:  $e(0) \exp(-K t)$ (cf. Sec. \ref{sec:OUprocess}). Therefore, the following bound always holds:
%
\begin{equation}\label{eq:errorboundgrowingconttimeKI}
	\norm{e (t)}_\infty \leq \norm{ e(0) \exp(-K t)}_{\infty} +t \, \maxerrn.
\end{equation}
Note, the term $\maxerr$ depends on the learning experience. As we know from Thm. \ref{thm:KIdesiderata}, the uncertainty of the learner (and hence $\maxerrn$ vanishes with increasing data size). Hence, the state  error bound can be reduced if the environment of the agent is uniformly explored.

\subsubsection{An agent exploration scenario} 
\label{sec:agentexploration}
%\textbf{An agent exploration scenario.}  
\begin{figure}\label{fig:explorsingleint}
        \centering
				\subfigure[Path taken by the exploring agent.]{
    \includegraphics[width =5cm, height =4cm]{content/figures/situationexplore1.pdf}
  } %\hspace{2cm}
		%\subfigure[Phase plot prior training.]{
    %\includegraphics[scale =.4]{content/figures/situationexplore.pdf}
  %} %\hspace{2cm}
	%\subfigure[Full history of exploring agent.]{
   %\includegraphics[scale =.35, clip, trim = 3.5cm 9cm 4cm 5cm]{content/figures/animate_hist.pdf}
  %}
       \caption{Path of an agent exploring the map. The agent obtains a new drift training example every 7 seconds. The start position is denoted by $S_1$. The subsequent waypoints (given by the setpoints), denoted by $SP1,...,SP11$, were determined incrementally as the positions of maximum posterior uncertainty. That is, on the basis of data set $\data_n$ the agent chose a new setpoint $\xi$ with components $\xi_j \in \arg\max_{x \in \statespace} \prederr_{n,j}(x)$. Note how the agent became increasingly successful in reaching his waypoints. With increasing learning experience, the trajectories become increasingly straight. This is a result of the adaptive element becoming more successful over time in cancelling the nonlinearity of the uncertain drift.} 
       %
\end{figure}	  
%The bounds we derived provide an insight into a trade-off between uncertainty and tighten with decreasing maximal uncertainty $\maxerr$ as well as with increased feedback gain. The understanding between 
%
%
As an illustration, consider the a situation in which an agent is deployed in a confined state space, say an ``arena'' $\statespace = [0, 10] \times  [0,10]$. The agent's plant adheres to a dynamic model as above with uncertain drift field $f(x) = \begin{cases}  [\sin(x_1); 2 \cos(x_2)], &\,  x \in \statespace\\ [0;0], &\text{otherwise}. \end{cases} $. Initially the agent is uncertain about the drift. However, we assume it can acquire a sample point of the drift at its current location once every $\tinc =7$ seconds at times $n\tinc$ ($n \in \nat_0$). The observations are obtained by taking finite differences with sample rate $0.01$. To account for numerical error we allow for a small observational noise level of $\varepsilon =0.01$.  At the initial learning step $n=0$, we initialise the controller's adaptive element $\nu_{ad} = \predfn$ with a kinky inference predictor (cf. Def. \ref{def:KIL}). The KI learner is given the correct parameters $L=2,p=1$ and we set the upper and lower bound functions $\lbf$ and $\ubf$ to zero outside the arena $\statespace$. This folds in the knowledge about the state of affairs outside of the map. 
The agent aims to explore the state space in a manner suitable to decrease the maximal uncertainty $\maxerrn$ over time. Starting out with an estimated initial maximal uncertainty of $4.8$, the agent is only allowed to stop the exploration process when $\maxerrn < 0.5$. 
For simplicity, we let the agent adopt a heuristic strategy and define a setpoint at the location $\xi_n \in \statespace$ where the current uncertainty is highest. This can be determined by setting $\xi_n \in \arg\max_{x \in \statespace } \prederrn(x)$. This minimizer, as well as the pertaining maximum uncertainty $\maxerrn$, required for computing the stopping criterion of the exploration process, is found by optimising each component uncertainty function $\prederrn$. As we have noted before, Lem. \ref{lem:Hoeldarithmetic} allows us to obtain a H\"older constant and exponent for this function. This renders it amenable to the bounded optimisation approach discussed in Sec. \ref{sec:Hoelder_opt_basic_brief}. In our specific case, this is even easier, since the $\prederrn$ is even Lipschitz. Therefore, it can be optimised (and the optimum be bounded) using Shupert's algorithm \cite{Shubert:72}.  
Starting at position $S_1 = [1;1]$, the agent computes new setpoints with this optimisation procedure every $\tinc = 7$ seconds. The agent then observes a new drift sample, updates the data set, computes a new setpoint and starts over.  A simulation of the path taken by the agent is depicted in Fig. \ref{sec:agentexploration}. Notice how the controller becomes increasingly successful in reaching the waypoints  and in cancelling out the nonlinearity of the dynamics. An excerpt of the inferred beliefs over the drift at three different stages of the online learning process is depicted in Fig. \ref{fig:postbeliefsexplor}. It shows how a new training example is inserted at the point of maximum uncertainty (Fig. \ref{fig:kipredexpl1} vs Fig. \ref{fig:kipredexpl2}), decreasing the overall maximum uncertainty given by the floor and ceiling functions. The last plot (Fig. \ref{fig:kipredexplfin}) shows the inferred model over the drift at the end of the exploration process. 
\begin{rem}
We can see that the maximum uncertainty is mainly due to the belief over the first component $\predf_{n,1}$. This can be attributed to the fact that the maximum slope of the ground truth of this component function is $1$. Therefore, the assumed Lipschitz constant $L=2$ is a conservative choice for this function, which means the bounds are not as tight as possible. By contrast, for the second component function, this constant is tight. We can see this where the slope is steepest, few training examples are needed to shrink the uncertainty down to negligible amounts (that is, down to the level of observational noise). As a way out, it would have been possible to choose a separate constant for both component functions. For the first component function, $L_1=1$ would have been the best choice. 
\end{rem}

\subsubsection{Stability and safety bounds}
The attractive property of the bound of Eq. \ref{eq:errorboundgrowingconttimeKI} is that it holds for all times (not just in the limit) and that $K$ would not even have to be stable (i.e. Hurwitz). The clear disadvantage is that the bound grows with time and hence, does not provide a guarantee of stability of the state trajectory. 
To overcome this limitation, we will now provide a simple stability guarantee for the closed-loop trajectory of the kinky inference controlled plant. Since $F_n$ is bounded (due to the guaranteed conservatism of the KI rule), this is a situation where our learning guarantees can be combined with  standard results from robust control \cite{Kofman2007}. To this end, we remind the reader of the following fact:

\begin{lem}[\cite{Kofman2007}] \label{lem:kofman}
Consider the system $\dot x = A x + v$ where matrix $A$ is stable and has the Jordan canonical form 
$J = V^{-1} A V$. Suppose that $v(t) \leq \bar v, \forall t \in [0,T]$. Finally for a matrix $M$ and vector $x$ let $\abs{M}$ and $\abs{x}$ denote their respective component-wise moduli.
Then, we have:

 (I) $\abs{x(t)} \leq \abs{V} \abs{\text{Re}(J)^{-1}} \abs{V^{-1}} \, \bar v$, provided $\abs{V^{-1} x(0) \leq  \abs{\text{Re}(J)^{-1}} \abs{V^{-1}}}$.  

(II) Furthermore,  $\forall \epsilon >0 \exists \bar t \forall t \geq \bar t:  \abs{x(t)} \leq \abs V |\text{Re}(J)^{-1}| \, \abs{V^{-1}} \, \bar v + \abs V \epsilon $ (regardless of the initial condition). 
\end{lem}


\begin{figure*}
        \centering
				%\subfigure[Post. belief with three data points.]{
    %\includegraphics[scale =.33]{content/figures/driftpred1.pdf}
  %}
	\subfigure[KI predictions after 6 learning steps.]{\label{fig:kipredexpl1}
   \includegraphics[width = 5.3cm, clip, trim = 1cm 9cm 1cm 10cm]{content/figures/driftpred41.pdf}
  }
	\subfigure[KI predictions after one additional learning step.]{\label{fig:kipredexpl2}
    \includegraphics[width = 5.3cm, clip, trim = 1cm 9cm 1cm 10cm]{content/figures/driftpred51.pdf}
  }
	\subfigure[Final prediction.]{\label{fig:kipredexplfin}
   \includegraphics[width = 5.3cm, clip, trim = 1cm 9cm 1cm 9cm]{content/figures/driftpred111.pdf}
  }         %add desired spacing between images, e. g. ~, \quad, \qquad etc. 
          %(or a blank line to force the subfigure onto a new line)
       \caption{Excerpt of the evolution of the posterior beliefs during the exploration phase. The agent starts out with a kinky inference learner that has initial knowledge of the fact that velocity outside the arena $\statespace$ is zero. The agent utilises the uncertainty bounds (floor and ceiling function) to guide exploration. That is, it puts a new setpoint at the position of the maximum uncertainty. The agent 
			stops exploring after the maximal uncertainty threshold $\maxerr$ is below  $0.5$. The top row shows the posteriors over the first output component of the learned drift vector field. The bottom row depicts the posterior beliefs over the second output dimension of the drift vector field. Plots Fig. \ref{fig:kipredexpl1} and Fig. \ref{fig:kipredexpl2} show how the exploration heuristic manages (after some learning) to explore the parts of state space with high uncertainty.} 
       \label{fig:postbeliefsexplor}
\end{figure*}	 

 


%Before proceeding, we remind ourselves of the following well-known facts (adapted from \cite{Haddad2008} ):
%
%
%\begin{thm} [Barbashin-Krasovskii-LaSalle invariance theorem]
%Let $\dot x = f(x(t))$, $x(t_0) = x_0, t \geq t_0$ be a dynamic system where $f$ is differentiable.
%Assume $I \subset \statespace$ is a positively invariant set (that is: $x(t) \in I$ implies $x(t') \in I, \forall t' \geq t$). Assume there exists a continuously differentiable function $V:I \to \Real$ such that $\dot V(x) =(\nabla_x V)(x)  f(x) \leq 0, \forall x \in I$. Let $Z:= \{ x \in I \, | \, (\nabla_x V)  (x)  f(x) =0 \}$ and $M \subset Z$ denote the largest invariant set contained in $Z$.
%We have: If $x(0) \in I $ then $x(t) \stackrel{t \to \infty}{\longrightarrow} M \subset Z$.
%\end{thm}
%
%For our purposes, the point of the theorem is that via construction of a \textit{Lyapunov function} $V$, one can establish stability bounds on the state trajectory, without the need to solve the integral curve. 
%
%Invoking a generalisation \cite{Bhat2009} of La Salle's invariance principle\cite{lasalle1960}, we can provide the following stability guarantee:
%
%\begin{ques}
%\jcom{Does the following argument go through for time-varying reference?}
%\end{ques}

Harnessed with our Thm. \ref{thm:KIdesiderata} about the KI learner, we can immediately apply the lemma to yield the following simple stability guarantee:
\begin{thm}\label{thm:KIvelocagstab}
Assume we are given a sample $\data=\Bigl\{\bigl(x_j, f(x_j), \varepsilon(x_j) \bigr) \,|\, j=1,...,N\Bigr\}$ of uncertain drift $f \in \Kprior$ where $\Kprior$ is defined as in Sec. \ref{sec:KI_core}. For each output dimension $i$ we train a kinky inference learner on the data yielding predictors $\predf_{n,i}$ with error function $\prederr_{n,i}$ (as per Def. \ref{def:KIL}) for components $i=1,...,d$. Define $\maxerr_i := \sup_{ x \in \statespace \subset \Real^d} \prederr_i(x)$ to be the maximum prediction error in any dimension on the state space. 
Desiring to stabilise setpoint $\xi$, we apply the control $u:= g^{-1}(x) ( -K e - \predf + \dxref)$. This causes the plant to follow trajectory $x(t) = \xref(t) + e(t)$ where $\xref = \xi$.
We have:
\begin{enumerate}
	\item  For dimensions $(i=1,...,d)$, define the interval $S_i:= [- \frac{\maxerr_i}{w_i}, \frac{\maxerr_i}{w_i}]$. 
 $S=S_1 \times \ldots \times S_d$ is an invariant set for the error dynamics. That is, $e(t) \in S $ implies $e(t') \in S, \forall t'\geq t$.
	\item For any initial error $e(0)$, $e(t)$ converges to the largest invariant set in $S$.
\end{enumerate}

\end{thm}
\begin{proof}
One approach of showing the result would be to construct a piece-wise Lyapunov type function that is zero on $S$ and assumes values proportional to the distance of the error to the set $S$ otherwise. We can then apply one of the more modern versions of La Salle's invariance theorem \cite{Bhat2009,Haddad2008} to conclude the statement.
However, the statement also follows directly from Lem. \ref{lem:kofman}. To see this, we choose $K =A$, $F_n(x(t)) = v(t)$ where $A$ and $v$ are as defined in the lemma. 


By assumption, $K$ is stable. Since in our simple case, $K$ is diagonal, $J = K$ and $V = I_d$ is the identity matrix. Hence, $\abs{V}=\abs{V^{-1}} =I_d$. Because of the diagonal form of $K$, we have the simple situation of no component interactions. Hence, for each component $i \in \{1,...,d\}$, we have 
$\abs{e_i(t)} \leq \abs{V_{ii}} \abs{M_{ii}^{-1}} \abs{V_{ii}^{-1}} \maxerr_i= \frac{1}{w_i} \maxerr_i$ where we also have utilised $F_{n,i}(x) \leq \maxerr_{n,i}$ which holds by Thm. \ref{thm:KIdesiderata}.
This shows that $S$ is invariant.
The second statement about convergence to $S$ now follows directly from Lemma \ref{lem:kofman} (II).
%
%
%=========== Old proof ==========================
%We show the stability result for each component $i$ in isolation. So, let $i \in \{1,...,d\}$. 
%Define the function $V_i(e_i) = \begin{cases} \text{dist}(e_i,S)^2 & e_i \notin S\\ 0 & e_i \in S. \end{cases}$
%Furthermore, $\dot V_i(e_i) = 2 e_i \dot e_i = 2 e_i \, (F - w e_i)$.
%
%(i) Obviously, $V_i$ is pos. semi-definite. 
%
%(ii) We show $\dot V_i(e_i) \leq 0, \forall e_i$:
%
%
%Two cases: a) $e_i > \frac{\maxerr_i}{w_i} \geq 0$: Since $w_i \geq 0$ this implies $ \maxerr_i - w_i e_i \leq 0$. Since $F \leq \maxerr_i$, we also have $F - w_i e_i \leq 0$. Hence, $\dot V_i(e_i) =  2 e_i (F - w_i e_i) \leq 2 e_i (\maxerr_i - w_i e_i)\leq 0$.
%
%b) Showing $\dot V_i(e_i) \leq 0$ for  $e_i < - \frac{\maxerr_i}{w_i} \leq 0$ is completely analogous.
%In conclusion, we have established $\dot V_i(x) \leq 0, \forall x$.
%
%(iii) Define  $\Omega_i(e_i(0)) = [-r_i,r_i]$ where $r_i = \max\{ \abs{e_i(0)},\frac{\maxerr_i}{w_i}\}$. That is, $\Omega_i(e_i(0)) $ is the smallest ball around $0$ that contains both $e_i(0)$ and $S$. By convexity of $V_i$, $V(r_i) \geq V_i(x), \forall x \in \Omega_i$  and $\Omega_i(e_i(0))$ is an invariant set (which also follows from Thm. 7.1 in \cite{Bhat2009}). 
%Owing to (i) - (iii), we can apply La Salle's invariance principle \cite{lasalle1960} in the form given by \cite{astroembook2008} (or again Thm. 7.1 in \cite{Bhat2009}) to conclude that $\text{dist}(e_i(t), S_i) = \inf\{ \abs{e_i(t) - s } : s \in S_i \} \to 0 \, (t \to \infty)$.
%
%In summary, we have shown that $e_i(t) \in \Omega_i$ and $e_i(t) \to S_i$.
%%In particular, $e(t) \in \tube= \Omega_1(e(0)) \times \ldots \Omega_d(e(0))$ and 
\end{proof} 


 \begin{figure}
        \centering
							%\subfigure[Uncoordinated plans (prior learning).]{
    %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4.5cm 9cm]{content/figures/rsstalkfigs/maspriorlearn3d.pdf}
  %}\label{fig:preplans_uncoord3d}
	%\subfigure[Coordinated plans (prior learning).]{
   %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4.5cm 9cm]{content/figures/rsstalkfigs/maspriorlearn3d_coord.pdf}
  %} \label{fig:preplans_coord3d}
	%\subfigure[Uncoordinated plans  (after learning).]{
    %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4.5cm 9cm]{content/figures/rsstalkfigs/maspostlearn3d_uncoord.pdf}
  %}\label{fig:postplans_uncoord3d} 
	%\subfigure[Coord. plans (after learning).]{
   %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4.5cm 9cm]{content/figures/rsstalkfigs/maspostlearn3d_coord.pdf}
  %} \label{fig:postplans_coord3d}       
				\subfigure[Uncoordinated plans (prior learning).]{\label{fig:preplans_uncoord}
    \includegraphics[width = 3.9cm]{content/figures/prelearninit.pdf}
  }
	\subfigure[Coordinated plans (prior learning).]{\label{fig:preplans_coord}
   \includegraphics[width = 3.9cm]
	{content/figures/coordcoordviaK.pdf}
  } 
	\subfigure[Uncoordinated plans  (after learning).]{\label{fig:postplans_uncoord} 
    \includegraphics[width = 4.0cm]{content/figures/postlearninit.pdf}
  }
	\subfigure[Coord. plans (after learning).]{\label{fig:postplans_coord}  
   \includegraphics[width = 4.0cm]{content/figures/coordcoord_learned.pdf}
  }      
       \caption{The grey boxes represent the agents' safety regions (invariant sets plus a margin taking into account the agent radius of 0.1) for various choices of maximal uncertainties $\maxerr_i$ and feedback gains $w_i$ for dimensions $i \in \{1,2\}$. Fig. \ref{fig:preplans_uncoord}: $w_i=2, \maxerr_i = 4.8$. Fig. \ref{fig:preplans_coord}: $w_i=5.3, \maxerr_i = 4.8$. Fig. \ref{fig:postplans_uncoord}: $w_i=2, \maxerr_i = 0.5$. Fig. \ref{fig:postplans_uncoord}: $w_i=0.55, \maxerr_i = 0.5.$}
\label{fig:coordlearningagentsplansKIvel}
\end{figure}	  

 
The theorem tells us something interesting about the connection between learning success (measured by the $\maxerr_i$) and the magnitude of the feedback gain. That is, in order to stabilise the system up to a desired error bound, we can either increase the gain or reduce the uncertainty by learning. To illustrate this point, assume we have two agents $1$ and $2$ tasked to stabilise their plants at setpoints $S1=[4;5]$ and $S2=[6;5]$, respectively. Having a diameter of $0.2$, and being immersed in an uncertain drift field as considered in the exploration scenario above, the agents desire to find a ``plan'' of how to set their feedback gains such that it is guaranteed that they stabilise their setpoints (up to a certain error) while being guaranteed to avoid a collision with the other agent. Thm. \ref{thm:KIvelocagstab} helps us to achieve this.  That is, the agents can ``coordinate'' their plans gains such that their invariant sets do not overlap and are sufficiently small. 
Consider Fig. \ref{fig:coordlearningagentsplansKIvel}. 
Fig. \ref{fig:preplans_uncoord} depicts a situation the agents would encounter if they were initialised with the configuration the agent in our exploration scenario had prior to learning. That is, their uncertainties were $4.8$ and the feedback gain was set to a diagonal matrix with diagonal entries $w_i =2$. As the figure shows, the safety regions (invariant sets, plus some margin representing the agents' diameter) overlap and hence, a collision cannot be ruled out. As one way out, we can employ Thm. \ref{thm:KIvelocagstab} to adjust the gain such that the regions will be disjoint (Fig. \ref{fig:preplans_coord}). 

In order to improve control success without having to resort to higher feedback gain, the agent can alternatively attempt to bring down the uncertainty by learning. That is, we can incorporate the learning experience gained during the agent exploration phase (see above) to bring down the prediction uncertainties $\maxerr_i$ in each dimension, while leaving the gains unaltered (Fig. \ref{fig:postplans_uncoord}).
As we can see from the plots, the reduced uncertainties ensure that the safety regions are disjoint, without further need to increase the gain.
If all we were interested in was collision avoidance (or if we wanted to save control energy), we now could decrease the gains $w_i$ as much as possible without making the safety regions overlap again (Fig. \ref{fig:postplans_coord}). Once, again Thm. \ref{thm:KIvelocagstab} tells us how to set the gains accordingly. 


Note that so far, we have assumed that the agents are homogenous and have not taken (differing) notions of cost into account. For instance, an agent who experiences a lower loss when expending control energy might be assigned a higher gain than one with a lower notion of control cost. Automating such coordination methods in continuous time is the subject of Ch. \ref{ch:contcoll}. Our bounds for the kinky inference controllers could be combined with the coordination methods developed therein. This would yield a method for autmated coordination with guaranteed continuous time-collision avoidance of the learning agents.
%\subsection{A simple agent exploration scenario}%\label{sec:agentexploration}





  %
%\subsection{Multi-agent collision avoidance taking epistemic uncertainty into account}
  %
%\begin{figure*} 
        %\centering
							%\subfigure[Uncoordinated plans (prior learning).]{
    %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/preplans3d2ag_uncoord.pdf}
  %}\label{fig:preplans_uncoord3d}
	%\subfigure[Coordinated plans (prior learning).]{
   %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/preplans3d2ag_coord.pdf}
  %} \label{fig:preplans_coord3d}
	%\subfigure[Uncoordinated plans  (after learning).]{
    %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/postplans3d2ag_uncoord.pdf}
  %}\label{fig:postplans_uncoord3d} 
	%\subfigure[Coord. plans (after learning).]{
   %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/postplans3d2ag_coord.pdf}
  %} \label{fig:postplans_coord3d}       
				%\subfigure[Uncoordinated plans (prior learning).]{
    %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/preplans2d2ag_uncoord.pdf}
  %}\label{fig:preplans_uncoord}
	%\subfigure[Coordinated plans (prior learning).]{
   %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/preplans2d2ag_coord.pdf}
  %} \label{fig:preplans_coord}
	%\subfigure[Uncoordinated plans  (after learning).]{
    %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/postplans2d2ag_uncoord.pdf}
  %}\label{fig:postplans_uncoord} 
	%\subfigure[Coord. plans (after learning).]{
   %\includegraphics[width = 3.9cm, clip, trim = 3.5cm 9cm 4cm 9cm]{content/figures/postplans2d2ag_coord.pdf}
  %} \label{fig:postplans_coord}       
       %\caption{Multi-agent collision avoidance before and after learning. Top row: Draws from the Ito hulls of the agents' uncertain trajectories. Bottom row: Paths with 50\% error ellipses drawn at regular time intervals. 
			%Before the map is explored, the agents' uncertainties are so large that Agent 2 (blue agent) does not reach his goal location (5,9) without incurring a collision with subjective probability $< 0.05$. Therefore, collision avoidance causes him to wait until Agent 1 has completely passed by to her goal location (10,5) and causes Agent 1 to miss his destination.  }
			%\label{fig:coordlearningagentsplans}
%\end{figure*}	     
           




\input{content/ch_kinkyinf/KIMRAC_wingrock.tex}

\subsection{Boundedness in discrete-time systems}
\label{sec:KIMRACstabbounds}

In the previous subsection, we gave an illustration KI-MRAC -- a combination of a feedback-linearising controller with our KI learning method. The results are encouraging -- our adaptive control law managed to learn a dynamic system online and to track a reference in the presence of wingrock dynamics where other state of the art methods failed. 

However, KI-MRAC is an online learning approach and assumed the training corpus could be updated with the entire history of past observations. This might work for short time horizons. Unfortunately, such assumptions are often unrealistic. Despite being a fast nonparametric method, eventually the size of the training corpus will grow to an extent where the required prediction response time exceeds the desired control refresh frequency. 
%\jcom{Should explain these notions in background section}

Furthermore, the controller in KI-MRAC did not take the uncertainty into account when making decisions on the control action. 


In this section, we assume that predictions are made on the basis of a fixed offline training corpus.
This means that the uncertainty will always be bounded from below. 
For this setting, we are interested in the following questions:
\begin{enumerate}
\item 
Given a linearising adaptive control law in GP-MRAC in offline learning mode, can we quantify a bound on the error dynamics ? That is, folding in the prediction bounds of our KI learner, how do we construct a robust tube around the nominal trajectory that is guaranteed to contain the real trajectory? 
\item 
How can we utilise the tubes for collision avoidance in a multi-agent setting?
\end{enumerate}
The second question will be addressed in Sec. \ref{sec:KIcollavoiddiscretetime} in the context of predictive (open-loop) control. In this section, we will lay some of the ground work and address the first question.
However, this is a question that is also of relevance for collision avoidance since it can be harnessed to bound the trajectory within a safety region or tube. The latter can be converted into collision avoidance constraints or a criterion function (cf.  Ch. \ref{ch:contcoll}, Ch. \ref{ch:discrcoll} as well as references in Sec. \ref{sec:optMPC}).
 %Our exposition will focus on discrete-time systems.




%\section{Boundedness of the discrete-time closed-loop dynamics of an agent utilising Kinky inference learning to identity a H\"older continuous drift vector field}
%\subsubsection{Discrete-time dynamics}
The exposition in this section will focus on KI-MRAC in discrete-time. Here, we assume the dynamics are given as first-order Euler approximations of the second-order control-affine dynamics of Sec. \ref{sec:KIMRAC}.
This means that the error dynamics as per Eq. \ref{eq:errordynmrac} translate to the recurrence relation:
\begin{equation}\label{eq:errordynmrac_KIMRACdiscrete}
	e_{k+1}  = M e_k + \tinc F_k
\end{equation}


where $\tinc \in \Real_+$ is a positive time increment,  $k$ is the time step index. Furthermore,

\begin{equation}
	F_k: = F_k(x) =F_n(x) = f(x) - \predf(x) = B \bigl(\nu_{ad}(x_k) -  \tilde a(x_k)\bigr) 
\end{equation}
is an uncertain increment due to the model error of the learner (cf. Sec. \ref{sec:firstorderctrlandKI} or Eq. \ref{eq:errordynmrac}), 						$B = \left(\begin{array}[h]{c}
				O_m \\ \tinc I_m
						\end{array}\right)$ and 
%\begin{equation}
	%M_k = \left(\begin{array}[h]{cc}
				%I_m &  \, O_{m}\\
				%-\tinc K_1(k) & I_m- \tinc K_2(k) 
						%\end{array}\right) 
%\end{equation}
\begin{equation}
	M = \left(\begin{array}[h]{cc}
				I_m &  \, \tinc I_{m}\\
				-\tinc K_1 & I_m- \tinc K_2 
						\end{array}\right) 
\end{equation}
					is the (error state) transition matrix. 
					%
					%Note, in contrast to Sec. \ref{sec:KIMRAC} we allow for time-varying feedback gains $K_i(k) \,\, (i =1,2)$. 
					
This is completely consistent with the uncertain recurrence considered in Eq. \ref{eq:dynextstatediscrete1}, with the exception, that the uncertainty of increment $F_k$ is not probabilistic but interval-bounded due to Thm. \ref{thm:KIdesiderata}. Employing the same type of argument as in Lem. \ref{lem:origprocdiscrfacts} we see that the solution of the recurrence is given by:
%\begin{equation}
	%\vc e_k = M_{k-1} \vc e_{k-1} + \tinc \vc F_{k-1} = \ldots = P_{k-1,0} \, \vc e_0 + \tinc \sum_{i=0}^{k-1}  P_{k-1,i+1} \, \vc F_i		
%\end{equation}
\begin{equation}
	\vc e_k = M \vc e_{k-1} + \tinc \vc F_{k-1} = \ldots = M^k \, \vc e_0 + \tinc \sum_{i=0}^{k-1} M^{k-1-i} \, \vc F_i	.	
\end{equation}		 

%where as before, we define 
%
%\begin{equation}
	%P_{j,i}  =
	%\begin{cases}
	 %M_j \, M_{j-1} \, \ldots M_i&, i \leq j\\
	%I_d, &,i > j.
	%\end{cases}
%\end{equation}

We desire to bound the  norm of the error. To this end, we leverage that the norms are sub-additive and sub-multiplicative to deduce: 
%\begin{equation}
	%\norm{\vc e_k} \leq   \matnorm{P_{k-1,0}} \, \norm{\vc e_0} + \tinc \sum_{i=0}^{k-1}  \matnorm{P_{k-1,i+1}} \, \norm{\vc F_i}		
%\end{equation}
%
%where $\matnorm{\cdot}$ is a sub-multiplicative matrix norm pertaining to vector norm $\norm{\cdot}$. So, if $\norm{\cdot} = \norm{\cdot}_2$ is the Euclidean norm, then $\matnorm{\cdot}$ is the spectral norm.
%
%For the special case of constant $M:= M_k, \forall k$ we get:
\begin{equation}
	\norm{\vc e_k} \leq   \matnorm{M^{k}} \, \norm{\vc e_0} + \tinc \sum_{i=0}^{k-1}  \matnorm{M^{k-1-i}} \, \norm{\vc F_i}		\leq \matnorm{M^{k}} \, \norm{\vc e_0} + \tinc \maxerrn_k	 \sum_{i=0}^{k-1}  \matnorm{M^{k-1-i}}
\end{equation}
where $\maxerrn_k	= \max_{i=1,...,k-1} \norm{F_i}$.
As in Sec. \ref{sec:firstorderctrlandKI}, we assume there exists a maximum model error norm $\maxerrn$, i.e. $\forall i: F_i \leq \maxerrn$. 
For instance, if we construct the tube for collision avoidance within a confined part of state space (such as an arena), if we know the drift is periodic or bounded, we can set $\maxerrn = \sup_{x} \norm{F(x)}_{\infty}$. 
Then we have: \vspace{-1em}
\begin{equation}
	\norm{\vc e_k} \leq   \matnorm{M^{k}} \, \norm{\vc e_0} + \tinc \maxerrn \sum_{i=0}^{k-1}  \matnorm{M^{k-1-i}}.
\end{equation}

As shown in Sec. \ref{sec:critexpintbnddisturb}, the right hand side is convergent provided the gains $K_1, K_2$ are chosen such that $M$ is stable, i.e. $\specrad(M) <1$, and provided $\maxerrn_k$ is is bounded. 

Whether or not $M$ is stable, in low dimensions, the sums can be computed offline and in advance. This is of great benefit if 
 the controller that is building on the error bounds is utilising optimisation-based control with a finite time-horizon (for a related setup, see cf. Sec. \ref{sec:KIcollavoiddiscretetime}).   

To get a (conservative) closed-form bound on the error norms, Sec. \ref{sec:critexpintbnddisturb}
contains a derivation and discussion of the following result:

\begin{thm}\label{thm:normboundsboundeddisturbmainbody}
Let $(F_k)_{k \in \nat_0}$ be a norm-bounded sequence of vectors with $\maxerrn_k :=\max_{i \in \{0,\ldots,k-1\}} \norm{F_i} \leq \maxerrn \in \Real$. 
For sequence $(x_k)_{k \in \nat_0}$ defined by the linear recurrence 
	$x_{k+1}  = M x_k + \tinc F_k \,\,\,\, (k \in \nat_0)$, we can define the following bounds:
	
	\begin{enumerate}
	\item $\norm{\vc x_k} \leq \matnorm{M^{k}} \, \norm{\vc x_0} + \tinc \maxerrn_{k} \sum_{i=0}^{k-1}  \matnorm{M^{i}}$. If $\specrad(M) <1$ and $\exists \maxerrn$ $:\maxerrn\geq \maxerrn_k \geq 0, \forall k$, the right-hand side converges as $k \to \infty$.
	\item Let $k_0 \in \nat, k_0 >1$ such that $\matnorm{M^k} <1, \forall k \geq k_0$ and let $\varphi := \matnorm{M^{k_0}} <1$ and let $\delta_k := \floor{k/k_0}$. If $r:=\specrad(M) < 1$, for $k > k_0$,  we also have:
	\begin{align} \norm{\vc x_k} &\leq c \, \varphi^{\delta_k} \, \norm{\vc x_0} + \tinc \maxerrn_{k} \Bigl(\sum_{j=0}^{k_0-1} \matnorm{M^j} + c \, k_0 \frac{\varphi-\varphi^{ \floor{\frac{k}{k_0}}+1 }}{1-\varphi} \Bigr) \stackrel{k \to \infty}{\longrightarrow} \tinc \maxerrn_{k} \sum_{j=0}^{k_0-1} \matnorm{M^j} 
	+ \frac{\tinc \maxerrn_{k}\,  c\, k_0 \,\varphi}{1-\varphi} 
	\end{align}
	where possible choices for $c \in \Real$ are: 
(i) $c= \max\{\matnorm{M^i} | i=1,\ldots,k_0-1 \}$ 

or (ii) $c = \frac{1}{(d-1)!} \Bigl(\frac{1 - d}{\log r}\Bigr)^{d-1}	\matnorm{M}^{d-1}\, \, \, r^{\frac{1 - d}{\log r}-d+1}  $.
%
Since $\matnorm{M} \geq 1$, one can also choose (iii) $c:= \matnorm{M}^{k_0}$. %In this case, $\matnorm{M^k} \leq  \, \matnorm{M}^{k_0}^{(k \text{ div } k_0) +1} $.

\item If $\matnorm{M} < 1$, the following holds:
$\norm{\vc x_k} \leq \matnorm{M}^{k} \, \norm{\vc x_0} + \tinc \maxerrn_{k}   \frac{1-\matnorm{M}^k}{1-\matnorm{M}}. $
	\end{enumerate}
\begin{proof}
1) is due to Eq. \ref{eq:9jn44mm4}. 2) follows by Lem. \ref{lem:CMEbndnormofM2k2}  and  Lem. \ref{lem:alphaYtermbnds2}. 3) Is a direct application of the geometric sum formula in conjunction with the condition that $\matnorm{M}<1$ and that the matrix norm is sub-multiplicative. 3) is a consequence of the geometric sum formula (Lem. \ref{lem:geommatseries}). 
\end{proof}	
\end{thm}


In Ch. \ref{ch:critexp}, we derive a fast, conservative algorithm that can calculate a sequence of improving upper bounds on the ``critical matrix exponent'' $k_0$. This is of interest in cases where its naive computation is expensive (e.g. in high-dimensional systems). Furthermore, the chapter contains an illustration of how the error norm bounds decrease with decreasing spectral radius $\specrad(M)$.

Finally, Sec. \ref{sec:KIcollavoiddiscretetime} considers an application of a bound similar to 1) in Thm. \ref{thm:normboundsboundeddisturbmainbody} to the different setting where there is no feedback gain and where the reference control is determined via mixed-integer programming in the context of MPC with collision avoidance constraints. However, we point out that in such a setting, one could also use feedback control in combination with predictive control. For this situation, the bounds of Thm. \ref{thm:normboundsboundeddisturbmainbody} would be of immediate utility.


  