\section{Outlook on additional learning-based control methods}
In the following we will briefly discuss applications of KI to the system identification and inversion control (KI-SIIC), followed by an application to inversion dynamics model learning and control (IMLC)  \cite{Tuongmodellearningsurvey2011}. While providing rigorous examinations and comparisons was beyond the time limit and scope of this work, we have included the latter application to highlight our method's applicability in control that is based on black-box learning \cite{Ljungperspectives:2010}.

\subsection{Kinky inference for system identification and inversion control (KI-SIIC)}
In this section, we will apply kinky inference to our SIIC approach of Ch. \ref{ch:SPSIIC}. Let $q$ be a generalised coordinate (a configuration) and $x = [q; \dot q]$ be the state.  
Given the second-order control-affine dynamics $\ddot q = a(x) + b(x) u$, we infer both $a$ and $b$ with a separate KI learner. The online learning process other-wise proceeds just as described in Ch. \ref{ch:SPSIIC}. Note, in the case of random field learning, we placed a log-normal prior over the components of $b$ to learn it. The motivation was to encode the knowledge that $b$ was positive and bounded away from zero. Fortunately, in our KI framework, we can encode knowledge of boundedness quite explicitly by setting the lower bound function $\lbf$ to zero. As before, we set the pseudo-control to linear feedback with gain weights $w_1=w_2=2$. This weight is large enough to drive the linearised dynamics to the goal, but sufficiently weak to ensure that learning success was important to achieve stabilisation.

As an illustration that our SIIC framework also works well with the KI learner in place of the random fields, we have repeated a double pendulum simulation. The results of the learning process are depicted in Fig. \ref{fig:hfesiic}. In this simulation, we have repeated the stabilisation task of moving the pendulum to the goal state $\xi =[\pi;\pi;0;0]$. Firstly, we recorded the state and control trajectories for an episode of ten seconds of online learning. The resulting squared control error evolution (measuring the squared distances of the state variables to the goal state $\xi$ at each time step) is depicted in Fig. \ref{fig:hfesiic21}. During online learning, we allowed the controller to update the training examples to be updated every $\Delta_\lambda = 0.3$ seconds and set the observational uncertainty level to $\varepsilon \equiv 0.01$. In the initial round, the controller was unsuccessful to stabilise the double pendulum. Next, we repeated online learning for another nine episodes. Then, we restarted the trained controller with learning switched off. This time, the controller was more successful in achieving the task, without further learning, although having some remaining error that seemed to become worse again towards the end of the recorded time span (see Fig. \ref{fig:hfesiic22}). As expected, this remaining error was removed when we restarted the simulation on the bases of additional learning experience (refer to  Fig. \ref{fig:hfesiic23}). The control signal generated by KI-SIIC in the last run is depicted in Fig. \ref{fig:hfesiic4}. 
Unsurprisingly, the control signal generated by kinky inference can indeed be ``kinky'', i.e. non-smooth. This in contrast to RF-SIIC which produced smooth control signals once learning was switched off. We attribute this behaviour to the kinks in the predictions of the kinky inference rule. However, in applications where this ought to be a problem, we expect to be able to alleviate this issue by replacing the vanilla KI rule by the smoothed version introduced in Sec. \ref{sec:SKI}.



%
 %\begin{figure}
        %\centering
			%\subfigure[State evolution (1st round).]{\label{fig:hfesiic1}
    %\includegraphics[width = 3.9cm]{content/Ch_kinkyinf/figs/hfesiic1}
  %}
%%
	%\subfigure[Control evolution (1st round).]{\label{fig:hfesiic2}
   %\includegraphics[width = 4cm, clip, trim = 0cm 0cm 0cm 0cm]
%{content/Ch_kinkyinf/figs/hfesiic2}
  %} 
	%\subfigure[State evolution (after learning).]{\label{fig:hfesiic3} 
    %\includegraphics[width = 3.9cm]{content/Ch_kinkyinf/figs/hfesiic3}
  %}
	%%
 	%\subfigure[Control evolution  (after learning).]{\label{fig:hfesiic4} 
    %\includegraphics[width = 4cm, clip, trim = 0cm 0cm 0cm 0cm]{content/Ch_kinkyinf/figs/hfesiic4}
  %}
%
        %\caption{Fig. \ref{fig:hfesiic1}: State trajectory of the first online learning round. The blue and green lines are the configurations $q_1,q_2$ (angles in radians), respectively. The red and cyan lines are the pertaining velocities. Fig. \ref{fig:hfesiic2}: The control signal. As in Ch. \ref{ch:SPSIIC}, clearly visible are the drops to zero injected to take measurements about $a$ without the influence of $b$. Fig. \ref{fig:hfesiic3} and Fig. \ref{fig:hfesiic3} show the state and control trajectories in the 20th run with learning switched off. This run was preceded by $10$ episodes of online learning lasting ten seconds each. As can be seen from the plots, no additional learning was necessary to successfully stabilise the pendulum.}
%\label{fig:hfesiic}
%\end{figure}	 

\begin{figure}
        \centering
			\subfigure[Error evolution (1st trial).]{\label{fig:hfesiic21}
    \includegraphics[width = 3.9cm]{content/Ch_kinkyinf/figs/HfeSIIC_sqrddist2ep}
  }
%
	\subfigure[Error evolution (10 trials).]{\label{fig:hfesiic22} 
    \includegraphics[width = 3.9cm]{content/Ch_kinkyinf/figs/HfeSIIC_sqrddist10ep}
  }
	%
 	\subfigure[Error evolution  (20 trials).]{\label{fig:hfesiic23} 
    \includegraphics[width = 4cm, clip, trim = 0cm 0cm 0cm 0cm]{content/Ch_kinkyinf/figs/HfeSIIC_sqrddist20ep}
  }
	 	\subfigure[Control evolution  (after learning).]{\label{fig:hfesiic4} 
    \includegraphics[width = 4cm, clip, trim = 0cm 0cm 0cm 0cm]{content/Ch_kinkyinf/figs/hfesiic4}
  }


        \caption{Fig. \ref{fig:hfesiic21}- Fig. \ref{fig:hfesiic21}: Depicted are the squared distances to the goal as a function of time steps. The abscissa marks the time steps the squared distances were recorded. The step size was 0.01 seconds. The blue and green lines are the squared distances to the goal (i.e. errors) of the configurations $q_1,q_2$ (angles in radians), respectively. The red and cyan lines are the pertaining squared velocity errors. Fig. \ref{fig:hfesiic21} shows the squared errors during the first episode of online learning. Fig. \ref{fig:hfesiic22} shows the squared error evolution during control (with online learning switched off) on the basis of learning experience gathered over the course of 10 preceding learning episodes of 10 seconds each. Fig. \ref{fig:hfesiic23} depicts this error evolution on the basis of 20 preceding episodes. Not surprisingly, the controller becomes better at controlling the plant with increased learning experience. The last plot depicts the control evolution (as a function of time in seconds) of the last episode (with learning switched off).
}
\label{fig:hfesiic}
\end{figure}	



\subsection{Kinky inference in inverse dynamics model learning (KI-IML)}
So far, we have considered the learning of \textit{forward} dynamic models of the form $\dot x = \psi(x,u)$ which we identified and then feedback linearised.  An alternative that has become popular in robotics is \textit{inverse (dynamics) model learning (IML)}.
The inverse dynamics model is the mapping $\psi^{-} : \statespace \times \statespace \to \ctrlspace,$
$ (x,\dxref) \mapsto u$ where $\dxref$ is a desired reference state derivative. In a configuration space formulation this translates to a mapping $ (q,\dot q,\ddot q) \mapsto u$ which can be learned by observing data of the form $\data =\{ \bigl((q_i,\dot q_i, \ddot q_i), u_i) \bigr)| i=1,...,N\}$. While having the disadvantage of having to learn over a markedly higher dimensional space, the advantage is that being a black-box model, the IML approach has a great degree of flexibility. Also, because it does not linearise the dynamics, it might be able to benefit from the intrinsic properties of the dynamics rather than cancelling them.


Learning inverse dynamic models with standard Gaussian process regression has been considered in \cite{GPbook:2006,Chai2008}. Analogous approaches have considered the application of nonparametric learning methods to learning inverse kinematics models \cite{souza2001} or inverse operational space models \cite{Petersoperational2006}.
To speed up computation of the IML problem, 
Tuong et. al. \cite{TuongSeegerPeters2009} propose to combine the predictions of many GPs (each having small data sets that contain localised information) to make local predictions at different parts of state space. 
A similar idea, but with linear prediction models, had been proposed in LWPR \cite{lwpr2000,Schaal2002}.
In their paper, Tuong et. al. \cite{TuongSeegerPeters2009} compare a variety of learning methods against standard Gaussian process regression (GPR) for learning inverse dynamic models of real robots. Their data suggests that GPR requires high computational cost for learning and prediction but outperforms other methods such as LWPR \cite{lwpr2000} and LGP \cite{TuongSeegerPeters2009} in terms of prediction accuracy and tracking performance. 
Therefore, we use the standard method of GPR as a baseline method. We will refer to the resulting controller as \textit{GP-IMLC} and compare it to a controller that substitutes the GP by our kinky inference learner. This new controller will be referred to as \textit{kinky inference inverse model learning (KI-IML)}.

Note, in order to speed up computation, we could just as easily apply the localisation methods proposed in \cite{TuongSeegerPeters2009} to the KI-IML approach. This may also have the advantage that a variety of localised KI-IML models would benefit from local estimates of the H\"older constant.
For now however, we restrict ourselves to the standard method. To test the viability of kinky inference for inverse dynamics model learning and control, we repeated our double-pendulum experiment from the previous subsection. This time, we replaced the KI-SSIC controller by KI-IML. 

Note, without further modifications,  KI-IML per se does not lend itself to online learning. To see this, consider a situation where at time $t$, the system is in a state $x(t) = [q(t);\dot q(t)]$ where the KI rule is very uncertain about the reference acceleration $\ddot q_{ref}$. Furthermore, if the present acceleration $\ddot q$ is very distinct from the desired one $\ddot q_{ref}$ (as measured by metric $\metric_\inspace$) then incorporation of a new sample point $\Bigl( \bigl(q(t), \dot q(t), \ddot q(t)\bigr),u\bigl(q(t), \dot q(t), \ddot q(t)\bigr)\Bigr) $ provides little information about the control $u\bigl((q(t), \dot q(t), \ddot q_{ref} \bigr)  \bigr)$ for the reference acceleration $\ddot q_{ref}$. This may mean that the controller may fail to reach the goal. As a solution, it might be conceivable to feed the controller a reference trajectory that smoothly connects the true reference with the current state and acceleration. However, we will not consider this any further. Instead, we will confine our exposition to offline learning.




To this end, we generate an offline sample set $\data$ containing tuples of the form $\Bigl( \bigl(q, \dot q, \ddot q\bigr), u \Bigr) $. The sample inputs $\bigl(q, \dot q, \ddot q\bigr)$ were drawn uniformly at random from the hyperrectangle $[-10,10]^4 \times [-50,50]$. That is, the state training example inputs were allowed to assume values between $-10$ and $10$ while we drew  accelerations from the larger interval [-50,50]. The H\"older constant was adapted to the random sample utilising the lazy update rule outlined in Sec. \ref{sec:lazylipconstupdate}. On the basis of the accordingly adapted KI rule, the pertaining control was determined by inverting the ground-truth dynamics with these inputs (and adding a bit of noise of magnitude up to 0.01). 

We tested the control performance of KI-IML in our double-pendulum test-bed. Here, KI-IML made inference  about the correct control on the basis of the sample $\data$. As a reference acceleration, we chose $\ddot q_{ref} = w(\xi_1 - q) + w (\xi_2 -\dot q)$ where $\xi_1 $ was the goal angle setpoint and $\xi_2$ the goal angular velocity. This choice means that in the limit of dense data, a perfectly trained KI-IML controller would control the system such that the closed-loop dynamics would mimic a double-integrator controlled by a P-controller with setpoint $\xi$.


\begin{figure}
        \centering
			\subfigure[Double-pendulum controlled by KI-IMLC.]{\label{fig:IML0}
    \includegraphics[width = 6.5cm]{content/Ch_kinkyinf/figs/HfeIML_animatehist2}
  }
				\subfigure[Error evolutions (1st try).]{\label{fig:IML1}
    \includegraphics[width = 4.5cm]{content/Ch_kinkyinf/figs/distIMLvSIICpend2}
  }
%
	\subfigure[Error evolutions (2nd try).]{\label{fig:IML2} 
    \includegraphics[width = 4.5cm]{content/Ch_kinkyinf/figs/distIMLvSIICpend2_allhyperparoptbadGPIML}
  }				

%
	%\subfigure[Error evolutions (2nd try).]{\label{fig:IML2} 
    %\includegraphics[width = 3.9cm]{content/Ch_kinkyinf/figs/distIMLvSIICpend2_allhyperparoptbadGPIML}
  %}
	%
 	%\subfigure[Error evolution  (20 trials).]{\label{fig:hfesiic23} 
    %\includegraphics[width = 4cm, clip, trim = 0cm 0cm 0cm 0cm]{content/Ch_kinkyinf/figs/HfeSIIC_sqrddist20ep}
  %}
	 	%\subfigure[Control evolution  (after learning).]{\label{fig:hfesiic4} 
    %\includegraphics[width = 4cm, clip, trim = 0cm 0cm 0cm 0cm]{content/Ch_kinkyinf/figs/hfesiic4}
  %}


        \caption{Fig. \ref{fig:IML1}: Example of KI-IML being successful at driving a learned double-pendulum to goal state $\xi = [\pi;\pi/2;0;0]$. Fig. \ref{fig:IML1}: Depicted is a comparison of the error evolutions, i.e. the distances to the goal as a function of time (seconds), of the control task solved by different methods. All methods managed to stabilise the system well on the basis of the randomly generated data set. Fig. \ref{fig:IML2} exposes a recurring issue we encountered with GP-IML. Depending on the randomly generated data set, GP-IMLC sometimes failed. We attribute this to the outcome of hyperparameter optimisation.
}
\label{fig:IML1}
\end{figure}	




As a first illustration, consider Fig. \ref{fig:IML0}. On the basis of a random data set of size 20, and with reference parameter $w =20$, we tasked KI-IMLC to control the double-pendulum to reach the goal state $\xi=[\pi;\pi/2,0;0]$. In this situation, KI-IMLC had no difficulty. As a first comparison, we redid the experiment with the different the different algorithms GP-IMLC, RF-SIIC (from Ch: \ref{ch:SPSIIC}), KI-IMLC and KI-SIIC. For the purposes of the comparison, the SIIC methods were trained offline on the same randomly generated data set as the other methods. As in previous experiments, the initial state was $x_0=[0;0;-1;-1]$ and the goal state was chosen to be $\xi=[\pi;\pi/2,0;0]$. The random field methods, were allowed to perform hyperparameter optimisation.
In the first run (Fig. \ref{fig:IML0}), all methods accomplished the task with little rest error.
However, repeating experiments like this several times, we frequently encountered GP-IMLC struggling on a variety of instances. Inspecting the trained hyperparameters gives us reason to conjecture that this might be due to the fact that the optimisation hyperparameter optimisation procedure failed to produce good results and got stuck in a local minimum of the marginal posterior log-likelihood. Note, we did not register this problem with any of the other methods. One explanation might be the fact that, owing to the increased dimensionality of the input space in the IMLC setup, GP-IMLC had to optimise over a higher-dimensional space than RF-SIIC. As a note of caution though, is that in the present setup, both methods are not entirely comparable on a level playing field. This is results from the fact that in the current setup, GP-IMLC generated the control by imitating a feedback controller via pure learning. By contrast, the feedback pseudo-control employed in the SIIC method may be able the compensate for some learning error (provided the model over $b(x)$, as defined above, is not profoundly off). 

While time did not permit this, in future work, we would like to make a more extensive comparison on higher-dimensional robotic tracking tasks. Here, we would endow the methods with forward references (instead of eror feedback). We anticipate that this will increase comparability and shed light on the respective inherent benefits and drawbacks more clearly. 

%
%%Simulations}
%In the related work that has applied GP-IML to robotic tracking (e.g. \cite{tuongACC2008}) the authors trained the models with large numbers of data points (around 1000) and on exactly  
%
%Note that the random generation of the offline data set gives the learners a harder task than in the related literature on GP-IML tracking in robotics \cite{tuongACC2008,TuongSeegerPeters2009}. In lieu to imitation learning, in these works, the authors trained their models with large data sets that contained records from the robot performing the desired tasks. This means that the data sets contained exactly the right kind of information. Similarly, in the online learning scenarios considered in Ch. \ref{ch:SPSIIC} and in the previous subsection, the data sets could be kept sparse since the controller automatically decided whenever a data point promised to be informative. 
%
%By contrast, the random selection of data points in the simulations of this section means that the majority of examples will bear little information about the situations that are encountered during online control.
%This means we will require much larger data sets to guarantee comparable density in those parts of state (and acceleration) space that matter.

