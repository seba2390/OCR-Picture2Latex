\section{Uncertain Lipschitz constants}
Throughout the previous section we have assumed that the Lipschitz constants were known to us. 
However, in many cases, we may be uncertain about the best Lipschitz constant. We assume the uncertainty is expressed as a distribution. Adopting an epistemological point of view, we interpret probability densities over events as degrees of beliefs that these events hold true (see Jaynes Book for a discussion). 
In the following subsection, we describe how to utilize the densities over the best Lipschitz constant $L* \in \Real_+$ to yield a (once again, conservative) bound on the probability of our functions estimates and integrands not being conservative. After that, the subsequent subsection will be devoted to the question of how to update our beliefs over $L*$ in the light of new function evaluations.

\subsection{Conservative bounds with uncertain Lipschitz constants}

Let $p: \Real_+ \to \Real_+$ be a density encoding our belief over best Lipschitz constant $L^*$ of target function $f$.
Assume we construct a Lipschitz enclosure $\einschluss (\ell)$ based on choosing $\ell \in \Real_+$ as a Lipschitz constant.  How large do we need to choose $\ell$ to guarantee that $f$ is completely contained in the enclosure ? Since we are uncertain about the true Lipschitz constant, this question can only be framed probabilistically. That is, for a given certainty threshold  $\theta \in (0,1)$ we desire to find $\ell > 0$ such that 
\[ \theta \leq \Pr[ f \in \einschluss (\ell) ]. \]  

\begin{thm}
Let $P: t \mapsto \int_{x \leq t} p(x) \,dx $ our density's cumulative distribution function (cdf).
We have, \[P(\ell) \geq \theta \Rightarrow \theta \leq \Pr[ f \in \einschluss (\ell) ].  \] 
\begin{proof} 
Note, as proven above $\forall \ell \geq L :   \Pr[ f \in \einschluss (\ell) | L^*=L] = 1$. 

Hence,
$
\Pr\bigl[ f \in \einschluss (\ell) \bigr] 
= \int_0^\ell \Pr\bigl[ f \in \einschluss (\ell) | L^*=L\bigr] \,dP(L) 
 + \int_\ell^\infty \Pr\bigl[ f \in \einschluss (\ell) | L^*=L\bigr] \,dP(L) 
=  \int_0^\ell \,dP(L) + \int_\ell^\infty \Pr\bigl[ f \in \einschluss (\ell) | L^*=L\bigr] \,dP(L) \\ 
\geq \int_0^\ell \,dP(L) 
= P(\ell).$

\end{proof}
\end{thm}

That is to say, in order to guarantee conservativeness of our enclosure, all we need to do is to compute it on the basis of 
a fixed Lipschitz constant that is just large enough such that $P(\ell) = \int_0^l p(L) \,dL \geq \theta$. Notice, if the integral cannot easily be determined in closed-form, but a Lipschitz constant for $p$ is known, we can employ our method described previously (based on a known constant), to evaluate $P(\ell)$ conservatively. 

\begin{cor}
Let $S := \int_I f(t) \, \d t$. 
With the definitions introduced above, we have  \[P(\ell) \geq \theta \Rightarrow \theta \leq \Pr\Bigl[ \hat S^\decke_N \geq S \geq \hat S^\boden_N  \Bigr]  \] 
where $\hat S^\decke_N , \hat S^\boden_N $ where computed assuming a Lipschitz constant of $\ell$.

\end{cor}

\subsection{Learning a posterior over the Lipschitz constant from data}

Assume we hold a prior belief over the best Lipschitz constant $L^*$ encoded as a density $p_0 : I \subset \Real_+ \to [0,1]$.
Assume we are given a sample $G = \{x_i,y_i\}_{i=1,...,N}$ of input-function value pairs, $x_i \in \mathcal X$, $y_i = f(x_i) \in \outspace, \forall i$, the question arises of to calculate a posterior in the light of the new data. 

Since we assume $f :\mathcal X \to \mathcal Y$ is Lipschitz we have $ \frac{d_\outspace(y_i,y_j)}{d_{\inspace}(x_i,x_j)} \leq L^*,  (i,j =1,...,N, x_i \neq x_j)$ where $L^*$ is the best Lipschitz number. So, our observations allow us to infer that $L_D := \max_{i,j,i\neq j} \frac{d_\outspace(y_i,y_j)}{d_{\inspace}(x_i,x_j)} $ as a lower bound on the best Lipschitz number. 
The remaining question is how to derive a posterior over $L^*$ based on this newly observed lower bound. 

\begin{ques}
How can we prove that there is no information about the Lipschitz constant in the data other than $L_D$? That is, in particular, that \[p(L^*|D) \stackrel{!}{=}p(L^* |L_D) ??\]
\end{ques}


We will discuss two approaches, the Minimum-Relative-Entropy approach and Bayesian inference.

\subsubsection{Minimum-Relative-Entropy Approach} 

Computing posteriors from priors subject to constraints is the focus of max-ent inference. 

Remember, 
\begin{defn}[Relative Entropy] For two densities $p,q: I  \to [0,1]$ the \textit{relative entropy} or \textit{Kullback-Leibler divergence} is defined as 
\[\KLD {p}{q} = \int_{I} p(x) \log \frac{p(x)}{q(x)} d\mu(x)\] where $\mu$ is a measure appropriate for domain $I$.
\end{defn}

For example, if $I \subset \Real$ then $\mu$ is normally tacitly assumed to be the standard Lebesgue measure. If $I$ is a discrete set, $\mu$ is noramlly assumed to be the counting measure. In this case \[\KLD {p}{q} = \sum_{x\in I} p(x) \log_2 \frac{p(x)}{q(x)}. \]

For our problem, we can pose the desired posterior density as the solution to the variational problem:

\begin{align}
\argmin_p & \KLD {p}{p_0}\\
\text{s.t.:}&\\
& p(l) \in [0,1], \forall l \geq 0\\
&\SP{p}{e}_{L_2(\Real)} =1\\
& \SP{p}{\chi_{[0,L_D]}}_{L_2(\Real)} =0
\end{align}

where $e: t \mapsto 1$ is the constant mapping to $1$ and $\chi_{[0,L_D]}$ is the indicator function for interval $[0,L_D]$.

\begin{ques}
Can we solve this variational problem in closed form? How can we solve them numerically. If we have no clue, we can make everything discrete as follows...
\end{ques}

\emph{Discretization}.
For practical purposes, it will be convenient to reduce the problem to a standard convex optimization problem as follows:
Let $J_1,...,J_m \subset I$ be a partition of $I\subset \Real_+$.
Instead of expressing a belief as a density on a continuous interval we may limit our modelling efforts to defining a belief as a discrete distribution function $\pi_0 : \{ 1,...,m\} \to [0,1], i \mapsto \Pr[ L \in J_i]$.  

\begin{rem}
Notice, that this belief encoding may be easier to specify than the density on a continuum of points. 
For instance, assume $I = [a,b] , 0<a<b \in \Real_+ \cup \{\infty\}$ and let $a=t_0<t_1<...<t_m =b$ such that $J_i = [t_{i-1},t_i], (i=1,...,m)$.
Then, knowing the continuous density $p_0$ on $I$ allows us to compute the discrete density via $ \pi_0 (i) = \int_{J_i} p_0(t) \,dt=P_0 (t_i) - P_0(t_{i-1})$. The inverse computation (from discrete to continuous distribution) is not possible without further assumptions.
\end{rem}

We anticipate our discretization to spawn information loss, since we only encode information about the interval not about the relative location of the Lipschitz number within the interval. This will be investigated next.

%Let $\tau \in (0,1]$ denote this relative interval coordinate. That is to say, Lipschitz number $L = t_{i-1}+ \tau (t_i -t_{i-1}) $ is completely given by coordinates $(i,\tau) \in \{1,...,m\} \times (0,1]$ . 
%
%After discretization, our uncertainty about the exact Lipschitz number (not just the interval) is described by $\tilde p_0(L) =  \pi_0(i) \, p_\tau[\tau | i ]$. Assuming that, after discretization, we are completely oblivious about the location within the interval, we have $p_\tau [\tau | i ] = \frac{1}{| (0,1] |} =1$.
%Hence, 
%
%The magnitude of the information loss due to discretziation could be quantified as 
%\begin{align*}
 %H_{\tilde p_0}-H_{ p_0}  &=\int_I p_0(t) \log p_0(t) \, dt  - \bigl(\sum_{i=1}^m \pi_0(i)   \log \pi_0(i) \bigr) \\
%& =  \sum_{i=1}^m \int_{J_i} p_0(t) \log p_0(t) \, dt  \\ 
%&- \bigl(\sum_{i=1}^m \int_{J_i} p_0(t) \,dt  \log \int_{J_i} p_0(t) \,dt \bigr)\\
%% 
%& =  \sum_{i=1}^m \Bigl(\int_{J_i} p_0(t) \log [p_0(t)] \, dt  \\
%&-   \int_{J_i} p_0(t) \,dt  \log [\int_{J_i} p_0(t) \,dt] \Bigr)\\
%& \stackrel{!?}{\leq} 0
%\end{align*}
%
%
%WEIRD !!!  The loss should be greater than 0, not smaller ! 
%Mhh... perhaps I should instead try this (?? Makes any difference??)

In the discretized situation, our ignorance over $L^*$ can be encoded by the density  $\tilde p_0(L) =  \sum_{i=1}^m \pi_0(i) \, p_L [L| i ]$. Assuming that, after discretization, we are completely oblivious about the location within the interval, we have $p_L [L | i ] = \frac{1}{| J_i |} =\frac{1}{| t_i-t_{i-1} |}$.
Hence, $\tilde p_0(L)  =  \sum_{i=1}^m  \frac{\pi_0(i)}{| J_i |} $. 

The magnitude of the information loss due to discretziation could be quantified as 
\begin{align*}
 H_{\tilde p_0}-H_{ p_0}  &=\int_I p_0(L) \log p_0(L) \, dL  - \int_I \tilde p_0(L)    \log  \tilde p_0(L) \, dL \\
& = \int_I p_0(L) \log p_0(L) \, dL  \\
&- \int_I\sum_{i=1}^m \pi_0(i) \, p_L [L| i ]    \log \bigl(\sum_{i=1}^m \pi_0(i) \, p_L [L| i ]\bigr) \, dL \\
& = \int_I p_0(L) \log p_0(L) \, dL  \\
&- \int_I\sum_{i=1}^m \frac{\pi_0(i)}{|J_i|}     \log \bigl(\sum_{i=1}^m \frac{\pi_0(i)}{|J_i|} \bigr) \, dL \\
& = \int_I p_0(L) \log p_0(L) \, dL  \\
&- |I|  \log \bigl(\sum_{i=1}^m \frac{\pi_0(i)}{|J_i|} \bigr) \sum_{i=1}^m \frac{\pi_0(i)}{|J_i|}     \\
& =  \sum_{i=1}^m \Bigl(\int_{J_i} p_0(t) \log [p_0(t)] \, dt \Bigr)   \\
&- |I|  \log \bigl(\sum_{i=1}^m \frac{\int_{J_i} p_0(t) dt}{|J_i|} \bigr) \sum_{i=1}^m \frac{\int_{J_i} p_0(t) dt}{|J_i|}     \\
& \stackrel{!?} {\geq }0
\end{align*}

 
However, the benefit is that (continuous) extremization problem is reduced to a tractable, discrete optimization problem of dimensionality $m$. In fact, it is a convex program with linear constraints:

\begin{align}
\argmin_p & \KLD {\pi}{\pi_0}\\
\text{s.t.:}&\\
& \pi(i) \in [0,1], \forall i \in\{1,...,m\}\\
&\SP{p}{e}_{2} =1\\
& \SP{p}{n_{L_D}}_{2} =0
\end{align}
where $e$ is a vector of $m$ ones and $n_{L_D}$ is suitably defined to mimic a discretized version of the indicator function above and whose $i$th component is defined as follows:

$n_{L_D} (i) = \begin{cases} 1, t_i \geq L_D\\ 0, \text{otherwise.} \end{cases}$ Of course, if $t_{i-1} < L_D < t_i$ for some $i$, we have once again thrown away information,as the constraint $\SP{p}{n_{L_D}}_{2} =0$ does not capture the knowledge that there must be zero probability mass on $K_i' := [t_{i-1},L_D)$ as well. To cover this, we could consider 
introducing an additional component representing the probability on $K_i$  and replace the original meaning of the original $i$th component by representing the probability on  $J_i' = [L_D, t_i] =J_i - K_i$ instead of on $J_i$. In this case, the CP becomes $m+1$-dimensional. Since this is always possible to do, we can assume, without loss of generality, that $L_D $ will always be an element of $t_1,...,t_m$ for some $m$.


Note, an $m$-dimensional CP can be solved in worst-time complexity $\mathcal O(m^3)$ (LOOK IT UP!!).


\begin{rem}
We could investigate how the information loss (which we would expect to grow as a function of $m$) compares to the 
the gain in computational complexity. We should then conceive a common exchange rate / currency / trade-off parameter to 
find a good cut-off point.
\end{rem}

\subsubsection{Bayesian approach}

By Bayes theorem, we can write 
\[p(L|L_D) = \frac{p(L_D|L) \, p_0(L) }{ \int_I p(L_D|L) \, p_0(L) \, \d L}.\]

If we are uncertain about which empirical Lipschitz constant we observe given the real Lipschitz constant  $L^* = L$ we set the Likelihood function 
\[ p(L_D|L) = \begin{cases} 0, L_D > L\\ \frac{1}{L}, otherwise. \end{cases}.\]

Of course, if the definite integrals of prior $p_0$ are not known in closed form, we need to approximate numerically. This is generally unproblematic, as the 
%\begin{ques}[@Mike]
%What is the likelihood function $p(L_D|L)= ... ?$
%\end{ques}