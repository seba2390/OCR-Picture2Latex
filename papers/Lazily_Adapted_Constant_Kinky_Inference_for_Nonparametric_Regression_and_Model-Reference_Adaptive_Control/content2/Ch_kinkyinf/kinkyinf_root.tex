%\begin{abstract}
%\begin{quote}
%\input{content/content_hoelderquad2/abstract}
%\end{quote}
%\end{abstract}

%\noindent
%In this chapter, we propose a method for inductive inference that yields error bounds on the accuracy of the drawn conclusions. 
%The approach yields a non-parametric machine learning method which is applied to system identification and control. The provided error bounds are an attractive property since they offer guidelines for adjusting the control to take the remaining uncertainty of a partially identified system into account. This is shown to be beneficial in settings where we desire to impose robustness constraints on collision avoidance or stability and helps to guide state space exploration in active learning. Based on our nonparametric inference rule, we construct a variety of learning-based controllers for some of which we provide guarantees of stability and construct robust tubes. In addition to our theoretical guarantees, our simulations suggest that kinky inference is a very flexible and fast learning approach that is well suited for learning and controlling dynamical systems and that it can outperform state-of-the-art methods. 
%
%
%
%
\vspace{-3em}
\section{Introduction}
In order to generalise beyond an observed sample, inductive inference needs to make assumptions about the underlying ground truth function. In this chapter, we will derive non-parametric learning and inference methods that fold in knowledge about boundedness, H\"older continuity and error bounds on the observations and inputs. Since these types of a priori knowledge impose weak forms of regularity, the method will be applicable to non-smooth functions that contain kinks- a property that is reflected in the predictions of the inference method and that gave rise to the catch-phrase \emph{kinky inference}.
While weak enough an assumption to encompass rich function classes, the approach has the advantage of yielding bounds on the inference.

The approach yields a non-parametric machine learning method which is applied to system identification and control. The provided error bounds are an attractive property since they offer information on bounded stability and provide guidelines for adjusting the control to take the remaining uncertainty of a partially identified system into account. This can be beneficial in settings where we desire to impose robustness constraints on collision avoidance (see Sec. \ref{sec:KIMRAC}) or stability and can help to guide state space exploration in active learning. Based on our nonparametric inference rule, we construct a variety of learning-based controllers for some of which we provide guarantees of stability and construct robust tubes. In addition to our theoretical guarantees, our simulations suggest that kinky inference is a very flexible and fast learning approach that is well suited for learning and controlling dynamical systems and that it can outperform state-of-the-art methods. 


%\subsection{Related Work}
%Supervised machine learning algorithms construct models of functions to generalise from a data set of function values. We are interested in developing a method that can quantify the uncertainty around the predictions inferred from the learned model. Few methods seem able to do so, with the exception of Bayesian methods. To the best of our knowledge, Gaussian process regression is the only approach in Bayesian inference that combines the capablity to learn rich function classes with analytic tractability on the other (cf. Sec. \ref{GPbackground}, \cite{GPbook:2006}).
%%
%%By contrast, Gaussian processes learning is an example of a nonparametric machine learning method (cf. Sec. \ref{GPbackground}). 
%
%As is the case with most nonparametric learning methods, the computational complexity for training and prediction of GPs grows with number of training examples. In fact, the computational complexity for training is cubic and the computational complexity for inference (prediction) is quadratic in the number of data points \cite{GPbook:2006} (although the former can be reduced to quadratic either approximately \cite{Gibbs1997} or exactly for specific cases \cite{Storkey1999} and the latter effort can be reduced to linear growth by recent SDE conversion techniques \cite{Reece2010,SimoAISTATS2012}). As in most non-parametric methods, the increased computational complexity is rewarded with a reduced inductive bias. That is, when choosing the right covariance function, Gaussian processes can learn any smooth function. Unfortunately, to date it is not well-understood how to fold in the uncertainty quantifications of GPs into the predictions of state-dependent system dynamics in a conservative (that is uncertainty-preserving) manner. This is important if we desire to give error bounds on the control success of a control law that is based on an uncertain dynamics model identified by a GP.
%
%In this work, we go an alternative route. Instead of placing a prior measure over some probability space of function values, we leverage a priori knowledge about H\"older continuity and boundedness of the ground-truth function.
%%
%%In order to generalise beyond unobserved function values kinky inference utilises assumptions of H\"older regularity and, in so far available, information of boundedness of the ground truth function. 
%
%H\"older continuity is a generalisation of Lipschitz continuity. While not being harnessed much within the artificial intelligence and machine learning communities, Lipschitz properties are widely used in applied mathematics to establish error bounds and, among many other, finds application in optimisation \cite{Shubert:72,direct:93} and quadrature \cite{Baran2008,curbera1998,dereich2006}. Upon submission of this work, we also have become aware of related work in function estimation and interpolation \cite{Cooper2006,Cooper1995,Zabinsky2003}. Zabinsky et. al. \cite{Zabinsky2003} consider the problem of estimating a one-dimensional Lipschitz function using a ceiling and floor function as bounding functions an making predictions by taking the average of these functions. Our inference rule can be seen as a generalisation of their approach. Our method provides extensions to multi-dimensional H\"older continuous functions with erroneous observations and inputs, can fold in additional knowledge about boundedness, learn parameters from data and provides guarantees such as uniform convergence of the error. As part of the analysis of our method we construct bounding functions we call \textit{ceiling} and \textit{floor} functions. The construction of similar functions is a recurring theme that, in the Lipschitz context, can be found in global optimisation \cite{Shubert:72} and quadrature \cite{Baran2008} as well as in the analysis of linear splines for function estimation \cite{Cooper1995,Zabinsky2003}. Cooper \cite{Cooper2006,Cooper1995} utilises such upper and lower bound functions in a multi-dimensional setting to derive probabilistic PAC-type error bounds \cite{Valiant1984} for a linear interpolation rule assuming the data is sampled uniformly at random on a hypercube domain. Our inference rule is different from this and our guarantees do not rely on distributional assumptions, although there is some overlap in the ideas for analysing generalisation performance. In future work, we would be interested in utilising Cooper's statistical analysis techniques to extend our guarantees to stochastic settings.
%
%None of the aforementioned works seems to be able to cope with interval-bounded errors, folds in additional knowledge such as boundedness or considers adaptation of the Lipschitz (H\"older) constant to the data.
%Finally, we are not aware of any work that employs these methods in the context of system identification, control or collision avoidance as we do.
%\subsection{Contributions}
%\input{content/Ch_kinkyinf/preliminaries.tex}
%\input{content/Ch_kinkyinf/topological_inference.tex}
%\input{content/Ch_kinkyinf/problem_def}


%
%\input{content/Ch_kinkyinf/est_multidim}
\input{content/Ch_kinkyinf/KI_core}
%\input{content/Ch_kinkyinf/KI_theory_derivations}

\input{content/Ch_kinkyinf/constantadaptation}


\input{content/Ch_kinkyinf/SIctrl_Hfe}
\input{content/Ch_kinkyinf/KISIICKIIML}
%\input{content/Ch_kinkyinf/quad1d}

%\input{content/Ch_kinkyinf/KIMPCcollavoid.tex}

%\input{content/Ch_kinkyinf/optimisation}


%\subsection{Multi-dimensional domains via Fubini's Theorem}
%\label{sec:quadr_multidim}
%\subsection{Optional: Dumping information to avoid the curse of dimensionality.}




\section{Conclusions}
This chapter has introduced the kinky inference framework as an approach to inductive inference in dynamical systems. Folding in knowledge about boundedness and H\"older continuity (with respect to an arbitrary pseudo-metric) the method has been demonstrated to be able to infer unobserved function values successfully. Assuming any observational or input uncertainties are interval-bounded, the method is capable of providing bounds around its predictions. That is, for any finite sample we have shown that the inferred function value is correct up to lying within a given hyperrectangle. We have shown that this uncertainty vanishes (uniformly) in the limit of data sets that (uniformly) converge to a dense subset of input space. Provided no additional a priori knowledge about the target function is known, our bounds are provably tight. We have discussed several variations of the kinky inference rule, including a smooth version, the capability of handling input uncertainty and we have provided two methods for updating beliefs over the H\"older constant. 


We have discussed applications to system identification and control in uncertain dynamical systems highlighting the benefits (and some of its shortcomings) of kinky inference in this application domain. 
In particular, the benefits are rapid learning and prediction, the error bounds around the predictions as well as the flexibility to learn rich function classes. 

In the design of learning-based controllers, the uncertainty bounds afforded by our KI rule are a particularly useful feature. These allowed us to convert the prediction bounds into bounds on the error dynamics. That is, assuming the prior assumptions are correct (and no H\"older constant updates are necessary),  we have obtained stability bounds for the closed-loop dynamics. 



To highlight the applicability of KI in control, we have provided simulations for the different controllers in the model-reference adaptive control framework, in combination with our SIIC control approach of Ch. \ref{ch:SPSIIC} and in inverse dynamics model learning and control. Apart from simple illustrations in the control of double-pendula, we have also provided simulations of KI-MRAC in tracking control problem of the rudder of an unstable fighter-aircraft in the presence of wingrock. Here, KI-MRAC proved competitive with recent state-of-the art algorithms in the test scenario considered in \cite{Chowdhary2013,ChowdharyCDC2013,chowdharyacc2013}. Furthermore, across the board of a several hundred of randomized problem instances, KI-MRAC largely outperformed all competing controllers. 

In Ch. \ref{ch:discrcoll}, we will present an additional application in the context of multi-agent control. There we will utilise the uncertainty bounds afforded by the kinky inference rule to construct robust tubes. These will be converted into robust collision avoidance constraints that are incorporated into the optimisation problem of a predictive controller. This construction will yield a multi-agent controller for KI learners that folds in the posterior knowledge about an uncertain drift.

\textit{Future work} will be invested in extended tests on realistic, higher-dimensional systems and explore the benefits and weaknesses of the different approaches. To this end, we will employ results on input and observational noise to a greater extent than we had to in our proof-of-concept simulations. 
Other extensions of interest pertain to the H\"older constants. At the moment, these are global estimates. In a spirit similar to localised model learning \cite{lwpr2000,TuongSeegerPeters2009}, it may be beneficial to entertain a number of local KI models, each basing their inferences and constant estimates on local information. This might be beneficial not only from a computational point of view, but also provide improved inferences if there is a large variation of the H\"older constant across state space.

Another avenue would be to address the problem of data scarification. That is, given a sample size budget, which data points are most informative? 

From a theoretical point of view, we believe to be able to prove bounds on the error dynamics of KI-IMLC. As the treatment of \cite{Tuongmodellearningsurvey2011} indicates, this would be the first time a guarantee on an inversion model learning-based controller would be given.



%\subsection{Extensions and Future Work}
%\label{sec:kinkyinf_futurework}
%In this chapter, we have considered kinky inference. Our exposition focussed on the simple case where the output space metric was given by a standard norm on finite-dimensional vector space. Future work will examine 
%taking advantage of the generality of the notion of a norm and will look into application to more general metric spaces. An immediate extension would be to the learning of mean-square integrable stochastic processes. 
 %Here the metric could be the \emph{natural semi-metric} $\metric(X,Y) = \expect{ (X-Y)^2}$ which is known to induce a topology on the index set of the process \cite{Piterbarg1991}. 
%
%Additional work will examine our learning methods when applied to real mechanical systems. 
%
%Moreover, extending  \cite{Baran2008}, we have work under preparation for cubature of H\"older continuous functions. Here the simple idea is to integrate the ceiling and floor functions (which are integrable) in closed form. Since we have proven that both of these bounding functions converge uniformly in the limit of increasing data size $N_n$ (provided the grids converge to a dense set of the domain), convergence of the integral estimate is implied. 
%
%The analysis conducted in the context of cubature error may also become of relevance to study the function approximation error of the kinky inference learner and to give optimal active sampling guarantees. 
%So far, we have only given convergence guarantees to the ground truth in the limit of a dense subset of the input space. While we have given error bounds for each prediction for a given sample, we have not provided error bounds for the entire function approximation error specified, for example, with respect to some norm over functions. Questions of interest include how one might sample optimally in order to decrease the error at a maximal rate. If samples are drawn from a distribution, how does the choice of distribution affect the error convergence rate? How does the choice of input metric affect things? And what are the computational trade-offs? That is, if one is subject to diminishing returns, when should one stop sampling? Here, one might explore in how far the analysis provided by Cooper \cite{Cooper2006} in the context of linear spline interpolation rules might be transferred to our kinky inference situation. As an alternative, we have recently become aware of the field of scattered data approximation \cite{wendlandscattereddata2005}, which seems to share a great many goals with the machine learning community. Studying our problems under the lens of the tools and frameworks developed by this community may be a fruitful direction to consider. \jcom{perhaps this in related work}




%
%\appendix{OLD stuff:}
%
%\input{content/intro}
%\input{content/est_enclosingfcts}
%\input{content/quad_shubertstyle}
%\input{content/multidimdom}
%\input{content/intersectionsofenclosure}
%\input{content/gridrefinements}
%\input{content/uncertainLipconst}
%\input{content/notes}

%\begin{small}
%\bibliographystyle{IEEEtran}
%\bibliography{content/lit}
%\end{small}
