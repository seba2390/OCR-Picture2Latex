%\begin{abstract}
%We propose a method for inductive inference that yields error bounds on the accuracy of the drawn conclusions. 
%The approach yields a nonparametric machine learning method with theoretical guarantees. We apply the method to system identification and control and show how the prediction bounds can be utilised in collision avoidance under uncertain dynamics. Here the constituent agents learn to update their dynamics models and take the rest uncertainty into account properly in order to compute plans that are guaranteed to avoid collisions.
%\end{abstract}
%\begin{abstract}
%An approach for inductive inference that yields error bounds on the accuracy of the drawn conclusions is considered. Generalising existing work spread across various communities under headlines such as \emph{Lipschitz Interpolation} or \emph{Nonlinear Set Interpolation (NSI)}, the approach can be understood as a nonparametric machine learning method with theoretical guarantees. In the basic setup, the method can learn any H\"older continuous function with given parameters. In this paper, it is shown how to adapt parameters to the data such that the resulting machine learning approach becomes a universal approximator in a strong sense. That is, the inference rule introduced can learn any continuous function on compact support up to an arbitrarily small error. The second part of the paper, applies the new inference rule to \emph{nonparametric model-reference adaptive control (MRAC)}. For discrete-time systems with uncertain drift stability of the resulting online learning-based controller (\emph{KI-MRAC}) is proven.
%Across a range of simulated aircraft roll-dynamics and performance metrics KI-MRAC outperforms recently proposed alternatives that were based on \emph{Gaussian processes} and \emph{RBF-neural networks}. 
%%We provide a proof of stability for our nonparametric adaptive controller. 
%\end{abstract}


\begin{abstract}
Techniques known as \emph{Nonlinear Set Membership} prediction, \emph{Lipschitz Interpolation} or \emph{Kinky Inference} are  approaches to machine learning that utilise \emph{presupposed} Lipschitz properties to compute inferences over unobserved function values. Provided a bound on the true best Lipschitz constant of the target function is known a priori they offer convergence guarantees as well as bounds around the predictions.
Considering a more general setting that builds on H\"older continuity relative to pseudo-metrics, we propose an online method for estimating the H\"older constant online from function value observations that possibly are corrupted by bounded observational errors. Utilising this to compute  adaptive parameters within a kinky inference rule gives rise to a nonparametric machine learning method, for which we establish strong \emph{universal approximation} guarantees. That is, we show that our prediction rule can learn any continuous function in the limit of increasingly dense data to within a worst-case error bound that depends on the level of observational uncertainty.
%To provide evidence that our method can be beneficial not only in theory but also in practice, 
We apply our method in the context of \emph{nonparametric model-reference adaptive control (MRAC)}. 
Across a range of simulated aircraft roll-dynamics and performance metrics our approach outperforms recently proposed alternatives that were based on \emph{Gaussian processes} and \emph{RBF-neural networks}. For discrete-time systems, we provide stability guarantees for our learning-based controllers both for the batch and the online learning setting. 
\end{abstract}