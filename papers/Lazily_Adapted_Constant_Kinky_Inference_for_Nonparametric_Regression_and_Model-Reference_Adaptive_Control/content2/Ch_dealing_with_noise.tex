\section{Dealing with noise}



\begin{figure}
        \centering
				  \subfigure[$L(0) = L(1)= 5$;$\obserrbnd = 0.5$; $\hestthresh =1$]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/LACKInoise_e_alpha_trueLinit_7tex.pdf}
    \label{fig:LACKInoise1}
  } 	
	 \subfigure[$L(0) =0.01, L(1) = 2.8 $;$\obserrbnd = 0$; $\hestthresh =0$.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/LACKInoise_noe_noalpha_smallLinit_7tex.pdf}
    \label{fig:LACKInoise2}
  } 	
  %
  %\hspace{2cm}
	  \subfigure[$L(0) =0.01, L(1) = 1.76 $;$\obserrbnd = 0.5$; $\hestthresh =1$.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/LACKInoise_e_alpha_smallLinit_7tex.pdf}
    \label{fig:LACKInoise3}
  } 
  				  \subfigure[$L(0) = L(2)= 5$;$\obserrbnd = 0.5$; $\hestthresh =1$.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/LACKInoise_e_alpha_trueLinit_77tex.pdf}
    \label{fig:LACKInoise4}
  } 	
	 \subfigure[$L(0) =0.01, L(2) = 14.9 $;$\obserrbnd= 0$; $\hestthresh =0$.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/LACKInoise_noe_noalpha_smallLinit_77tex.pdf}
    \label{fig:LACKInoise5}
  } 	
  %
  %\hspace{2cm}
	  \subfigure[$L(0) =0.01; L(2)= 3.18$;$\obserrbnd = 0.5$; $\hestthresh =1$.]{
    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
    \includegraphics[width = 4.1cm]
								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
								{content/figs/LACKInoise_e_alpha_smallLinit_77tex.pdf}
    \label{fig:LACKInoise6}
  } 	
   \caption{LACKI inference on the basis of two noisy data sets 
   $\data_1,\data_2$ with varying parameter settings. Target (blue curve) was $f:x \mapsto 5 \sqrt{\abs x}$ and observational model (magenta curve) $\tilde f: x \mapsto f(x) + \phi_x$ where $\phi_x$ is a uniformly distributed disturbance with $\phi_x \stackrel{i.i.d.}{\sim} \mathcal U(-0.5,0.5),\forall x$. 
  %
   \textbf{1st column:} Initial constant L(0) = 5; assumed obs. error bound $\obserrbnd= 0.5$; $\hestthresh = 2 \obserrbnd=1$. 
   %
   \textbf{2nd column:} Initial constant L(0) = 0.01; assumed obs. error bound $\obserrbnd = 0$; $\hestthresh = 2 \obserrbnd =0$.
   %
   \textbf{3rd column:} Initial constant L(0) = 0.01; assumed obs. error bound $\obserrbnd = 0.5$; $\hestthresh = 2 \obserrbnd =1$.
   %
   \textbf{Top row:} Data set $\data_1$ of size 7. 
   \textbf{Bottom row:} Data set $\data_2$ of size 77.
   In all cases the exponent parameter was set to $\hexp = 0.5$.}
			\label{fig:LACKInoise}
\end{figure}	

\subsection{Uncertain inputs}
So far, we have considered output uncertainty but not input uncertainty. That is, we have assumed that the function values $f(s_i)$ at the sample inputs are uncertain but that query inputs $x$ as well as sample 
inputs $s_i$ are known exactly. However, in many applications this assumption typically is not met. For example, in Sec. \ref{sec:hfeandcontrol}, we will employ KI for learning and controlling dynamic systems.
Here the target functions are state-dependent vector fields and the state measurements often are corrupted by noise (e.g. due to sensor noise or delays) which can affect both training data and query inputs. In the latter case, the controller might query the KI method to predict 
a function value $f(x)$ based on the assumption of the plant being in state $x$ while the plant may in fact be in a different state $x+\delta_x$. We would like to guarantee that our inference over the actual function value still is within the predicted bounds provided the input estimation error $\delta_x$ is known or known to be bounded. 
Fortunately, the H\"older property of the target can help to address the problem of quantifying the prediction error due to input uncertainty. For instance, consider $\metric(x,x') = \norm{x-x'}$. By H\"older continuity, the error of the prediction of the $j$th output component of the target function $f$ is bounded by $\abs{f_j(x+\delta_x) - f_j(x)} \leq \norm{\delta_x}^p$. 

Firstly, assume the training inputs $s_i$ were certain but the query inputs $x$ during prediction are uncertain.
Therefore, assuming we know an upper bound $\errinp(x)$ on the input uncertainty, i.e. 
$\norm{\delta_x}^p\leq \errinp(x), \forall x$, we can add this term to the uncertainty estimate $\prederr(x)$ as per Eq. \ref{eq:KIprederr}.
%This means that the additional uncertainty only affects the prediction of this particular query point. 

Secondly, consider the case were both the sample inputs $s_i$ and the query inputs $x$ are uncertain with the uncertainty being quantified by the function $\errinp(\cdot)$. In this case, the uncertainty of the sample inputs
 affects the posterior hypothesis (i.e. the enclosure) over $f$ and hence, the inferences over function values at all inputs. 
By the same argument as for the case of query noise, we accommodate for this fact leveraging the H\"older property. Instead of adjusting just the prediction uncertainty, we simply add $ \errinp(\cdot)$ to the error function $\varepsilon(\cdot)$ (that we have previously used to model observational 
error of the function values). That is, the observational uncertainty model has absorbed the input uncertainty.  Predictions with uncertain query points can then be done as per Def. \ref{def:KIL} (but with $ \errinp(\cdot)+ \varepsilon(\cdot)$ in place of $\varepsilon(\cdot) )$.