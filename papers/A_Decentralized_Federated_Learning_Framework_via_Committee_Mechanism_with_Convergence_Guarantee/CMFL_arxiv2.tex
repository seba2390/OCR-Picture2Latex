
%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


\usepackage{textcomp}
% \usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{tabularx}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[labelfont=bf,format=plain,justification=raggedright,singlelinecheck=false]{caption}
\usepackage{hyperref} 
\hypersetup{hypertex=true,
            colorlinks=true,
            linkcolor=red,
            anchorcolor=red,
            citecolor=red}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}[section]
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newcommand{\etal}{\emph{et al.} }
\newcommand{\w}{\mathbf{w}}
\newcommand{\s}{S_a^{(t)}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\p}{\mathcal{P}}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm
% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.



% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix



%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Decentralized Federated Learning Framework via Committee Mechanism with Convergence Guarantee}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Chunjiang Che,
 Xiaoli Li,~\IEEEmembership{Student Member,~IEEE,} \\
 Chuan Chen,~\IEEEmembership{Member,~IEEE,}
 Xiaoyu He,
 and~Zibin Zheng, ~\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space
 \thanks{Chunjiang Che, Xiaoli Li, Chuan Chen, and Xiaoyu He are with the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China. Zibin Zheng is the School of Software Engineering, Sun Yat-sen University, Zhuhai, China. e-mail: \{chechj, lixli27\}@mail2.sysu.edu.cn, \{chenchuan,
hexy73, zhzibin\}@mail.sysu.edu.cn.
 Corresponding author: Chuan Chen.}% <-this % stops a space
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Federated learning allows multiple participants to collaboratively train an efficient model without exposing data privacy. However, this distributed machine learning training method is prone to attacks from Byzantine clients, which interfere with the training of the global model by modifying the model or uploading the false gradient. In this paper, we propose a novel serverless federated learning framework \textit{Committee Mechanism based Federated Learning} (CMFL), which can ensure the robustness of the algorithm with convergence guarantee. In CMFL, a committee system is set up to screen the uploaded local gradients. The committee system selects the local gradients rated by the elected members for the aggregation procedure through the selection strategy, and replaces the committee member through the election strategy. Based on the different considerations of model performance and defense, two opposite selection strategies are designed for the sake of both accuracy and robustness. Extensive experiments illustrate that CMFL achieves faster convergence and better accuracy than the typical Federated Learning, in the meanwhile obtaining better robustness than the traditional Byzantine-tolerant algorithms, in the manner of a decentralized approach. In addition, we theoretically analyze and prove the convergence of CMFL under different election and selection strategies, which coincides with the experimental results. 
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Decentralized Federated Learning, Committee Mechanism, Byzantine Robustness, Theoretical Convergence Analysis.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

\IEEEPARstart{N}{owadays}, data arise from a wide range of sources, such as mobile devices, commercial, industrial and medical activities. These data are further used for training the Artificial Intelligence (AI) models applied in a variety of fields. The conventional AI methods always require uploading the source data to a central server. However, this is usually impractical due to data privacy or commercial competition. Federated Learning (FL) \cite{konecny2017federated}, which allows multiple devices to train a shared global model without uploading the local source data to a central server, is an effective way to solve the aforementioned problem. In FL settings, multiple clients (also known as participants) are responsible for model training and uploading the local gradients, while the central server is responsible for the model aggregation. A single round of FL mainly follows the following four steps: (1) the multiple clients download a global model from the server, and train their local models on their local datasets; (2) the clients upload the local gradients to the server, and the server aggregates the received multiple local gradients to construct the global gradient; (3) the server uses the global gradient to update the global model; (4) the clients download the global model to the local to continue the next training round. The above operations will be repeated until the algorithm converges. %After a series of iterations, the global model will have a higher accuracy rate than the local model.


%malicous-server \cite{10.1007/978-981-10-5421-1_9}\cite{8835245}

It is fascinating that FL can perform model training without uploading source data, McMahan \etal \cite{McMahanMRHA17} proposed that FL can achieve a similar test accuracy as the centralized method based on the full training dataset while providing stronger privacy guarantees. However, Lyu \etal \cite{DBLP:journals/corr/abs-2003-02133} proposed that the conventional FL is vulnerable to malicious attacks from the Byzantine clients and the central server. For example, the Byzantine clients upload false gradients to affect the performance of the global model, which may lead to training failure \cite{Blanchard2017Byzantine}\cite{247652}. Besides, the presence of malicious server is widely considered in FL\cite{8737416}\cite{9293091}\cite{DBLP:journals/corr/BonawitzIKMMPRS16}\cite{DBLP:journals/corr/abs-2109-14236}, and Hu \etal \cite{abs-2010-10996} proposed that external attacks on the central server will cause the entire learning process to terminate. In recent years, there have been a lot of works to solve the security problem of FL. Some works aim to design a robust aggregation rule to reduce the negative impact of malicious gradients\cite{munozgonzalez2019byzantinerobust}. For example, Blanchard \etal \cite{Blanchard2017Byzantine} proposed a Byzantine-tolerant algorithm Krum, which can tolerate Byzantine workers via aggregation rule with resilience property. Similarly, Yin \etal \cite{pmlr-v80-yin18a} proposed two robust distributed gradient descent algorithms based on coordinate-wise median and trimmed mean operations for Byzantine-robust distributed learning. Chen \etal \cite{10.1145/3154503} proposed a simple variant of the classical gradient descent method based on the geometric median of means of the gradients, which can tolerate the Byzantine failures. Some other works detect Byzantine attackers and remove the malicious local gradients from aggregation by clustering\cite{9054676}\cite{yadav2021clustering}. Different from the aforementioned works, Li \etal \cite{li2020learning} propose a framework that trains a detection model on the server to detect and remove the malicious local gradients. Although they can guarantee the convergence and Byzantine-tolerant, they cannot provide effective defense in the presence of malicious servers\cite{kairouz2021advances}.  

% block-fl \cite{8892848}\cite{8998397}
% p2p \cite{Liu2020}\cite{chen2021ppt}
% gossip \cite{DBLP:journals/jpdc/HegedusDJ21}

As for the further comment, the typical FL requires a central server to complete the gradient aggregation procedure, thus it is difficult to find a fully trusted server in actual scenarios. Beyond that, the entire FL system will be paralyzed if the server suffers a malicious attack. Therefore, a lot of works are devoted to designing a serverless FL framework to reduce the risk of single point failure\cite{pappas2021ipls}. Some existing works design serverless FL frameworks by learning from network protocols such as P2P \cite{roy2019braintorrent}\cite{lalitha2019peertopeer} and Gossip\cite{hu2019decentralized}\cite{10.1007/978-3-030-22496-7_5}. These approaches treat clients as network nodes, which communicate with each other according to the improved network protocol and complete the local training and the aggregation of the global model. Besides, other approaches employ blockchain technology to complete the work of the server to achieve a serverless FL framework\cite{8733825}\cite{9019859}\cite{8843900}\cite{8905038}. They treat clients as blockchain nodes, record the local gradients uploaded on the block, and then make the leading who completes the PoW (Proof of Work) to ensure the aggregation procedure. However, few works present the theoretical convergence analysis of the serverless FL, leading to the lack of performance guarantee. 


%In typical FL settings, the server randomly selects clients to upload local gradients, which lacks careful consideration. Recently, more and more work regards client selection as a meaningful attempt to optimize the FL training process. However, most works assume that the computing/communication resources are limited, and the information fed back by the user is used to help the server make decisions and select a more appropriate subset of clients to execute the aggregation procedure\cite{8761315}\cite{8994206}\cite{9237168}. Few works consider the client selection issues in the Byzantine attack scenario. 

In this paper, we comprehensively consider Byzantine attacks of both clients and the central server, and design a serverless \underline{F}ederated \underline{L}earning framework under \underline{C}ommittee \underline{M}echanism (CMFL), in which some participating clients are appointed as committee members, and the committee members take the responsibility for monitoring the entire FL training process and ensuring the dependable aggregation of the local gradients. The CMFL consists of three components: the scoring system, the election strategy and the selection strategy. The scoring system plays a role in distinguishing different clients. The election strategy is responsible for selecting some clients who can represent the majority of clients to become members of the new committee. Based on different considerations, we propose two opposite selection strategies to make the committee members verify local gradients uploaded by clients. A selection strategy is designed to ensure the robustness of the training process, where the committee members accept those local gradients who are similar to their own local gradients but reject those local gradients who are significantly different from their own ones. The other selection strategy is designed to accelerate the convergence of the global model in a non-attack scenario, where the committee members accept those local gradients who are different from their own local gradients but reject those gradients who are similar to their own ones. {Compared to some existing Byzantine-tolerant algorithms based on local gradient validation, such as Median\cite{pmlr-v80-yin18a}, Trimmed Mean\cite{pmlr-v80-yin18a} and Krum\cite{Blanchard2017Byzantine}, CMFL achieves better performance and higher robustness over various datasets, illustrated by the experimental results. Besides, CMFL can achieve a higher security level due to its decentralized setting.} Theoretical analysis on the convergence of the model is further presented for the performance guarantee, which illustrates the impact of the proposed election and selection strategies. Extensive experiments further demonstrate the outperformance of CMFL compared with both the typical and Byzantine-tolerant FL models, coinciding with the theoretical analysis on the efficiency of the proposed election and selection strategies. In summary, we highlight the contributions as follow:

\begin{itemize}
	\item We propose a serverless FL framework: CMFL, with the ability to monitor the gradient aggregation procedure and prevent both malicious clients and the server from hindering the training of the global model. 
  \item We propose an election strategy and two selection strategies suitable for different scenarios, which can ensure the robustness of the algorithm or accelerate the training process.
	\item We give the proof and analysis of the convergence of the proposed serverless FL framework for the theoretical guarantee, which considers the influence of election and selection strategies on the performance of the global model. 
	\item We conduct extensive experimental results to show that CMFL has a faster model convergence rate and better model performance than the typical and Byzantine-tolerant FL models.
\end{itemize}

The remainder of this paper is organized as follows. Section \ref{relatedWork} surveys related work. Section \ref{background} briefly outlines the background and problem formulation of FL. Section \ref{CMFL} introduces our proposed framework. We show the convergence analysis in Section \ref{convergenceAnalysis} and complexity analysis in Section \ref{ComplexityAnalysis}. Experimental results and analysis are summarized in Section \ref{experiments}. We conclude the paper in Section \ref{conclusion}. Finally, Supplement gives the convergence proof.

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)


% \hfill mds

% \hfill August 26, 2015

\section{Related Work}
\label{relatedWork}

Konečný \etal \cite{konecny2015federated} introduce a new distributed optimization setting in machine learning in 2015. Based on this setting the concept of FL is proposed\cite{konecny2017federated}, which aims to train an efficient centralized model in a scenario where the training data is distributed across a large number of clients. However, these frameworks suffer from heterogeneous clients, failing to achieve satisfactory performance on the Non-IID dataset. To further handle such issues, several works extend FL to Non-IID dataset. Li \etal \cite{li2020convergence} presents the theoretical guarantees under Non-IID settings and analyzes the convergence of FedAvg. Li \etal \cite{li2020federated} propose a framework \textit{FedProx} with convergence guarantees to tackle heterogeneity on the Non-IID dataset. Yue \etal \cite{zhao2018federated} proposed a strategy to mitigate the negative impact of Non-IID data by share a small subset of data between all the clients. Briggs \etal \cite{9207469} improve the pe{}rformance of FL on the Non-IID dataset by introducing a hierarchical clustering step to separate clusters of clients. {For the study on data heterogeneity, Haddadpour \etal \cite{DBLP:journals/corr/abs-1910-14425} generalize the local stochastic and full gradient descent with a new scheme, periodic averaging, to solve nonconvex optimization problems in FL. Dinh \etal \cite{DBLP:journals/ton/DinhTNHBZG21} proposed FEDL, a FL algorithm which can handle heterogeneous user data without any assumptions except strongly convex and smooth loss functions. Liu \etal \cite{DBLP:journals/tpds/LiuCCZ20} proposed momentum federated learning (MFL) to accelerate the convergence, and they establish global convergence properties of MFL and originate an upper bound on the convergence rate.} Different from the aforementioned works, the proposed framework provides a new perspective to enhance the performance on the Non-IID setting.
%Although this novel distributed training method has a fascinating attraction, there are many problems that need to be solved urgently, especially the Byzantine attacks.

The attractiveness of Federated learning relies on the trainable centralized model on user equipment without uploading user data. However, such a framework is vulnerable to Byzantine attackers due to the lack of identity authentication for local gradients. Lots of works have designed a series of Byzantine-tolerant algorithms to further ensure the robustness of the training process. For example, Blanchard \etal \cite{Blanchard2017Byzantine} proposed \textit{Krum}, which aims to select the global model update based on the Euclidean distance between the local models. Yin \etal \cite{pmlr-v80-yin18a} proposed \textit{Median} and \textit{Trimmed Mean}, which are designed to remove extreme local gradients to ensure the robustness of the algorithm. The Median method constructs a global gradient, where each element is the median of the elements in the local gradients with the same coordinate, while the Trimmed Mean method removes the maximum and minimum fraction of elements in the local gradients, and then performs a weighted average on the remaining local gradients to construct the global gradient. Muñoz-González \etal \cite{munozgonzalez2019byzantinerobust} propose a Hidden Markov Model to learn the quality of local gradients and design a robust aggregation rule, which discards the bad local gradients in the aggregation procedure. The aforementioned algorithms are all trying to ensure the robustness of FL by designing a more appropriate aggregation mechanism. However, these works can not provide effective defense in the presence of malicious servers. 

In real applications, commercial competition makes it difficult to find a fully trusted central server among the participants. In addition, server error or malicious server will also cause irreparable damage to the FL system. In this way, many serverless FL frameworks are proposed to solve these problems. Among these approaches, some of them achieve a serverless FL framework by imitating existing network protocols. For example, Abhijit \etal \cite{roy2019braintorrent} proposed a P2P serverless FL framework, where any two clients exchange information end-to-end and update their local models at each epoch. Hu \etal \cite{hu2019decentralized} used the Gossip protocol to complete the model aggregation process, which takes on the role of the central server. Besides, other works build a serverless FL framework based on the blockchain system. For example, Kim \etal \cite{8733825} proposed a blockchain FL architecture, in which they divide the blockchain nodes into devices and miners. The device nodes provide data, train the model locally, and upload the local gradients to their associated miner in the blockchain network. Miner nodes exchange and verify all the local gradients. Although these works have obtained the corresponding performance to some degree, they lack the theoretical analysis of the model convergence under serverless FL framework and consideration of Byzantine attacks of clients. 

%More and more works pay attention to the problem of client selection. They tried to design a mechanism to select a better subset of clients instead of simple random sampling. Takayuki \etal \cite{8761315} propose a new FL protocol called FedCS, which requires the client to feedback information about its computing resources and communication status to the server to help the server choose a more suitable subset of clients to complete the model training. Jiawen \etal \cite{8994206} considered this problem theoretically. They expressed the problem of wireless channel resource allocation and client selection as an optimization problem, and gave a closed-form expression of the convergence speed of the FL algorithm to quantify the effect of wireless channel resources on the training process. Jie \etal \cite{9237168} also did similar work. They brought a new long-term perspective to the resource allocation problem in federated learning. They propose that the learning rounds not only depend on each other in time but also have different effects on the final model. Based on this insight, they formulated a random optimization problem for client selection and loan allocation under long-term resource constraints, and proposed an optimization algorithm for the optimization problem and long-term performance guarantee. The above work is all considering the problem of client selection under resource constraints, but few works essentially study the same problem in the Byzantine attack scenario. 

To deal with the limitations of the existing works, we proposed a serverless FL framework under committee mechanism. In scenarios that consider the Byzantine attacks, the framework can be efficient Byzantine-tolerant of malicious clients and servers. In scenarios without considering the Byzantine attacks, the framework can achieve better performance of the global model on the Non-IID dataset. Beyond that, we present the theoretical analysis of the convergence under the framework.

%In general, the existing research works on FL have the following shortcomings
%\begin{enumerate}
%\item \textit{Lack of efficient Byzantine-tolerant algorithms}. Existing Byzantine-tolerant algorithms such as Krum, Median and Trimmed Mean cannot achieve satisfactory model performance. 
%\item \textit{Lack of Byzantine fault-tolerant algorithms that can resist malicious server attacks}. Very few Byzantine-tolerant algorithms can resist malicious server attacks because they do not implement the serverless FL framework.
%\item \textit{Lack of research work on client selection considering Byzantine attacks}. Very little work takes into account the client selection problem under Byzantine attacks, which is a new idea for designing efficient Byzantine-tolerant algorithms.
%\item \textit{Lack of proof and analysis of convergence based on the serverless FL framework}. Many works have implemented the serverless FL framework, but very few of them have completed the convergence proof and analysis of the framework. 
%\end{enumerate}

%And in the following sections, we will describe our proposed serverless FL framework, which can compensate for the first three shortcomings. 


% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

\section{Background}
\label{background}

\subsection{Federated Learning}

A typical FL framework consists of a central server and multiple clients. The server maintains a global model, and each client maintains a local model. At the beginning of training, the global model and all local models will be initialized randomly. And then the following steps will be performed at each communication round to continue the training process\cite{konecny2017federated}:

\begin{enumerate}[1.]
\item The server randomly selects a subset of clients, which then download the global model to the local.
\item Each client in the subset performs a certain number of Stochastic Gradient Descent (SGD)\cite{stich2019local}\cite{wang2019adaptive} and computes the local gradient.
\item The clients in the subset send their local gradients to the server.
\item The server receives the local gradients and performs the Federated Averaging (FedAvg) algorithm \cite{pmlr-v54-mcmahan17a} to construct a global gradient, which is used to update the global model. 
\end{enumerate}  

The above steps will be iterated until the algorithm converges or the model accuracy meets the requirements. 

\subsection{Problem Formulation}

We consider the typical FL setup with total $K$ clients. The $k$-th client for $k = 1,...,K$ owns a local dataset $\D_k$, which contains $n_k=|\D_k|$ data samples. We denote the user-defined loss function for sample $\xi$ and model parameter vector $\w$ as $f(\w,\xi)$, the local objective function  $F_k(\w)$ on the $k$-th client can be written as follows:

\begin{equation}
F_k(\w) = \frac{1}{n_k}\sum_{\xi \in \D_k}f(\w,\xi).
\end{equation}We consider the following global objective function:

\begin{equation}
F(\w) = \sum_{k=1}^K p_kF_k(\w),
\end{equation}where $p_k = n_k/\sum_{k=1}^K n_k$ denotes the weight of the dataset on the $k$-th client. Formally, $\nabla F_k(\w_{k,i}^t)$ denotes the local gradient over dataset $\mathcal{D}_k$. Assume that at round $t$ the $k$-th client trains its local model $\w_{k,i}^{t}$ over mini-batch $\B_{k,i}^{t}$ for $i$ iterations of SGD, where $\B_{k,i}^{t}$ is randomly sampled from $\D_k$. Then the $k$-th client computes the local gradient $g_k(\w_{k,i}^{t},\B_{k,i}^{t})$ by the following formula:

\begin{equation}
g_k(\w_{k,i}^{t},\B_{k,i}^{t}) = \frac{1}{|\B^t_{k,i}|}\sum_{\xi \in \B_{k,i}^{t}}\nabla f(\w_{k,i}^{t},\xi).
\end{equation}The $g_k(\w_{k,i}^{t},\B_{k,i}^{t})$ is used to update the local model $\w_{k,i}^{t}$ as follows:

\begin{equation}\label{localUpdate2}
\w_{k,i+1}^{t} = \w_{k,i}^{t} - \eta_{i}^t g_k(\w_{k,i}^{t},\B_{k,i}^{t}),
\end{equation}where $\eta_{i}^t$ represents the local learning rate at iteration $i$ of round $t$ and $\tau$ represents the maximal local SGD iterations. And after $\tau$ iterations of local updating, the local gradient $g_{k}(\w_{k,\tau}^{t},\B_{k,\tau}^{t})$ is sent to the server to construct a global gradient as follows:

\begin{equation}
\label{aggregation}
\overline{g}^{t} = \sum_{k \in S^{t}}p_{k,S^t} g_k(\w_{k,\tau}^{t},\B_{k,\tau}^{t}), 
\end{equation}where $S^{t}$ denotes the subset of clients and $p_{k,S^t} = n_k/\sum_{k\in S^t}n_k$ is the weight of the dataset on the $k$-th client of $S^t$. The $\overline{\w}^{t}$ is updated at each round as follows:

\begin{equation}
\label{globalUpdate}
\overline{\w}^{t+1} = \overline{\w}^{t} - \eta_t \overline{g}^{t},
% = \overline{\w}^{(t)} - \eta_t(\sum_{k \in S^{(t)}}p_k g_k(\w_k^{(t+\tau)},B_k^{(t+\tau)})).
\end{equation}where $\eta_t$ represents the global learning rate.

\section{Committee Mechanism based Federated Learning}
\label{CMFL}

The typical FL system is vulnerable to Byzantine attacks and malicious servers due to its disability to implement a serverless framework and the lack of verification for the uploaded local gradients. The key insight of CMFL is to utilize the committee mechanism to implement a decentralized framework. Under such a decentralized framework, we appoint some training clients as the committee members, which are responsible for filtering the local gradients uploaded by the remaining clients. The committee members must reach a consensus on which the uploaded gradient should be used for the global updating. In this way, the aggregation process is controlled by all the committee members rather than one untrustworthy central server. As long as the number of the honest committee members is greater than that of the malicious members, attacks from malicious clients become meaningless, since the decision of the committee depends on the majority of the committee members. In order to guarantee the honesty of committee members and achieve a secure aggregation, we design a novel committee mechanism, including a scoring system, selection strategy, election strategy, and committee consensus protocol. The detailed introduction of the proposed framework and the training progress is involved in this section and the further theoretical analysis of the framework is illustrated in Section \ref{convergenceAnalysis}.


% The proposed serverless FL framework CMFL overcomes these two flaws by introducing the committee mechanism. Under this framework, we appoint some clients participating in the training process as the committee members, which are responsible for filtering the local gradients uploaded by the remaining clients. Therefore, how to select committee members and aggregate the appropriate parameter from the clients requires careful consideration. In this work, a reliable election strategy and two opposite selection strategies are designed for convergence guarantees. The detailed introduction of the proposed framework and the training progress is involved in this section and the further theoretical analysis of the framework is illustrated in Section \ref{convergenceAnalysis}.


\subsection{Framework of CMFL}\label{FrameworkOfCMFL}

In the CMFL framework shown in Figure \ref{fig:CMFL}, the clients are divided into three categories: \textit{training client}, \textit{committee client}, and \textit{idle client}. At each round, the following steps are performed to complete the training process:

\begin{itemize}
\item \textbf{Activate.} A part of the idle clients are activated to be the training clients, which participate in the training at this round.
\item \textbf{Training.} The training clients and the committee clients download the global model to the local for training, while the idle clients stay idle until the next round. The training clients and the committee clients perform SGD over their local dataset and compute the local gradients. The difference is that the local gradients on the training clients are used to update the global model, while the local gradients on the committee clients are used to verify the gradients uploaded by the training clients. 
\item \textbf{Scoring.} The training clients send their local gradients to each committee client and the committee clients assign a score on them according to an established scoring system. 
\item \textbf{Selection.} Only the qualified gradients according to the set selection strategy will be used to construct the global gradient. 
\item \textbf{Aggregation.} The clients who meet the selection strategy are responsible for completing the aggregation procedure, which is called the \textit{aggregation client}. 
\item \textbf{Election.} An election strategy is designed to complete the replacement of committee members. A part of the training clients who meet the election strategy become the new committee clients.
\item \textbf{Step Down.} The prior committee clients become the idle clients, waiting for the next participate.
\end{itemize}

In a decentralized framework, we design a committee consensus protocol based on Practical Byzantine Fault Tolerance(pBFT)\cite{PBFT}, to complete the Selection, Aggregation, and Election process.


\begin{figure*}
	\centering
	\includegraphics[width=17cm]{./img/CMFL.jpg}
	\caption{The training process of CMFL are as follows: (1) Training clients are selected randomly from the idle clients; (2) The training clients and committee clients download the global model $\overline{\w}^{t}$ and start training, then send their local gradients $\hat{g}_k^{t}$ to the committee clients; (3) The committee client assigns a score to each client according to the scoring system; (4) The committee selects the aggregation clients according to the selection strategy; (5) The local gradients uploaded by the aggregation clients are used to construct the global gradient $\overline{\w}^{t+1}$; (6) New committee clients are elected from the training clients according to the election strategy; (7) The committee clients of the last round are added to the idle client list.}
	\label{fig:CMFL}
\end{figure*}

Supposed that $C$ clients will be selected as committee clients, the local model of $c$-th committee client for $c = 1,...,C$ at round $t$ is expressed as $\w_{c,\tau}^{t}$. Assume that we have a total of $K$ clients, and $m$ gradients will be accepted at each round, which are verified by $C$ committee members. Also, we represent the committee client set, the training client set and the aggregation client set at round $t$ as $S_c^{t}$, $S_b^{t}$ and $S_a^{t}$ respectively. The relevant symbols are shown in Table \ref{notation}.

% The $\overline{\w}_{c,\tau}^{t} = \sum_c^C p_c\w_{c,\tau}^{t}$ represents the average value of the local models over all committee clients, and $p_c=n_c/\sum_{c=1}^Cn_c$ denotes the weight of the dataset on the $c$-th committee client. In each round, $\tau$ epochs of SGD will be perform at local, and maximal $T$ communications rounds will be excuted. We represent the proportion of active clients as $\delta$, and the learning rate is $\eta_t$.

\begin{table}[h]
\caption{Notation}
\label{notation}
\centering
\begin{tabular}{c c}
\toprule
  Notation & Description \\
  \midrule
  $K$ & Number of total clients \\
  $T$ & Number of maximal communication rounds \\ 
  $\tau$ & Number of maximal SGD iterations\\
  $\D_k$ & Local dataset\\
  $\B_{k,i}^{t}$ & Mini-batch randomly sampled from $\D_k$\\
  $\w_{k,i}^{t}$ & Local model parameter\\
  $\overline{\w}^{t}$ & Global model parameter\\
  $\w_k^*$ & Optimal local model parameter\\
  $\w^*$ & Optimal global model parameter\\
  $\w_{c,i}^{t}$ & Local model parameter of committee client\\
  $f(\w,\xi)$ & Overall loss function\\
  $F_k(\w)$ & Local objective function\\
  $g_k(\w_{k,i}^{t},\B_{k,i}^{t})$ & Mini-batch local gradient for $i$ iteration\\
  $\nabla F_k(\w_{k,i}^{t})$ & Full local gradient for $i$ iteration\\
  $\hat{g}_k^t$ & Local gradient for $\tau$ iterations\\
  $F^*_k$ & Optimal value of the local objective function\\
  $F^*$ & Optimal value of the global objective function\\
  $S_a^{t}$ & Aggregation client set\\
  $S_b^{t}$ & Training client set\\
  $S_c^{t}$ & Committee client set\\
  $m$ & Number of clients in $S_a^{t}$\\
  $n$ & Number of clients in $S_b^{t}$\\
  $C$ & Number of clients in $S_c^{t}$\\
  $\p_k^c$ & Score of the $k$-th client assigned by the $c$-th client\\
  $\p_k$ & Final score of the $k$-th client\\
\bottomrule
\end{tabular}

\end{table}

\subsection{Committee Mechanism}

A committee system is set up to complete the screening of local gradients. The committee system consists of the scoring system, the election strategy, the selection strategy and the committee consensus protocol. 

\subsubsection{Scoring System}

The key insight of the scoring system is to distinguish between the honest and malicious gradients by calculating their Euclidean distance. The Euclidean distance of two honest gradients is lower than that of an honest gradient and a malicious gradient. Based on this insight, the scoring system is designed as comparing the Euclidean distance of two gradients, where the clients who upload the honest gradients can obtain higher scores while the clients who upload the malicious gradients will obtain the lower score. Assume that the local gradient on the $k$-th training client at round $t$ is denoted as $\hat{g}_{k}^{t} = g_{k}(\w_{k,\tau}^{t},\B_{k,\tau}^{t})$, and the local gradient on the the $c$-th committee client at round $t$ is expressed as $\hat{g}_c^{t} = g_c(\w_{c,\tau}^{t},\B_{k,\tau}^{t})$. The score $\p_k^c$ of the $k$-th training client assigned by the $c$-th committee client is computed as follows:

\begin{equation}
\p_k^c = \frac{1}{||\hat{g}_k^{t} - \hat{g}_c^{t}||^2_2}.
\end{equation}
Since $\hat{g}_k^t$ and $\hat{g}_c^t$ are local gradients generated from different clients, we assume that $\hat{g}_k^t \neq \hat{g}_c^t$ for any $k \in S_b^t, c \in S_c^t$. We define 

\begin{equation}\label{computeScore}
\begin{split}
&\p_k =\frac{1}{\frac{1}{C}\sum_c^C ||\hat{g}_k^{t} - \hat{g}_c^{t}||^2_2} = \frac{C}{\sum_c^C\frac{1}{\p_k^c}}
\end{split}
\end{equation} as the final score of the $k$-th training client. The scoring principle is mainly based on the Euclidean distance between the local gradients $\hat{g}_k^{t}$ and $\hat{g}_c^{t}$. Another insight remains that Byzantine attackers usually replace some local gradients with malicious gradients, which directly increases the Euclidean distance between these gradients and the honest gradients. As a result, when the proportion of malicious clients is within a tolerable range, the score of the malicious training clients is expected to be lower than the honest training clients. However, in a scenario without Byzantine attacks, the score represents the degree of heterogeneity of clients. A higher score means a higher degree of heterogeneity. 


\subsubsection{Selection Strategy}
\label{selection}

We design the selection strategy for determining which uploaded gradient is used to update the global model. Based on our scoring system, we can achieve a secure aggregation by accepting the gradients with high scores, since the malicious gradients obtain low scores. Besides, the convergence analysis and experimental result show that the opposite strategy performs better in a non-attack scenario. Therefore, two opposite selection strategies are designed for different considerations as follows:  

\begin{itemize}
\item \textbf{Selection Strategy \uppercase\expandafter{\romannumeral1}.} The selection strategy \uppercase\expandafter{\romannumeral1} selects several local gradients with relatively higher scores to construct a global gradient for the update of the global model. In other words, we hope that the local gradients which are similar to the committee gradients in the Euclidean space will participate in the construction of the global gradient. In practice, we sort the local gradients according to their scores and only accept the top $\alpha\%$ of them. We design the selection strategy for the consideration of robustness. Those malicious gradients and honest gradients are far from each other in the Euclidean space, and choosing the gradients close to the committee gradients under the condition that the committee clients are honest can achieve a more robust aggregation process. However, it is difficult for those clients with obvious differences to be selected to participate in the aggregation procedure, which makes it hard for the global model to learn the comprehensive data characteristic, resulting in a decline in performance. Overall, selection strategy \uppercase\expandafter{\romannumeral1} is suitable for FL scenarios with Byzantine attacks. 
\item \textbf{Selection Strategy \uppercase\expandafter{\romannumeral2}.} The selection strategy \uppercase\expandafter{\romannumeral2} selects several local gradients with relatively lower scores to construct a global gradient for the update of the global model. That is, we hope that the local gradients which are different from the committee gradients in the Euclidean space will participate in the construction of the global gradient. Similar to the selection strategy \uppercase\expandafter{\romannumeral1}, we sort the local gradients according to their scores and only accept the bottom $\alpha\%$ of them in practice. We design the selection strategy \uppercase\expandafter{\romannumeral2} for the consideration of convergence rate and performance of the global model. Constructing a global gradient by selecting those local gradients with obvious differences allows the global model to learn more comprehensively, gaining a faster convergence and better performance of the global model. However, this strategy can not provide robustness guarantees, mainly since that the malicious gradients will be preferentially used in global learning, leading to a sharp drop in the performance of the global model. Overall, selection strategy \uppercase\expandafter{\romannumeral2} is suitable for FL scenarios without Byzantine attacks. 
\end{itemize}

\subsubsection{Election Strategy}
\label{election}

The election strategy is designed for guaranteeing the honesty of committee clients. The committee members reach a consensus on their decisions, and the committee's decision depends on the majority of the committee members. However, the malicious clients mixed into the committee may interfere with the committee's decision-making. Therefore, the committee must guarantee that its honest members are more than malicious members. Otherwise, the committee can not filter out the malicious gradients. We sort the training clients according to their scores and then select these training clients closed to the middle position as the committee clients for the next round. We design the election mechanism based on the following two considerations:

\begin{itemize}
  \item \textit{Robustness}. According to the above analysis, as malicious training clients will get lower scores than honest training clients, they are expected to locate at the end of the sorted sequence. Therefore, choosing the training client closed to the middle or upper position prevents the malicious training clients from becoming the committee clients in the next round, thereby guaranteeing the security of the global model. Although there may still be a small number of malicious clients mixed into the committee members when the proportion of malicious clients is relatively large, it is difficult to affect the judgment of the whole committee because of their exceeding small proportion of the committee members.
  \item \textit{System Stability}. Those training clients with the highest scores are not chosen to be the new committee members in order to avoid the system from relying too much on the initialization of committee members. When we choose those training clients with the highest scores to form a new committee, the learning direction of the global model will be completely determined by the initial committee members. It is because those clients with a large Euclidean distance between the local gradient and the committee gradients are not only difficult to be selected to participate in the aggregation procedure, but also lost the opportunity to run for the next round of committee members. This is in line with our intuition that “committee members should be those who can represent the majority”.
\end{itemize}


\subsubsection{Committee Consensus Protocol}
\label{CommitteeConsensus}

In our decentralized framework, the committee clients must reach a consensus to complete the scoring, aggregation, selection, and election process. Thus we design a committee consensus protocol, which is inspired by the pBFT\cite{8069090}. The designed committee consensus protocol(CCP) is as follows(at round $t$):

\begin{enumerate}
\item After the scoring process, each committee client obtains the scores of the training clients. Then every committee client broadcast its scores to the other committee clients. Each committee client is able to calculate the total score of every training client by Eq. \eqref{computeScore}.
\item A committee client $p$ is selected as the primary committee client by random, while other committee clients are regarded as the replicate committee clients. 
\item Each committee client decides its aggregation set $S_{a,c}^t$ according to the selection strategy. Since all scores are broadcasted among committee clients, the aggregation sets among honest committee clients are the same. 
\item The primary committee client creates a request $\langle$ \textit{Request, $S_{a,p}^t$, operation, timestamp} $\rangle$ to ask whether its $S_{a,p}^t$ is correct. Then the primary committee client broadcasts the request to all the replicate committee clients. The operation in the request is to aggregate the local gradients uploaded by aggregation clients as Eq.\eqref{aggregation}
\item All the replicate committee clients execute the request. Each of them checks whether the $S_{a,p}^t$ is the same as its own $S_{a,c}^t$. If so, it performs the aggregation process as Eq. \eqref{aggregation}. After aggregation, it checks whether the result is consistent with the request. If so, the executing result $\langle$ \textit{Reply, timestamp, $S_{a,p}^t$, response} $\rangle$ is returned to the primary committee client.
\item The primary committee client checks whether it has received at least $\lfloor C/2 \rfloor + 1$ identical results from replicate committee clients. If so, the consensus is reached; otherwise, the primary committee client should be reassigned and steps 3)-6) should be repeated.
\item Similarly, steps 3)-6) are repeated to reach consensus on new committee client set $S_c^{t+1}$(However, the aggregation client set $S_a^t$ in step 3)-6) should be replaced by new committee client set $S_c^{t+1}$).
\item The primary committee client broadcasts the global model to all other clients. The next training round is ready to start.  
\end{enumerate} 


\begin{algorithm}
\caption{Committee Consensus Protocol}
\begin{algorithmic}[1]
\label{algorithm_CCP}
\REQUIRE $t$, $S_c^t$, $S_b^t$\\ %算法的输入参数：Input
\ENSURE global model $\w^t$, new committee client set $S_c^{t+1}$\\ %算法的输出：Output
\FOR{$c \in S_c^t$}
\FOR{$k \in S_b^t$}
\STATE The $c$-th committee client broadcasts score $\p_k^c$ to other committee clients.
\ENDFOR
\ENDFOR  
\FOR{$c \in S_c^t$}
\FOR{$k\in S_b^t$}
\STATE The $c$-th committee client calculates the total score $\p_k$ of the $k$-th training client.
\ENDFOR
\ENDFOR
\STATE {The committee performs voting($S_a^t$, aggregtion) as Algorithm \ref{voting} and gets the global model $\w^t$. }
\STATE The committee performs voting($S_c^{t+1}$, None) as Algorithm \ref{voting}.
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{The voting algorithm}
\begin{algorithmic}[1]
\label{voting}
\REQUIRE The client set $S$ and the opertation $o$\\ %算法的输入参数：Input
\ENSURE {The result $r$ of the operation $o$}\\ %算法的输出：Output
\WHILE{No consensus on $S$}
\STATE The primary committee client $p$ is selected from committee clients by random.
\FOR{$k \in S_c^t$}
\STATE The $k$-th committee client decides its client set $S^k$ according to the corresponding strategy (selection strategy/election strategy). 
\ENDFOR
\STATE {$p$ performs the operation $o$ and gets $r$.}
\STATE $p$ creates a request $\langle$ \textit{Request, $S^p$, $o$, timestamp} $\rangle$ and broadcasts it to all the replicate committee clients.
\FOR{$k \in S_c^t \setminus \{p\}$}
\STATE The $k$-th replicate committee client receives the request from $p$.
\IF{$S^k == S^p$}
\STATE The $k$-th committee client performs the operation $o$.
\STATE Return $\langle$ \textit{Reply, timestamp, $S^p$, response} $\rangle$ to $p$.
\ENDIF
\IF{$p$ receives at least $\lfloor C/2\rfloor + 1$ response}
\STATE Consensus is reached on $S$.
\ENDIF 
\ENDFOR
\STATE { Return $r$.}
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Training Algorithm}

In this section, we introduce the serverless training algorithm of CMFL. Firstly, all clients randomly initialize the local model and some clients are randomly selected as the committee clients. In each communication round, the algorithm performs the following five steps:
\begin{enumerate}
  \item \textit{Random Sampling}: A part of the clients from the non-committee clients will be selected randomly as the training clients while the other clients become the idle clients. At round $t$ all training clients and committee clients download the global model from the primary committee client as the local model.
  \item \textit{Local Training}: All training clients and committee clients perform $\tau$ rounds of SGD over the local datasets and compute the local gradients. The training clients send their local gradients to each committee client for verification. 
  \item \textit{Gradient Verification}: Each committee client assigns a score on each training client according to the scoring system. Then they execute CCP to reach a consensus on aggregation client set $S_a^t$. 
  % Those local gradients who meet the selection strategy will be accepted while those gradients who do not meet will be rejected. Then all the accepted gradients will be sent to the training client with the highest score to complete the aggregation procedure. 
  \item \textit{Global Model Updating}: In CCP, the local gradients uploaded by clients in $S_a^t$ are aggregated for constructing the global gradient, which is used to update the global model according to the Eq. \eqref{globalUpdate} when the consensus is reached.

  % The client with the highest score receives all the accepted gradients and constructs the global gradient for global model updating according to the Eq. \eqref{globalUpdate}. 
  \item \textit{Election of New Committee Members}: The committee clients execute CCP to reach a consensus on new committee clients set $S_c^{t+1}$ 
  % The new committee clients will be elected from the training clients, which will be determined according to the election strategy.
\end{enumerate}

The algorithm repeats the above five steps until the algorithm converges or $t$ exceeds the defined maximum communication round $T$.

\begin{algorithm}
\caption{The training algorithm of CMFL}
\begin{algorithmic}[1]
\label{algorithm_CMFL}
\REQUIRE $\tau$, $T$, $K$, $m$, $C$, $\eta_t$, $\eta_i^t$\\ %算法的输入参数：Input
\ENSURE target global model $\w^T$\\ %算法的输出：Output
\STATE Initialize $\w^{1}$, $S_c^{1}$ and $S_b^{1}$ randomly.
\FOR{$t=1$ to $T$}
% \IF{$t\%\tau == 0$}
\FOR{$k \in S_b^{t} \cup S_c^{t}$}
\FOR{$i=1$ to $\tau$}
\STATE The $k$-th client runs the SGD over the local dataset by $\w_{k,i}^{t} \Leftarrow \w_{k,i-1}^{t} - \eta_i^t g_{k}(\w_{k,i-1}^{t},\B_{k,i-1}^{t})$.
\ENDFOR
\ENDFOR
\FOR{$k \in S_b^{t}$}
\FOR{$c \in S_c^{t}$}
\STATE The $k$-th training client send its local gradient $\hat{g}_k^{t}$ to the $c$-th committee client.
\STATE The $c$-th committee client assign a score $\p_k^c$ on the $k$-th training client/local gradient according to the scoring system.
\ENDFOR
% \STATE The $k$-th training client computes the total score by Eq. \eqref{computeScore}
\ENDFOR
\STATE The committee clients execute CCP to complete the selection, aggregation, and election process.
% \STATE Sort the $S_b^{t}$ according to their scores in descending order.
% \IF{selection strategy \uppercase\expandafter{\romannumeral1}}
% \STATE $(S_b^{t}[1],...,S_b^{t}[m])$ forms the aggregation client set $S_a^{t}$.
% \ELSE
% \STATE $(S_b^{t}[K-m+1],...,S_b^{t}[K])$ forms the aggregation client set $S_a^{t}$.
% \ENDIF
% \FOR{$k \in S_a^{t}$}
% \STATE The $k$-th training client sends its local gradient $\hat{g}_k^{t}$ to the $S_a^{t}[1]$-th client.
% \ENDFOR
% \STATE The $S_a^{t}[1]$-th training client update the global model by $\w^{t+1} \Leftarrow \w^{t} - \eta_t(\sum_{k\in S_a^t} p_{k,S_a^t}\hat{g}_k^{t})$
% \STATE The $(S_a^{t}[m/2-C/2],...,S_a^{t}[m/2+C/2])$ forms the new committee client set $S_c^{t+1}$.
\STATE The $S_b^{t}$ be reinitialized to form the $S_b^{t+1}$.
\FOR{$k\in S_b^{t+1} \cup S_c^{t+1}$}
\STATE The $k$-th client downloads the global model from the primary committee client.
\ENDFOR
% \ELSE
% \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}





% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.
\section{Theoretical Analysis}

In this section, we show the theoretical analysis of CMFL, including convergence analysis and complexity analysis. 

\subsection{Convergence Analysis}
\label{convergenceAnalysis}

In this section, we conduct the convergence analysis of the proposed framework CMFL. First, we introduce some basic assumptions used for the convergence analysis in Section \ref{assumption}. Second, we introduce two definitions that facilitate our analysis in Section \ref{convergence_definition}. Finally, we present our result and analysis for convergence of CMFL in Section \ref{convergence_result}. The proof of the convergence is presented in the Supplement.

For the purpose of convergence proof and analysis, we define $F^* = \min_{\w}F(\w) = F(\w^*)$ as the optimal value of the global objective function, where $\w^*$ denotes the optimal global model. In the same way we define $F_k^* = \min_{\w}F_k(\w) = F_k(\w_k^*)$ as the optimal value of the $k$-th client's local objective function, where $\w_k^*$ denotes the optimal local model of $k$-th client.

\subsubsection{Assumptions}\label{assumption}

First, we introduce the assumptions used for convergence analysis.

\begin{assumption}\label{A1} 
(Lipschitz Gradient). \textit{$F_1,...,F_K$ are all $\mathcal{L}$-smooh: for all $\mathbf{v}, \w \in R^n$, $k = 1, ..., K$, $F_k(\mathbf{v}) \leq F_k(\w) + (\mathbf{v}-\w)^T\nabla F_k(\w) + \frac{L}{2}||\mathbf{v}-\w||^2_2$.}
\end{assumption}

\begin{assumption}\label{A2} 
($\mu$-strongly Convex Gradient). \textit{$F_1,...,F_K$ are all $\mu$-strongly convex: for all $\mathbf{v}, \w \in R^n$, $k = 1, ..., K$, $F_k(\mathbf{v}) \geq F_k(\w) + (\mathbf{v}-\w)^T\nabla F_k(\w) + \frac{\mu}{2}||\mathbf{v}-\w||^2_2$}
\end{assumption}


\begin{assumption}\label{A3} 
(Bounded Variance). \textit{For the mini-batch $B_{k,i}^t$ uniformly sampled randomly from $k$-th client's dataset $\D_k$, the resulting stochastic gradient is unbiased: $E[g_k(\w_{k,i}^t,\B_{k,i}^t)]=\nabla F_k(\w_{k,i}^t)$ for all $k=1,...,K, t=1,...,T, i=1,...,\tau$. And the variance of stochastic grandient in each client is bounded: $E||g_k(\w_{k,i}^t,\B_{k,i}^t)-\nabla F_k(\w_{k,i}^t)||^2 \leq \sigma^2$.}
\end{assumption}

\begin{assumption}\label{A4}
(Bounded Gradient). \textit{The local gradient's expected squared norm is uniformly bounded: $\mathbb{E}||g_k(\w_{k,i}^t,\B_{k,i}^t)||^2 \leq G^2$ for all $k = 1,...,K,t=1,...,T, i=1,...,\tau$.}
\end{assumption}

\begin{assumption}\label{A5}
(Bounded Objective Function). \textit{For any aggregation client set $S_a\notin \varnothing$ and the optimal committee client set $S_c^* \notin \varnothing$, the difference of local optimal objective function is bounded: $\mathbb{E}[||\sum_{k \in S_a}p_{k,S_a}F_k^*-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*||] \leq \kappa^2$, where $S_c^*$ satisfies that $S_c^* = \arg\min_{S_c}\sum_{k\in S_c}p_{k,S_c}F_k^*$ and $|S_c^*| = C$.} 
\end{assumption}

% \textbf{Assumption \uppercase\expandafter{\romannumeral4}.} In the selection phase \ref{selection} the expected squared norm of stochastic gradient is uniformly bounded, i.e., $\mathbb{E}[\sum_{k \in \s}||\nabla F_k(W_k^{(t)},B_k^{(t)})||] \leq mG_1^2$

% \textbf{Assumption \uppercase\expandafter{\romannumeral5}.} In the election phase \ref{election} the expected squared norm of stochastic gradient is uniformly bounded, i.e., $\mathbb{E}[\sum_{k' \in S_{t-1}}||\nabla F_{k'}(W_{k'}^{(t-1)},B_{k'}^{(t-1)})||] \leq mG_2^2$

Assumption \ref{A1} and \ref{A2} are standard conditions in FL setting\cite{9252927}\cite{9261995}\cite{9003425}\cite{amiri2020convergence} and many common machine learning optimization algorithms meet these assumptions, such as the $\ell_2$-norm regularized linear regression, logistic regression, and softmax classifier \cite{li2020convergence}. Assumption \ref{A3} is a form of bounded variance between the local objective functions and the global objective function\cite{8664630}, and Assumption \ref{A4} is fairly standard in nonconvex optimization literature\cite{pmlr-v48-reddi16}\cite{pmlr-v97-ward19a}\cite{9069945}\cite{9148862}. They are widely used in the FL convergence analysis, such as Li \etal \cite{li2020convergence} and Cho \etal \cite{cho2020client}. Assumption \ref{A5} is used to constrain the optimal objective function deviation between the aggregation client set and the committee client set caused by the committee mechanism. For the need of convergence proof, we define $S_c^*$ as the set which contains $C$ clients with the smallest optimal local objective function $F_k^*$. 

\subsubsection{Definition}\label{convergence_definition}

We introduce two related definitions for the convenience of analysis as follows.

% We introduce two metrics: the \textit{Training-Committee Gap} and the \textit{Mass Opposition Value}, which are directly related to the convergence rate of the model. Besides, we introduce the other two metrics: the \textit{Committee-Local Gap} and the \textit{Committee-Global Gap}, which are directly related to the convergence error of the model. 

% \begin{definition}\label{G1}
% \textbf{(Training-Committee Gap).} In the selection phase \ref{selection} for aggregation client set $S_a^{(t)}$ and committee client set $S_c^{(t)}$ at round $t$, we define the training-committee european gap $G_1^{(t)}$ as 

% \begin{equation}
% G_1^{(t)} = \frac{1}{mC}\sum_{k \in S_a^{(t)}}\sum_{c \in S_c^{(t)}} \mathbb{E}||\hat{g}_k^{(t)} - \hat{g}_c^{(t)}||^2_2, 
% \end{equation}

% where $\hat{g}_k^{(t)}$ and $\hat{g}_c^{(t)}$ denote the local gradients of the $k$-th training client and the $c$-th committee client at round $t$. We define $\overline{G_1} = \sum_{i=t_0}^{t_0+\tau-1}G_1^{(i)}$ as the sum of training-committee european gaps for $\tau$ epochs at a round.

% \end{definition}

% \begin{definition}\label{G2}
% \textbf{(Mass Opposition Value).} In the election phase \ref{election} for committee client set $S_c^{(t)}$ at round $t$ and previous training client set $S_t^{(t-1)}$, we define the mass opposition value $G_2^{(t)}$ as 

% \begin{equation}
% G_2^{(t)} = \frac{1}{mC}\sum_{k \in S_t^{(t-1)}}\sum_{c \in S_c^{(t)}} \mathbb{E}||\hat{g}_k^{(t-1)} - \hat{g}_c^{(t)}||^2_2, 
% \end{equation}

% where $\hat{g}_k^{(t-1)}$ denotes the local gradient of the $k$-th training client at round $t-1$. Also, we define $\overline{G_2} = \sum_{i=t_0}^{t_0+\tau-1}G_2^{(i)}$ as the sum of mass opposition values for $\tau$ epochs at a round.

% \end{definition}

% \begin{definition}\label{D1}
% \textbf{(Committee-Local Gap).} In the selection phase \ref{selection} for aggregation client set $S_a^{(t)}$ and committee client set $S_c^{(t)}$ at round $t$, we define the committee-local gap as 

% \begin{equation}
% D_1^{(t)} = \frac{1}{mC}\sum_{k \in S_a^{(t)}}\sum_{c \in S_c^{(t)}}||\w_k^* - \overline{\w}_c^{(t)}||^2_2,
% \end{equation}

% where $\w_k^*$ denotes the local optimal model of the $k$-th training client, and $\w_c^{(t)}$ denotes the local model of the $c$-th committee client at round $t$. 


% \end{definition}
% \begin{definition}\label{D2}
% \textbf{(Committee-Global Gap).} In the election phase \ref{election} for committee client set $S_c^{(t)}$ at round $t$, we define the committee-global gap $D_2^{(t)}$ as 

% \begin{equation}
% D_2^{(t)} = \frac{1}{C}\sum_{c \in S_c^{(t)}}||\w^*-\overline{\w}_c^{(t)}||^2_2,
% \end{equation}

% where $\w^*$ denotes the global optimal model. 

% \end{definition}

\begin{definition}\label{heterogeneity}
\textbf{(Degree of Heterogeneity).} \textit{We use 
\begin{equation}
\Gamma = F^* - \sum_{k=1}^Kp_kF_k^* = \sum_{k=1}^Kp_k(F_k(\w^*)-F_k(\w_k^*))
\end{equation}
to quantify the degree of heterogeneity among the clients.}
\end{definition}
Li \etal \cite{li2020convergence} proposed this definition, which is widely used in the convergence analysis of FL on Non-IID dataset\cite{9337227}. In the Non-IID FL scenarios, the $\Gamma$ remains nonzero and its value reflects the heterogeneity of the data distribution. In the IID FL scenarios, with the growth of $K$ the $\Gamma$ gradually goes to zero.

\begin{definition}\label{CommitteeSkew}
\textbf{(Aggregation-Committee Gap).} \textit{For any aggregation client set $S_a^t$, we define  
\begin{equation}
\begin{split}
&\varphi(S_a^{t},\w) \\
=& \frac{\mathbb{E}[\sum_{k \in S^{t}_a}p_{k,S_a^t}F_k(\w)-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*]}{F(\w) - \sum_{k=1}^Kp_kF_k^*} \geq 0,
\end{split}
\end{equation}
where $\mathbb{E}$ denotes the expectation over all randomness in the previous iterations, and $S_c^*$ denotes the optimal committee client set.}
\end{definition}
$\varphi(S_a^t,\w)$ changes with the changes of $S_a^t$ and $\w$ during training. An upper bound $\varphi_{max}$ and a lower bound $\varphi_{min}$ are defined to obtain a conservative error bound in the convergence analysis:
\begin{equation}
\begin{split}
&\varphi_{min} = \min_{S_a^t,\w}\varphi(S_a^t,\w),\varphi_{max} = \max_{S_a^t}\varphi(S_a^t,\w^*).
\end{split}
\end{equation}




\subsubsection{Convergence Result and Analysis}\label{convergence_result}

We analyze the convergence of Algorithm \ref{algorithm_CMFL} in this section and find an upper bound of $\mathbb{E}[F(\overline{\w}^{(T)})] - F^*$, which denotes the convergence error of the global model after $T$ rounds:

\begin{theorem}\label{theorem1}
(Convergence of Committee Mechanism based Federated Learning). \textit{Under Assumption \ref{A1} to \ref{A5}, Definition \ref{heterogeneity} to \ref{CommitteeSkew} and the learning rate $\eta_t$, where $ \eta_t = \frac{1}{\mu(t+\gamma)}$ and $\gamma = \frac{4L}{\mu}$, the error after $T$ rounds of CMFL satisfies}
\begin{small}
\begin{equation}
\begin{aligned}
&\mathbb{E}[F(\overline{\w}^{T})] - F^*\\
\leq& \frac{1}{T+\gamma}\left[ \frac{4L(32\tau^2G^2 + \sum_{k=1}^Kp_k\sigma_k^2) + 24L^2\kappa^2}{3\mu^2 \varphi_{min}} \right. \\
&\left. + \frac{8L^2\Gamma}{\mu^2}+\frac{L\gamma||\overline{\w}^{1} - \w^*||^2}{2}\right] + \frac{8L\Gamma}{3\mu}\left(\frac{\varphi_{max}}{\varphi_{min}}-1\right),
\end{aligned}
\end{equation}
\end{small}
\end{theorem}
where $L$, $\mu$ and $\sigma$ represent the constant light indicated in the Assumption \ref{A1}, \ref{A2} and \ref{A3}. $G$ and $\kappa$ represent the upper bound values defined in Assumption \ref{A4} and \ref{A5} .$\Gamma$ denotes the heterogeneity among the clients according to the Definition \ref{heterogeneity}. These are all constant while $\varphi_{min}$ and $\varphi_{max}$ will be different depending on the selection strategy. The impact of selection strategy on $\varphi_{min}$ is analyzed below. CMFL affects the performance of the global model by altering $\varphi_{min}$. The proof of the theorem is shown in Supplement. 

\textbf{The impact of selection strategy on $\varphi_{min}$.} According to the definition, the value of $\varphi$ is positively correlated with $\sum_{k\in S_a^t}p_{k,S_a^t}F_k(\w)$, which represents the average local loss of the model over the aggregation client set $S_a^t$. Under our framework, the lower bound of $\varphi(S_a^{t},\overline{\w}^{t})$ defined as $\varphi_{min}$ affects the convergence rate of the global model, where $\overline{\w}^{t}$ represents the global model at round $t$. In general, those clients whose data set distribution is similar to that of the majority have a relatively low local loss, while others have a relatively high local loss. We call the former \textit{universal clients} and the latter \textit{extreme clients}. The tendency to choose a universal client for aggregation results in low $\varphi_{min}$, and the tendency to choose an extreme client for aggregation results in high $\varphi_{min}$. The building of the election strategy ensures that committee members can represent the majority, and the scoring system is designed as the clients similar to committee members can get higher scores, so clients with high scores are more likely to have a low local loss. When we adopt selection strategy \uppercase\expandafter{\romannumeral1}, we get a low $\varphi_{min}$. Instead, when we adopt selection strategy \uppercase\expandafter{\romannumeral2}, we get a high $\varphi_{min}$.

\textbf{Effect of $\varphi_{min}$ on convergence rate.} Note that a higher $\varphi_{min}$ results in faster convergence at the rate $\mathcal{O}(\frac{1}{T\varphi_{min}})$. That is, adopting the selection strategy \uppercase\expandafter{\romannumeral1} make the convergence of the global model slows down, while the selection strategy \uppercase\expandafter{\romannumeral2} accelerates the convergence of the global model, which is verified in the following experiments. However, considering the reality of Byzantine attacks, the first selection strategy is a more appropriate choice. 



% The resulting term is divided into two terms according to whether it can vanish: the vanishing error term related to the convergence rate and the non-vanishing bias related to the convergence error. 

% \textbf{Vanishing Error Term.} This term can vanish means that the value of this term will become infinitely close to 0 as the infinite increase of $T$. On the other hand, the factor of this term directly affects the convergence rate. We put forward two insights based on the term:

% \begin{itemize}

% \item \textit{Insight 1}: A closer Euclidean distance between the accepted training clients and committee clients results in faster convergence at rate $\mathcal{O}(\frac{\overline{G_1}}{T})$. Based on this insight we design our scoring system and selection strategy. The scoring system reflects the Euclidean distance between each training client and the committee clients, the higher score means the closer Euclidean distance. And the selection strategy selects those local gradients uploaded by the training clients with high scores to construct the global gradient, which aims to minimize $\overline{G_1}$ to accelerate convergence.
% \item \textit{Insight 2}: A lower mass opposition value of the committee members results in faster convergence at rate $\mathcal{O}(\frac{\overline{G_2}}{T})$. Based on this insight we design our election strategy. To minimize the $\overline{G_2}$, the election strategy elects those training clients that are in the middle position in the sequence sorted based on the score as the new committee clients. And the mechanism not only speeds up convergence but also prevents malicious clients with low scores from becoming new committee clients.

% \end{itemize}

% \textbf{Non-vanishing bias}. This term directly affects the convergence error because it cannot be infinitely close to 0 with the infinite increase of $T$, and our committee mechanism can minimize this term as much as possible. We also put forward the other two insights based on the term:

% \begin{itemize}
% \item \textit{Insight 3}: The Selection Mechanism makes $D_1^{(T)}$ become smaller and smaller with the gradual increase of $T$, since $\w_k^{(T)}$ will get closer and closer to $\w_k^*$. In this case, minimizing $D_1^{(T)}$ is equivalent to minimizing $\overline{G_1}$, as the Selection Mechanism aims to do.
% \item \textit{Insight 4}: The Election Mechanism always selects those clients that can represent the majority to become the new committee clients, which gives each client the opportunity to become the committee member and allows the global model to learn in the overall direction. Intuitively, we can minimize $D_2^{(T)}$ in this way but since the unknown $\w^*$ it is difficult for us to prove this in a more professional way. 
% \end{itemize}



\subsection{Complexity Analysis}
\label{ComplexityAnalysis}
In this section, we analyze the computation complexity and communication complexity of the proposed framework CMFL. For each complexity analysis, the time and overhead of CMFL are considered. {Recall the previous notations, we define $C$ as the number of committee clients, $n$ as the training clients.} Noted that $m$ represents the number of aggregation clients. 

\subsubsection{Computaion Complexity}

There are five phases related to computation complexity as follows:

\begin{itemize}
  \item \textbf{Local Training}. Each training client and committee client performs SGD locally before they upload their local gradients. The computation overhead of $k$-th client is $\mathcal{O}(|\D_k|\cdot|\w|)$, where $|\D_k|$ is the number of data samples, $|\w|$ is the model size. Assumed that $|\D|$ represents the average value of $|\D_k|$ over all clients, the total computation overhead is {$\mathcal{O}((n + C)|\D|\cdot|\w|)$}. Because each client performs local training in parellel, the computation time of local training is $\mathcal{O}(|\D|\cdot|\w|)$. 
  \item \textbf{Scoring}. In the phase of scoring, each committee client assigns a score to each training client. According to the scoring system, the computation overhead of scoring for one committee client is {$\mathcal{O}(n \cdot |\w|^2)$}. The total computation overhead of scoring is {$\mathcal{O}(n\cdot C \cdot |\w|^2)$}. As each committee client performs scoring operation in parallel, the computation time of scoring is {$\mathcal{O}(n \cdot |\w|^2)$}. 
  \item \textbf{Aggregation}. In the phase of aggregation, only $m$ local gradients are used for aggregation, so the computation overhead of aggregation for one committee client is $\mathcal{O}(m\cdot |\w|)$. According to the CCP, each committee client performs aggregation so the total computation overhead of aggregation is {$\mathcal{O}(m\cdot C\cdot|\w|)$}. Since the aggregation process is performed in parallel, the computation time of aggregation is $\mathcal{O}(m\cdot |\w|)$. 
  \item \textbf{Selection}. In the phase of selection, each committee client sorts the received local gradients and selects $m$ local gradients for aggregation. The computation overhead is {$\mathcal{O}(nlogn)$}, which can be ignored since it is much smaller than the computation overhead of the above three phases. 
  \item \textbf{Election}. In the phase of the election, each committee client determines its new committee client set based on the sorted local gradients. The computation overhead is $\mathcal{O}(1)$, which can be ignored because it is much smaller than the computation overhead of the above four phases. 
\end{itemize}

\subsubsection{Communication Complexity}
Assumed that each client owns the same maximum bandwidth, and $r$ represents the max transmission rate, the communication time of transferring data $s$ is computed as $T_{transmission} = s/r$. There are three phases related to communication complexity as follows:
\begin{itemize}
  \item \textbf{Gradient Uploading}. After local training each training client uploads its local gradient to all the committee clients, so the communication overhead of uploading for one client is {$\mathcal{O}(C \cdot |\w|)$}. The total communication overhead of uploading is {$\mathcal{O}(n \cdot C \cdot |\w|)$}. Since each training client performs the uploading process in parellel, the communication time of uploading is {$\mathcal{O}(C \cdot |\w|/r)$}. 
  \item \textbf{Global Model Downloading}. After global aggregation, the primary committee client broadcasts the global model to all training clients in the next round. The total communication overhead of downloading is {$\mathcal{O}(n \cdot |\w|)$}, thus the communication time of downloading is {$\mathcal{O}(n \cdot |\w|/r)$}. 
  \item \textbf{Broadcasting}. Committee clients should perform CCP to reach consensus, in this phase the primary committee client broadcast $S_a^t$ and $S_c^{t+1}$ to other committee clients, which occur communication overhead. However, this communication overhead can be ignored because it is much smaller than the communication overhead of uploading and downloading. 
\end{itemize}
% \subsection{Communication-Efficient Method}

% In synchronize training of CMFL, the training clients must wait for data transmission and several operations of the committee before the next round. Besides, the training clients may be heterogeneous, which are equipped with different GPU, CPU, mobile computing, etc. The system heterogeneity leads to vastly different local training times as well as data transmission time. The committee will not perform the aggregation before receiving all the gradients, slowing down the training process. Therefore, we design a communication-efficient method to tackle this problem.

% In CMFL, the computation and communication time of local training can be overlapped with other process. Therefore, we utilize the timing model proposed by Li \etal \cite{NEURIPS2018_2c6a0bae} to implement the asynchronous training, which relaxes the iteration dependency to 2. That is, each training client locally trains the model and completes the data transfering by running two parellel treads. In this method, the update at round $t$ depends on the update at round $t-2$. The asynchronous training process is shown in Figure \ref{fig:TimingModel}. The total runtime during one round is $T = max\{T_{train}, max\{T_{uplink_1}, T_{downlink_1}\} + T_{scoring} + T_{aggregation} + max\{T_{uplink_2}, T_{downlink_2}\} \}$.
{Besides, system heterogeneity is a widespread problem in the field of Federated Learning. The differences in the computational performance of clients lead to various time consumptions, which becomes the bottleneck limiting the efficiency of the Federated Learning system. Some clients even drop out during the training process. There are some methods to alleviate the performance degradation caused by system heterogeneity, such as setting a maximum waiting time or a minimum number of received gradients. Indeed, system heterogeneity is an important issue in Federated Learning, and more works need to be carried out to address it.}









\section{Experiments}
\label{experiments}


In this section, we first present our experimental setup in Section \ref{setup}, which includes the datasets, models, and experimental environment. Then, we evaluate our proposed framework CMFL by five sets of following experiments, and the results and analysis are presented in Section \ref{nomaltrainingexperiment} to Section \ref{CommitteeAnalysisEvaluation}.

\begin{enumerate}
  \item \textbf{Normal Training Experiment.} We test the performance of CMFL without considering the Byzantine attack and compare it with typical FL.
  \item \textbf{Robustness Comparative Experiment.} We evaluate the Byzantine resilience of CMFL and compare it with several Byzantine-tolerant algorithms. 
  \item \textbf{Hyperparameter Analysis Experiment.} We vary the hyper-parameter $\alpha$, $\omega$, and $\epsilon$ and show how it affects the performance.
  \item \textbf{Efficiency Experiment.} We evaluate the efficiency of CMFL and compare it with other decentralized FL frameworks while considering the computation and communication overhead.
  \item \textbf{Committee Member Analysis Experiment.} We track the malicious clients that have been mixed into the committee members to analyze the impact of these malicious clients on the performance of the global model. 
\end{enumerate}
  


\subsection{Experimental Setup} \label{setup}

\subsubsection{Datasets and Models}

{We evaluate CMFL over three datasets with a Non-IID setting, including FEMNIST, Sentiment140, and Shakespeare.}

\begin{itemize}
  \item \textbf{FEMNIST-AlexNet.}\cite{caldas2019leaf}  FEMNIST (Federated Extended MNIST) is a real-world distributed dataset formed by a specific division of the EMNIST dataset. This dataset is used to train a model for handwritten digit/character recognition tasks, which contains 805263 images of $28 \times 28$ pixels, divided into 64 categories, each category represents a type of handwritten digit/character ($0-9$ and $a-z$). The FEMNIST dataset divides the EMNIST dataset into 3550 parts in a specific way and stores them on each client to simulate a real federated learning scenario. We use the convolutional neural network AlexNet as the basic experimental model for this image classification dataset.
  \item \textbf{Sentiment140-LSTM.} \cite{sent140} Sentiment140 is a real-world distributed dataset which focus on the text sentiment analysis task, including 1,600,000 tweets extracted using the Twitter API. The data in this dataset has been annotated (0 = negative, 2 = neutral, 4 = positive). It can be used to discover the sentiment of a brand, product, or topic on Twitter. We regard each Twitter account as a client and choose a two-layer LSTM binary classifier as the basic experimental model. The LSTM binary classifier includes 256 hidden units with pretrained 300D GloVe embedding\cite{pennington2014glove}.  
  \item {\textbf{Shakespeare-LSTM.} \cite{DBLP:conf/aistats/McMahanMRHA17} Shakespeare is a real-world distributed dataset built from \textit{The Complete Works of William Shakespeare}. Each speaking role corresponds to a different device, and we choose a two-layer LSTM classifier as the basic experimental model, which contains 100 hidden units with an 8D embedding layer. There are 80 classes of characters in dataset. The model takes a sequence of 80 characters as input, embeds each character into a learned 8-dimensional space, and outputs one character for each training sample after 2 LSTM layers and a densely connected layer.}
\end{itemize}

\begin{table*}[h]
\caption{Statistics of datasets}
\label{noniidlevel}
\centering
\begin{tabular}{c c c c c}
\toprule
  Dataset & Number of devices & Total samples & Mean & Stdev \\
  \midrule
  FEMNIST & 3,550 & 805,263 & 226.83 & 88.94 \\
  Sentiment140 & 772 & 40,783 & 53 & 32\\ 
  {Shakespeare} & {143} & {517,106} & {3616} & {6808}\\
\bottomrule
\end{tabular}



\end{table*}

The Non-IID levels of these two datasets are shown in Table \ref{noniidlevel}.

\subsubsection{Environment}


\hspace{1.0em} \textbf{Global Setup}: At the beginning of each communication round, $10\%$ of the idle clients are activated to participate in the training, while the rest clients wait for the next selection. At one communication round, each client runs one iteration of SGD for local training. Besides, the initial committee clients are selected from all clients at random. In order to better demonstrate the experimental effect, we set different learning rates $\eta$ in different datasets.


\textbf{Machines}: We perform our experiment on a commodity machine with Intel Core CPU i9-9900X containing a clock rate of 3.50 GHz with 10 cores and 2 threads per core. And we utilize Geforce RTX 2080Ti GPUs to accelerate training. The learning model is implemented in Python 3.7.6 and Tensorflow 1.14. 

\textbf{Hyper-parameter Notation}: In each communication round, $10\%$ of the whole clients are activated to participate in the training of that round, $\omega\%$ of which become the committee clients and the rest become the training clients. Each round $\alpha\%$ of the training clients become the aggregation clients, whose local gradients are accepted to use for the constructing of the global gradient. $\epsilon$ presents the percentage of malicious clients. 



\subsection{Nomal Training Experiment}\label{nomaltrainingexperiment}

\subsubsection{Experiment Setting}

In this experiment we set $\alpha = 40$, $\omega=40$ and $\epsilon=0$ to simulate a non-attack FL scene. The $\eta$ in FEMNIST and Sentiment140 is $0.001$ and $0.005$ respectively. In this experiment, we compared typical FL and CMFL under these two selection strategies:

\begin{itemize}
	\item \textbf{Typical FL.} Typical FL uses all the local gradients uploaded by the training client for the construction of global gradients.
	\item \textbf{CMFL with Selection Strategy \uppercase\expandafter{\romannumeral1}.} The original intention of this selection strategy is to resist Byzantine attacks. The committee accepts the local gradients uploaded by the high-score training clients while rejects those uploaded by low-score training clients.
	\item \textbf{CMFL with Selection Strategy \uppercase\expandafter{\romannumeral2}.} This selection strategy is opposite to selection strategy \uppercase\expandafter{\romannumeral1}, accepting the local gradients uploaded by the low-score clients while rejects those uploaded by high-score training clients.
\end{itemize}

\begin{figure*}[htbp]
	\centering
	
	\subfigure[FEMNIST-Accuracy]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/CMFL_FL_strategy_acc.png}
			%\caption{fig1}
		\end{minipage}%
	}%
  \subfigure[Sentiment140-Accuracy]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/FL_CMFL_strategy_acc_sent140.png}
      %\caption{fig1}
    \end{minipage}%
  }%
  \subfigure[Shakespeare-Accuracy]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/FL_CMFL_strategy_acc_shakes_new.png}
      %\caption{fig1}
    \end{minipage}%
  }%

	\subfigure[FEMNIST-Loss]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/CMFL_FL_strategy_loss.png}
			%\caption{fig2}
		\end{minipage}%
	}%
  \subfigure[Sentiment140-Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/FL_CMFL_strategy_loss_sent140.png}
      %\caption{fig2}
    \end{minipage}%
  }%  
  \subfigure[Shakespeare-Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/FL_CMFL_strategy_loss_shakes_new.png}
      %\caption{fig2}
    \end{minipage}%
  }%
	
	\centering
	\caption{The performance of global model among the typical FL and CMFL with two selection strategies.}
	\label{fig:CMFL_FL_strategy}
\end{figure*}
\subsubsection{Result Analysis}


We show the result in Figure \ref{fig:CMFL_FL_strategy}. Note that the performance of CMFL under selection strategy \uppercase\expandafter{\romannumeral2} is better than that of CMFL under selection strategy \uppercase\expandafter{\romannumeral1} and typical FL. The CMFL under the selection strategy \uppercase\expandafter{\romannumeral1} has the slowest model convergence rate and the worst model accuracy among the three. The result shows that it is a more appropriate design for the committee to accept the local gradients uploaded by the low-score clients when in non-attack FL scene, which can not only enhance the global model performance but also accelerate the convergence of the global model. This is because over the Non-IID dataset, the CMFL under the selection strategy \uppercase\expandafter{\romannumeral1} makes it difficult for a few clients which are quite different from other clients to participate in the aggregation process, resulting in the training of the global model only using part of the data instead of all of the data. {Figure \ref{fig:cleint_aggregation} proves it. We record the aggregation times of each client and plot them as a curve graph. From the curve, we can see that the aggregation times of client under FL conform to a Gaussian distribution, causing FL selects the aggregation randomly. Besides, we found that a high percentage of clients never participate in the aggregation process when performing CMFL under selection strategy \uppercase\expandafter{\romannumeral1}. That is why CMFL under selection strategy \uppercase\expandafter{\romannumeral1} achieves worse performance than typical FL and CMFL under selection strategy \uppercase\expandafter{\romannumeral2}. It is difficult for the global model to learn comprehensive knowledge while using this kind of training algorithm. However, the CMFL under the selection strategy \uppercase\expandafter{\romannumeral2} significantly reduces the number of such clients. In this way more clients can participate in the aggregation process, making the global model learn a more comprehensive knowledge. Figure \ref{fig:client_acc} and \ref{fig:aggregation_acc} explain why CMFL under selection strategy \uppercase\expandafter{\romannumeral2} achieves a better performance than typical FL. We record the testing accuracy of each client after training. Figure \ref{fig:client_acc} shows that CMFL under selection strategy \uppercase\expandafter{\romannumeral2} has least clients with low accuracy, while has most clients with high accuracy. That is because those clients with low accuracy have more opportunities to participate in the aggregation process, which is proved by Figure \ref{fig:aggregation_acc}. Figure \ref{fig:aggregation_acc} shows that the aggregation times of clients with low accuracy in CMFL under selection strategy \uppercase\expandafter{\romannumeral2} are much more than other training methods. By reducing the proportion of clients with low accuracy, CMFL under selection strategy \uppercase\expandafter{\romannumeral2} achieves a better performance than typical FL. } 

% The committee accepts the local gradient uploaded by low-score clients means that the committee is more inclined to select those clients that are not similar to its members to participate in the aggregation process, which makes those different clients easier to participate in the aggregation process and makes full use of all data for the training of the global model.

Nevertheless, it is not advisable to choose strategy \uppercase\expandafter{\romannumeral2} when considering the Byzantine attack. According to the scoring system we designed, the malicious client will get a lower score than the honest client. Taking the selection strategy \uppercase\expandafter{\romannumeral2} means that Byzantine attackers can easily attack the global model by uploading malicious local gradients. In this scenario, choosing strategy \uppercase\expandafter{\romannumeral1} is a more appropriate choice by avoiding the local gradients with low scores to participate in the aggregation, which is likely to be malicious gradients. Although the effect of CMFL under strategy \uppercase\expandafter{\romannumeral1} in the non-attack FL scenario is slightly weaker than that of the typical FL, it can make the training process more robust in a wider range of realistic scenarios. 

\begin{figure*}[htbp]
  \centering
  
  \subfigure[]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=6cm]{img/distributed_map.png}\label{fig:cleint_aggregation}
    \end{minipage}%
  }%
  \subfigure[]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=6cm]{img/acc_distributed.png}\label{fig:client_acc}
    \end{minipage}%
  }%
  \subfigure[]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=6cm]{img/acc_times.png}\label{fig:aggregation_acc}
    \end{minipage}%
  }%
  
  \centering
  \caption{Figure (a) shows the number of clients with different aggregation times. Figure (b) shows the number of clients with different accuracy. Figure (b) shows the aggregation times of clients with different accuracy.}
  \label{fig:furtherAnalysis}
\end{figure*}

\subsection{Robustness Comparative Experiment}\label{comparativeExperiment}


\subsubsection{Experiment Setting}

In this experiment we set $\alpha = 40$, $\omega=40$ and $\epsilon=10$, and we test the Byzantine resilience of CMFL under two selection strategies by comparing it with the following Byzantine-tolerant algorithms, which are all aim to design a robust gradient aggregation method.

\begin{itemize}
  \item \textbf{Median}\cite{pmlr-v80-yin18a}: This aggregation method first sorts the gradients and selects the median to be the global gradient.
  \item \textbf{Trimmed Mean}\cite{pmlr-v80-yin18a}: This aggregation method first sorts the local gradients, then removes the maximum value of $\beta\%$ and the minimum value of $\beta\%$, and finally aggregates the remaining local gradients to construct the global gradient, where $\beta \in [0,50)$.
  \item \textbf{Krum}\cite{Blanchard2017Byzantine}: This aggregation method assumes that the directions of the local gradients uploaded are relatively similar and sort these local gradients according to their similarity. The original Krum algorithm only selects the first local gradient after sorting as the global gradient.
  \item \textbf{Multi-Krum}\cite{Blanchard2017Byzantine}: The Multi-Krum algorithm is an improved version of the original Krum, which selects the top $\alpha\%$ of the sorted local gradients to construct the global gradient. Compared with the original Krum, Multi-Krum reduces the fluctuation of the global model effect caused by random factors, making the training process more stable. Since Multi-Krum uses more local gradients to construct the global gradients, it can make the global model converge faster than the original Krum.
\end{itemize}

We compare these Byzantine-tolerant algorithms with CMFL considering three different types of attacks: gradient scaling attack, same-value gaussian attack, and back-gradient attack, where the attackers are all aim to compromise some clients and upload the malicious gradients.

\begin{itemize}
	\item \textbf{Gradient Scaling Attack}: The malicious clients multiply each element in the local gradient by a random value $\lambda\in [a, 1)$, where $a$ is a defined constant which indicates the magnitude of the attack. In this experiment we set $a=0.5$. 
	\item \textbf{Same-value Attack}\cite{DBLP:conf/aaai/LiXCGL19}: The malicious clients replace the local gradient with a vector of the same size whose elements are all 0.
	\item \textbf{Back-gradient Attack}\cite{10.1145/3128572.3140451}: The malicious clients replace the local gradient with a vector of the same size in the opposite direction.
\end{itemize}

\subsubsection{Result Analysis}

We show the performance of CMFL compared with other Byzantine-tolerant algorithms under various attacks in Figure \ref{fig:EXP1_FEMNIST}-\ref{fig:EXP1_Shakes}. We analyzed the performance of each Byzantine-tolerant algorithm.

\begin{itemize}
\item \textbf{CMFL-selection strategy \uppercase\expandafter{\romannumeral1}} always reaches the fastest global convergence speed and highest accuracy among these five algorithms. Note that in the model accuracy curve, the CMFL curve fluctuates more obviously than Median and Multi-Krum, which is the normal curve fluctuation caused by the replacement of committee members. Nevertheless, its overall accuracy rate is still higher than the other algorithms.
\item \textbf{CMFL-selection strategy \uppercase\expandafter{\romannumeral2}} achieves as poor performance as typical FL. Obviously, the malicious clients can easily participate in the aggregation process.
\item \textbf{Median}\cite{pmlr-v80-yin18a} has maintained a relatively stable performance under a variety of attacks. It only selects the median of the local gradients as the global gradient, which is regarded as a relatively conservative approach. Although Median can effectively resist Byzantine attacks, it is difficult to achieve excellent model performance.
\item \textbf{Trimmed Mean}\cite{pmlr-v80-yin18a} has an unstable performance under different attacks. Trimmed Mean performs well under gradient scaling attack but still not as good as Median and Multi-Krum, and performed extremely badly under same-value attack and back-gradient attack. This Byzantine-tolerant algorithm cannot effectively resist the Byzantine attacks. 
\item \textbf{Krum}\cite{Blanchard2017Byzantine} performs poorly under all three attacks, and there were sharp fluctuations in the training process. Krum only selects the first local gradient after sorting as the global gradient each time. This aggregation method that does not consider the overall results in severe fluctuations of the global model.
\item \textbf{Multi-Krum}\cite{Blanchard2017Byzantine} has the second-best model performance overall. Multi-Krum improves the original Krum and eliminates the jitter generated during the training process. 
\end{itemize}

Note that the CMFL is the only framework without involving a central server, which naturally achieves robustness against the influence of a malicious server. Other Byzantine-tolerant algorithms cannot achieve the same robustness owing to their naturally centralized architecture.


\begin{figure*}[htbp]
	\centering
	
	\subfigure[Gradient Scaling Attack Loss]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/mj_attack_method_loss_1.png}
			%\caption{fig1}
		\end{minipage}%
	}%
	\subfigure[Same-value Attack Loss]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/mj_attack_method_loss_2.png}
			%\caption{fig2}
		\end{minipage}%
	}%
	\subfigure[Back-gradient Attack Loss]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/mj_attack_method_loss_6.png}
			%\caption{fig2}
		\end{minipage}%
	}%	
	
	\subfigure[Gradient Scaling Attack Accuracy]{
		\begin{minipage}[t]{0.295\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/mj_attack_method_acc_1.png}
			%\caption{fig2}
		\end{minipage}
	}%
	\subfigure[Same-value Attack Accuracy]{
		\begin{minipage}[t]{0.295\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/mj_attack_method_acc_2.png}
			%\caption{fig2}
		\end{minipage}
	}%
	\subfigure[Back-gradient Attack Accuracy]{
		\begin{minipage}[t]{0.295\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/mj_attack_method_acc_6.png}
			%\caption{fig2}
		\end{minipage}%
	}%
	
	\centering
	\caption{Performance of CMFL compared with other Byzantine-tolerant algorithms under various attacks over FEMNIST dataset. CMFL\uppercase\expandafter{\romannumeral1} represents CMFL under selection strategy \uppercase\expandafter{\romannumeral1} and CMFL \uppercase\expandafter{\romannumeral2} represents CMFL under selection strategy\uppercase\expandafter{\romannumeral2}.}
	\label{fig:EXP1_FEMNIST}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  
  \subfigure[Gradient Scaling Attack Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/new_sent140_attack_method_loss_1_2000round.png}
      %\caption{fig1}
    \end{minipage}%
  }%
  \subfigure[Same-value Attack Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/new_sent140_attack_method_loss_2_2000round.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  \subfigure[Back-gradient Attack Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/new_sent140_attack_method_loss_6_2000round.png}
      %\caption{fig2}
    \end{minipage}%
  }%  
  
  \subfigure[Gradient Scaling Attack Accuracy]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/sent140_attack_method_acc_1_2000round.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Same-value Attack Accuracy]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/sent140_attack_method_acc_2_2000round.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Back-gradient Attack Accuracy]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/sent140_attack_method_acc_6_2000round.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  
  \centering
  \caption{Performance of CMFL compared with other Byzantine-tolerant algorithms under various attacks over Sentiment140 dataset.}
  \label{fig:EXP1_Sent140}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  
  \subfigure[Gradient Scaling Attack Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/shakes_attack1_loss.png}
      %\caption{fig1}
    \end{minipage}%
  }%
  \subfigure[Same-value Attack Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/shakes_attack2_loss.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  \subfigure[Back-gradient Attack Loss]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/shakes_attack6_loss.png}
      %\caption{fig2}
    \end{minipage}%
  }%  
  
  \subfigure[Gradient Scaling Attack Accuracy]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/shakes_attack1_acc.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Same-value Attack Accuracy]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/shakes_attack2_acc.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Back-gradient Attack Accuracy]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/shakes_attack6_acc.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  
  \centering
  \caption{Performance of CMFL compared with other Byzantine-tolerant algorithms under various attacks over Shakespeare dataset.}
  \label{fig:EXP1_Shakes}
\end{figure*}





%\subsubsection{Gaussian Attack} 
%The attacker adds Gaussian random vectors with zero mean and standard deviation $b$ to the local gradients to perturb the local gradients, where $b$ is also a defined constant which indicates the magnitude of the attack.


\subsection{Hyper-parameter Analysis Experiment}\label{parameterAnalysisExperiment}

\subsubsection{Experiment Setting}

In this experiment, we consider the effect of hyper-parameter by varying the hyper-parameter $\alpha$, $\omega$ and $\epsilon$. Specifically, we consider the back-gradient attack and designed three sets of sub-experiments. In each set of sub-experiments, we fixed one of the hyper-parameters and changed the other two hyper-parameters to analyze their impacts on the model performance.

\begin{itemize}
	\item \textbf{Sub-experiment \uppercase\expandafter{\romannumeral1}.} Fixed $\alpha=40$ and vary $\omega, \epsilon$ to $\{10,20,30,40,50\}$.
	\item \textbf{Sub-experiment \uppercase\expandafter{\romannumeral2}.} Fixed $\omega=40$ and vary $\alpha, \epsilon$ to $\{10,20,30,40,50\}$.
	\item \textbf{Sub-experiment \uppercase\expandafter{\romannumeral3}.} Fixed $\epsilon=10$ and vary $\alpha, \omega$ to $\{10,20,30,40,50\}$.
\end{itemize}

\begin{figure*}[htbp]
	\centering
	
	\subfigure[$\alpha=40$]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/complete_aggregation.png}
			%\caption{fig1}
		\end{minipage}%
	}%
	\subfigure[$\omega=40$]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/complete_committee.png}
			%\caption{fig2}
		\end{minipage}%
	}%
	\subfigure[$\epsilon=10$]{
		\begin{minipage}[t]{0.3\linewidth}
			\centering
			\includegraphics[width=5.5cm]{img/complete_attack.png}
			%\caption{fig2}
		\end{minipage}%
	}%
	
	\centering
	\caption{Model performance of CMFL with varying the hyper-parameters.}
	\label{fig:EXP2}
\end{figure*}

% \begin{figure*}[htbp]
% 	\centering
	
% 	\subfigure[$\alpha=40$]{
% 		\begin{minipage}[t]{0.35\linewidth}
% 			\centering
% 			\includegraphics[width=6cm]{img/complete_aggregation.png}
% 			%\caption{fig1}
% 		\end{minipage}%
% 	}%
% 	\subfigure[$\omega=40$]{
% 		\begin{minipage}[t]{0.35\linewidth}
% 			\centering
% 			\includegraphics[width=6cm]{img/complete_committee.png}
% 			%\caption{fig2}
% 		\end{minipage}%
% 	}%
% 	\subfigure[$\epsilon=40$]{
% 		\begin{minipage}[t]{0.35\linewidth}
% 			\centering
% 			\includegraphics[width=6cm]{img/complete_attack_high_acc}
% 			%\caption{fig2}
% 		\end{minipage}%
% 	}%	
	
% 	\subfigure[$\alpha=10$]{
% 		\begin{minipage}[t]{0.345\linewidth}
% 			\centering
% 			\includegraphics[width=6.2cm]{img/complete_aggregation_low_acc}
% 			%\caption{fig2}
% 		\end{minipage}
% 	}%
% 	\subfigure[$\omega=10$]{
% 		\begin{minipage}[t]{0.345\linewidth}
% 			\centering
% 			\includegraphics[width=6.2cm]{img/complete_committee_low_acc}
% 			%\caption{fig2}
% 		\end{minipage}
% 	}%
% 	\subfigure[$\epsilon=10$]{
% 		\begin{minipage}[t]{0.345\linewidth}
% 			\centering
% 			\includegraphics[width=6.2cm]{img/complete_attack.png}
% 			%\caption{fig2}
% 		\end{minipage}%
% 	}%
	
% 	\centering
% 	\caption{Model performance of CMFL with varying the hyper-parameters.}
% 	\label{fig:EXP2}
% \end{figure*}

\subsubsection{Result Analysis}
 
We show the results in Figure \ref{fig:EXP2} and the analysis as follows:

\begin{itemize}
\item \textbf{Appropriately increasing the number of committee members can enhance the performance of the global model.} The results show that within a suitable parameter value range (e.g., $\alpha=10, \epsilon \leq 30, \omega \leq 30$), more committee members lead to better global model performance. This is mainly since an appropriate increase in the number of committee members can enhance the robustness of the committee to a certain extent, in the meanwhile avoid the existence of "dictators" and prevent the training process from being controlled by a small number of clients, which makes it difficult for some other clients to participate in the aggregation process. 
\item \textbf{Appropriately increasing the proportion of the aggregation clients can enhance the performance of the global model.} Within a suitable parameter value range, a higher proportion of the aggregation clients leads to better global model performance. Because in the normal operation of the committee, sorting the local gradients uploaded by the training client according to their scores makes the honest local gradients come first and the malicious gradients second. In this ideal situation, we control the proportion of aggregated clients to be less than a threshold $\chi$ to prevent malicious gradients from participating in the aggregation process. Under the limit of $\alpha \in (0,\chi]$, the increase of $\alpha$ means that more honest local gradients are selected to construct the global gradient in each round, which can achieve better global model performance.
% \item \textbf{More malicious clients may lead to better global model performance in some special cases.} Note that in certain parameter combinations, more malicious clients make the global model have better performance, which is an unexpected result. This is because if the committee has enough members, a small number of mixed malicious clients cannot influence the decision made by the committee. In the case where most of the committee members are honest clients, the committee will still assign the lowest score to malicious clients. However, a small number of malicious clients mixed into committee members will make the scores meaningless for the honest clients. Specifically, since the scores made by the malicious committee members make the total scores of each training client almost the same, the scoring system lost its distinguishing ability of the honest clients to some degree. Therefore, every honest client owner has the same opportunity to participate in the aggregation process, which makes the global model have better performance.
\item \textbf{Excessive $\alpha$, $\omega$, and $\epsilon$ will lead to a cliff-like decline in the performance of the global model.} When we increase the proportion of committee members, it means that both malicious clients and honest clients have a greater probability of being elected as committee members. This increases the probability of malicious clients mixing into committee members and damage the committee’s scoring system, making it difficult to assign the correct score to the training clients. In the worst situation, once the proportion of the malicious clients among committee members is relatively large, the scores of the malicious clients will be higher than those of the honest clients, resulting in the malicious local gradient uploaded by the malicious clients being used to construct the global gradient. Hence, the performance of the global model will be devastatingly damaged. When we increase the proportion of the aggregation clients, it also means increasing the probability of malicious local gradients participating in the aggregation procedure. When the proportion of aggregation clients exceeds the threshold that the system can tolerate, the malicious local gradients are used to construct the global gradient, which causes irreparable damage to the performance of the global model. By the same token, if there are too many malicious clients, the malicious local gradients uploaded by them are more likely to participate in the aggregation process. As long as a malicious local gradient is aggregated, the performance of the global model will be greatly reduced.
\end{itemize}


\subsection{Efficiency Experiment}
  
In this section, we evaluate the efficiency of CMFL over FEMNIST and Sentiment140 dataset. Besides, we compare it with other decentralized FL frameworks. 

\subsubsection{Experiment Settting}

\textbf{Implementation Detail}. In this experiment, we simulate the data transferring by calculating the transmission time using $T_{transmission} = s/r$, where $s$ represents the data and $r$ represents the transmission rate. We let the program process wait for $T_{transmission}$ second when it needs to transmit data for simulating the data transferring in a real scene. The maximum number of communication rounds is 600. 

\textbf{Hyper-parameter Setting}. In order to exclude the influence of irrelevant variables, we conduct this evaluation without considering the Byzantine attacks. The number of committee clients and training clients is set as 43 and 65. And the aggregation rate $\alpha$ is set as $40\%$. We set different maximum transmission rates for one client: $1Mps$, $10Mps$, and $100Mps$, and under each transmission rate, we evaluate the performance of CMFL using wall-clock time, which includes computation time and communication time. 

\textbf{Comparative Method}. We compare the efficiency of CMFL with three algorithms: typical FL, BrainTorrent\cite{DBLP:journals/corr/abs-1905-06731}, and GossipFL\cite{DBLP:journals/corr/abs-1908-07782}.

\subsubsection{Result Analysis}
{
Figure \ref{fig:communication} shows the result. Noted that the size of AlexNet ($\approx 200$M) is much bigger than that of LSTM ($\approx 16$k), so the communication time over FEMNIST dataset is much longer than that over the Sentiment140 dataset. The overall performance of CMFL is better than the other two decentralized FL frameworks but worse than typical FL. Typical FL has an overall better performance than CMFL because the clients do not have to communicate with each other. It is a centralized framework so the clients just send their local gradients to a server for aggregation. Never can the other three decentralized frameworks do that because the client has to broadcast their local gradients to other clients for reaching consensus, in which case the transmission of the local gradients occurs communication overhead. With the increasing transmission rate, the advantage of typical FL becomes smaller. The performance of the typical FL is taken over by CMFL when the transmission is up to $100Mps$ over the Sentiment140 dataset. CMFL always has a better performance than the other two decentralized frameworks, because it utilized the committee system to reduce the communication overhead, which is proved in Figure \ref{fig:communication_computation}. The clients do not need to broadcast their local gradients to other clients but send their local gradients to committee clients. The committee clients can reach a consensus with lower communication overhead by performing CCP, while the other two decentralized frameworks must cost higher communication overhead. Considering the malicious server in a real scenario, CMFL can achieve both robustness and efficiency. 
}



\begin{figure*}[htbp]
  \centering
  
  \subfigure[FEMNIST-1Mps]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_femnist_1.png}
      %\caption{fig1}
    \end{minipage}%
  }%
  \subfigure[FEMNIST-10Mps]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_femnist_10.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  \subfigure[FEMNIST-100Mps]{
    \begin{minipage}[t]{0.3\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_femnist_100.png}
      %\caption{fig2}
    \end{minipage}%
  }%  
  
  \subfigure[Sentiment140-1Mps]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_sent140_1.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Sentiment140-10Mps]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_sent140_10.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Sentiment140-100Mps]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_sent140_100.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  
  \centering
  \caption{Performance of typical FL and three decentralized FL frameworks under different transmission rates over FEMNIST dataset and Sentiment140 dataset.}
  \label{fig:communication}
\end{figure*}

\begin{figure*}[htbp]
  \centering
  
  % \subfigure[FEMNIST-1Mps]{
  %   \begin{minipage}[t]{0.3\linewidth}
  %     \centering
  %     \includegraphics[width=5.5cm]{img/communication_femnist_1.png}
  %     %\caption{fig1}
  %   \end{minipage}%
  % }%
  % \subfigure[FEMNIST-10Mps]{
  %   \begin{minipage}[t]{0.3\linewidth}
  %     \centering
  %     \includegraphics[width=5.5cm]{img/communication_femnist_10.png}
  %     %\caption{fig2}
  %   \end{minipage}%
  % }%
  % \subfigure[FEMNIST-100Mps]{
  %   \begin{minipage}[t]{0.3\linewidth}
  %     \centering
  %     \includegraphics[width=5.5cm]{img/communication_femnist_100.png}
  %     %\caption{fig2}
  %   \end{minipage}%
  % }%  
  
  \subfigure[Sentiment140-1Mps]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_computation_sent140_1.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Sentiment140-10Mps]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_computation_sent140_10.png}
      %\caption{fig2}
    \end{minipage}
  }%
  \subfigure[Sentiment140-100Mps]{
    \begin{minipage}[t]{0.295\linewidth}
      \centering
      \includegraphics[width=5.5cm]{img/communication_computation_sent140_100.png}
      %\caption{fig2}
    \end{minipage}%
  }%
  
  \centering
  \caption{The total communication time and computation time of typical FL and three decentralized FL frameworks over the Sentiment140 dataset.}
  \label{fig:communication_computation}
\end{figure*}



\subsection{Committee Member Analysis Experiment}\label{CommitteeAnalysisEvaluation}

\subsubsection{Experiment Setting}

In this experiment we set $\alpha = 40, \omega = 30$ and vary $\epsilon$ to $\{10,20,30,40,50\}$. By recording the number of malicious training clients, malicious committee clients and malicious aggregation clients during the training process, we analyze the influence of committee members on the performance of the global model. 

\begin{figure}
	\centering
	\includegraphics[width=7cm]{./img/malicious_committee_num.png}
	\caption{Number of malicious clients in the training process, where $N_1$ denotes the number of malicious training clients, $N_2$ denotes the malicious committee clients and $N_3$ denotes the malicious aggregation clients.}
	\label{fig:malicious_committe}
\end{figure}

\subsubsection{Result Analysis}

We show the result in Figure \ref{fig:malicious_committe}. Since in each round the training clients are randomly selected from the non-committee clients, when $\epsilon$ increases, the number of malicious training clients also increases. And with the increase of $\epsilon$, some malicious clients will inevitably be mixed into the committee members. Nevertheless, a small number of miscellaneous malicious clients cannot destroy the entire committee's scoring system and influence the committee's judgment, which instead improve the performance of the global model. This is mainly because the scoring system does not lose the ability to distinguish between the malicious clients and the honest clients since those malicious clients still receive a score much lower than that of honest clients. But for the honest clients, they get almost the same score due to the extreme scores assigned by the malicious committee clients. In this case, our method is equivalent to the typical FL method without considering the Byzantine attack. The typical FL achieves better performance than CMFL in such a setting, which has shown in Experiment \ref{nomaltrainingexperiment}. However, if there are too many mixed malicious clients, the committee's scoring system will be destroyed and the committee members will lose the ability to eliminate malicious local gradients, resulting in a sharp drop in the performance of the global model.  

\section{Conclusion}
\label{conclusion}

In this paper, we propose a serverless FL framework under committee mechanism, which can ensure robustness when considering Byzantine attacks. Besides, we present the convergence guarantees for our proposed framework. Motivated by the insight from the theoretical analysis we design the election and selection strategies, which empower the model the robustness against both the Byzantine attack and malicious server problem. The experiment results demonstrate the outperformance of the model over the typical federated learning and Byzantine-tolerant models, which further verify the effectiveness and robustness of the proposed framework. 

Currently, the proposed framework mainly ensures the robustness of the aggregation procedure by detecting the abnormal local gradients. While in the face of targeted attacks, such as a backdoor attack, how to design suitable election and selection strategies with theoretical guarantee remains an open and worth-exploring topic in the future. 




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices



% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

The research is supported by the National Key R\&D Program of China (2020YFB1006001), the National Natural Science Foundation of China (62176269, 62006252), the Key-Area Research and Development Program of Guangdong Province (2020B010165003), and the Innovative Research Foundation of Ship General Performance (25622112).



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% \<OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% 模版自带的注释方式
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}
\bibliographystyle{IEEEtran}
\bibliography{ref}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}
 [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/chunjiang.jpg}}]{Chunjiang Che} is currently working toward the B.E. degree in the School of Computer Science, Sun Yat-sen University, Guangzhou, China. His recent research interests include blockchain, federated learning and optimization.
\end{IEEEbiography}
% insert where needed to balance the two columns on the last page with
% biographies
%\newpage
\begin{IEEEbiography}
 [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/xiaoli.jpg}}]{Xiaoli Li} is currently working toward the PhD degree in the School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China. She received the Master's degree in computer architecture from University of Electronic Science and Technology of China, Chengdou, China, in 2011. Her research interests include services computing, software engineering, cloud computing, machine learning and federated learning.
\end{IEEEbiography}
%\begin{IEEEbiographynophoto}{Fenfang Xie}
% Biography text here.
%\end{IEEEbiographynophoto}
%%fenfang xie%%

\begin{IEEEbiography}
 [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/chenchuan.jpg}}]{Chuan Chen} received the B.S. degree from Sun Yat-sen University, Guangzhou, China, in 2012, and the Ph.D. degree from Hong Kong Baptist University, Hong Kong, in 2016. He is currently an Associate Professor with the School of Computer Science and Engineering, Sun Yat-Sen University. He published over 50 international journal and conference papers. His current research interests include machine learning, numerical linear algebra, and numerical optimization.
\end{IEEEbiography}
\begin{IEEEbiography}
 [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/xiaoyu.jpg}}]{Xiaoyu He}
received the B.Eng. degree in computer science and technology from Beijing Electronic Science and Technology Institute, Beijing, China, in 2010, the M.Sc. degree in public administration from South China University of Technology, Guangzhou, China, in 2016, and the Ph.D. degree in computer science from Sun Yat-sen University, Guangzhou, in 2019. He is currently a postdoctoral fellow at School of Data and Computer Science, Sun Yat-sen University. His research interests include evolutionary computation and machine learning.
\end{IEEEbiography}
\begin{IEEEbiography}
  [{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{img/zibinzheng.png}}]{Zibin Zheng} received his PhD degree from the Chinese University of Hong Kong in 2011. He is currently a professor in the School of Computer Science and Engineering at Sun Yat-sen University, China. He has published over 150 international journal and conference papers, including three ESI highly cited papers. According to Google Scholar, his papers have more than 13,590 citations, with an H-index of 54. His research interests include blockchain, smart contract, services computing, software reliability.
\end{IEEEbiography}


% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




\onecolumn
\appendices
\renewcommand\thesection{\Alph{section}}

\section{Proof of the Theorem 1}
\label{proof}

\subsection{Definition and Lemma}

In our analysis, the $(t)$ is defined as the index of the total local SGD iterations, where $(i) = t\tau + i$. We iterpret $\w_k^{(i)}$ as the local model of $k$-th client at iteration $(t)$. In such a setting, the aggregation client set and the commitee client set are expressed as $S_a^{(t)}$ and $S_c^{(t)}$. Note that the aggregation clients and committee clients perform $\tau$ iterations of local SGD, the aggregation client set $S_a^{(t)}$ and the committee client set $S_c^{(t)}$ remain the same for every $\tau$ iterations. That is, $S_a^{(t)} = S_a^{(t+1)} = ...=S_a^{(t+\tau-1)}$ and $S_c^{(t)} = S_c^{(t+1)} = ...=S_c^{(t+\tau-1)}$, where $t \% \tau = 0$. The local gradient of $k$-th client is expressed as $g_k(\w_k^{(t)},\B_{k}^{(t)})$, which is used for the local SGD training:
\begin{equation}
\begin{split}
&\w_k^{(t+1)} = \w_k^{(t)} - \eta_tg_k(\w_k^{(t)},\B_{k}^{(t)}),
\end{split}
\end{equation}where $\w_k^{(t)}$ denotes the local model of the $k$-th client at iteration $t$. Actually we only update the global model $\overline{\w}^{(t)}$ after every $\tau$ rounds, but for the need of convergence proof and analysis we assume the $\overline{\w}^{(t)}$ is updated at each iteration as follows:
\begin{equation}\label{globalUpdate2}
\begin{split}
&\overline{\w}^{(t+1)} = \overline{\w}^{(t)} - \eta_t\overline{g}^{(t)} = \overline{\w}^{(t)} - \eta_t(\sum_{k\in S^{(t)}_a}p_{k,S_a^{(t)}}g_k(\w_k^{(t)},\B_k^{(t)})),
\end{split}
\end{equation}where $\eta_t$ is the learning rate at iteration $(t)$. At iteration $(t)$ which satisfies $t\%\tau=0$, the $k$-th client downloads the $\overline{\w}^{(t)}$ to the local as the new local model $\w_k^{(t)}$, as a result, the whole updating method of the local model can be written as follows:

\begin{equation}
\label{localUpdate}
\w_k^{(t+1)} = \left\{
\begin{aligned}
& \w_k^{(t)} - \eta_t g_k(\w_k^{(t)},\B_k^{(t)}), (t+1) \% \tau \neq 0 \\
&\overline{\w}^{(t+1)}, (t+1) \% \tau = 0.
\end{aligned}
\right.
\end{equation}Then we introduce our proposed lemmas.
% \begin{lemma}\label{l1}
% \textbf{(Global-Local Optimal Gap).} According to the definitions of $\w^*$, $\w^*_k$ and $\overline{\w}_c^{(t)}$, we have that
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\w^*)-F_k^*) \\
% \leq& \frac{L}{2}\sum_{k \in \s}||\w^* - \w_k^*||^2 \\
% \leq& \frac{L}{2}\sum_{k \in \s}||\w^* - \overline{\w}_c^{(t)}||^2 + \frac{L}{2}\sum_{k \in \s}||\overline{\w}_c^{(t)}-\w_k^*||^2 \\
% &- L\sum_{k \in \s}\langle \w^*-\overline{\w}_c^{(t)},\w_k^*-\overline{\w}_c^{(t)} \rangle.
% \end{split}
% \end{equation}
% \end{lemma}
% \begin{lemma}\label{l2}
% \textbf{(Global-Local Parameter Gap).} According to the definitions of $\overline{\w}^{(t)}$ and $\w_k^{(t)}$, we have that
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 \\
% =& \sum_{k \in \s}||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 + \sum_{k' \in S_a^{(t-1)}}||\w_{k'}^{(t-1)}-\overline{\w}_c^{(t)}||^2 \\
% &- \sum_{k \in \s}\sum_{k \in S_a^{(t-1)}}\langle\w_k^{(t)} - \overline{\w}_c^{(t)},\w_{k'}^{(t-1)} - \overline{\w}_c^{(t)}\rangle.
% \end{split}
% \end{equation}
% \end{lemma}
% \begin{lemma}\label{l3}
% \textbf{(Global Parameter-Optimal Gap)}. According to the definitions of $\overline{\w}^{(t)}$, $\w^*_k$ and $\overline{\w}_c^{(t)}$, we have that
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\overline{\w}^{(t)})-F_k^*) \\
% \geq& \frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\w_k^* - \overline{\w}_c^{(t)}||^2 \\
% &+\frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\overline{\w_c}^{(t)}-\w_{k'}^{(t-1)}||^2 \\
% &+\frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}\langle\w^*_k-\overline{\w}_c^{(t)},\overline{\w}_c^{(t)}-\w_{k'}^{(t-1)}\rangle.
% \end{split}
% \end{equation}
% \end{lemma}
\begin{lemma}\label{l1}
\textbf{(Global-Local Gradient Product). } \textit{According to the definitions of $\nabla F_k(\w_k^{(t)})$, we have that}
\begin{equation}
\begin{split}
&-\langle \overline{\w}^{(t)}-\w^*,\sum_{k\in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})\rangle \\
\leq& \frac{1}{2\eta_t}\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 + \frac{\eta_t}{2}\sum_{k\in S_a^{(t)}}p_{k,\s}||\nabla F_k(\w_k^{(t)})||^2 - \sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)})-F_k(\w^*))\\
&- \frac{\mu}{2}\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2. 
\end{split}
\end{equation}
\end{lemma}
\begin{lemma}\label{l2}
\textbf{(Local Parameter-Optimal Gap).} \textit{Let $v_t = 2\eta_t-4L\eta_t^2 = 2\eta_t(1-2L\eta_t)$, we have that}
\begin{equation}
\begin{split}
&-\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*)\\
\leq& -(1-\eta_tL)\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F^*)+ \frac{1}{v_t}\sum_{k\in \s}p_{k,\s}||\w_k^{(t)} - \overline{\w}^{(t)}||^2.
\end{split}
\end{equation}
\end{lemma}
% \begin{lemma}\label{l3}
% \textbf{(Training-Committee European Bound).} According to the Definition \ref{G1} and \ref{G2}, we have that
% \begin{equation}
% \begin{split}
% &\mathbb{E}\sum_{k \in \s}||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 \leq 4m\eta_t^2\tau^2G_1^2, 
% \end{split}
% \end{equation}
% and
% \begin{equation}
% \begin{split}
% &\mathbb{E}\sum_{k' \in S_a^{(t-1)}} ||\w_{k'}^{(t-1)}-\overline{\w}_c^{(t)}||^2 \leq 4m\eta_t^2\tau^2G_2^2.
% \end{split}
% \end{equation}
% \end{lemma}

\begin{lemma}\label{l4}
\textbf{(Client Heterogeneous Bound).} \textit{According to the Assumption 4, we have that}
\begin{equation}
\begin{split}
&\mathbb{E}[\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)}-\w_k^{(t)}||^2] \leq 16\eta_t^2\tau^2G^2.
\end{split}
\end{equation}
\end{lemma}


\subsection{Proof of Lemma}

In this section we will prove Lemma \ref{l1} to \ref{l4}. 
% \subsubsection{Proof of Lemma \ref{l1}}
% Due to the Assumption \ref{A1}, we have
% \begin{equation}
% \begin{split}
% F_k(\w^*) - F_k^* \leq \frac{L}{2} ||\w^*-\w_k^*||^2.
% \end{split}
% \end{equation}
% The above operations are consistent with the sum operation, so we have
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\w^*) - F_k^*) \leq \frac{L}{2}\sum_{k \in \s}||\w^* - \w_k^*||^2.
% \end{split}
% \end{equation}
% In order to consider the impact of the committee node model on the training process, we introduce $\overline{\w}_c^{(t)}$ into the above formula:
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\w^*) - F_k^*) \leq \frac{L}{2}\sum_{k \in \s}||\w^* - \overline{\w}_c^{(t)} + \overline{\w}_c^{(t)} - \w_k^*||^2.
% \end{split}
% \end{equation}
% Expand the above formula, we have
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\w^*) - F_k^*) \\
% \leq& \frac{L}{2}\sum_{k \in \s}||\w^* - \overline{\w}_c^{(t)}||^2 + \frac{L}{2}\sum_{k \in \s}||\overline{\w}_c^{(t)}-\w_k^*||^2 \\
% &- L\sum_{k \in \s}\langle\w^*-\overline{\w}_c^{(t)},\w_k^*-\overline{\w}_c^{(t)}\rangle.
% \end{split}
% \end{equation}
% \subsubsection{Proof of Lemma \ref{l2}}
% Due to $\overline{\w}^{(t)}$ can be written as $\overline{\w}^{(t)} = \frac{1}{m}\sum_{k' \in S_{t-1}}\w_{k'}^{(t-1)}$, we have
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 \\
% =& \sum_{k \in \s}||\w_k^{(t)} - \frac{1}{m}\sum_{k' \in S_a^{(t-1)}}\w_{k'}^{(t-1)}||^2 \\
% =& \sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\w_k^{(t)} - \w_{k'}^{(t-1)}||^2.
% \end{split}
% \end{equation}
% Like the proof in Lemma \ref{l1}, we introduce $\overline{\w}_c^{(t)}$ into the above formula:
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}||\overline{\w}^{(t)} - W_k^{(t)}||^2 \\
% =& \sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\w_k^{(t)} - \overline{\w}_c^{(t)} + \overline{\w}_c^{(t)} - \w_{k'}^{(t-1)}||.
% \end{split}
% \end{equation}
% The above formula can be written as:
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 \\ 
% =& \sum_{k \in \s}\sum_{k \in S_a^{(t-1)}}||\w_k^{(t)}-\overline{\w}_c^{(t)}-(\w_{k'}^{(t-1)}-\overline{\w}_c^{(t)})||^2.
% \end{split}
% \end{equation}
% We continue to expand the above formula and complete the proof. 
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 \\ 
% =& \sum_{k \in \s}\sum_{k \in S_a^{(t-1)}}(||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 + ||\w_{k'}^{(t-1)} - \overline{\w}_c^{(t)}||^2 \\
% &- 2\langle\w_k^{(t)} - \overline{\w}_c^{(t)},\w_{k'}^{(t-1)}-\overline{\w}_c^{(t)}\rangle) \\
% =& \sum_{k \in \s}||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 + \sum_{k' \in S_a^{(t-1)}}||\w_{k'}^{(t-1)}-\overline{\w}_c^{(t)}||^2 \\
% &- \sum_{k \in \s}\sum_{k \in S_a^{(t-1)}}\langle\w_k^{(t)} - \overline{\w}_c^{(t)},\w_{k'}^{(t-1)} - \overline{\w}_c^{(t)}\rangle.
% \end{split}
% \end{equation}
% \subsubsection{Proof of Lemma \ref{l3}}
% Due to the Assumption \ref{A2}, we have
% \begin{equation}
% \begin{split}
% &F_k(\overline{\w}^{(t)})-F_k^* \geq \frac{\mu}{2}||\overline{\w}^{(t)}-\w_k^*||^2. 
% \end{split}
% \end{equation}
% The above operations are consistent with the sum operation, so we have that
% \begin{equation}
% \begin{split}
% \sum_{k \in \s}(&F_k(\overline{\w}^{(t)})-F_k^*) \geq \frac{\mu}{2}\sum_{k \in \s}||\overline{\w}^{(t)}-\w_k^*||^2. 
% \end{split}
% \end{equation}
% Like the proof in Lemma \ref{l1} we introduce $\overline{\w}_c^{(t)}$ into the above formula:
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\overline{\w}^{(t)})-F_k^*)  \\
% \geq& \frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\w_k^* - \overline{\w}^{t} + \overline{\w}^{(t)} - \w_{k'}^{t-1}||^2. 
% \end{split}
% \end{equation}
% We continue to expand the above formula and complete the proof. 
% \begin{equation}
% \begin{split}
% &\sum_{k \in \s}(F_k(\overline{\w}^{(t)})-F_k^*) \\
% \geq& \frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\w_k^* - \overline{\w}_c^{(t)}||^2 \\
% &+\frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}||\overline{\w_c}^{(t)}-\w_{k'}^{(t-1)}||^2 \\
% &+\frac{\mu}{2m}\sum_{k \in \s}\sum_{k' \in S_a^{(t-1)}}\langle\w^*_k-\overline{\w}_c^{(t)},\overline{\w}_c^{(t)}-\w_{k'}^{(t-1)}\rangle. 
% \end{split}
% \end{equation}
\subsubsection{Proof of Lemma \ref{l1}}
\begin{proof}
We introduce $\w_k^{(t)}$ into the formula:
\begin{equation}
\begin{split}
&-\langle \overline{\w}^{(t)}-\w^*,\sum_{k\in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})\rangle \\
=& -\sum_{k\in \s}\langle\overline{\w}^{(t)}-\w^*,p_{k,\s}\nabla F_k(\w_k^{(t)})\rangle \\
=& -\sum_{k \in \s}p_{k,\s}\langle\overline{\w}^{(t)} - \w_k^{(t)} + \w_k^{(t)} - \w^*,\nabla F_k(\w_k^{(t)})\rangle \\
=& -\sum_{k \in \s}p_{k,\s}\langle\overline{\w}^{(t)} - \w_k^{(t)},\nabla F_k(\w_k^{(t)})\rangle -\sum_{k \in \s}p_{k,\s}\langle\w_k^{(t)}-\w^*,\nabla F_k(\w_k^{(t)})\rangle.
\end{split}
\end{equation}
According to the Cauchy-Schwarz inequality and AM-GM inequality, we have
\begin{equation}
\begin{split}
&-\langle\overline{\w}^{(t)}-\w^*,\sum_{k\in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})\rangle \\
\leq& \frac{1}{2}\sum_{k \in \s}p_{k,\s}(\frac{1}{\eta_t}||\overline{\w}^{(t)}-\w_k^{(t)}||^2 + \eta_t||\nabla F_k(\w_k^{(t)})||^2)-\sum_{k \in \s}p_{k,\s}\langle\w_k^{(t)}-\w^*,\nabla F_k(\w_k^{(t)})\rangle\\
\leq& \frac{1}{2}\sum_{k \in \s}p_{k,\s}(\frac{1}{\eta_t}||\overline{\w}^{(t)}-\w_k^{(t)}||^2 + \eta_t||\nabla F_k(\w_k^{(t)})||^2)-\sum_{k \in \s}p_{k,\s}(\w_k^{(t)}-\w^*)^T\nabla F_k(\w_k^{(t)}).
\end{split}
\end{equation}
Due to the Assumption 2, we expand the above formula and complete the proof:
\begin{equation}
\begin{split}
&-\langle\overline{\w}^{(t)}-\w^*,\sum_{k\in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})\rangle \\
\leq&  \frac{1}{2}\sum_{k \in \s}p_{k,\s}(\frac{1}{\eta_t}||\overline{\w}^{(t)}-\w_k^{(t)}||^2 + \eta_t||\nabla F_k(\w_k^{(t)})||^2)- \sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}-F_k(\w^*))-\frac{\mu}{2}||\w_k^{(t)} - \w^*||^2) \\
=& \frac{1}{2\eta_t}\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 + \frac{\eta_t}{2}\sum_{k\in S_a^{(t)}}p_{k,\s}||\nabla F_k(\w_k^{(t)})||^2 - \sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)})-F_k(\w^*))\\
&- \frac{\mu}{2}\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2. 
\end{split}
\end{equation}
\end{proof}
\subsubsection{Proof of Lemma \ref{l2}}
\begin{proof}
The original formula can be written as:
\begin{equation}
\begin{split}
&-\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*) \\
=& -\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k(\overline{\w}^{(t)}) + F_k(\overline{\w}^{(t)}) - F_k^*) \\
\leq& -\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k(\overline{\w}^{(t)})) - \sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F_k^*).
\end{split}
\end{equation}
Due to the assumption 2 we have
\begin{equation}
\begin{split}
&-\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*) \\
\leq& \sum_{k \in \s}p_{k,\s}[\frac{\eta_t}{2}||\nabla F_k(\overline{\w}^{(t)})||^2 + \frac{1}{2\eta_t}||\w_k^{(t)} - \overline{\w}^{(t)}||^2 - \frac{\mu}{2}||\w_k^{(t)}-\overline{\w}^{(t)}||^2 - (F_k(\overline{\w}^{(t)})-F_k^*)]. 
\end{split}
\end{equation}
We continue to expand the above formula as follows:
\begin{equation}
\begin{split}
&-\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*) \\
=& \frac{\eta_t}{2}\sum_{k \in \s}p_{k,\s}||\nabla F_k(\overline{\w}^{(t)})||^2 - \sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F_k^*) + (\frac{1}{2\eta_t} - \frac{\mu}{2})\sum_{k \in \s}p_{k,\s}||\w_k^{(t)} - \overline{\w}^{(t)}||^2. 
\end{split}
\end{equation}
Due to the Assumption 1, we have
\begin{equation}
\begin{split}
&-\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*) \\
\leq& \eta_tL\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F_k^*) - \sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F_k^*) + \frac{1-\eta_t\mu}{2\eta_t}\sum_{k \in \s}p_{k,\s}||\w_k^{(t)} - \overline{\w}^{(t)}||^2 \\
=& -(1-\eta_tL)\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F^*) + \frac{1-\eta_t\mu}{2\eta_t}\sum_{k \in \s}p_{k,\s}||\w_k^{(t)} - \overline{\w}^{(t)}||^2.
\end{split}
\end{equation}
As $\frac{1-\eta_tL}{2\eta_t} \leq \frac{1}{v_t}$, we continue to expand the formula and complete the proof:
\begin{equation}
\begin{split}
&-\sum_{k \in \s}(F_k(\w_k^{(t)}) - F_k^*) \\
\leq& -(1-\eta_tL)\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F^*) + \frac{1}{v_t}\sum_{k\in \s}p_{k,\s}||\w_k^{(t)} - \overline{\w}^{(t)}||^2.
\end{split}
\end{equation}
\end{proof}
% \subsubsection{Proof of Lemma \ref{l3}}
% Due to the difinition of $\overline{\w}_c^{(t)}$, we have that
% \begin{equation}
% \begin{split}
% &\mathbb{E}\sum_{k \in \s}||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 \\
% =& \mathbb{E}\sum_{k \in \s}||\w_k^{(t)} - \frac{1}{C}\sum_{c_k \in S_c}\w_{c_k}^{(t)}||^2 \\
% =& \frac{1}{C}\sum_{c_k\in S_c}\sum_{k\in \s}\mathbb{E}||\w_k^{(t)} - \w_{c_k}^{(t)}||^2.
% \end{split}
% \end{equation}
% Because $\w_k^{(t)}$ and $\w_{c_k}^{(t)}$ have the same initialization, we can bound the above formula as:
% \begin{equation}
% \begin{split}
% &\mathbb{E}\sum_{k \in \s}||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 \\
% \leq& \frac{1}{C}\sum_{c\in S_c^{(t)}}\sum_{k\in \s}\mathbb{E}||\sum_{i=t_0}^{t_0+\tau-1}\eta_i(g_{k}(\w_{k}^{(i)},B_{k}^{(i)}) \\
% &- g_{c}(\w_{c}^{(i)},B_{c}^{(i)}))||^2 \\
% =& \frac{\eta_{t_0}^2\tau}{C}\sum_{c\in S_c^{(t)}}\sum_{k\in \s}\sum_{i=t_0}^{t_0+\tau-1}\mathbb{E}||g_{k}(W_{k}^{(i)},B_{k}^{(i)}) \\
% &- g_{c}(\w_{c}^{(i)},B_{c}^{(i)})||^2.
% \end{split}
% \end{equation}
% We use Definition \ref{G1} to quantify the Euclidean distance between $g_{k}(W_{k}^{(i)},B_{k}^{(i)}$ and $g_{c}(\w_{c}^{(i)},B_{c}^{(i)})$. Due to the fact that $\eta_t$ is non-increasing and 
% \begin{equation}
% \begin{split}
% &\mathbb{E}\sum_{k \in \s}||\w_k^{(t)} - \overline{\w}_c^{(t)}||^2 \\
% \leq& \frac{\eta_{t_0}^2\tau}{C}\sum_{c\in S_c^{(t)}}\sum_{k\in \s}\sum_{i=t_0}^{t_0+\tau-1}{G_1^{(i)}}^2 \\
% \leq& m\eta_{t_0}^2\tau^2\overline{G_1}^2 \\
% \leq& 4m\eta_t^2\tau^2\overline{G_1}^2. 
% \end{split}
% \end{equation}
% In the same way, we have that
% \begin{equation}
% \begin{split}
% &\mathbb{E}\sum_{k' \in S_a^{(t-1)}} ||\w_{k'}^{(t-1)}-\overline{\w}_c^{(t)}||^2 \leq 4m\eta_t^2\tau^2\overline{G_2}^2.
% \end{split}
% \end{equation}

\subsubsection{Proof of Lemma \ref{l4}}
\begin{proof}
According to the update rule, for $k$ and $k'$ which are in the same set $\s$, the term $||\w_{k'}^{(t)}-\w_k^{(t)}||^2$ will be zero when $k = k'$. As a result we have
\begin{equation}
\label{eq32}
\begin{split}
&\sum_{k\in \s}p_{k,\s}||\overline{\w}^{(t)}-\w_k^{(t)}||^2 \\
=& \sum_{k\in \s}p_{k,\s}||\sum_{k' \in \s}p_{k',\s}\w_{k'}^{(t)}-\w_k^{(t)}||^2 \\
=& \sum_{k\in \s}p_{k,\s}||\sum_{k' \in \s}(p_{k',\s}\w_{k'}^{(t)}-p_{k',\s}\w_k^{(t)})||^2 \\
=& \sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}||\w_{k'}^{(t)}-\w_k^{(t)}||^2.
\end{split}
\end{equation}Since the selected local models are udpated with the global model at every $\tau$, for any $t$ there is a $t_0$ satisfies that $0 \leq t-t_0 \leq \tau$ and $\w_{k'}^{(t_0)} = \w_k^{(t_0)} = \overline{\w}^{(t)}$. Therefore for any $(t)$ the term $||\w_{k'}^{(t)}-\w_k^{(t)}||^2$ can be bound by $\tau$ epochs. With non-increasing $\eta_t$ over $(t)$ and $\eta_{t_0} \leq 2\eta_t$, Eq. \ref{eq32} can be further bound as
\begin{equation}
\label{eq33}
\begin{split}
&\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}||\w_{k'}^{(t)}-\w_k^{(t)}||^2\\
\leq& \sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s} ||\sum_{i=t_0}^{t_0+\tau-1} \eta_i(g_{k'}(\w_{k'}^{(i)},B_{k'}^{(i)})-g_k^{(i)}(\w_k^{(t)},B_k^{(i)}))||^2\\
\leq& \eta_{t_0}^2\tau\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}\sum_{i=t_0}^{t_0+\tau-1}||g_{k'}(\w_{k'}^{(i)},B_{k'}^{(i)})-g_k^{(i)}(\w_k^{(t)},B_k^{(i)})||^2\\
\leq& \eta_{t_0}^2\tau\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}\sum_{i=t_0}^{t_0+\tau-1}[2||g_{k'}(\w_{k'}^{i},B_{k'}^{(i)})||^2 + 2||g_k(\w_k^{(i)},B_k^{(i)})||^2].
\end{split}
\end{equation}According to the Assumption 4, the expectation over Eq.\ref{eq33} can be written as
\begin{equation}
\begin{split}
& \mathbb{E}[\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}||\w_{k'}^{(t)}-\w_k^{(t)}||^2] \\
=& 2\eta_0^2\tau \mathbb{E}[\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}\sum_{i=t_0}^{t_0+\tau-1}(||g_{k'}(\w_{k'}^{i},B_{k'}^{(i)})||^2 + ||g_k(\w_k^{(i)},B_k^{(i)})||^2)]\\
\leq&  2\eta_0^2\tau \mathbb{E}_{\s}[\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}\sum_{i=t_0}^{t_0+\tau-1}2G^2]\\
=& 2\eta_0^2\tau \mathbb{E}_{\s}[\sum_{k\neq k' \atop k,k'\in \s}2p_{k,\s}p_{k',\s}\tau G^2]\\
\leq& 2\eta_0^2\tau \mathbb{E}_{\s}[\sum_{k\neq k' \atop k,k'\in \s}2\tau G^2].
\end{split}
\end{equation}Since there are at most $m(m-1)$ pairs such that $k \neq k'$ in $\s$, we have
\begin{equation}
\begin{split}
& \mathbb{E}[\sum_{k\neq k' \atop k,k'\in \s}p_{k,\s}p_{k',\s}||\w_{k'}^{(t)}-\w_k^{(t)}||^2] \\
\leq& \frac{16\eta_t^2(m-1)\tau^2G^2}{m}\\
\leq& 16\eta_t^2\tau^2G^2.
\end{split}
\end{equation}
\end{proof}

\subsection{Proof of Theorem 1}
\begin{proof}
According to Eq. \eqref{globalUpdate2} we define $\mathcal{H}(\w,t+1)$ as
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) = ||\overline{\w}^{(t+1)}-\w^*||^2 = ||\overline{\w}^{(t)} - \eta_t\overline{g}^{(t)} - \w^*||^2.
\end{split}
\end{equation}
The $\mathcal{H}(\w,t+1)$ can be written as:
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) \\
=& ||\overline{\w}^{(t)} - \w^* - \eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) + \eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) -\eta_t \overline{g}^{(t)}||^2.
\end{split}
\end{equation}
We expand the above formula as follows:
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) \\
=& ||\overline{\w}^{(t)} - \w^* -\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})||^2+ ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 \\
&+ \underbrace{2\eta_t\langle\overline{\w}^{(t)} - \w^* -\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}),\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \overline{g}^{(t)}\rangle}_{A_1}.
\end{split}
\end{equation}
Due to the Assumption 3, we have $\mathbb{E}[A_1]=0$ and expand the rest of the formula further:
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) \\
=& ||\overline{\w}^{(t)} - \w^* -\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})||^2+ ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 +A_1\\
=& ||\overline{\w}^{(t)} - \w^*||^2 + ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})||^2 - 2\eta_t\langle\overline{\w}^{(t)}-\w^*,\sum_{k \in S_a^{(t)}}p_{k,\s}\nabla F_k(\w_k^{(t)})\rangle \\
&+ ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 + A_1.
\end{split}
\end{equation}
Due to the Lemma \ref{l1}, we have that:
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) \\
\leq& ||\overline{\w}^{(t)} - \w^*||^2 + ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)})||^2 + ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 \\
&+ \sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 + \eta_t^2p_{k,\s}||\nabla F_k(\w_k^{(t)})||^2 - 2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)})-F_k(\w^*)) \\
&- \eta_t\mu\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2 +A_1\\
=& ||\overline{\w}^{(t)} - \w^*||^2 + 2\eta_t^2\sum_{k \in \s}p_{k,\s}||\nabla F_k(\w_k^{(t)})||^2 + \sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 - 2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)})-F_k(\w^*))\\
&-\eta_t\mu\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2 + ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2+A_1\\
\leq& ||\overline{\w}^{(t)}-\w^*||^2 + \underbrace{4L\eta_t^2\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)})-F_k^*)}_{A_2} - \underbrace{2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)})-F_k(\w^*))}_{A_3} \\
&+ \sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)}- \w_k^{(t)}||^2 -\eta_t\mu\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2 + ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 +A_1. 
\end{split} 
\end{equation}
The difference of $A_2$ and $A_3$ can be written as
\begin{equation}
\begin{split}
&A_2 - A_3 \\
=& (4L\eta_t^2 - 2\eta_t)\sum_{k \in \s}p_{k,\s}F_k(\w_k^{(t)}) - 4L\eta_t^2\sum_{k \in \s}p_{k,\s}F_k^* + 2\eta_t\sum_{k \in \s}p_{k,\s}F_k(\w^*)\\
=& (4L\eta_t^2 - 2\eta_t)\sum_{k \in \s}p_{k,\s}F_k(\w_k^{(t)}) - (4L\eta_t^2-2\eta_t)\sum_{k \in \s}p_{k,\s}F_k^* + 2\eta_t \sum_{k \in \s}p_{k,\s}F_k(\w^*) - 2\eta_t  \sum_{k \in \s}p_{k,\s}F_k^* \\
=& (4L\eta_t^2-2\eta_t)\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*) + 2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w^*) - F_k^*).
\end{split} 
\end{equation}
Let $v_t = 2\eta_t-4L\eta_t^2 = 2\eta_t(1-2L\eta_t)$, we have that:
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) \\ 
\leq& ||\overline{\w}^{(t)}-\w^*||^2 - v_t\sum_{k \in \s}p_{k,\s}(F_k(\w_k^{(t)}) - F_k^*) + 2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w^*) - F_k^*) + \sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 \\
&- \eta_t\mu\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2 + ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 + A_1.
\end{split} 
\end{equation}
Due to the Lemma \ref{l2}, we have that:
\begin{equation}
\begin{split}
&\mathcal{H}(\w,t+1) \\
 \leq& ||\overline{\w}^{(t)} - \w^*||^2 +2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*) + 2\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2 \\
&- v_t(1-\eta_tL)\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F_k^*) - \eta_t\mu\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2 \\
&+ ||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2 + A_1.
\end{split} 
\end{equation}
We next solve the expectation over $\mathcal{H}(\w,t+1)$:
\begin{equation}
\begin{split}
&\mathbb{E}[\mathcal{H}(\w,t+1)]=\mathbb{E}[||\overline{\w}^{(t+1)} - \w^*||^2] \\
\leq& \mathbb{E}[||\overline{\w}^{(t)} - \w^*||^2] +\mathbb{E}[2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*)] + \mathbb{E}[2\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2] \\
&- \mathbb{E}[v_t(1-\eta_tL)\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)}) - F_k^*)] - \mathbb{E}[\eta_t\mu\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}-\w^*||^2] \\
&+ \mathbb{E}[||\eta_t\sum_{k \in \s}p_{k,\s}\nabla F_k(\w_k^{(t)}) - \eta_t\overline{g}^{(t)}||^2] + \mathbb{E}[A_1].\\ 
\end{split} 
\end{equation}
Due to Assumption 3 and $\mathbb{E}[A_1] = 0$, we have
\begin{equation}
\begin{split}
&\mathbb{E}[||\overline{\w}^{(t+1)} - \w^*||^2] \\
\leq& \mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] + 2\eta_t\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*)] + 2\mathbb{E}[\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2] \\
&- v_t(1-\eta_tL)\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)})-F_k^*)] - \eta_t\mu\mathbb{E}[\sum_{k \in \s}p_{k,\s}||\w_k^{(t)}- \w^*||^2] + \eta_t^2\sum_{k=1}^Kp_k^2\sigma_k^2.  \\
=& (1-\eta_t\mu)\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] + 2\eta_t\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*)] + 2\mathbb{E}[\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2]
\\
&- v_t(1-\eta_tL)\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)})-F_k^*)] + \eta_t^2\sum_{k=1}^Kp_k^2\sigma_k^2 \\
=& (1-\eta_t\mu)\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] + \mathbb{E}[\mathcal{Q}(\w,k,t)] + \eta_t^2\sum_{k=1}^Kp_k^2\sigma_k^2,
\end{split} 
\end{equation}
where $\mathcal{Q}(\w,k,t)$ are defined as follows:
\begin{equation}
\begin{split}
&\mathcal{Q}(\w,k,t) \\
=& 2\eta_t\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*)+ 2\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2-v_t(1-\eta_tL)\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)})-F_k^*).
\end{split} 
\end{equation}
Note that $S_c^* = \arg\min_{S_c}\sum_{k\in S_c}p_{k,S_c}F_k^*$. Due to the Lemma \ref{l4}, the expectation of the $\mathcal{Q}(\w,k,t)$ can be written as:
\begin{equation}
\begin{split}
&\mathbb{E}[\mathcal{Q}(\w,k,t)] \\
=& -v_t(1-\eta_tL)\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)})-F_k^*)] + 2\eta_t\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*]+ 2\mathbb{E}[\sum_{k \in \s}p_{k,\s}||\overline{\w}^{(t)} - \w_k^{(t)}||^2]\\
=& -v_t(1-\eta_tL)\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)})-F_k^*)] + 2\eta_t\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\w^*)-F_k^*)]+ 32\eta_t^2\tau^2G^2\\
=&-v_t(1-\eta_tL)\mathbb{E}[\sum_{k \in \s}p_{k,\s}(F_k(\overline{\w}^{(t)})-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^* + \sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^* - \sum_{k \in \s}p_{k,\s}F_k^*]\\
&+ 2\eta_t\mathbb{E}[\sum_{k \in \s}p_{k,\s}F_k(\w^*)- \sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^* + \sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^* -\sum_{k \in \s}p_{k,\s}F_k^*]+ 32\eta_t^2\tau^2G^2\\
=& -v_t(1-\eta_tL)(\mathbb{E}[\sum_{k \in \s} p_{k,\s}F_k(\overline{\w}^{(t)}) - \sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*] + \mathbb{E}[\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^* - \sum_{k \in \s}p_{k,\s}F_k^*]) \\
&+ 2\eta_t(\mathbb{E}[\sum_{k \in \s}p_{k,\s}F_k(\w^*)-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*]+\mathbb{E}[\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*- \sum_{k\in \s}p_{k,\s}F_k^*]) + 32\eta_t^2\tau^2G^2\\
=& -v_t(1-\eta_tL)\mathbb{E}[\sum_{k \in \s}p_{k,\s}F_k(\overline{\w}^{(t)})-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*] + 2\eta_t(\mathbb{E}[\sum_{k \in \s}p_{k,\s}F_k(\w^*)-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*] \\
&- (2\eta_t-v_t(1-\eta_tL))\mathbb{E}[\sum_{k \in \s}p_{k,\s}F_k^*-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*] +32\eta_t^2\tau^2G^2.\\
\end{split} 
\end{equation}
According to the Assumption 5 and Definition 1 and 2, we have
\begin{equation}
\begin{split}
&\mathbb{E}[\mathcal{Q}(\w,k,t)] \\
\leq& -v_t(1-\eta_tL)\mathbb{E}[\varphi(\s,\overline{\w})(F(\overline{\w})-\sum_{k=1}^Kp_{k}F_k^*)] + 2\eta_t\mathbb{E}[\varphi(\s,\w^*)(F*-\sum_{k=1}^K p_kF_k^*)] \\
&+ (2\eta_t-v_t(1-\eta_tL))||\mathbb{E}[\sum_{k \in \s}p_{k,\s}F_k^*-\sum_{k' \in S_c^*}p_{k',S_c^*}F_{k'}^*]||+ 32\eta_t^2\tau^2G^2\\
\leq& -v_t(1-\eta_tL)\mathbb{E}[\varphi(\s,\overline{\w})(F(\overline{\w})-\sum_{k=1}^Kp_kF_k^*)] + 2\eta_t\mathbb{E}[\varphi(\s,\w^*)(F^*-\sum_{k=1}^K p_kF_k^*)] \\
&+ (2\eta_t-v_t(1-\eta_tL))\kappa^2 + 32\eta_t^2\tau^2G^2\\
\leq& -v_t(1-\eta_tL)\varphi_{min}\mathbb{E}[(F(\overline{\w})-\sum_{k=1}^Kp_kF_k^*)] + 2\eta_t\varphi_{max}\mathbb{E}[(F(\w^*)-\sum_{k=1}^Kp_kF_k^*)] + (2\eta_t-v_t(1-\eta_tL))\kappa^2 \\
&+ 32\eta_t^2\tau^2G^2\\
\leq& \underbrace{-v_t(1-\eta_tL)\varphi_{min}\mathbb{E}[(F(\overline{\w})-\sum_{k=1}^Kp_kF_k^*)]}_{A_4} + 2\eta_t\varphi_{max}\Gamma + 6L\eta_t^2\kappa^2+ 32\eta_t^2\tau^2G^2.
\end{split} 
\end{equation}
We can expand the $A_4$ as 
\begin{equation}
\begin{split}
&A_4 \\
=& -v_t(1-\eta_tL)\varphi_{min}\mathbb{E}[(F(\overline{\w})-\sum_{k=1}^Kp_kF_k^*)] \\
=&-v_t(1-\eta_tL)\varphi_{min}\sum_{k=1}^Kp_k(\mathbb{E}[F(\overline{\w})]-F^* + F^*-F_k^*) \\
=& -v_t(1-\eta_tL)\varphi_{min}\sum_{k=1}^Kp_k(\mathbb{E}[F_k(\overline{\w}^{(t)})]-F^*) -v_t(1-\eta_tL)\varphi_{min}\sum_{k=1}^Kp_k(F^*-F_k^*)\\
=& -v_t(1-\eta_tL)\varphi_{min}(\mathbb{E}[F(\overline{\w}^{(t)})]-F^*) - v_t(1-\eta_tL)\varphi_{min}\Gamma\\
\end{split} 
\end{equation}
\begin{equation}
\begin{split}
\leq& -\frac{v_t(1-\eta_tL)\mu \varphi_{min}}{2}\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2]- v_t(1-\eta_tL)\varphi_{min}\Gamma\\
\leq& -\frac{3\eta_t\mu\varphi_{min}}{8}\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] - 2\eta_t(1-2L\eta_t)(1-\eta_tL)\varphi_{min}\Gamma\\
\leq& -\frac{3\eta_t\mu\varphi_{min}}{8}\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] - 2\eta_t\varphi_{min}\Gamma + 6\eta_t^2\varphi_{min}L\Gamma.
\end{split} 
\end{equation}
So we have
\begin{equation}
\begin{split}
&\mathbb{E}[\mathcal{Q}(\w,k,t)] \\
=& -\frac{3\eta_t\mu\varphi_{min}}{8}\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] + 2\eta_t\Gamma(\varphi_{max}-\varphi_{min}) + \eta_t^2(6\varphi_{min}L\Gamma + 32\tau^2G^2 + 6L\kappa^2).
\end{split} 
\end{equation}
As a result, we have
\begin{equation}
\begin{split}
&\mathbb{E}[||\overline{\w}^{(t+1)}-\w^*||] \\
\leq& [1-\eta_t\mu(1+\frac{3\varphi_{min}}{8})]\mathbb{E}[||\overline{\w}^{(t)}-\w^*||^2] + 2\eta_t\Gamma(\varphi_{max}-\varphi_{min}) + \eta_t^2(6\varphi_{min}L\Gamma + 32\tau^2G^2 + 6L\kappa^2 + \sum_{k=1}^Kp_k\sigma_k^2).
\end{split} 
\end{equation}
By defining $\Delta_{t+1} = \mathbb{E}[||\overline{\w}^{(t+1)}-\w^*||]$, $B = 1+\frac{3\varphi_{min}}{8}$, $C = 6\varphi_{min}L\Gamma + 32\tau^2G^2 + 6L\kappa^2 + \sum_{k=1}^Kp_k\sigma_k^2$, $D = \Gamma(\varphi_{max}-\varphi_{min})$, we have
\begin{equation}
\begin{split}
&\Delta_{t+1} \leq (1-\eta_t\mu B)\Delta_t + \eta_t^2C + \eta_tD.
\end{split} 
\end{equation}
If we set $\Delta_t \leq \frac{\psi}{t+\gamma}$, $\eta_t = \frac{\beta}{t + \gamma}$ and $\beta > \frac{1}{\mu B}$, $\gamma > 0$ by induction, we have
\begin{equation}
\begin{split}
&\psi = \max{\left\{ \gamma||\overline{\w}^1-\w^*||^2,\frac{1}{\beta \mu B - 1}(\beta^2C + D\beta(t+\gamma))\right\}}.
\end{split} 
\end{equation}
Then by the L-smoothness of $F(\cdot)$, 
\begin{equation}
\begin{split}
&\mathbb{E}[F(\overline{\w}^{(t)})] - F^* \leq \frac{L}{2}\Delta_t \leq \frac{L}{2}\frac{\psi}{\gamma + t}.
\end{split} 
\end{equation}
Finally, we complete the proof of Theorem 1:
\begin{equation}
\begin{split}
&\mathbb{E}[F(\overline{\w}^{T})] - F^*\\
\leq& \frac{1}{T+\gamma}\left[ \frac{4L(32\tau^2G^2 + \sum_{k=1}^Kp_k\sigma_k^2) + 24L^2\kappa^2}{3\mu^2 \varphi_{min}}  + \frac{8L^2\Gamma}{\mu^2}+\frac{L\gamma||\overline{\w}^{1} - \w^*||^2}{2}\right] + \frac{8L\Gamma}{3\mu}\left(\frac{\varphi_{max}}{\varphi_{min}}-1\right),
\end{split} 
\end{equation}
where the $T$ means the maximal communication rounds, which satisfies $T = i\tau$ for $i = 1,2,...$ in realistic scenarios.
\end{proof}


\end{document}


