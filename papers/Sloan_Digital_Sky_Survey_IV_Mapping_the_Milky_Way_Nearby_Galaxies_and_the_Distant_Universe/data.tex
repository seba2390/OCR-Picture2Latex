\begin{table*}[t!]
\caption{
\label{table:drs} SDSS-IV Data Releases}
\begin{tabular}{ccccccc}
\hline\hline
Name & Release Date & Data Through & eBOSS & MaNGA & APOGEE-2N &
APOGEE-2S \\
\hline
DR13 & 2016 Jul & 2015 Jul & SEQUELS\tablenotemark{a}
& New data and products\tablenotemark{b} & New products & --- \cr
DR14 & 2017 Jul & 2016 Jul & New data & New data & 
 New data & --- \cr
DR15 & 2018 Jul & 2017 Jul & --- & New data and
products\tablenotemark{c}
& --- & --- \cr
{\it DR16} & {\it 2019 Jul}  & {\it 2018 Jul } & {\it New data} &
{\it New data} & {\it New data} & {\it New data} \cr
{\it DR17} & {\it 2020 Dec} & {\it 2019 Jul } & {\it New data} &
{\it New data} & {\it New data} & {\it New data} \cr
\hline
\end{tabular}
\tablecomments{
The timing of the last two data releases will be based on available
funding.  ``New data'' means that new data are being released. ``New
products'' means that new types of data analysis are being
released.
\tablenotetext{a}{DR13 contains the remainder of the SEQUELS program, begun
in SDSS-III and completed in SDSS-IV, and new reductions for BOSS
data, but no new eBOSS data.}
\tablenotetext{b}{DR13 and DR14 contain MaNGA Data Release Pipeline results;
these are calibrated spectral data cubes.}
\tablenotetext{c}{DR15 contains MaNGA Data Analysis Pipeline results;
these include maps of derived quantities from the spectral data
cubes.}
}
\end{table*}

SDSS-IV data management encompasses the transfer of data among survey
facilities, long-term archiving of data and metadata, documentation,
and distribution to the collaboration and the public. We build on the
data distribution systems developed for SDSS-I through SDSS-III.

The central data system for SDSS-IV is the Science Archive Server
(SAS) hosted by the University of Utah Center for High Performance
Computing. The SAS serves as a data repository with all survey
targeting data, raw data, and reduced data on disk, and has associated
computing to perform reductions and other critical operations. It has
a current capacity of around 1 petabyte, in order to accommodate the
variety of necessary imaging data sets and spectroscopic reduction
versions produced during the survey. A Science Archive Mirror (SAM) at
a separate location contains a copy of all the archived data; the SAM
is housed by the National Energy Research Scientific Computing Center
(NERSC) at the Lawrence Berkeley National Laboratory during the
lifetime of the survey. In addition, the archived data are backed up
on long term tape storage at the High Performance Storage System
(HPSS) at NERSC.  The SAS system also contains the project wiki, used
for documentation and internal communication, and a {\tt subversion}
server used for software version control. These systems are also
backed up at the SAM.

Survey targeting data, plate design data, and other data associated
with the observational planning are stored on the SAS and information
is distributed from there to the University of Washington plate
drilling facility and to APO and LCO as necessary for conducting
operations. Data and metadata from the plate drilling quality
assurance process are backed up to the SAS. At APO, the plate-plugging
metadata, observing logs, telescope telemetry, and the raw data are
transferred each day from the previous night's observing to the SAS
(\citealt{weaver15a}) and backed up on the SAM and HPSS. A similar
system is installed at LCO.

The eBOSS, MaNGA, and APOGEE-2 pipelines are run automatically on each
night's data as they arrive. For eBOSS, this process consists of the
full pipeline through the production of 1D calibrated spectra,
redshifts, and other parameters, for each completed plate. For MaNGA,
this process consists of the Data Reduction Pipeline executed for each
completed plate. However, currently the Data Analysis Pipeline is
experiencing more development and is not run automatically; it is
instead run periodically based on accumulated data and progress in DAP
development. For APOGEE-2, the visit spectrum reductions and radial
velocity determinations are performed automatically.  However, because
the combined spectra require multiple visits and because of its
computational expense, the ASPCAP analysis is performed periodically
on large sets of plates, again based on accumulated data and progress
in ASPCAP development.

The primary point of data access for collaboration members is the
SAS. Collaboration members can access data on the SAS through {\tt
ssh} connections. SAS also provides {\tt http}, {\tt rsync}, and
Globus access to the data files. These methods are available also to
the astronomical community for publicly released data both for the SAS
and SAM. We provide a web interface and an application program
interface (API) on SAS to the eBOSS and APOGEE. A similar set of
interfaces is being developed for MaNGA called Marvin, which will
additionally have a Python module for interaction with the API. The
data directory structure and file format documentation is provided as
a ``data model.''\footnote{\link{http://data.sdss.org/datamodel}}

Public data releases incorporate both the SAS data interface and the
Catalog Archive Server (CAS), hosted at Johns Hopkins University. The
CAS contains catalog data from the SDSS imaging and spectroscopic
survey; it does not currently include images or spectra (other than
JPEG and PNG versions, respectively, for visual browsing). The total
database size is approximately 12 Tb, which is dominated by SDSS
imaging catalogs. The CAS provides web browser-based access in
synchronous mode via the SkyServer web application\footnote{\tt
http://skyserver.sdss.org/} and in asynchronous mode with the CASJobs
batch query service.\footnote{\tt http://skyserver.sdss.org/casjobs/}

The SkyServer (\citealt{szalay02a}) supports multiple levels of data
access ranging from simple form-based queries aimed at novice users to
raw SQL queries for expert users. The SkyServer includes interfaces
displaying the SDSS and 2MASS imaging and the locations of SDSS
spectroscopic and imaging catalog entries, as well as an Explore tool
for each object showing the spectra and listing key
parameters.

CASJobs (\citealt{li08b}) gives each user their own server-side
database called MyDB, along with the ability to submit arbitrarily
complex SQL queries in batch mode and redirect the output to their
MyDB.  Users may import their own data to cross-match with the SDSS
data. There is a Groups feature to allow users to share their data
with collaborators. CASJobs also supports a command-line mode of query
submission. For SDSS-IV, SkyServer and CASJobs are integrated into
the SciServer collaborative data-driven science framework\footnote{\tt
http://sciserver.org/} with seamless single sign-on access to several
new services such as Compute, SciDrive, SciScript and SkyQuery. Compute
includes a Jupyter notebook server that has fast server-side access to
CASJobs and other data sets.

The SDSS data distribution system is heavily used. The CASJobs system
has approximately 2000 unique users each year. The SkyServer system
experiences tens of millions of queries each year. The SAS system is
used to download tens of terabytes of data per year by public
users. The SDSS help desk email account fields around 500 inquiries
per year.

We plan to release data on regular intervals. The released data
include targeting data, raw and reduced spectroscopic data including
of calibrations, derived quantities of several varieties, and
value-added catalogs provided by collaboration members. All metadata
and intermediate data are included and documented. Table
\ref{table:drs} shows our nominal data release plans. The data
releases include not just SDSS-IV data but also data from previous
phases of SDSS, and the services host all previous data releases. New
types of analysis or increments of new data may be added based on
availability. Because of funding uncertainty, the timing of the last
two data releases remains unclear; nevertheless, SDSS-IV is committed
to a final public release of all of its data.
