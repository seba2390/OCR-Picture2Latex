\section{Empirical Evaluation}
\label{sec:Experiments}
\subsection{Experimental setup}
\label{sec:ExpSetup}
%\subsubsection{Datasets} %~\footnote{\url{https://github.com/sidooms/MovieTweetings/}}~\cite{dooms2013movietweetings}
%\vspace{4mm}
\noindent \textbf{Datasets and data split. }Table~\ref{tab:DatasetStatsticsSmall} describes our datasets. 
We use MovieLens 100K (ML-100K), 1 Million (Ml-1M), 10 Million (ML-10M) ratings~\cite{harper2016movielens}, Netflix, and 
MovieTweetings 200K (MT-200K)~\cite{dooms2013movietweetings}.  In the MovieLens datasets, every consumer has rated at least 20 movies ($\tau = 20$), with  $r_{ui} \in \{ 1, \dots, 5\} $ (ML-10M has half-star increments). % for ML-10m $r_{ui} \in \{ 1,1.5,2, \dots, 5\} $ 
MT-200K  contains voluntary movie ratings posted on twitter, with $r_{ui} \in \{ 0, \dots, 10\} $.  Following~\cite{hernandez2014probabilistic}, we preprocessed this dataset to map the ratings to  the interval $[1,5]$. Due to the extreme sparsity of this dataset and to ensure every user has some data to learn from, we   filtered the users to  keep those with at least 5 ratings ($\tau = 5$). 

%\textcolor{red}{dataset stat:(Total number of ratings:450,883), (Number of unique users:40,399), (Number of unique items:23,204). }
%From Figure ~\ref{fig:longTailDist}, we observe the MT-200k dataset has a more skewed distribution and a higher fraction of long-tail items.
%and with a sparsity level $\eta=  1 - \frac {|\mathcal{D}|}{|\mathcal{U}| * |\mathcal{I}|} =0.99$
%In our work, we are interested in determining the long-tail preferences of the entire range of users.  
%\textcolor{red}{Since our focus is on long-tail item promotion and item-space coverage improvement, we have conducted our experiments in practical settings that include the full  range of users and items. }

\iffalse
\subsubsection{Data Split. }We split each dataset into train and test sets. 
There are three possible schemes for splitting the data. The first is to randomly split the dataset into a train (80\%) and a test (20\%) set. Note in this case, it is possible that some users are not present in the train phase. The second is to fix the number of train ratings per user,~i.e.,  $ \psi= 10$~\cite{lee2014local,weimer2007maximum} and use the rest of the user's ratings in the  test set, and evaluate ranking based algorithms.  Users whose total ratings are fewer than  $\psi$ are dropped. This scheme however is biased toward frequent users~\cite{lee2014local}. A third scheme, is to keep a fixed ratio $\kappa$ of each user's ratings in the train set, and move the rest to the test set~\cite{lee2014local}. ~\cite{weimer2007maximum} also introduced the  weak generalization setup where the task is to predict the rank of of un-rated items for users known at training time. This method is applicable for all data splitting schemes. The strong generalization evaluation scheme is not of interest to us.
We use the third scheme. 
\fi

Our  selected datasets  have varying density levels. Additionally,  MT-200K and Netflix  include a large number of  difficult infrequent users, i.e., in MT-200K,  47.42\% (3.37\% in Netflix) of the users have rated fewer than 10 items, with the minimum being 4. 
We chose these datasets to study performance in settings  where  users  provide few feedback~\cite{kanagal2012supercharging,liu2017experimental}.


Next,  we randomly split each dataset into train and test sets by keeping a fixed ratio $\kappa$ of each user's ratings in the train set and moving the rest to the test set~\cite{lee2014local}.  
\iffullpaper 
 This way, when $\kappa =0.8$, an infrequent user with 5 ratings will have 4 train and 1 test rating,  while  a user with  100 ratings, will have 80 train ratings and the rest in test. 
 \fi
 For ML-1M and ML-10M, we set $\kappa=0.5$.  For MT-200K, we set $\kappa=0.8$.  For Netflix, we use their probe set as our test set, and remove the  corresponding ratings from train. We remove users in the probe set who do not appear in train set, and vice versa. 
%   Note, by using the  fixed ratio scheme, 


%\footnote{  Many   ranking-based CF algorithms,  such as CofiRank~\cite{weimer2007maximum}, split their data by keeping a fixed  number of ratings, e.g., $\kappa=N+10$, per user  in train, and moving the  rest of the user's ratings to the  test set.  Infrequent users,  whose total ratings are fewer than $\kappa$, are dropped. As stated in~\cite{lee2014local}, although this data splitting scheme has low variance, it leads to evaluations that are  biased toward frequent users. }

 %\footnote{RAP: 1. you should explain in one sentence why you're saying all of this. I know from discussing it with you that this means that this is not the normal way of doing things, but others may not, so it comes off as weird as to why you're saying it all. 2. You need a sentence on why you have $\kappa$ set to different values for the different datasets.}
  
   
 %We use the train set  to recommend unseen items and evaluate  performance using the test set.
 %For both train and test sets,  we consider items in the lower $ 20\%$ of the total ratings, as long-tail items.
%We do not filter the users as rigorously as prior work,  and  
%We adopt the fixed ratio per user experimental setting~\cite{lee2014local} where a fixed proportion $\kappa=0.8$ of each user's history is kept in train and the rest is moved to test.

%Experimental Set-up 

  %Under this experimental setting, the performance of algorithms like NMF, RegSVD, and CoFiRank may degrade in precision, compared to the original papers. % and integrating the preferences in a generic framework that relies on base recommenders such as PureSVD and Pop. 
 
%Note,  an alternative approach common in ranking literature, including CofiRank~\cite{weimer2007maximum},   is to fix the number of train ratings per user,~i.e.,  $ \kappa= N + 20$~\cite{lee2014local,weimer2007maximum} and use the rest of the user's ratings in the  test set.  Infrequent users,  whose total ratings are fewer than  $\kappa$, are dropped. This scheme  is therefore biased toward frequent users~\cite{lee2014local}. 

%Furthermore, collaborative filtering methods (e.g.,~\cite{niemann2013new,Adamopoulos:2014:OCB:2645710.2645752}) often remove infrequent users from the test set. Consequently, they report better  accuracy results  since the difficult users are removed~\cite{mnih2007probabilistic}.  By using the  fixed ratio scheme, our  train data sets   typically include a large number of difficult infrequent users, e.g., in MT-200K,  47.42\% (3.37\% in Netflix) of the users have rated fewer than 10 items. The minimum number of ratings per user in both MT-200K and Netflix is 4. 




\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{llllllll}
  \toprule
  {\bf Dataset} & {\bf $|\mathcal{D}|$} & {\bf $|\mathcal{U}|$} & {\bf $|\mathcal{I}|$}  & d$\%$ & $\mathcal{L} \%$ & {\bf $\kappa$} & {\bf $\tau$ } \\ \midrule
  ML-100K  & 100K & 943& 1682  & 6.30 &  66.98   &0.5 & 20 \\
  
  ML-1M   & 1M & 6,040& 3,706 & 4.47 & 67.58  &  0.5 & 20  \\
  
  ML-10M & 10M & 69,878 & 10,677 & 1.34& 84.31  & 0.5 &  20 \\
  
  %MT-200k-m5  & 172,506 & 7,969 & 13,864 & 0.16\%& 86.84 \%   & 0.8 & 5 \\
  % MT-200k-m5-mapped
  MT-200k  & 172,506 & 7,969 & 13,864 & 0.16 & 86.84    & 0.8 & 5  \\ 
  
  %MT-200k-m20 & 96,788 & 1,564 &  11,729 & 0.53\% &  77.61\% & 0.5 & 30  \\
  
  %MT-200k-m20-mapped & 96,788 & 1,564 &  11,729 & 0.53\% &  77.61\% & 0.5 & 30 \\
  
  Netflix & 98,754,394 &  459,497 & 17,770  &1.21 & 88.27  & -& - \\
   \bottomrule
\end{tabular}
\caption{Datasets description.  $|\mathcal{D}|$ is number of ratings in dataset.  Density is  $ \text{d}\%=  |\mathcal{D}| / ( |\mathcal{U}| * |\mathcal{I}|) \times 100\%$.   Long-tail percentage is  $\mathcal{L}\% = (|\LT| / |\itemsinTrainset|) \times 100\%$. Train-test split ratio per user is $\kappa$,  $\tau$ is the minimum number of ratings per user. }
\label{tab:DatasetStatsticsSmall}
\end{table}

\input{PerfMetrics}


\vspace{4mm}
\noindent \textbf{Other algorithms and their configuration. } We compare against, or integrate the following methods  in our framework~\footnote{We report the default configurations specified in the original work for most algorithms.}.
 %For a more detailed analysis  refer to~\cite{ourFullVersion}.}.
 %experiments on other configurations  of some of the above frameworks, and report details for algorithm parameters in different datasets.,  and report the  parameters that led to the best  performance.
\begin{itemize}

\item \textbf{Rand} is  non-personalized and randomly suggests $\size$ unseen items from among all items.  It obtains high coverage and novelty,  but low accuracy~\cite{vargas2014improving}. 

\item \textbf{Pop}~\cite{cremonesi2010performance} is a non-personalized  algorithm. For  ranking tasks, it obtains high accuracy~\cite{cremonesi2010performance,vargas2014improving}, since it takes advantage of the popularity bias of the data. However, Pop makes trivial recommendations that lack novelty~\cite{cremonesi2010performance}.  

\item \textbf{RSVD}~\cite{koren2009matrix} is a latent-factor model for  rating prediction. We used \texttt{LIBMF}, with  L2-Norm as the loss function, and L2-regularization,  and Stochastic Gradient Descent (SGD) for optimization. We also tested the same model with non-negative constraints (RSVDN)~\cite{zhuang2013fast}, but did not find significant performance difference. We omit RSVDN from our results.  We performed 10-fold cross validation and tested:  number of latent factors  $g \in \{8,  20, 40, 50, 80, 100\}$, L2-regularization coefficients  $\lambda \in \{0.001, 0.005, 0.01, 0.05, 0.1\}$,  learning rate $\eta \in \{0.002,0.003,0.01, 0.03\}$. For each dataset, we used the parameters that led to best performance  
\iffullpaper
 (see Appendix~\ref{sec:configurationofR-SVD}).
\else
(see~\cite{ourFullVersion}).
\fi

\item \textbf{PSVD}~\cite{cremonesi2010performance} is a latent factor model,  known for  achieving high accuracy and novelty~\cite{cremonesi2010performance}. In PSVD,  missing values are imputed by zeros and conventional SVD is performed.  
\iffullpaper
 We used Python's \texttt{sparsesvd}  module and tested number of latent factors $g \in \{10, 20, 40, 60, 100, 150, 200, 300\}$. We report results for two configurations:  one with $10$ latent factors (PSVD10), and one with 100 latent factors (PSVD100). 
\else
We used Python's \texttt{sparsesvd}  module and tested two configurations:  one with $10$ latent factors (PSVD10), and one with 100 latent factors (PSVD100). 
\fi  

\item \textbf{CoFiRank}~\cite{weimer2007maximum} is a  ranking prediction model that can optimize directly the  Normalized Discounted Cumulative Gain (NDCG) ranking measure~\cite{voorhees2001overview}.   %We  tested dimensions $g \in \{10,100\}$,   $\lambda \in \{5,10\}$. However, 
We used the source code from~\cite{weimer2007maximum}, with parameters set according to~\cite{weimer2007maximum}:  100 dimensions and $\lambda =10$, and default values for other parameters. We experimented with both regression (squared) loss (CofiR100) and NDCG loss (CofiN100). 
Similar to~\cite{balakrishnan2012collaborative,volkovs2012collaborative}, we found CofiR100 to perform consistently better than CofiN100 in our experiments on ML-1M and ML-100K. We only report results for CofiR100.  


\item \textbf{Ranking-Based Techniques (RBT)}~\cite{adomavicius2012improving} maximize coverage by re-ranking the output of a rating prediction model according to a re-ranking criterion. We implemented  two variants: one that re-ranks  a few of the items in the head according to their popularity (Pop  criterion), and another which re-ranks according to the  average rating (Avg criterion). As in~\cite{adomavicius2012improving}, we set $T_{max}=5$ in all datasets. The parameter $T_R$ controls the extent of re-ranking. We tested $T_{R}\in [4,4.2,4.5]$, and found $T_R=4.5$ to yield more accurate results. Furthermore, because our datasets contain a wider range of users compared to~\cite{adomavicius2012improving}, we set $T_H=1$ on all datasets, except ML-10M and Netflix, where we set $T_H=0$.  To refer to RBT variants, we  use  $\texttt{RBT}(\texttt{ARec}, \texttt{Re-ranking criterion})$.


\item \textbf{Resource~allocation}~\cite{ho2014likes} 
  is an method for re-ranking the output of a rating prediction model. It has two phases: \begin{enumerate*}
\item resources are  allocated to items according to the received ratings, and
\item the resources are distributed according to the relative preferences of the users, and top-$\size$ sets are generated by  assigning a a 5D score (for accuracy, balance, coverage, quality, and quantity of long-tail items) to every user-item pair.
\end{enumerate*}
We use the variants  proposed in~\cite{ho2014likes},  which are combinations of the scoring function (5D) with the rank by rankings (RR) and accuracy filtering (A) algorithms (Section 3.2.2 in~\cite{ho2014likes}). We use the template $\texttt{5D} (\texttt{ARec}, \texttt{A}, \texttt{RR})$ to show the different combinations, where A and RR are optional.  
We implemented and ran all four variants  with default values set according to~\cite{ho2014likes}: $k=3.|\mathcal{I}|$ and $q=1$. 
\iffullpaper
\else
We report results for 5D(ARec) and 5D(ARec, A, RR).%, with others in~\cite{ourFullVersion}.
\fi

\item \textbf{Personalized Ranking Adaptation (PRA)}~\cite{jugovac2017efficient} is  a generic re-ranking framework, that first estimates user tendency for various criteria like diversity and novelty, then  iteratively and greedily re-ranks  items in the head of the recommendations  to match the top-$\size$ set  with the user tendencies. We compare with the novelty-based variant of this framework, which relies  on item popularity statistics  to measure   user novelty tendencies. We use the  the mean-and-deviation based heuristic, that  is measured using the popularity of rated items, and was shown to provide comparable results with other heuristics in~\cite{jugovac2017efficient}.  For the configurable parameters, we followed~\cite{jugovac2017efficient}: Sample set size $S_u \in \min(|\mathcal{I}^{\mathcal{R}}_u|,10)$,  the exchangeable set size $|X_u| \in \{10,20\}$, and used ``optimal swap'' strategy with $maxSteps=20$. We use the  template $\texttt{PRA}(\texttt{ARec}, |X_u|)$ to refer to variants of PRA.
%then  and then  iteratively and greedily re-ranks items a small number of items  to match top-$\size$ set characteristics  with user preferences.%Recently,~\cite{jugovac2017efficient} proposed a generic Personalized Ranking Adaptation (PRA) framework. The idea is to  estimate user tendency for various objectives like diversity and item popularity using various heuristics, and to recommend top-$\size$ sets that match those tendencies.  We use \textbf{PRA} as a baseline.  The framework iteratively and greedily re-ranks items in the head of the recommendations to match them with the user preferences.
\end{itemize}

