\iffalse
The  motivation for this is two-fold. First, as shown in most recommenders that focus only on accuracy, has low coverage and tend to recommend popular items that the users is likely already aware of. By considering coverage, we promote long-tail items into recommendation sets and consequently introduce novelty and serendiptiy into recommendation sets, and allow users to discover items they were not aware of~\cite{cremonesi2010performance, jambor2010optimizing, yin2012challenging}. This is satisfactory for the consumers, particularly for those with  niche tastes. Second,  long-tail item promotion results in item inventory exposure, and helps spread the demand more evenly between the hits and niche items. This is  satisfactory for the producer~\cite{azaria2013movie,vargas2014improving}.  However, it is essential to promote long-tail items in a targeted manner. 
\fi
\section{GANC: Generic Re-ranking Framework}
\label{sec:RiskbasedReranking} 
We define user satisfaction  based on the accuracy of the top-$\size$ set and its coverage of the item space, so as to introduce novelty and serendipity into recommendation sets.   %on the relevance of items in the top-$\size$ set and the promotion of long-tail items. 
We consider three components for our framework: \begin{enumerate*}
\item an accuracy recommender (ARec) that is responsible for suggesting accurate top-$\size$ sets. 
\item a coverage recommender (CRec)  that is responsible for  suggesting top-$\size$ sets that maximize coverage across the item space, and consequently promote long-tail items. 
\item the user preference for long-tail novelty  $\theta_u \in [0,1]$. 
\end{enumerate*} 
We use the template $\texttt{GANC} (\texttt{ARec}, \bm{\theta}, \texttt{CRec)}$ to specify the used components. 

We define individual user value functions  for a top-$\size $ set $\mathcal{P}_u$  as%The  value of a set of items $\mathcal{P}_u$ for user $u$, is quantified using a personalized value function %take the cooperative mixture of experts view, and
\begin{equation} 
\small
v_u(\mathcal{P}_u) =  (1-\theta_u) a(\mathcal{P}_{u})  + \theta_{u} c(\mathcal{P}_{u})  \label{eq:vurelcov}
\end{equation}
% $v_{u}, a, c :\mathcal{I}^{|\mathcal{A}|} \rightarrow [0,1]$ and $\theta_u \in [0,1]$ is a user-specific trade-off value.
where  $a(.)$  measures the score of a set of items according to the accuracy recommender, and $c(.)$  measures the score of a set according to the coverage recommender. With slight abuse of notation, let $a(i)$ and  $c(i)$ denote the accuracy score and  coverage score  of a single item $i$. We ensure $a(i), c(i) \in [0,1]$ to have the same scale. Furthermore, we define  $a(\mathcal{P}_u) = \sum_{i \in \mathcal{P}_u} a(i) $ and $c(\mathcal{P}_u) = \sum_{i \in \mathcal{P}_u} c(i) $.

The user  value function in Eq.~\ref{eq:vurelcov} positively rewards  sets that increase coverage.  Similar intuitions for encouraging solutions with desirable properties,~e.g., diverse solutions, have been used in related work~\cite{ agrawal2009diversifying}. %,qin2013promoting}.  %lin2011class,gollapudi2009axiomatic,
  However, their trade-off parameters  are typically obtained via parameter tuning or cross validation. By contrast, we impose personalization via the user preference estimate, $\theta_u$, that  is learned based on historical rating data. Next, we list the various base recommender models integrated into GANC.
 
\iffalse
Competitive Mixture of Experts :The accuracy recommender and the coverage recommender can be viewed as two experts, and  any of our risk measures can be used as a probabilistic switching indicator. Specifically,  the top-$\size$ set for a user is determined as follows:
\begin{enumerate}
%\item Generate random sample $b \sim \text{Bern}(\theta_u)$
\item With probability $1-\theta_u$ we choose the set suggested by the accuracy recommender, i.e., $v_u(\mathcal{P}_u) =   a(\mathcal{P}_{u}) $. 
\item With probability $\theta_u$ we suggest the top-$\size$ set suggested by the coverage recommender, i.e., $ v_u(\mathcal{P}_u) =   c(\mathcal{P}_{u})$.  
\end{enumerate} 
This model resolves scale issues between the two objective functions. 
\fi
 
\subsection{Accuracy recommender}
\label{sec:accuracyRecommender}
The accuracy recommender provides an accuracy score, $a(i)$, for each  item $i$.  We experiment with three models (Section~\ref{sec:ExpSetup} provides details and setup configurations): 
\begin{itemize}
\item \textbf{Most popular (Pop)} ~\cite{cremonesi2010performance}  is non-personalized and recommends the most popular unseen items. It makes accurate recommendations, but has low coverage and novelty~\cite{cremonesi2010performance}. Since it does  not score items, we define $a(i)=1$ if item $i$ is in the top-$\size$ set suggested by Pop, otherwise $a(i)=0$. 


\item \textbf{Regularized SVD (RSVD)}~\cite{zhuang2013fast} learns latent factors for users and items by analyzing  user-item interaction data (e.g.,~ratings). The  factors  are then used to predict the values of unobserved ratings. We use RSVD to compute a predicted rating matrix $\mathbf{\hat{R}} \in \mathbb{R}^{\vert \mathcal{U} \vert \times \vert \mathcal{I} \vert}$. We normalize the predicted rating vectors  of all users  to ensure  $\hat{r}_{ui} \in [0,1]$, and define $a(i) =\hat{r}_{ui}$. %These scores correspond to relevance scores. %(Unless otherwise stated, $\hat{r}$ denotes normalized values.)  % $a(\mathcal{P}_u ; \mathbf{\hat{R}} ) =\frac{1}{\size} \sum_{i \in \mathcal{P}_u}{\hat{r}_{ui}}$. Note,  the scores obtained from MF correspond to relevance scores, 


\item  \textbf{PureSVD (PSVD)}~\cite{cremonesi2010performance} is also a latent factor model.  We follow the same procedure  as RSVD, using PSVD factors~\cite{cremonesi2010performance}. Note, PSVD scores  correspond to associations between users and items.
\end{itemize}
\iffalse
 We use Pop, MF, and PureSVD in our framework.  Since Pop does not output scores for items, we define $a(i)=1$ if item $i$ is in the top-$\size$ set suggested by pop, otherwise $a(i)=0$. Note, the risk estimated in our framework  allows us to introduce personalization into this non-personalized method.
For MF and PureSVD, we use the predicted factors to compute the predicted rating matrix $\mathbf{\hat{R}} \in \mathbb{R}^{\vert \mathcal{U} \vert \times \vert \mathcal{I} \vert}$.   Next, we normalize the completed rating vectors  of all users to ensure  $\hat{r}_{ui} \in [0,1]$. (Unless otherwise stated, $\hat{r}$ denotes normalized values.) $a(i) =\hat{r}_{ui} $. % $a(\mathcal{P}_u ; \mathbf{\hat{R}} ) =\frac{1}{\size} \sum_{i \in \mathcal{P}_u}{\hat{r}_{ui}}$.
 Note,  the scores obtained from MF correspond to relevance scores, while  those obtained from PureSVD  correspond to associations between users and items.
MF accurately predicts the relevance of  items to users.  We combine the predicted ratings of unseen items with the   ratings of seen items, and form the completed the user-item interaction matrix,  $\mathbf{\hat{R}} \in \mathbb{R}^{\vert \mathcal{U} \vert \times \vert \mathcal{I} \vert}$.   Next, we normalize the completed rating vectors  of all users to ensure  $\hat{r}_{ui} \in [0,1]$. (Unless otherwise stated, $\hat{r}$ denotes normalized values.) $a(i) =\hat{r}_{ui} $. % $a(\mathcal{P}_u ; \mathbf{\hat{R}} ) =\frac{1}{\size} \sum_{i \in \mathcal{P}_u}{\hat{r}_{ui}}$.
 
\begin{equation}
\small
w_u(\mathcal{P}_u ; \mathbf{\hat{R}} ) =\frac{1}{\size} \sum_{i \in \mathcal{P}_u}{\hat{r}_{ui}}
\label{eq:modular-relevance}
\end{equation}
\fi 


\subsection{Coverage recommender}
The coverage recommender provides  a coverage score, $c(i)$, for each  item $i$. We use  three coverage recommenders: 
\begin{itemize}


\item \textbf{Random (Rand)}  recommends  $\size$ unseen items randomly. It has high coverage, but  low accuracy~\cite{vargas2014improving}. We define $c(i) \sim $unif$(0,1)$. %Performance-wise it is similar to Bal. (Dynamic coverage), with better  gini performance.

%\subsubsection{Divide} This model applies the same strategy as the accuracy recommender, $c(i)= a(i)$, if  item $i$ is a long-tail item in the train set, otherwise $c(i)=0$. 

\iffalse
\subsubsection{Binary Complement  SVD} Here, we represent the user-item rating matrix using binary values, where 1 indicates the user rated the item and 0 indicates the item has not received any rating. We then apply SVD on the matrix. The resulting $\mathbf{\hat{R}}$ is estimated as in PureSVD. However to derive a model that  promote long-tail items,  we  need to flip the result $\hat{r}^{'} = 1 - \hat{r}$.
\fi

\item \textbf{Static (Stat)} focuses exclusively on promoting less popular items. We define $c(i)$ to be a monotone decreasing function of $f_i^{ \trainset}$,  the popularity  of $i$ in the train set $\trainset$. We use $c(i) =  \frac{1}{ \sqrt{f_i^{\trainset}+1 }}$ in our work.  %,i.e., $f(i) = |\usersofIteminTrainset|$. 
Note the gain of recommending an item is constant. 

%$\mathcal{A}$ a partial allocation where a subset of the users have been assigned top-$\size$ sets
%$c(\mathcal{P}_u,\mathcal{A}) = \frac{1}{\size} \sum_{i \in \mathcal{P}_u}\frac{1}{f(i,\mathcal{A}) + 1}\label{eq:coverage}$


\item \textbf{Dynamic (Dyn)}  allows us to better correct for the popularity bias in recommendations. In particular,  rather than the train set $\trainset$,  we  define $c(i)$ based on the set of recommendations  made so far. Let $\mathcal{P}  = \{ \mathcal{P}_u \}_{u=1}^ {|\mathcal{U}|} $ with $|\mathcal{P}_u| = \size$, denote the collection of top-$\size$ sets assigned to all users, and $\mathcal{A}  = \{ \mathcal{A}_u \}_{u=1}^ {|\mathcal{U}|} $  with $\mathcal{A}_u \subseteq \mathcal{P}_u$, denote  a partial collection where  a subset of users have been assigned  some items. We measure the long-tail appeal  of an item  using a monotonically decreasing function of the popularity of $i$ in $\mathcal{A}$, i.e.,  $f_i^{\mathcal{A}}$. We use 
$c(i) = \frac{1}{\sqrt{ f_i^{\mathcal{A}} + 1} }\label{eq:coverage}$  in our work.  The main intuition is that recommending an item  has a diminishing returns property:  the more the item is recommended, the less the gain of the item in coverage, i.e., $c(i)=1$ when $\mathcal{A} = \emptyset$, but decreases as the item is more frequently suggested.%, $c(i)$ decreases.  % Next, we discuss how to integrate this recommender into our framework. 

\end{itemize}
%This recommender (Bal.) encourages the balanced promotion of all items.  It  provides the upper bound  performance achievable in terms of gini and coverage.
  
 
\subsection{Optimization Algorithm for GANC }
\label{sec:optimizationAlgorithm}
The overall goal of the framework is to find an optimal top-$\size$ collection $\mathcal{P}  = \{ \mathcal{P}_u \}_{u=1}^ {|\mathcal{U}|} $  that  maximizes  the  aggregate of the user value functions:
\begin{align}
\small
\max_{\mathcal{P} } \ v(\mathcal{P}) = \sum_{u \in \mathcal{U}} v_{u}(\mathcal{P}_{u})
\label{eq:overallValueFunction}
\end{align}
%where $v_u(.)$ is the value function of user $u$, $\mathcal{P}_u$ is the top-$\size$ set for user $u$, and $\mathcal{P}$ is the collection of all top-$\size$ sets, i.e.,  $\mathcal{P}  = \{ \mathcal{P}_u \}_{u=1}^ {|\mathcal{U}|} $. 

The combination of Rand and Stat with the accuracy recommenders in Section~\ref{sec:accuracyRecommender}, result in value functions that can be optimized greedily and independently, for each user. Using Dyn, however,  creates a dependency between the optimization of user value functions, where the items suggested to one user depend on those suggested to previous users.  Therefore,  user value functions can no longer be optimized independently. 
\input{algSeqSample}

\vspace{4mm}
\noindent \textbf{Optimization algorithm for GANC with Dyn. }
\label{sec:optimizingFrameworkWithDynamic}
Because Dyn is monotonically decreasing in $f_i^{\mathcal{A}}$, when used in Eq.\ref{eq:vurelcov}, the overall objective in Eq.~\ref{eq:overallValueFunction} becomes submodular across users. Maximizing a submodular function is NP-hard~\cite{khuller1999budgeted}.  %Maximizing a submodular function is NP-hard~\cite{khuller1999budgeted}.
However, a key observation is that  the constraint of recommending $\size$ items to each user, corresponds to a partition matroid over the users.
 Finding  a top-$\size$ collection $\mathcal{P}$ that maximizes Eq.~\ref{eq:overallValueFunction} is therefore an instance of  maximizing a submodular function subject to a matroid constraint \iffullpaper
(see Appendix~\ref{sec:submodularmonotoneproof}).
\else
(see~\cite{ourFullVersion} for details).
\fi
A \textit{Locally Greedy} heuristic, due to Fisher et al.~\cite{fisher1978analysis}, can  be applied:  consider the users separately and in arbitrary order. At each step, select a user $u$ arbitrarily, and greedily construct $\mathcal{P}_u$  for that user. Proceed until all users have been assigned top-$\size$ sets. Locally Greedy produces a solution at least half the optimal value for maximizing a submodular monotone function subject to a matroid constraint~\cite{fisher1978analysis}.

\iffalse
\begin{theorem}
\label{thm:greedylocallygreedy}
The locally greedy heuristic is a $1/2$-approximation algorithm for maximizing Eq.~\ref{eq:overallValueFunction} when $v$ is submodular monotone increasing~\cite{fisher1978analysis}.
\end{theorem}
\fi

However, locally greedy is sequential and has a computational complexity of $O(\vert \mathcal{U} \vert . \vert \mathcal{I}\vert . \size)$ which is not scalable.  Instead, we introduce a heuristic we call \textit{Ordered Sampling-based Locally Greedy} (OSLG).  Essentially, we make two modifications based on the user long-tail preferences:  first, proportionate to the distribution of user long-tail preferences $\bm{\theta}$, we sample a subset of users, and run the sequential algorithm on this sample only.   Second, to allow the system to recommend  more established or popular products to users with lower long-tail preference, instead of arbitrary order,  we  modify  locally greedy to consider users in increasing long-tail preference.        
\iffalse
to improve the scalability of the algorithm, we only run the sequential part  on a representative sample  of the users $\mathcal{S} \subset \mathcal{U}$, and propagate the information for users not included in the sample. 
first,  locally greedy, operates on users  sequentially and in arbitrary order.   We  modify locally greedy to consider users in increasing risk degree. 
\fi


Algorithm~\ref{algo:algSeqSample} shows GANC with OSLG optimization:  We use $\mathbf{f}_i$ as a shorthand for $f_i^{\mathcal{A}}$, the  popularity of item $i$ in the current set of recommendations, used in Dyn. First, $\mathbf{f}$ is initialized and user preferences $\bm{\theta}$ are estimated (line~\ref{algo:algSeqSample:line:initilize}). Next, we  use  Kernel density estimation (KDE)~\cite{sheather1991reliable} to approximate the Probability density function (PDF) of $\bm{\theta}$, and use the PDF to draw a sample $\mathcal{S}$ of size $S$ from $\bm{\theta}$ and find the corresponding users in $\mathcal{U}$ (line~\ref{algo:algSeqSample:line:sample}). The sampled users are then sorted in increasing long-tail preference (line~\ref{algo:algSeqSample:line:sortUsers}), and the algorithm iterates over the users. In each iteration, it updates the Dyn function (line~\ref{algo:algSeqSample:line:updateCoverageFunction1}) and assigns a top-$\size$ set  to the current user by maximizing her value function (line~\ref{algo:algSeqSample:line:top-NAssign1}). The Dyn function parameter  $\mathbf{f}$ is then updated w.r.t. the recently assigned top-$\size$ set (line~\ref{algo:algSeqSample:line:updateStep}). Moreover, $\mathbf{f}$ is associated with the current long-tail preference estimate $\theta_u$, and stored (line~\ref{algo:algSeqSample:line:storeshit}). The algorithm then proceeds to the next user. Since Dyn  is monotonically decreasing in $\mathbf{f}_i$, frequently recommended items are weighted down by the value function of subsequent users. Consequently, as we reach users who prefer long-tail items and discovery, their value functions prefer relatively unpopular items that have not been recommended before. Thus, the induced user ordering, results in the promotion of long-tail items to the right group of users, such that we obtain better coverage while maintaining user satisfaction.

 For each user not included in the sample set, $u \not\in \mathcal{S}$, we find the most similar user $s \in \mathcal{S}$, where similarity  is defined as $|\theta_s-\theta_u|$ (line~\ref{algo:algSeqSample:line:getFrequency}), and use $F(\theta_s)$ to compute the coverage score (line~\ref{algo:algSeqSample:line:updateCoverageFunction2}), and assign a top-$\size$ set. Observe,  the value functions of $u \in \mathcal{U} \setminus \mathcal{S}$, are independent of each other, and  lines	\ref{algo:algSeqSample:line:getFrequency}-\ref{algo:algSeqSample:line:updateP}  can be performed in parallel. The computational complexity of the sequential part drops to $O(\vert \mathcal{S} \vert . \vert \mathcal{I}\vert . \size)$ at the  cost of  $O(\vert \mathcal{S} \vert . \vert \mathcal{I}\vert )$ extra memory.

%\vspace{4mm}
% For Dyn, we report  sample size e.g.,~GACN(Pop, Dyn500, $\theta^{G}$) uses 500 samples. 

\iffalse
\begin{align*}
\texttt{Pref. Model}_{\texttt{Acc. Rec.}}^{\texttt{Cov. Rec.}}
\end{align*}
\fi