\subsection{Distribution of long-tail novelty preferences}
Figure~\ref{fig:Hist-LT-PreferenceModels} plots the histogram of various long-tail preference 
\iffullpaper
models. 
\else 
(results for other datasets are omitted due to space limitations, but have similar trends). 
\fi 
We observe $\theta^A_u$ is skewed to the right. This is due to the sparsity problem, where the majority of users rate a few items. $\theta^N_u$  is also skewed to the right across all datasets, due to both the popularity bias and sparsity problems~\cite{agarwal_chen_2016,Marlin07collaborativefiltering}. 
On the other hand, $\theta^G_u$ is normally distributed, with a larger mean and  a larger variance, on all datasets. In the experiments in Section~\ref{sec:perfOSLG}, we study the effect of these preference models on performance. 

\iffalse
One explanation for the bias of all preference estimates towards lower values, is the popularity bias of  training data. In some datasets, such as MovieLens, movies are presented to users to warm-start the recommender system. The presented movies, and the observed ratings, can be biased toward more popular movies~\cite{agarwal_chen_2016,Marlin07collaborativefiltering}. As a result, the preference estimates are biased towards lower values for datasets that exhibit this popularity bias. 

Note,  on the MovieTweetings dataset, where users voluntarily rate movies, and the effect of presentation bias is negligible,   $\theta^*$ is normally distributed  with $\mu_{\theta^*}=0.522$ and $\sigma_{\theta^*}=0.178 $, while $\theta^T$ and $\theta^N$ remain skewed toward lower values. 

Note, popularity bias and sparsity are inherent characteristics of the recommendation domain, although some datasets suffer more.  Totally eliminating this bias, may not be advantageous. 
\fi
%On Netflix ($\mu_{\theta^*}=0.323, \sigma_{\theta^*}=0.133$)
%On ML-10M ($\mu_{\theta^*}=0.312, \sigma_{\theta^*}=0.135$)
%On ML-1M ($\mu_{\theta^*}=0.349, \sigma_{\theta^*}=0.133$)


\iffullpaper
\begin{figure*}[t]
  %\vspace{-20pt}
\centering
        \subfloat[ML-100K]
        {\includegraphics[scale=0.20]{Figures/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}
		 \label{fig:ML-Users-vs-theta}
        } 
         \subfloat[ML-1M]
        {\includegraphics[scale=0.20]{Figures-ml-1m/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}%twoHistogramsofthetaTNRthetaTFIDFthetaStarthetaNorNLT.pdf}
        
		 \label{fig:ML-Users-vs-theta}
        }         
        \subfloat[ML-10M]
	    {\includegraphics[scale=0.20]{Figures-ml-10m/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}
		 \label{fig:ML-Users-vs-theta}
        }
        \subfloat[MT-200K]
        {\includegraphics[scale=0.20]{Figures-mt-200k-m5-mapped/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}%twoHistogramsofthetaTNRthetaTFIDFthetaStarthetaNorNLT.pdf}
		 \label{fig:ML-Users-vs-theta}
        } 
        \subfloat[Netflix]
        {\includegraphics[scale=0.20]{FiguresSum/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}
		 \label{fig:ML-Users-vs-theta}
        } 

\caption{Histogram of long-tail novelty preference models. Observe $\theta^A_u$ is skewed toward smaller values because of sparsity, i.e.,~the majority of users rate a few items.  $\theta^N_u$ is also biased toward smaller values, due to a combination of popularity bias and sparsity. $\theta^T_u$ and $\theta^G_u$ are less biased and more normally distributed and  alleviate both problems.} %In addition to the disadvantages mentioned in Section~\ref{sec:simple-lt-pref},
\label{fig:Hist-LT-PreferenceModels}
\end{figure*}

\else

\begin{figure}[t]
  %\vspace{-20pt}
\centering
         \subfloat[ML-1M]
        {\includegraphics[scale=0.28]{Figures-ml-1m/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}%twoHistogramsofthetaTNRthetaTFIDFthetaStarthetaNorNLT.pdf}
		 \label{fig:ML-Users-vs-theta}
        }          
        \subfloat[Netflix]
        {\includegraphics[scale=0.28]{FiguresSum/CombinedHistograms/twoHistogramsofthetaTNRthetaNorNLTthetaTFIDFthetaStar2.pdf}
		 \label{fig:ML-Users-vs-theta}
        } 

\caption{Histogram of long-tail novelty preference models. Observe $\theta^A_u$ is skewed toward smaller values because of sparsity, i.e.,~the majority of users rate a few items.  $\theta^N_u$ is also biased toward smaller values, due to a combination of popularity bias and sparsity. $\theta^T_u$ and $\theta^G_u$ are less biased and more normally distributed and  alleviate both problems.} %In addition to the disadvantages mentioned in Section~\ref{sec:simple-lt-pref},
\label{fig:Hist-LT-PreferenceModels}
\end{figure}

\fi


\iffullpaper

\begin{figure*}[t]
\centering
		\subfloat[PSVD100]
		{
		\includegraphics[width=0.22\textwidth]{Figures-ml-1m/SampleSize/thetaStar2/pureSVD_best/fmeasureoverSampleSize.pdf} 
		}
		\subfloat[PSVD10]
		{
		\includegraphics[width=0.22\textwidth]{Figures-ml-1m/SampleSize/thetaStar2/pureSVD_10/fmeasureoverSampleSize.pdf} 
		}
		\subfloat[Pop]
		{
		\includegraphics[width=0.22\textwidth]{Figures-ml-1m/SampleSize/thetaStar2/pop_best/fmeasureoverSampleSize.pdf} 
		}
        \subfloat[RSVD]
		{
		\includegraphics[width=0.22\textwidth]{Figures-ml-1m/SampleSize/thetaStar2/rhat_best/fmeasureoverSampleSize.pdf} 
		}
		
		
\caption{Performance of GANC(ARec, $\bm{\theta}^{G}$, Dyn) with OSLG optimization, as sample size (S) is varied. The accuracy recommender ARec is indicated in each sub-figure. Dataset is ML-1M.}
\label{fig:SampleSize1}
\end{figure*}


\begin{figure*}[t]
\centering
		\subfloat[PSVD100]
		{
		\includegraphics[width=0.22\textwidth]{Figures-mt-200k-m5-mapped/SampleSize/thetaStar2/pureSVD_best/fmeasureoverSampleSize.pdf} 
		}
		\subfloat[PSVD10]
		{
		\includegraphics[width=0.22\textwidth]{Figures-mt-200k-m5-mapped/SampleSize/thetaStar2/pureSVD_10/fmeasureoverSampleSize.pdf} 
		}
		\subfloat[Pop]
		{
		\includegraphics[width=0.22\textwidth]{Figures-mt-200k-m5-mapped/SampleSize/thetaStar2/pop_best/fmeasureoverSampleSize.pdf} 
		}
        \subfloat[RSVD]
		{
		\includegraphics[width=0.22\textwidth]{Figures-mt-200k-m5-mapped/SampleSize/thetaStar2/rhat_best/fmeasureoverSampleSize.pdf} 
		}
		
\caption{Performance of GANC(ARec, $\bm{\theta}^{G}$, Dyn) with OSLG optimization, as sample size (S) is varied. The accuracy recommender ARec is indicated in each sub-figure. Dataset is MT-200K.}
\label{fig:SampleSize2}
\end{figure*}

\else
\begin{figure}[b]
\centering
		\subfloat[ARec is PSVD100]
		{
		\includegraphics[width=0.23\textwidth]{Figures-ml-1m/SampleSize/thetaStar2/pureSVD_best/fmeasureoverSampleSize.pdf} 
		}
		\subfloat[ARec is PSVD10]
		{
		\includegraphics[width=0.23\textwidth]{Figures-ml-1m/SampleSize/thetaStar2/pureSVD_10/fmeasureoverSampleSize.pdf} 
		}
\caption{Performance of GANC(ARec, $\bm{\theta}^{G}$, Dyn) with OSLG optimization, as sample size (S) is varied. The accuracy recommender ARec is indicated in each sub-figure. Dataset is ML-1M.}
\label{fig:SampleSize}
\end{figure}
\fi
\subsection{Performance of GANC with Dyn coverage recommender}
\label{sec:perfOSLG}
When Dyn is integrated in GANC, we use OSLG optimization. We run variants of GANC that involve   randomness (e.g., sampling-based variants) 10 times and report the average. 

\vspace{4mm}
\noindent \textbf{Effect of sample size. }The  sample size $S$ is a  system-wide hyper-parameter in the OSLG algorithm,  and should be determined w.r.t. preference for accuracy or coverage, and the accuracy recommender.  For tuning $S$, we run experiments on ML-1M, and  assess its effect   on F-measure  and coverage. 
\iffullpaper
As shown in Figures~\ref{fig:SampleSize1} and~\ref{fig:SampleSize2}, increasing $S$, increases coverage and decreases the F-measure of  most  accuracy recommenders.   Regarding RSVD, the scores output by this model lead to the initial decrease and later increase of  F-measure.  Since  we want  to maintain  accuracy, we fix $S=500$ in the rest of our experiments,  although a larger $S$ can be used. 
\else
Figure~\ref{fig:SampleSize} shows the effect of increasing $S$ on coverage and F-measure for two of our base accuracy recommenders (other results are omitted but the trends are similar~\cite{ourFullVersion}). Since  we want  to maintain  accuracy, we fix $S=500$ in the rest of our experiments,  although a larger $S$ can be used. 
\fi


\vspace{4mm}
\noindent \textbf{Effect of the user long-tail novelty preference model, the accuracy recommender, and their interplay. } 
We evaluate GANC with Dyn coverage, i.e., GANC(ARec, $\bm{\theta}$, Dyn),  while varying the accuracy recommender ARec, and the long-tail novelty preference model $\bm{\theta}$. We examine the following long-tail preference models: 
\begin{itemize}
\item \textbf{Random} $\bm{\theta}^{R}$ randomly initializes $\theta^{R}_u$ (10 times per user).%, run the algorithm and average over the  10 runs. 
\item \textbf{Constant} $\bm{\theta}^{C}$ assigns the same  constant value  $C$ to all users. We report results for $C=0.5$. %We tested $C \in {0.2,0.5,0.7}$, and report the best result. %on each dataset. \fi
\item \textbf{Normalized Long-tail} $\bm{\theta}^{N}$ (Eq.~\ref{eq:nlt-risk}) measures the proportion of long-tail items the user has rated in the past. 
\item \textbf{Tfidf-based} $\bm{\theta}^{T}$ (Eq.~\ref{eq:tfidf-risk}) incorporates user interest and popularity of items. 
\item \textbf{Generalized} $\bm{\theta}^{G}$  (Eq.~\ref{eq:overallRiskUpdate}) incorporates user interest, popularity of items, and the preferences of other users. 
\end{itemize}

Due to sampling ($S=500$), we run each variant 10 times (with random shuffling for  $\bm{\theta}^C$) and average over the runs.  We run the experiments on ML-1M.  %We set $S=|\mathcal{U}|$, so as to remove the effect of sampling, and run the experiments on ML-1M and MT-200K.

Figure~\ref{fig:thetavsthetaStar} shows performance of GANC(ARec, $\bm{\theta}$, Dyn) as $\bm{\theta}$ and the accuracy recommender (ARec) are varied. Across all rows, as expected, the accuracy  recommender on its own, typically obtains the best F-measure.  Moreover, $\bm{\theta}^{R}$ and $\bm{\theta}^{C}$ often have the lowest F-measure. Different variants of GANC with $\bm{\theta}^{N},\bm{\theta}^{T}$, and $\bm{\theta}^{G}$ are generally in the middle, which shows that these  preference estimates are better than  $\bm{\theta}^{R}$ and $\bm{\theta}^{C}$ in terms of accuracy.  For Stratified Recall, similar trends as accuracy are observed in the ranking of the algorithms.   The trends are approximately the same as we vary  the accuracy recommender  in Figures~\ref{fig:thetavsthetaStar}.b,~\ref{fig:thetavsthetaStar}.c, and~\ref{fig:thetavsthetaStar}.d. 

 
%Re-read
%In Figure~\ref{fig:thetavsthetaStar}.a,  we observe $\theta^{R}$ and $\theta^{C}$  are able to  increase the novelty and coverage of recommendations.  On the other hand, $\theta^{T}$ and $\theta^{N}$, are not able to sufficiently increase coverage and F-measure since they are biased toward lower values (Figure~\ref{fig:Hist-LT-PreferenceModels}), and are more restrained in promoting long-tail items. Compared to $\theta^{T}$ and $\theta^{N}$,  $\theta^{*}$  obtains the highest coverage and novelty levels. 
%By using better estimates, we can increase novelty and coverage while  maintaining reasonable levels of  precision and recall.



\begin{figure*}[p]
\centering 
        \subfloat[Accuracy recommender (ARec) is RSVD]
        {
		\includegraphics[width=0.88\textwidth]{Figures-ml-1m/metrics/sf_gini_over_N_impMetrics_cbr_rhat_best_500.pdf}
		\label{fig:ThetaConfigs}
        }
        
        \subfloat[Accuracy recommender (ARec) is PSVD100]
        {
		\includegraphics[width=0.88\textwidth]{Figures-ml-1m/metrics/sf_gini_over_N_impMetrics_cbr_pureSVD_best_500.pdf}
		\label{fig:ThetaConfigs}
        }
        
        \subfloat[Accuracy recommender (ARec) is PSVD10]
        {
		\includegraphics[width=0.88\textwidth]{Figures-ml-1m/metrics/sf_gini_over_N_impMetrics_cbr_pureSVD_10_500.pdf}
		\label{fig:ThetaConfigs}
        }
        
        \subfloat[Accuracy recommender (ARec) is Pop]
        {
		\includegraphics[width=0.88\textwidth]{Figures-ml-1m/metrics/sf_gini_over_N_impMetrics_cbr_pop_best_500.pdf}
		\label{fig:ThetaConfigs}
        }
        
\caption{Performance of GANC(ARec, $\bm{\theta}$, Dyn), with fixed sample size $S =|500|$,  different accuracy recommenders (ARec), and different long-tail preference models ($\bm{\theta}$). Dataset is ML-1M. The trends are approximately the same as we vary  the accuracy recommender  in Figures~\ref{fig:thetavsthetaStar}.a,~\ref{fig:thetavsthetaStar}.b,~\ref{fig:thetavsthetaStar}.c, and~\ref{fig:thetavsthetaStar}.d. In each row, ARec typically achieves the highest F-measure, but performs poorly w.r.t. coverage and gini.  Variants of our framework that use $\bm{\theta}^{G}$, $\bm{\theta}^{T}$, $\bm{\theta}^{N}$, obtain higher f-measure levels compared to those that use    $\bm{\theta}^{R}$ and $\bm{\theta}^{C}$. They also improve stratified recall, independent of the accuracy recommender. Stratified Recall emphasizes novelty and accuracy}.%, while LTAccuracy emphasizes novelty and coverage.}
\label{fig:thetavsthetaStar}
\end{figure*}

\iffalse 
\input{otherResults}
\fi

\section{Comparison with other recommendation models}
We conduct two rounds of experiments: first, we evaluate re-ranking frameworks that post-process rating-prediction models, then we  study general top-$\size$ recommendation algorithms. For GANC, we run variants that involve randomness (e.g., sampling-based variants)  10 times, and report the average.

\subsection{Comparison with re-ranking methods for rating-prediction}
\label{sec:re-rankingratingprediction}
Standard re-ranking frameworks typically use a rating prediction model as their  accuracy recommender. In this section, we use RSVD as the underlying rating prediction model, and analyze performance across datasets with varying density levels.   We  compared the standard greedy ranking strategy of RSVD with 
 5D(RSVD), 5D(RSVD, A, RR),
 %5D(RSVD), 5D(RSVD, A), 5D(RSVD, RR), 5D(RSVD, A, RR),
 RBT(RSVD, Pop), RBT(RSVD, Avg),  PRA(RSVD,10),  PRA(RSVD,20), GANC(RSVD, $\bm{\theta}^T$, Dyn), and  GANC(RSVD, $\bm{\theta}^G$, Dyn). We report results for $\size=5$, since users rarely look past the items at the top of  recommendation sets.  Table~\ref{tab:re-rankingREGSVD} shows top-$5$ recommendation performance.  
% We have omitted ML-100K and Netflix due to space limitations, although their results are similar to ML-1M, and MT-200K, respectively.

\begin{table*}[pt]
\centering
\footnotesize
%\begin{tabular}{lllllllll}
%  \toprule
%&Alg.&HM-Rank & HM-Score&PR@5&F@5&L@5&C@5&G@5 \\

%\begin{tabular}{lllllllllll}
%  \toprule
%&Alg.&HM-Rank & HM-Score&PR@5&F@5&P@5&R@5&L@5&C@5&G@5 \\

\begin{tabular}{lllllllc}
  \toprule
&Alg.&F@5&S@5&L@5&C@5&G@5& Score\\
 \toprule 
\iffullpaper
\multirow{9}{*}{\rotatebox[origin=c]{90}{~ML-100K}} 
\input{FiguresSum/Tables_ICDE/rMF-ml-100k-8_Partial-latex.txt}
\midrule
\fi
\multirow{9}{*}{\rotatebox[origin=c]{90}{~ML-1M}}
\input{FiguresSum/Tables_ICDE/rMF-ml-1m-8_Partial-latex.txt}
\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{~ML-10M}} 
\input{FiguresSum/Tables_ICDE/rMF-ml-10m-8_Partial-latex.txt}
\midrule
 \multirow{9}{*}{\rotatebox[origin=c]{90}{~MT-200K}}
\input{FiguresSum/Tables_ICDE/rMF-mt-200k-m5-mapped-8_Partial-latex.txt}
\iffullpaper
\midrule
\multirow{9}{*}{\rotatebox[origin=c]{90}{~Netflix}} 
\input{FiguresSum/Tables_ICDE/rMF-netflix-8_Partial-latex.txt}
\fi
\bottomrule
\end{tabular}  
\caption{Top-5 recommendation performance for re-ranking a rating prediction model, RSVD. \iffullpaper  \else We have omitted ML-100K and Netflix, since they obtain similar results as ML-1M and MT-200K, respectively. \fi    
 The metrics are (F)measure@5, (S)tratified Recall@5, (L)TAccuracy@5, (C)overage@5, and (G)ini@5. Bolded entries show the best value for each metric, with relative rank of each algorithm on each metric inside parenthesis. For all models,  improving trade-offs is better on dense datasets. Regarding our two variants of GANC  (with fixed sample size $S=500$), they outperform others in all metrics except LTAccuracy, in dense settings (ML-1M).  On all datasets, our models obtain the lowest average rank across all metrics (last column). Overall, the results suggest a different accuracy recommender should be used in sparse settings.   }
\label{tab:re-rankingREGSVD}
\end{table*}

 Regarding RSVD, the model obtains high LTAccuracy, but under-performs all other models in coverage and gini. Essentially, RSVD picks a small subset of the items, including long-tail items,  and recommends them to all the users. After all, RSVD model is trained by minimizing Root Mean Square Error (RMSE) accuracy measure, which is  defined w.r.t. available data and not the complete data~\cite{steck2010training}. Therefore, when the model is used to choose a few items ($\size$)  from among \emph{all}  available items, as is required in top-$\size$ recommendation problems, it does not obtain good overall performance. 
 
In dense settings (ML-1M), GANC outperforms other models in all metrics except LTAccuracy. In sparse settings, except on ML-10M,  GANC has at least one variant in the top 2 methods w.r.t.  \iffullpaper F-measure. \else F-measure (see~\cite{ourFullVersion}). \fi  In both sparse and dense settings, except on ML-10M, GANC has at least one variant in the top 2 methods w.r.t.  stratified recall. Other methods, e.g.,~5D, focus exclusively on LTAccuracy and  reduce F-measure and stratified recall. 

In summary, the performance of  RSVD depends on the dataset density. On the sparse datasets, it  does not provide accurate suggestions w.r.t. F-measure, and subsequent re-ranking models make less accurate suggestions.  Although another reason for the smaller F-measure values on  datasets like  ML-10M (and Netflix), is the larger item space size. Top-5 recommendation, corresponds to a very small proportion of the overall item space. Re-ranking a rating prediction model like RSVD, is mostly effective for  dense settings.  However, our framework is generic,  and enables us to plug-in a different accuracy recommender. We show the results for this in the next section.   Moreover, all re-ranking techniques increase coverage, but reduce accuracy. 5D(RSVD) obtains the highest novelty among all models, but reduces accuracy significantly. On most datasets, GANC significantly increases coverage and decreases gini, while maintaining reasonable levels of accuracy.  
%(3.37\% in Netflix) 

\subsection{Comparison with top-N item recommendation models}
\label{sec:top-nrecommendation}

\begin{figure*}[t]	
\centering
	\subfloat[]
	{
		\includegraphics[width=1\textwidth]{FiguresSum/sfLTAccuracyvsfmeasurezainab.pdf} 
	}

\caption{Accuracy vs Coverage vs Novelty. The head of the arrow shows our main model GANC(ARec, $\bm{\theta}^G$, Dyn) with sample size  $S=500$, while the bottom shows the underlying accuracy recommender (ARec).   On MT-200K ARec is Pop. On other datasets it is PSVD-100.  Note, RSVD is consistently dominated by all other models in F-measure and coverage.  }
\label{fig:pcn}
\end{figure*}	

As shown previously, in sparse settings, re-ranking frameworks that rely on rating prediction models do not generate accurate solutions. In this section we plug-in a different accuracy recommender w.r.t. dataset density. 
On MT-200K,  we  plug-in  Pop. On all other datasets we plug-in  PSVD100 as the accuracy recommender.
 For GANC, we use three variants which differ in their coverage recommender: GANC(ARec, $\bm{\theta}^G$, Dyn), GANC(ARec, $\bm{\theta}^G$, Stat) and GANC(ARec, $\bm{\theta}^G$, Rand). We also compare to the generic re-ranking framework, PRA(ARec, 10). For both GANC and PRA, we always plug-in the same accuracy recommender. Furthermore, we compare with standard top-$\size$ recommendation algorithms:  Rand, Pop, RSVD, CofiR100, PSVD10, PSVD100.  


Figure~\ref{fig:pcn} compares  accuracy, coverage and novelty trade-offs. On all datasets,  Rand achieves the best coverage and gini, but  the lowest  accuracy.  Similar to~\cite{cremonesi2010performance,vargas2014improving}, we find the non-personalized method Pop,  which takes advantage of the  popularity bias of the data, is a strong contender in accuracy, but under-performs in coverage and novelty. 

Regarding GANC,  Figure~\ref{fig:pcn} shows the best improvements in coverage are obtained when we use either Rand or Dyn coverage recommenders. Stat is generally not a strong coverage recommender,~e.g, GANC(ARec, $\bm{\theta}^G$, Stat) obtains high novelty (LTAccuracy)  on ML-10M, but does not lead to significant improvements in coverage.  This is because Stat has constant gain for long-tail promotion and focuses exclusively on recommending (a small subset of the) long-tail items.  Our main model is GANC(ARec, $\bm{\theta}^G$, Dyn). An interesting observation is that on MT-200K, both GANC(ARec, $\bm{\theta}^G$, Dyn) and GANC(ARec, $\bm{\theta}^G$, Rand), that use the non-personalized accuracy recommender Pop as ARec, are competitive with algorithms like  PSVD100 or CofiR100.


%END
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


