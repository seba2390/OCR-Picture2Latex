\section{Long-tail novelty preference}
\label{sec:lt-pref}
We begin this section by  introducing our notation. We then describe various models  for measuring user long-tail novelty preference (Section~\ref{sec:simple-lt-pref}).% and propose a new  optimization framework to learn these preferences (Section~\ref{sec:learning-lt-pref}). %user long-tail novelty
\iffullpaper
\begin{figure*}[t]
\centering
        \subfloat[ML-100K]
        {					 
		\includegraphics[scale=0.27]{./Figures/general/popoverthetaTNR.pdf}
   		\label{fig:ML10m-pop-vs-number-ratings}
        }
        \subfloat[ML-1M]
        {
		\includegraphics[scale=0.27]{./Figures-ml-1m/general/popoverthetaTNR.pdf}
    		\label{fig:ML1m-pop-vs-number-ratings}
        }
        \subfloat[ML-10M]
        {					 
		\includegraphics[scale=0.27]{./Figures-ml-10m/general/popoverthetaTNR.pdf}
   		\label{fig:ML10m-pop-vs-number-ratings}
        }
         \subfloat[MT-200K]
        {					 
		\includegraphics[scale=0.27]{Figures-mt-200k-m5-mapped/general/popoverthetaTNR.pdf}
   		\label{fig:MT-pop-vs-number-ratings}
        }
        \subfloat[Netflix]
        {					 
		\includegraphics[scale=0.27]{FiguresSum/CombinedHistograms/popoverthetaTNR.pdf}
   		\label{fig:MT-pop-vs-number-ratings}
        }
 		        
\caption{For each user $u$, we consider the set of items rated by $u$ in train $\itemsofUserinTrainset$, and compute its average popularity $ \bar{a} =\frac{1 }{|\itemsofUserinTrainset|} \sum_{i \in \itemsofUserinTrainset} f_i^{\trainset}$. The x-axis shows the binned normalized $|\itemsofUserinTrainset|$, while the y-axis plots the mean of the corresponding $\bar{a}$ values. The average popularity of items rated decreases as the number of items rated increases. }
\label{fig:pop-vs-no-items}
\end{figure*}

\else % if you just want to figures in the paper, use this section.
\begin{figure}[t]
\centering
         \subfloat[ML-1M]
        {
		\includegraphics[scale=0.33]{./Figures-ml-1m/general/popoverthetaTNR.pdf}
    		\label{fig:ML1m-pop-vs-number-ratings}
        } 
        \subfloat[Netflix]
        {					 
		\includegraphics[scale=0.33]{FiguresSum/CombinedHistograms/popoverthetaTNR.pdf}
   		\label{fig:MT-pop-vs-number-ratings}
   		}
\caption{For each user $u$, we consider  $\itemsofUserinTrainset$, the set of items rated by $u$ in train, and compute its average popularity $ \bar{a} =\frac{1 }{|\itemsofUserinTrainset|} \sum_{i \in \itemsofUserinTrainset} f_i^{\trainset}$. The x-axis shows the binned normalized $|\itemsofUserinTrainset|$, while the y-axis plots the mean of the corresponding $\bar{a}$ values. The average popularity of items rated decreases as the number of items rated increases. }
\label{fig:pop-vs-no-items}
\end{figure}
\fi
\input{DataModel}

\subsection{Simple long-tail novelty preference models}
\label{sec:simple-lt-pref}
 % Measures the an individual's desire to own exclusive goods.
Users have different preferences for discovering long-tail items.  Given the train set $\trainset$, we need  a measure of the user's willingness to explore less popular items.  Let  $\theta_u^m$ denote user $u$'s preference for long-tail novelty as measured by  model $m$.

Figure~\ref{fig:pop-vs-no-items} plots the average popularity of rated items vs.~the number of rated items (or \emph{user activity})  for 
\iffullpaper
our datasets. 
\else
ML-1M and Netflix. 
\fi
 As user activity increases, the average popularity of rated items decreases. This motivates an \textit{Activity} measure $\simpleRisk = \vert \itemsofUserinTrainset \vert$. But,  most users  only rate  a few items, and $\simpleRisk$ does not indicate whether those items were long-tail  or popular.  %it is possible that two users with equal activity focus on different types of items: one rates popular items, while the other rates long-tail items. Furthermore, most users rate only a few items (small $\simpleRisk$). % and $\simpleRisk$ does not indicate whether the user rated popular items, or long-tail ones.% (confirmed empirically in Figure~\ref{fig:Hist-LT-PreferenceModels}).

%Naturally, we expect consumers who rate more items to be more willing to explore unpopular items. We can define a \textit{Simple} measure using $\simpleRisk = \vert \itemsofUserinTrainset \vert$. However, most users rate only a few items and this measure does not distinguish between type of items they rated. 

Instead, we can define a \textit{Normalized Long-tail} measure
\begin{align}
\small\LTRisk = \frac{  \vert \itemsofUserinTrainset  \cap \LT \vert}{\vert \itemsofUserinTrainset \vert}
\label{eq:nlt-risk}
\end{align} 
 % $\LTRisk = \frac{  \vert \itemsofUserinTrainset  \cap \LT \vert}{\vert \itemsofUserinTrainset \vert}$.   
which is the fraction of long-tail items in the user's  rated items.  The higher this fraction, the higher her  preference for long-tail items. However, $\LTRisk$ does not capture the user's interest (e.g,~rating) in the items, and does not distinguish between the various long-tail items.

%In our case, documents correspond to users and the words in each document correspond to the items. The frequency of a word in a document ($tf$) is the rating given to an item by a user ($r_{u,i}$). The $idf$ term is $\frac{n}{f_{i}}$,


To resolve both problems, we can use similar notions as in TFIDF~\cite{salton1988term}.  The rating an item receives from a particular user reflects its importance for that user. To capture the discriminative power of the item among the set of users and control for the fact that some items are generally more popular, we also incorporate an inverse popularity factor  ($|\usersofIteminTrainset|$) that is logarithmically scaled. We define \textit{TFIDF-based} measure using 
\begin{align}
\small
\tfidfRisk = \frac{ 1}{\vert \itemsofUserinTrainset \vert} \sum_{i \in \itemsofUserinTrainset} r_{ui} \log \left( \frac{|\mathcal{U}|}{ |\usersofIteminTrainset|} \right)
\label{eq:tfidf-risk}
\end{align}
This measure increases proportionally to the user rating $r_{ui}$, but is also counterbalanced by the popularity of the item. A higher  $\tfidfRisk$ shows more preference for less popular items. 
Although $\tfidfRisk$ considers both  the user interest $r_{ui}$, and the  item popularity $|\usersofIteminTrainset|$, it has no indication about the preferences of the users in $\usersofIteminTrainset$. 
%The problem is that, two  items $i_1$ and $i_2$, with equal popularity $|\mathcal{U}_{i_1}^{\trainset}| = |\mathcal{U}_{i_2}^{\trainset}|$, can have user sets with  entirely different long-tail preferences.  
To address this limitation, observe Eq.~\ref{eq:tfidf-risk} can be re-written  as
\begin{align}
\small
\tfidfRisk = \frac{ 1}{\vert \itemsofUserinTrainset \vert} \sum_{i \in \itemsofUserinTrainset} \theta_{ui}  = \frac{ \sum_{i \in \mathcal{I}_u} w_i \theta_{ui}}{ \sum_{i \in \mathcal{I}_u} w_i }
\label{eq:tfidf-risk-rewrite}
\end{align}
where $\theta_{ui} = r_{ui} \log \left( \frac{|\mathcal{U}|}{|\usersofIteminTrainset|} \right)$ is a \textit{per-user-item} long-tail preference value, and  $w_{i} = 1$ for all items.  Basically,  $\tfidfRisk$ gives equal importance to all items and is a crude average of $\theta_{ui}$.

%and .

%\input{RunningExample}

We can generalize the idea in Eq.~\ref{eq:tfidf-risk-rewrite}. Specifically, for each user, we consider a  \textit{generalized long-tail novelty} preference estimate, $\theta_u^G$.  We assume $\theta_u^G$ is a  weighted average of $\theta_{ui}$.  However, rather than imposing equal weights, we define  $w_i$ to  indicate an \textit{item importance} weight. Our second assumption is that an item is important when its users do not regard it as a mediocre  item; when their preference for that item deviates  from their generalized preference.  In other words,  an item $i$ is important when $\sum_{u \in \usersofIteminTrainset} (\theta_{ui} - \theta_u^G)^2$ is large.   Since $w_i$ and $\theta^G_u$ influence each other, below we  describe how to learn these variables in a joint optimization objective. 


%Before that, we note two important observations: 1.~Our approach is inspired by truth discovery approaches~\cite{},  where the goal is infer trustworthy information. In that setting, sources provide claims (similar to $\theta_{ui}$), each claim has a true value, and sources are assinged reliability scores. The main principle however, is that sources that provide true information more often will be assigned higher reliabiity degrees, and the information provided by reliable sources is regarded as truths.   idea of allowing weights $w_i$, per-user-item preferences $\theta_{ui}$, and a generalized preference for each user $\theta_u^{*}$ is  

%It does not consider  the long-tail preferences of other of users $i$. i.e.,~$\usersofIteminTrainset$.
%\footnote{ We introduce  the item \textit{long-tail importance weight} $w_i$, to distinguish between items based on the long-tail peference of their user groups. Moreover,  we  need to learn a \textit{generalized} long-tail preference estimate,  $\theta_u^*$, for each user, that is capable of differentiating between users based on the types of items they rate. The estimates  $w_i$ and $\theta^G_u$ are dependent and should be learned in a joint optimization objective. }

%In order to distinguish between items based on the long tail preference of their users, we introduce the notion of  item long-tail importance, $w_i$.

\subsection{Learning generalized long-tail novelty preference}
\label{sec:learning-lt-pref}
\iffalse
\begin{align}
\small
\epsilon = \sum_{i \in  \itemsinTrainset}  \sum_{u \in \usersofIteminTrainset}   w_i \epsilon_{ui}^2  =  \sum_{i \in  \itemsinTrainset} \sum_{u \in \usersofIteminTrainset} w_i \big( \theta_{ui} -  \theta_{u}^G \big)^2
\label{eq:totalSquaredError}
\end{align}
\fi
We define  $\epsilon_{i} = \big[ \sum_{u \in \usersofIteminTrainset}  1- \big( \theta_{ui} -\theta_u^G\big)^2 \big] $ as  the \textit{item mediocrity coefficient}. Assuming $|\theta_{ui} - \theta_u^G| \leq 1$ (explained later), the maximum  of $\epsilon_i$ is obtained  when $\theta_{ui} = \theta_u^G$.  We formulate our objective as:
\begin{align}
\ O(\mathbf{w},\bm{\theta}^G)= \sum_{i \in  \itemsinTrainset } w_i \big[ \sum_{u \in \usersofIteminTrainset}  1- \big( \theta_{ui} -\theta_u^G\big)^2 \big] =  \sum_{i \in  \itemsinTrainset } w_i \epsilon_i \nonumber
\end{align}
which is the total weighted mediocrity. Here, $ \itemsinTrainset$ are the items in train, $\usersofIteminTrainset$  denotes users that rated item $i$ in the train set (Section~\ref{sec:Notation}), and $\theta_{ui}$  is the per-user-item preference value,  computed from rating data.   Our objective  has two unknown variables: $\bm{\theta}^G \in \mathbb{R}^{|\mathcal{U}|}$, $\mathbf{w} \in \mathbb{R}^{|\mathcal{I}|}$.
We use an alternating optimization approach for optimizing   $\mathbf{w}$ and  $\bm{\theta}^G$. When optimizing w.r.t. $\mathbf{w}$,  we must minimize the objective function in accordance with our intuition about $w_i$. In particular, for larger mediocrity coefficient, we need smaller weights. On the other hand, when optimizing w.r.t. $\bm{\theta}^G$, we need to increase the closeness between $\theta_u^G$ and all $\theta_{ui}$'s, which is aligned with our intuition about $\theta_u^G$. So, we have to maximize the objective function w.r.t.  $\bm{\theta}^G$.  Our final objective is a minimax problem:  
\begin{align}
&\min_{\mathbf{w}} \max_{\bm{\theta}^G} &  O(\mathbf{w},\bm{\theta}^G)  - \lambda_1 \sum_{i \in \itemsinTrainset } \log{w_i}   
\label{eq:thetaStarOpt}
\end{align}
\iffalse
\begin{align}
\min_{\mathbf{w}} \max_{\bm{\theta}^G}\ O(\mathbf{w},\bm{\theta}^G)= \sum_{i \in  \itemsinTrainset } w_i \big[ \sum_{u \in \usersofIteminTrainset}  - \big( \theta_{ui} -\theta_u^G\big)^2 \big] 
\label{eq:thetaStarOpt}
\end{align}
\fi
where we have added a regularizor term $\sum_{i \in \itemsinTrainset } \log{ w_i}$ to  prevent $w_i$ from approaching 0~\cite{li2015discovery}. % To solve  Eq.~\ref{eq:thetaStarOpt} we use alternating optimization methods~\cite{koren2009matrix}, which iterate between fixing  one set of values and solving for the other until convergence. 

When $\bm{\theta}^G$ is fixed, we need to solve a minimization problem involving only $\mathbf{w}$. By taking the derivative w.r.t. $w_i$ we have  
\begin{align}
w_i = \frac{\lambda_1}{ \sum_{u \in \usersofIteminTrainset} 1 - \big(\theta_{ui} -\theta_u^G\big)^2  } = \frac{\lambda_1}{\epsilon_i}
\label{eq:weightUpdate}
\end{align}
When $\mathbf{w}$ is fixed, we need to solve a maximization problem involving $\bm{\theta}^G$. Taking the derivative w.r.t. $\theta_u^G$ we derive
\begin{align}
\theta_u^G =\frac{\sum_{i \in \itemsofUserinTrainset} w_i \theta_{ui}}{\sum_{i \in \itemsofUserinTrainset} w_i} 
\label{eq:overallRiskUpdate}
\end{align}

Essentially, Eq.~\ref{eq:weightUpdate} characterizes an item's weight,  based on the item mediocrity. The higher the mediocrity, the lower the weight.  Moreover, for every user $u$, $\theta_u^G$ is a weighted average of all $\theta_{ui}$. Note, in Eq.~\ref{eq:overallRiskUpdate}, $\theta_u^G=\tfidfRisk$ when $w_i =1$ for all items.  Our generalized $\theta_u^G$ incorporates both the user interest and popularity of items (via $\theta_{ui}$), and  the preferences of other users of the item (via $w_i$). Furthermore, since we need $|\theta_{ui} - \theta_u^G| \leq 1$, and prefer  $\theta_u^G \in [0,1]$, we project all $\theta_{ui}$ to the $  [0,1]$ interval before applying the algorithm. We also set $\lambda_1=1$.

%------------------------------------------
\iffalse ICDE Submission
\subsection{Learning generalized novelty preference}
\label{sec:learning-lt-pref}
We want to learn a  generalized long-tail preference estimate $\theta_u^G$ for each user such that $\sum_{u \in \mathcal{U}} \sum_{i \in \itemsofUserinTrainset} w_i \theta_{ui} \approx \sum_{u \in \mathcal{U}} \sum_{i \in \itemsofUserinTrainset} w_i \theta_u^G $ holds for all users.  We define the \textit{per-user-item error}  as $\epsilon_{ui} = w_i ( \theta_{ui} - \theta_u^G)$. Our objective is to minimize the total squared error

\begin{align}
\small
\epsilon = \sum_{i \in  \itemsinTrainset} \sum_{u \in \usersofIteminTrainset}  \epsilon_{ui}^2  =  \sum_{i \in  \itemsinTrainset} \sum_{u \in \usersofIteminTrainset} w_i^2 \big( \theta_{ui} -  \theta_{u}^G \big)^2
\label{eq:totalSquaredError}
\end{align}
where  $ \itemsinTrainset$ are the items in train, $\usersofIteminTrainset$  denotes users that rated item $i$ in the train set (Section~\ref{sec:Notation}), and $\theta_{ui}$ is computed from rating data. The objective has two unknown variables: $\bm{\theta}^G \in \mathbb{R}^{|\mathcal{U}|}$, $\mathbf{w} \in \mathbb{R}^{|\mathcal{I}|}$.  Since $\mathbf{w}=0$ is a trivial solution for Eq.~\ref{eq:totalSquaredError}, we impose a relaxed constraint $\sum_{i \in \itemsinTrainset } w_i=1$ which requires at least one  non-zero weight. Note, our objective is equivalent to a minmax problem as follows:
\begin{align}
\min_{\mathbf{w}} \max_{\bm{\theta}^G}\ O(\mathbf{w},\bm{\theta}^G)= \sum_{i \in  \itemsinTrainset } w_i^2 \big[ \sum_{u \in \usersofIteminTrainset}  - \big( \theta_{ui} -\theta_u^G\big)^2 \big] 
\label{eq:thetaStarOpt}
\end{align}

Furthermore, we introduce an $L_2$-regularizor for $\mathbf{w}$ to prevent over-fitting, e.g., it prevents large weights when $\big( \theta_{ui} -\theta_u^G\big)^2$ is close to zero and  brings numerical stability for the computations. 
We also add another regularizor that is proportionate to the frequency of the item, i.e., when the item  is popular, the weight should be small.   Our final objective is: 

\begin{align}
&\min_{\mathbf{w}} \max_{\bm{\theta}^G} &  O(\mathbf{w},\bm{\theta}^G)  + \sum_{i \in \itemsinTrainset } w_i ^2 + \sum_{i \in \itemsinTrainset }\sum_{u \in \usersofIteminTrainset} w_i^2 & \nonumber \\ 
&s.t. &\sum_{i \in \itemsinTrainset } w_i=1 &
\label{eq:thetaStarOpt}
\end{align}

To solve  Eq.~\ref{eq:thetaStarOpt} we use alternating optimization methods~\cite{koren2009matrix}, which iterates between fixing  one set of values and solving for the other until convergence. Specifically, when $\theta^G$ is fixed, we need to solve a minimization problem involving only $\mathbf{w}$   with the constraint $\sum_{i \in \itemsinTrainset } w_i=1$. We use the method of lagrange multipliers ($\lambda$). By taking the derivative w.r.t. $w_i$ we have  
\begin{align}
w_i = \frac{\lambda}{ 2 \Big( \sum_{u \in \usersofIteminTrainset} 1 - \big(\theta_{ui} -\theta_u^G\big)^2 \Big)  + 2 }
\label{eq:weightUpdate}
\end{align}
and from the constraint   we derive 
\begin{align}
\lambda = \bigg( \sum_{i \in \itemsinTrainset } \Big(  2 \Big( \sum_{u \in \usersofIteminTrainset} 1 - \big( \theta_{ui} -\theta_u^G \big)^2 \Big) + 2 \Big)^{-1}\bigg)^{-1}
\end{align}
When $\mathbf{w}$ is fixed, we need to solve a maximization problem involving $\bm{\theta}^G$. Taking the derivative w.r.t. $\theta_u^G$ we derive
\begin{align}
\theta_u^G =\frac{\sum_{i \in \itemsofUserinTrainset} w_i^2 \theta_{ui}}{\sum_{i \in \itemsofUserinTrainset} w_i^2} 
\label{eq:overallRiskUpdate}
\end{align}

Regarding Eq.~\ref{eq:weightUpdate}, $( \theta_{ui} -\theta_u^G)^2$  measures the deviation  of user $u$ from her generalized long-tail preference, for item $i$.  Essentially, Eq.~\ref{eq:weightUpdate} characterizes an item's weight,  based on the deviation of all the users who rated that item from their generalized long-tail preference,  i.e,~$\sum_{u \in \usersofIteminTrainset} ( \theta_{ui} -\theta_u^G)^2$.  An item  has a higher weight when the users who rated  that item deviate a lot from their generalized preference.  Moreover, in Eq.~\ref{eq:overallRiskUpdate}, $\theta_u^G=\tfidfRisk$ when $w_i =1$ for all items.  Since we prefer $\theta_u^G \in [0,1]$ we project all $\theta_{ui}$ to the $  [0,1]$ interval before applying the algorithm.


\fi

%---------------------------------------------------Some math that has not been included
\iffalse
\textcolor{red}{ One way to characterize an item's riskiness level is to measure the deviation of all the users who rated that item from their aggregate risk degree. Note, $\|\theta_{ui} -\theta_u^G\|^2$  measures the deviation of user $u$ from her aggregate risk for item $i$.    To capture this intuition,  we introduce the combined error as: 
\begin{align}
\small
\epsilon = \sum_{i \in \mathcal{I}} \sum_{u \in \usersofIteminTrainset} w_i \theta_{ui} - \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i} w_i \theta_{u}^G 
\end{align}
 Our objective is to minimize  the square of the error term, i.e.,  $\min \| \epsilon \|^2$. 
}
\fi
\iffalse
in the following manner: 
\begin{align}
\small 
\min \  \| \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i} w_i \theta_{ui} - \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i} w_i \theta_{u}^G   \|^2
\label{eq:minWeightedDistance}
\end{align}

Furthermore, to simplify the objective function, we introduce an upper bound. It can be shown that for a constant $C$, 
we have:
\begin{align}
\small 
\| \epsilon \|^2 &= \|  \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i} w_i \theta_{ui} - \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i} w_i \theta_{u}^G  \|^2 \label{eq:minWeightedDistance}\\ &=
\| \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i}w_i (\theta_{ui}-\theta_{u}^G) \|^2 \\&\leq
C \sum_{i \in \mathcal{I}} \sum_{u \in \mathcal{U}_i} w_i^2\| (\theta_{ui}-\theta_{u}^G)\|^2\label{eq:upperBound}
\end{align}

Rather than minimize Eq.~\ref{eq:minWeightedDistance}, we minimize upper bound Eq.~\ref{eq:upperBound}. 
\fi
\iffalse
 we propose the following objective function, 

\begin{align}
&\min_{\mathbf{w}} \max_{\bm{\theta}^G} & \alpha \sum_{i \in  \itemsinTrainset } w_i^2 \sum_{u \in \usersofIteminTrainset} C - \|\theta_{ui} -\theta_u^G\|^2 + \sum_{i \in \itemsinTrainset }\|w_i\|^2 & \nonumber \\ 
&s.t. &\sum_{i \in \itemsinTrainset } w_i=1 &
\label{eq:thetaStarOpt}
\end{align}

where  $\theta^G \in \mathbb{R}^{|\mathcal{U}|}$ is the aggregate user risk degrees,  and  $\mathbf{w} \in \mathbb{R}^{|\mathcal{I}|}$  is the factor-per-item weights. Here, $C$ is a constant that is chosen to ensure  $ C - \|\theta_{ui} -\theta_u^G\|^2 \geq 0$.  %  is greater than the maximum of $\theta_{ui}$ 
The first term captures the main intuition: when the value of $\sum_{u \in \usersofIteminTrainset} \|\theta_{ui} -\theta_u^G\|^2$  is large, $ C - \|\theta_{ui} -\theta_u^G\|^2$ is small,   larger weights are assigned to $w_i$ to minimize the loss function, and vice versa.   
We use the square of the $w_i$ to  bound the minimization problem. %prevent negative values for $w_i$ and
The second term is a regularization constraint to prevent over-fitting, e.g., it prevents large weights when $\|\theta_{ui} -\theta_u^G\|^2$ is close to C, and brings numerical stability for the computations. 
The constraint $\sum_{i \in \itemsinTrainset } w_i=1$  prevents the trivial solution $\mathbf{w}=0$ and imposes the constraint at least one weight is not zero. $\alpha$ is a trade-off parameter.  
\fi
%---------------------------------------------------
%---------------------------------------------------
\iffalse
\subsection{Dirichlet-based Risk} 
Recall, long tail items are denoted by $\mathcal{I}^{\mathcal{L}} \subset \mathcal{I}$. Assume the ratings $\mathcal{R}^{\mathcal{I}}$ are a set of $|\mathcal{R}|$ draws from a multinomial random variable  $R$. The ratings come from a vocabulary of size $|\mathcal{U}|$. The parameter vector of the vocabulary $\pmb{\theta}$ can be modelled with a Dirichlet distribution, $\pmb{\theta} \sim \text{Dir} (\pmb{ \theta|\alpha})$. The posterior of the parameter given the observations $\mathcal{R}$ can be obtained as follows:

\begin{align*}
p(\theta| \mathcal{R}^L, \alpha) &= \prod_{n=1}^{N=|\mathcal{R}^L|} \frac{p(r_n|\theta) p(\theta|\alpha)}{\int_{n=1}^{N} p(r_n|\theta) p(\theta|\alpha) d\theta } \\
&=\frac{1}{Z} \prod_{u=1}^{|\mathcal{U}|}  \theta_u^{n(u)} \frac{1}{\Delta(\alpha)} \theta_u^{\alpha_u-1}\\
&=\text{Dir}(\theta|\alpha + n)
\end{align*}

We use  the  expectation of the Dirichlet distribution, i.e., $E[\theta_u] = \frac{\alpha_u + n(u)}{\sum_{u \in \mathcal{U}} \alpha_u + n(u)}$ as an indicator of the user $u$'s risk degree. Note, for each user this translates to the number of long tail items  rated normalized by the the total number of long tail ratings in the train data.

\textcolor{red}{ \textbf{Heuristic Risk. }To discriminate among users who have rated the same number of long-tail items,  we can consider popularity of items and the values of user ratings. The rating an item receives from a particular user reflects its importance for that user. However, to capture the discriminative power of the item among the set of users and control for the fact that some items are generally more popular, we also incorporate an inverse popularity factor ($|\usersofIteminTrainset|$)   % Users that rate less popular items higher (higher values of $r_{u,i}$ for lower values of $f_i$) can be inferred as risk loving users. 
% We define {\bf heuristic risk} by% as in Equation~\ref{eq:heuristic-risk}. 
\begin{equation}
\small
\heuristicRisk = \frac{ 1}{\vert \itemsofUserinTrainset \vert} \sum_{i \in \itemsofUserinTrainset} \frac{r_{ui}}{|\usersofIteminTrainset|} 
\label{eq:heuristic-risk}
\end{equation}
where the users train ratings are normalized,~i.e., $r_{ui} \in [0,1]$ This measure increases proportionally to the rating a user has given an item, but is also counterbalanced by the popularity of the item. A higher value for $\heuristicRisk$ corresponds to a higher preference for less popular items.
}
\fi

%\subsubsection{Skew-based risk}
%Alternatively, we can measure the consumers risk degree using the \emph{skew} measure defined with regard to the rating vs. item popularity distribution. In this approach, first the items are binned according to their rating frequency. The niche items that have been rated fewer times, are on the left of this spectrum while the popular items that have been rated more frequently, are on the right of the spectrum. For each user, we  coompute the sum of the user's ratings for items in each bin. We then calculate the skew of the resulting distribution. A  left-skewed distribution corresponds to a distribution where the mass of the user's ratings are on the right of the spectrum, the more popular items. Hence a risk-averse user has a left-skewed distribution. Similarly, a risk-loving user has a right-skewed distribution where the mass of the user's ratings are over the niche items. We used scipy.stat  to compute skew
%
%\begin{equation}
%\theta_u^s = \frac{ 1}{\vert \mathcal{I}_{u} \vert} \sum_{i \in \mathcal{I}_{u}} \frac{r_{ui}}{f_{i}+1} 
%\label{eq:skew-based-risk}
%\end{equation}

%\subsubsection*{Entropy-based risk}
%Alternatively, we can measure the users tendency toward popular items using \emph{entropy}. In this approach, for each item rated by the user, $i \in \mathbf{p}_u$, we consider the number of good ratings (4 or 5) the item has received across all users.  We then  estimate an item-popularity histogram, by binning the items with regard to the frequency of good ratings, with the popular items on one end of the spectrum and the niche items on the other end. We define  $\theta_{u}$ as the entropy of this item-popularity histogram: 
%\begin{align}
%\theta_u = - \sum_{j = 1}^{B} n_{j} \log (n_{j})
%\end{align}
%where $n_{j}$ is the number of items that fall in bin $j$. 
%\textcolor{red}{A risk-loving user tends to have a uniform distribution.
%Using entropy , A risk-loving user will tend to have a more uniform distribution whereas a user with a distribution more skewed towards popular items in the item-popularity histogram is considered to be risk-averse.}

%\subsubsection*{Gini Coefficient risk}
%\subsection*{Variance over mean}

 % as well as the underlying base recommender system (focusing on predictive accuracy) used to collect our training data,%in the system used for acquiring our training data, $\mathbf{R}$. 


\iffalse
To resolve both problems, we can use similar notions as in TFIDF~\cite{salton1988term}, and define  a \textit{TFIDF-based} measure 
\begin{align}
\small
\tfidfRisk = \frac{ 1}{\vert \itemsofUserinTrainset \vert} \sum_{i \in \itemsofUserinTrainset} r_{ui} \log \big( \frac{|\mathcal{U}|}{ |\usersofIteminTrainset|} \big)
\label{eq:tfidf-risk}
\end{align}
 The rating an item receives from a particular user $r_{ui}$ reflects its importance for that user. To capture the discriminative power of the item among the set of users and control for the fact that some items are generally more popular, Eq.~\ref{eq:tfidf-risk} also incorporates an inverse popularity factor  ($|\usersofIteminTrainset|$) that is logarithmically scaled.% We define \textit{TFIDF-based} measure using 
 This measure increases proportionally to the rating a user has given an item, but is also counterbalanced by the popularity of the item. A higher  $\tfidfRisk$ shows more preference for less popular items. 
\fi