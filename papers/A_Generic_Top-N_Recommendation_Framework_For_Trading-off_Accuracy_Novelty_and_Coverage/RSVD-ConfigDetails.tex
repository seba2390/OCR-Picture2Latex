\subsection{Regularized SVD configuration}
\label{sec:configurationofR-SVD}
\begin{table}[ht]
\centering
\scriptsize
\begin{tabular}{lllllllll}
  \toprule
  &\multicolumn{4}{c}{R-SVD} & \multicolumn{4}{c}{R-SVDN} \\
 \cmidrule(r){2-5} \cmidrule(r){6-9} 
  {\bf Dataset}  &  $\eta$ & $\lambda$ & $g$ & RMSE  &  $\eta$ & $\lambda$ & $g$ & RMSE  \\ \midrule
  ML-100K &0.03 & 0.05  & 100 &  0.935   & 0.03 &0.05 & 100& 0.935\\
  
  ML-1M  & 0.03 & 0.05 & 100 &  0.868   & 0.03 & 0.05 & 100 & 0.875   \\
  
  ML-10M  &0.003 & 0.005 &  20 & 0.872   & 0.003 & 0.005& 20 &0.872    \\
  
  %MT-200k-m5 & 0.01 &  0.1 & 40 & 1.4786   &0.01 &0.1& 40 & 1.4785  \\
  
  MT-200k  & 0.01 &  0.01 & 40 & 0.761   &0.01 &0.01& 40 & 0.761 \\ 
  
  %MT-200k-m20 &  0.01 & 0.01 & 8 & 1.521  & 0.01 & 0.01 & 8 &1.5209\\
  
  %MT-200k-m20-mapped &  0.01 & 0.01 & 8 & 0.7822  & 0.01 & 0.01 & 8 & 0.7822 \\
  
  Netflix & 0.002 & 0.05 &  100 & 0.979  & 0.002 & 0.05 & 100 & 0.979 \\
   \bottomrule
\end{tabular}
\caption{R-SVD and R-SVDN  parameters on different datasets. $g$ is the number of latent factors, $\eta$ is the learning rate, $\lambda$ is the L2-reqularization coefficient.}
\label{tab:DatasetStatsticsFull}
\end{table}

\iffalse
\begin{table*}[t]
\centering
\scriptsize
\begin{tabular}{llllllllllllllll}
  \toprule
  &&&&&&&& \multicolumn{4}{c}{R-SVD} & \multicolumn{4}{c}{R-SVDN} \\
 \cmidrule(r){9-12} \cmidrule(r){13-16} 
  {\bf Dataset} & {\bf $|\mathcal{D}|$} & {\bf $|\mathcal{U}|$} & {\bf $|\mathcal{I}|$}  & d$\%$ & $\mathcal{L} \%$ & {\bf $\kappa$} & {\bf $\tau$ } &  $\eta$ & $\lambda$ & $g$ & RMSE  &  $\eta$ & $\lambda$ & $g$ & RMSE  \\ \midrule
  MovieLens  & 100K & 943& 1682  & 6.30\% &  66.98 \%  &0.5 & 20 & 0.03 & 0.05  & 100 &  0.9347   & 0.03 &0.05 & 100& 0.9347\\
  
  MovieLens   & 1M & 6,040& 3,706 & 4.47\% & 67.58 \% &  0.5 & 20 & 0.03 & 0.05 & 100 &  0.8684   & 0.03 & 0.05 & 100 & 0.875   \\
  
  MovieLens & 10M & 69,878 & 10,677 & 1.34\%& 84.31 \% & 0.5 &  20& 0.003 & 0.005 &  20 & 0.8719  & 0.003 & 0.005& 20 &0.8719    \\
  
  MT-200k-m5  & 172,506 & 7,969 & 13,864 & 0.16\%& 86.84 \%   & 0.8 & 5 & 0.01 &  0.1 & 40 & 1.4786   &0.01 &0.1& 40 & 1.4785  \\
  
  MT-200k-m5-mapped  & 172,506 & 7,969 & 13,864 & 0.16\%& 86.84 \%   & 0.8 & 5 & 0.01 &  0.01 & 40 & 0.761   &0.01 &0.01& 40 & 0.761 \\ 
  
  MT-200k-m20 & 96,788 & 1,564 &  11,729 & 0.53\% &  77.61\% & 0.5 & 30 & 0.01 & 0.01 & 8 & 1.521  & 0.01 & 0.01 & 8 &1.5209\\
  
  MT-200k-m20-mapped & 96,788 & 1,564 &  11,729 & 0.53\% &  77.61\% & 0.5 & 30 & 0.01 & 0.01 & 8 & 0.7822  & 0.01 & 0.01 & 8 & 0.7822 \\
  
  Netflix & 98,754,394 &  459,497 & 17,770  &1.21\% & 88.27 \% & - & -& 0.002 & 0.05 &  100 & 0.9788  & 0.002 & 0.05 & 100 & 0.9789 \\
   \bottomrule
\end{tabular}
\caption{Datasets description and parameters. $\kappa$ is the train-test split ratio, $\tau$ is the minimum \# ratings per user in the dataset, $\mathcal{D}$. Density is  $\text{d}\%= ( |\mathcal{D}| / |\mathcal{U}| * |\mathcal{I}|) \times 100\%$.   Long-tail percentage is  $\mathcal{L}\% = (|\LT| / |\itemsinTrainset|) \times 100\%$. For R-SVD and P-SVD, $g$ is the number of latent factors, $\eta$ is the learning rate, $\lambda (\lambda =\lambda_Q=\lambda_P)$ is the L2-regularization coefficients. For P-SVD,  $g^{'}$ the number of latent factors. MT-200k-m20 to  MT-200k-m30 }
\label{tab:DatasetStatsticsFull}
\end{table*}
\fi
%\iffullpaper
%\vspace{4mm}
%\noindent \textbf{Baseline Algorithms Configuration. }  
%Table~\ref{tab:DatasetStatsticsFull}, in the Appendix,  provides the details for algorithm parameters in different datasets. We used \texttt{LIBMF}~\cite{zhuang2013fast} with  $L_{2}$-Norm as the loss function, and $L_{2}$ regularization,  and Stochastic Gradient Descent (SGD) for Matrix factorization.  For each dataset, we performed 5-fold cross validation to find the best set of parameters in terms of RMSE. We experimented with the parameters $g \in \{10, 40, 80, 100, 300\}, \lambda_Q \in \{0.01, 0.05, 0.1\}, \eta \in \{0.002,0.003,0.01, 0.03\}$. We report the performance of the best parameters and the selected models in  Table~\ref{tab:DatasetStatsticsFull}. %For the ML-10M and Netflix we used the same parameters setting as in~\cite{zhuang2013fast}, i.e., number of latent factor $g=40$, $L_{2}$ regularization coefficients $\lambda_Q=\lambda_P=0.05$.  For Netflix we set the learning rate $\eta=0.002$ and obtained test RMSE $= 1.0003$. For ML-10m we set $\eta=0.003$ and obtained test RMSE $= 0.8794$. For MT-200K we used $\lambda_Q=\lambda_P = 0.1,  g= 40, \eta=0.01$ with test RMSE $= 1.4687$. For CofiRank, we experimented with both regression (squared) loss (Cofi$^R$) and NDCG loss (Cofi$^N$).  We used the parameter values  provided in the paper~\cite{weimer2007maximum},  100 dimensions and $\lambda =10$, and default values for other parameters (source code). Similar to \cite{balakrishnan2012collaborative,volkovs2012collaborative}, we found Cofi$^R$ to perform consistently better than Cofi$^N$ in our experiments on ML-1M and MT-200K. We only report results for Cofi$^R$.   For the Resource allocation re-ranking~\cite{ho2014likes} method, we implemented and ran all four variants with default values set according to~\cite{ho2014likes}. We set $k= 5 * \vert \mathcal{I} \vert $.  For PureSVD we use Python's \texttt{sparsesvd} module.

Table~\ref{tab:DatasetStatsticsFull} provides details for the setup of Regularized SVD (R-SVD) and the same model with non-negative constraints (R-SVDN). We use \texttt{LIBMF}~\cite{zhuang2013fast} with  L2-Norm as the loss function, and L2-regularization,  and SGD for optimization. We performed 10-fold cross validation and tested:  number of latent factors  $g \in \{8,  20, 40, 50, 80, 100\}$, L2-regularization coefficients  $\lambda \in \{0.001, 0.005, 0.01, 0.05, 0.1\}$,  learning rate $\eta \in \{0.002,0.003,0.01, 0.03\}$. For each dataset, we used the parameters that led to best performance.

\iffalse
\subsection{Other baselines configuration}
 provides the details for algorithm parameters in different datasets. We conducted  experiments over the parameter space of the various algorithms and report the  parameters that led to the best  performance in  Table~\ref{tab:DatasetStatsticsFull}.
 For CofiRank, we experimented with both regression (squared) loss (Cofi$^R$) and NDCG loss (Cofi$^N$).  We use the parameter values  provided in the paper~\cite{weimer2007maximum},  100 dimensions and $\lambda =10$, and default values for other parameters (source code). Similar to \cite{balakrishnan2012collaborative,volkovs2012collaborative}, we found Cofi$^R$ to perform consistently better than Cofi$^N$ in our experiments on ML-1M and MT-200K. We only report results for Cofi$^R$.   For the Resource allocation re-ranking~\cite{ho2014likes} method, we implemented and ran all four variants with default values set according to~\cite{ho2014likes} and MF parameters set according to Table~\ref{tab:DatasetStatsticsFull}. For PureSVD we use Python's \texttt{sparsesvd} module and tested $g^{'} \in \{10, 20, 40, 60, 100, 150, 200, 300\}$.
\fi