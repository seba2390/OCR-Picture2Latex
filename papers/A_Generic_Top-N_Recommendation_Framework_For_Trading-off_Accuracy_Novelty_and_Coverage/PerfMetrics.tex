\begin{table}[t]
\centering
\small
\begin{tabular}{ll}
  \toprule
  %{\bf Measure} & Formula \\ \midrule
\multirow{5}{*}{\parbox{0.07\textwidth}{Local \\ Ranking \\ Accuracy \\ Metrics}}   
   %& $\predictionFormula$ \\ [0.2cm]
   &  $\precisionFormula$   \\[0.2cm]
   & $\recallFormula$  \\[0.2cm]
   & $\FmeasureFormula$ \\[0.2cm]
   %& \multirow{2} {*} {\parbox{0.3\textwidth} {$\DCGFormula$ \\ [0.1cm] $\nDCGFormula$}} \\[0.1cm]
   %& \\[0.1cm]
   \midrule  
   \multirow{4}{*}{\parbox{0.07\textwidth}{Longtail  \\ Promotion}} 
	%&$\LTPrecisionFormula$   \\[0.1cm]   
   & $\LTAccuracyFormula$ \\[0.2cm]
   & $\StratRecallFormula$ \\[0.2cm] % , $\StratWeightFormula$
	%&\\[0.1cm]   
   \midrule
  \multirow{3}{*}{\parbox{0.07\textwidth}{Coverage \\ Metrics}} 
  & $\coverageFormula$   \\[0.2cm]
  & $\giniFormula$ \\[0.2cm]
  %&$\LTCoverageFormula$ \\[0.1cm]  
  \iffalse
   \midrule 
   \multirow{3}{*}{\parbox{0.07\textwidth}{Weighted \\Harmonic \\ Mean}} 
   \\
    &$\WeightedHMFormula$ \\[0.1cm] 
    \\
   \fi 
  \bottomrule
  
\end{tabular}
\caption{Performance Metrics. %For accuracy,  we use local ranking metrics~\cite{agarwal_chen_2016}, where  each metric is computed per user and then averaged across all users.  
Notation is in Section~\ref{sec:Notation}. For gini, the vector 
$\mathbf{f}$ is  sorted in  non-decreasing order of recommendation frequency of items,~i.e., $f[j] \leq f[j+1]$.
}
\label{tab:perfMetrics}
\end{table}
\vspace{4mm}
%\label{sec:PerformanceMetrics}
%We focus on accuracy, coverage and novelty aspects of top-$\size$ recommendation. 

\noindent \textbf{Test ranking protocol and performance metrics. }For testing, we adopt the ``All unrated items test ranking protocol''~\cite{steck2013evaluation,vargas2014improving} where for each user, we generate the top-$\size$ set by  ranking all items that do not appear in the train set of that user \iffullpaper 
(details in Appendix~\ref{test-protocol-effect}).
\else 
(see~\cite{ourFullVersion} for details  and  experiments with different ranking protocol).
\fi %Note, this test protocol is a generalization of the one in~\cite{cremonesi2010performance}, and  
 
Table~\ref{tab:perfMetrics} summarizes the performance metrics. To measure how accurately an algorithm can rank items for each user,  we use local rank-based precision and recall~\cite{agarwal_chen_2016,vargas2014improving,steck2013evaluation}, where each metric is computed per user and then averaged across all users. Precision is the proportion of relevant test items in the top-$\size$ set, and recall is the proportion of relevant test items retrieved from among a user's relevant test items. As commonly done in the literature~\cite{agarwal_chen_2016,niemann2013new}, for each user $u$, we define her relevant test items as those that she rated highly,~i.e., $ \relevantItemsofUserinTestSet = \{i: i \in \itemsofUserinTestset ,  r_{ui} \geq 4\}$. Note,  because the collected datasets have many missing ratings, the hypothesis that only the observed test ratings are relevant,  underestimates the true precision and recall~\cite{steck2013evaluation}. But,  this  holds  for all algorithms, and the  measurements are known to reflect performance in real-world settings~\cite{steck2013evaluation}. F-measure is the harmonic mean of precision and recall.
\iffalse
For algorithms that re-rank rating-prediction models, we also report the prediction  metric (Prediction@$\size$) that is known to be correlated with precision~\cite{adomavicius2011maximizing}. It is the average predicted rating value of the top-$\size$ set, according to the  underlying rating-prediction model. It measures the deviation of any top-$\size$ from the default greedy strategy of a rating prediction model~\cite{adomavicius2011maximizing,ho2014likes}. %We normalized $\hat{r}_{ui}$ to $[0,1]$. 
\fi
%Furthermore, we use discounted cumulative gain (DCG)~\cite{voorhees2001overview}, which measures the accumulated  gain of items in the recommendation list while logarithmically discounting the gain of each item by its position. Note, $i$ ranges over positions $1$ to $\size$. We set $rel_{ui} = r_{ui}$ (the observed rating) when $i$ is in the test set ($i \in \itemsofUserinTestset $) and is recommendable  ($\{i: i \in \itemsinTrainset \setminus \itemsofUserinTrainset\}$), and zero otherwise. Normalized DCG (NDCG) is  obtained by normalizing by the ideal DCG measure.



%In our work, we consider the cold-start definition of novelty~\cite{Kaminskas:2016:DSN:3028254.2926720}. We therefore  focus exclusively on long-tail items and measure novelty using  Long-Tail Accuracy (LTAccuracy@$\size$)~\cite{ho2014likes},  the ratio of long-tail items in a top-$\size$ set.
We use Long-Tail Accuracy (LTAccuracy@$\size$)~\cite{ho2014likes} to measure the novelty of recommendation lists. It computes the proportion of the recommended items that are unlikely to be seen by the user.  Moreover, we use Stratified Recall (StratRecall@$\size$)~\cite{steck2013evaluation} which measures the ability of a model to compensate for the popularity bias of items w.r.t train set. Similar to~\cite{steck2013evaluation}, we set $\beta=0.5$. Note, LTAccuracy emphasizes a combination of novelty and coverage, while Stratified Recall emphasizes a combination of novelty and accuracy. 
%To evaluate long-tail item promotion, we use Long-Tail Accuracy (LTAccuracy@$\size$) and Long-Tail Coverage (LTCoverage@$\size$)~\cite{ho2014likes}. 

%Coverage  is a system-side metric  that indicates the overall number of distinct items  exposed to the entire users~\cite{vargas2014improving,ho2014likes}. 

For  coverage we use two metrics:  Coverage@$\size$  is the ratio of the total number of distinct  recommended  items to the total number of items~\cite{ho2014likes,vargas2014improving}. A maximum value of $1$ indicates  each item in $\mathcal{I}$ has been recommended at least once. 
%We also evaluate Long-Tail Coverage (LTCoverage@$\size$)~\cite{ho2014likes}.
%This measure is equivalent to normalized aggregate diversity@$\size$  proposed in ~\cite{adomavicius2012improving}. 
Gini~\cite{lorenz1905methods},  measures the inequality among values of a frequency distribution $\mathbf{f}$. It lies in $[0,1]$, with $0$ representing perfect equality, and larger  values representing  skewed distributions.  In Table~\ref{tab:perfMetrics},  $\mathbf{f}$ is the recommendation frequency of items,  and is sorted in non-decreasing order, i.e.,~$f[j] \leq f[j+1]$.% Let  $\mathbf{f}$ denote  the recommendation frequency of items. We sort the items in  non-decreasing order of  recommendation frequency, i.e., $\mathbf{f}_i \leq \mathbf{f}_{i+1}$.

\iffalse
To summarize performance of different algorithms w.r.t. multiple performance metrics,  we compute their weighted harmonic mean,  and rank the algorithms. Since different performance metrics can have different data ranges, we normalize  each metric to the range $[0,1]$.  In table~\ref{tab:perfMetrics},  $\mathbf{x}$ denotes the vector of performance measurements (e.g.$x_1$=precision, $x_2$=recall), and $\mathbf{w}$ their corresponding weights. Note, the harmonic mean of precision and recall is commonly known as F-measure. 
\fi