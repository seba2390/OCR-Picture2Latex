\newpage
\subsection{Effect of test ranking protocol on performance metrics}
\label{test-protocol-effect}

\begin{figure}[t]
\centering

\subfloat[All unrated items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures/crossValidation/sfLTAccuracyvsfmeasure_rankp_all_items.pdf}}

\subfloat[Rated test-items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures/crossValidation/sfLTAccuracyvsfmeasure_rankp_test_only.pdf}}

\subfloat[All unrated items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures/crossValidation/sfLTAccuracyvsprecision_rankp_all_items.pdf}}

\subfloat[Rated test-items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures/crossValidation/sfLTAccuracyvsprecision_rankp_test_only.pdf}}

\caption{Comparing the trade-offs  for top-5 recommendation, when using different ranking protocols. Dataset is ML-100K.}

\label{fig:CP-LC-tradeoffs-ml-100k}
\end{figure}



In our empirical study, we studied off-line recommendation performance of top-$\size$ recommendation algorithms from three perspectives: accuracy, novelty, and coverage (Section~\ref{sec:ExpSetup}). The  choice of \emph{test ranking protocol}~\cite{steck2013evaluation} is also an important aspect in off-line evaluations. The  test ranking protocol describes which items in the test set are ranked for each user~\cite{steck2013evaluation}. We use the definitions in~\cite{steck2013evaluation}: 
\begin{itemize}
\item \textbf{Rated test-items ranking protocol}: for each user, we only rank  the observed items in the test set of  that user. %with we refer to a ranking of items that  are observed in the test set of a user.
\item \textbf{All unrated items ranking protocol}: for each user, we rank all items that do not appear in the train set of that user.
%With \emph{all unrated items} ranking protocol we refer to a ranking  of all unrated test items, regardless of whether the user has provided feedback on them or not. 
\end{itemize}

\begin{figure}[t]
\centering

\subfloat[All unrated items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures-ml-1m/crossValidation/sfLTAccuracyvsfmeasure_rankp_all_items.pdf}}

\subfloat[Rated test-items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures-ml-1m/crossValidation/sfLTAccuracyvsfmeasure_rankp_test_only.pdf}}

\subfloat[All unrated items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures-ml-1m/crossValidation/sfLTAccuracyvsprecision_rankp_all_items.pdf}}

\subfloat[Rated test-items ranking protocol]{\includegraphics[width=0.4\textwidth]{Figures-ml-1m/crossValidation/sfLTAccuracyvsprecision_rankp_test_only.pdf}}

\caption{Comparing the trade-offs  for top-5 recommendation, when using different ranking protocols. Dataset is ML-1M.}

\label{fig:CP-LC-tradeoffs-ml-1m}
\end{figure}

%In other words, their measurements only consider  the subset of  items that have received feedback. Naturally
%We begin this section by introducing different test ranking protocols. We then  use them to  evaluate off-line performance of standard accuracy focused CF models,  from accuracy, coverage, and novelty perspectives.
%However, since by definition,  error metrics are measured only on the observed user feedback (subset of the items), they are not a precise indicator of accuracy as experienced by the user~\cite{cremonesi2010performance}.

In~\cite{steck2013evaluation}, it was shown  that  the choice of test ranking protocol can affect accuracy measurements considerably. Specifically,  accuracy is  measured either  by error metrics (e.g., RMSE and MAE) or ranking metrics (e.g., recall and precision) in  recommendation settings ~\cite{steck2013evaluation}. Error metrics are defined w.r.t.  the observed user feedback only.  Rating prediction models that optimize these metrics in the train phase, e.g., R-SVD, typically  adopt  the rated test-items ranking protocol in the test phase. However,  error metrics are not  precise indicators of  accuracy,  since they only consider the subset of items that have feedback. But  in real-world  applications, the system must find and rank  a  few items ($\size$)  from among \emph{all} items~\cite{cremonesi2010performance,steck2013evaluation}. 

Ranking metrics can be  measured on the observed  user feedback, or on \emph{all} items.  When  measured on the observed  user feedback, these metrics can  be strongly biased because of the  popularity bias of datasets~\cite{agarwal_chen_2016,cremonesi2010performance,steck2011item,vargas2014improving,steck2013evaluation}.  Therefore,  in top-$\size$ recommendation settings, these metrics are measured  using  the all-items ranking protocol,  to better reflect  accuracy as experienced by users  in real-world applications~\cite{steck2013evaluation,vargas2014improving}.


We extend the empirical study of~\cite{steck2013evaluation} by evaluating  the effect of the test ranking protocol on  accuracy, coverage, and novelty.  We use standard  accuracy-focused CF models (introduced in Section~\ref{sec:related-work}).  We set $\size=5$, and ran experiments on  ML-100K and Ml-1M  datasets, shown in  Figures~\ref{fig:CP-LC-tradeoffs-ml-100k}  and~\ref{fig:CP-LC-tradeoffs-ml-1m}, respectively.    %Details of experimental set up is provided in Section~\ref{sec:Experiments}. 

\balance

We analyze Figure~\ref{fig:CP-LC-tradeoffs-ml-1m}, which shows the results on the ML-1M dataset (Figure~\ref{fig:CP-LC-tradeoffs-ml-100k} has similar trends).  The first observation is that  all  algorithms obtain higher F-measure scores using the rated test-item ranking protocol. In particular, for the all unrated items ranking protocol, Figure~\ref{fig:CP-LC-tradeoffs-ml-1m}.a,  F-measure lies in $[0,0.2]$ (with the corresponding precision in $[0,0.5]$ in Figure~\ref{fig:CP-LC-tradeoffs-ml-1m}.c ). 
For the rated test-items ranking protocol  F-measure lies in $[0.2,0.4]$ (with precision in $[0.6,0.9]$) .  As a specific example, consider Rand which  randomly  suggests items according to  the ranking protocol.   As expected, random suggestion from among all items has low F-measure and precision. However, random suggestion from among the test items of each user, leads to an average F-measure of almost $0.25$ (precision approximately $0.6$). This demonstrates the bias  of the rated test-items ranking protocol. 
Similar to~\cite{cremonesi2010performance}, we observe Pop  is a strong contender in accuracy metrics, using both test protocols~\cite{cremonesi2010performance}. Recent work in~\cite{liu2017experimental} also confirmed that Pop outperformed more sophisticated algorithms for tourists datasets,   that are sparse  and where the users have few and  irregular interests (only visit popular locations). 
In addition, although R-SVD and R-SVDN are less accurate using the all unrated items ranking protocol, they obtain the highest F-measure scores using the rated test-items ranking protocol. This is because these models are optimized w.r.t. the  observed user feedback. Therefore, the rated test-items ranking protocol is to their advantage because it also considers  only the observed user feedback. 


Coverage is also affected by the ranking protocol. The rated test-items ranking protocol  results in better coverage for all algorithms except Random.  This is because random suggestion from among a user's test items, is a more constrained algorithm compared to random from among all train items. 
Regarding the effect of the ranking protocol on  LTAccuracy,  it lies in $[0,1]$ for the all unrated test items ranking protocol,  but drops to $[0,0.2]$ for the rated test-items ranking protocol.  %By definition,  long-tail items are those that receive fewer ratings,  therefore, they receive fewer ratings in the  test set, and since the observed test-items are ranked per user, LTAccuracy  scores are generally lower. 

 
Regarding the  trade-off between  metrics, irrespective of the ranking protocol, we observe that  Pop makes accurate yet trivial recommendations that lack novelty,   as indicated by the low LTaccuracy and coverage in Figure~\ref{fig:CP-LC-tradeoffs-ml-1m}. For the PureSVD models (P-SVD), on ML-100K and ML-1M, increasing the number of factors reduces  F-measure, and results in an increase in coverage and LTAccuracy.  Among all baselines, CoFiRank with regression loss (CofiR10, CofiR100) has reasonable coverage, precision, and long-tail accuracy. 

Overall, our experiments in this section confirm the findings of prior work~\cite{cremonesi2010performance,steck2013evaluation,agarwal_chen_2016}:  due to the popularity bias of recommendation datasets, rank-based precision  is strongly biased when it is measured using the rated test-items ranking  protocol.  This is demonstrated by the results of Pop, which  achieves an F-measure of $0.3$  on ML-100K  and  ML-1M, with even higher precision scores on those datasets.    It outperforms personalized models like CoFiRank.  However, using the all-items ranking protocol, the performance of these models aligns better with expectation.   Following prior research on top-$\size$ recommendation ~\cite{steck2013evaluation,vargas2014improving}, we conducted  the experiments in Section~\ref{sec:Experiments}  using the all-items ranking protocol.
 


%----------------------------------------------------------------------------------------------------


%R-SVD and R-SVDN When using the all unrated items ranking protocol, these models have low precision, low coverage yet have good long-tail accuracy. This indicates they recommend the same long-tail items to all users. Switching the ranking protocol, we see the models obtain better coverage, yet lower LTAccuracy.

%Overall precision and recall are biased when using rated test-items ranking protocol. %random recommendation from among the rated test-items, 


 %However,  rated test-items with local ranking metrics can be biased toward popular items~\cite{cremonesi2010performance,agarwal_chen_2016}.


%\subsection{Effect of Popularity Bias  on Top-$\size$ Recommendation  Performance}
%\subsection{Popularity bias and data sparsity}

%While the precise relationship between the objectives is not clear,  typically, an increase in coverage results in a decrease of accuracy.  

%Alternative testing strategies and criteria should be used to evaluate the novelty of recommendations. Although we use ranking accuracy using precision and recall (detailed in Section~\ref{sec:PerformanceMetrics}) but also use additional criteria to evaluate the novelty of recommendation lists. 
 
%But, the data suffers from popularity bias and is sparse. In order to make accurate suggestions,  these techniques focus on  the dense regions of data, where the popular items lie~\cite{cremonesi2010performance,vargas2014improving,steck2013evaluation,steck2011item}. 

 
%Standard CF techniques that focus on accuracy can be biased toward recommending  popular  items.
%In the literature, the problem is alleviated by either adopting additional performance criteria that measure the ability of model to recommend long-tail items~\cite{niemann2013new,vargas2014improving,ho2014likes, puthiya2016coverage}, or by using leave-one-out cross validation for long-tail items~\cite{cremonesi2010performance,yin2012challenging} and measuring long-tail precision and recall. Following the former, we adopt a range of evaluation metrics including  long-tail accuracy, which exclusively focuses on long-tail items in top-$\size$ sets.  




 % ~\cite{cremonesi2010performance} argued that although error metrics are computationally convenient, they are not suitable for assessing  top-$\size$ accuracy, and instead ranking metrics are more suitable. 

%Following~\cite{steck2013evaluation}, we refer to the former as \emph{test-items ranking protocol}, and the latter as \emph{all-items ranking protocol}

    %Rating CF models, that quantify accuracy using error metrics, are trained and tested on  observed user feedback. Because the observed data suffers from popularity bias  their accuracy measurements are not close to what the user experiences in practice. 

 %\cite{steck2013evaluation} examined the ranking evaluation protocol in recommendation, and found ...    or by using leave-one-out cross validation for long-tail items~\cite{cremonesi2010performance,yin2012challenging} and measuring long-tail precision and recall. 
 
 %The resource allocation models (5D, 5DRR, 5DARR and 5DA) modify MF and lead to improvements in Coverage and long-tail accuracy while lowering precision.

%As shown in~\cite{cremonesi2010performance}, a non-personalized~\textbf{Most Popular} (Pop) algorithm that recommends $\size$ most popular unseen items,  can achieve high precision and recall~\cite{cremonesi2010performance}. Recent work in~\cite{liu2017experimental} also confirmed that  Most popular recommendation outperformed more sophisticated algorithms for tourists datasets,   that are sparse  and where the users have few and  irregular interests (only visit popular locations).  But Most popular makes   makes trivial recommendations that lack novelty.
  