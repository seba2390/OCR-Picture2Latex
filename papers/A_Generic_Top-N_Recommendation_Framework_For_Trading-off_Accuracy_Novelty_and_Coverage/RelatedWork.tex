\section{Related Work}
\label{sec:related-work}
We provide an overview of related work here. A detailed description of  baselines and methods integrated in our framework is provided in Section~\ref{sec:Experiments}.

%Methods that are integrated in our framework, and baselines,  are bolded in this section. 
%\subsection{Popularity bias and Sparsity of CF  Data}
%\label{Popularity-bias}
%\vspace{3mm}



%\subsection{Accuracy-focused CF Methods} 
%\label{sec:acc-focused-CF-methods}
%Standard CF methods target predictive accuracy and are grouped into rating prediction or ranking prediction CF~\cite{lee2012comparative}. %herlocker2002empirical,

%\vspace{4mm}
%\noindent \textbf{Accuracy-focused CF Methods} are grouped into rating prediction or ranking prediction CF~\cite{lee2012comparative}. We next describe two  groups. 

\vspace{4mm}
\noindent \textbf{Accuracy-focused rating prediction CF models} aim to accurately predict unobserved user ratings~\cite{koren2008factorization,lee2012comparative}.  % target predictive accuracy formulate  recommendation as a rating prediction problem. 
Accuracy is measured using  error metrics such as RMSE or Mean Absolute Error that are defined w.r.t. the observed user feedback.
%RAP: I took out the short versions RMSE and MAE because we never used them anywhere else. I also took out the discussion of training and testing phase since I don't think people will care and it saved a live.
%In the training phase,  the value of unobserved user ratings are predicted so that they accurately reflect the user's observed ratings. In the testing phase, the value of withheld user ratings are predicted and accuracy is measured. %The scoring component predicts the value of unobserved user ratings so that they accurately reflect the user's observed ratings.  The  ranking component then greedily selects the top $\size$ items based  on their predicted value. 
%These  methods can be categorized into memory-based, model-based, or hybrid combinations~\cite{koren2008factorization}. Memory-based CF include the neighbourhood models, user-based CF~\cite{herlocker1999algorithmic}, and item-based CF~\cite{sarwar2001item} that make recommendations based on similarities between users, or items. Model-based methods train a parametric model on the user-item matrix and base their recommendations on the model. Matrix Factorization-based (MF) models~\cite{koren2009matrix, koren2008factorization} %, paterek2007improving 
%, a.k.a.~Singular Value Decomposition (SVD) models, belong to this  group. 
Generally, these methods are grouped into memory-based or model-based~\cite{koren2008factorization}. Earlier memory-based or neighbourhood models used the rating data directly to compute similarities between users~\cite{herlocker1999algorithmic} or items~\cite{sarwar2001item}. However,  these models are not scalable on large-scale datasets like Netflix. Model-based methods  instead train a parametric model on the user-item matrix.  Matrix Factorization models~\cite{koren2008factorization,koren2009matrix}, a.k.a.~Singular Value Decomposition (SVD) models, belong to this group and are  known for both  scalablility and accuracy~\cite{cremonesi2010performance}.  %herlocker1999algorithmic, paterek2007improving
% We consider four models from this category
 \iffullpaper 
These models factorize the user-item matrix into a product of two matrices; one  maps the users ($\mathcal{U}$), and the other maps items ($\mathcal{I}$), into a latent factor space of dimension $g \ll \min(\vert \mathcal{U} \vert, \vert \mathcal{I} \vert )$. Here,  each user $u$, and each item $i$, are  represented by a factor vector, $\mathbf{p}_u \in \mathbb{R}^{g}$, and $\mathbf{q}_i \in \mathbb{R}^{g}$, respectively. Ratings are estimated using $\hat{r}_{ui} = \mathbf{q}^{T}_i \mathbf{p}_u$. 
%However, due to the large number of missing values in the user-item matrix,  conventional SVD cannot be applied to learn the factors.  Instead, the  regularized squared error on the observed ratings is minimized.
Due to the large number of missing values in the user-item matrix,  the  regularized squared error on the observed ratings is minimized. The resulting objective function is  non-convex, and iterative methods such as Stochastic Gradient Descent (SGD) or Alternating Least Squares (ALS)~\cite{koren2009matrix, mnih2007probabilistic,rendle:tist2012} %srebro2004maximum,paterek2007improving
 can be used to find a local minimum.
\else
Using the user-item interaction data, these models learn latent factors for representing the users and items. Unobserved user ratings are estimated using the latent factors~\cite{koren2009matrix, mnih2007probabilistic,rendle:tist2012,zhuang2013fast}.   
\fi 
%From this category, we use a Regularized SVD  (RSVD) model and the same model with non-negative constraints (RSVDN)~\cite{koren2009matrix,zhuang2013fast}, in Section~\ref{sec:Experiments}.
From this group, we use  RSVD and RSVDN~\cite{koren2009matrix,zhuang2013fast}, in Section~\ref{sec:Experiments}.
  
 
\vspace{2mm}
\noindent \textbf{Accuracy-focused ranking prediction CF models} focus on accurately compiling ranked lists.  
The intuition is that since predicted rating values are not shown to the user, the focus should be on accurately selecting and ranking items.
Accordingly, accuracy is measured using  ranking metrics,  like recall and precision,  that can be   measured either on the observed  user feedback, or on \emph{all} items~\cite{cremonesi2010performance,steck2013evaluation}. 
%For ranking tasks, Most Popular (Pop) obtains high precision and recall~\cite{cremonesi2010performance,vargas2014improving}. PureSVD (PSVD)~\cite{cremonesi2010performance} and CofiRank (Cofi)~\cite{weimer2007maximum} are well-known  latent factor models for ranking,  with others in~\cite{balakrishnan2012collaborative,lee2014local}. We use Pop, PSVD, and Cofi in our experiments.
For ranking tasks, Pop obtains high precision and recall~\cite{cremonesi2010performance,vargas2014improving}. PSVD~\cite{cremonesi2010performance} and CoFiRank~\cite{weimer2007maximum} are well-known  latent factor models for ranking,  with others in~\cite{balakrishnan2012collaborative,lee2014local}. We use Pop, PSVD, and CofiRank in  Section~\ref{sec:Experiments}.



%In PureSVD,  missing values are imputed by zeros and conventional SVD is performed. CoFiRank can directly optimize the  Normalized Discounted Cumulative Gain (NDCG) ranking measure~\cite{voorhees2001overview}.  There  exist other techniques that optimize ranked loss measures~\cite{balakrishnan2012collaborative,lee2014local,weimer2007maximum}. 

  
%In~\cite{cremonesi2010performance} it was argued that because predicted rating values are not shown to the user in top$\size$ recommendation,  rather than optimizing error metrics (e.g.,~Root Mean Square Error (RMSE)), the focus should be on accurately compiling ranked lists. Ranking  metrics like precision and recall directly evaluate top-$\size$ performance~\cite{cremonesi2010performance} . 

 
%Due to selection bias in CF data,  the item distribution in the collected datasets  is biased toward popular items.  As shown in~\cite{cremonesi2010performance},   if the test item distribution is biased,  

\iffalse
\subsection{Multi-objective CF Methods}
Research targeting multiple objectives beside  accuracy,  is divided into two groups~\cite{adomavicius2011maximizing,niemann2013new}: 1.~those that directly devise a model that accounts for additional objectives, and 
2.~re-ranking techniques that post-process the output of a standard model.
 %1.~those that directly improve the scoring phase of to account for additional objectives, and 
%2.~re-ranking techniques that modify the ranking component of standard methods.
\fi

%\subsection{Long-Tail Promotion and Sales Diversity} %Maximizing sales diversity ensures that long-tail items will be recommended. The aggregate diversity of recommendations across users=to as sales diversity=or coverage, %(also referred to as aggregate diversity, sales diversity, or coverage) 
\vspace{2mm}
\noindent \textbf{Multi-objective methods}  devise new models that optimize several objectives, like  coverage and novelty, in addition to accuracy~\cite{vargas2014improving,steck2011item, yin2012challenging,niemann2013new,shi2013trading}.  In~\cite{niemann2013new},  items are assumed to be similar if they significantly co-occur with the same items. This leads to better representations for  long-tail items, and increases their chances of being recommended. A new performance measure that combines accuracy and popularity bias is proposed in~\cite{steck2011item}. This measure can be gradually tuned towards recommending long-tail items.  More recently, the idea of recommending users to items as a means of improving sales diversity, and implicitly, recommendation novelty, while retaining precision, has been explored in~\cite{vargas2014improving}.  In comparison to both~\cite{steck2011item,niemann2013new}, we focus on targeted promotion of long-tail items. While~\cite{vargas2014improving} focuses on neighbourhood models for top-$\size$ recommendation, our framework is generic and independent of the recommender models. 
Graph-based approaches for long-tail item promotion are studied in~\cite{yin2012challenging,shi2013trading}. They  construct a bipartite user-item graph and use a random walk to trade-off between popular and long-tail items. Rather than devise new  multi-objective models, we post-process existing models.
% A graph-based approach that  simultaneously trades-off among accuracy, sales diversity, and long-tail criteria, is proposed~\cite{shi2013trading}.  Here, each edge is associated with a cost and a transition probability. The latter encodes the likelihood of a user rating/purchasing an item whereas the former trades off relevance of recommendations and promotion of long-tail items. A similar approach  is employed in~\cite{yin2012challenging} where edge transition probabilities are discounted by item popularity. In both, a random walk leads to a trade-off between popular and long-tail items.  Other examples include clustering  of items according to their popularity~\cite{park2013adaptive}, learning classifiers for the items and ranking potential users of rare items~\cite{huang2005item}, and combining CF with decision tree models in people-to-people recommendations~\cite{krzywicki2012using}. 


\vspace{2mm}
\noindent \textbf{Re-ranking methods}  post-process the output of a standard model to account for additional objectives like coverage and diversity rather than devising a new model. These algorithms are very efficient. 
%Re-ranking to  simultaneously combine accuracy, diversity, novelty, and serendipity is  studied in~\cite{zhang2012auralist}.Re-ranking to maximize diversity within individual top-$\size$ is explored in~\cite{qin2013promoting,zhang2008avoiding,ziegler2005improving}.%ashkan2015optimal,
\cite{ziegler2005improving} explores re-ranking to maximize diversity within individual top-$\size$. It shows that users preferred diversified lists despite their lower accuracy. However, diversifying individual top-$\size$ sets does not necessarily increase  coverage~\cite{jambor2010optimizing,niemann2013new}. %(aggregate diversity (or coverage))
Re-ranking techniques that directly maximize coverage  and promote long-tail items are explored in~\cite{adomavicius2011maximizing,adomavicius2012improving,ho2014likes,jugovac2017efficient}. In contrast to~\cite{adomavicius2011maximizing,adomavicius2012improving,ho2014likes} that re-rank rating prediction models, our framework is generic and is independent of the base recommendation  model. Furthermore,  our long-tail  personalization is independently learned from interaction data.  PRA~\cite{jugovac2017efficient} is also generic framework for re-ranking, although we differ in our long-tail novelty prefernce modelling. We use RBT~\cite{adomavicius2012improving}, Resource allocation~\cite{ho2014likes}, PRA~\cite{jugovac2017efficient} as baselines since we share similar objectives.

\iffalse
 %~\cite{adomavicius2011maximizing,adomavicius2012improving,ho2014likes}  post-process the output of  rating prediction models to maximize coverage.
Unlike~\cite{adomavicius2011maximizing,adomavicius2012improving,ho2014likes} that re-rank rating prediction models, our framework is generic and the long-tail  personalization is independently learned. A maximum flow or maximum bipartite matching problem is solved  in~\cite{adomavicius2011maximizing}, while~\cite{adomavicius2012improving} proposes the RBT framework that  re-ranks a few of the items  according to a given ranking strategy, e.g. the popularity of items or  the average ratings of items.  However,  this approach requires extensive tuning  to find trade-off values. We use \textbf{RBT} as a baseline.  
A resource allocation approach for re-ranking the output of a rating prediction model, is presented in~\cite{ho2014likes}. Their approach has two phases: \begin{enumerate*}
\item resources are  allocated to items according to the received ratings, and
\item the resources are distributed according to the relative preferences of the users, and top-$\size$ sets are generated by  assigning a a 5D score (for accuracy, balance, coverage, quality, and quantity of long-tail items) to every user-item pair.
\end{enumerate*}
As baselines, we use \textbf{5D, 5DA, 5DRR}, and \textbf{5DARR} that are various combinations of the scoring function (5D) with the rank by rankings (RR) and accuracy filtering (A) algorithms (Section 3.2.2 in~\cite{ho2014likes}).

Recently,~\cite{jugovac2017efficient} proposed a generic Personalized Ranking Adaptation (PRA) framework. The idea is to  estimate user tendency for various objectives like diversity and item popularity using various heuristics, and to recommend top-$\size$ sets that match those tendencies.  We use \textbf{PRA} as a baseline. The framework iteratively and greedily re-ranks items in the head of the recommendations to match them with the user preferences.

\subsection{Modelling User Novelty Preference} 
While~\cite{lacerda2013building,Teo:2016:APD:2959100.2959171,yin2012challenging,zhang2008avoiding} profile  users based on their preference for diverse recommendation sets, we profile users based on their tendencies towards novel items. 
\fi

\vspace{2mm}
\noindent \textbf{Modelling user novelty preference} is studied in~\cite{jambor2010optimizing,jugovac2017efficient,kapoor2015like,oh2011novel,Zhao:2016:MNR:2911451.2911488}.  As explained in~\cite{kapoor2015like}, an item can be novel in three ways: \begin{enumerate*}
\item it is new to the system and is unlikely to be seen by most users (cold-start), \item it existed in the system but is new to a single user, \item  it existed in the system, was previously known by the user, but is a forgotten item.
\end{enumerate*}
\cite{kapoor2015like,Zhao:2016:MNR:2911451.2911488} focus on definitions 2 and 3 of novelty, which are useful in settings where seen items can be recommended again, e.g.,~music recommendation. 
For defining users' novelty preferences, tag and temporal data are used in~\cite{kapoor2015like,Zhao:2016:MNR:2911451.2911488}.
\iffullpaper
A logistic regression model is used to predict user novelty preferences in~\cite{kapoor2015like}, while~\cite{Zhao:2016:MNR:2911451.2911488} learn  a curiosity model for each user based on her access history and item tags.
\fi
\iffalse
The curiosity model is a function of item novelties. The item  novelty for that user is defined based on the user's item access frequency, recency of access, and item tags. ~\cite{Zhao:2016:MNR:2911451.2911488} proposed a framework to accurately recommend items that the user is curious about. Gross movie earnings are used for defining users' tendencies for novelty in~\cite{oh2011novel}.  In particular, to measure the tendency of users toward popular items,~\cite{oh2011novel} define a notion of personal popularity tendency (PPT) per user using the average gross earnings of each movie and based on the past ratings of the user. The computation of PPT, however, is based on the  assumption that  the total earnings of a movie reflects its popularity. The idea is to match the PPT of each user with the PPT of recommended items, thereby increasing the novelty of recommendation lists in a targeted manner. 
\fi
In~\cite{oh2011novel}, gross movie earnings are assumed to reflect  movie popularity, and are used to define a personal popularity tendency (PPT) for each user. The idea is to match the PPT of each user with the PPT of recommended items~\cite{oh2011novel}. 
The major difference between our work and~\cite{kapoor2015like,oh2011novel,Zhao:2016:MNR:2911451.2911488} is that we focus on the  cold-start definition of novelty (definition 1), we do not use contextual or temporal information to define users' tendencies for novelty, and we consider domains where  each item is accessed at most once (seen items cannot be recommended). 


In~\cite{jambor2010optimizing},  users are  characterized  based on their tendency towards disputed items, defined as items with  high average rating and high variance. These items  are claimed to  be in the long-tail. We differ in terms of long-tail novelty definition, and consequently our preference estimates.  \iffullpaper Furthermore,~\cite{jambor2010optimizing}  independently solve a constrained convex optimization problem for each user, with the user's disputed item tendency as a constraint. \fi \cite{zhang2013personalize,wang2009portfolio} use a user risk indicator to decide  between a personalized  and a non-personalized model; both focus on accuracy. In contrast, we combine accuracy and coverage models. Furthermore, while their risk indicators are optimized via cross validation, we learn the users' long-tail preferences.

PRA~\cite{jugovac2017efficient}   models user  preference for  long-tail novelty using on  item popularity statistics. In contrast,  we consider  additional information, like if the user found the item interesting,  and the long-tail preferences of other users  of the item.

\vspace{2mm}
\noindent \textbf{CF interaction data properties and test ranking protocol } %\footnote{RAP: this is a very weird start to the related work. I would expect you to talk about related algorithms, instead you're talking about bias in the dataset. However, as mentioned, you also bias the dataset by removing those with fewer than 5 ratings. Further, in reading this, it's not clear what the point is of this sub-section. I think that you're trying to tell us that your data preparation is better than others, but that doesn't come all the way across. I'd also explicitly state that you discuss algorithms that you compare against in the experimental section}
are two important aspects to consider in recommendation setting. CF interaction data suffers from  popularity bias~\cite{agarwal_chen_2016}.  In the movie rating domain, for instance, users are more likely to rate movies they know and like~\cite{cremonesi2010performance,steck2011item,steck2013evaluation,agarwal_chen_2016,steck2010training}. As a result, the partially observed interaction data is  not a  random subset of the (unavailable) complete interaction data. %(it is Missing Not At Random (MNAR)).  
Furthermore, many real-world CF interaction datasets~\cite{dooms2013movietweetings,kanagal2012supercharging,liu2017experimental,zolaktaf2015learn} are sparse, and the majority of  items and users  have few observations available. Due to  the popularity bias and sparsity of datasets,  many accuracy-focused  CF models are also biased toward recommending popular items. 
 % For example, almost $87\%$ of the movies in MovieTweetings~\cite{dooms2013movietweetings} are long-tail, and approximately $47.42\% $ of the users  have rated fewer than 10 items. 
% almost $88\%$ of the items in the Netflix rating dataset are long-tail, and approximately, $3.37\%$ of users have rated fewer than 10 items.  MovieTweetings~\cite{dooms2013movietweetings}is a movie rating dataset collected from Twitter.
  
 Moreover, some accuracy evaluation protocols are also  biased and tend to reward  popularity biased algorithms~\cite{cremonesi2010performance,vargas2014improving,steck2013evaluation}.  In~\cite{steck2013evaluation}, the main evaluation protocols are assessed in  detail, and the ``All unrated items test ranking protocol'' is described to be closer to the accuracy the user experience in real-world recommendation setting, where performance is measured using the complete data rather than available data~\cite{steck2013evaluation}. Following~\cite{steck2013evaluation,vargas2014improving}, and w.r.t. the additional experiments we conducted 
\iffullpaper
in Appendix~\ref{test-protocol-effect},
\else 
in~\cite{ourFullVersion},
\fi
 we chose the ``All unrated  items test ranking protocol'' for experiments in  Section~\ref{sec:Experiments}.

%-----------------------


%PRA~\cite{jugovac2017efficient} relies  on item popularity statistics  to measure    popular item tendencies, e.g.,~using the mean and standard deviation of the popularity of rated items, in addition to other heuristics.   We use \textbf{PRA} as a baseline.

%mean-standard-deviation variant of \textbf{PRA}. % These approaches are shown to achieve similar performances.  

 %Their framework also requires tunable parameters, and the diversity component is non-personalized. Furthermore, since the optimization is solved independently for each user, it is not clear how the final solution compromises between the satisfaction of the users to promote items that are not yet discovered and increase sales diversity.
% There is also work in Selective diversification \cite{santos2010selectively} refers to the idea of learning trade-off parameters in search result diversification. Different queries require different diversification strategies. \cite{maksai2015predicting}

 \iffalse  

\textcolor{red}{modify later:}Finding an allocation that maximizes both the satisfaction of the users $s(\mathcal{P})$ and the satisfaction of the producer $p(\mathcal{P})$  is a multicriterion  combinatorial optimization problem  ($ \max _{ \mathcal{P} = (\mathcal{P}_1 , ..., \mathcal{P}_U)}    \left [ s(\mathcal{P}),  p(\mathcal{P}) \right]$) %/decision making (subbranch of multicriterion decision making)
~\cite{hwang2012multiple, miettinen2012nonlinear,ehrgott2000survey}. 

\begin{equation}
\begin{aligned}
\small
& \underset{ \mathcal{P} = (\mathcal{P}_1 , ..., \mathcal{P}_U)} {\max}  
& & \left [ s(\mathcal{P}),  p(\mathcal{P}) \right] \\  
& \text{Subject to} 
& & \mathcal{P}_{u} \subseteq \mathcal{I}, |\mathcal{P}_{u}| \leq \size, \mathcal{P}_{u} \cap \mathcal{I}_{u} = \emptyset, \forall u \in \mathcal{U}. 
\end{aligned}
\label{eq:bijectiveOpt}
\end{equation}
 The two objectives are conflicting and a single optimal solution, that simultaneously maximizes both objectives, does not exist. Instead, there exists a set of nondominated solutions, also called the \textit{Pareto optimal} set. A solution dominates another if it as good or better in all objectives and better in at least one objective\cite{boyd2004convex}. Pareto optimal solutions are mathematically equivalent. However, we need a single \textit{best} Pareto optimal solution that compromises between the satisfaction of the right group of users and the producer. 

There are a number of ways to find Pareto optimal solutions~\cite{boyd2004convex,  hwang2012multiple, miettinen2012nonlinear}. A standard approach is to bound one objective and optimize the other.  By altering the  bounds and the objective functions to optimize, (every) Pareto optimal solutions can be generated.  Another approach is to \textit{scalarize} the objectives into a single function of the form  $(1-\mu)s(\mathcal{P}),  + \mu  p(\mathcal{P})$, where $\mu$ trades-off consumer and producer satisfaction, and both functions are in the same unit. Different $\mu$ values, result in different Pareto optimal solutions. However, because the interplay and relation between individual user satisfaction functions and the producer satisfaction is unclear, is is difficult to determine appropriate bounds or good trade-off values to generate the best Pareto optimal solution.  One option is to generate a set of Pareto optimal solutions,~e.g., by  systematically varying the budget values or $\mu$. But this process  is computationally expensive and requires a solution selection phase, where the  best solution is chosen wrt a separate set of criteria.

Instead, we combine the objective function formulation with the solution selection phase, to automatically generate the best answer. We achieve this by constructing a personalized value function for each user. 

\fi


%below I have related work on diversity
\iffalse
\subsection{Top-$\size$ Recommendation Diversity}  

\textcolor{red}{more on diversity?}
For instance, in~\cite{yin2012challenging} measures diversity in the past purchases of individual users.~\cite{zhang2008avoiding} explicitly: represent user intents,~\cite{lacerda2013building} enhanced the diversity and novelty in individual recommendation lists by building informative user profiles from user-item interactions. ~\cite{di2014analysis} analyze users propensity toward diversity in recommendations, too simple,heuristic and only for MMR? In the diversification literature that model user intents
The appeal of recommendation sets depends on  several criteria, e.g.,~\cite{zhang2012auralist} simultaneously targets accuracy, diversity, novelty, and serendipity, and offers a hybrid re-ranking framework for combining the qualities.  Diversity within individual top-$\size$ sets, also impacts the appeal of lists. In~\cite{ziegler2005improving}, the utility of a recommendation list for the user is defined as a composition  of  two quality metrics, accuracy  and diversity of products offered in the top-$\size$ set. %Recommendation lists are entities in their own right rather than aggregations of highly accurate, yet independent, items. 
A topic diversification heuristic is proposed in~\cite{ziegler2005improving}, which uses an intra-list similarity measure based on classification taxonomies to re-rank lists generated by item-based CF  or user-based CF. They showed that users preferred diversified lists despite their lower accuracy. The problem of maximizing diversity of  top-$\size$ sets while also maintaining adequate similarity to the user is examined in~\cite{zhang2008avoiding} and a constrained optimization problem that trades off relevance and diversity is formulated. A submodular (formally defined in Section~\ref{sec:TheoreticalAnalysis}
) framework using a linear combination of relevance and diversity is formulated in~\cite{qin2013promoting}, where  relevance is captured using PMF~\cite{mnih2007probabilistic} and an entropy regularizer is proposed for modelling diversity. \cite{ashkan2015optimal} formulate the task a maximization of a modular function on the polytope of a submodular function, which is solved optimally by a greedy method. However, their approach is practical when the size of the recommended list is short, and they require an external source for the topics. In contrast to our work, the above methods are either heuristic, or require additional information sources to explicitly diversify the lists, and   the trade-off between relevance and diversity  is non-personalized, and does not target long-tail item promotion. %Instead, we are the first to propose a re-ranking based method which considers relevance, diversity in recommendations and coverage across the item space in a submodular maximization framework. 
%  whereas diversity in recommendation lists increases the utility of recommendation lists and helps avoid user boredom~\cite{zhang2009enhancing}.  
%In~\cite{bradley2001improving}, the diversity in the recommendations is explicitly maximized while still recommending relevant products to users. 

Diversity in RS is related to diversity  in Information Retrieval (IR)~\cite{vargas2011intent,Castells2015},~e.g., summarization and search. %Both search and summarization in IR require selecting relevant and diverse subsets of documents. 
Relevance and non-redundancy are defined as desirable properties of good summaries in document summarization~\cite{lin2011class}.
Search result diversification aims to to retrieve a ranked list of relevant documents that cover all aspects of an (ambiguous) query~\cite{agrawal2009diversifying, carbonell1998use,drosou2010search, gollapudi2009axiomatic}. Many solution formulations are based on the Maximal Marginal Relevance (MMR) objective function~\cite{carbonell1998use}, that combines relevance and diversity: $j^* = \argmax_{j \in \mathcal{I} \setminus \mathcal{A} } \left[  \lambda_{MMR} r_{uj} - (1-\lambda_{MMR}) \max_{i \in \mathcal{A}} s(i, j)\right]$. %, a linear combination of a relevance and a diversity component
The idea is to  select documents that are relevant to the query, but dissimilar to already selected documents. Among various diversification methods, relevance estimates are commonly produced by standard retrieval approaches, and the major difference is in the diversity estimate. Some methods opt for implicit diversity modelling~\cite{gollapudi2009axiomatic} while others explicitly model query aspects and diversity~\cite{agrawal2009diversifying,santos2010exploiting}. An axiomatic approach to diversification is described in~\cite{gollapudi2009axiomatic} and a number of diversification  objectives are formulated. When the distance function is a metric, efficient algorithms can be designed for maximizing the diversity~\cite{gollapudi2009axiomatic}.  
Submodular diversity components are also common in diversification algorithms~\cite{abbassi2013diversity,agrawal2009diversifying};  a  submodular diversification framework that relies on a taxonomy, and aims to minimize the dissatisfaction of the average user is given in~\cite{agrawal2009diversifying}. Most search result diversification algorithms provide non-personalized diversified lists, although~~\cite{vallet2012personalized} developed a framework for personalized diversification of search results.
 % e.g. ~maxsumdispersion where$f(\mathcal{S}) = (k-1) \sum_{u \in S} w(u) +  2\lambda \sum_{u,v \in \mathcal{S}} d(u,v) $, for which the greedy algorithm is a 2-approximation algorithm when the distance function is a metric. 
 %graph-based query suggestion diversification \cite{ma2010diversifying}.
\fi


