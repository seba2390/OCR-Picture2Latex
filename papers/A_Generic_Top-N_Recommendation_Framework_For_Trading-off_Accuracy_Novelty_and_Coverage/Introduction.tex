\section{Introduction}
\label{sec:Introduction}


The goal in top-$\size$ recommendation is to recommend to each
consumer a small set of $\size$ items from a large collection of
items~\cite{cremonesi2010performance}.  For example, Netflix may want
to recommend $\size$ appealing movies to each consumer.  Collaborative
Filtering (CF)~\cite{herlocker2002empirical,lee2012comparative} is a
common top-$\size$ recommendation method.  CF infers user interests by
analyzing partially observed user-item interaction data, such as user
ratings on movies or historical purchase
logs~\cite{kanagal2012supercharging}. The main assumption in CF is that
users with similar interaction patterns have similar interests.


Standard CF methods for top-$\size$ recommendation focus on making  suggestions  that accurately reflect the user's preference history. However, as  observed in previous work,  CF recommendations are generally biased toward  popular items, leading to a rich get richer effect~\cite{vargas2014improving,steck2011item}.  The major reasons for this are \textit{popularity bias} and \textit{sparsity} of CF interaction data (detailed in Section~\ref{sec:related-work}). In a nutshell, to maintain  accuracy, recommendations are generated from the dense regions of the data,  where the popular items lie.  

However,  accurately suggesting popular items, may not be satisfactory for the consumers. For example, in Netflix, an accuracy-focused movie recommender may recommend ``Star Wars: The Force Awakens'' to users who have seen ``Star Wars: Rogue One''.  But, those users are probably already aware of ``The Force Awakens''. Considering additional factors, such as novelty of recommendations,  can lead to more effective suggestions~\cite{cremonesi2010performance,Castells2015,zhang2008avoiding,ziegler2005improving,zhang2012auralist}. 
%Second, accuracy-focused models typically achieve a   overall item-space coverage across their recommendations,  whereas high item-space coverage helps providers of the items increase revenue
%, users satisfaction since they are  likely already aware of or can find these items on their own.  

Focusing on popular items also adversely affects the satisfaction of  the providers of the items. This is because  accuracy-focused models typically achieve a  low overall item space coverage across their recommendations, whereas   high item space coverage helps providers of the items increase their revenue~\cite{vargas2014improving,Castells2015,adomavicius2011maximizing,anderson2006thelongtail, yin2012challenging,adomavicius2012improving}.
%accuracy-focused models typically achieve a

In contrast to the relatively small number of popular items, there are copious  {\it long-tail\/} items that have fewer observations (e.g., ratings) available. More precisely,  using the Pareto  principle (i.e.,~the $80/20$ rule),  long-tail items can be defined as items that generate the lower $20\%$ of observations~\cite{yin2012challenging}. Experimentally we found that these items correspond to almost $85\%$ of the items in several datasets (Sections~\ref{sec:Notation} and \ref{sec:Experiments}). %Table~\ref{tab:DatasetStatsticsSmall})


As previously shown, one way to improve the novelty of top-$\size$ sets is to recommend interesting long-tail items~\cite{cremonesi2010performance,ge2010beyond}.  The intuition  is that since they have fewer observations available,  they are more likely to be unseen~\cite{Kaminskas:2016:DSN:3028254.2926720}.  
 %For example, in online commerce,  newly added items are long-tail items that are yet to be discovered.  
Moreover, long-tail item promotion also results in higher overall coverage of the item space%, which increases profits for providers of the items
~\cite{vargas2014improving,Castells2015,zhang2008avoiding,zhang2012auralist,adomavicius2011maximizing,anderson2006thelongtail,yin2012challenging,jambor2010optimizing}. Because long-tail promotion reduces accuracy~\cite{steck2011item}, there are trade-offs to be explored.


%original submitted to ICDE
%This work studies three aspects of top-$\size$ recommendation: accuracy, novelty, and item-space coverage, and examines their trade-offs. In most previous work, predictions of a base recommendation system are re-ranked to handle their trade-offs~\cite{adomavicius2012improving,jambor2010optimizing,zhang2013personalize,wang2009portfolio}. Due to performance considerations, however, these techniques are not customized per user. For example,  parameters that balance the trade-off between novelty and accuracy are cross-validated at a global level.  This can be detrimental since users have varying preferences for  objectives such as long-tail novelty. We explore how to  automatically infer  user  preference for long-tail novelty, and how to leverage  it to correct  the popularity bias in standard recommender models. Our work does not rely on any additional contextual data, although such data, if available, can help promote newly-added long-tail items~\cite{agarwal2009regression,Saveski:2014:ICR:2645710.2645751}.

This work studies three aspects of top-$\size$ recommendation: accuracy, novelty, and item space coverage, and examines their trade-offs. In most previous work, predictions of a base recommendation algorithm are \textit{re-ranked} to handle these trade-offs~\cite{adomavicius2012improving,jambor2010optimizing,zhang2013personalize,wang2009portfolio}. The re-ranking models are computationally efficient but suffer from two drawbacks. First, due to performance considerations,  parameters that balance the trade-off between novelty and accuracy  are not customized per user. Instead they are cross-validated at a global level.  This can be detrimental since users have varying preferences for  objectives such as long-tail novelty. Second,  the re-ranking methods are often limited to a specific base recommender  that may be sensitive to dataset density. 
As a result, the datasets are pruned and the problem is studied in dense settings~\cite{adomavicius2012improving,ho2014likes}; but real world  scenarios are often sparse~\cite{kanagal2012supercharging,liu2017experimental}.   
% Because  dataset density can impact the performance of most base recommenders (like R-SVD), which in turn affects the performance of the re-ranking model, 

\iffalse
We address these limitations by directly inferring  user  preference for long-tail novelty  from interaction data.  This  allows us to customize the re-ranking  per user, and design a \textit{generic} framework, which resolves the second problem. In particular, since the long-tail novelty preferences are estimated independently of any base  recommender model, we can  plug-in an appropriate base recommender w.r.t. the dataset sparsity.% including ones that are more suitable for sparse settings.  

Modelling  user  preference for  long-tail novelty using only item popularity statistics, e.g., the average popularity of rated items as in~\cite{jugovac2017efficient}, disregards additional information like whether the user found the item interesting and the long-tail preferences of other users  of the items. \iffalse To incorporate them, we introduce the notion of  \emph{item long-tail importance}. Both  user long-tail preferences and item long-tail importance are dependent:  a user has high preference for discovering long-tail items if she is interested in important long-tail items, and an item that is associated with many of these kinds of users is likely to be more important.  We propose a joint optimization framework to directly learn,  from interaction data, both the users' long-tail preferences and the  items' long-tail importance. \fi
We propose an optimization approach that  incorporates  this information and  directly learns,  from interaction data, the users' long-tail novelty preferences.

Next, we use these learned preferences  to design a  top-$\size$ recommendation framework thats is generic, and provides customized balance between accuracy, novelty, and coverage. We refer to it as framework as GANC.  Using GANC, we design a novel algorithm, {\it Ordered Sampling-based Locally Greedy (OSLG)\/}, that relies on the learned long-tail novelty preferences  to scalably correct for popularity bias. Our work does not rely on any additional contextual data, although such data, if available, can help promote newly-added long-tail items~\cite{agarwal2009regression,Saveski:2014:ICR:2645710.2645751}. In summary:
\fi

We address the first limitation by directly inferring  user  preference for long-tail novelty  from interaction data.   Estimating these  preferences  using only item popularity statistics, e.g., the average popularity of rated items as in~\cite{jugovac2017efficient}, disregards additional information, like whether the user found the item interesting or the long-tail preferences of other users  of the items. We propose an approach that  incorporates  this information and  learns the users' long-tail novelty preferences from interaction data.

This approach allows us to customize the re-ranking  per user, and  design a \textit{generic} re-ranking framework, which resolves the second limitation of prior work. In particular, since the long-tail novelty preferences are estimated independently of any base recommender, we can  plug-in an appropriate one w.r.t. different factors, such as the dataset sparsity.

Our top-$\size$ recommendation framework, \textbf{GANC}, is \textbf{G}eneric, and provides customized balance between \textbf{A}ccuracy, \textbf{N}ovelty, and \textbf{C}overage. % Moreover, based on the learned long-tail novelty preferences, we also design a novel algorithm, {\it Ordered Sampling-based Locally Greedy (OSLG)\/}, that relies on the learned long-tail novelty preferences  to scalably correct for popularity bias. 
Our work does not rely on any additional contextual data, although such data, if available, can help promote newly-added long-tail items~\cite{agarwal2009regression,Saveski:2014:ICR:2645710.2645751}. In summary:

%\input{RunningExample}

%We propose a novel approach that allows us to  promote long-tail items in a targeted manner, thereby improving the novelty of top-$\size$ sets, the overall item-space coverage across recommendations, while maintaining reasonable levels of accuracy.

%Next, we integrate these learned preferences  in a generic  top-$\size$ recommendation framework to provide customized balance between accuracy and coverage.

%sequentially make recommendations, while adjusting its parameters with regard to the set of top-$\size$ recommendations made so far. However, since  sequential parameter updates  cause  scalability issues, we propose a sampling based algorithm. This variant of our framework, called {\it Ordered Sampling-based Locally Greedy (OSLG)\/},  allows us to  correct for the popularity bias in recommendations with regard to individual user long-tail preferences. 

%ICDE submission
%Our framework differs with  prior work in the following aspects:  unlike~\cite{adomavicius2011maximizing,adomavicius2012improving,zhang2013personalize,ho2014likes},  the long-tail preference personalization in our framework is learned rather than optimized using cross-validation or parameter tuning. In other words, our personalization method is independent of the underlying base  recommendation models.  Moreover, our framework is  generic. This enables us to  plug-in several base recommenders, and evaluate their  effectiveness without requiring  extensive tuning for the accuracy and coverage trade-off. 


%\vspace{-2.8pt}
\begin{itemize}

\item  We examine various measures for estimating user long-tail novelty preference in Section~\ref{sec:lt-pref} and formulate an optimization problem  to directly learn users' preferences for long-tail  items from interaction data in Section~\ref{sec:learning-lt-pref}. %In addition, we introduce several heuristics for measuring the user preference for less common items from historical rating data.% 

\item  We integrate the user preference estimates into GANC %, a generic re-ranking framework that provides customized balance between accuracy, novelty, and coverage 
(Section~\ref{sec:RiskbasedReranking}), and  introduce {\it Ordered Sampling-based Locally Greedy (OSLG)\/}, a scalable algorithm that relies  on user long-tail preferences to correct the popularity bias (Section~\ref{sec:optimizationAlgorithm}).
%We introduce OSLG, a scalable algorithm that relies  on user long-tail preferences to  maximize item space coverage \textcolor{red}{while maintaining acceptable levels of accuracy} (Section~\ref{sec:optimizationAlgorithm}).

\item   We conduct an extensive empirical study and evaluate performance from  accuracy, novelty, and coverage perspectives (Section~\ref{sec:Experiments}).  We use five  datasets with varying density and difficulty levels. %:  Netflix, MovieTweetings, and MovieLens (100K, 1M, 10M). 
  In contrast to most related work,  our evaluation considers realistic settings that include a large number of infrequent  items and users. %This enables us to study the impact of  data density on the performance trade-offs of several  state of the art top-$\size$ recommendation algorithms. %   %,  and use the all-items ranking protocol~\cite{steck2013evaluation,vargas2014improving}, where performance is measured using all items with train data. to evaluate the performance of several  state of the art top-$\size$ recommendation algorithms 
 
\item Our empirical results confirm that the performance of re-ranking models is impacted by the underlying   base recommender and the dataset density. Our generic approach enables us to easily incorporate a suitable base recommender to devise an effective solution for both dense and sparse settings. In dense settings, we use the same base recommender as existing re-ranking approaches, and we outperform them in accuracy and coverage metrics. For sparse settings, we plug-in a more suitable base recommender, and devise an effective solution that is competitive with existing top-$\size$ recommendation methods in accuracy and novelty. 

%Directly estimating the long-tail novelty preferences allows us to customize re-ranking per user, and  devise a generic framework.   
 
\end{itemize}

Section~\ref{sec:related-work} describes related work. Section~\ref{sec:conclusion} concludes.
