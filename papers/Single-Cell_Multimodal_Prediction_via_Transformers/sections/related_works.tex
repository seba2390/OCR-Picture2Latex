\section{Related Work}
% \vspace{-0.5em}
In this section, we review related works of our proposed framework, including GNNs, transformers, and other in single-cell analysis.
% deep learning methods 

% \vspace{-1.5em}
\subsection{Deep Learning on Multimodal Integration} 
% \vspace{-0.5em}
There is a growing number of deep learning-based methods for multimodal single-cell analysis in the community. For instance,
scMDC~\cite{lin2022clustering} is an end-to-end autoencoder-based model with one encoder and two decoders. The encoder takes the concatenation of two modalities as an input and then reconstructs two modalities separately via two individual decoders.
% where the learned latent embedding would be used for clustering analysis.
DCCA~\cite{zuo2021deep} learns a coordinated but distinct representation for each omics data by mutually supervising each other on the basis of semantic similarity across embeddings, and then reconstructs back to the original dimension as output via a decoder for each omics data.
Cross-modal Autoencoders~\cite{yang2021multi} utilize multiple autoencoders to map different modalities onto the same latent space, and incorporate prior knowledge through the use of adversarial loss and paired anchor loss in the training process. 
BABEL~\cite{wu2021babel} consists of two neural-network-based encoders and two decoders for translation between gene expression and chromatin accessibility. Both Cross-modal Autoencoders and BABEL focus on multimodal translation by adding interoperability constraints to train multiple encoders and decoders. 
Another approach, scMM~\cite{minoura2021mixture}, captures nonlinear latent structures with variational autoencoders. It exploits a mixture-of-expert framework with a deep generative model and attains end-to-end learning by modeling raw counts of each modality. While these models have made significant advancements in multimodal integration, most of them are based on autoencoders and tend to overlook the underlying biological interactions of molecules and cells.
% However, most models for multimodal integration are based on autoencoders, which overlook the underlying biological interactions of molecules and cells. 

% \vspace{-1.9em}
\subsection{GNNs and Transformers in Single-Cell} % Analysis} 
% \vspace{-0.5em}
To capture the biological interactions of molecules and cells, there has been an increasing number of GNNs and transformer frameworks published in the field of single-cell analysis. 
One benefit of transformers applied in single-cell data is to capture long-range dependency in a global view. Another benefit is to interpret biological phenomena via the attention mechanism in transformers.
From GNNs' perspective, graphs are natural to represent all kinds of data in single-cell data, like gene-to-gene graphs, cell-to-cell graphs, and cell-to-gene graphs. Another benefit of GNNs is to easily add prior knowledge into graphs, like pathways between genes or overlaps between genes and peaks. % domain knowledge or
For example, scGNN~\cite{wang2021scgnn} models cell-cell interaction by incorporating GNN with multi-modal autoencoders. Specifically, scGNN builds a cell graph by capturing cell-type-specific regulatory signals and utilizes a Left-Truncated Mixture Gaussian model for scRNA-Seq data. GLUE~\cite{cao2022multi} pre-trains modality-specific variational autoencoders to get cell embeddings and then encodes a knowledge-based graph with GNNs. The next step involves performing an adversarial multimodal alignment of the cells through an iterative optimization process. In addition, ScMoGNN~\cite{wen2022graph} models the cell similarity and feature similarity by building a cell-feature graph and extracts information from data with a graph encoder. ScMoGNN takes advantage of gene pathway data as prior knowledge to enhance the graph and denoise the data. Moreover, scBERT~\cite{yang2022scbert} follows the pre-training and fine-tuning paradigm of bidirectional encoder representations from transformers (BERT) for cell annotation of scRNA-seq data. The process of annotation involves extracting high-level patterns of cell types from the reference dataset. Different from these approaches which focus on single-modality data,  we are the first to introduce transformers and GNNs to single-cell multimodal prediction. 