The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. 
Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a \method{} framework which can readily incorporate external domain knowledge and model the interactions within each modality and cross modalities.
Extensive experiments demonstrate that \method{} achieves superior performance on various benchmark datasets. Remarkably, \method{} won a Kaggle silver medal with the rank of $24 / 1221$ (Top 2\%) {\it without ensemble} in a NeurIPS 2022 competition\footnote{\url{https://nips.cc/virtual/2022/competition/50092}}. Our implementation is publicly available at Github\footnote{\url{https://github.com/OmicsML/scMoFormer}}.

%Nevertheless, the integration of the acquired multimodal data to gain a comprehensive view presents a significant challenge. This includes the difficulty in modeling relationships between modalities and incorporating a large amount of single-modality datasets into subsequent analyses. 










