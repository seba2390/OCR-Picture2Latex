% \newpage

% \section{Technical Details}
% \label{app:node}
% \noindent\textbf{Node Feature Initialization.} With the graph structure, next we discuss how to initialize the node features. 
% Since the RNA counts $\mathbf{X}_\text{g}$ show drastic sparsity along with high dimension, it is not practical to be directly used as cell features. Hence, we first denoise the data and reduce the dimension by Singular Value Decomposition (SVD). To alleviate the effects of cell-to-cell heterogeneity and extremely large counts, we conduct library size normalization and centered log-transformation. The preprocessed data $\bar{\mathbf{X}}$ is then passed through the SVD algorithm. Cell features are initialized with the reduced features $\mathbf{h}^0_{\text{c}} \in \mathbb{R}^{N_{\text{c}}\times d_0}$. We then initialize gene features by the weighted sum of the reduced features $\mathbf{h}^0_{\text{g}} = \bar{\mathbf{X}}_\text{g}^T \cdot \mathbf{h}^0_{\text{c}} \in \mathbb{R}^{N_{\text{g}}\times d_0}$ with the normalized counts $\bar{\mathbf{X}}_\text{g}$ as the weights. In the studied problem, proteins are the target modality for prediction; thus they are initialized randomly based on their indices.

\section{Experimental Details} 
\label{app:exp}
We summarize the dataset details in Table~\ref{table:dataset}. The experimental results on CBMC are shown in Appendix~\ref{app:cbmc}.
\begin{table}[htb]
    \centering
    \vspace{-1.em}
    \caption{Dataset Statistics.}
    \vspace{-1.2em}
    \scalebox{1.}{
    \begin{tabular}{c|ccc}
        \toprule
        & \textbf{CITE} & \textbf{GEX2ADT} & \textbf{CBMC}\\ \midrule
        Number of RNA & 22,050 & 13,953 & 20,501 \\ 
        Number of Proteins & 140 & 134 & 10\\
        Train Cells & 42,843 & 66,175 & 7755 \\
        Test Cells & 28,145 & 1,000 & 916 \\
        RNA Zero Rate & 0.780 & 0.904 & 0.953 \\
        % Protein Zero Rate
        \bottomrule
    \end{tabular}
    }
    \vspace{-1.8em}
    \label{table:dataset}
\end{table}


% \subsection{Parameter Setting.}
% To benchmark the performance of baselines and \method{}, we uniformly employ inductive settings among both datasets. On the CITE dataset, we use the data measured on day 4 for testing and randomly split $80/20\%$ of the data prior to day 4 for training and validation. On the GEX2ADT dataset, we randomly pick $15\%$ of the training data for validation and evaluate the predictions on the testing set. For BABEL, the hidden dimension is tuned from $\{16, 32, 64, 128\}$. For CMAE, the weights of adversarial loss and reconstruction loss are chosen from $\{0.1, 1, 2.5, 5, 10\}$. For scMM, the hidden dimensions are tuned from $\{16, 32, 64, 128\}$. For ScMoGNN, the weight decay parameter of the optimizer is tuned from $\{5\times 10^{-6},1 \times 10^{-5}, 5 \times 10^{-5}, 1 \times 10^{-4}\}$.



\section{More Experimental Results}

\iffalse
\subsection{Paired T-Test of Performance}
To provide evidence for the improvement of \method{} over scMoGNN in Table~\ref{table:main}, we conducted paired t-tests and included the p-values in Table~\ref{tab:ttest}. For RMSE and MAE, the alternative hypothesis is that the scores of \method{} are lower than those of scMoGNN, and vice versa for Corr. From the table, we observe that: In the CITE dataset, all three metrics show significant improvement with p-values smaller than $10^{-4}$. In the GEX2ADT dataset, the p-values of RMSE and MAE are greater than $0.1$. However, it is important to note that the performance gap in the NeurIPS 2021 competition leaderboard\footnote{\url{https://eval.ai/web/challenges/challenge-page/1111/leaderboard/2860}} for the GEX2ADT task was already relatively small. 
% For reference, we have listed the top 5 results (based on RMSE) from the competition below.
% \begin{itemize}
%     \item Guanlab-dengkw: 0.3854
%     \item
% DANCE (scMoGNN)	0.3898
% \item
% scJoint	0.3954
% \item 
% AXX	0.3979
% \item
% MIM UW	0.3982
% \end{itemize}
Therefore, while \method{} did show improvement over scMoGNN in GEX2ADT dataset, it is worth considering the competitive context.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\caption{Paired t-test results.}\label{tab:ttest}
\begin{tabular}{@{}lccc@{}}
\toprule
Paired t-test & \textbf{RMSE} & \textbf{MAE} & \textbf{Corr} \\ \midrule
CITE          & 5.25$\times 10^{-5}$          & 6.53$\times 10^{-5}$         & 1.64$\times 10^{-5}$          \\
GEX2ADT       & 0.194         & 0.183        & 0.053         \\ \bottomrule
\end{tabular}
\end{table}
\fi



% \subsection{Performance on CBMC CITE-seq Dataset}\label{app:cbmc}
\noindent \textbf{Performance on CBMC CITE-seq Dataset.}
\label{app:cbmc}
We have conducted additional experiments on the CBMC CITE-seq dataset obtained from Seurat~\cite{hao2021integrated}, which contains 20501 RNAs and 10 proteins. For the purpose of comparison, we employed scMoGNN along with \method{} since the former is the best performing baseline. The results of five runs are presented in Table~\ref{tab:cbmc}. Our findings show that \method{} consistently outperformed scMoGNN across all three metrics, which aligns with our previous observations on the CITE and GEX2ADT datasets.
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\begin{table}[h]
\centering
\vspace{-0.5em}
\caption{Results on CBMC dataset.}\label{tab:cbmc}
\vspace{-1.2em}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{@{}lccc@{}}
\toprule
CBMC       & \textbf{RMSE}                & \textbf{MAE}                 & \textbf{Corr}                \\ \midrule
scMoGNN    & 0.60699$\pm$0.01400 & 0.43439$\pm$0.00708 & 0.95043$\pm$0.00334 \\
\method{} & \textbf{0.57272}$\pm$\textbf{0.02660} & \textbf{0.41790}$\pm$\textbf{0.01831} & \textbf{0.9541}8$\pm$\textbf{0.00381} \\\bottomrule
\end{tabular}
}
\vspace{-1.8em}
\end{table}

% \subsection{Handling Other Single-Cell Multimodal Prediction Tasks}\label{app:others}

% In the paper, we focused on the specific task GEX2ADT as a showcase. It is important to note that our framework is versatile and can be applied to other modality prediction tasks. Specifically, \method{} can be extended to:
% \begin{itemize}
% \item \textbf{ADT2GEX:} In our framework, we constructed a multimodal heterogeneous graph consisting of gene nodes, protein nodes and cell nodes. For the GEX2ADT task, we removed the cell-protein edges to eliminate information leakage. Similarly, for the ADT2GEX task, protein measurements were incorporated into the cell-protein edges, and the cell-gene edges were removed. Cell embeddings were initialized by the reduced protein levels and protein embeddings were initialized by the weighted sum of cell embeddings with normalized protein measurements as weights. With the same multimodal transformer module and a prediction layer, \method{} can predict gene expression from protein levels.
% \item \textbf{GEX2ATAC and ATAC2GEX:} In the context of the GEX2ATAC and ATAC2GEX tasks, we removed the protein nodes from the multimodal graph. To initialize the cell embeddings for the GEX2ATAC task, we used the reduced gene expression values, while for gene embeddings, we computed a weighted sum of cell embeddings using normalized gene measurements as weights. For the ATAC2GEX task, we masked the cell-gene edges that were present in the testing set. We initialized the cell embeddings using the reduced ATAC measurements, and the gene embeddings were randomly initialized.
% \end{itemize}
% We have compared the performance of \method{} with scMoGNN on the four tasks included in the NeurIPS 2021 competition~\cite{luecken2021sandbox}. Note that \method{} outperformed scMoGNN in tasks that involve protein modality and vice versa. These results suggest that incorporating prior information of protein nodes can enhance the performance of \method{} when protein modality is present. The RMSE scores across five runs are summarized as in Table~\ref{tab:others}.

% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% \begin{table}[h]
% \centering
% \caption{Results on other Single-Cell Mutimodal Tasks.}\label{tab:others}
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{@{}lcccc@{}}
% \toprule
%            & GEX2ADT             & ADT2GEX             & GEX2ATAC            & ATAC2GEX            \\ \midrule
% scMoFormer & \textbf{0.4198}7$\pm$\textbf{0.00234} & \textbf{0.31547}$\pm$\textbf{0.00184} & 0.17885$\pm$0.00008 & 0.23988$\pm$0.00031 \\
% scMoGNN    & 0.42576$\pm$0.01180 & 0.32250$\pm$0.00136 & \textbf{0.17823}$\pm$\textbf{0.00011} & \textbf{0.23021}$\pm$\textbf{0.00219} \\ \bottomrule
% \end{tabular}
% }
% \end{table}

% \subsection{Training Efficiency of \method{}} \label{sec:efficiency}
% Since scMoGNN is the best-performing baseline, we compared the running time and total GPU memory of \method{} and scMoGNN across five runs on one Quadro RTX 8000 GPU. The results are summarized in Table~\ref{tab:efficiency}. We observed that with higher GPU consumption, \method{} required a significantly shorter running time compared to scMoGNN. It is worth noting that \method{} was configured with a relatively large batch size of 8000 cells per batch and a hidden dimension of 512, in order to achieve better performance and better utilization of computing resources. One can surely reduce the required GPU memory of \method{} by limiting the setting accordingly. 

% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% \begin{table}[h]
% \centering
% \caption{Efficiency Comparison}\label{tab:efficiency}
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{@{}l|cc|cc@{}}
% \toprule
% \multicolumn{1}{l|}{} & \multicolumn{2}{l|}{\textbf{Running time (min)}} & \multicolumn{2}{l}{\textbf{GPU memory (GB)}}         \\ \midrule
%                       & CITE                   & GEX2ADT                 & CITE                   & \multicolumn{1}{l}{GEX2ADT} \\
% \method{}            & 24.82$\pm$4.16         & 17.95$\pm$3.56          & \multicolumn{1}{r}{38} & 21                          \\
% scMoGNN               & 58.89$\pm$7.77         & 108.54$\pm$21.22        & \multicolumn{1}{r}{26} & 12                          \\ \bottomrule
% \end{tabular}
% }
% \end{table}



% \noindent \textbf{Can \method{} process large datasets?} In practical applications, it exhibits the capability to process large datasets. We will address this issue from the perspectives of both the model and the data:
% \begin{itemize}

% \item \textbf{Data aspect:} Due to technological or biological reasons, the number of RNA nodes and the number of protein nodes will be similar across datasets. Therefore, large datasets mean more cells, which can be handled by mini-batching cells.
% \item \textbf{Model aspect:} In our multimodal transformer module, we employed linearized transformers~\cite{choromanski2021rethinking} with linear space and time complexity, which can be conveniently adapted to large datasets. Concerning GNNs, we incorporated GraphSAGE~\cite{hamilton2017inductive}, whose space and time complexity are related to the number of edges and hidden layer dimensions. Indeed, we can control the number of edges by mini-batching cells and make appropriate adjustments based on available computational resources.
% \end{itemize}




% \subsection{Comparasion of Different Fusion Strategies}

% In our multimodal transformer framework, we utilized GNNs to form bridges between transformers. In each layer, the embeddings of a given modality pass through both its corresponding intra-modal transformer and inter-modal GNNs simultaneously. The updated embeddings are generated by the summation of intra-modal transformer outputs and the incoming inter-modal information. For instance, in the case of protein nodes, information from protein nodes and gene nodes is fused after being processed by protein transformers and gene-protein (gene to protein) GNNs, respectively. Formally, the updates of node embeddings in $\ell$ layer  are achieved by Eq.~\ref{eq:fusion} in section 4.2. After going through $L$ layers of multimodal transformers, the predictions are generated by processing cell embeddings with one more prediction layer. To optimize the whole framework, we adopt an MSE loss to measure the difference between the predictions and the ground-truth values.

% Our framework is based on hybrid fusion. Layer-wise speaking, we refer to our fusion strategy as concurrent fusion since node representations simultaneously go through GNNs and transformers. For instance, in the case of protein nodes, information from protein nodes and gene nodes is fused after being processed by protein transformers and gene-protein (gene to protein) GNNs, respectively. In the following experiment, we compared the performance of layer-wise concurrent fusion, GNN-first fusion, and mixed fusion. Still, in the case of protein nodes, we implemented GNN-first fusion by first taking the summation of protein embeddings and the gene-protein GNNs outputs, and then passing through protein transformers. For mixed fusion, we utilized concurrent fusion on protein and gene nodes while applying GNN-first fusion on cell nodes. We summarize the prediction RMSEs and the standard deviations across five runs in Table~\ref{tab:fusion}.

% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% \begin{table}[h]
% \centering
% \caption{The prediction RMSEs for different Fusion strategies.}\label{tab:fusion}
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{@{}lccc@{}}
% \toprule
%         & Concurrent          & GNN-first           & Mixed               \\ \midrule
% CITE    & \textbf{1.62720}$\pm$\textbf{0.00731} & 1.63054$\pm$0.01018 & 1.63092$\pm$0.00631 \\
% GEX2ADT & \textbf{0.41987}$\pm$\textbf{0.00234} & 0.42861$\pm$0.00094 & 0.42948$\pm$0.00158 \\ \bottomrule
% \end{tabular}
% }
% \end{table}


% As presented in the table, the original concurrent fusion strategy exhibited superior performance compared to the other two fusion strategies. The concurrent setting allows each modality to utilize intra-modal information prior to combining with other modalities, resulting in better performance since each modality holds varying importance for downstream tasks. However, the improvement in performance was not significant enough for the CITE dataset. This observation is consistent with the findings of other experiments. Since the RNA zero rate in the CITE dataset is considerably lower than that of the GEX2ADT dataset, the significance of data-specific information (cell nodes) in the CITE dataset outweighs the importance of other modalities, resulting in a relatively smaller performance boost.

% \section{Discussion}
% \subsection{Why No Cell-Cell Graph and Cell-Protein Graph?}\label{app:remark}
% In Remark of Section~\ref{sec:subgraph}, we provided an explanation for our decision to exclude the cell-cell and cell-protein graphs. Additionally, through our empirical analysis, we observed a decrease in performance on both datasets when these links were incorporated into our heterogeneous graph. In comparison to the original w/o neither graph setting, the performance drop was evident in both the w/ cell-protein graph and w/ cell-cell graph settings, with the former demonstrating a more significant decline. The prediction RMSEs and standard deviations of five runs are summarized in Table~\ref{tab:graph_variants}.

% % Please add the following required packages to your document preamble:
% % \usepackage{booktabs}
% \begin{table}[h]
% \caption{The prediction RMSEs on different graph variants.}\label{tab:graph_variants}
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{@{}lcccc@{}}
% \toprule
%         & w/o neither graph   & w/ cell-cell graph  & w/ cell-protein graph & w/ both graphs      \\ \midrule
% CITE    & \textbf{1.62720}$\pm$\textbf{0.00731} & 1.68932$\pm$0.00751 & 1.71742$\pm$0.01692   & 1.70094$\pm$0.02207 \\
% GEX2ADT & \textbf{0.41987}$\pm$\textbf{0.00234} & 0.42441$\pm$0.00094 & 0.43911$\pm$0.00414   & 0.42983$\pm$0.00231 \\ \bottomrule
% \end{tabular}
% }
% \end{table}

% \noindent \textbf{Why cell-cell graph did not help:} In the above experiment, we constructed a k-NN cell-cell graph by measuring the similarity in gene expression between cells. Nevertheless, since gene expression measurements often suffer from noise and sparsity, this static cell-cell graph may have introduced biased information into the downstream task. To address this issue, we utilized a cell transformer module to learn the dynamic cell-cell interactions via multi-head attention mechanism, which led to improved performance.

% \noindent \textbf{Why cell-protein graph did not help:} To construct the cell-protein links, we incorporated target surface protein levels into edge weights, which is similar to how we built the cell-gene graph. However, these links conveyed information about the prediction targets of the training set, causing the model to overfit easily. Hence, eliminating the cell-protein links served to eradicate information leakage.




% \subsection{Compared to Methods in the NeurIPS Competition}
% The differences between \method{} and the methods better than \method{} in the NeurIPS competition are summarized as follows:
% \begin{itemize}
% % \item \textbf{Small performance gap:} It is worth noting that the performance gap between \method{} and other leading methods is minimal. In the final leaderboard, \method{} obtained a rank of 24, with a difference of 0.0045 in performance compared to the best-performing method. Additionally, the gap in performance between the 10th and 33rd ranked teams was less than 0.001.
% \item \textbf{Ensemble:} Other methods adopted an ensemble learning approach. Specifically, these methods incorporated ensemble strategy that consists of multiple pre-processing pipelines, multiple feature engineering approaches, multi-model stacking and multiple post-processing pipelines. In contrast, \method{} was designed to construct a single, comprehensive model that incorporates multiple modalities into a unified framework. Notably, we achieved comparable performance to these ensemble approaches.
% \item \textbf{Biological insights:} Other methods did not integrate biological prior knowledge into their methodology, or only used it for preprocessing purposes. In contrast, our approach incorporated biological prior through the use of GNNs and transformers, which are able to efficiently capture the complex relationships between biological entities. This allows our method to more effectively leverage the available biological knowledge and improve the accuracy of the predictions.
% \end{itemize}

% \subsection{Future Work}
% % Our work is an extension of the model we implemented in the NeurIPS 2022 competition. 
% Our framework of multimodal transformers with the cross-modality heterogeneous graph goes far beyond the specific downstream task of modality prediction, and there are lots of potentials to be further explored. Our graph contains three types of nodes. while the cell embeddings are used for predictions, the remaining protein embeddings and gene embeddings may be further interpreted for other tasks. The similarities between proteins may show data-specific protein-protein relationships, while the attention matrix of the gene transformer may help to identify marker genes of each cell type. Additionally, we may achieve gene interaction prediction using the attention mechanism.
% % under adequate regulations. 
% % We expect \method{} to be capable of much more than just modality prediction. Note that currently, we fuse information from different transformers with message-passing GNNs. 
% To extend more on transformers, a potential next step is implementing cross-attention cross-modalities. Ideally, all three types of nodes, namely genes, proteins, and cells, would be jointly modeled using a large transformer that includes specific regulations for each modality. 
% The self-attention within each modality would reconstruct the prior interaction network, while the cross-attention between modalities would be supervised by the data observations. Then, The attention matrix will provide insights into all the internal interactions and cross-relationships. With the linearized transformer, this idea would be both practical and versatile.

% \subsection{Broader Impact}
% This work build a transformer based framework \method{} for single-cell multimodal prediction. Our \method{} contributes to the advancement of multimodal single-cell technology by addressing the challenges associated with modeling complex interactions among different omics modalities. By leveraging transformers in an end-to-end manner, \method{} enhances our ability to acquire deeper understanding of cellular states and dynamics that can have broad implications for various fields, including cell biology, genomics, and biomedical research.

% By incorporating single-cell modality prediction task, \method{} helps to study the relationships and regulations between DNA, RNA, and protein, thereby expanding our understanding of the intricate molecular processes within cells. 
% % It enhances our knowledge of gene expression regulation, shedding light on the mechanisms that govern how genetic information is transcribed into RNA molecules and subsequently translated into proteins. 
% This understanding can conceivably provide crucial insights into the fundamental processes that drive cellular functions and development.

% All datasets used in this work are public available. Our work presents no foreseeable risks or potential adverse effects.