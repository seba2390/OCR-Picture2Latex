\section{Experiment}
In this section, we present the experimental results of \method{} against baselines on benchmark datasets. In particular, we aim to answer the following questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1:} How does \method{} perform compare against baselines based on various evaluation metrics?
    \item \textbf{RQ2:} Given various choices of the PEs, how do they affect the performance of \method{}?
    \item \textbf{RQ3:} What is the best way to fuse information across modalities?
    \item \textbf{RQ4:} Can \method{} handle other modality prediction tasks?
    \item \textbf{RQ5:} Can \method{} process large datasets?
    \item \textbf{RQ6:} How does each of the model component impact the performance of \method{}?
\end{itemize}
Before presenting our experimental results and observations, we first introduce the experimental settings.
% We evaluate the ablation of remove transformers.

\subsection{Experimental Settings}
\subsubsection{Datasets}
We follow the setting of the NeurIPS multimodal single-cell integration competition of the year 2021~\cite{luecken2021sandbox} and 2022
% \footnote{https://nips.cc/virtual/2022/competition/50092} 
and collect the joint measurements of gene expression and surface protein levels datasets from the competitions. Both datasets contain the raw counts, which represent the number of reads per gene per cell, as well as the normalized counts. For the NeurIPS 2021 competition, we pick the data corresponding to the task of protein abundance prediction via gene expression and refer to it as ``GEX2ADT''. The processed data is centered and log-transformed for denoising purposes. For the competition in 2022, which we refer to as ``CITE'', the objective is to utilize CITE-seq~\cite{stoeckius2017simultaneous} data measured from days 2, 3, and 4 to predict the protein level on day 7 from different individuals. It is worth mentioning that the protein level testing data is not available during the completion of this work. Therefore, we simulate the competition scenario by treating the training data from day 4 as our testing set. The processed RNA data is centered and log-transformed, while the normalized protein levels are denoised and scaled by background~\cite{kotliarov2020broad}. We summarize the dataset details in Table~\ref{table:dataset} in Appendix~\ref{app:exp}. 
% The experimental results on CBMC are shown in Appendix~\ref{app:cbmc}.
% \input{tables/dataset.tex}

\subsubsection{Baselines}
We evaluate the performance of \method{} against state-of-art multimodal prediction models among the task of using gene expression to predict surface protein levels. The selected baselines are as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Cross-modal Autoencoders}~\cite{yang2021multi}, short for CMAE, incorporated multiple autoencoders to integrate multimodal data and utilized domain knowledge by adding discriminative loss to the training process to align shared markers or clusters.
    \item \textbf{BABEL}~\cite{wu2021babel} proposed a general framework for multimodal translation with modality-specific encoders and decoders. Note that initially, BABEL focused on RNA and ATAC-seq~\cite{buenrostro2013transposition} data. In this evaluation, we repurpose BABEL to the RNA to protein setting.
    \item \textbf{scMM}~\cite{minoura2021mixture} modeled the multimodal data with generative setting. We note that the input of scMM is restricted to raw counts, and the predictions are scaled as centered log-transformed data.
    \item \textbf{ScMoGNN}~\cite{wen2022graph} involved domain knowledge like biological pathways to enhance the GNNs. The original ScMoGNN followed a transductive setting. In this work, we implement an inductive setting of ScMoGNN for a fair comparison with the baselines.
\end{itemize}

\subsubsection{Parameter Setting}
To benchmark the performance of baselines and \method{}, we uniformly employ inductive settings among both datasets. On the CITE, we use the data measured on day 4 for testing and randomly split $80/20\%$ of the data prior to day 4 for training and validation. On the GEX2ADT, we randomly pick $15\%$ of the training data for validation and evaluate the predictions on the testing set. For BABEL, the hidden dimension is tuned from $\{16, 32, 64, 128\}$. For CMAE, the weights of adversarial loss and reconstruction loss are chosen from $\{0.1, 1, 2.5, 5, 10\}$. For scMM, the hidden dimensions are tuned from $\{16, 32, 64, 128\}$. For ScMoGNN, the weight decay parameter of the optimizer is tuned from $\{5\times 10^{-6},1 \times 10^{-5}, 5 \times 10^{-5}, 1 \times 10^{-4}\}$.

\subsection{Evaluation of Predictions}
\input{tables/main_results.tex}
We evaluate the final protein-level prediction performance using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). Meanwhile, because multimodal data usually suffers from the influence of batch effects and unbalanced measuring depth, the count's scale of each cell may vary significantly, which will substantially affect the RMSE and MAE metrics. Therefore, we also include the Pearson correlation coefficient (Corr), which is cell-wise normalized by the mean and variance of the input, as a robust and scale-free metric to evaluate the predictions. A lower RMSE or MAE score indicates a geometrically closer estimation of the protein levels, while a higher Corr score suggests a statistically more similar match to the actual value. We report the mean and the standard deviation of each metric across five different runs, and the results are illustrated in Table~\ref{table:main}. The best performance is highlighted in bold. 

To answer the first question, we note that our \method{} consistently outperforms all other baselines according to all three metrics on both datasets, indicating that \method{} successfully captures the quantitative characteristics of target protein levels given the input gene expression measurements. Particularly, for the CITE, \method{} achieved significantly lower RMSE compared to the second-best model ScMoGNN, by $0.04$. More importantly, \method{} achieved a significant improvement in terms of the Pearson correlation metrics over all other baselines, with a noticeably lower performance variation across runs.
% indicating the stability of our model.

We further analyze the performance of different models on proteins that are least well captured by any models. Specifically, for each model, we compute the RMSE for each protein separately and identify ten proteins that resulted in the highest average RMSE across all models. As shown in Figure~\ref{fig:bot10comp}, \method{} and ScMoGNN achieved relatively stable results and are consistently better compared to BABEl and CMAE.

\begin{figure}[htb]
    \centering
    \vspace{-1.2em}
    \caption{Least well-predicted protein comparison.}
    % \vspace{-1em}
    \includegraphics[width=0.47\textwidth]{images/bot10_perf_comp_boxplot.png}
    \vspace{-1.2em}
    \label{fig:bot10comp}
\end{figure}

\vspace{-0.5em}
\subsection{Positional Encoding}
\label{sec:pe}
As mentioned in Section~\ref{sec:graph_trans}, we implement Laplacian PE~\cite{dwivedi2020generalization} and random walk PE~\cite{dwivedi2022graph} to capture positional information of prior knowledge graph. For ease of notation, we use the abbreviation PE to refer to the PE. To benchmark the impact of the two types of PE among two datasets and answer the second question, we show the performance of \method{} with each PE and compare them with the scenario without any PE. The mean and standard deviation of RMSE scores of five runs are shown in Table~\ref{table:pe}. 

According to the results, the influence of PE varies among datasets. \method{} reaches the best RMSE score on CITE without PE, while two types of PE both improve the performance on GEX2ADT. Notice that in Table~\ref{table:dataset}, the RNA zero rate of CITE is significantly lower compared to the GEX2ADT, providing the model with greater access to data-specific information. If the data contains sufficient information, then the neighborhood information from the GNNs alone is adequate and there is no need for the extra prior knowledge from the PEs. This is further supported by the observation that random walk PE performs better than Laplacian PE in both datasets. The Laplacian PE models global information by using the spectral information of the graph Laplacian, while the random walk PE encodes local information by accessing the landing probability of a $k$-step random walk. In cases where the prior knowledge may be noisy for downstream tasks, the local information alone is enough for predictions and the global structure becomes redundant.

\input{tables/pe.tex}

\subsection{Comparasion of Different Fusion Strategies}

Our framework is based on hybrid fusion. Layer-wise speaking, we refer to our fusion strategy as \textit{concurrent} fusion since node representations simultaneously go through GNNs and transformers. In the case of protein nodes, information from protein nodes and gene nodes is fused after being processed by protein transformers and gene-protein (gene to protein) GNNs, respectively. In the following experiment, we compared the performance of layer-wise \textit{concurrent} fusion, \textit{GNN-first} fusion, and \textit{mixed} fusion. Still, in the case of protein nodes, we implemented \textit{GNN-first} fusion by first summing protein embeddings and the gene-protein GNNs outputs, and then passing through protein transformers. For \textit{mixed} fusion, we utilized \textit{concurrent} fusion on protein and gene nodes while applying \textit{GNN-first} fusion on cell nodes. We summarize the prediction RMSEs and the standard deviations across five runs in Table~\ref{tab:fusion}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\vspace{-0.5em}
\caption{The prediction RMSEs for different Fusion strategies.}\label{tab:fusion}
\vspace{-1.2em}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{@{}lccc@{}}
\toprule
        & Concurrent          & GNN-first           & Mixed               \\ \midrule
CITE    & \textbf{1.62720}$\pm$\textbf{0.00731} & 1.63054$\pm$0.01018 & 1.63092$\pm$0.00631 \\
GEX2ADT & \textbf{0.41987}$\pm$\textbf{0.00234} & 0.42861$\pm$0.00094 & 0.42948$\pm$0.00158 \\ \bottomrule
\end{tabular}
}
\vspace{-1.2em}
\end{table}

As presented in the table, the original \textit{concurrent} fusion strategy exhibited superior performance compared to the other two fusion strategies. The \textit{concurrent} setting allows each modality to utilize intra-modal information prior to combining with other modalities, resulting in better performance since each modality holds varying importance for downstream tasks. However, the improvement in performance was not significant enough for the CITE. This observation is consistent with the findings of other experiments. Since the RNA zero rate in the CITE is considerably lower than that of the GEX2ADT, the significance of data-specific information (cell nodes) in the CITE outweighs the importance of other modalities, resulting in a relatively smaller performance boost.

\subsection{Handling Other Single-Cell Multimodal Prediction Tasks}

In this work, we focused on the specific task GEX2ADT (gene expression to protein levels) as a showcase. However, our framework is versatile and can be applied to other modality prediction tasks, i.e., the other three tasks mentioned in the NeurIPS 2021 competition~\cite{luecken2021sandbox} including ADT2GEX (protein levels to gene expression), GEX2ATAC (gene expression to chromatin accessibility) and ATAC2GEX (chromatin accessibility to gene expression). 
% Specifically, \method{} can be extended to:
% \begin{itemize}

\noindent \textbf{ADT2GEX:} In our framework, we constructed a multimodal heterogeneous graph consisting of gene, protein and cell nodes. For the GEX2ADT task, we removed the cell-protein edges to eliminate information leakage. Similarly, for the ADT2GEX task, we incorporate protein measurements into the cell-protein edges while removing the cell-gene edges. Cell embeddings were initialized by the reduced protein levels, and protein embeddings were initialized by the weighted sum of cell embeddings, where the weights are with the normalized protein levels. %With the same multimodal transformer module and a prediction layer, \method{} can predict gene expression from protein levels.

\noindent \textbf{GEX2ATAC and ATAC2GEX:} In the context of the GEX2ATAC and ATAC2GEX tasks, we removed the protein nodes from the multimodal graph. To initialize the cell embeddings for the GEX2ATAC task, we used the reduced gene expression values, while for gene embeddings, we computed a weighted sum of cell embeddings using normalized gene measurements as weights. For the ATAC2GEX task, we masked the cell-gene edges in the testing set. We initialized the cell embeddings using the reduced ATAC measurements, and the gene embeddings were randomly initialized.
% \end{itemize}

We have compared the performance of \method{} with scMoGNN on the four tasks in Table~\ref{tab:others}. Note that \method{} outperformed scMoGNN in tasks that involve protein modality and vice versa. These results suggest that incorporating prior information of protein nodes can enhance the performance of \method{} when protein modality is present. The RMSE scores across five runs are summarized as in Table~\ref{tab:others}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\vspace{-0.5em}
\caption{Results on other Single-Cell Mutimodal Tasks.}\label{tab:others}
\vspace{-1.2em}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|cccc}
\toprule
           & GEX2ADT             & ADT2GEX             & GEX2ATAC            & ATAC2GEX            \\ \midrule
scMoFormer & \textbf{0.4198}7$\pm$\textbf{0.00234} & \textbf{0.31547}$\pm$\textbf{0.00184} & 0.17885$\pm$0.00008 & 0.23988$\pm$0.00031 \\
scMoGNN    & 0.42576$\pm$0.01180 & 0.32250$\pm$0.00136 & \textbf{0.17823}$\pm$\textbf{0.00011} & \textbf{0.23021}$\pm$\textbf{0.00219} \\ \bottomrule
\end{tabular}
}
\vspace{-1.8em}
\end{table}

\subsection{Training Efficiency of \method{}} \label{sec:efficiency}
Since scMoGNN is the best-performing baseline, we compared the running time and total GPU memory of \method{} and scMoGNN across five runs on one Quadro RTX 8000 GPU. The results are summarized in Table~\ref{tab:efficiency}. We observed that with higher GPU consumption, \method{} required a significantly shorter running time compared to scMoGNN. It is worth noting that \method{} was configured with a relatively large batch size of 8000 cells per batch and a hidden dimension of 512, in order to achieve better performance and better utilization of computing resources. One can surely reduce the required GPU memory of \method{} by limiting the setting accordingly. 

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\centering
\vspace{-0.5em}
\caption{Efficiency Comparison}\label{tab:efficiency}
\vspace{-1.2em}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{@{}l|cc|cc@{}}
\toprule
\multicolumn{1}{l|}{} & \multicolumn{2}{l|}{\textbf{Running time (min)}} & \multicolumn{2}{l}{\textbf{GPU memory (GB)}}         \\ \midrule
                      & CITE                   & GEX2ADT                 & CITE                   & \multicolumn{1}{l}{GEX2ADT} \\
\method{}            & 24.82$\pm$4.16         & 17.95$\pm$3.56          & \multicolumn{1}{r}{38} & 21                          \\
scMoGNN               & 58.89$\pm$7.77         & 108.54$\pm$21.22        & \multicolumn{1}{r}{26} & 12                          \\ \bottomrule
\end{tabular}
}
\vspace{-0.8em}
\end{table}

\noindent \textbf{Can \method{} process large datasets?} In practical applications, it exhibits the capability to process large datasets. We address this issue from the perspectives of both the model and the data:
\begin{itemize}[leftmargin=*]
\item \textbf{Data aspect:} Due to technological or biological reasons, the number of RNA nodes and the number of protein nodes will be similar across datasets. Therefore, large datasets mean more cells, which can be handled by mini-batching cells.
\item \textbf{Model aspect:} In our multimodal transformer module, we employed linearized transformers~\cite{choromanski2021rethinking} with linear space and time complexity, which can be conveniently adapted to large datasets. Concerning GNNs, we incorporated GraphSAGE~\cite{hamilton2017inductive}, whose space and time complexity are related to the number of edges and hidden layer dimensions. Indeed, we can control the number of edges by mini-batching cells and make appropriate adjustments based on available computational resources.
\end{itemize}

\subsection{Ablation Study}
% To have a comprehensive understanding of our propose framework, we perform ablation study to analyze the influence of modules within \method{}. Specifically, we first examine the individual impact of multimodal transformer modules and later demonstrate the performance boost of transformers, equipped with prior knowledge, over GNNs. 
Table~\ref{table:main} demonstrates that models that incorporate domain knowledge perform better in modality prediction compared to those that do not. BABEL, ScMoGNN, and \method{} are the three models that make use of domain knowledge, and they show improved performance compared to the other two models. Among these three models, ScMoGNN, which is based on GNNs, performs better than BABEL, while \method{} outperforms all other models with its combination of transformers and GNNs framework. Given that \method{} includes a multimodal heterogeneous graph and three transformers, this raises the questions: \textit{Why no cell-cell graph and cell-protein graph? Which transformer has the biggest impact on performance? How much do the transformers contribute to the improvement in performance?}

\subsubsection{Exclusion of Cell-Cell Graph and Cell-Protein Graph.}\label{app:remark}
In Remark of Section~\ref{sec:subgraph}, we provided an explanation for our decision to exclude the cell-cell and cell-protein graphs. Additionally, through our empirical analysis, we observed a decrease in performance on both datasets when these links were incorporated into our heterogeneous graph. In comparison to the original w/o neither graph setting, the performance drop was evident in both the w/ cell-protein graph and w/ cell-cell graph settings, with the former demonstrating a more significant decline. The prediction RMSEs and standard deviations of five runs are summarized in Table~\ref{tab:graph_variants}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[h]
\vspace{-0.5em}
\caption{The prediction RMSEs on different graph variants.}\label{tab:graph_variants}
\vspace{-1.2em}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
        & w/o neither graph   & w/ cell-cell graph  & w/ cell-protein graph & w/ both graphs      \\ \midrule
CITE    & \textbf{1.62720}$\pm$\textbf{0.00731} & 1.68932$\pm$0.00751 & 1.71742$\pm$0.01692   & 1.70094$\pm$0.02207 \\
GEX2ADT & \textbf{0.41987}$\pm$\textbf{0.00234} & 0.42441$\pm$0.00094 & 0.43911$\pm$0.00414   & 0.42983$\pm$0.00231 \\ \bottomrule
\end{tabular}
}
\vspace{-1.2em}
\end{table}

\noindent \textbf{Why cell-cell graph did not help:} In the above experiment, we constructed a k-NN cell-cell graph by measuring the similarity in gene expression between cells. Nevertheless, since gene expression measurements often suffer from noise and sparsity, this static cell-cell graph may have introduced biased information into the downstream task. To address this issue, we utilized a cell transformer module to learn the dynamic cell-cell interactions via multi-head attention mechanism, which led to improved performance.

\noindent \textbf{Why cell-protein graph did not help:} For the cell-protein links, we incorporated target surface protein levels into edge weights, similar to how we built the cell-gene graph. However, these links conveys information about the prediction targets of the training set, causing the model to overfit easily. Hence, eliminating the cell-protein links served to eradicate information leakage.

\subsubsection{Influence of Every Transformer} The propose multimodal transformers consist of three different transformers, namely the cell transformer, gene transformer and protein transformers. As our predictions are based on the cell readout, it is expected that each of the three transformers will have different levels of impact on the performance. To quantify the specific impact of a single transformer, we conduct an experiment by removing the other two transformers and measuring the prediction RMSE scores. The results of the evaluation, including the scores of three partial models and the model with no transformers, are summarized in 
% Table~\ref{table:one_trans}.
Figure~\ref{fig:one_trans}.
% \input{tables/one_trans.tex}
\begin{figure}[htb]%
    \vspace{-2.em}
    \centering
    \caption{RMSE$\downarrow$ results of keeping only one Transformer.}%
    \vspace{-1.2em}
    \subfloat[CITE]{{\includegraphics[width=0.5\linewidth]{images/cite_onetrans.png} }}%
    \subfloat[GEX2ADT]{{\includegraphics[width=0.5\linewidth]{images/gex_onetrans.png} }}%
    \label{fig:one_trans}%
    \vspace{-1.8em}
\end{figure}

% \vspace{-1.2em}
The performance of the gene transformer and protein transformer is better than that of the cell transformer in the CITE, while it is the opposite in the GEX2ADT. This can be explained by the difference in RNA zero rate between the two datasets, as shown in Table~\ref{table:dataset}. For the GEX2ADT, the high RNA zero rate means less information, making the cell transformer crucial in increasing performance by drawing more information from the data. On the other hand, the CITE has a lower zero rate, meaning it provides more information, allowing the gene transformer and protein transformer to enhance the model by adding external biological knowledge.

\subsubsection{How to Utilize Prior Knowledge} To answer the third question, we compare \method{} with two GNN-based models in Table~\ref{table:ablation}. The model "GNN-prior" refers to the GNNs that built on the same graph in Section~\ref{sec:graph_const}, while the "GNN" model is constructed using only the cell-gene graph without incorporating any prior information. The results show that the incorporation of prior knowledge into the graph results in a slight performance boost in both datasets. However, when multimodal transformers are included, the performance improvement is much more pronounced. This highlights the usefulness of prior knowledge and the importance of transformers to effectively incorporate this information into the model.
\input{tables/ablation.tex}








