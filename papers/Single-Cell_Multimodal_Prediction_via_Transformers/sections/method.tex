\section{Our Multimodal transformer Framework}
% \vspace{-0.5em}

% \jt{I think we need to improve the logic and the representation of this section. We have to motivate each subsection. For example why we need a multimodal heterogeneous graph. Then although our method is based on existing transformer, we have to embed it to our context and consider it as our method. I would suggest you draw a figure to introduce the whole framework with the major components and then introduce each component.}
We now introduce the proposed multimodal framework \method{}. An shown in Figure~\ref{fig:framework}, it consists of multimodal graph construction, a multimodal transformer, and a prediction layer. In brief, we first construct a heterogeneous graph that contains cell, gene, and protein nodes together with their interactions. We then utilize multiple (graph) transformers to extract rich cell representations and predict each cell's surface protein abundance levels. % In the following subsections, we will detail these key components.  

\begin{figure*}[t]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.8\linewidth]{images/method/model.png}
    \vspace{-1.5em}
    \caption{An illustration of \method{}. In this framework, three important components are included: graph construction, multimodal transformer, and prediction layer.}
    \label{fig:framework}
    \vspace{-1.5em}
\end{figure*}


\vspace{-0.4em}
\subsection{Multimodal Graph Construction}
\label{sec:graph_const}
% \vspace{-0.5em}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In multimodal single-cell data, multiple types of data is simultaneously measured within each cell. Naturally, it brings heterogeneity into the data. As for gene expression and protein abundance data, the cells, proteins and genes represent distinct node types and the connections between them show diverse biological meanings. Thus, to simultaneously model the molecule interactions and cell-cell relationships, we construct a multimodal heterogeneous graph containing genes, proteins and cells. The graph construction process is illustrated in Figure~\ref{}.

% Before we present the graph structure, we first introduce the biological network we use in our framework. The STRING~\cite{szklarczyk2023string} database provides a comprehensive resource of protein-protein interactions and the functional relationships between different proteins. In addition to protein-protein information, the STRING database also incorporates information from other sources such as genomic context, including conserved neighborhood, co-expression, gene fusion, and co-occurrence. The database enables mapping proteins to genes via functional genomics data collection (ArrayExpress)~\cite{athar2019arrayexpress}, which provides the desired biological prior of gene-gene and protein-protein interactions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this subsection, we introduce how we construct the graph for multimodal single-cell data. Specifically, we construct a heterogeneous graph containing three different types of nodes. It has four subgraphs as shown in Figure~\ref{fig:framework}, i.e., a protein-protein graph, a gene-gene graph, a gene-protein graph, and a cell-gene graph. Next, we detail how to conduct these subgraphs. 
% i.e., a cell-gene graph, a gene-gene graph, a protein-protein graph and a gene-pretain graph. Next, we will detail how to conduct these subgraphs. \jt{please follow the order of these four subgraphs to introduce how to construct them b}

% \vspace{-1em}
\subsubsection{Subgraph Construction}\label{sec:subgraph} 
% Before we present the construction of the multimodal heterogeneous graph, We first describe how we build the subgraphs within modalities and between modalities.

\noindent \textbf{Protein-Protein Graph.} 
%We start our graph with protein nodes. 
To integrate prior biological information into protein-protein graph, we refer to the STRING~\cite{szklarczyk2023string} database. STRING provides a comprehensive resource of protein-protein interactions and the functional relationships between different proteins. It contains seven different channels covering varied aspects of sources, including genomic context, experiment results, and text mining efforts. We use the combined confidence score of all channels as edge weights to comprehensively enhance the graph. The proteins in STRING are labeled in Ensemble~\cite{cunningham2022ensembl} protein (ENSP), and the mapping from ENSP to the protein preferred name is provided in additional information resource. Notice that regularly one protein may have multiple aliases. To match the protein display names with ENSP, we utilize GeneCards~\cite{stelzer2016genecards} to find all possible aliases of our target proteins. In a nutshell, the mapping from STRING nodes to our target proteins is given by ENSP $\shortrightarrow$ possible aliases of proteins $\shortrightarrow$ the display names of proteins within the dataset.


\noindent\textbf{Gene-Gene Graph.} To maintain consistency and eliminate potential noise from multiple sources of prior information, we also utilized the STRING database to construct the gene-gene graph and combined all seven channels of the STRING database to enhance the graph as much as possible. Note that although the gene and protein nodes are biologically equivalent in the sense of prior information, we process them separately to more specifically handle data-specific information related to the target labels. The genes are labeled in Ensemble gene (ENSG) and additional matching efforts are needed to form the gene-gene graph. We utilize the MyGeneInfo~\cite{wu2014mygene} gene query service to align the STRING protein nodes, encoded by ENSP, with the input gene nodes, encoded by ENSG.


\noindent\textbf{Gene-Protein Graph.} Now we have two separate graphs among proteins and genes respectively, and we would like to form a general frame by adding the connections between genes and proteins. Following the central dogma of molecular biology, information flows from RNA to proteins via translation. This encoding relationship is recorded within the gene and protein nomenclature. Specifically, the gene names from existing single-cell multimodal datasets contain two parts: the ENSG and the symbol or name of corresponding proteins. That is, if a gene and a protein share the same symbol, it means the target protein is encoded by the gene. Utilizing biological information, we link the proteins to genes by matching their symbols. 


\noindent\textbf{Cell-Gene Graph.} The constructed gene-protein graph is fully based on prior knowledge, and we integrate data-based information into the multimodal heterogeneous graph by involving cell nodes. The gene expression counts of multimodality data imply the data-specific relationships between genes and cells. Naturally, a cell and a gene are connected if the gene expresses within that cell. Note that the number of genes detected within each cell varies significantly, which depicts that the raw counts of RNA abundance show substantial heterogeneity among cells. In addition, the raw data includes extremely large counts, making it impractical to apply counts data as the edge weights between cells and genes. Thus, we normalize each cell by total counts over all genes and take the logarithm. 
%, i.e., library size normalization and centered log-transformation. 
The processed data is then fed into the multimodal graph to describe the cell-gene links.



\noindent\textbf{Remark.} 
%In the above discussion, we have introduced four types of subgraphs, i.e., a protein-protein graph, a gene-gene graph, a gene-protein graph, and a cell-gene graph. 
It is worth noting that we do not construct the cell-cell graph. Instead, we use the transformer to learn the cell-cell relationships via the attention mechanism. This is in contrast with some previous studies that utilize a static cell-cell similarity graph generated from the input features, which might be prone to inaccurate cell relationships due to the noisy nature of single-cell data. On the other hand, we do not explicitly establish connections between the cells and the target protein nodes, as it might cause the model to easily overfit. More discussions are included in Appendix~\ref{app:remark}.
%\wei{For the protein-cell graph, ..?}


% \jt{In particular, the gene nodes and the protein nodes correspond to the input features and the target labels of the cross-modality prediction task, while the cell nodes represent individual samples. We model the cell-gene connections naturally as the gene expression matrix defined by the input features. On the other hand, because biological systems are highly complex, where many groups of genes operate in conjunction to carry out specific biological processes or pathways, incorporating prior knowledge about gene or protein interactions is beneficial for predicting downstream phenotypic measurements, such as the surface protein abundance in our work. Several efforts have been put into compiling comprehensive genome-wide interaction information. In this work, we use the STRING~\cite{szklarczyk2023string} network as the basis for the gene/protein interactions, which combines various protein interaction evidence, for example, genomic contexts, high throughput experiments, and text-mined relationships from the literature. Finally, we utilize the MyGeneInfo~\cite{wu2014mygene} gene query service to align the STRING protein nodes, encoded by Ensembl~\cite{birney2004overview} protein (ENSP), with the input gene nodes (Ensembl gene, ENSG) and the target nodes (protein names). The above construction leads to a heterogeneous cell-gene-protein graph that we use in our model. }


% nodes. Specifically, we have three different types of nodes, i.e., protein nodes, gene nodes and cell nodes. A cell node is connected to gene nodes if the corresponding genes exist in this cell, and the edge weights are represented as the gene abundance. The gene-gene and protein-protein connections are given by the STRING database. While STRING contains seven different channels covering varied aspects of sources,  

% \vspace{-1.5em}
\subsubsection{Heterogeneous Graph Construction} %With the aforementioned subgraphs, we are now ready to explain how we construct the multimodal heterogeneous graph.
% \vspace{-0.2em}

\noindent\textbf{Formal Graph Definition.} We now formally define the multimodal heterogeneous graph. Let $\mathbf{A}$ be the adjacency matrix of the multimodal heterogeneous graph with $\mathcal{V} = \left(\mathbf{v}_{\text{p}}, \mathbf{v}_{\text{g}}, \mathbf{v}_{\text{c}} \right)$ as the node set, %where we have
% \begin{equation}
%     \begin{split}
%         & \mathbf{v}_{\text{p}} = \left(v_{\text{p}}^1, v_{\text{p}}^2, \dots, v_{\text{p}}^{N_{\text{p}}} \right), \\ 
%         & \mathbf{v}_{\text{g}} = \left(v_{\text{g}}^1, v_{\text{g}}^2, \dots, v_{\text{g}}^{N_{\text{g}}} \right), \\
%         & \mathbf{v}_{\text{c}} = \left(v_{\text{c}}^1, v_{\text{c}}^2, \dots, v_{\text{c}}^{N_{\text{c}}} \right),
%     \end{split}
% \end{equation}
and $N_{\text{p}}$, $N_{\text{g}}$, $N_{\text{c}}$ are the number of proteins, genes, cells, respectively. Denote $\mathcal{E}\in \mathcal{V} \times \mathcal{V}$ as the edge set, we write the multimodal heterogeneous graph as $\mathcal{G} = \left(\mathcal{V}, \mathcal{E} \right)$. Let $N =(N_{\text{p}}+N_{\text{g}}+N_{\text{c}})$ be the total number of nodes, the adjacency matrix $\mathbf{A}$ can be written as follows:
\begin{equation}
    \mathbf{A}=\left(\begin{array}{ccc}
        \mathbf{A}_{\text{p}} & \mathbf{S}^T & \mathbf{0} \\
        \mathbf{S} & \mathbf{A}_{\text{g}} & \mathbf{A}_\text{RNA}^T \\
        \mathbf{0} & \mathbf{A}_\text{RNA} & \mathbf{0} \\
    \end{array}\right) \in \mathbb{R}^{N\times N},
\end{equation}
where $\mathbf{A}_{\text{p}} \in \mathbb{R}^{N_{\text{p}}\times N_{\text{p}}}$ is the protein-protein interaction graph; $\mathbf{A}_{\text{g}} \in \mathbb{R}^{N_{\text{g}}\times N_{\text{g}}}$ denotes the gene-gene graphs mapped from STRING via MyGene.info~\cite{lelong2022biothings}; $\mathbf{S} \in \mathbb{R}^{N_{\text{g}}\times N_{\text{p}}}$ is the encoding relationship between genes and proteins; and $\mathbf{A}_\text{RNA} \in \mathbb{R}^{N_{\text{c}}\times N_{\text{g}}}$ represents the gene expression. 
% For the node features initialization, we please refer the readers to Appendix~\ref{app:node}.

\noindent\textbf{Node Feature Initialization.} With the graph structure, next we discuss how to initialize the node features. 
Since the RNA counts $\mathbf{X}_\text{g}$ show drastic sparsity along with high dimension, it is not practical to be directly used as cell features. Hence, we first denoise the data and reduce the dimension by Singular Value Decomposition (SVD). To alleviate the effects of cell-to-cell heterogeneity and extremely large counts, we conduct library size normalization and centered log-transformation. The preprocessed data $\bar{\mathbf{X}}$ is then passed through the SVD algorithm. Cell features are initialized with the reduced features $\mathbf{h}^0_{\text{c}} \in \mathbb{R}^{N_{\text{c}}\times d_0}$. We then initialize gene features by the weighted sum of the reduced features $\mathbf{h}^0_{\text{g}} = \bar{\mathbf{X}}_\text{g}^T \cdot \mathbf{h}^0_{\text{c}} \in \mathbb{R}^{N_{\text{g}}\times d_0}$ with the normalized counts $\bar{\mathbf{X}}_\text{g}$ as the weights. In the studied problem, proteins are the target modality for prediction; thus they are initialized randomly based on their indices.

% \vspace{-1.3em}
\subsection{Multimodal Transformers}
% \vspace{-0.5em}
% The proposed \method{} seeks to address the challenge of heterogeneity in the multimodal heterogeneous graph consisting of distinct modalities: cells, genes, and proteins. 
To effectively handle the heterogeneity in the heterogeneous graph, we will introduce how \method{}  employs multiple transformers to process each individual modality and utilizes a cross-modality aggregation mechanism to combine the information.
% The information obtained from these individual transformers is then aggregated through a cross-modality aggregation mechanism. % Subsequently, we will present the transformers assigned to each modality and elaborate on how to coherently integrate them.

%Since there are three modalities in the multimodal heterogeneous graph, i.e., cells, genes, and proteins, a single transformer will fall short in dealing with such heterogeneity. In the proposed \method{}, we use multiple transformers to deal with different data modalities and adapt a cross-modality aggregation to aggregate the information from these transformers.  Next, we introduce the transformer corresponding to each modality and then discuss how to coherently integrate them. 
% To raise the scalability of our \method{} framework and integrate STRING~\cite{szklarczyk2023string} information into the transformers, we apply Performer~\cite{choromanski2021rethinking} to cells and GraphGPS~\cite{rampasek2022recipe} to genes and proteins. Random walk positional encoding~\cite{dwivedi2022graph} is introduced to genes and proteins to encode positional information.

% \vspace{-0.5em}
\subsubsection{Cell Transformer}
Transformer~\cite{vaswani2017attention} has made significant achievements in the field of Natural Language Processing (NLP) in recent years. The attention mechanism can capture high-order and non-Euclidean connections between nodes, which is desired to explore the cell-cell relationships within single-cell multimodal data. We denote the queries, keys, and input cell embeddings as $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{h}_{\text{c}} \in \mathbb{R}^{N_{\text{c}}\times d}$ with input dimension $d$, 
% \jt{what are n and d here I think the number of cells has been defined before???}. 
the scaled dot-product attention can be formulated as $\operatorname{Attn}(\mathbf{h}_\text{c})=\operatorname{softmax}\left({\mathbf{Q} \mathbf{K}^{T}}/{\sqrt{d}}\right) \mathbf{h}_\text{c}$,
% \begin{equation}
%     \operatorname{Attn}(\mathbf{h}_\text{c})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}}{\sqrt{d}}\right) \mathbf{h}_\text{c},
% \end{equation}
where $\mathbf{A}_{\text{attn}} = \operatorname{softmax}\left({\mathbf{Q} \mathbf{K}^{T}}/{\sqrt{d}}\right)$ is the attention matrix of cell-cell interactions. Despite being effective in various NLP tasks, the original transformer has limitations in scalability due to its relatively high space complexity of $O(N_\text{c}^2 + N_\text{c} d)$ and time complexity of $O(N_\text{c}^2Â·d)$. This issue considerably limits the application of transformers in single-cell multimodal analysis. Since typically multimodal data includes tens of thousands of cells, it is not applicable to implement the original transformer directly on cells. 

To address the scalability issue, we employ generalized kernelizable attention~\cite{choromanski2021rethinking} as a computationally efficient approximation of traditional attention. The attention blocks is kernelized in the form $\mathbf{A}(i, j)=\mathrm{K}\left(\mathbf{q}_{i}^{\top}, \mathbf{k}_{j}^{\top}\right)$, where $\mathbf{q}_{i}$ stands for the $i^{t h}$ row of query $\mathbf{Q}$ and $\mathbf{k}_{j}$ denotes the $j^{t h}$ row of key $\mathbf{K}$. The kernel $\mathrm{K}: \mathbb{R}^{d} \times \mathbb{R}^{d} \rightarrow \mathbb{R}_{+}$ is specified as $\mathrm{K}(\mathbf{x}, \mathbf{y})=\mathbb{E}\left[\phi(\mathbf{x})^{\top} \phi(\mathbf{y})\right]$,
% \begin{equation}
%     \mathrm{K}(\mathbf{x}, \mathbf{y})=\mathbb{E}\left[\phi(\mathbf{x})^{\top} \phi(\mathbf{y})\right],
% \end{equation}
where $\phi: \mathbb{R}^{d} \rightarrow \mathbb{R}_{+}^{r}$ denotes the feature mapping. Let $\mathbf{Q}^{\prime}$, $\mathbf{K}^{\prime} \in \mathbb{R}^{N_\text{c} \times r}$ be the approximate query and key with rows given as $\phi\left(\mathbf{q}_{i}^{\top}\right)^{\top}$ and $\phi\left(\mathbf{k}_{i}^{\top}\right)^{\top}$ respectively, the kernel approximation of cell attention is formulated as
\begin{equation}
    \resizebox{0.45\textwidth}{!}{$
    \begin{split}
    \operatorname{Attn}(\mathbf{h}_\text{c})=\widehat{\mathbf{D}}^{-1}\left(\mathbf{Q}^{\prime}\left(\left(\mathbf{K}^{\prime}\right)^{\top} \mathbf{h}_\text{c}\right)\right),   \; \text{with}\; \widehat{\mathbf{D}}=\operatorname{diag}\left(\mathbf{Q}^{\prime}\left(\left(\mathbf{K}^{\prime}\right)^{\top} \mathbf{1}_{N_\text{c}}\right)\right).
    \end{split}
    $}
\end{equation}
%The $\mathbf{1}_{n}$ denotes the all-ones vector of dimension $n$
With kernel of dimension $r$, the space complexity and time complexity is reduced to $O(N_\text{c} r+N_\text{c} d+r d)$ and $O(N_\text{c} r d)$, respectively. The generalized kernelizable attention boosts our cell transformer to linear space and time complexity, while still delivering results comparable to regular transformers~\cite{choromanski2021rethinking}. The attention mechanism in the model captures the intricate relationships between cells, resulting in an improved representation of the individual cells. 

\subsubsection{Gene Transformer and Protein Transformer}
\label{sec:graph_trans}
 
To leverage biological insights, we include STRING~\cite{szklarczyk2023string} as an addition to provide local information of genes and proteins. Although STRING provides solid prior networks, there may still exist data-related concerns when applied to sequencing data. For instance, the NeurIPS 2022 competition dataset
\footnote{https://www.kaggle.com/competitions/open-problems-multimodal/} 
contains $22,050$ genes, but only $13,101$ of these genes have interactions recorded in the STRING database. This issue also occurs in proteins, as the NeurIPS 2022 competition dataset collects $140$ proteins and only $120$ of them are found within the STRING~\cite{szklarczyk2023string} networks. This means that information about interactions within the remaining molecules is not available unless additional global information is included. To address the concerns, we utilized graph transformers to encode both the local and global information about genes and proteins. Specifically, following GraphGPS~\cite{rampasek2022recipe}, we adapt the graph transformers for gene-gene and protein-protein graphs separately, i.e., gene transformer and protein transformer. 
% \jt{instead introduce a new concept of molular, we just use one of them to illustate as shown by the following sentence. please adjust the following paragraphs accordningly. I adjust some but not all. }
Since the gene transformer and protein transformer have the same architecture, we will use the gene transformer to illustrate the details. 

A gene transformer layer consists of two parallel components: a message-passing GNN block and a global attention block. The GNN block subtracts the gene interaction information from the prior local network, while the attention block learns global data-specific relationships by allowing each gene to attend to all other genes. The results from two blocks are summed together and then processed by fully connected layers to update the gene embeddings. 
% \jt{I think we have to follow the notations we define early. please check and adjust??} 
Recall that the adjacency matrix of the gene-gene graph is denoted as $\mathbf{A}_{\text{g}} \in \mathbb{R}^{N_{\text{g}}\times N_{\text{g}}}$, let $\mathbf{h}_g^{\ell} \in \mathbb{R}^{N_{\text{g}} \times d_{\ell}}$ with dimension $d_{\ell}$ be the gene embedding of $\ell$-th layer, the update functions are as follows: 
% \begin{equation}
%     \begin{split}
%         \mathbf{h}_{{\text{g}},M}^{\ell+1} & =\operatorname{GNN}^{\ell}\left(\mathbf{h}_{\text{g}}^{\ell}, \mathbf{A}_{\text{g}}\right), \\
%         \mathbf{h}_{{\text{g}},T}^{\ell+1} & =\operatorname{Attn}^{\ell}\left(\mathbf{h}_{\text{g}}^{\ell}\right), \\
%         \mathbf{h}_{\text{g}}^{\ell+1} & =\operatorname{MLP}^{\ell}\left(\mathbf{h}_{{\text{g}},M}^{\ell+1}+\mathbf{h}_{{\text{g}},T}^{\ell+1}\right), 
%     \end{split}
% \end{equation}
\begin{equation}
    \resizebox{0.45\textwidth}{!}{$
    \begin{split}
      \mathbf{h}_{\text{g}}^{\ell+1}  =\operatorname{MLP}^{\ell}\left(\mathbf{h}_{{\text{g}},M}^{\ell+1}+\mathbf{h}_{{\text{g}},T}^{\ell+1}\right), \; \text{with} \; \mathbf{h}_{{\text{g}},M}^{\ell+1} =\operatorname{GNN}^{\ell}\left(\mathbf{h}_{\text{g}}^{\ell}, \mathbf{A}_{\text{g}}\right), 
        \mathbf{h}_{{\text{g}},T}^{\ell+1} =\operatorname{Attn}^{\ell}\left(\mathbf{h}_{\text{g}}^{\ell}\right), 
    \end{split}
    $}
\end{equation}
where $\operatorname{GNN}^{\ell}$ and $\operatorname{Attn}^{\ell}$ represent the GNN block and the global attention mechanism; $\operatorname{MLP}^{\ell}$ is a $2$-layer MLP block. The GNN block brings in prior biological insights, while the attention block allows resolving the expressivity bottlenecks caused by over-smoothing~\cite{kreuzer2021rethinking} and over-squashing~\cite{alon2021on}.


% \jt{ please adjust this paragraph. Previous version you introduce molavulr to unitfy gene and proteins but we use gene to illustrate. Please adjust the paragraph} 
Positional encoding (PE) is applied to the gene transformer layer to provide another solution to the expressivity bottlenecks among gene-gene graphs. Message-passing GNNs update gene node embeddings by aggregating local neighborhood representation given by gene knowledge graphs. By incorporating additional positional information, PE helps to differentiate nodes that have the same local surroundings but distinct positions. Accompanied by prior gene-gene interaction networks, PE distinguishes genes by the absolute position of them within the STRING network. This is important from a biological perspective as each gene functions differently. Here, we implement Laplacian PE~\cite{dwivedi2020generalization} and random walk PE~\cite{dwivedi2022graph}. The Laplacian PE captures the spectral information of graph Laplacian by its eigenvectors. Denote the graph Laplacian of input gene graph as $\mathbf{L}_{\text{g}}$, the matrix factorization of $\mathbf{L}_{\text{g}}$ is formulated as
\begin{equation}
    \mathbf{L}_{\text{g}}=\mathbf{I}-\mathbf{D}_{\text{g}}^{-1 / 2} \mathbf{A}_{\text{g}} \mathbf{D}_{\text{g}}^{-1 / 2}=\mathbf{U}_{\text{g}}^{T} \mathbf{\Lambda}_{\text{g}} \mathbf{U}_{\text{g}},
\end{equation}
where $\mathbf{D}_{\text{g}}$ is the degree matrix of gene graph, $\mathbf{\Lambda}_{\text{g}}$ and $\mathbf{U}_{\text{g}}$ correspond to the eigenvalues and eigenvectors respectively. The gene node Laplacian PE of dimension $k$ is defined as the $k$ smallest non-trivial eigenvectors. While Laplacian PE embeds the positional information from the graph Laplacian, the random walk PE tends to grasp the positional information given by the graph clusters. The random walk PE of dimension $k$ is defined with $k$-steps of the random walk
\begin{equation}
    \mathbf{p}_{i}=\left[\mathrm{RW}_{i i}, \mathrm{RW}_{i i}^{2}, \cdots, \mathrm{RW}_{i i}^{k}\right] \in \mathbb{R}^{k},
\end{equation}
where $\mathrm{RW}=\mathbf{A}_{\text{g}} \mathbf{D}_{\text{g}}^{-1}$ is the random walk operator. The term $\mathrm{RW}_{i i}^k$ represents the landing probability of a gene node $i$ to itself after $k$ steps. The processed PE is then combined to gene features through a fully connected layer.

\subsubsection{Cross-Modality Aggregation}
 Transformers are constructed separately for each modality. To build a bridge among those transformers, we implement message passing GNNs among the links which connect nodes of distinct types. The information from cell transformer will denoise the prior knowledge by adding data-specific details into gene embeddings and protein embeddings. Meanwhile, information from gene transformer and protein transformer will bring in biological insights to cell transformer to predict the target proteins. Particularly, we take the advantage of GraphSAGE~\cite{hamilton2017inductive} to transfer the information. Denote the $i$-th destination node as $v_i$ and $j$-th source node as $u_j$.
% node type as "dst" and  "src"its  Let the source node type be and its node be $u$. 
The information from the source nodes to the destination nodes is updated by:
\begin{equation}
    \resizebox{0.45\textwidth}{!}{$
    % \begin{split}
         \mathbf{h}_{v_i}^{(\ell+1)} = \operatorname{Update}\left(\mathbf{h}^{\ell}_{v_i}, \mathbf{h}^{\ell}_{\mathcal{N}(v_i)} \right) \; \text{with} \; \mathbf{h}_{\mathcal{N}(v_i)}^{\ell} = \operatorname{Aggregate}\left(\left\{\mathbf{h}_{u_j}^{\ell}, \forall\ u_j \in \mathcal{N}(v_i)\right\}\right) 
        % \mathbf{h}_{\text{dst},i}^{(\ell+1)}&= \operatorname{Norm}\left(\mathbf{h}_{\text{dst},i}^{(\ell+1)}\right).
    % \end{split}
    $}
\end{equation}
For node $v_i$, the message passing GNN aggregates information from its neighbors through aggregator function ($\operatorname{Aggregate}$). 
% weighted by edge weight $e_{i,j}$. 
The neighborhood information $\mathbf{h}_{\mathcal{N}(v_i)}^{\ell}$ is then combined to the embeddings $\mathbf{h}_{v_i}^{\ell}$ and processed by an updating procedure. The newly generated embeddings are normalized before next iteration. The GNN modules facilitate communication between the transformers, enabling the transformers to leverage various forms of information during the training process.

Now we summarize the workflow of \method{}. As shown in Figure\ref{fig:framework}, we apply transformers within each type of node and utilize message passing GNNs to form the bridges between transformers. In a formal way, for $\ell$-th layer, denote the cell transformer as $\operatorname{Trans}^{\ell}_{\text{c}}$, gene and protein graph transformer as $\operatorname{GT}^{\ell}_{\text{g}}$ and $\operatorname{GT}^{\ell}_{\text{p}}$ respectively. Let $\operatorname{MPG}^{\ell}_{\text{g} \shortrightarrow \text{p}}$ and $\operatorname{MPG}^{\ell}_{\text{p} \shortrightarrow \text{g}}$ be the message passing GNN modules between genes and proteins, and $\operatorname{MPG}^{\ell}_{\text{g} \shortrightarrow \text{c}}$ and $\operatorname{MPG}^{\ell}_{\text{c} \shortrightarrow \text{g}}$ are for the links between genes and cells. We process the cell embeddings with MLP as cell readouts, where $\operatorname{FC}^{\ell}$ represents the $\ell$-th fully connected layer. The updates of node embeddings are achieved by 
\begin{equation}\label{eq:fusion}
    \begin{split}
        \mathbf{h}_{\text{g}}^{\ell+1} =& \operatorname{GT}_{\text{g}}^{\ell}\left(\mathbf{h}_{\text{g}}^{\ell}, \mathbf{A}_{\text{g}}\right)  + \operatorname{MPG}^{\ell}_{\text{p} \shortrightarrow \text{g}}\left(\mathbf{h}_{\text{p}}^{\ell}, \mathbf{S}^T\right) + \operatorname{MPG}^{\ell}_{\text{c} \shortrightarrow \text{g}}\left(\mathbf{h}_{\text{c}}^{\ell}, \mathbf{A}_{\text{RNA}}\right), \\
        \mathbf{h}_{\text{c}}^{\ell+1} =& \operatorname{Trans}^{\ell}_{\text{c}}\left(\mathbf{h}_{\text{c}}^{\ell}\right) + \operatorname{MPG}^{\ell}_{\text{g} \shortrightarrow \text{c}}\left(\mathbf{h}_{\text{g}}^{\ell}, \mathbf{A}_{\text{RNA}}^T\right) + \operatorname{FC}^{\ell}\left(\mathbf{h}_{\text{c}}^{\ell}\right), \\
        \mathbf{h}_{\text{p}}^{\ell+1} =& \operatorname{GT}_{\text{p}}^{\ell}\left(\mathbf{h}_{\text{p}}^{\ell}, \mathbf{A}_{\text{p}}\right) + \operatorname{MPG}^{\ell}_{\text{g} \shortrightarrow \text{p}}\left(\mathbf{h}_{\text{g}}^{\ell}, \mathbf{S}\right), 
    \end{split}
\end{equation}
where $\mathbf{S}$, $\mathbf{A}_{\text{p}}$, $\mathbf{A}_{\text{g}}$, $\mathbf{A}_{\text{RNA}}$ are adjacency blocks defined in Section~\ref{sec:graph_const}. 

\textbf{Prediction Layer.}
With the number of layers be $L$, the predictions are given by one extra full connected layer $\operatorname{FC}^{L +1}$ as
\begin{equation}
    \begin{split}
      \widehat{\mathbf{X}}_{\text{p}} = \operatorname{FC}^{L + 1}\left(\mathbf{h}_{\text{c}}^{L+1} \right), \; \text{with} \;  \mathbf{h}_{\text{c}}^{L+1} = \operatorname{Concat}\left(\mathbf{h}_{\text{c}}^{\ell},\ \ell\ \text{in}\ \{1, 2, \dots, L\} \right),
    \end{split}
\end{equation}
where $\widehat{\mathbf{X}}_{\text{p}}\in \mathbb{R}^{N_{\text{c}}\times N_{\text{p}}}$ is the final prediction. To optimize the framework, we adapt a Mean Square Error (MSE) loss to measure the difference between the predictions and the ground-truth values:
\begin{equation}
    \mathcal{L} \left(\widehat{\mathbf{X}}_{\text{p}}, \mathbf{X}_{\text{p}}\right) = \frac{1}{N_p N_c} \sum_{i=1}^{N_p}\sum_{i=1}^{N_c}\left(\mathbf{X}_{\text{p}}^{ij}-\widehat{\mathbf{X}}_{\text{p}}^{ij}\right)^{2} .
\end{equation}