%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{cite}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[normalem]{ulem}
\usepackage{balance}

\newcommand{\eg}{\emph{e.g.},}
\newcommand{\ie}{\emph{i.e.},}
\newcommand{\etal}{\emph{et~al.}}
\def\tabref#1{Table~\ref{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BIBLIOGRAPHY

% \usepackage[style=ieee]{biblatex}
% \renewcommand*{\bibfont}{\footnotesize}
% \addbibresource{references.bib}
\newcommand{\milad}[1]{{\color{red}#1}}
\newcommand{\filip}[1]{{\color{green}#1}}

% \DeclareSourcemap{
%   \maps{
%     \map{
%       \pertype{article}
%       \step[fieldset=language, null]
%       \step[fieldset=url, null]
%       \step[fieldset=doi, null]
%       \step[fieldset=issn, null]
%       \step[fieldset=isbn, null]
%       \step[fieldset=note, null]
%       \step[fieldset=editor, null]
%       \step[fieldset=urldate, null]
%       \step[fieldset=file, null]
%     }
%   }
% }
% \DeclareSourcemap{
%   \maps{
%     \map{
%       \pertype{inproceedings}
%       \step[fieldset=language, null]
%       \step[fieldset=url, null]
%       \step[fieldset=doi, null]
%       \step[fieldset=issn, null]
%       \step[fieldset=isbn, null]
%       \step[fieldset=note, null]
%       \step[fieldset=editor, null]
%       \step[fieldset=urldate, null]
%       \step[fieldset=file, null]
%     }
%   }
% }
% \DeclareSourcemap{
%   \maps{
%     \map{
%       \pertype{incollection}
%       \step[fieldset=language, null]
%       \step[fieldset=url, null]
%       \step[fieldset=doi, null]
%       \step[fieldset=issn, null]
%       \step[fieldset=isbn, null]
%       \step[fieldset=note, null]
%       \step[fieldset=editor, null]
%       \step[fieldset=urldate, null]
%       \step[fieldset=file, null]
%     }
%   }
% }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\LARGE \bf
Demonstrating Autonomous 3D Path Planning on \\ a Novel Scalable UGV-UAV Morphing Robot
}
% Suggestion: Toward Autonomous Navigation of a Novel Multi-Modal Morphing Robot

\author{Eric Sihite$^{1}$, Filip Slezak$^{1}$, Ioannis Mandralis$^{1}$, Adarsh Salagame$^{2}$, Milad Ramezani$^{3}$, \\ Arash Kalantari$^{4}$, Alireza Ramezani$^{2*}$, and Morteza Gharib$^{1}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$Authors are with the Department of Aerospace Engineering, California Institute of Technology, Pasadena, USA. Emails: 
        {\tt\small esihite, fslezak, imandralis, mgharib@caltech.edu}}%
\thanks{$^{2}$Authors are with the Silicon Synapse Labs, Department of Electrical and Computer Engineering, Northeastern University, Boston, USA. Emails: 
        {\tt\small salagame.a, a.ramezani@northeastern.edu}}%
\thanks{$^{3}$Author is with the Robotics and Autonomous Systems, DATA61, CSIRO, Brisbane, QLD 4069, Australia. Email: 
        {\tt\small milad.ramezani@data61.csiro.au}}%
\thanks{$^{4}$Author is with the Jet Propulsion Laboratory, Pasadena, USA. Email: 
        {\tt\small arash.kalantari@jpl.nasa.gov}}%
\thanks{$^{*}$Corresponding author.}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Some animals exhibit multi-modal locomotion capability to traverse a wide range of terrains and environments, such as amphibians that can swim and walk or birds that can fly and walk. This capability is extremely beneficial for expanding the animal's habitat range and they can choose the most energy efficient mode of locomotion in a given environment. The robotic biomimicry of this multi-modal locomotion capability can be very challenging but offer the same advantages. However, the expanded range of locomotion also increases the complexity of performing localization and path planning. In this work, we present our morphing multi-modal robot, which is capable of ground and aerial locomotion, and the implementation of readily available SLAM and path planning solutions to navigate a complex indoor environment.


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

% {\color{red}
% \lipsum
% }

% background and motivation


% REFERENCE LIST (comment latter)
% SLAM on mobile robots \cite{leonard1991simultaneous, gao2020stereo}.\\
% SLAM (RTAB-MAP) \cite{labbe2019rtab, labbe2013appearance}.\\
% SLAM (others) \cite{mur2017orb}.\\
% Multi-modal path planning \cite{sharif_energy_2018, sharif2019new, suh_optimal_2019, araki_multi-robot_2017, MM_PRM}. \\
% Ben's work with Husky 3D path planning with RRT and A$^*$ \cite{sihite2022efficient}\\
% Examples of multi-modal robots \cite{MM1, MM2, MM3, MM4,ANYmal_Wheeled,salamandre, flying_monkey, Flying_star}

Robotic multi-modal locomotion can be a significant ordeal. The prohibitive design restrictions include a tight power budget, limited payload, complex multi-modal actuation, excessive number of active and passive joints involved in each mode, sophisticated control, autonomy, and environment-specific models (since different environments are involved), to name a few, which have alienated these concepts. That said, the number of designs surrounding ground-aerial locomotion is not small. The robotic community has endorsed the importance of these systems and tirelessly introduces interesting and novel concepts \cite{araki_multi-robot_2017, Flying_star, peterson2011experimental, tagliabue2020shapeshifter, sihite2021unilateral, liang2021rough, dangol2021hzd, dangol2021control, sihite2021optimization, ramezani2021generative}. % only ground aerial vehicles; please do not cite other multi-modal systems, salagame2022letter
However, these systems are too small to carry heavy perception pieces of equipment. The most notable examples are legged-wheeled systems \cite{schwarz2016hybrid, suzumura2013real, thomson2012kinematic, grand2004stability, bjelonic2019keep} which can carry large equipment for autonomy but their modes of mobility (legged and wheeled) do not lead to significantly different mobility capabilities in terms of environment traversability. 


\begin{figure}[t]
\vspace{0.08in}
    \centering
    \includegraphics[width=\linewidth]{figures/autonomous_flight2.png}
    \caption{Composite image showing the path taken by the robot as it autonomously navigated the walled environment. The robot flew over the wall in UAS mode, transformed into UGV mode, then autonomously drove to the final waypoint. ABCDE letters correspond to specific actions time stamped in Fig \ref{fig:odometry}.}
    \label{fig:composite}
\vspace{-0.5cm}
\end{figure}

\begin{figure}[t]
\vspace{0.08in}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/cover.png}
    \caption{Illustrations of our multi-modal morphing robot that is capable of transforming between the unmanned ground vehicle (UGV) and unmanned aerial system (UAS) configurations.}
    \label{fig:robot_overview}
\vspace{-0.5cm}
\end{figure}



\begin{figure}[t]
\vspace{0.08in}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/hardware_architecture.png}
    \caption{The robot's system architecture for control, sensing, and SLAM.}
    \label{fig:system_architecture}
\vspace{-0.5cm}
\end{figure}

This paper provides an overview of our robot, called the M4 \cite{sihite2023multi, mandralis2023minimum}, which possesses several modes of mobility and can carry large payloads (scalable design). M4's scalability positions it in a unique situation even when only two of its mobility forms, ground and aerial, are considered. M4 can be employed in many applications such as Search And Rescue (SAR), planetary exploration, last mile package delivery, and emergency payload transport operations. In these operations, mobile robots face different environments. For instance, in SAR operations and in the aftermath of unique incidents such as flooding, one event may accompany another disaster. A hurricane may produce flooding and wind damage, or a landslide may dam a river and create a flood. Therefore, the more locomotion plasticity a robot possesses, the higher the chance of success. 

% So far, the robotic community has tried to address these problems with robots that can carry large computers and sensors and, at best, deliver 2 modes of operation, such as wheeled and legged locomotion. Employing a system with 7 modes of mobility not only changes the problem of decision-making and autonomy but also changes the way information such as traversability maps are created and interpreted.  

Perception, localization, path-planning, and high-level decision-making have been extensively developed in robotics, and we recognize this fact. However, these advances have remained limited to ground-aerial vehicles that cannot carry large payloads. If we limit ourselves to two modes of operations simultaneously, the ground-aerial robots introduced so far are systems that are limited to small payloads. These systems have remained small due to the conflicting requirements of ground-aerial locomotion,~\ie~they are not scalable. They cannot carry extra payloads such as perception sensors like cameras, lidar sensors, and powerful enough computers needed for self-contained, fully autonomous operations in a similar fashion that legged robots -- which have scalable designs thanks to their operational nature based on contact forces -- perform autonomous tasks today. It can be seen that even when the modes of operation are limited to ground and aerial locomotion, the notions of autonomy are less explored.  


%They cannot carry extra payloads such as {\color{blue} heavy} perception sensors {\color{blue}like} {\color{red} {(cameras,}} lidars, \textbf{[other sensors?]}) and powerful {\color{blue} enough} computers needed for self-contained, fully autonomous operations in a similar fashion that legged robots -- which have scalable designs thanks to their operational nature based on contact forces -- perform autonomous tasks today. It can be seen that even when the modes of operation are limited to ground and aerial locomotion, the notions of autonomy are less explored.  

This paper briefly overviews joint collaborative work between Caltech, Northeastern University, and Jet Propulsion Lab (JPL) in designing a platform called M4 with enhanced locomotion plasticity. In this paper, only ground-aerial locomotion is employed for autonomous multi-modal mobility. Extensive technical content about mechanism design and hardware aspects is skipped as the main goal is to limit the current draft to autonomy implementations of M4 in its ground-aerial modes only skipping other capabilities. The authors recognize the results presented here do not contribute to robot autonomy from a technical standpoint or introduce new algorithms. However, this work is distinguished in the following ways; 
%
{%\color{blue} 
The ability to realize ground and aerial mobility in a scalable fashion (\ie~the payload can be large) has never been achieved before, and ground-aerial systems introduced so far are limited to small systems with very restricted sensing and computing capabilities. The opportunity offered by a scalable multi-modal robot that can carry large electronics despite the conflicting requirements dictated by ground and aerial locomotion unlocks unexplored paths, particularly in robot perception, localization, and decision-making at a totally different level. For instance, often mobile robots are limited to fixed constraints in terms of their perception ranges. Following successful DARPA sub-T stories, adding robot hitch-hikers such as quadcopters to ground robots has been widely utilized. The multi-modal robot recruited in this paper combines these opportunities in a single, scalable platform, avoiding all other challenges associated with robot hitch-hiker concepts (\eg~data transfer, homing, docking, etc.) yet offering sub-T-like capabilities. Future autonomy and path planning for M4 will focus not just on the binary choice of aerial versus ground locomotion but on planning paths that utilize the full capabilities of M4. This paper, however, focuses on the initial steps towards this eventual goal by demonstrating autonomous ground-aerial locomotion.}

% {\color{red} (1) The multi-modal capability of the robot, i.e., the capability to perform ground and aerial locomotion, simplifies the path planning in one way and complicates it in another, e.g., we can take a simple straight line path over objects but expanded the environment to the 3D space; (2) Loop closure using Visual-SLAM (V-SLAM) to autonomously track waypoints in a multi-modal environment; (3) Odometry using purely onboard components in a GPS-denied environment can be very challenging with multiple potential sources of error and instability, e.g., vibrations from the propellers and the complexity of integrating of multiple components.}\filip{how are (2) and (3) different from drones?}

% This work is organized as follows: we present the mechanical design and system architecture of the M4, then an overview of V-SLAM and path planning algorithms, followed by experimental results and discussions, and finally, the concluding remarks.


\section{Mechanical Design and System Overview}


\begin{table}[t]
\vspace{0.08in}
\caption{M4 Hardware Configuration}
\centering
\begin{tabular}{lccr}
\hline
\multicolumn{4}{c}{Configuration Size}                                           \\ \hline
\multicolumn{1}{l}{UGV}                    & & & 0.70$\times$0.35$\times$0.35 $m^3$                   \\
\multicolumn{1}{l}{UAS}                    & & & 0.70$\times$0.70$\times$0.30 $m^3$                   \\
\multicolumn{1}{l}{MIP}                    & & & 1.00$\times$0.50$\times$0.30 $m^3$                   \\ \hline
\multicolumn{1}{l}{Thrust/Weight}          & & & 9kg/6kg                           \\ \hline
\multicolumn{4}{c}{Actuators}                                                    \\ \hline
\multicolumn{1}{l}{Wheels}                 & & & 4 motors                          \\
\multicolumn{1}{l}{Rotors}                 & & & 4 motors                          \\
\multicolumn{1}{l}{Joints}                 & & & 8 servos                          \\ \hline
\multicolumn{1}{l}{Power}                  & & & 6S 4000mAh LiPo                   \\ \hline
\multicolumn{4}{c}{Compute Units}                                                    \\ \hline
\multicolumn{1}{l}{Perception \& Navigation}      & & & Nvidia Jetson Nano                \\
\multicolumn{1}{l}{Wheels \& Joints Control}&& & Arduino Due                       \\
\multicolumn{1}{l}{Rotors Control}         & & & Cube Orange                       \\
\multicolumn{1}{l}{Communication Unit}           & & & ESP-WROOM-32 \\ \hline
\end{tabular}
\label{tab:hardware_config}
\vspace{-0.5cm}
\end{table}

% M4 introduction and capabilities
Our transforming robot, shown in Fig.~\ref{fig:robot_overview}, can switch its modes of mobility between the unmanned ground vehicle (UGV), unmanned aerial system (UAS), mobile inverted pendulum (MIP), quadrupedal, thruster-assisted MIP, legged locomotion, and manipulation. The robot possesses an articulated body with four legs, each leg has a total of two hip degrees of freedom (DOF) and a shrouded propeller that act as a wheel and a thruster simultaneously. Eight independent joint actuators translate the legs forward, backward, and sideways, and the shrouded propellers are attached to the leg ends. Additionally, it is equipped with an Intel RealSense D455 stereo depth (RGB-D) camera for perception, localization, and mapping.

% Robot mechanical details
As summarised in~\tabref{tab:hardware_config}, the robot weighs approximately 6.0 kg with all components, which include the onboard computers for low-level control and data collection, sensors (encoders, inertial measurement unit, stereo cameras), communication devices for teleoperation, and power electronic components. Most of the robot's weight stems from the high-power components consisting of 4 wheel motors, 4 propeller motors, 8 joint servos, 8 motor drivers, and a 6S 4000mAh battery. When in UGV mode, the robot measures 0.7 m in length and 0.35 m in both width and height. When in the MIP mode and dynamically balancing on its two wheels, it is 1.0 m tall, which permits reaching a better vantage point for data collection using its exteroceptive sensors. When in UAS configuration, the robot is 0.3 m tall, and the propellers' center points can reach a maximum distance of 0.45 m far apart from each other. Each propeller-motor combination can generate a maximum thrust force of approximately 2.2 kg-force, therefore reaching roughly 9 kg thrust force in total for an approximately 1.5 thrust-to-weight ratio. Its legs are 0.3 m long, including its 0.25 m in diameter wheels, which allows for traversing bumpy terrain.



% \begin{table}
%     \caption{Impact of the travel distance in the subgraphs.}
%     \centering
%     \resizebox{\linewidth}{!}{\begin{tabular}{ccccccccc}
%     \hline
%        Distance & \multicolumn{2}{c}{Oxford} & \multicolumn{2}{c}{U.S.} & \multicolumn{2}{c}{R.A.} & \multicolumn{2}{c}{B.D.} \\
%         m    & AR@1 & AR@1\% & AR@1 & AR@1\% & AR@1 & AR@1\% & AR@1 & AR@1\% \\\hline
%         50    & 74.3 &  99.2  & 57.1 & 89.3 & 52.3 & 87.6 & 42.1 &  73.8  \\
%         100     & 91.1 &  99.9  & 78.7 & 98.4 & 87.8 & 98.8 & 73.1 &  89.5  \\
%         200    & \textbf{98.0} & \textbf{99.9}   & \textbf{98.0} & \textbf{100.0}  & \textbf{94.3} & \textbf{100.0}  & \textbf{98.0} & \textbf{99.8}   \\
%         300    & 88.6 & 97.1 & 83.1 & 99.3 & 69.7 & 98.8 & 90.4 &  99.6  \\ \hline
%     \end{tabular}}
%     \label{tab:ablate_distance}
% \end{table}

% System architecture
Figure \ref{fig:system_architecture} shows the system architecture and data pipelines of the robot. The robot utilizes a Jetson Nano (Quad-core ARM A57 CPU and 128-core Maxwell GPU) computer, and several microcontrollers in the system: Arduino Due (32-bit Atmel SAM3X8E ARM Cortex-M3), Orange Cube flight controller (32-bit STM32H753 ARM Cortex-M7), and ESP-WROOM-32 (Xtensa 32-bit LX7 dual-core processor). Each microcontroller performs specific tasks and communicates with the others through serial. The Arduino Due handles the communication between all components and drives all actuators except the thruster motors. The Orange Cube controls the robot's aerial mobility using the open-source ArduPilot framework and communicates with the Arduino through MAVLink messaging protocols. The Jetson Nano is a powerful and small computer used to process the stereo depth camera data and SLAM, which then are relayed to the flight controller. Finally, the ESP32 acts as a simple relay between the Jetson Nano and Arduino Due due to incompatibilities with rosserial that sometimes triggered the system to reset, which can be dangerous if happened mid-flight.




%Wi-Fi \filip{not true anymore, esp no longer used for wifi, just a ros jetson relay due to arduino incompatibility} receiver for transmitting other external data to the robot and communicating over ROS serial.

% Hardware and software integration
% {\color{blue}
% Hardware, and software integration here. Explain how the user interfaces with the robot and sends waypoints to the robot. Maybe some info on the controller too.
% }
% For this autonomous navigation task, an operator will provide a target position to the robot through RViz and the robot will then figure out a way to reach that position. The system relies on a single stereo depth camera (Intel RealSense D455, which has an onboard IMU) fixed at the front end. The information is then processed on the Jetson Nano running ROS software. The RTAB-Map SLAM algorithm relies on visual-inertial odometry (VIO) to reconstruct a point cloud representation of the environment and provide pose estimates of the robot. This point cloud is then filtered by object height and projected onto the driving plane and obstacles are inflated to provide a collision safety margin for which the robot is assimilated to a sphere in space. The resulting map is then used by MoveBase A* global planner and DWA (dynamic window approach) local planner for 2.5D ground navigation. When a path to the desired target exists, motor commands are transferred to the main ground controller for execution. Then, the map and pose estimates are continuously updated and a path is recomputed live to account for new observations or displacement errors. If a path cannot be found, the robot assumes that an aerial path exists, based on the operator's knowledge of the environment, and will trigger a flight to its target. As soon as that signal is received, the robot will morph into UAS mode and receive the target position to generate guided mode waypoints for the ArduPilot flight controller. Then a state machine will communicate with ArduPilot over MAVLink to fly the robots to the desired waypoints with a straight line and fixed height trajectory to the target. After landing, the robot awaits the operator's next target, ready to morph back into UGV if its next goal is reachable by ground mobility.
 



\section{SLAM and Path Planning}

% {\color{blue}
% RTAB-MAP and A$^*$ algorithm descriptions here. Also short details on how the robot is controlled to follow the desired path.
% }
% Refs: 
% 1) ORB: An efficient alternative to SIFT or SURF
% 2) Multi-Session for Illumination Invariant Re-Localization
% 3) “RTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online OperatioN
% Local path planner: Dynamic Window Approach (DWA) algorithm


In this section, we describe two key aspects of mobile robotics: sensing and modeling the environment and then planning action based on that information. For the M4 we leverage a V-SLAM approach for state estimation and a multi-modal probabilistic roadmap (MM-PRM) for a multi-modal approach to localization and navigation.

\subsection{Simultaneous Localization and Mapping}

Simultaneous Localization and Mapping (SLAM) and path planning are crucial concepts in autonomy. SLAM refers to estimating the robot's pose (in our case $\mathbf{x}\in\mathbb{R}^6$) and mapping the environment $\mathcal{M}$ simultaneously, typically by measuring the rotation of wheels or other locomotion mechanisms. Path planning, on the other hand, involves finding the optimal path for a robot (relying on the SLAM output) to reach a desired destination or complete a task.

If SLAM is inaccurate, path planning algorithms may generate suboptimal paths or cause the robot to deviate from its intended path, which can lead to errors or even collisions. In this work, to achieve accurate SLAM measurements, we use Real-Time Appearance-Based Mapping (RTAB-Map)~\cite{labbe2019rtab} based on the integration of RGBD and inertial measurements. In our system's SLAM module, ORB/GFTT visual features~\cite{orb, gftt} are extracted, and a bag of words~\cite{bow} is used for loop closure detection.

% To achieve accurate SLAM measurements, it is important to consider factors such as wheel slippage, uneven terrain, and changes in friction. This can be done through various methods, such as using multiple sensors and advanced filtering techniques.
Once accurate SLAM measurements are obtained, path planning algorithms can be used to generate the most efficient path for a robot to reach its goal. These algorithms consider factors such as obstacle avoidance, terrain conditions, and energy consumption.
One common approach to path planning is the use of algorithms such as A* or Dijkstra's algorithm~{\cite{A_star, dijkstra1959note, comboplanner}}, which generates a graph of the environment and calculates the shortest path between the robot's current position and its goal. These algorithms can be further optimized by incorporating heuristics or other techniques~{\cite{visgraph, Astar_heuristics, Astar_time}}.






\begin{figure*}[t]
\vspace{0.08in}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/results_odometry.png}
    \caption{Visualization of the SLAM algorithm as the robot explored and followed the desired waypoints. (A) and (B) show the online ground waypoint generation using A$^*$ algorithm and the robot's ground trajectory tracking. In (B), a waypoint that can't be solved by the A$^*$ was given to the robot. It must transform into UAS mode and fly over the obstacles to reach the waypoint, as shown in (C) and (D). Then once the robot has landed, it transformed back into UGV mode and navigated itself towards the final waypoint, as shown in (D) and (E).}
    \label{fig:odometry}
\vspace{-0.5cm}
\end{figure*}

% \subsection{Path Planning using PRM and A$^*$}
{
% \color{blue}
\subsection{High-level Decision Making and Path Planning}

The objective of the path planning strategy is to minimize the total energy consumed by the robot by prioritizing ground over aerial locomotion. To achieve this, the path planning algorithm set up the environment by discretizing it into nodes where each node is associated with a locomotion mode (ground or aerial). The path planning module in our system represents the environment as a graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}\in\{\mathbf{x}\}^n_{i=1}$ are $n$ possible robot poses within the graph. 
% \filip{To make the computations more efficient, a partial 3D pose is used for ground locomotion and a full 6D pose is used by the pilot during flight}
~$\mathcal{E}$ denotes a set of edges between nodes. 

% Given a goal $\mathbf{x}_g$, the objective is  
% $argmin_{\mathbf{x}\in \text{OPEN}}~(g(\mathbf{x})+h(\mathbf{x},\mathbf{x}_g)\neq \mathbf{x}_g)$, where $g$ is the sum of the current path cost (for robot pose $\mathbf{x}$) from the start and $h$ a heuristic estimate of the cost of current state to the goal. OPEN is a list of robot poses used to propagate information to compute path costs.
% DWA as local planner \cite{fox1997dynamic}
% For collision avoidance, we benefit from the Dynamic Window Approach (DWA)~\cite{fox1997dynamic}, which casts several paths in front of the robot (based on the robot kinematic configuration) and scores them based on some criteria,~\eg~obstacle proximity, goal proximity. The path with the maximum score is the optimum path toward the desired goal. 


% Overview
% The objective of the path planning strategy is to minimize the total energy consumed by the robot by prioritizing ground over aerial locomotion. To achieve this, the path planning algorithm set up the environment by discretizing it into nodes where each nodes is associated with a locomotion node (ground or aerial). The nodes are then connected by edges and a cost is computed for each of them. Finally, an A$^\star$algorithm is used to determine the optimal path defined by a set of waypoints to the target position.

% Discretization
Following the work done in \cite{sihite2022efficient, MM_PRM}, the 3D environment is discretized into a set of nodes and edges with the 3D multi-modal probabilistic roadmap (MM-PRM) which takes into account the multi-modal nature of the robot's locomotion. The classical PRM algorithm builds a graph in the defined space by generating a certain number of nodes, where each node is created with a random position. When a node is created, it will search for the nearest nodes already present in the graph and then connect to them to form edges while checking that it does not cross any obstacles. This classical method can be extended into the 3D environment by creating ground and aerial nodes separately, then connecting the nodes with edges to form a single multi-modal environment. This version of the PRM algorithm requires the definition of 3 parameters: the number of ground surface nodes $N_w$, the number of nodes describing flyable space $N_f$, and the maximum distance between neighboring nodes $R$.

%\milad{Follow the notation I used above. In the standard format vectors should be denoted as lower case and bold\\}
% Node generation and definitions
New ground nodes $\mathbf x_{new}$ are randomly assigned according to the following constraint:
%
\begin{equation}
\mathbf x_{new} \in \{(x, y, z): z=z_{GND})\},
\end{equation}
%
\noindent where $x$, $y$, and $z$ represents the node position in Euclidean space, and $z_{GND}$ is the ground elevation. Similarly, new nodes in the flyable task space are obtained as follows:
%
\begin{equation}
\mathbf x_{new} \in \{ (x, y, z): z > 0, z \ne z_{GND} \}.
\end{equation}
%
The edges ($E$) are created by searching for the neighboring nodes, which are found using the following condition:
%
\begin{equation}
    \mathbf x_{Nearest} = \{\mathbf x \in \mathcal{N} : \| \mathbf x_{new} - \mathbf x \| \leq R\},
\end{equation}
%
\noindent where $\mathcal{N}$ is the set of nodes already created, $R$ is the maximum radius distance, and $\| . \|$ is the Euclidean norm. 



% This method is adapted to generate a graph for unimodal robots by constraining the node generation to a single mode (e.g., create only ground nodes for a wheeled robot or create nodes in aerial space for a quadcopter). 

% In this work, our robot can move both on the ground and in aerial space. Therefore, it is necessary to create 2 sets of constraints when generating the nodes. Thus, the main difference with the classical PRM algorithm is that a constraint is added on a certain number of nodes to ensure a sufficient number of nodes in each mode. This extended version of the PRM algorithm requires the definition of 3 parameters: the number of ground surface nodes $N_w$, the number of nodes describing flyable space $N_f$, and the maximum distance between neighboring nodes $R$.

% The $random\_walking\_node()$ function is used to only generate nodes where the robot can walk (either on the ground or on the arrival platform):

% \begin{equation}
%  Xnew =\underset{x,y,z\in C}{rand}(x,y,z) \in \{(x, y, 0) \cup (x, y, z_{platform})\}
% \end{equation}
% where $C$ represents the set of points in the limits of the environment.

% While the $random\_flying\_node()$ function creates only nodes in the airspace where the robot can fly: 

% \begin{equation}
%  Xnew = \underset{x,y,z\in C}{rand}(x,y,z)  \in \{ (x, y, z): z > 0 \text{ \& }  z \ne z_{platform} \}
% \end{equation}
% 



% \begin{algorithm}[t]

% \caption{3D MM-PRM Algorithm}\label{alg:3D_PRM}
% \KwIn{$R$ radius of neighbors, $N_w$ number of walking node, $N_f$  number of flying nodes}
% \KwOut{$N$ and $E$ respectively sets of nodes and edges}
% $N \gets \emptyset$\;
% $E \gets \emptyset$\;
% \While{$n \leq (N_w + N_f)$} {
%     \uIf{$n \leq N_w$}{
%     $Xnew \gets random\_walking\_node()$\;
%     }
%     \Else{
%     $Xnew \gets random\_flying\_node()$\;
%     }
%     \If{$obstacles\_free(Xnew)$}{
%         $N \cup Xnew$\;
%         $n \gets n + 1$\;
%         $Xnearest \gets nearest(N, R, Xnew)$\;
%         \For{$node \in Xnearest$}{
%             \If {$clear\_edge(Xnew, node)$}{
%             $E \cup \{Xnew, node\}$\;
%             }
%         } 
%     }
% }
% \Return{$N,E$}
% \end{algorithm}


% \begin{figure}[t]
%     \centering
%     \vspace{0.1in}
%     \includegraphics[width = \linewidth]{figures/prm_edges.png}
%     \caption{Example of graph generated by the 3D MM-PRM Algorithm with the following parameters: $R = 4$ meters, $N_w = 300$, and $N_f = 300$.}
%     \vspace{-0.1in}
%     \label{fig:ex_PRM}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \vspace{0.1in}
%     \includegraphics[width = 0.8\linewidth]{figures/uniform_grid.png}
%     \includegraphics[width = 0.8\linewidth]{figures/prm.png}
%     \caption{Representation of the set of nodes generated by the two discretization methods. The MM-RPM method generates a significantly reduced amount of nodes which greatly reduces the computational time and cost in performing the path finding algorithm.}
%     \vspace{-0.1in}
%     \label{fig:node_comparison}
% \end{figure}


% \subsubsection{Search of neighbors}



% The cost and time of calculation are very strongly linked to the choice of the values of the algorithm parameters ($R$, $N_w$, $N_f$). The greater the total number of nodes or the greater the radius of acceptance of the neighbors, the greater the computation time and cost will be. Therefore, it is necessary to study the convergence of the result in function of the parameters in order to optimize to computation cost. We identified the parameters that led to best results. The parameters are $R = 4$ meters, $N_w = 300$ and $N_f = 300$. An example of the graph built with the 3D MM-PRM algorithm is presented in the Fig.~\ref{fig:ex_PRM}.


% We found that compared to a uniform discretization with 0.25m-wide grids, the 3D MM-PRM algorithm produces a graph representative of the environment with a minimal number of nodes as shown in Fig.~\ref{fig:node_comparison}. This reduces the cost and the computing time while avoiding any compromises on the performance concerning the optimality of the path obtained. The comparison between these two methods is summarized briefly in Table~\ref{tab:comp_2_methods}, which shows the significant reduction in computational time when using the PRM algorithm.

% \begin{table}[t]
% \caption{Comparison of the two discretization methods}
% \label{tab:comp_2_methods}
% \centering
% \begin{tabular}{|l|r|r|}
% \hline
%                          & \multicolumn{1}{c|}{3D MM-PRM} & \multicolumn{1}{c|}{Uniform Grid} \\ \hline
% Number of Nodes              & 500                      & 9892                         \\ 
% Number of Edges             & 30920                    & 219340                       \\ 
% Computation Time {[}s{]} & 12.1                     & 78.29                        \\ \hline
% \end{tabular}

% \end{table}


\subsubsection{Calculation of Locomotion Cost}
\label{sec:cost_calculation}

% \begin{figure*}[t]
%     \centering
%     \vspace{0.1in}
%     \includegraphics[width = 0.32\linewidth]{figures/environment_A.png}
%     \includegraphics[width = 0.32\linewidth]{figures/environment_B.png}
%     \includegraphics[width = 0.32\linewidth]{figures/environment_C.png}
%     \caption{The trajectories generated by the path planning algorithm on three different environments. The environment A will be used in the Husky simulation for tracking the generated trajectory and show Husky's multi-locomotion capability.}
%     \vspace{-0.1in}
%     \label{fig:environments}
% \end{figure*}





To calculate the locomotion cost including legged and aerial, it is necessary to not only determine the costs associated with each modes but also the cost corresponding to the transition from one mode to another. As such, the cost of transport on a ground edge $C_g$ is calculated using the motor power consumption $P_m$, which is integrated over the time of wheeled locomotion. The total joint power consumption is computed based on the torque and the angular velocity of each joint. The time of legged locomotion is calculated based on the distance $d$ between the two nodes. As a result, $C_g$ is given by:
\begin{equation}
    % C_g =  P_m\frac{d}{v_w}
    \textstyle
    C_g =  \int_0^{t_d} P_m(\tau)d\tau.
\end{equation}
%
% The walking velocity is considered as constant, $v_w = 0.2ms^{-1}$.
The energetic cost on a flying edge $C_f$ is computed using the power consumption $P_f$ in hovering, the robot forward velocity $v_f$ in flying mode, and the altitude $z$ of the two nodes. Hence, $C_f$ is given by:
\begin{equation}
    %C_f = P_f\frac{d}{v_f}+ mg(z_2 - z_1),
    C_f = P_f \, (d / v_f) + mg(z_2 - z_1),
    \label{eq_Cf}
\end{equation}
%
\noindent where $z_1$ and $z_2$ are respectively the altitudes of the nodes 1 and 2, $m$ is the mass of the robot and $g$ is the gravitational acceleration constant. Last, the transition cost $C_t$ between the two modes is determined based on the power consumption of the joints during the morphing process $P_t$. Then, $P_t$ is integrated over the time of transition $t_t$ which yields:
%
\begin{equation}
\textstyle
    C_t = \int_0^{t_t} P_s(\tau)d\tau .
\end{equation}
%
\noindent These three energetic costs are employed to determine the optimal path in the edge space generated by MM-PRM algorithm using the A$^\star$ algorithm.

\subsubsection{Optimal Path Using 3D A* Algorithm}

\begin{figure*}[t]
\vspace{0.08in}
    \centering
    \includegraphics[width=0.45\linewidth]{figures/plot_full.png}
    \hfill
    \includegraphics[width=0.45\linewidth]{figures/plot_flight.png}
    % \includegraphics[width=\linewidth]{figures/plot_full.png}
    % \includegraphics[width=\linewidth]{figures/plot_flight.png}
    \caption{Plots of the robot's pose as estimated by the stereo depth camera Visual SLAM (V-SLAM) localization and the target waypoints. The left plot shows the states throughout the entire experiment while the right plot shows the states during aerial mobility. The EKF estimate from the flight controller (FC) is also provided to show the delay between stereo depth camera's and the FC's internal pose estimates. The shaded region illustrates the tracking margin of error in the controller.}
    \label{fig:odometry_plot}
\vspace{-0.5cm}
\end{figure*}

To find the optimal path in the graph, the A$^\star$ path search algorithm \cite{A_star, dijkstra1959note} is used with a heuristic function adapted to the robot's multi-modal abilities.
%\filip{\sout{. The improved version of Dijkstra's algorithm \cite{dijkstra1959note} is employed to find the optimal path by using a heuristic function.} with a heuristic function adapted to the robot's multi-modal abilities.}
%The algorithm computes the best path to each node in order to only visit the most promising nodes. This avoids going through all possible paths and, therefore, finding the first-best optimal path with a low computational cost. 
Each time the algorithm explores $n$-th node, it calculates the minimum cost for the A$^*$ objective function $g(\mathbf{x}) + h(\mathbf{x},\mathbf{x}_g)$.
% $f(n)$ necessary to reach the goal by passing through it using the following formula:
% %
% \begin{equation}
%     f(n) = g(n) + h(n),
% \end{equation} 
% %
% \noindent where $g(n)$ is the real cost from the start to the $n$-th node, computed based on \eqref{eq:g(n)}, and $h(n)$ denotes the heuristic cost to the goal. 
The heuristic cost $h(\mathbf{x},\mathbf{x}_g)$ is calculated by summing the following two costs. First, the cost of walking on flat ground to the goal in a straight line is calculated. Second, the cost of flying vertically along the z-axis to the goal is obtained. Since the cost of walking is much lower than flying, this is the most optimal way to move between two points if there is no obstacle or impassable terrain between the current and target positions. The following cost for $g(\mathbf{x})$ is defined:
%
\begin{equation}
    \textstyle
    g(\mathbf{x}) = \sum_{i = 0}^{E_w} C_{w,i} + \sum_{j = 0}^{E_f} C_{f,j} + N_t\, C_{t}
    \label{eq:g(n)}
\end{equation}
%
\noindent where $E_w$ and $E_f$ are the number of walking and flying edges traveled by the robot, respectively. Furthermore, $C_{w,i}$ is the cost on the walking edge $i$, $C_{f,j}$ is the cost on the flying edge $j$, and $N_t$ is the number of mode transitions made by the robot (ground to aerial, or vice versa).
}

For collision avoidance, we benefit from the Dynamic Window Approach (DWA)~\cite{fox1997dynamic}, which casts several paths in front of the robot (based on the robot kinematic configuration) and scores them based on some criteria,~\eg~obstacle proximity, goal proximity. The path with the maximum score is the optimum path toward the desired goal. 



\section{Experimental Results and Discussions}



% Experimental setup
% \milad{It is worth mentioning ROS communication between the SLAM module and navigation stack}
% \milad{Also refer to the use of RTAB-Map package here \url{https://introlab.3it.usherbrooke.ca/mediawiki-introlab/images/7/7a/Labbe18JFR_preprint.pdf}\\}\filip{see \cite{labbe2019rtab}}

A set of experiments were conducted inside the California Institute of Technology's CAST Arena. An obstacle course was set up for the robot to perform localization, path planning, and trajectory tracking using the robot's ground and aerial multi-modal locomotion. The goal of this experiment is to show the effectiveness of the navigation stack, and the versatility of our transforming robot where it can fly over difficult or unpassable terrains. Therefore, a "walled" area was set up where the robot must fly over the wall to reach the target position, as illustrated in Fig.~\ref{fig:composite}. 

For this autonomous navigation task, an operator will provide a target position to the robot through RViz and the robot will then figure out a way to reach that position. The system relies on a single stereo depth camera (Intel RealSense D455, which has an onboard IMU) fixed at the front end of the robot. The information is then processed on the Jetson Nano running ROS software. The RTAB-Map SLAM algorithm relies on VIO to reconstruct a point cloud representation of the environment and provide pose estimates of the robot. This point cloud is then filtered by object height and projected onto the driving plane and obstacles are inflated to provide a collision safety margin for which the robot is assimilated to a sphere in space. The resulting map is then used by MoveBase A* global planner and DWA (dynamic window approach) local planner for 2.5D ground navigation. When a path to the desired target exists, motor commands are transferred to the main ground controller for execution. Then, the map and pose estimates are continuously updated and a path is recomputed live to account for new observations or displacement errors. If a path cannot be found, the robot assumes that an aerial path exists, based on the operator's knowledge of the environment, and will trigger a flight to its target. As soon as that signal is received, the robot will morph into UAS mode and receive the target position to generate guided mode waypoints for the ArduPilot flight controller. Then a state machine will communicate with ArduPilot over MAVLink to fly the robots to the desired waypoints with a straight line and fixed height trajectory to the target. After landing, the robot awaits the operator's next target, ready to morph back into UGV if its next goal is reachable by ground mobility. 

Three waypoints were given to the robot to follow, where the first and last waypoints can be reached by ground mobility while the second waypoint crosses the wall for the robot to fly over. Throughout the experiments, the operator only provided the robot with the waypoints while the robot autonomously performed the control, waypoint tracking, and transformation. We attached the robot to a safety tether to prevent a crash in case of emergency or controller failure in the air. We also attached ethernet and data cables to the robot for data logging which we can use to plot the tracking performance, alongside the localization and odometry visualization by the stereo depth camera.

% Result presentation
The results of the experiment can be seen in Fig.~\ref{fig:composite}, \ref{fig:odometry}, and \ref{fig:odometry_plot}. Figure~\ref{fig:composite} is a composite image showing the robot's path as it flew over the wall in UAV mode, landed on the other side of the wall, transformed back into the UGV mode, then drove to the last waypoint. Figure~\ref{fig:odometry} shows the stereo depth camera's visualization of the detected environment, the odometry using SLAM, path planning using the A$^*$ algorithm, and the path taken by the robot. Figure~\ref{fig:odometry_plot} shows the SLAM odometry using the stereo depth camera, desired goal, aerial waypoints, and Ardupilot's EKF pose estimates while in the air. 
%
% Result discussion
As shown in Fig.~\ref{fig:odometry}, the robot has successfully reached the desired waypoints using only the odometry from the onboard sensors. The ground trajectory tracking performance worked well, as shown in Fig.~\ref{fig:odometry_plot}, where the robot can reach the target position using simple forward and turning speed commands. On the other hand, aerial trajectory tracking has some oscillations and overshoots, which are pronounced in the altitude and heading measurements. Our tests showed that there is approximately a second delay between when the command is sent and the robot's actual movement.
%, which is reflected in the SLAM's measurement to be sent to the Arduino. 
This delay is likely caused by the limitation of either the stereo camera or the onboard computer. This delay could cause a stability issue for longer flights, therefore we kept the flight time short which works well for the simple trajectory that we used in the experiment. 







\section{Conclusions and Future Work}

% {\color{red}
% \lipsum[1-2]
% }

In this paper, we presented our multi-modal morphing robot and the implementation of SLAM, path planning, and trajectory tracking using only the onboard computer and sensors in an indoor environment. The robot utilized its onboard stereo depth camera to perform SLAM and autonomously navigated a complex indoor environment, and transformed between UGV and UAS modes to drive and fly over obstacles. Experimental results show that the robot can estimate its pose using vision-based SLAM, perform path planning, track the waypoints generated by the path planner, and navigate to the target position using both ground and aerial modes. There are some issues that need to be addressed in future work, such as the pose estimation delays that can cause instability during flight. Once these issues have been resolved, we can then proceed to conduct outdoor experiments to show that the robot can perform well in an outdoor unstructured environment.



% \addtolength{\textheight}{-10cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% \IEEEtriggeratref{24} % trigger new column at this reference
\balance{}
% \printbibliography
 
\bibliographystyle{IEEEtran}
% \bibliography{./bibliography/IEEEabrv,./bibliography/IEEEexample}
\bibliography{references}
\end{document}
