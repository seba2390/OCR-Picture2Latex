
\myparagraph[0]{Interpretability.}
In order to make machine learning models more interpretable, a variety of techniques has been developed.
On the one hand, 
    research has been undertaken to develop model-agnostic explanation methods for which the model behaviour
    under different inputs is analysed; this includes among others \cite{lundberg2017unified,petsiuk2018rise,lime}.
    While their generality and the applicability to any model are advantageous,
    these methods typically require evaluating the respective model several times and are therefore costly
    approximations of model behaviour.
    %
On the other hand,
    many techniques that explicitly take advantage of the internal computations have been proposed for explaining
    the model predictions, including, for example, \cite{simonyan2013deep,springenberg2014striving,zhou2016CAM,selvaraju2017grad,shrikumar2017deeplift,sundararajan2017axiomatic,srinivas2019full,bach2015pixel}.\\
    %
In contrast to techniques that aim to explain models \emph{post-hoc},
some recent work has focused on designing new types of network architectures, which are \emph{inherently} more interpretable.
Examples of this are the prototype-based neural networks~\cite{chen2019looks}, the BagNet~\cite{brendel2018approximating}
and the self-explaining neural networks (SENNs)~\cite{melis2018towards}.
Similarly to our proposed architectures,
    the SENNs and the BagNets derive their explanations 
    from a linear decomposition of the output into contributions from the input (features).
This \emph{dynamic linearity}, i.e., the property that the output is computed via some form of an input-dependent linear mapping, is additionally shared by the entire model family of piece-wise linear networks (e.g., ReLU-based networks). In fact, the contribution maps of the CoDA-Nets are conceptually similar to  evaluating the `Input$\times$Gradient' (IxG), cf.~\cite{adebayo2018sanity}, on piece-wise linear models, which also yields a linear decomposition in form of a contribution map.
However, in contrast to the piece-wise linear functions, we combine this \emph{dynamic linearity} with a structural bias towards an alignment between the contribution maps and the discriminative patterns in the input. This results in explanations of much higher quality, whereas IxG on piece-wise linear models has been found to yield unsatisfactory explanations of model behaviour~\cite{adebayo2018sanity}.

\myparagraph{Architectural similarities.} In our CoDA-Nets, the convolutional kernels are dependent on the specific patch that they are applied on; i.e., a CoDA-Layer might apply different filters at every position in the input. As such, these layers can be regarded as an instance of dynamic local filtering layers as introduced in~\cite{jia2016dynamic}.
Further, our dynamic alignment units (DAUs) share some high-level similarities to attention networks, cf.~\cite{xu2015show}, in the sense that each DAU has a limited budget to distribute over its dynamic weight vectors (bounded norm), which is then used to compute a weighted sum. However, whereas in attention networks the weighted sum is typically computed over vectors, which might even differ from the input to the attention module, a DAU outputs a \emph{scalar} which is a weighted sum of all scalar entries in the input. Moreover, we note that at their optimum (maximal average output over a set of inputs), the DAUs solve a constrained low-rank matrix approximation problem~\cite{eckart1936approximation}. While low-rank approximations have been used for increasing parameter efficiency in neural networks, cf.~\cite{yu2017compressing}, this concept has to the best of our knowledge not been used in order to endow neural networks with a structural bias towards finding low-rank approximations of the input for increased interpretability in classification tasks. Lastly, the CoDA-Nets 
are related to capsule networks. However, whereas in classical capsule networks the activation vectors of the capsules directly serve as input to the next layer, in CoDA-Nets the corresponding vectors are used as convolutional filters. 
We include a detailed comparison in the supplement. 