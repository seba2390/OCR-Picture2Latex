
Neural networks are powerful models that excel at a wide range of tasks.
However, they are notoriously difficult to interpret and extracting explanations 
    for their predictions is an open research problem. Linear models, in contrast, are generally considered interpretable, because
    the \emph{contribution} 
    (`the weighted input') of every dimension to the output is explicitly given.
Interestingly, many modern neural networks implicitly model the output as a linear transformation of the input;
    a ReLU-based~\cite{nair2010rectified} neural network, e.g.,
    is piece-wise linear and the output thus a linear transformation of the input, cf.~\cite{montufar2014number}.
    However, due to the highly non-linear manner in which these linear transformations are `chosen', the corresponding contributions per input dimension do not seem to represent the learnt model parameters well, cf.~\cite{adebayo2018sanity}, and a lot of research is being conducted to find better explanations for the decisions of such neural networks, cf.~\cite{simonyan2013deep,springenberg2014striving,zhou2016CAM,selvaraju2017grad,shrikumar2017deeplift,sundararajan2017axiomatic,srinivas2019full,bach2015pixel}.
    
In this work, we introduce a novel network architecture, the \textbf{Convolutional Dynamic Alignment Networks (CoDA-Nets)}, {for which the model-inherent contribution maps are faithful projections of the internal computations and thus good `explanations' of the model prediction.} 
There are two main components to the interpretability of the CoDA-Nets. 
    First, the CoDA-Nets are \textbf{dynamic linear}, i.e., they compute their outputs through a series of input-dependent linear transforms, which are based on our novel \mbox{\textbf{Dynamic Alignment Units (DAUs)}}. 
        As in linear models, the output can thus be decomposed into individual input contributions, see Fig.~\ref{fig:teaser}.
    Second, the DAUs are structurally biased to compute weight vectors that \textbf{align with \mbox{relevant} patterns} in their inputs. 
In combination, the CoDA-Nets thus inherently  
produce contribution maps that are `optimised for interpretability': 
since each linear transformation matrix and thus their combination is optimised to align with discriminative features, the contribution maps reflect the most discriminative features \emph{as used by the model}.

With this work, we present a new direction for building inherently more interpretable neural network architectures with high modelling capacity.
In detail, we would like to highlight the following contributions:
\begin{enumerate}[wide, label={\textbf{(\arabic*)}}, itemsep=-.5em, topsep=0em, labelwidth=0em, labelindent=0pt]
    \item We introduce the Dynamic Alignment Units (DAUs), which 
    improve the interpretability of neural networks and have two key properties:
    they are 
    \emph{dynamic linear} 
    and align their weights with discriminative input patterns.
    \item Further, we show that networks of DAUs \emph{inherit} these two properties. In particular, we introduce Convolutional Dynamic Alignment Networks (CoDA-Nets), which are built out of multiple layers of DAUs. As a result, the \emph{model-inherent contribution maps} of CoDA-Nets highlight discriminative patterns in the input.
    \item We further show that the alignment of the DAUs can be promoted 
    by applying a `temperature scaling' to the final output of the CoDA-Nets. 
    \item We show that the resulting contribution maps 
    perform well under commonly employed \emph{quantitative} criteria for attribution methods. Moreover, under \emph{qualitative} inspection, we note that they exhibit a high degree of detail.
    \item Beyond interpretability, 
    CoDA-Nets are performant classifiers and yield competitive classification accuracies on the CIFAR-10 and TinyImagenet datasets.
\end{enumerate}