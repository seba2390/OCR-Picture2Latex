\newpage
\section{Low-rank matrix approximations}
\label{sec:low_rank_matrices}
In the following, we first introduce the standard formulation of the low-rank matrix approximation problem and present the well-known solution via singular value decomposition.
We then introduce an additional constraint to this standard formulation and show that at their optimum (maximal average output), the DAUs solve this constrained approximation problem.

\myparagraph{Low-rank approximation problem.}
Given a data matrix\footnote{In the context of our work, we can think of  $\mat m$ as storing $n$ data samples of dimensionality $m$; e.g., when considering images, each column might correspond to one vectorised image from a dataset of $n$ images. The low-rank approximation problem aims to approximate this dataset with $r$ independent variables (dimensionality reduction).} $\mat M \in \mathbb R^{m\times n}$, the low-rank approximation problem is typically formulated as $\min_{\mat M'}(||\mat M-\mat M'||_F)$ with $\textit{rank}(\mat M') \leq  r$ and $||\cdot||_F$ the Frobenius norm.
The Frobenius norm can be written as 
a sum over the column differences $||\mat \Delta_i||_F^2 = ||\vec m_i||_2^2 + ||\vec m'_i||^2_2 - 2\vec m^T_i\vec m_i'$ with $\vec m_i, \vec m_i'\in\mathbb R^{m}$ the $i$-th columns of the matrices $\mat m$ and $\mat m'$ respectively.
Note that since the $\vec m_i$ are fixed (they just correspond to the fixed input matrix $\mat m$), the optimisation objective is equivalent to
\begin{align}
	\label{eq:low_r_recon}
	&\min \textstyle\sum_i ||||\vec m'_i||^2_2 - 2\vec m^T_i\vec m_i'=\\
	\label{eq:svd2}
	&\min\textstyle\sum_i ||||\vec m'_i||^2_2 - 2||\vec m_i||||\vec m'_i||\cos(\alpha_i)\quad,
\end{align}
 with $\alpha_i$ the angle between $\vec m_i$ and $\vec m_i'$. Hence, optimising the Frobenius norm of the difference matrix finds a trade-off between aligning the directions (angles) of the columns of $\mat m$ and $\mat m'$ and approximating the correct norms of the original columns in \tmat m.
The optimal solution for this problem is given by singular value decomposition (SVD)~\citesupp{eckart1936approximation} of \tmat m, and the optimal $\mat m'$ can be written as 
\begin{align}
\label{eq:optimal_svd}
\mat m' = \mat U_r \mat U^T_r\mat m =\mat U\mat \Sigma_r\mat v^T\quad,
\end{align}
where $\mat U_r$ is the matrix of left singular vectors of $\mat m$ up to the $r$-th vector\footnote{A short proof of the equality to the right can be found at the end of this section.}; the remaining vectors are replaced by zero-vectors. Note that the first formulation of the solution ($\mat u_r\mat u_r^T\mat m $) 
emphasises a property that we will encounter again for the DAUs. In particular, this formulation highlights the fact that the optimal matrix $\mat m'$ is given by reconstructing the individual data points from the $r$-dimensional eigenspace spanned by the eigenvectors of the data points; i.e., each column $\vec m_i'$ can be calculated as $\mat u_r\mat u_r^T\vec m_i$.
The right hand side of the equation shows the conventional SVD-notation of the low-rank approximation solution, with $\mat u\in \mathbb R^{m\times m}, \mat \Sigma_r \in \mathbb R^{m\times n}\, \mat V \in \mathbb R^{n\times n}$ and $\mat \Sigma_r$ being a rectangular diagonal matrix with the first $r$ singular values as non-zero entries.


\myparagraph{Constrained low-rank approximations}
Now we diverge from the standard low-rank approximation formulation and impose an additional constraint on $\mat m'$ (red box):
\newcommand{\mycbox}[1]{%
\colorlet{oldcolor}{.}
  {\color[RGB]{150, 0, 0}
  \boxed{\color{oldcolor}#1}}
}
\begin{align}
    &\textstyle\min_{\mat M'}(||\mat M-\mat M'||_F)\;\text{,}\\
    \text{s.t.}\quad \textit{rank}(&\mat m')  \leq r \;\mycbox{\land\; ||\vec m_i'||_2^2 = 1 \;\forall i}\;\textbf{.}
    \label{eq:constraint}
\end{align}


When combining eqs.~\eqref{eq:svd2} and \eqref{eq:constraint}, it becomes clear that the optimisation problem can now be formulated as 
\begin{align}
    \textstyle\max\sum_i{\vec m}_i^T\vec m_i' = \max\sum_i||\vec m_i||_2\cos \left(\angle(\vec m_i,\vec m_i')\right)\; ,
    \label{eq:new_goal2}
\end{align}
since the norm of $\vec m_i'$ is fixed and the addition of or multiplication with a fixed constant does not change the location of the optima.

As a corollary, we note that \emph{any} matrix $\mat m'$ with a maximum rank $r$ that maximises eq.~\eqref{eq:new_goal2} gives a solution to the constrained approximation problem, \emph{independent} of the norm of its columns: the columns can be normalised \emph{post-hoc} to fulfill the new constraint, since rescaling does not change neither the rank of the matrix, nor the score of the objective function in eq.~\eqref{eq:new_goal2}.

Further, we note that the unnormalised optimal matrix $\widetilde{\mat m}'$ can be factorised as 
\begin{align}
    \label{eq:factorise}
    &\widetilde{\mat m}' = \mat a\mat b\mat m \; ,\\
    \text{s.t.}\quad  &\mat m' = G(\widetilde{\mat m}') = G(\mat{ABM})
\end{align}
with $\mat A\in \mathbb R^{m\times r}$ and $\mat b \in \mathbb R^{r\times n}$, similar to the solution to the unconstrained matrix approximation problem in eq.~\eqref{eq:optimal_svd}.
Here, $G$ is a function that normalises the columns of a matrix to unit norm (as discussed in the previous paragraph, in order to fulfill the norm constraint, we can normalise the columns \emph{post-hoc}).

Finally, this allows us to rewrite the objective function in eq.~\eqref{eq:new_goal2} as
\begin{align}
    \max \sum_i g(\mat{AB}\vec m_i)\vec m_i =\max \sum_i \vec w(\vec m_i)\vec m_i
\end{align}
with $g(\vec v)$ normalising its input vector to unit norm. 
Comparing this with eq.~(1) in the main paper, we see that this is equivalent to maximising a DAU without bias term for maximal output over the $n$ columns in $\mat m$ and therefore, at their optimum, the DAUs solve the constrained low-rank matrix approximation problem in eq.~\eqref{eq:constraint}. In particular, the DAUs achieve this by encoding common inputs in the eigenvectors of $\mat{ab}$, which allows for an optimal angular reconstruction of the inputs, similar to reconstructing from the first PCA components in a PCA decomposition.
The main difference to PCA is that PCA yields optimal reconstructions under the L2-norm, whereas the DAUs yield optimal angular reconstructions.

\myparagraph{Proof of $\mat U_r\mat U^T_r \mat m = \mat u \mat\Sigma_r\mat v^T$.}\\
In order to see that this equality holds, we first write matrix $\mat m$ as
\begin{align}
 \mat m = \mat U\mat \Sigma\mat v^T \quad (\text{SVD-form})
\end{align}
Now, we multiply both sides with $\mat U_r\mat U_r^T$ from the left:
\begin{align}
  \mat u_r \mat U^T_r  \mat M  &= \mat u_r \mat u_r^T\mat U\mat \Sigma\mat v^T \\
 \xrightarrow{\mat U_r^T \mat U = \mat I_r}
 \mat u_r \mat U^T_r  \mat M  &= \mat u_r \mat I_r\mat \Sigma\mat v^T \\
 \Rightarrow \mat u_r \mat U^T_r  \mat M  &= \mat U\mat \Sigma_r\mat v^T\quad \qed.
\end{align}
Here, we made use of the fact that $\mat u$ is an orthogonal matrix and therefore $\mat U^T\mat u =\mat I$; since we only use the first $r$ eigenvectors, we obtain the truncated identity matrix $\mat i_r$ when multiplying $\mat u^T_r\mat u$.
Further, in the last line, we used that $\mat u\mat\Sigma_r = \mat U_r\mat \Sigma_r$.

