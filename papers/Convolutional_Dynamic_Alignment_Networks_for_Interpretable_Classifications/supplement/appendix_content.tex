%%%%%%%%% TITLE
{
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\renewcommand{\thefigure}{\thesection\arabic{figure}}
\renewcommand{\thetable}{\thesection\arabic{table}}

\appendix
\onecolumn 
\flushleft

\begin{center}
{
\huge\bf \vspace{1em}Supplementary Material\\[2em]}
\end{center}

\newcommand{\additem}[2]{%
\item[\textbf{(\ref{#1})}] 
    \textbf{#2} \dotfill\makebox{\textbf{\pageref{#1}}}
}

\newcommand{\addsubitem}[2]{%
    \\[.5em]\indent\hspace{1em}
    \textbf{(\ref{#1})}
    #2 \dotfill\makebox{\textbf{\pageref{#1}}}
}

\newcommand{\adddescription}[1]{\newline
\begin{adjustwidth}{1cm}{1cm}
#1
\end{adjustwidth}
}
{\vspace{2em}\bf\Large Table of Contents\\[2em]}

In this supplement to our work on Convolutional Dynamic Alignment Networks (CoDA-Nets), we provide:
\begin{enumerate}[label={({\arabic*})}, topsep=1em, itemsep=.5em]
    \additem{sec:additional_figures}{ 
    Additional qualitative results} 
    \adddescription{In this section, we show additional \emph{qualitative} results on the Imagenet subset as well as additional comparisons between the model-inherent contribution maps and other methods for importance attribution. Further, we show the effect of regularising the linear mappings on the contribution maps for models trained on the CIFAR-10 dataset. Lastly, we show the results of the `sanity check' by Adebayo et al.~\citesupp{adebayo2018sanity} as well as the contribution maps of a piece-wise linear model (ResNet-56).}
    \additem{sec:additional_quantitative}{ 
    Additional quantitative results} 
    \adddescription{In this section, we show additional \emph{quantitative} results. In particular, we show the accuracies of the temperature-regularised models and of models of different sizes (DAU rank ablation). Further, we show interpretability results for models trained with the L2 non-linearity, with explicit regularisation of the linear mapping $\mat w_{0\rightarrow L}$, and for a pre-trained ResNet-56 for comparison. Moreover, we show results for the \emph{pixel removal metric} when removing the most important pixels first.}
    \additem{sec:training}{ 
    Implementation details} 
    \adddescription{In this section, we present architecture and training details for our experiments and describe in detail how the convolutional Dynamic Alignment Units are implemented. Further, we discuss the results of ResNets on the Imagenet subset under the exact same training scheme for comparison.}
    \additem{sec:low_rank_matrices}{
    Relation to low-rank matrix approximations
    }
    \adddescription{In this section, we discuss the relationship between the Dynamic Alignment Units and the problem of low-rank matrix approximation.
    }
    \additem{sec:capsule_comparison}{
    Relation to capsule networks}
    \adddescription{In this section, we discuss the relationship between the Dynamic Alignment Units and capsules~\citesupp{sabour2017dynamic}. In particular, we rewrite the standard capsule formulation, which allows us to compare them more easily to our work. Under this new formulation, it becomes clear that the two approaches share similarities, but also that there exist important differences.
    }
    
\end{enumerate}
}
\clearpage
\justify
\input{supplement/A-qualitative_figures}
\clearpage
\input{supplement/B-quantitative_results}

\input{supplement/C-1-implementation_details}

\input{supplement/D-low_rank}

\input{supplement/E-capsules}

\clearpage
{
\justifying
\bibliographystyleS{supplement/ieee_fullname}
\bibliographyS{supplement/supp_bib}
}
% 