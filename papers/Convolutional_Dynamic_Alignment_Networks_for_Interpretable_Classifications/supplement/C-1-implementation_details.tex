\section{Implementation details}
\label{sec:training}
\subsection{Training and architecture details}
\myparagraph{Architectures.} The architectures used for the experiments in section 4 are given in Table~\ref{tbl:archs}.
All activation maps are padded with $(k - 1)/2$ zeros on each side, such that the spatial dimensions are only reduced by 
    the strides; here, $k$ refers to the kernel size.
As can be seen, the activation maps thus still have a spatial resolution after the last layer, which we further reduce with a global sum-pooling layer.
Note that global sum-pooling is just a linear layer with no trainable parameters and therefore still allows for linear decomposition.
The input itself consists of 6 channels, the image and its negative, as explained in section~3.4;
    hence, the first layer takes an input with 6 channels per pixel.

\myparagraph{Training details.} We use the pytorch library~\citesupp{pytorch} and optimise all networks with the Adam optimiser~\citesupp{kingma2014adam} with default values.
As for the loss function, we use the binary cross entropy loss to optimise class probabilities individually (as `one-vs-all' classifiers).
For all networks, we used a base learning rate of $2.5 \times10^{-4}$; for the Imagenet experiment, we employed learning rate warm-up and linearly increased the learning rate from $2.5 \times10^{-4}$ to $1 \times10^{-3}$ over the first 15 epochs.
Further, we trained for 200 epochs on CIFAR-10, for 100 epochs on TinyImagenet, and for 60 epochs on the Imagenet subset; we decreased the learning rate by a factor of 2 after every 60/30/20 epochs on CIFAR-10/TinyImagenet/Imagenet.
We used a batch size of 16, 128, and 64 for CIFAR-10, TinyImagenet, and Imagenet respectively. For the Imagenet subset, we additionally used RandAugment~\citesupp{cubuk2019randaugment} with parameters $n=2$ and $m=9$; for this, we relied on the publicly available implementation at \url{https://github.com/ildoonet/pytorch-randaugment} and followed their augmentation scheme. The qualitatively evaluated model (see Figs.~5,~8, and~A1-A4) for the Imagenet subset was trained with $T=1e5$ and achieved a top-1 accuracy of 76.5\%. For comparison, we trained several ResNet-50 models (taken from the pytorch library~\citesupp{pytorch}) with the exact same training procedure, i.e., batch size, learning rate, optimiser, augmentation, etc.). The best ResNet-50 out of 4 runs achieved 79.16\% top-1 accuracy\footnote{The best test accuracies per run are given by 79.16\%, 79.04\%, 78.86\%, and 78.7\% respectively.}, which outperforms the CoDA-Net but is nevertheless comparable. While it is surely possible to achieve better accuracies for both models, long training times for the CoDA-Nets have thus far prevented us from properly optimising the architectures both on the 100 classes subset, as well as on the full Imagenet dataset. In order to scale the CoDA-Net models to larger datasets, we believe it is important to first improve the model efficiency in future work.
Lastly, when regularising the matrix entries of $\mat W_{0\rightarrow L}$, see eq.~(11),
    we regularised the absolute values for the true class $c$, $\left[\mat M_{0\rightarrow L}\right]_c$,
    and a randomly sampled incorrect class per image.
    
    
% 
% 
\subsection{Convolutional Dynamic Alignment Units}
\label{subsec:CoDAUs}
\input{supplement/C-3-CoDA-implementation}
\subsection{Attribution methods}
\label{subsec:att_details}
\input{supplement/C-4-attribution_methods}
% 
% 
\subsection{Evaluation metrics}
\label{subsec:metric_details}
\input{supplement/C-5-evaluation_metrics}
\begin{table*}
    \input{supplement/C-2-architecture_table}
    \vspace{1em}
    \caption{Architecture details for the experiments described in section 4.}
    \label{tbl:archs}
\end{table*}
{\centering
\RestyleAlgo{ruled}\LinesNumbered\setlength{\algomargin}{1.5em}
\begin{algorithm*}[htpb]
\caption{Implementation of a Convolutional Dynamic Alignment Layer}
  \label{alg:dlcconv2d}
    \setstretch{1.25}
  \DontPrintSemicolon
  \newcommand{\mycomment}[1]{{\color[RGB]{112, 128, 144}\textit{\# #1}}\;}
  \newcommand{\self}{{\bf\color[RGB]{0,24,128}{self}}}
  \newcommand{\pykey}[1]{{\bf\color[RGB]{0,128,24}{#1}}}
  \newcommand{\pyword}[1]{{\bf\color[RGB]{197,117,50}{#1}}}
  \SetKwFunction{FMain}{DAUConv2d(nn.Module)}
  \SetKwFunction{Finit}{\_\_init\_\_}
  \SetKwFunction{Ffwd}{forward}
  \SetKwProg{Fn}{\pykey{class}}{:}{}
  \SetKwProg{Imp}{}{}{}
  \SetKwProg{Df}{\pykey{def}}{:}{}
  \Imp{\normalfont \pykey{from} torch \pykey{import} nn}{}\vspace{-.25em}
  \Imp{\normalfont \pykey{import} torch.nn.functional \pykey{as} F}{}\vspace{-.6em}\;\vspace{-.25em}
  \Fn{\FMain}{\;
    \Df{\Finit{\textit{\self, in\_channels, out\_channels, rank, kernel\_size, stride, padding, act\_func}}}{
        \mycomment{act\_func: non-linearity for scaling the weights. E.g., L2 or SQ.}
        \mycomment{out\_channels: Number of convolutional DAUs for this layer.}
        \mycomment{rank: Rank of the matrix $\mat{AB}$.}
        %
        \mycomment{`dim\_reduction' applies matrix $\mat b$.}
        \self.dim\_reduction = nn.Conv2d(\textit{in\_channels, rank, kernel\_size, stride, padding}, bias=\pyword{False})\;
        \mycomment{Total dimensionality of a single patch}
        \self.patch\_dim = in\_channels $\ast$ kernel\_size $\ast$ kernel\_size\;
        \mycomment{`weightings' applies matrix $\mat a$ and adds bias $\vec b$.}
        \self.weightings = nn.Conv2d(\textit{rank, out\_channels $\ast$ \self.patch\_dim , kernel\_size=1, bias=\pyword{True}})\;
        \self.act\_func = act\_func\;
        \self.out\_channels = out\_channels\;
        \self.kernel\_size = kernel\_size\;
        \self.stride = stride\;
        \self.padding = padding\;
        }\;
        \Df{\Ffwd{\textit{\self, in\_tensor}}}{
        \mycomment{Project to lower dimensional representation, i.e., apply matrix $\mat B$. This yields $\mat B\vec p$ for every patch $\vec p$.}
        reduced = \self.dim\_reduction(in\_tensor)\;
        \mycomment{Get new spatial size height h and width w}
        h, w = reduced.shape[--2:]\;
        batch\_size = in\_tensor.shape[0]\;
        \mycomment{Apply matrix $\mat A$ and add bias $\vec b$,
        yielding $\mat a\mat b\vec p + \vec b$ for every patch $\vec p$.}
        weights = \self.weightings(reduced)\;
        \mycomment{Reshape for every location to size \textit{patch\_dim}$\times$out\_channels}
        weights = weights.view(batch\_size, \self.patch\_dim, out\_channels, h, w)\;
        \mycomment{Apply non-linearity to the weight vectors, yielding $\vec w(\vec p) = g(\mat{ab} \vec p + \vec b$) as in eq.~(1) for every patch $\vec p$.}
        weights = \self.act\_func(weights, dim=1)\;
        \mycomment{Extract patches from the input to apply dynamic weights to patches.}
        patches = F.unfold(in\_tensor, \self.kernel\_size, padding=\self.padding, stride=\self.stride)\;
        \mycomment{Reshape for applying weights.}
        patches = patches.view(batch\_size, \self.patch\_dim, 1, h, w)\;
        \mycomment{Apply the weights to the patches.}
        \mycomment{As can be seen, the output is just a weighted combination of the input, i.e., a linear transformation.}
        \mycomment{The output can thus be written as $\vec o = \mat W(\vec x)\vec x$.}
        \KwRet\ (patches $\ast$ weights).sum(1)}
        }\end{algorithm*}
}