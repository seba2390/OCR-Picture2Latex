\section{Additional qualitative results}
\label{sec:additional_figures}

\myparagraph{Additional Imagenet examples.} In Figs.~\ref{fig:first_25} and~\ref{fig:second_25} we present additional qualitative examples of the model-inherent contribution maps. In particular, we show the decomposition of the model predictions into input contributions for 50 out of 100 classes; for each class, we show the most confidently classified image and show the classes in sorted order (by confidence). In Figs.~\ref{fig:first_10_comp_1} and~\ref{fig:first_10_comp_2}, we moreover show the contributions maps for the first 10 of the overall most confidently classified images next to the attribution maps from the post-hoc importance attribution methods for qualitative comparison. We note that GradCam consistently highlights very similar regions to the CoDA-Net contribution maps, but does so at a lower resolution. All contribution maps based on the CoDA-Net use the same linear color scale, which has been set to ($-v$, $v$) with $v$ the 99.75th percentile over all absolute values in the contributions maps shown in Figs.~\ref{fig:first_25} and~\ref{fig:second_25}. For reproducing the presented contribution maps and more, please visit \url{github.com/moboehle/CoDA-Nets}.

\begin{figure*}[t]
    \centering
    \fcolorbox[rgb]{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{
    \includegraphics[width=.95\textwidth]{resources/first_25.pdf}
    }
    \caption{The first 25 most confident classifications decomposed into the contributions from each spatial location, filtered to 1 image per class. Positive (negative) contributions for the ground truth class are shown in red (blue).}
    \label{fig:first_25}
    \vspace{-1em}
\end{figure*}

\begin{figure*}[t]
    \centering
    \fcolorbox[rgb]{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{
    \includegraphics[width=.95\textwidth]{resources/second_25.pdf}
    }
    \caption{The 26th to the 50th most confident classifications decomposed into the contributions from each spatial location, filtered to 1 image per class. Positive (negative) contributions for the ground truth class are shown in red (blue).}
    \label{fig:second_25}
\end{figure*}

\begin{figure*}[t]
    \centering
    \fcolorbox[rgb]{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{
    \includegraphics[width=.95\textwidth]{resources/first_10_1.pdf}}
    \caption{Comparison between attribution methods for the 4 most confident classifications. We show positive importance attributions in red, negative attributions in blue; for RISE we use its default visualisation. Note that the model seems to align the weights well with the ornamental eyespots of the peacocks (also see the peacock in Fig.~\ref{fig:first_10_comp_2}) or the heads of the ducks in Fig.~\ref{fig:first_10_comp_2}. While the latter are also highlighted by GCam, the former constitute a structure that is too fine-grained for GCam to resolve properly.}
    \label{fig:first_10_comp_1}
\end{figure*}

\begin{figure*}[t]
    \centering
    \vspace{-1em}
    \fcolorbox[rgb]{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{0.9176470588235294, 0.9176470588235294, 0.9490196078431372}{
    \includegraphics[width=.95\textwidth]{resources/first_10_2.pdf}}
    \caption{Comparison between attribution methods for the 5th to the 10th most confident classifications. We show positive importance attributions in red, negative attributions in blue; for RISE we use its default visualisation. }
    \label{fig:first_10_comp_2}
    \vspace{-2em}
\end{figure*}

\myparagraph[0]{Regularising the linear mapping on CIFAR-10.}
As described in the main paper, the explicit representation of the model computations as a linear mapping, i.e., $$\hat{\vec y}(\vec x) = \mat w_{0\rightarrow L}(\vec x) \vec x\quad,$$
allows to explicitly regularise the linear mappings and thereby the model-inherent contribution maps. In Fig.~\ref{fig:cifar10_regul} we qualitatively show how the regularisation impacts the contribution maps. In particular, we see that without any regularisation the contribution maps are difficult to interpret. As soon as even a small regularisation is applied, however, the maps align well with the discriminative parts of the input image. 
\begin{figure*}
    \centering
    \includegraphics[width=.95\textwidth]{resources/cifar10-regul.pdf}
    \caption{Visualising the qualitative effect of explicitly regularising the linear mapping $\mat w_{0\rightarrow L}$ on CIFAR-10. While the contribution maps without regularisation are noisy, they become sharper as soon as a regularisation is applied. }
    \label{fig:cifar10_regul}
    \vspace{-1em}
\end{figure*}


\myparagraph{Sanity check.}
In~\citesupp{adebayo2018sanity}, the authors found that many commonly used methods for importance attribution are not \emph{model-faithful}, i.e., they do not reflect the learnt parameters of the model. In order to test this, they proposed to examine how the attributions change if the model parameters are randomised step by step. If the attributions remain stable under model randomisation, they cannot be assumed to explain a specific model, but rather reflect properties of the general architecture and the input data. In Fig.~\ref{fig:sanity_check}, we show how the model-inherent contribution maps behave when re-initialising the CoDA-Net layers one at a time to a random parameter setting, starting from the deepest layer. As can be seen, the contribution maps get significantly perturbed with every layer that is reset to random parameters; thus, the contribution maps pass this `sanity check' for attribution methods.
\clearpage
\begin{figure*}[t]
    % \vspace{-1em}
    \centering
    \centering
    \includegraphics[width=.95\textwidth, trim=1em 1em 1em 1em, clip]{supplement/resources/layer_perturbations.pdf}
    \caption{Sanity check experiment as in~\protect\citesupp{adebayo2018sanity}. If the importance attributions remain stable under parameter randomisation, they cannot be assumed to faithfully reflect the learnt parameters of the model. Since  the contribution maps get significantly perturbed when re-initialising layers
            from network output (left) to network input (right), the model-inherent contribution maps thus pass this sanity check.
            Positive contributions are shown in red, negative contributions in blue.
    }
    \label{fig:sanity_check}
\end{figure*}


\myparagraph{Contribution maps of a piece-wise linear model.}
In Fig.~\ref{fig:resnet_maps} we show contribution maps obtained from different pre-trained ResNet architectures obtained from
    \url{https://github.com/akamaster/pytorch_resnet_cifar10}.
In particular, we visualise the `Input$\times$Gradient' method. Note that this yields contribution maps, since piece-wise linear models, such as the ResNets, produce input-dependent linear mappings, similar to the CoDA-Nets. These contribution maps, however, are rather noisy and do not reveal particularly relevant features.

\renewcommand{\putfig}[1]{%
\begin{subfigure}[b]{.45\textwidth}
                \centering
                \includegraphics[width=\textwidth, trim=1em 6em 1em 5em, clip]{supplement/resources/resnet_maps_#1.pdf}
            \end{subfigure}
}
\begin{figure*}
\centering
\begin{subfigure}[c]{\textwidth}
    \centering
    % \vspace{-1.5em}
    \putfig{0}
    \putfig{1}
    \putfig{2}
    \putfig{3}
    \putfig{4}
    \putfig{5}
    \putfig{6}
    \putfig{7}
    \vspace{-.5em}
\end{subfigure}
\caption{`Input$\times$Gradient' evaluated on different ResNets.
            Since ResNets are piece-wise linear functions, s.t. \mbox{$\vec y(\vec x) = \mat M(\vec x) \vec  x + \vec{b}(\vec x)$},
            this is the ResNet-based equivalent to the CoDA-Net contribution maps.
            }
\label{fig:resnet_maps}
\end{figure*}