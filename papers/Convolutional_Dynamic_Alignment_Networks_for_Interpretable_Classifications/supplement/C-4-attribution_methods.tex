In section~4.2, we compare the model-inherent contribution maps of the CoDA-Nets to those of the following methods for importance attribution:
 the gradient of the class logits with respect to the input image~\citesupp{baehrens2010explain} (Grad), `Input$\times$Gradient' (IxG, cf.~\citesupp{adebayo2018sanity}), GradCam~\citesupp{selvaraju2017grad} (GCam), Integrated Gradients~\citesupp{sundararajan2017axiomatic} (IntG), DeepLIFT~\citesupp{shrikumar2017deeplift},
    several occlusion sensitivities (Occ-K, with K the size of the occlusion patch)~\citesupp{zeiler2014visualizing},
    RISE~\citesupp{petsiuk2018rise}, and LIME~\citesupp{lime}.

For RISE and LIME, we relied on the official implementations available at \url{https://github.com/eclique/RISE} and 
\url{https://github.com/marcotcr/lime} respectively. For RISE, we generated 6000 masks with parameters $s=6$ and $p_1=0.1$.
For LIME, we evaluated on 256 samples per image and used the top 3 features for the attribution;
    for the segmentation, we also used the default parameters, namely `quickshift' with $max\_dist=200$, $ratio=0.2$, and a kernel size of 4.

For Grad, GCam, IxG, IntG, DeepLIFT, and the occlusion sensitivities,
    we relied on the publicly available pytorch library `captum' (\url{https://github.com/pytorch/captum}).
GCam was used on the last activation map before global sum-pooling.
The occlusion sensitivities were used with 
    strides of 2 on CIFAR-10 and strides of 4 for TinyImagenet.
Finally, for IntG we used 20 steps for the integral approximation.
