\subsection{Interpretability of CoDA-Nets}
\label{subsec:intp_results}
\input{4.3.1-result_figure}
In the following, we evaluate the model-inherent contribution maps and compare them to other commonly used me\-thods for importance attribution.
The evaluations are based on the XL-CoDA-SQ (T=6400) for TinyImagenet and the S-CoDA-SQ (T=1000) for CIFAR-10, see~Table~\ref{tbl:result_table} for the respective accuracies.
Further, we evaluate the effect of training the same CIFAR-10 architecture with different temperatures $T$; as discussed in sec.~\ref{subsec:coda}, we expect the interpretability to \emph{increase} along with $T$, since for larger $T$ a stronger alignment is required in order for the models to obtain large class logits. Evaluations of models trained with L1-regularisation of the matrices $\mat m_{0\rightarrow L}$ (eq.~\eqref{eq:loss}) and of models with the L2 non-linearity (eq.~\eqref{eq:nonlin}) are included in the supplement. The respective results are similar to those presented here. Before turning to the results, however, in the following we will first present the attribution methods used for comparison and discuss the evaluation metrics employed for quantifying their interpretability.

\myparagraph{Attribution methods.} We compare the model-inherent contribution maps (cf.~eq.~\eqref{eq:contrib}) to other common approaches for importance attribution. 
In particular, we evaluate against several perturbation based methods such as RISE~\cite{petsiuk2018rise}, LIME~\cite{lime}, and several occlusion attributions~\cite{zeiler2014visualizing} (Occ-K, with K the size of the occlusion patch). Additionally, we evaluate against common gradient-based methods. These include the gradient of the class logits with respect to the input image~\cite{baehrens2010explain} (Grad), `Input$\times$Gradient' (IxG, cf.~\cite{adebayo2018sanity}), GradCam~\cite{selvaraju2017grad} (GCam), Integrated Gradients~\cite{sundararajan2017axiomatic} (IntG), and DeepLIFT~\cite{shrikumar2017deeplift}. As a baseline, we also evaluated these methods on a pre-trained ResNet-56~\cite{he2016deep} on CIFAR-10, for which we show the results in the supplement.


\myparagraph{Evaluation metrics.}
Our quantitative evaluation of the attribution maps is based on the following two methods:
we
    \mbox{\colornum{(1)} evaluate} a localisation metric by adapting the pointing game~\cite{zhang2018top} to the CIFAR-10 and TinyImagenet datasets, and 
    \mbox{\colornum{(2)} analyse} the model behaviour under the pixel removal strategy employed in~\cite{srinivas2019full}.
For~\colornum{(1)}, 
we evaluate the attribution methods on a grid of $ n\times n$ with $n=3$ images sampled from the corresponding datasets; in every grid of images, each class may occur at most once. 
For a visualisation with $n=2$, see
Fig.~\ref{fig:multi_image}.
\input{4.3.4-multi-image}
\input{4.3.5-comparison_figure}
%
For each occurring class, we can measure how much positive importance an attribution method assigns to the respective class image.
Let $\mathcal{I}_c$ be the image for class $c$, then the score $s_c$ for this class is calculated as 
\begin{align}
\label{eq:loc_metric}
            \textstyle s_c = \frac{1}{Z}\sum_{p_c \in \mathcal{I}_c} p_c \quad \text{with}\quad 
            Z = {\sum_k\sum_{p_c \in \mathcal{I}_k} p_c}\quad ,
        \end{align}
        with $p_c$ the positive attribution for class $c$ assigned to the spatial location $p$.
        This metric has the same clear oracle score  $s_c=1$ for all attribution methods (all positive attributions located in the correct grid image)
        and a clear score for completely random attributions $s_c=1/n^2$ (the positive attributions are uniformly distributed over the different grid images).
        Since this metric depends on the classification accuracy of the models, we sample the first $500$ (CIFAR-10) or $250$ (TinyImagenet) images according to their class score for the ground-truth class\footnote{
        We can only expect an attribution to specifically highlight a class image if this image can be correctly classified on its own. If all grid images have similarly low attributions, the localisation score will be random.
        }; note that since all attributions are evaluated for the same model on the same set of images, this does not favour any particular attribution method.\\
For~\colornum{(2)}, we show how the model's class score behaves under the removal of an increasing amount of \emph{least important} pixels,
    where the importance is obtained via the respective attribution method. Since the first pixels to be removed are typically assigned negative or relatively little importance, we expect the model to initially increase its confidence (removing pixels with \emph{negative} impact) or maintain a similar level of confidence (removing pixels with \emph{low} impact) if the evaluated attribution method produces an accurate ranking of the pixel importance values. 
Conversely, if we were to remove the \emph{most important} pixels first, we would expect the model confidence to quickly decrease. However, as noted by \cite{srinivas2019full}, removing the most important pixels first introduces artifacts in the most important regions of the image and is therefore potentially more unstable than removing the least important pixels first.
Nevertheless, the model-inherent contribution maps perform well in this setting, too, as we show in the supplement.
Lastly, in the supplement we qualitatively show that they pass the `sanity check' of \cite{adebayo2018sanity}.

\myparagraph[0]{Quantitative results.}
In Fig.~\ref{fig:localisation}, we compare the contribution maps of the CoDA-Nets to other attributions under the evaluation metrics discussed above.
It can be seen that the CoDA-Nets~\colornum{(1)}
    perform well under the localisation metric given by eq.~\eqref{eq:loc_metric} and outperform all 
    the other attribution methods evaluated on the same model, both for TinyImagenet (top row, left) and CIFAR-10 (top row, center); note that we excluded RISE and LIME on CIFAR-10, since the default parameters do not seem to transfer well to this low-resolution dataset. 
Moreover, \colornum{(2)} 
    the CoDA-Nets perform well in the pixel-removal setting: 
    the \emph{least salient} locations according to the model-inherent contributions indeed seem to be among the least relevant for the given class score on both datasets, see Fig.~\ref{fig:localisation} (bottom row, left and center). 
Further, in Fig.~\ref{fig:localisation} (right column),  we show the effect of temperature scaling on the interpretability of CoDA-Nets trained on CIFAR-10. The results indicate that the alignment maximisation is indeed crucial for interpretability and constitutes an important difference of the CoDA-Nets to other dynamic linear networks such as piece-wise linear networks (ReLU-based networks). In particular, by structurally requiring a strong alignment for confident classifications, the interpretability of the CoDA-Nets forms part of the optimisation objective.
Increasing the temperature increases the alignment and thereby the interpretability of the CoDA-Nets. While we observe a downward trend in classification accuracy when increasing $T$, the best model at $T=10$ only slightly improved the accuracy compared to $T=1000$ ($93.2\%\rightarrow 93.6\%$); for more details, see supplement.

In summary, the results show that by combining dynamic linearity with a structural bias towards an alignment with discriminative patterns, we obtain models which inherently provide an interpretable linear decomposition of their predictions. 
Further, given that we better understand the relationship between the intermediate computations and the optimisation of the final output in the CoDA-Nets, we can emphasise model interpretability in a principled way by increasing the `alignment pressure' via \emph{temperature scaling}.
    
\myparagraph[0]{Qualitative results.} In Fig.~\ref{fig:quality}, we visualise spatial contribution maps of the L-CoDA-SQ model (trained on Imagenet-100) for some of its most confident predictions. Note that these contribution maps are linear decompositions of the output and the sum over these maps yields the respective class logit. In Fig.~\ref{fig:comparison}, we additionally present a visual comparison to the best-performing post-hoc attribution methods; note that RISE cannot be displayed well under the same color coding and we thus use its default visualisation. We observe that the different methods are not inconsistent with each other and roughly highlight similar regions. However, the inherent contribution maps are of much higher detail and compared to the perturbation-based methods do not require multiple model evaluations. Much more importantly, however, all the other methods are attempts at approximating the model behaviour \emph{post-hoc}, while the CoDA-Net contribution maps in Fig.~\ref{fig:quality} are derived from the model-inherent linear mapping that is used to compute the model output.