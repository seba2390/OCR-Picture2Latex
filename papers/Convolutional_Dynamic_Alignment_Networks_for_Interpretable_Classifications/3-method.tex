\section{Dynamic Alignment Networks}
\label{subsec:alignment}

\input{3.0-method_intro}
\input{3.1-alignment_figure}
\subsection{Dynamic Alignment Units}
\label{subsec:align_units}
We define the Dynamic Alignment Units (DAUs) by
\begin{align}
    \label{eq:au}
    \text{DAU}(\vec x) = g(\mat a \mat b\vec x +\vec b)^T \vec x = \vec w(\vec x)^T\, \vec x\quad \textbf{.}
\end{align}
% 
Here, $\vec x\in\mathbb R^{d}$ is an input vector, $\mat a\in\mathbb R^{d\times r}$ and $\mat b \in \mathbb R^{r\times d}$ are trainable transformation matrices, $\vec b\in\mathbb R^{d}$ a trainable bias vector, and \mbox{$g(\vec u)=\alpha(||\vec u||)\vec u$} is a non-linear function that scales the norm of its input. {In contrast to using a single matrix $\mat m \in\mathbb R^{d\times d}$, using $\mat{ab}$ allows us to control the maximum rank $r$ of the transformation and to reduce the number of parameters}; we will hence refer to $r$ as the rank of a DAU. 
%
As can be seen by the right-hand side of eq.~\eqref{eq:au}, the DAU linearly transforms the input $\vec x$ (\colornum{P1}). At the same time, given the quadratic form ($\vec x^T\mat B^T\mat A^T\vec x$) and the  rescaling function $\alpha(||\vec u||)$, the output of the DAU is a non-linear function of its input. In this work, we focus our analysis on 
two choices for $g(\vec u)$ in particular\footnote{
In preliminary experiments we observed comparable behaviour over a range of different normalisation functions such as, e.g., L1 normalisation.}, namely rescaling to unit norm ($\text{L2}$) and the squashing function ($\text{SQ}$, see \cite{sabour2017dynamic}):
\begin{align}
    \label{eq:nonlin}
    \text{L2}(\vec u) = \frac{\vec u}{||\vec u||_2} \;\;\text{and}\;\;
    \text{SQ}(\vec u) = \text{L2}(\vec u) \times \frac{||\vec u||^2_2}{1+||\vec u||_2^2}
\end{align}
Under these rescaling functions, the norm of the weight vector is upper-bounded: $||\vec w(\vec x)|| \leq 1$. Therefore, the output of the DAUs is upper-bounded by the norm of the input:
\begin{align}
    \text{DAU}(\vec x) = 
    ||\vec w(\vec x)|| \hspace{.2em} ||\vec x|| \cos(\angle(\vec x, \vec w(\vec x)))\leq ||\vec x||
    \label{eq:bound}
\end{align}
As a corollary, for a given input $\vec x_i$, the DAUs can only achieve this upper bound if $\vec x_i$ is an eigenvector (EV) of the linear transform $\mat{AB}\vec x+ \vec b$. Otherwise, the cosine in eq.~\eqref{eq:bound} will not be maximal\footnote{
Note that $\vec w(\vec x)$ is proportional to $\mat{ab}\vec x + \vec b$. The cosine in eq.~\eqref{eq:bound}, in turn, is maximal if and only if $\vec w(\vec x_i)$ is proportional to $\vec x_i$ and thus, by transitivity, if $\vec x_i$ is proportional to $\mat{ab}\vec x_i + \vec b$. This means that $\vec x_i$ has to be an EV of $\mat{ab}\vec x +\vec b$ to achieve maximal output.}. 
As can be seen in eq.~\eqref{eq:bound}, maximising the average output of a DAU over a set of inputs $\{\vec x_i|\,i=1, ..., n\}$
maximises the alignment between $\vec w(\vec x)$ and $\vec x$ (\colornum{P2}).
In particular, it optimises the parameters of the DAU such that the \emph{most frequent input patterns} are encoded as EVs in the linear transform $\mat{ab}\vec x + \vec b$, similar to an $r$-dimensional PCA decomposition ($r$ the rank of $\mat{ab}$). In fact, as discussed in the supplement, the optimum of the DAU maximisation solves a low-rank matrix approximation~\cite{eckart1936approximation} problem similar to singular value decomposition.
\begin{figure}[t!]
    \centering
    \includegraphics[height=6.5em]{resources/evs.pdf}
    \caption{\small Eigenvectors (EVs) of \tmat{AB} after maximising the output of a rank-3 DAU over a set of noisy samples of 3 MNIST digits. Effectively, the DAUs encode the most frequent components in their EVs, similar to a principal component analysis (PCA).
    }
    \label{fig:EVs}
\end{figure}
%
As an illustration of this property, in Fig.~\ref{fig:EVs} we show the 3 EVs\footnote{Given $r=3$, the EVs maximally span a 3-dimensional subspace.} of matrix $\mat{ab}$ (with rank $r=3$, bias $\vec b=\vec 0$) after optimising a DAU over a set of $n$ noisy samples of 3 specific MNIST~\cite{lecun2010mnist} images; for this, we used $n=3072$ and zero-mean Gaussian noise. As expected, the EVs of \tmat{ab} encode the original, noise-free images, since this on average maximises the alignment (eq.~\eqref{eq:bound}) between the weight vectors $\vec w(\vec x_i)$ and the input samples $\vec x_i$ over the dataset.
%
%

\subsection{DAUs for classification}
\label{subsec:classification}
{DAUs can be used directly for classification by applying $k$ DAUs in parallel to obtain an output \mbox{$\hat{\vec y}(\vec x)=\left[\text{DAU}_1(\vec x), ..., \text{DAU}_k(\vec x)\right]$}. 
Note that this is a linear transformation $\hat{\vec y}(\vec x)$$=$$\mat W(\vec x) \vec x$, with each row in $\mat w$$\in$$\mathbb R^{k \times d}$ corresponding to the weight vector $\vec w_j^T$ of a specific DAU $j$.
In particular, consider 
a dataset $\mathcal D = \{(\vec x_i, \vec y_i)|\, \vec x_i\in\mathbb R^d, \vec y_i\in\mathbb R^k\}$ of $k$ classes with `one-hot' encoded labels $\vec y_i$ for the inputs $\vec x_i$.
To optimise the DAUs as classifiers on $\mathcal D$,} we can apply a sigmoid non-linearity to each DAU output and optimise the loss function $\mathcal L = \sum_i\text{BCE}(\sigma(\hat{\vec y}_i), \vec y_i)$, where \text{BCE} denotes the binary cross-entropy and $\sigma$ applies the sigmoid function to each entry in $\hat{\vec y}_i$. Note that for a given sample, \text{BCE} either maximises (DAU for correct class) or minimises (DAU for incorrect classes) the output of each DAU. Hence, this classification loss will still maximise the (signed) cosine between the weight vectors $\vec w(\vec x_i)$ and $\vec x_i$. 

To illustrate this property, in Fig.~\ref{fig:alignment} (top) we show the weights $\vec w(\vec x_i)$ for several samples of the digit `3' after optimising the DAUs for classification on a noisy MNIST dataset; the first two are correctly classified, the last one is misclassified as a `5'. As can be seen, the weights align with the respective input (the weights for different samples are different). However,  different parts of the input are either positively or negatively correlated with a class, which is reflected in the weights: for example, the extended stroke on top of the `3' in the misclassified sample is assigned \emph{negative weight} and, since the background noise is \emph{uncorrelated} with the class labels, it is not represented in the weights. 

In a classification setting, the DAUs {thus} encode \emph{the most frequent discriminative patterns} in the linear transform $\mat{ab}\vec x + \vec b$ such that the dynamic weights $\vec w(\vec x)$ align well with these patterns.
Additionally, since the output for class $j$ is a linear transformation of the input (\colornum{P1}), we can compute the contribution vector $\vec s_j$ containing the per-pixel contributions to this output by the element-wise product ($\odot$)
\begin{align}
\label{eq:contrib_1}
    \vec s_j(\vec x_i) = \vec w_j(\vec x_i)\odot\vec x_i\quad ,
\end{align}
 see Figs.~\ref{fig:teaser} and
\ref{fig:alignment}. 
Such linear decompositions constitute the model-inherent `explanations' which we evaluate in sec.~\ref{sec:results}.
\subsection{Convolutional Dynamic Alignment Networks}
\label{subsec:coda}
The modelling capacity of a single layer of DAUs is limited, similar to a single linear classifier. However, DAUs can be used as the basic building block for deep convolutional neural networks, which yields powerful classifiers. Importantly, in this section we show that such a Convolutional Dynamic Alignment Network (CoDA-Net) inherits the properties (\colornum{P3}) of the DAUs by maintaining both the dynamic linearity (\colornum{P1}) as well as the alignment maximisation (\colornum{P2}). For a convolutional dynamic alignment layer, each filter is modelled by a DAU, similar to dynamic local filtering layers~\cite{jia2016dynamic}. Note that the output of such a layer is also a dynamic linear transformation of the input to that layer, since a convolution is equivalent to a linear layer with certain constraints on the weights, cf.~\cite{convlin}. We include the implementation details in the supplement.
Finally, at the end of this section, we highlight an important difference between output maximisation and optimising for classification with the {BCE} loss. In this context we discuss the effect of \emph{temperature scaling} and present the loss function we optimise in our experiments.

\myparagraph{Dynamic linearity (\colornum{P1}).} In order to see that the linearity is maintained, we note that the successive application of multiple layers of DAUs also results in a dynamic linear mapping. Let $\mat W_l$ denote the linear transformation matrix produced by a layer of DAUs and let $\vec a_{l-1}$ be the input vector to that layer; as mentioned before, each row in the matrix $\mat w_l$ corresponds to the weight vector of a single DAU\footnote{
Note that this also holds for convolutional DAU layers. Specifically, each row in the matrix $\mat w_l$ corresponds to a single DAU applied to exactly one spatial location in the input and the input with spatial dimensions is vectorised to yield $\vec a_{l-1}$. For further details, we kindly refer the reader to~\cite{convlin} and the implementation details in the supplement of this work.}. As such, the output of this layer is given by 
\begin{align}
    \vec a_l = \mat W_l (\vec a_{l-1}) \vec a_{l-1}\quad .
\end{align}
In a network of DAUs, the successive linear transformations can thus be collapsed. In particular, \emph{for any pair of activation vectors} $\vec{a}_{l_1}$ and $\vec{a}_{l_2}$ with ${l_1}<{l_2}$, the vector $\vec{a}_{l_2}$ can 
    be expressed as a linear transformation of $\vec{a}_{l_1}$:
\begin{align}
\label{eq:collapse}
    \vec{a}_{l_2} &= \mat{W}_{{l_1}\rightarrow {l_2}} \left(\vec{a}_{l_1}\right)\vec{a}_{l_1} \quad 
        \\{with} \quad \mat{W}_{{l_1}\rightarrow {l_2}}\left(\vec{a}_{l_1}\right) &= \textstyle\prod_{k={l_1}+1}^{l_2} \mat{W}_k \left(\vec{a}_{k-1}\right)\quad \text{.}
\end{align}
For example, the matrix $\mat W_{0\rightarrow L}(\vec{a}_0 = \vec{x}) = \mat W(\vec{x})$ models the linear transformation from the input to the output space, see Fig.~\ref{fig:teaser}.
Since this linearity holds between any two layers, the $j$-th entry of any activation vector $\vec a_l$ in the network can be decomposed into input contributions via:
    \begin{align}
    \label{eq:contrib}
        \vec{s}_{j}^l(\vec x_i) = \left[\mat W_{0\rightarrow l} (\vec{x}_i)\right]_j^T \odot \vec x_i\quad \text{,}
    \end{align}
    with $[\mat W]_j$ the $j$-th row in the matrix.
%

\myparagraph{Alignment maximisation (\colornum{P2}).}
Note that the output of a CoDA-Net is bounded independent of the network parameters: since each DAU operation can---independent of its parameters---at most reproduce the norm of its input (eq.~\eqref{eq:bound}), the linear concatenation of these operations necessarily also has an upper bound which does not depend on the parameters.
Therefore, in order to achieve maximal outputs on average (e.g., the class logit over the subset of images of that class), all DAUs in the network need to produce weights $\vec w (\vec a_l)$ that align well with the class features. In other words, the weights will align with discriminative patterns in the input.
For example, in Fig.~\ref{fig:alignment} (bottom), we visualise the `global matrices' $\mat W_{0\rightarrow L}$ and the corresponding contributions (eq.~\eqref{eq:contrib}) for a $L=5$ layer CoDA-Net. As before, the weights align with discriminative patterns in the input and do not encode the uninformative noise.
%
%
%
%

\myparagraph[0]{Temperature scaling and loss function.} 
\begin{figure}[t]
    \centering
    \includegraphics[width=.45\textwidth]{resources/Temperature_qualitative.pdf}
    \caption{\small By lowering the upper bound (cf.~eq.~\eqref{eq:bound}), the correlation maximisation in the DAUs can be emphasised.
    We show contribution maps for a model trained with different temperatures.
    }
    \label{fig:scaling}
\end{figure}
So far we have assumed that minimising the {BCE} loss for a given sample is equivalent to applying a maximisation or minimisation loss to the individual outputs of a CoDA-Net. While this is in principle correct, {BCE} introduces an additional, non-negligible effect: \emph{saturation}. Specifically, it is possible for a CoDA-Net to achieve a low {BCE} loss without the need to produce well-aligned weight vectors. As soon as the classification accuracy is high and the outputs of the networks are large, the gradient---and therefore the \emph{alignment pressure}---will vanish. This effect can, however, easily be mitigated:
 as discussed in the previous paragraph, the output of a CoDA-Net is upper-bounded \textit{independent of the network parameters}, since each individual DAU in the network is upper-bounded. 
By scaling the network output with a temperature parameter $T$ such that 
    $\hat{\vec y} (\vec x) = T^{-1} \mat W_{0\rightarrow L}(\vec x)\,\vec x$, 
we can explicitly decrease this upper bound and thereby increase the \emph{alignment pressure} in the DAUs by avoiding the early saturation due to {BCE}.
In particular, the lower the upper bound is, the stronger the induced DAU output maximisation should be, since the network needs to accumulate more signal to obtain large class logits (and thus a negligible gradient). This is indeed what we observe both qualitatively, cf.~Fig.~\ref{fig:scaling}, and quantitatively, cf.~Fig.~\ref{fig:localisation} (right column).
Alternatively, the representation of the network's computation as a linear mapping allows to directly regularise what properties these linear mappings should fulfill. For example, we show in the supplement that by regularising the absolute values of the matrix $\mat W_{0\rightarrow L}$, we can induce sparsity in the signal alignments, which can lead to sharper heatmaps.
%
The overall loss for an input $\vec x_i$ and the target vector $\vec y_i$ is thus computed as 
    \begin{align}
        \label{eq:loss}
        \mathcal{L}(\vec x_i, \vec y_i) &= 
        \text{BCE}(\sigma(T^{-1} \mat W_{0\rightarrow L}(\vec x_i)\,\vec{x}_i + {\vec{b}}_0)\,,\, \vec{y}_i) \\&+ 
        \lambda \langle | \mat W_{0\rightarrow L}(\vec x_i) |\rangle\quad \text{.}
    \end{align}
    Here, $\lambda$ is the strength of the regularisation, $\sigma$ applies the sigmoid activation to each vector entry,
    ${\vec{b}}_0$ is a fixed bias term, and $\langle|\mat W_{0\rightarrow L}(\vec x_i)|\rangle$ refers to the mean over the absolute values of 
        all entries in the matrix $\mat W_{0\rightarrow L}(\vec x_i)$.
%
%
%
\subsection{Implementation details}
\label{subsec:details}
\myparagraph[-.25]{Shared matrix \tmat b.} In our experiments, we opted to share the matrix $\mat b\in \mathbb R^{r\times d}$ between all DAUs in a given layer. This increases parameter efficiency by having the DAUs share a common $r$-dimensional subspace and still fixes the maximal rank of each DAU to the chosen value of $r$. 

\myparagraph[-.25]{Input encoding.} 
In sec.~\ref{subsec:align_units}, we showed that the norm-weighted cosine similarity between the dynamic weights and the layer inputs is optimised and the output of a DAU is at most the norm of its input. This favours pixels with large RGB values, since these have a larger norm and can thus produce larger outputs in the maximisation task. To mitigate this bias, we add the negative image as three additional color channels and thus encode each pixel in the input %is encoded 
as 
\mbox{[$r$, $g$, $b$, $1-r$, $1-g$, $1-b$]}, with $r, g, b\in [0, 1]$.
