
In this section, we present our novel type of network architecture: the Convolutional Dynamic Alignment Networks (CoDA-Nets). For this, we first introduce Dynamic Alignment Units (DAUs) as the basic building blocks of CoDA-Nets and discuss two of their key properties in sec.~\ref{subsec:align_units}. Concretely, we show that these units linearly transform their inputs with dynamic (input-dependent) weight vectors and, additionally, that they are biased to align these weights with the input during optimisation. 
We then discuss how DAUs can be used for classification (sec.~\ref{subsec:classification}) and how we build performant networks out of multiple layers of convolutional DAUs (sec.~\ref{subsec:coda}). Importantly, the resulting \emph{linear decompositions} of the network outputs are optimised to align with discriminative patterns in the input, making them highly suitable for interpreting the network predictions. 

In particular, we structure this section around the following \textbf{three important properties} (\colornum{P1-P3}) of the DAUs:
\\[.25em]
\colornum{P1: Dynamic linearity.} The DAU output $o$ is computed as a dynamic (input-dependent) linear transformation of the input $\vec x$, such that \mbox{$o=\vec w(\vec x)^T\vec x=\sum_jw_j(\vec x)x_j$}. 
Hence, 
$o$ can be decomposed into contributions  
from individual input dimensions, which are given by $w_j(\vec x)x_j$ for dimension $j$.
\\[.5em]
\colornum{P2: Alignment maximisation.} Maximising the average output of a single DAU over a set of inputs $\vec x_i$ 
maximises the alignment between inputs $\vec x_i$ and the weight vectors $\vec w(\vec x_i)$. As the modelling capacity of $\vec w(\vec x)$ is restricted, $\vec w(\vec x)$ will encode the most frequent patterns in the set of inputs $\vec x_i$.
\\[.5em]
\colornum{P3: Inheritance.} When combining multiple DAU layers to form a \mbox{Dynamic} Alignment Network (DA-Net), the properties \colornum{P1} and \colornum{P2} are \emph{inherited}. In particular, DA-Nets are dynamic linear (\colornum{P1}) and maximising the last layer's output induces an output maximisation in the constituent DAUs (\colornum{P2}).
\\[.5em]
These properties increase the interpretability
of a DA-Net, such as a CoDA-Net (sec.~\ref{subsec:coda}) for the following reasons.
First, the output of a DA-Net can be decomposed into contributions from the individual input dimensions, similar to linear models (cf.~Fig.~\ref{fig:teaser}, \colornum{P1} and \colornum{P3}).
Second, we note that optimising a neural network for classification applies a maximisation to the outputs of the last layer for every sample. 
This maximisation aligns the dynamic weight vectors $\vec w(\vec x)$ of the constituent DAUs of the DA-Net with their respective inputs (cf.~Fig.~\ref{fig:alignment}, \colornum{P2} and \colornum{P3}).

 Importantly, the weight vectors will align with the \emph{discriminative} patterns in their inputs when optimised for classification as we show in sec.~\ref{subsec:classification}.
As a result, the model-inherent contribution maps of CoDA-Nets are optimised to align well with \emph{discriminative input patterns} in the input image 
and the interpretability of our models thus forms part of the global optimisation procedure.