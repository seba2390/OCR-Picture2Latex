\subsection{Setup and model performance}
\label{subsec:accuracy}
\input{4.2-result_table}
\myparagraph[-.25]{Datasets.} We evaluate and compare the accuracies of the CoDA-Nets to other work on the CIFAR-10~\cite{krizhevsky2009cifar10} and the TinyImagenet~\cite{tinyimagenet} datasets. We use the same datasets for the quantitative evaluations of the model-inherent contribution maps. Additionally, we qualitatively show high-resolution examples from a CoDA-Net trained on the first 100 classes of the Imagenet dataset. 

\myparagraph[-.25]{Models.} 
We evaluate models of four different sizes
denoted by (S/M/L/XL)-CoDA on CIFAR-10 (S and M), Imagenet-100 (L), and TinyImagenet (XL); these models have 8M (S), 28M (M), 48M (L), and 62M (XL) parameters respectively; see the supplement for an evaluation of the impact of model size on accuracy.
All models feature 9 convolutional DAU layers and a final sum-pooling layer, and mainly vary in the number of features, the rank $r$ of the DAUs,  and the convolutional strides for reducing the spatial dimensions. 
No additional methods such as residual connections, dropout, or batch normalisation are used. This 9-layer architecture was initially optimised for the CIFAR-10 dataset and subsequently adapted to the TinyImagenet and Imagenet-100 datasets. 
Further, we investigate the effect that the temperature $T$, the regularisation $\lambda$, and the non-linearities (\text{L2}, \text{SQ}, see eq.~\eqref{eq:nonlin}) have on the CoDA-Nets. Given the computational cost of the regularisation (two additional passes to extract and regularise $\mat w_{0\rightarrow L}$), evaluate the regularisation on models trained on CIFAR-10. Lastly, models marked with $T$ ($\lambda$) in Table \ref{tbl:result_table} were trained with $\lambda$$=$$0$ ($T$$=$$64$, equiv.~to `average pooling'). Details on architectures and training procedure are included in the supplement.

\myparagraph{Classification performance.} In Table \ref{tbl:result_table} we compare the performances of our CoDA-Nets to several other published results. Note that the referenced numbers are meant to be used as a gauge for assessing the CoDA-Net performance and do not exhaustively represent the state of the art. In particular, we would like to highlight that the CoDA-Net performance is on par to models of the VGG~\cite{vgg} and ResNet~\cite{he2016deep} model families on both datasets. Moreover, under the same data augmentation (RandAugment~\cite{cubuk2019randaugment}), it achieves similar results as the WideResNet-28-2~\cite{zagoruyko2016wide} on CIFAR-10.
Additionally, we list the reported results of the SENNs~\cite{melis2018towards} and the DE-CapsNet~\cite{jia2020capsnet} architectures for CIFAR-10. Similar to our CoDA-Nets, the SENNs were designed to improve network interpretability and are also based on the idea of explicitly modelling the output as a dynamic linear transformation of the input. On the other hand, the CoDA-Nets share similarities to capsule networks, which we discuss in the supplement; to the best of our knowledge, the \mbox{DE-CapsNet} currently achieves the state of the art in the field of capsule networks on CIFAR-10. 
Overall, we observed that the CoDA-Nets deliver competitive performances that are fairly robust to the non-linearity (\text{L2}, \text{SQ}), the temperature ($T$), and the regularisation strength ($\lambda$). We note that on average SQ performed better than L2, which we ascribe to the fact that SQ avoids up-scaling vectors with low norm ($||\vec v||<1$, see eq.~\eqref{eq:nonlin}).

\myparagraph[-.3]{Efficiency considerations.} 
The CoDa-Nets achieve good accuracies on the presented datasets, exhibit training behaviour that is robust over a wide range of hyperparameters, and are as fast as a typical ResNet at inference time.
However, under the current formulation and without highly optimised GPU implementations for the DAUs, training times are significantly longer for the CoDA-Nets.
While we are currently working on an improved and optimised version of CoDA-Nets, we were not yet able to generate results for the full ImageNet dataset.
On the 100 classes subset, however, the evaluated L-CoDA-SQ network achieved competitive performance (76.5\% accuracy, for details see supplement) and offers highly detailed explanations for its predictions, as we show in Figs.~\ref{fig:quality} and \ref{fig:comparison}.
