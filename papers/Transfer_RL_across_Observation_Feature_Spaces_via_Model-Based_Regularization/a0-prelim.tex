\section{Additional Preliminary Knowledge}
\label{app:prelim}


For any policy $\pi$, its Q value $Q^\pi$ is the unique fixed point of the Bellman operator

\begin{equation}
\label{eq:bellman_pi}
    (\bellman^\pi Q) (o,a) = \mathbb{E}_{o^\prime \sim P(o,a), a^\prime \sim \pi(o^\prime)}[R(o,a) + \gamma  Q(o^\prime, a^\prime)] 
\end{equation}  

% The Q value of an optimal policy, denoted as $Q^*$, is the unique fixed point of the optimality Bellman operator
% \begin{equation}
%     (\bellman^* Q) (o,a) = \mathbb{E}_{o^\prime \sim P(o,a)}[R(o,a) + \gamma \max_{a^\prime \in \actions} Q(o^\prime, a^\prime)] 
% \end{equation} 
% One can use policy iteration or value iteration\ys{cite} to find the optimal policy with the highest Q value $Q^*$.
The optimal policy can be found by \textit{policy iteration}~\citep{howard1960dynamic}, where one starts from an initial policy $\pi_0$ and repeats policy evaluation and policy improvement. More specifically, at iteration $k$, the algorithm evaluates $Q_{\pi_k}$ via Equation~(\ref{eq:bellman_pi}), then improves the policy by $\pi_{k+1}(o) := \mathrm{argmax}_{a\in\actions} Q_{\pi_k}(o,a), \forall o \in\states.$
% \begin{equation}
%     \pi_{k+1}(o) := \mathrm{argmax}_{a\in\actions} Q_{\pi_k}(o,a), \forall o \in\states.
% \end{equation}
It is well-known that the policy iteration algorithm converges to the optimal policy under mild conditions~\citep{puterman2014markov}.
When the dynamics $P$ and $R$ are unknown, reinforcement learning algorithms use interaction samples from the environment to approximately solve $\hat{Q}_{\pi_k}$.
% =:Q_k$ \fh{What is $=:$? You mean $:=$? Why do you denote it to be $Q_k$?}. 
Prior works~\citep{BertsekasTsitsiklis96,munos2005error} have shown that if the approximation error is bounded by a small constant, the performance of $\pi_k$ as $k\to\infty$ is guaranteed to be close to the optimal policy value $Q_{\pi^*}$, which we also denote as $Q^*$.