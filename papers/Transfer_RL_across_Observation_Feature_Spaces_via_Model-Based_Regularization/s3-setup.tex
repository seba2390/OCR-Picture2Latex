\vspace{-1.5em}
\section{Problem Setup: Transfer Across Different Observation Spaces}
\label{sec:setup}
\vspace{-0.5em}

We aim to transfer knowledge learned from a source MDP to a target MDP, whose observation spaces are different while dynamics are structurally similar.
Denote the source MDP as $\mathcal{M}\source = \langle \states\source, \actions, P\source, R\source, \gamma \rangle$, and the target MDP as $\mathcal{M}\target = \langle \states\target, \actions, P\target, R\target, \gamma \rangle.$ 
% Although our method can be generalized to different action spaces as will be clear after the introduction of the algorithm, we focus on the scenario that the two MDPs share the same action space.
Note that $\states\source$ and $\states\target$ can be significantly different, such as $\states\source$ being a low-dimensional vector space and $\states\target$ being a high-dimensional pixel space, which is challenging for policy transfer since the source target policy have different input shapes and would typically be very different architecturally. 

% Although the dynamics of the two MDPs $(P\source, R\source)$ and $(P\target, R\target)$ are defined on different state spaces, we suppose they share some structural similarities, otherwise knowledge transfer is not meaningful. 
% As motivated in the introduction, the target task has a different observation while the underlying dynamics do not change.
% More formally, we make the following mild assumption.
In this work, as motivated in the Introduction, we focus on the setting wherein the dynamics (($P\source, R\source$) and ($P\target, R\target$)) of the two MDPs between which we transfer knowledge are defined on different observation spaces but share structural similarities. Specifically, we make the assumption that there exists a mapping between the source and target observation spaces such that the transition dynamics under the mapping in the target task share the same transition dynamics as in the source task. We formalize this in Assumption~\ref{assum:similarity}:

\begin{assumption}
\label{assum:similarity}
There exists a function $f: \states\target \to \states\source$ such that $\forall o_i\target, o_j\target \in \states\target, \forall a\in\actions$,
% \setlength\abovedisplayskip{2pt}
% \setlength\belowdisplayskip{2pt}
\begin{equation*}
    P\target(o_j\target|o_i\target,a) = P\source(f(o_j\target) | f(o_i\target), a), \quad 
    R\target(o_i\target,a) = R\source(f(o_i\target),a).
\end{equation*}
% $P\target(o_j\target|o_i\target,a) = P\source(f(o_j\target) | f(o_i\target), a)$, $R\target(o_i\target,a) = R\source(f(o_i\target),a)$.
\end{assumption}
\textbf{Remarks.} 
(1) Assumption~\ref{assum:similarity} is mild as many real-world scenarios fall under this assumption. For instance, when upgrading the cameras of a patrol robot to have higher resolutions, such a mapping $f$ can be a down-sampling function.
(2) $f$ is a general function without extra restrictions. $f$ can be a many-to-one mapping, i.e., more than one target observations can be related to the same observation in the source task. $f$ can be non-surjective, i.e., there could exist source observations that do not correspond to any target observation.
% (3) Many real-world scenarios fall under this assumption. For instance, when upgrading the cameras of a patrol robot to have higher resolutions, the assumption of the existence of such a mapping $f$ is reasonable as it can be 
% such a mapping $f$ can be a simple down-sampling function. Therefore, 
% the new observations can be mapped to the old observation space by appropriate down-sampling. \fh{what about games?}
% \fh{This provides flexibility xxxxxx.}
% \fh{(3) Many real-world applications fall under this assumption. For instance, upgrading the cameras of a patrol robots to higher resolutions, designing harder games (polish the wording), xxx }

% Assumption~\ref{assum:bijection} suggests that the source observation and the target observation are two different views of the same environment. For example, in a maze environment, one agent is given the precise locations (in x-y axes) of itself and the exit, while another agent can observe a top-down view of the whole map. The dynamics of the maze is the same for the two different agents, but the latter task is much harder due to the high-dimensional observation space. 
% Our goal is to extract knowledge of the environment dynamics from the source task, and transfer the knowledge to the target task to facilitate learning.

Many prior works~\citep{mann2013directed,brys2015policy} have similar assumptions, but require prior knowledge of such an inter-task mapping to achieve knowledge transfer. However, such a mapping might not be available in practice. 
% Our algorithm does not assume any prior knowledge of the mapping $f$, although Assumption~\ref{assum:similarity} assumes the existence of such an underlying mapping.
% To address this problem, we propose a new algorithm that uses the dynamics similarities to improve target task learning performance, without explicitly matching the observations.
As an alternative, we propose a novel transfer algorithm in the next section that \textit{does not} assume any prior knowledge of the mapping $f$. The proposed algorithm learns a latent representation of the observations and a dynamics model in this latent space, and then the dynamics model is transferred to speed up learning in the target task. 
% \ac{please check this last pargraph as it is very important.}
% Intuitively, $o\source\in\states\source$ and $o\target\in\states\target$ correspond to similar latent states if $f(o\target)=o\source$. 
% However, the main difficulty is that the inter-task mapping $f$ is not known to the learner, either in source task learning or in target task learning. 
% Therefore, many prior works on transferring across different observation spaces which require knowledge of the inter-task mapping are not applicable. 
% More importantly, it is not easy to directly learn the mapping by comparing and matching the source and the target observations \fh{as did in reference xxx}, since $o\source$ and $o\target$ can be drastically different.

% To address the above difficulties, we propose a new algorithm that uses the dynamics similarities to improve target task learning performance, without explicitly matching the observations.  

% \ys{learning representations for transfer}