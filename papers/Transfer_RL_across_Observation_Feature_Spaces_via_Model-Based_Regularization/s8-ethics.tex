\section*{Ethics Statement}

Transfer learning aims to apply previously learned experience to new tasks to improve learning efficiency, which is becoming more and more important nowadays for training intelligent agents in complex systems.
This paper focuses on a practical but rarely studied transfer learning scenario, where the observation feature space of an RL environment is subject to drastic changes. Driven by theoretical analysis on representation learning and its relation to latent dynamics learning, we propose a novel algorithm that transfers knowledge between tasks with totally different observation spaces, without any prior knowledge of an inter-task mapping.

This work can benefit many applications as suggested by the examples below. \\
(1) In many real-life environments where deep RL is used (e.g. navigating in a building), the underlying dynamics (e.g. the structure of the building) are usually fixed, but what features the observation space has is designed by human developers (e.g. what sensors are installed) and thus may change frequently during the development. When the agent gets equipped with better sensors, our algorithm makes it possible to reuse previously learned models when learning with the new sensors. \\
(2) An agent usually learns better with a compact observation space (e.g. a low-dimensional vector space containing its location and the goal's location) than a rich/noisy observation space (e.g. an image containing the goal). However, a compact observation is usually more difficult to construct in practice as it may require expert knowledge and human work. In this case, one can extract compact observations in a few samples and pre-train a policy with our Algorithm~\ref{alg:source}, then train the agent in the real environment with rich observations with our Algorithm~\ref{alg:target} using the learned dynamics models. Our experiment in Figure~\ref{fig:all} shows that the learning efficiency in the rich-observation environment can be significantly improved with our proposed transfer method.

% Another interesting application of our method is curriculum learning with more and more challenging observation designs. 
% Observations in real-world environments are usually rich and redundant.
% Directly learning from rich inputs may be hard, but we can first let the agent learn from an informative vector observation (that can be available in a simulator), then let the agent transfer the knowledge to richer observations. By repeating this process, the agent can finally adapt to rich observations in the real world with high performance.


\section*{Reproducibility Statement}

For theoretical results, we provide concrete proofs in Appendix~\ref{app:proofs} and Appendix~\ref{app:avi}. For empirical results, we illustrate implementation details in Appendix~\ref{app:exp}. The source code and running instructions are provided in the supplementary materials.