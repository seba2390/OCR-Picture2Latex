
\section{Additional Discussion for Representation Sufficiency}
\label{app:repre}

\paragraph{Good Representation for A Fixed Policy} We slightly abuse notation and use $\apx$ to denote the approximation operator for state value function $V:\states\to\mathbb{R}$, similar to the approximation of $Q$ as introduced in Section~\ref{sec:representation}. The following definition characterizes the sufficiency of a representation mapping in terms of evaluating a fixed policy. 
\begin{definition}[Sufficient Representation for A Fixed Policy]
\label{def:suf_policy}
A representation mapping $\phi$ is \textbf{sufficient} for a policy $\pi$ w.r.t. approximation operator $\apx$ iff $\apx V^\pi = V^\pi$. More generally, for a constant $\epsilon \geq 0$, $\rep$ is $\boldsymbol{\epsilon}$\textbf{-sufficient} for $\pi$ iff $\|\apx V^\pi - V^\pi\| \leq \epsilon$.
\end{definition}

\textbf{Remarks.} 
(1) If $o_1, o_2 \in\states$ have different values under $\pi$, a good representation should be able to distinguish them, i.e., $\phi^*(o_1)\neq \phi^*(o_2)$. \\
(2) The approximation operator $\apx$ is linear if $\apx V = \mathrm{Proj}_{\rep} (V)$ where $\mathrm{Proj}_\rep(\cdot)$ denotes the orthogonal projection to the subspace spanned by the basis functions of $\langle \phi_1, \phi_2, \cdots, \phi_d \rangle$.


\paragraph{Model Sufficiency over Policies Induces Linear Learning Sufficiency}
It can be found from Definition~\ref{def:suf_learn} that $\phi$ is sufficient as long as it represents $\qpi$ for all $\pi\in\reppolicy$. Fitting various value functions to improve representation quality is proposed by some prior works~\cite{bellemare2019geometric,dabney2020the} and shown to be effective. However, learning and fitting many policy values could be computationally expensive, and is not easy to be applied to transfer learning between tasks with different observation spaces. Can we regularize the representation with the latent dynamics instead of policy values?
Proposition~\ref{prop:lin_suf} below shows that if $\phi$ is linearly sufficient for all dynamics pairs $(\ppi, \rpi)$ induced by policies in $\reppolicy$ and the dynamics pairs associated with all actions, then $\phi$ is linearly sufficient for learning. 

For notation simplicity, assume the state space is finite.
Then, let $(\ppi, \rpi)$ be the transition matrix and reward vector induced by policy $\pi$, i.e., $\ppi[i,j] = \mathbb{E}_{a \sim \pi(o_i)} [P(o_j|o_i,a)]$, $\rpi[i] = \mathbb{E}_{a \sim \pi(o_i)} [R(o_i,a)]$. 
Similarly, let $\pact[i,j] = P(o_j|o_i,a)$, $\ract[i] = R(o_i,a)$.
We let $\rep$ denote the representation matrix, where the $i$-th row of $\rep$ refers to the feature of the $i$-the observation.

\begin{proposition}[Linear Sufficiency Induced by Policy-based Model Sufficiency]
\label{prop:lin_suf}
For representation $\rep$, if for all $\pi\in\reppolicy, a\in\actions$,  there exist $\appi,\arpi,\apa,\ara$ such that $\rep\appi=\ppi\rep$, $\rep\arpi=\rpi$, $\rep\apa=\pact\rep, \rep\ara=\ract$, i.e. $\rep$ is linearly sufficient both policy-based dynamics and policy-independent dynamics models, i.e., 
then $\rep$ is linearly sufficient for a task $\mdp$ w.r.t. approximation operator $\apx$.
\end{proposition}

Proposition~\ref{prop:lin_suf} proven in Appendix~\ref{proof:lin_suf} suggests that we can let representation fit $(\ppi,\rpi)$ for many different $\pi$'s. However, it could be computationally intractable since the policy space is large. More importantly, it is not memory-friendly to store and transfer a large number of dynamics models for all $(\appi,\arpi)$. 
In our Proposition~\ref{prop:nonlin_suf}, we show that learning sufficiency can be induced by policy-independent model sufficiency, which is much simpler as there is no need to learn and store $(\appi,\arpi)$ for many policies. As a trade-off, the policy-independent model induces non-linear sufficiency instead of linear sufficiency, requiring a more expressive approximation operator.


% \textbf{Error Bound for Imperfect Model Sufficiency}\quad
% For simplicity of illustration, our Proposition~\ref{prop:lin_suf} and Proposition~\ref{prop:nonlin_suf} consider representations that are exactly sufficient for dynamics. But these results are further relaxed to Theorem~\ref{thm:model_learn_error} in Section~\ref{sec:theory}, a general guarantee where the dynamics model learning is not perfect. 


\textbf{Latent MDP induced by Representation}\quad
We follow the analysis by~\citet{gelada2019deepmdp} and define a new MDP under the representation mapping $\phi$: $\tilde{\mdp} = \langle \tilde{\states}, \actions, \tilde{P}, \tilde{R}, \gamma \rangle$, where for all $o\in\states$, $\phi(o)\in\tilde{\states}$,
$\tilde{P}(\phi(o),a) = \apa\phi(o) $, $\tilde{R}(\phi(o),a) = \ara \phi(o)$. 
Let $\tilde{V}$ denote the value function in $\tilde{\mdp}$, and let $\tilde{\pi}$ denote a policy in $\tilde{\mdp}$.
We make the following mild assumption
\begin{assumption}
\label{assump:lips}
There exists a constant $\lipsvalue$ such that $$ |\tilde{V}_{\tilde{\pi}}(\phi(o_1)) - \tilde{V}_{\tilde{\pi}}(\phi(o_2)) | \leq \lipsvalue \|\phi(o_1)-\phi(o_2)\|, \forall \tilde{\pi}:\tilde{O}\to\actions, o_1,o_2\in\states.$$
\end{assumption}
This assumption is mild as any MDP with bounded reward has bounded value functions.