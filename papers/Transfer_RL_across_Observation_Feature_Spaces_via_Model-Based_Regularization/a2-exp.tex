\section{Experiment Details and Additional Results}
\label{app:exp}

\subsection{Experiment Setting Details}
\label{app:exp_setting}

\subsubsection{Baselines}
\label{app:exp_baseline}
\begin{itemize}
    \item \textbf{Single}: A DQN or SAC learner on the target domain without any auxiliary tasks.
    \item \textbf{Auxiliary}: On the target domain, the encoder $\phi\target$ is optimized based on the loss $L_{\text{base}}(\phi\target, \pi\target) + \lambda \Big[L_P(\phi\target;\hat{P}\target)+ L_R(\phi\target;\hat{R}\target)\Big]$. Compared with our transfer algorithms which transfer the learned dynamics from source domain to the target domain, it learns the dynamics model $(\hat{P}\target, \hat{R}\target)$ on the target domain from scratch. Here we set $\lambda$ to be the same as our transferred algorithm (values of $\lambda$ are provided in Appendix~\ref{app:exp_drl}). The purpose of this baseline is to test whether the efficiency of our proposed transfer algorithms come from the transferred latent dynamics or from the auxiliary loss (or potentially both). 
    \item \textbf{Fine-tune}: To test whether our transfer algorithms benefit from loading the learned policy head $\pi\source$, on the target domain, we load the weights of $\pi\target$ from the trained source policy head $\pi\source$ and train the DQN or SAC agent without any auxiliary loss.
    \item \textbf{Time-aligned}:
    \citet{gupta2017learning} propose to learn aligned representations for two tasks, under the assumption that the source-task agent and the target-task agent reach similar latent sates at the same time step, i.e. $\phi\target(s\target_t)=\phi\source(s\source_t)$. Note that this assumption is valid when the initial state is fixed and the transitions are all deterministic. 
    Although in our setting, the agent can not learn both tasks simultaneously, we can adapt the idea of time-based alignment and encourage the target encoder to map target observations to the source representations happening at the same time step.\\
    % We adapt the idea from ~\citep{gupta2017learning} to verify that the simple alignment approach would not work in our setting. 
    % If we assume that when executing the same policy, the source and the target domain are moving forward at the same pace since they correspond to the same latent state, then we might expect the representations of source and target observations at every time step to match, i.e. $\phi\target(s\target_t)=\phi\source(s\source_t)$ at every time step $t$ when executing the same policy. 
    In our experiments, we store $N$ source trajectories $\Big\{s^{i}_0,a^{i}_0,s^{i}_{1}, a^{i}_{1},...,\Big\}_{i=1}^{N}$ collected during source task training. Then on the target domain, we first collect $N$ trajectories following the same action as the one collected from the source domain. In other words, at time step $t$ of the $i$-th trajectory, we take action $a_{t}^i$. %We cut off the target trajectory when the length of the $i$-th target trajectory becomes greater than the pre-collected $i$-th source trajectory. 
    After the target trajectories are collected, we minimize the alignment loss $L_{\text{align}}(\phi\target)=\mathbb{E}\Big[\big(\phi\target(s\target_t)-\phi\source(s\source_t)\big)^2\Big]$ to enforce that observations from source and target domain at the same time-step have the same representations.\newline
    In our experiments, we set $N$ to be 10\% of the training trajectories. (We also experimented with larger $N$', for example using all the training trajectories, but the differences are minor.) In terms the alignment loss, we optimize the loss for 1000 epochs with batch size equal to 256, where at each epoch we sample a batch of paired source and target observations and compute the alignment loss. After pre-training the target encoder, we load the weight into $\phi\target$ and resumes the normal DQN or SAC training. \\
    Our experimental results show that, although more training steps are given to the time-aligned learner, it does not outperform the single-task learner, and sometimes fails to learn (e.g. in 3DBall). The main reason is that the time-based assumption does not hold in practice as initial states are usually randomly generated. 
    Therefore, even though the agent exactly imitates the source-task policy at every step, 
    % because we cannot control the initial state distribution and the noise in each step of the transitions, we cannot expect 
    the observations from source and target task do not necessarily match at every time-step. 
    In environments with non-deterministic transitions, the state mismatch will be a more severe issue and may lead to an unreasonable encoder.
    % We implement this as a seperate baseline and empirically verify that this approach cannot improve the transfer efficiency because of the reasons we mentioned above. 
\end{itemize}

\subsubsection{Environments}
\label{app:exp_env}

\textbf{Environment Settings in Vec-to-pixel Tasks}\quad
\begin{itemize}
    \item CartPole: The source task is the same as the ordinary CartPole environment on Gym. For the pixel-input target task, we extract the screen of the environment which is of size (400,600), and crop the pixel input to let the image be centered at the cart. The resulting observation has size (40,90) after cropping. We take the difference between two consecutive frames as the agent's observation.
    \item Acrobot: The source task is the same as the ordinary Acrobot environment on Gym. For the pixel-input target task, we first extract the screen of the environment which is of size (150,150), and then down-sample the image to (40,40). We also take the difference between two consecutive frames as the agent's observation.\newline
    \item Cheetah-Run: The source task is the Cheetah Run Task provided by DeepMind Control Suite (DMC)~\citep{tassa2018deepmind}. For the target task, we use the image of size (84,84) rendered from the environment as the agent's observation.
\end{itemize}
\textbf{Environment Settings in More-sensor Tasks}\quad
For the target task of MuJoCo environments, we add the center of the mass based inertia and velocity into the observations of the agent, concatenating them with the original observation on the source task. Consequently, in the target environments, the dimensionality of the observation space on target task become much larger than that of the source task. On Hopper, the dimensionality of the target observation is 91, whereas the the source observation space only has 11 dimensions. The dimensionalities of target tasks on HalfCheetah, Hopper and Walker are 145, 91, 145 respectively.

\textbf{Environment Settings in Broken-sensor Tasks}\quad
3DBall is an example environment provided by the ML-Agents Toolkit~\citep{juliani2018unity}. In this task, the agent (a cube) is supposed to balance a ball on its top. At every step, the agent will be rewarded if the ball is still on its top. If the ball falls off, the episode will immediately end.
The highest episodic return in this task is 100. 
There are two versions of this game, which only differ by their observation spaces.
The simpler version (named 3DBall in the toolkit) has 8 observation features corresponding to the rotation of the agent cube, and the position and velocity of the ball.
The harder version (named 3DBallHard in the toolkit) does not have access to the ball velocity, but observes a stack of 9 past frames, each of which corresponds to the rotation of the agent cube, and the position of the ball, resulting in 45 observation dimensions at every step.
We regard 3DBall as the source task and 3DBallHard as the target task in our experiments.

\subsubsection{Implementation of Base DRL Algorithms and Hyper-parameter Settings}
\label{app:exp_drl}

\textbf{Implementation of DQN}\quad
To ensure that the base learning algorithm learns the pixel-input target tasks well, we follow the existing online codebases for pre-processing, architectures and hyperparameter settings in pixel CartPole\footnote{\href{https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html}{https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html}} and pixel Acrobot
% we follow the pre-processing, architectures, and hyperparameters from another codebase 
\footnote{\href{https://github.com/eyalbd2/Deep_RL_Course }{https://github.com\/eyalbd2\/Deep\_RL\_Course}}. On source domain, the DQN network has a 2-layer encoder and a 2-layer Q head of hidden size 64, and the representation dimension is set as 16. For pixel-input, the encoder has three convolution layers followed by a linear layer. The number of channels of the convolutional layers are equal to 16, 32, 32, respectively (kernel size=5 for all three layers). We use the Adam optimizer with learning rate $0.001$ and $\beta_1, \beta_2=0.9,0.999$. The target Q network is updated every 10 iterations.
% We let the dynamics models $\apa$ and $\ara$ be linear, so that the representation be more informative in terms of representing the transition dynamics.
In CartPole, we use a replay buffer with size 10000. In the more challenging Acrobot, we use a prioritized replay buffer with size 100000.
%\ys{Ruijie: can you put more details here? including hyperparameters, as in the %rebuttal}

\textbf{Implementation of SAC}\quad
% We conduct the experiments of transfer SAC on MuJoCo-v3 using TITAN RTX GPU.
For MuJoCo environments, we follow an elegant open-sourced SAC implementation\footnote{\href{https://github.com/pranz24/pytorch-soft-actor-critic}{https://github.com/pranz24/pytorch-soft-actor-critic}}.
The number of hidden units for all neural networks is 256.
The actor has a two-layer encoder and a two-layer policy head.
The two Q networks both have three linear layers.
The activation function is ReLU and the learning rate is $3\cdot 10^{-4}$.
% The regularization weight is set as 1.
We train the dynamics model and the reward model every 50k interactive steps in the source task.
For the DMC environment Cheetah-Run, we follow the open-sourced SAC implementation with an autoencoder \footnote{\href{https://github.com/denisyarats/pytorch_sac_ae}{https://github.com/denisyarats/pytorch\_sac\_ae}}.
The pixel encoder has three convolution layers and one linear layer. The number of channels for all convolutional layers is 32 and the kernel size is 3.
For the 3DBall environment, as it can only be learned within the ML-Agents toolkit, we directly use the SAC implementation provided by the toolkit with the default hyperparameter settings. 


\textbf{Implementation of Latent Dynamics Model}\quad
Note that our goal is to learn a good representation by enforcing it predicting the latent dynamics, different from model-based RL~\citep{hafner2019dream} that aims to learn accurate models for planning. Therefore, we let the dynamics models $\hat{P}$ and $\hat{R}$ be simple linear networks, so that the representation can be more informative in terms of representing dynamics and learning values/policies. For environments with discrete action spaces, we learn $|\actions|$ linear transition networks and $|\actions|$ linear reward models. For environments with continuous action spaces, we first learn an action encoder $\psi: \actions \to \mathbb{R}^d$ with the same encoding size $d$ as the state representation. Then, we learn a linear transition network and a linear reward network with $\hat{P}(\phi(o)\circ\psi(a))$ being the predicted next representation, and $\hat{R}(\phi(o)\circ\psi(a))$ being the predicted reward, where $\circ$ denotes element-wise product. In practice, we find this implementation achieves good performance across many environments. \\
In addition, note that due to the significant difference between source observation and target observation, the initial encoding scale could be very different in source and target tasks, making it hard for them to be regularized by the same dynamics model. Therefore, we normalize the output of both encoders to be a unit vector (l2 norm is 1), which remedies the potential mismatch in their scales.

\textbf{Hyperparameter Settings for Transfer Learning}\quad
In experiments, we find that it is better to set $\lambda$ relatively large when the environment dynamics are simple and the dynamics model is of high quality. When the environment dynamics is complex, we choose to be more conservative and set $\lambda$ to be smaller. Concretely, in CartPole, $\lambda$ is set as 18; in 3DBall, $\lambda$ is set as 10; in Acrobot, $\lambda$ is set as 5; in the remaining MuJoCo environments where dynamics are more complicated, $\lambda$ is set as 1. Although we use different $\lambda$'s in different environments based on domain knowledge, we find that different values of $\lambda$'s do not have much influence on the learning performance. Figure~\ref{fig:hyper} provided in Appendix~\ref{app:exp_results} shows a test on the hyper-parameter $\lambda$, where we can see that our algorithm effectively transfers knowledge under various values of $\lambda$. \\
Regarding the representation dimension, we set it to be smaller for simpler tasks, and larger for more complex tasks. In 3DBall, we set the encoding size to be 8; in CartPole, we set the encoding size as 16; in Acrobot, we set the encoding size as 32; in Cheetah-Run, we set the encoding size as 50; in MuJoCo tasks, we set the encoding size as 256. Again, we find that the feature size does not influence the performance too much. But based on the theoretical insights of learning minimal sufficient representation~\cite{achille2018emergence}, we believe that it is generally better to have a lower-dimensional representation while making sure it is sufficient for learning.

\newpage 
\subsection{Additional Experimental Results}
\label{app:exp_results}

\textbf{Ablation Study: Transferring Different Components}\\
Figure~\ref{fig:mujoco_ablation} shows the ablation study of our method in continuous control tasks. We compare our method with the following variants:\\
(1) learning auxiliary tasks without transfer, \\
(2) only transferring transition models $\hat{P}$ and \\
(3) only transferring reward models $\hat{R}$.

Compared with the single-task learning baseline (the blue curves), we find that all the variants of our method can make some improvements, which suggests that learning dynamics models as auxiliary tasks, transferring $\hat{P}$ and $\hat{R}$ are all effective designs for accelerating the target task learning. Finally, our method (the red curves) that combines the above components achieves the best performance, justifying the effectiveness of our transfer algorithm.


\begin{figure}[!htbp]
\centering
 \begin{subfigure}[t]{0.48\columnwidth}
  \centering
  \input{figs/halfcheetah_ablation}
 \end{subfigure}
 \hfill
 \hspace{-1em}
 \begin{subfigure}[t]{0.48\columnwidth}
  \centering
  \input{figs/hopper_ablation} 
 \end{subfigure}

 \begin{subfigure}[t]{0.48\columnwidth}
  \centering
  \input{figs/walker2d_ablation} 
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.48\columnwidth}
  \centering
  \input{figs/ball_ablation_all} 
 \end{subfigure}
 \vspace{-2em}
 \caption{\small{Ablation study of our method on different transferred components.}}
\label{fig:mujoco_ablation}
\end{figure}

\newpage 
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \input{figs/cart_ablation}
    \vspace{-1em}
    \caption{In the Vec-to-pixel CartPole environment, sanity check verifies the effectiveness of our algorithm design. Results are averaged over 20 random seeds.}
    \label{fig:cart_ablation}
    \vspace{-1em}
\end{wrapfigure}
\textbf{Sanity Check: Effectiveness of the Proposed Transfer Method}\\
We conduct another ablation study to evaluate each component of our algorithm in the CartPole environment as shown in Figure~\ref{fig:cart_ablation}. We find that when transferring the dynamics models with only a linear value head (the \textcolor{linearcolor}{\textbf{green}} curve), the agent fails to learn a good policy as we analyzed in Section~\ref{sec:theory}. If the dynamics models $(\hat{P},\hat{R})$ are randomly generated instead of being transferred from the source task (the \textcolor{randomcolor}{\textbf{orange}} curve), the agent does not learn, either. More importantly, if we learn dynamics models as auxiliary tasks in the target task without transferring them from the source (the \textcolor{auxcolor}{\textbf{purple}} curve), the agent learns a little better than a vanilla agent, but is worse than our proposed transfer algorithm. 
These empirical results have verified our theoretical insights and shown the effectiveness of our algorithm design. 


\begin{wrapfigure}{r}{0.5\textwidth}
% \vspace{-2em}
    \centering
    \input{figs/coeff_compare}
    \vspace{-1em}
    \caption{In the Vec-to-pixel CartPole environment, under different selections of hyperparameter $\lambda$, the algorithm works better than learning from scratch (when $\lambda=0$). Results are averaged over 20 random seeds.}
    \label{fig:hyper}
    \vspace{-1em}
\end{wrapfigure}
\textbf{Hyper-parameter Test}\\
Figure~\ref{fig:hyper} further visualizes how the hyperparameter $\lambda$ (regularization weight) influences the transfer performance in the Vec-to-pixel CartPole environment. It can be found that the agent generally benefits from a larger $\lambda$, which suggests that the model-based regularization has a positive impact on the learning performance. For a wide range of $\lambda$'s, the agent always outperforms the learner without transfer (the learner with $\lambda=0$). Therefore, our algorithm is not sensitive to the hyperparameter $\lambda$, and a larger $\lambda$ is preferred to get better performance.
In Appendix~\ref{app:exp_drl}, we have provided the $\lambda$ selections for all experiments.
