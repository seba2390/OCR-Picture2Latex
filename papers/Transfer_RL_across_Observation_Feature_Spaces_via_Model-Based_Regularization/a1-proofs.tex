%!TEX root = 0_iclr2022_main.tex

\section{Technical Proofs}
\label{app:proofs}

\subsection{Proof of Lemma~\ref{lem:bound_pi}}
\label{proof:bound_pi}

\begin{proof}[Proof of Lemma~\ref{lem:bound_pi}]

We first show that policy iteration with approximation operator $\apx$ starting from a policy $\pi_0\in\reppolicy$ generates a sequence of policies that are in $\reppolicy$. That is, for all $\pi_k$, and any $o_1,o_2\in\states$, $\pi_k(o_1)=\pi_k(o_2)$ if $\phi(o_1)=\phi(o_2)$.
We prove this claim by induction.

\textit{Base case:} when $k=0$, $\pi_0\in\reppolicy$.

\textit{Inductive step:} assume $\pi_k \in \reppolicy$ for $k\geq 0$, then for iteration $k+1$, we know that
% \setlength\abovedisplayskip{2pt}
% \setlength\belowdisplayskip{2pt}
\begin{equation}
    \pi_{k+1} (o) := \mathrm{argmax}_a Q_k(o,a)
\end{equation}
where $Q_k = \apx Q_{\pi_k}$. 

Based on the definition of $\apx$, $Q_k(o,a)=f(\phi(o);\theta_a)$ for some $\theta_a$. Hence, if $o_1$ and $o_2$ have the same representation, $Q_k(o_1,\cdot)$ and $Q_k(o_2,\cdot)$ are equal. As a result, $\pi_{k+1}(o_1)$ and $\pi_{k+1}(o_2)$ are equal, so $\pi_{k+1}\in\reppolicy$.

Next we prove the error bound in Lemma~\ref{lem:bound_pi}.
We start by restating the error bounds for approximate policy iteration by~\citet{BertsekasTsitsiklis96}: 
\begin{equation}
    \mathrm{limsup}_{k\to\infty} \|V^*-V^{\pi_k} \|_\infty \leq 
    \frac{2\gamma}{(1-\gamma)^2} \mathrm{sup}_k \| V_k - V^{\pi_k} \|_\infty
\end{equation}
where $\pi_k$ is the policy in the $k$-th iteration.
Then we extend the above result to the action value $Q$.

For any $\pi_k$ during the policy iteration (as proven above, $\pi_k\in\reppolicy$), if $\phi$ is $\epsilon$-sufficient for $\mdp$ as defined in Definition~\ref{def:suf_learn} with $\ell_\infty$ norm, then we have $\| Q_k - Q_{\pi_k} \|_\infty \leq \epsilon$. That is, $\forall o\in\states, a\in\actions, | Q_k(o,a) - Q_{\pi_k}(o,a) | \leq \epsilon $. Therefore, $\forall o\in\states$,
\begin{equation}
    |V_k(o) - V_{\pi_k}(o)| = | \sum_{a\in\actions} \pi(a|o) (Q_k(o,a) - Q_{\pi_k}(o,a)) | \leq \epsilon \label{eq:vk_bound}
\end{equation}

On the other hand, we can derive
\begin{align}
    \| Q^* - Q_{\pi_k} \|_\infty &= \max_{o,a} |Q^*(o,a) - Q_{\pi_k}(o,a) | \\
    &= \max_{o,a} |R(o,a) + \gamma \sum_{o^\prime\in\states}P(o^\prime|o,a)V^*(o^\prime) - R(o,a) - \gamma \sum_{o^\prime\in\states}P(o^\prime|o,a)V_{\pi_k}(o^\prime) | \\
    &= \gamma \|V^* - V_{\pi_k} \|_\infty \label{eq:opt_bound}
\end{align}

Combining Equation~(\ref{eq:vk_bound}) and~(\ref{eq:opt_bound}), we obtain
\begin{equation}
    \mathrm{limsup}_{k\to\infty} \|Q^* - Q_{\pi_k} \|_\infty \leq \frac{2\gamma^2}{(1-\gamma)^2} \epsilon.
\end{equation}

\end{proof}

\subsection{Proof of Proposition~\ref{prop:nonlin_suf}}
\label{proof:nonlin_suf}
\begin{proof}[Proof of Proposition~\ref{prop:nonlin_suf}]

Given that $P$ is deterministic for $o\in\states, a\in\actions$, we slightly abuse notation and let $o^\prime = P(o,a) = \pact(o)$ if $P(o^\prime|o,a)=1$.
If $\phi$ is sufficient for the dynamics models, i.e. $\forall o\in\states, a\in\actions$, $\apa \phi(o) = \phi(\pact(o))$, $\ara \phi(o) = \ract(o)$.
Then, we can define a new MDP $\tilde{\mdp} = \langle \tilde{\states}, \actions, \tilde{P}, \tilde{R}, \gamma \rangle$, where for all $o\in\states$, $\phi(o)\in\tilde{\states}$, and
$\tilde{P}(\phi(o),a) = \apa\phi(o) = \phi(P(o,a))$, $\tilde{R}(\phi(o),a) = \ara \phi(o) = R(o,a)$. 

Any policy $\pi\in\reppolicy$, based on the definition of $\reppolicy$, can be written as $\tilde{\pi}\circ\phi$, where $\tilde{\pi}$ is a deterministic policy in $\tilde{\mdp}$.
Next, we show that for all $o\in\states, a\in\actions$, $\qpi(o)=\tilde{Q}_{\tilde{\pi}}(\phi(o))$.

By definition of the Q value, we know 
\begin{align}
    \qpi(o,a) &= \mathbb{E}_{\pi,P}[\sum_{t=0}^\infty \gamma^t R(o_t, a_t)|o_0=o, a_0=a] \label{eq:q_original}\\
    \tilde{Q}_{\tilde{\pi}}(\phi(o)) 
    &= \mathbb{E}_{\tilde{\pi},\tilde{P}}[\sum_{t=0}^\infty \gamma^t \tilde{R}(\tilde{o}_t, \tilde{a}_t)| \tilde{o}_0=\phi(o), \tilde{a}_0=a] \label{eq:q_induced}
\end{align}

We claim that in the above equations, $\tilde{o_t} = \phi(o_t)$, $\tilde{a_t}=a_t$, for all $t\geq 0$. We prove the claim by induction.

When $t=0$, the claim holds as $\tilde{o_0}=\phi(o_0)=\phi(o)$, $\tilde{a_0}=a_0=a$.

Then, with inductive hypothesis that $\tilde{o_t} = \phi(o_t)$, $\tilde{a_t}=a_t$, we show the claim holds for $t+1$:

Action:
$a_{t+1} = \pi(o_{t+1}) = \tilde{\pi}(\phi(o_{t+1})) = \tilde{a}_{t+1}$.

State: $\tilde{o}_{t+1} = \tilde{P}(\tilde{o_t},a_t) = \tilde{P}(\phi(o_t),a_t) = \phi(P(o_t,a_t)) = \phi(o_{t+1})$. 

Hence, we have shown $\tilde{o}_t = \phi(o_t)$, $\tilde{a}_t=a_t$, for all $t\geq 0$, then the reward in the $t$-th step of Equation~(\ref{eq:q_original}) and (\ref{eq:q_induced}) are the same, as $\tilde{R}(\tilde{o_t},\tilde{a_t}) = \tilde{R}(\phi(o_t),a_t) = R(o_t,a_t)$. Therefore, $\qpi(o)=\tilde{Q}_{\tilde{\pi}}(\phi(o))$.

Therefore, for any $\pi\in\reppolicy$, its action value can be represented by $\tilde{Q}_{\tilde{\pi}}\circ\phi$. Therefore, $\phi$ is sufficient for learning in $\mdp$.


Next, we show that \textbf{$\phi$ is not necessarily linearly sufficient} for learning the task. 

Consider an arbitrary policy $\pi\in\reppolicy$. Without loss of generality, suppose $\rpi$ is linearly represented by $\phi$, i.e. $\ract(o,a) = \phi(o)^\top \ara$, then we have
\begin{align*}
    \rpi(o) &= \sum_{a\in\actions} \pi(a|\phi(o)) \ract(o,a) \\
    &= \sum_{a\in\actions} \pi(a|\phi(o)) \phi(o)^\top \ara \\
    &= \langle \pi(\phi(o)), \hat{R}^\top \phi(o) \rangle \\
    &= \langle \hat{R} \pi(\phi(o)), \phi(o) \rangle
\end{align*}

where $\hat{R}:=[\hat{R}_{a_1};\hat{R}_{a_1};\cdots;\hat{R}_{a_{|\actions|}} ]$. We can find that unless $\pi$ always takes the same action for all input states, $\phi$ is not guaranteed to linearly encode $\rpi$.

% A similar result can be obtained for the transition model, but for simplicity, consider an MDP with horizon 1. So the values are just the immediate rewards, and $\phi$ is linearly sufficient if it linearly represents $\qpi$.
% Hence, $\forall o \in\states$ and $\pi: \repspace \to \Delta(\actions)$, $\rpi=h_1(\phi(o); \hat{R},\pi)$. 
% Or equivalently, $\rpi=diag(\rep\hat{R}^\top\pi)$.

Similarly, for $\ppi$, suppose $\pact(\cdot|o,a)=\phi(o)^\top \apa$ we have
\begin{align*}
    \phi(\ppi(o)) &= \sum_{a\in\actions} \pi(a|\phi(o)) \pact(\cdot|o,a) \\
    &= \sum_{a\in\actions} \pi(a|\phi(o)) \phi(o)^\top \apa \\
    &= \hat{P}(\pi(\phi(o)), \phi(o), I) 
\end{align*}
where $\hat{P}:=[\hat{P}_{a_1};\hat{P}_{a_1};\cdots;\hat{P}_{a_{|\actions|}} ]$ is an $|\actions|\times d \times d$ tensor, and $\hat{P}(\cdot,\cdot,\cdot)$ denotes the multi-linear operation. Hence, if $\pi$ takes different actions in different states, $\phi$ may not linearly encode the transition, either.

Therefore, $\phi$ is not guaranteed to linearly encode $\rpi$ and $\ppi$, and thus is not guaranteed to linearly encode $\vpi$ and $\qpi$. 

\end{proof}


\subsection{Proof of Proposition~\ref{prop:lin_suf}}
\label{proof:lin_suf}
\begin{proof}[Proof of Proposition~\ref{prop:lin_suf}]

We first show that for any $\pi\in\Pi$, if $\phi$ is linearly sufficient for $(\ppi,\rpi)$, then there exists a vector $\omega \in \mathbb{R}^k$ such that $\vpi = \avpi = \rep\omega$.

Since $\rep$ is linearly sufficient for $\ppi$ and $\rpi$, we have $\rep \appi = \ppi \rep$ and $\rep \arpi = \rpi$ for some $\appi$ and $\arpi$. Let $\omega = (I-\gamma \appi)^{\dagger} \arpi$, then the Bellman error of $\avpi=\rep\omega$ can be computed as

\begin{align*}
    \rpi + \gamma\ppi\avpi - \avpi &= \rpi + \gamma\ppi\rep\omega - \rep\omega \\
    &= \rep \arpi + \gamma \rep \appi \omega - \rep\omega \\
    &= \rep (\arpi - (I-\gamma\appi) (I-\gamma \appi)^{\dagger} \arpi ) \\
    &= \rep (\arpi - \arpi) \\
    &= 0
\end{align*}

Therefore, $\avpi$ is a fixed point of the Bellman operator $\bellman^\pi$, and thus equal to $\vpi$.

%  we show that if $\rep$ is further linearly sufficient for $\ppi$ and $\rpi$, then $\qpi(\cdot,a)$ for any $a\in\actions$ can be represented by a linear approximation operator on $\rep$, i.e. $\mathrm{Proj}\rep(Q(\cdot,a))=Q(\cdot,a)$.

Next, as we know that $\qpi(\cdot,a) = \ract + \gamma \langle \pact, \vpi \rangle$, and $\rep\apa=\pact\rep, \rep\ara=\ract$, we can obtain
\begin{align*}
    \qpi(\cdot,a) &= \ract + \gamma \langle \pact, \vpi \rangle \\
    &= \rep\ara + \gamma \pact \rep \omega \\
    &= \rep\ara + \gamma \rep\apa \omega \\
    &= \rep (\ara + \gamma \apa \omega)
\end{align*}

Therefore, for any $\pi\in\reppolicy$, $\qpi$ can be linearly represented by $\rep$, and thus $\rep$ is linearly sufficient for learning by definition.

\end{proof}


\subsection{Proof of Theorem~\ref{thm:model_learn_error}}
\label{proof:model_learn_error}

% To derive Theorem~\ref{thm:model_learn_error}, we follow the analysis by~\citet{gelada2019deepmdp}, and define a new MDP induced by the representation mapping $\phi$: $\tilde{\mdp} = \langle \tilde{\states}, \actions, \tilde{P}, \tilde{R}, \gamma \rangle$, where for all $o\in\states$, $\phi(o)\in\tilde{\states}$,
% $\tilde{P}(\phi(o),a) = \apa\phi(o) $, $\tilde{R}(\phi(o),a) = \ara \phi(o)$. 

% We make the following mild assumption
% \begin{assumption}
% \label{assump:lips}
% There exists a constant $\lipsvalue$ such that $$ |\tilde{V}_{\tilde{\pi}}(\phi(o_1)) - \tilde{V}_{\tilde{\pi}}(\phi(o_2)) | \leq \lipsvalue \|\phi(o_1)-\phi(o_2)\|, \forall \tilde{\pi}:\tilde{O}\to\actions, o_1,o_2\in\states.$$
% \end{assumption}
% This assumption is mild as any MDP with bounded reward has bounded value functions.

% Then we proceed to prove Theorem~\ref{thm:model_learn_error}.

% \begin{lemma}[Sufficiency with Errors in Model Fitting]
% \label{lem:model_error}
% For an MDP $\mdp$, if encoder $\phi$ satisfies
% $\max_{o\in\states,a\in\actions}|R(o,a) - \phi(o) \hat{R}_a | \leq \epsilon_R$ and
% $\max_{o\in\states,a\in\actions} \| \mathbb{E}_{o^\prime\sim P(\cdot|o,a)} \phi(o^\prime) - \hat{P}_a (\phi(o)) \|_2 \leq \epsilon_P$, 
% then $\phi$ is $(1-\gamma)^{-1}(\epsilon_R+\gamma\kappa_V\epsilon_P)$-sufficient for learning $\mdp$, where $\kappa_V$ is the Lipschitz constant for value function as defined in Definition XX, Appendix XX.
% \end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:model_learn_error}]

Lemma 2 in~\citet{gelada2019deepmdp} is based on one policy in the induced MDP and bounded model errors. We can replace the Wasserstein distance $\mathcal{W}(\phi P(\cdot|o,a), \tilde{P}(\cdot|\phi(o),a))$ by the Euclidean distance $\| \phi P(o,a), \tilde{P}(\phi(o),a) \|$ as we focus on deterministic transitions.

For any policy $\pi\in\reppolicy$ that can be written as $\tilde{\pi}\circ\phi$, we have
\begin{equation}
    | Q_{\pi}(o,a) - \tilde{Q}_{\tilde{\pi}}(\phi(o),a) | \leq \frac{\epsilon_R + \gamma \lipsvalue \epsilon_P}{1-\gamma}
\end{equation}

Therefore, $\phi$ is $(1-\gamma)^{-1}(\epsilon_R+\gamma\lipsvalue\epsilon_P)$-sufficient for learning $\mdp$. 

Combined with Lemma~\ref{lem:bound_pi}, we can obtain the bound in Theorem~\ref{thm:model_learn_error}.

\end{proof}



\subsection{Proof of Theorem~\ref{thm:transfer_suf}}
\label{proof:transfer_suf}

\begin{proof}[Proof of Theorem~\ref{thm:transfer_suf}]
First of all, if there exists $\phi\target$ satisfying
$\apa(\phi(o_i))=\pact\target[i]\rep\target$, $\ara(\phi(o_i)) = \ract\target[i]$, $\forall o_i\in\states\target$, then it is sufficient for the dynamics of the target task, and thus sufficient for learning the target task as stated in Proposition~\ref{prop:nonlin_suf}. Therefore, our focus is to show the existence of such a representation.

As $\phi\source$ is sufficient for $\pact\source$ and $\ract\source$ for all $a\in\actions$, we have 
\begin{align}
    &\apa(\phi\source(o\source)) = P\source(o\source,a)\rep\source = \phi\source(P\source(o\source,a))\\
    &\ara(\phi\source(o\source)) = R\source(o\source,a)
\end{align}
where we let $P\source(o\source,a)$ denote the next state of $(o\source,a)$, given that $P$ is deterministic.

Based on Assumption~\ref{assum:similarity}, we know that there exists a function $f$ such that $\forall o\target\in\states\target$, $f(o\target)\in\states\source$, and $f(P\target(o\target,a))=P\source(f(o\target),a)$, $R\target(o\target,a)=R\source(f(o\target),a)$.
Hence, we can obtain 
\begin{align}
    &\apa(\phi\source(f(o\target))) = \phi\source(P\source(f(o\target),a)) = \phi\source(f(P\target(o\target,a)))\\
    &\ara(\phi\source(f(o\target))) = R\source(f(o\target),a) = R\target(o\target,a)
\end{align}
Let $\hat{\phi}\target := \phi\source\circ f$, then we get
\begin{align}
    &\apa(\hat{\phi}\target(o\target)) = \hat{\phi}\target (P\target(o\target,a))  \\
    &\ara(\hat{\phi}\target(o\target)) = R\target(o\target,a)
\end{align}

Therefore, $\hat{\phi}\target$ is a feasible solution satisfying model sufficiency in the target task, and thus is sufficient for learning.

Theorem~\ref{thm:transfer_suf} holds since we have shown (1) all feasible solutions to $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$ are sufficient for learning in $\mdp\target$, and (2) there exists at least one feasible solution to $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$.

% First of all, if there exists a representation $\rep\target$ satisfying $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$, then it is sufficient for the dynamics of the target task, and thus sufficient for learning the target task as stated in Proposition~\ref{prop:nonlin_suf}. Therefore, our focus is to show the existence of such a representation.

% Denote the cardinalities the source observation space and the target observation space as $n\source$ and $n\target$, respectively.
% Based on Assumption~\ref{assum:similarity}, there exists a matrix $F\in\mathbb{R}^{n\source\times n\target}$ where 
% \begin{equation*}
%     F[i,j]= \begin{cases} 
%       1 & \text{if } f(o_i\target) = o_j\source \\
%       0 & \text{otherwise}. 
%   \end{cases}
% \end{equation*}

% Then, we have
% \begin{align}
%     &(F\pact\source)[i,j] = \sum_{o\in\states\source} \mathbbm{1}[f(o_i\target)=o] \mathbbm{1}[\pact\source(o)=o_j\source]
%     = \mathbbm{1}[\pact\source(f(o_i\target)) = o_j\source ] \\
%     &(\pact\target F)[i,j] = \sum_{o\in\states\source} \mathbbm{1}[\pact\target(o_i\target)=o] \mathbbm{1}[f(o)=o_j\source]
%     = \mathbbm{1}[f(\pact\target(o_i\target)) = o_j\source ]
% \end{align}

% Since $\pact\source(f(o_i\target))=f(\pact\target(o_i\target))$ according to Assumption~\ref{assum:similarity}, we can conclude that $F\pact\source = \pact\target F$.

% Similarly, we conclude that $F\ract\source=\ract\target$ from Assumption~\ref{assum:similarity}.

% Given that $\rep\source$ is sufficient for $\pact\source, \ract\source$ with operators $\apa, \ara$, we know $\rep\source\apa = \pact\source\rep\source$ and $\rep\source\ara=\ract\source$. Multiply the two equations by $F$ from the left, we obtain
% \begin{align}
%     F \rep\source \apa &= F\pact\source\rep\source = \pact\target F\rep\source \\
%     F \rep\source \ara &= F\ract\source =\ract\target
% \end{align}

% Therefore, $\rep\target = F\rep\source$ is a feasible solution for $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$, and thus is sufficient for learning.

% Theorem~\ref{thm:transfer_suf} holds since we have shown (1) all feasible solutions to $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$ are sufficient for learning in $\mdp\target$, and (2) there exists at least one feasible solution to $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$.

\end{proof}