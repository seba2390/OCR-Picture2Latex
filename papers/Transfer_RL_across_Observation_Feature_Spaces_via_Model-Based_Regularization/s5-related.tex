\vspace{-0.5em}
\section{Related Work}
\label{sec:related}
\vspace{-0.5em}

\textbf{Transfer RL across Observation Feature Spaces.}
Transferring knowledge between tasks with different observation spaces has been studied for years. 
Many existing approaches\citep{taylor2007transfer,mann2013directed,brys2015policy} require an explicit mapping between the source and target observation spaces, which may be hard to obtain in practice.
\citet{raiman2019neural} introduce network surgery that deals with the change in the input features by determining which components of a neural network model should be transferred and which require retraining. However, it requires knowledge of the input feature maps, and is not designed for drastic changes, e.g. vector to pixel.
\citet{sun2020temple} propose a provably sample-efficient transfer learning algorithm that works for different observation spaces without knowing any inter-task mapping, but the algorithm is mainly designed for tabular RL and model-based RL which uses the model to plan for a policy, different from our setting.
\citet{gupta2017learning} achieve transfer learning between two different tasks by learning an invariant feature space, with a key time-based alignment assumption. We empirically compared this method with our proposed transfer algorithm in Section~\ref{sec:exp}.
% which does not hold when the environments are stochastic, or when the agent only has access to one environment.
Our work is also related to state abstraction in block MDPs, as studied by \citet{zhang2020invariant}. But the problem studied in~\citet{zhang2020invariant} is a multi-task setting where the agent aims to learn generalizable abstract states from a series of tasks.
Another related topic is domain adaptation in RL~\citep{higgins2017darla,eysenbach2020off,zhang2020learning}, where the target observation space (e.g. real world) is different from the source observation (e.g. simulator). However, domain adaptation does not assume drastic observation changes (e.g. changed dimension). Moreover, the aim of domain adaptation is usually zero-shot generalization to new observations, thus prior knowledge or a few samples of the target domain is often needed~\citep{eysenbach2020off}.
% However, domain adaptation is different from our setting as the source and target observations still have the same shapes, although having different demonstrations, e.g. the noise in background. 
% The aim of domain adaptation is to train a policy in the source domain that can generalize to the target domain, thus prior knowledge or a few samples of the target domain is often needed~\citep{eysenbach2020off}. Our work, in contrast, transfers knowledge between observations with different shapes, and aims for few-shot learning in the target task.


\textbf{Representation Learning in RL.}
In environments with rich observations, representation learning is crucial for the efficiency of RL methods. Learning unsupervised auxiliary tasks~\citep{jaderberg2016reinforcement} is shown to be effective for learning a good representation. 
The relationship between learning policy-dependent auxiliary tasks and learning good representations has been studied in some prior works~\citep{bellemare2019geometric,dabney2020the,lyle2021effect}, while our focus is to learn policy-independent auxiliary tasks to facilitate transfer learning. 
Using latent prediction models to regularize representation has been shown to be effective for various types of rich observations~\citep{guo2020bootstrap,lee2019stochastic}.  
\citet{gelada2019deepmdp} theoretically justify that learning latent dynamics model guarantees the quality of the learned representation, while we further characterize the relationship between representation and learning performance, and we utilize dynamics models to improve transfer learning.
\citet{zhang2020learning} use a bisimulation metric to learn latent representations that are invariant to task-irrelevant details in observation. As pointed out by~\citet{achille2018emergence}, invariant and sufficient representation is indeed minimal sufficient, so it is an interesting future direction to combine our method with bisimulation metric to learn minimal sufficient representations.
There is also a line of work using contrastive learning to train an encoder for pixel observations~\citep{srinivas2020curl,yarats2021reinforcement,stooke2021decoupling}, which usually pre-train an encoder based on image samples using self-supervised learning. However, environment dynamics are usually not considered during pre-training.
Our algorithm can be combined with these contrastive learning approaches to further improve learning performance in the target task.

