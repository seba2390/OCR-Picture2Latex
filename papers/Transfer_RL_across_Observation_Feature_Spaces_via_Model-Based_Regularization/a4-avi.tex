\section{Representation Learning for Approximate Value Iteration}
\label{app:avi}

Now we illustrate how our transfer algorithm and the proposed model-based regularization work for approximate value iteration.
We focus on the case where the reward function $R(o,a)\geq 0$ for all $o\in\states$ and $a\in\actions$.

\textbf{Preliminaries}\quad
The bases of value iteration is the Bellman optimality operator $\bellman^*$. For the value function, we have
\begin{equation}
    \bellman^* V(o) = \max_{a\in\actions} [ R(o,a) + \gamma \sum_{o\in\states} P(o^\prime|o,a) V(o^\prime) ]
\end{equation}
For the Q function, we have
\begin{equation}
    \bellman^* Q(o,a) =  R(o,a) + \gamma \sum_{o\in\states} P(o^\prime|o,a) \max_{a^\prime\in\actions} Q(o^\prime,a^\prime),
\end{equation}
where we slightly abuse notation and use $\bellman^*$ for both the value function and the Q function when there is no ambiguity. 

Starting from some initial $Q_0$ (or $V_0$) and iteratively applying $\bellman^*$, i.e., $Q_{k+1}=\bellman^* Q_k$, $Q_k$ (or $V_k$) can finally converge to $Q^*$ or $V^*$ when $k\to\infty$, which is known as value iteration.
When an approximation operator $\mathcal{H}$ is used, the process is called approximate value iteration (AVI): $Q_{k+1}=\apx \bellman^* Q_k$. A prior work has shown the following asymptotic result:
\begin{lemma}[Approximate Value Iteration for $Q$]
\label{lem:avi}
For a reinforcement learning algorithm based on value iteration with approximation operator $\mathcal{H}$, if $\|\mathcal{H}\bellman^* Q_k - \bellman^* Q_k\|_\infty\leq \epsilon$, for all $Q_k$ along the value iteration path, then we have
\begin{equation}
    \limsup_{k\to\infty} \|V^*-V_{\pi_k}\| \leq \frac{2\epsilon}{(1-\gamma)^2}.
\end{equation}
\end{lemma}


\textbf{Value Iteration Learning with Given Representation}\quad
Given a representation mapping $\phi$, we aim to learn an approximation function $h:\phi(\states)\to\mathbb{R}^{|A|}$ such that $\hat{Q}(o,\cdot)=\apf(\phi(o))\approx Q(o,\cdot)$. For notation simplicity, we further use $\apf(\phi(o),a)$ to denote the approximated Q value $\hat{Q}(o,a)$ for $a\in\actions$.
% More specifically, let $\hat{Q}_a(o)=\apf_a(\phi(o))$, where $\apf_a:\phi(\states)\to\mathbb{R}$ maps the encoding of an observation to its Q value for action $a\in\actions$. 
We start from an initial $\apf_0$ that has a uniform value $c$ for all inputs, where $c>0$ can be randomly selected. The initial approximation for Q value is $\hat{Q}_0=\apf_0\circ\phi$.
Then, at iteration $k>0$, we solve $\hat{Q}_k=\apx\bellman^*\hat{Q}_{k-1}=\apf_{k}\circ\phi$, where $\apf_k:=\mathrm{argmin}_\apf \| \apf\circ\phi - \bellman^*\hat{Q}_{k-1} \|_\infty$. We use a neural network (universal function approximator) to parameterize $h$, so the approximation error $\| \apf\circ\phi - \bellman^*\hat{Q}_{k-1} \|_\infty$ depends on the representation quality of $\phi$.

Therefore, a representation mapping $\phi$ is $\epsilon$-sufficient for learning with value iteration if $\|\apx\bellman^* Q_k - \bellman^* Q_k\|_\infty\leq \epsilon$. 
Next, we identify the relationship between policy-independent model sufficiency and the learning sufficiency with value iteration methods.

\textbf{Guaranteed Learning with Model-regularized Representation}\quad
We first make the following assumption for the learned approximation function $h$.
\begin{assumption}[Lipschitz Value Approximation]
\label{assump:lips_appro}
There exists a constant $\lipsapx$, such that $\forall k\geq 0, o_1,o_2\in\states, a\in\actions$, 
\begin{equation}
    | \apf_k(\phi(o_1),a) - \apf_k(\phi(o_2),a) | \leq \lipsapx \| \phi(o_1) - \phi(o_2) \|,
\end{equation}
where $\apf_k$ is the approximation function in the $k$-th iteration.
% For the learned dynamics models $(\apa, \ara)_{a\in\actions}$, there exists constants $\lipsp\leq \frac{1}{\gamma}$ and $\lipsr$ such that $\forall o_1,o_2\in\states, a\in\actions$,
% \begin{align}
%     &\| \apa(\phi(o_1)) - \apa(\phi(o_2)) \| \leq \lipsp \| \phi(o_1) - \phi(o_2) \|\\
%     & | \ara(\phi(o_1)) - \ara(\phi(o_2)) | \leq \lipsr \| \phi(o_1) - \phi(o_2) \|.
% \end{align}
\end{assumption}

Then, the following Theorem holds, which justifies that learning with model-regularized representation helps with value iteration learning.
\begin{theorem}
\label{thm:error_avi}
For an MDP $\mdp$, if encoder $\phi$ satisfies
$\max_{o\in\states,a\in\actions}|R(o,a) - \hat{R}_a(\phi(o)) | \leq \epsilon_R$ and
$\max_{o\in\states,a\in\actions} \| \mathbb{E}_{o^\prime\sim P(\cdot|o,a)} \phi(o^\prime) - \hat{P}_a (\phi(o)) \|_2 \leq \epsilon_P$ for dynamics models $(\apa,\ara)_{a\in\actions}$, 
then the approximated value iteration with approximation operator $\apx$ under Assumption~\ref{assump:lips_appro} satisfies 
% \setlength\abovedisplayskip{-2pt}
% \setlength\belowdisplayskip{2pt}
\begin{equation}
    \limsup_{k\to\infty} \|V^* - V^{\pi_k} \|_{\infty} \leq \frac{2}{(1-\gamma)^2}(\epsilon_R+\gamma\epsilon_P\lipsapx).
\end{equation}
\end{theorem}


\begin{proof}[Proof of Theorem~\ref{thm:error_avi}]
Let $\apf_k$ be the approximation function in the $k$-th iteration. That is, the approximated Q function in the $k$-th iteration is $\hat{Q}_k=\apf_k\circ\phi$. As the rewards of all state-action pairs are non-negative, we have $\apf_k(\phi(o),a)\geq 0$.

Define a function $\hat{\apf}_{k+1}$ as
\begin{equation}
    \hat{\apf}_{k+1}(\phi(o),a) = \ara(\phi(o)) + \gamma \max_{a^\prime\in\actions} \apf_k(\apa(\phi(o)), a^\prime)
\end{equation}

Given that 
\begin{align}
    \bellman^*\hat{Q}_k(o,a) &= R(o,a) + \gamma \mathbb{E}_{o^\prime\sim P(\cdot|o,a)}[\max_{a^\prime\in\actions} \hat{Q}_k(o^\prime,a^\prime)] \\
    &= R(o,a) + \gamma \mathbb{E}_{o^\prime\sim P(\cdot|o,a)}[\max_{a^\prime\in\actions} \apf_k(\phi(o^\prime), a^\prime)],
\end{align}
we have that for any $o\in\states$, $a\in\actions$,
\setlength\abovedisplayskip{3pt}
\setlength\belowdisplayskip{3pt}
\begin{align}
    &\left|\hat{\apf}_{k+1}(\phi(o),a) - \bellman^*\hat{Q}_k(o,a)\right| \\
    =& \left| \ara(\phi(o)) + \gamma \max_{a^\prime\in\actions} \apf_k(\apa(\phi(o)), a^\prime) - (R(o,a) + \gamma \mathbb{E}_{o^\prime\sim P(\cdot|o,a)}[\max_{a^\prime\in\actions} \apf_k(\phi(o^\prime), a^\prime)]) \right| \\
    \leq& \left| \ara(\phi(o)) - R(o,a)\right| + \gamma \left| \max_{a^\prime\in\actions} \apf_k(\apa(\phi(o)), a^\prime) - \mathbb{E}_{o^\prime\sim P(\cdot|o,a)}[\max_{a^\prime\in\actions} \apf_k(\phi(o^\prime), a^\prime)]  \right|\\
    \leq& \epsilon_R + \gamma \mathbb{E}_{o^\prime\sim P(\cdot|o,a)}[ \left| \max_{a^\prime\in\actions} \apf_k(\apa(\phi(o)), a^\prime) - \max_{a^\prime\in\actions} \apf_k(\phi(o^\prime), a^\prime) \right| ]\\
    \leq& \epsilon_R + \gamma \mathbb{E}_{o^\prime\sim P(\cdot|o,a)} \max_{a^\prime\in\actions} \left| \apf_k(\apa(\phi(o)), a^\prime) - \apf_k(\phi(o^\prime), a^\prime) \right| \label{eq:maxa}\\
    \leq& \epsilon_R + \gamma \mathbb{E}_{o^\prime\sim P(\cdot|o,a)} \max_{a^\prime\in\actions} \lipsapx \left\| \apa(\phi(o) - \phi(o^\prime) \right\| \label{eq:lips}\\
    \leq& \epsilon_R + \gamma \epsilon_P \lipsapx,
\end{align}
where (\ref{eq:maxa}) is due to the non-negativity of $h_k$, and (\ref{eq:lips}) is due to Assumption~\ref{assump:lips_appro}.

Now we have shown that the constructed $\hat{h}_{k+1}$ satisfies 
\begin{equation}
    \|\hat{\apf}_{k+1}\circ \phi - \bellman^* \hat{Q}_k \|_\infty \leq \epsilon_R + \gamma \epsilon_P \lipsapx.
\end{equation}

According to the definition of $\apx$, we obtain
\begin{equation}
\label{eq:apx_error}
    \|\apf_{k+1}\circ \phi - \bellman^* \hat{Q}_k \|_\infty \leq \|\hat{\apf}_{k+1}\circ \phi - \bellman^* \hat{Q}_k \|_\infty \leq \epsilon_R + \gamma \epsilon_P \lipsapx
\end{equation}
since $\apx$ finds a $\apf_{k+1}$ that minimizes the approximation error.

Therefore, Theorem~\ref{thm:error_avi} follows by combining Inequality~(\ref{eq:apx_error}) and Lemma~\ref{lem:avi}.

\end{proof}