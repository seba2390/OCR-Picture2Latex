\vspace{-0.5em}
\section{Experimental Evaluation}
\label{sec:exp}

% \ys{TODO: unify the names, colors and formats in all figures. }
% \ys{1. gridworld with DQN}
% \ys{2. cartpole with DQN}
% \ys{3. 3dball and mujoco with SAC}

% \ys{ablation: transfer with linear head; transfer only dynamics/only rewards; learn auxiliary tasks not transferred. 
% learning linearly sufficient rep for all policies (after submission)
% }
We empirically evaluate our transfer learning algorithm in various environments and multiple observation-change scenarios.
Detailed experiment setup and hyperparameters are in Appendix~\ref{app:exp}.
% As this paper studies a novel problem, we tried but could not find any existing method that works under the same setting. Therefore, we evaluate our algorithm by (1) comparing it with the base learning algorithm, and (2) performing ablation studies for different components of the algorithm.

\textbf{Baselines.}
To verify the effectiveness of our proposed transfer learning method, we compare our transfer learning algorithm with 4 baselines: 
(1) \textit{Single}: a single-task base learner. 
(2) \textit{Auxiliary}: learns auxiliary models from scratch to regularize representation. 
(3) \textit{Fine-tune}: loads and freezes the source policy head, and retrains an encoder in the target task. 
(4) \textit{Time-aligned}~\citep{gupta2017learning}: supposes the target task and the source task proceed to the same latent state given the same action sequence, and pre-trains a target-task encoder with saved source-task trajectories. More details of baseline implementations are in Appendix~\ref{app:exp_baseline}.

\textbf{Scenarios.}
As motivated in Section~\ref{sec:intro}, there are many scenarios where one can benefit from transfer learning across observation feature spaces. We evaluate our proposed transfer algorithm in 7 environments that fit various scenarios, to simulate real-world applications: \\
(1) \textit{Vec-to-pixel:} a novel and challenging scenario, where the source task has low-dimensional vector observations and the target task has pixel observations. We use 3 vector-input environments CartPole, Acrobot and Cheetah-Run as source tasks, and use the rendered image in the target task. \\
% \footnote{For CartPole and Acrobot, to ensure Markovian observation, let the agent observes (current\_screen - last\_screen). For Cheetah-Run, we use the pixel observation in DMC~\citep{tassa2018deepmind}.}
(2) \textit{More-sensor:} another challenging scenario where the target task has a lot more sensors than the source task. We use 3 MuJoCo environments: HalfCheetah, Hopper and Walker2d, whose original observation dimensions are 17, 11 and 17, respectively. We add mass-based inertia and velocity (provided by MuJoCo's API), resulting in 145, 91, 145 dimensions in the corresponding target tasks.  \\
(3) \textit{Broken-sensor:} we use an existing game 3DBall contained in the Unity ML-Agents Toolkit~\citep{juliani2018unity}, which has two different observation specifications that naturally fit our transfer setting: the source observation has 8 features containing the velocity of the ball; the target observation does not have the ball's velocity, thus the agent has to stack the past 9 frames to infer the velocity. Please see Appendix~\ref{app:exp_env} for more detailed descriptions of all the 7 environments.

\textbf{Base DRL Learners.}
What we propose is a transfer learning mechanism that can be combined with any existing DRL methods. For environments with discrete action spaces (CartPole, Acrobot), we use the DQN algorithm~\citep{mnih2015human}, while for environments with continuous action spaces (Cheetah-Run, HalfCheetah, Hopper, Walker2d, 3DBall), we use the SAC algorithm~\citep{haarnoja2018soft}. To ensure a  fair comparison, we use the same base DRL learner with the same hyperparameter settings for all tested methods, as detailed in Appendix~\ref{app:exp_drl}. As is common in prior works, our implementation of the RL algorithms is mostly a proof of concept, thus many advanced training techniques are not included (e.g. Rainbow DQN).

% \textbf{Scenario 1: Transfer Knowledge from Vector Observations to Pixel Observations}\quad
% As motivated in Figure~\ref{fig:example}, many environments can be learned by either vector-based observations or pixel-based observations. Training with compact vector observations is more efficient, but in real-world environments, compact vector features are difficult to collect, and only high-dimensional observations (e.g. images, language instructions) are provided. 
% Can we first train an agent in a simulator environment with vector inputs, then transfer the knowledge of environment dynamics to high-dimensional inputs to accelerate learning? 
% To verify that our transfer algorithm works for drastic observation changes, we first focus on a challenging scenario where we transfer knowledge from a compact observation space with vector features to a rich observation space with pixel images. 
% We implement a DQN learner~\citep{mnih2015human} on the classic CartPole environment in Gym~\citep{gym}. The source-task agent observes a 4-dimensional vector containing positions and velocities of cart and pole,
% % with the cart position, cart velocity, pole angle and pole angular velocity,
% while in the target task, the agent observes a screenshot of the rendered environment. 
% Figure~\ref{sfig:cartpole} shows a comparison between learning with our transfer algorithm and learning without our transfer algorithm, where the source task learning curve serves as an oracle.
% We can see that with our proposed transfer method, the agent learns the pixel observation much more efficiently than a single-task learning agent in the challenging target task with pixel observations, and even impressively achieves similar performance as in the simpler vector-based source task. 
% \textit{To the best of our knowledge, we are the first to achieve knowledge transfer from a vector-input environment to a pixel-input environment without any pre-defined mappings.}

% \begin{figure}
%     \centering
%     \input{figs/pixel_all}
%     \caption{Caption}
%     \label{fig:my_label}
% \end{figure}

\begin{figure}[!t]
\vspace{-2.5em}
\centering
 \begin{subfigure}[t]{0.01\columnwidth}
  \centering
  \input{figs/cart_new_baseline_smooth}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.31\columnwidth}
  \centering
  \input{figs/acrobot} 
 \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.31\columnwidth}
  \centering
  \input{figs/cheetah_run} 
 \end{subfigure}
 \vspace{-1em}
 
 \begin{subfigure}[t]{0.255\columnwidth}
  \centering
  \input{figs/halfcheetah_rebuttal}
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.245\columnwidth}
  \centering
  \input{figs/hopper_rebuttal} 
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.245\columnwidth}
  \centering
  \input{figs/walker2d_rebuttal} 
 \end{subfigure}
 \hfill
 \begin{subfigure}[t]{0.235\columnwidth}
  \centering
  \input{figs/new_ball_withsource} 
 \end{subfigure}
 \vspace{-2em}
 \caption{
 Our proposed transfer method outperforms all baselines in target tasks over all tested scenarios. (The dashed \textcolor{sourcecolor}{\textbf{green}} lines are the learning curves in source tasks.) Results are averaged over 10 random seeds. 
%  \small{(a) By transferring knowledge from vector-input CartPole to pixel-input CartPole, the agent achieves much better performance than learning the pixel-input CartPole from scratch. (b) Ablation study verifies the effectiveness of our algorithm design. (c) Under different selections of hyperparameter $\lambda$, the algorithm works better than learning from scratch (when $\lambda=0$). All results are averaged over 20 random seeds.}
 }
\label{fig:all}
\vspace{-1.5em}
\end{figure}

% \begin{figure}[!htbp]
% \vspace{-1em}
% \centering
%  \begin{subfigure}[t]{0.255\columnwidth}
%   \centering
%   \input{figs/halfcheetah_boostrap}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[t]{0.245\columnwidth}
%   \centering
%   \input{figs/hopper_boostrap} 
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[t]{0.245\columnwidth}
%   \centering
%   \input{figs/walker2d_boostrap} 
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[t]{0.235\columnwidth}
%   \centering
%   \input{figs/ball_new} 
%  \end{subfigure}
%  \vspace{-2em}
%  \caption{\small{Comparison between our proposed transfer algorithm and baselines in More-sensor and Broken sensor scenarios. Results are averaged over 10 random seeds.}}
% \label{fig:mujoco}
% \vspace{-1.5em}
% \end{figure}

\textbf{Results.}
Experimental results on all tested environments are shown in Figure~\ref{fig:all}. We can see that our proposed transfer method learns significantly better than the single-task learner, and also outperforms all baselines in the challenging target tasks. 
Our transfer method outperforms Auxiliary since it transfers dynamics model from the source task instead of learning it from scratch, and outperforms Fine-tine since it regularizes the challenging encoder learning with a model-based regularizer.
% the target observation much more efficiently than a single-task learning agent in the challenging target tasks. 
% Among all baseline methods, our transfer method achieves the best performance in all tasks. 
The Time-aligned method, although requires additional pre-training that is not shown in the figures, does not work better than Single in most environments, because the time-based alignment assumption may not hold as discussed in Appendix~\ref{app:exp_baseline}.
% Note that the Time-aligned method requires additional pre-training process that is not shown in the figures (to test whether alignment is achievable, we set the pre-training steps as many as in training, which doubles the total training steps of Time-aligned). 
In some environments (e.g. Hopper, Walker2d, 3DBall), our transfer algorithm even achieves better asymptotic performance than the source-task policy, which suggests that our method can be used for improving the policy with incremental observation design.
\textit{To the best of our knowledge, we are the first to achieve effective knowledge transfer from a vector-input environment to a pixel-input environment without any pre-defined mappings.}

% \textbf{Scenario 2: Transfer Knowledge for Harder Continuous Control}\quad
% We further evaluate our transfer learning algorithm in multiple continuous control tasks. HalfCheetah, Hopper and Walker2d are classic MuJoCo environments, where we increase the number of input features in the target tasks (See Appendix~\ref{app:exp} for details). 3DBall is a ball-balancing game contained in the Unity ML-Agents Toolkit~\citep{juliani2018unity}, which has two different observation specifications that naturally fit our transfer setting:
% the source observation has 8 features containing the velocity of the ball; the target observation (harder version) does not have the ball's velocity, but stacks the past 9 frames.
% the source observation has 8 features corresponding to the rotation of the agent cube, and the position and velocity of the ball;
% the target observation (harder version) has a stack of 9 past frames, each of which corresponds to the rotation of the agent cube, and the position of the ball.
% The results are shown in Figure~\ref{fig:mujoco}, where our transfer algorithm always outperforms the vanilla learner without knowledge transfer. An ablation study is provided in Figure~\ref{fig:mujoco_ablation} in Appendix~\ref{app:exp}, where we show that transferring only $\hat{P}$ or transferring only $\hat{R}$ are both better than a single-task learner, but worse than our method that transfers both $\hat{P}$ and $\hat{R}$.


% Therefore, we suggest 
% thus transferring the dynamics models do not help too much in the target, although it is still better than learning without transfer. Therefore, learning a reasonable dynamics model is important for  


\begin{wrapfigure}{r}{0.35\textwidth}
\vspace{-2em}
  \begin{center}
    \input{figs/halfcheetah_ablation_small}
  \end{center}
  \vspace{-1.5em}
  \caption{Ablation Study}
  \label{fig:ablation_halfcheetah}
\vspace{-1.5em}
\end{wrapfigure}

\textbf{Ablation Study and Hyper-parameter Test.}
To verify the effectiveness of proposed transfer method, we conduct ablation study and compare our method with its two variants: only transferring the transition model $\hat{P}$ and only transferring the reward model $\hat{R}$. Figure~\ref{fig:ablation_halfcheetah} shows the comparison in HalfCheetah, and Appendix~\ref{app:exp_results} demonstrates more results. We find that all the variants of our method can make some improvements, which suggests that \textit{transferring $\hat{P}$ and $\hat{R}$ are both effective designs for accelerating the target task learning.}
Figure~\ref{fig:cart_ablation} in Appendix~\ref{app:exp_results}
shows another ablation study where we investigate different selections of model regularizers and policy heads. 
% verifies our claim that the model-regularized representation is not linearly sufficient for learning.
% As discussed in Section~\ref{sec:theory}, the auxiliary model encourages the representation to be sufficient while not linearly sufficient. This claim is verified by Figure~\ref{fig:cart_ablation} in Appendix~\ref{app:exp}.
In Algorithm~\ref{alg:target}, a hyper-parameter $\lambda$ is needed to control the weight of the transferred model-based regularizer. Figure~\ref{fig:hyper} in Appendix~\ref{app:exp_results} shows that, for a wide range of $\lambda$'s, the agent consistently outperforms the single-task learner. 

\textbf{Potential Limitations and Solutions.}
As Figure~\ref{fig:all} shows, in some environments such as HalfCheetah, our transfer algorithm significantly outperforms baselines without transfer. But in Walker2d, the improvement is less significant, although transferring is still better than not transferring.
This phenomenon is common in model-based learning~\citep{nagabandi2018neural}, as state predicting in Walker2d is harder than that in HalfCheetah due to the complexity of the dynamics.
Therefore, we suggest using our method to transfer when the learned models $(\hat{P},\hat{R})$ in the source task are relatively good (error is low).
More techniques of improving model-based learning, such as bisimulation~\citep{zhang2020learning,castro2020scalable}, can be applied to further improve the transfer performance.


% \begin{figure}[!htbp]
% \centering
%  \begin{subfigure}[t]{0.245\columnwidth}
%   \centering
%   \input{figs/halfcheetah_ablation}
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[t]{0.245\columnwidth}
%   \centering
%   \input{figs/hopper_ablation} 
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[t]{0.245\columnwidth}
%   \centering
%   \input{figs/walker2d_ablation} 
%  \end{subfigure}
%  \hfill
%  \begin{subfigure}[t]{0.245\columnwidth}
%   \centering
%   \input{figs/ball_ablation} 
%  \end{subfigure}
%  \vspace{-2em}
%  \caption{\small{Comparison of single-task SAC and SAC combined with our transfer method in the target task.}}
% \label{fig:mujoco_ablation}
% \end{figure}

% \ys{discuss the limitation of model-based method.
% Compare HalfCheetah and Walker2d.
% Cite related papers.
% Ways to improve it.}




% Environments we plan to use:
% \begin{itemize}
%     \item Linear model: (1) linear Q learning (2) mujoco/ml-agents
%     \item Vectorized MuJoCo: from plain observation to rich observation
%     \item From vectorized MuJoCo to pixel-based MuJoCo
%     \item Pixel MuJoCo: transfer among tasks with different backgrounds (also encode source)
%     \item ML-Agents: 3dball, pushblock, ...
%     \item Transfer for lifelong
% \end{itemize}