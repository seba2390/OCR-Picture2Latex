\begin{figure}[t]
\vspace{-2em}
\begin{algorithm}[H]
\caption{Source Task Learning}
\label{alg:source}
\begin{algorithmic}[1] 
    \Require Regularization weight $\lambda$; update frequency $m$ for stable encoder.
    \State Initialize encoder $\phi\source$, stable encoder $\hat{\phi}\source$, policy $\pi\source$, transition prediction network $\hat{P}$ and reward prediction network $\hat{R}$.
    \For{$t=0,1,\cdots$}  
        \State Take action $a_t \sim \pi\source(\phi\source(o\source_t))$, get next observation $o_{t+1}\source$ and reward $r_t$, store to buffer.
        \State Sample a mini-batch $\{o_i,a_i,r_i,o^\prime_i\}_{i=1}^N$ from the buffer.
        \State Update $\hat{P}$ and $\hat{R}$ using one-step gradient descent with $\nabla_{\hat{P}} L_P(\hat{\phi}\source;\hat{P})$ and $\nabla_{\hat{R}} L_R(\hat{\phi}\source;\hat{R})$, where $L_P$ and $L_R$ are defined in Equation~(\ref{eq:model_loss}).
        \State Update encoder and policy by $\min_{\pi\source,\phi\source} L_{\text{base}}(\phi\source, \pi\source) + \lambda \big(L_P(\phi\source;\hat{P}) + L_R(\phi\source;\hat{R})\big).$
        \If{$t \mid m$}  Update the stable encoder $\hat{\phi}\source \leftarrow \phi\source.$
        \EndIf
    \EndFor 
\end{algorithmic}
\end{algorithm}
\vspace{-1.5em}
\end{figure}
% \vspace{-1em}
\begin{figure}[t]
\vspace{-2em}
\begin{algorithm}[H]
\caption{Target Task Learning with Transferred Dynamics Models}
\label{alg:target}
\begin{algorithmic}[1] 
    \Require Regularization weight $\lambda$; dynamics models $\hat{P}$ and $\hat{R}$ learned in the source task.
    \State Initialize encoder $\phi\target$, policy $\pi\target$
    \For{$t=0,1,\cdots$}  
        \State Take action $a_t \sim \pi\target(\phi\target(o_t\target))$, get next observation $o_{t+1}\target$ and reward $r_t$, store to buffer.
        \State Sample a mini-batch $\{o_i,a_i,r_i,o^\prime_i\}_{i=1}^N$ from the buffer.
        \State Update encoder and policy by $\min_{\phi\target, \pi\target} L_{\text{base}}(\phi\target,\pi\target) + \lambda \big(L_P(\phi\target;\hat{P}) + L_R(\phi\target;\hat{R}))$, where $L_P$ and $L_R$ are defined in Equation~(\ref{eq:model_loss}).
    \EndFor 
\end{algorithmic}
\end{algorithm}
\vspace{-2em}
\end{figure}
% \vspace{-1em}
\vspace{-0.5em}