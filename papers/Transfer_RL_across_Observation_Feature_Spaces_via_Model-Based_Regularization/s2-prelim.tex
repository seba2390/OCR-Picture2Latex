\vspace{-1em}
\section{Preliminaries and Background}
\label{sec:prelim}
% \subsection{Basic Notations for RL}
% \label{sec:notation}

\textbf{Basic RL Notations.}
An RL task can be modeled by a Markov Decision Process (MDP)~\citep{puterman2014markov}, defined as a tuple $\mdp=\langle \states,\actions,P,R,\gamma \rangle$, where $\states$ is the state/observation space, $\actions$ is the action space, $P$ is the transition kernel, $R$ is the reward function and $\gamma$ is the discount factor. 
At timestep $t$, the agent observes state $o_t$, takes action $a_t$ based on its \textit{policy} $\pi: \states \to \Delta(\actions)$(where $\Delta(\cdot)$ denotes the space of probability distributions), and receives reward $r_t=R(o_t,a_t)$. The environment then proceeds to the next state $o_{t+1} \sim P(\cdot|o_t, a_t)$.
The goal of an RL agent is to find a policy $\pi$ in the policy space $\Pi$ with the highest cumulative reward, which is characterized by the value functions.
The \textit{value} of a policy $\pi$ for a state $o\in\states$ is defined as $V^\pi(o) = \mathbb{E}_{\pi,P}[\sum_{t=0}^\infty \gamma^t r_t|o_{0}=o]$. The \textit{Q value} of a policy $\pi$ for a state-action pair $(o,a) \in \states \times \actions$ is defined as $Q^\pi(o,a) = \mathbb{E}_{\pi,P}[\sum_{t=0}^\infty \gamma^t r_t|o_0=o, a_0=a]$. Appendix~\ref{app:prelim} provides more background of RL.
% For any policy $\pi$, its value function is the fixed point of the \textit{Bellman operator}
% \begin{equation}
%     \bellman^\pi V(o) = \mathbb{E}_{a\sim \pi(o), o^\prime \sim P(o,a)}[R(o,a) + \gamma V(o^\prime)] 
% \end{equation}  

% \fh{Why do we emphasize policy iteration here? Could we move these discussions to appendix?}

\textbf{Representation Learning in RL.}
Real-world applications usually have large observation spaces for which function approximation is needed to learn the value or the policy. 
However, directly learning a policy over the entire observation space could be difficult, as there is usually redundant information in the observation inputs. A common solution is to map the large-scale observation into a smaller representation space via a \textit{non-linear encoder} (also called a \textit{representation mapping}) $\phi: \states \to \mathbb{R}^d$, where $d$ is the representation dimension, 
and then learn the policy/value function over the representation space $\phi(\states)$. In DRL, the encoder and the policy/value are usually jointly learned.
% For notation simplicity, we use $\rep$ to denote the representation matrix, where the $i$-th row of $\rep$ refers the the feature of the $i$-the observation.
% More specifically, let $\{\phi_1,\phi_2,\cdots,\phi_d\}$ be a set of linearly independent basis functions of the state, where $\phi_j(o)$ is the value of feature $j$ in state $o$. 
% In this paper, 
% Let $\rep$ denote a design matrix with $\rep_{i,j}=\phi_j(o_i)$, i.e., the basis functions span the columns of $\rep$ and the states span the rows of $\rep$.
% Throughout this paper, we refer to the representation of observations in a task as $\rep$.
% \fh{The last two sentences could be improved a bit and better incorporated with the previous policy space parameterized by $\rep$}.