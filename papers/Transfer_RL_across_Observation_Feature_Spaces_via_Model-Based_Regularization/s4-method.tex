% \vspace{-0.5em}
\section{Methodology: Transfer with Regularized Representation}
\label{sec:method}
% \vspace{-0.5em}

% The main idea of our proposed transfer method is to learn a latent dynamics model in the source task, and use the dynamics model to regularize representation learning in the target task. 
% This section is organized as follows.
%%%% FOR CAMERA READY - SHORT
In this section, we first formally characterize \textit{``what a good representation is for RL''} in Section~\ref{sec:representation}, then introduce our proposed transfer algorithm based on representation regularization  in Section~\ref{sec:algo}, and next provide theoretical analysis of the algorithm in Section~\ref{sec:theory}.
% The main idea of our proposed transfer method is to learn a latent dynamics model in the source task, and use the dynamics model to regularize representation learning in the target task. 
%%%% FOR CAMERA READY - Long
% To motivate our method, we first formally characterize \textit{``what a good representation is for RL''} in Section~\ref{sec:representation}. 
% Then, we introduce our across-observation transfer algorithm in Section~\ref{sec:algo}, which learns a latent dynamics model as an auxiliary task in the source task, and uses the dynamics model to regularize representation learning in the target task. 
% Theoretical analysis in Section~\ref{sec:theory} explains why such a latent dynamics model enables better representation learning and transfer learning.
%%%%%%%%%%

%%%% FOR SUBMISSION
% The main idea of our proposed transfer method is to learn a latent dynamics model in the source task, and use the dynamics model to regularize representation learning in the target task. This section is organized as follows.
% First, we formally characterize \textit{``what a good representation is for RL''} in Section~\ref{sec:representation}, which is crucial to motivate our algorithm. 
% Then, we introduce our proposed algorithm for across-observation transfer in Section~\ref{sec:algo}, where we learn a latent dynamics model as an auxiliary task in the source task, and transfer the dynamics model to the target task to regularize representation learning. 
% Theoretical analysis for the proposed algorithm is provided in Section~\ref{sec:theory}, which explains why the proposed method works and provides insights for the architecture design.
%%%%%%%%%%

% The analysis in Section~\ref{sec:representation} and~\ref{sec:theory} mainly focus on learning with policy iteration, and we provide analysis for value-based learning in Appendix~\ref{app:avi}.

\subsection{Characterizing Conditions for Good Representations}
\label{sec:representation} 
% Given a representation mapping $\rep$, the Q value of a policy $\pi$ can be approximated by solving $\hat{Q}^\pi=\apx Q^\pi$, where $\apx$ is an approximation operator $\apx Q^\pi := \mathrm{argmin}_f \mathbb{E}_{o\sim \mu(\pi)}[l(f(o)-Q^\pi(o))]$.

As discussed in Section~\ref{sec:prelim}, real-world applications usually have rich and redundant observations, where learning a good representation~\citep{jaderberg2016reinforcement,dabney2020the} is essential for efficiently finding an optimal policy.
However, the properties that constitute a good representation for an RL task are still an open question.
%Before introducing our algorithm that encourages the agent to learn a better representation with knowledge transfer, 
% In this section, we formulate conditions for a good representation and the relation between representation and learning performance.
Some prior works~\citep{bellemare2019geometric,dabney2020the,gelada2019deepmdp} have discussed the representation quality in DRL, but we take a different perspective and focus on characterizing the sufficient properties of representation for learning a task.
% \textit{what is a good representation/encoder for learning a near-optimal policy? }
% (2) How does learning an auxiliary dynamics model help with representation learning?
% (3) Is the auxiliary dynamics model transferable?
% (1) formulating sufficient conditions for a representation to enable guaranteed learning, and 
% (2) achieving representation sufficiency with transferable dynamics models.
% \ac{should we be more strongly calling out the novelty in this section? or should we be saying it is building on previous work?}

Given a representation mapping $\phi$, the Q value of any $(o,a)\in\states\times\actions$ can be approximately represented by a function of $\phi(o)$, i.e., $\hat{Q}(o, a) = h(\phi(o); \theta_a)$, where $h$ is a function parameterized by $\theta_a$.
% For example, if linear approximation is used, the Q value can be represented as $\hat{Q}(o,a) = \phi(o)^\top \theta_a$, where $\theta_a\in\mathbb{R}^d$ is a vector associated with action $a$. 
% The approximation error $\|Q-\hat{Q}\|$ depends on both the selection of the representation $\phi$ and the approximation parameters $\theta_a$.
To study the relation between representation quality and approximation quality,
we define an \textit{approximation operator} $\apx$, which finds the best Q-value approximation based on $\phi$. Formally, let $\Theta$ denote the parameter space of function $h\in\mathcal{H}$, then $\forall a\in\actions$, $\apx Q(o,a) := h(\phi(o);\theta^*_a)$, where $\theta^*_a=\mathrm{argmin}_{\theta\in\Theta} \mathbb{E}_o [\| h(\phi(o);\theta) - Q(\phi(o),a) \| ]$. Such a function $h$ can be realized by neural networks as universal function approximators~\citep{hornik1989multilayer}.
Therefore, the value approximation error $\|Q-\apx Q\|$ only depends on the representation quality, i.e., whether we can represent the Q value of any state $o$ as a function of the encoded state $\phi(o)$.
% \fh{$f$}\fh{Should we use $f$ or $\mathcal{F}$?}. \ys{this is an operator, so I think $\mathcal{F}$ is better. $f$ is already used in Assumption 1.}

The quality of the encoder $\phi$ is crucial for learning an accurate value function or learning a good policy. The ideal encoder $\phi$ should discard irrelevant information in the raw observation but keep essential information. 
% Additionally, it should also make it easy \ac{can we be more precise than make it easy?} to learn a policy in the resulting representation space.
% Additionally, it should also  make it easy \ac{can we be more precise than make it easy?} to learn a policy in the resulting representation space. 
In supervised or self-supervised representation learning~\citep{chen2020simple,achille2018emergence}, it is believed that a good representation $\phi(X)$ of input $X$ should contain minimal information of $X$ which maintaining sufficient information for predicting the label $Y$.
% $Z^*$ of input $X$ should be 
% (1) \textit{sufficient}, i.e., same mutual information $I(Z^* \wedge Y)=I(X \wedge Y)$ for the underlying label $Y$, 
% and (2) \textit{minimal}, i.e., smallest mutual information $I(Z^* \wedge X) \leq I(\tilde{Z} \wedge X)$ for all sufficient $\tilde{Z}$. 
% \ac{condense these two paragraphs?}
However, in RL, it is difficult to identify whether a representation is sufficient, since there is no label corresponding to each input. 
% Is there a way to determine whether a representation is good for an RL task?
The focus of an agent is to estimate the value of each input $o\in\states$, which is associated with some policy. Therefore, we point out that the representation quality in RL is \emph{policy-dependent}. 
Below, we formally characterize the sufficiency of a representation mapping in terms of a fixed policy and learning a task. 
% Then, minimality naturally holds if a representation is sufficient and shares the minimal mutual information with the input observation.
% Therefore, in this paper, our first contribution is to formally define the sufficiency of representation for RL tasks. Definition~\ref{def:suf_policy}
% and Definition~\ref{def:suf_learn} below define what a sufficient representation should be for a fixed policy and for learning an optimal policy, respectively.

\textbf{Sufficiency for A Fixed Policy.} If the agent is executing a fixed policy, and its goal is to estimate the expected future return from the environment, then a representation is sufficient for the policy as long as it can encode the policy value $\vpi$. A formal definition is provided by Definition~\ref{def:suf_policy} in Appendix~\ref{app:repre}.

% as studied in some literature~\citep{parr2008analysis,lyle2021effect}. 
% then the MDP reduces to a Markov Reward Process (MRP). The optimality of representation in MRPs is studied in some literature~\citep{parr2008analysis,lyle2021effect}. 
% In Definition~\ref{def:suf_policy}, Appendix~\ref{app:repre}, we propose a formal definition that a representation is sufficient for a fixed policy $\pi$ as long as it can encode the policy value $\vpi$. \ac{this paragraph does motivate the following paragraph but perhaps it can be shortened or maybe remove the sentence/reference to the appendix?}

\textbf{Sufficiency for Learning A Task.} The goal of RL is to find an optimal policy. Therefore, it is not adequate for the representation to only fit one policy.
% important to investigate the conditions under which a representation is sufficient for a changing policy distribution.
Intuitively, a representation mapping is sufficient for learning if we are able to find an optimal policy over the representation space $\phi(\states)$, which requires multiple iterations of policy evaluation and policy improvement.
% \ac{does this mean multiple iterations of policy improvement?}
% Therefore, the representation should be sufficient for multiple policies that are important for learning. 
Definition~\ref{def:reppolicy} below defines a set of ``important'' policies for learning with $\phi(\states)$.
% Definition~\ref{def:suf_learn} and Lemma~\ref{lem:bound_pi} below suggest that the representation is sufficient for learning a near-optimal policy as long as it is sufficient for a set of policies, $\reppolicy$ defined below.
% \textbf{Ultimate Goal: Good Representation for Learning A Task}\\
% In the extreme case, if $\rep$ is sufficient for all policies $\pi\in\Pi$, then $\rep$ should be sufficient for finding the optimal policy with the highest value. However, it is not necessary to satisfy the needs of all policies. 
\begin{definition}[Encoded Deterministic Policies]
\label{def:reppolicy}
For a given representation mapping $\phi(\cdot)$, define an encoded deterministic policy set $\reppolicy$ as the set of policies that are deterministic and take the same actions for observations with the same representations. Formally,
% $\reppolicy := \{ \pi \in \Pi: \exists \tilde{\pi}: \phi(\states) \to \actions, \text{ s.t. } \forall o\in\states, \pi(o) = \tilde{\pi}(\phi(o))  \}.$
\begin{equation}
    \reppolicy := \{ \pi \in \Pi \quad | \quad \exists \tilde{\pi}: \phi(\states) \to \actions \text{ s.t. } \forall o\in\states, \pi(o) = \tilde{\pi}(\phi(o))  \},
\end{equation}
where $\tilde{\pi}$ is a mapping from the representation space to the action space.
% which can be written as a composition of $\phi(\cdot)$ and some $\tilde{\pi}: \phi(\states) \to \actions$, i.e., 
\end{definition}
A policy $\pi$ is in $\reppolicy$ if it does not distinguish $o_1$ and $o_2$ when $\phi(o_1)=\phi(o_2)$. Therefore, $\reppolicy$ can be regarded as deterministic policies that make decisions for encoded observations. 
% \ac{slightly confusing word choice "parameterized in the representation space"}.
Now, we define the concept of sufficient representation for learning in an MDP.
\begin{definition}[Sufficient Representation for Learning]
\label{def:suf_learn}
A representation mapping $\phi$ is \textbf{sufficient} for a task $\mdp$ w.r.t. approximation operator $\apx$ if $\apx \qpi = \qpi$ for all $\pi\in\reppolicy$. Furthermore, \\
$\bullet$ $\phi$ is \textbf{linearly-sufficient} for learning $\mdp$ if $\exists\theta_a$ s.t. $\qpi(o,a)=\phi(o)^\top\theta_a$, $\forall a\in\actions, \pi\in\reppolicy$. \\
$\bullet$ $\phi$ is $\boldsymbol{\epsilon}$\textbf{-sufficient} for learning $\mdp$ if $\|\apx\qpi - \qpi\| \leq \epsilon$, $\forall \pi\in\reppolicy$.
% $\pi = \tilde{\pi}\circ\phi$, where $\tilde{\pi}$ is a policy mapping over the representation space, i.e., 
% $\tilde{\states}=\phi(\states)$ is the image of the observation space under encoder $\phi$.
\end{definition}

% Note that $\tilde{\pi}$ is a deterministic policy mapping over the representation space. \fh{To go back to the original policy space and characterize the subset of the policy space corresponding to the representation space.}\fh{Needs better explanation. Maybe we should give a name to $\reppolicy$. Do we care about how to characterize $\reppolicy$ or can we just leave it alone? Also you used undefined $\Phi$ in $\reppolicy$.} For a given representation mapping $\phi(\cdot)$, we use $\reppolicy$ to denote the set of policies which can be written as a composition of $\phi(\cdot)$ and some $\tilde{\pi}: \phi(\states) \to \actions$. In other words, 
% \begin{equation}
%     \reppolicy := \{ \pi \in \Pi | \quad \exists \tilde{\pi}: \phi(\states) \to \actions \text{ s.t. } \forall o\in\states, \pi(o) = \tilde{\pi}(\phi(o))  \}.
% \end{equation}
Definition~\ref{def:suf_learn} suggests that the representation is sufficient for learning a task as long as it is sufficient for policies in $\reppolicy$.
Then, the lemma below justifies that a nearly sufficient representation can ensure that approximate policy iteration converges to a near-optimal solution. (See Appendix~\ref{app:avi} for analysis on approximate value iteration.)

\begin{lemma}[Error Bound for Approximate Policy Iteration]
\label{lem:bound_pi}
If $\phi$ is $\epsilon$-sufficient for task $\mdp$ (with $\ell_\infty$ norm), then the approximated policy iteration with approximation operator $\apx$ starting from any initial policy that is encoded by $\phi$ ($\pi_0\in\reppolicy$) satisfies
\setlength\abovedisplayskip{-2pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
    \limsup_{k\to\infty} \|Q^* - Q^{\pi_k} \|_{\infty} \leq \frac{2\gamma^2\epsilon}{(1-\gamma)^2},
\end{equation}
where $\pi_k$ is the policy in the $k$-th iteration. 
\end{lemma}
Lemma~\ref{lem:bound_pi}, proved in Appendix~\ref{app:proofs}, is extended from the error bound provided by~\citet{BertsekasTsitsiklis96}. For simplicity, we consider the bound in $\ell_\infty$, but tighter bounds can be derived with other norms~\citep{munos2005error}, although a tighter bound is not the focus of this paper.

\textbf{How Can We Learn A Sufficient Representation?}
So far we have provided a principle to define whether a given representation is sufficient for learning. In DRL, the representation is learned together with the policy or value function using neural networks, but the quality of the representation may be poor~\citep{dabney2020the}, which makes it hard for the agent to find an optimal policy.
Based on Definition~\ref{def:suf_learn}, a natural method to learn a good representation is to let the representation fit as many policy values as possible as auxiliary tasks, which matches the ideas in other works. For example, \citet{bellemare2019geometric} propose to fit a set of representative policies (called \textit{adversarial value functions}). 
% but the exact solution is computationally intractable since there are an exponential number of policies to consider. \ac{I am not sure this last part is exactly correct. The adversarial value functions are a finite, representative set of value funcs that are at the vertices of the polytope. Ok to delete following "but..."}
\citet{dabney2020the} choose to fit the values of all past policies (along the \textit{value improvement path}), which requires less computational resource.
% but, during learning, the future policies in the value improvement path are unknown, thus the requirements for a representation being good are not fully satisfied.
% \ac{I would suggest deleting the "downsides" to both approaches i.e. "but.." and just present them as related work.}
Different from these works that directly fit the value functions of multiple policies, in Section~\ref{sec:algo}, we propose to \emph{fit and transfer an auxiliary policy-independent dynamics model}, which is an efficient way to achieve sufficient representation for learning and knowledge transfer, as theoretically justified in Section~\ref{sec:theory}.
% a computationally efficient way to realize sufficient representation for learning and knowledge transferring: \emph{to fit and transfer an auxiliary policy-independent dynamics model}, 
% and Section~\ref{sec:theory} justifies in theory that a model-sufficient representation is also sufficient for learning the RL task. 
% In Section~\ref{sec:algo}, we introduce our algorithm that uses a transferable model-based regularizer to facilitate learning. 
% Then, in Section~\ref{sec:theory}, we provide detailed theoretical analysis for why the dynamics model helps to learn a good representation and why it is transferable.

% Then, the linear approximation of the Q value can be represented in the matrix form
% \begin{equation}
%     \hat{Q} = \rep \Theta
% \end{equation}
% The linear approximation of the Q value can also be regarded as a low-rank approximation of $Q$.
% Given a fixed representation $\rep$, computing $\hat{Q}$ is the same as projecting $Q$ onto the representation space $\rep$.
% We define the \textit{projected Bellman operator} and the \textit{projected optimality Bellman operator} as
% \begin{align}
%     &\bellman^\pi_\rep Q = \Pi_\rep \bellman^\pi Q = \rep(\rep^\top \rep )^{-1}\rep^\top \bellman^\pi Q \\
%     &\bellman^*_\rep Q = \Pi_\rep \bellman^* Q = \rep(\rep^\top \rep )^{-1}\rep^\top \bellman^* Q
% \end{align} 

% \begin{lemma}
%  $\bellman^\pi_\rep$ and $\bellman^*_\rep$ are both contraction mappings, with fixed points $\Pi_\rep Q^\pi$ and $\Pi_\rep Q^*$.
% \end{lemma}


% In Section~\ys{todo}, we characterize the representation quality and analyze the optimality of the linearly approximated policy iteration.

\input{algo}

\subsection{Algorithm: Learning and Transferring Model-based Regularizer}
\label{sec:algo}
% \vspace{-0.5em}
Our goal is to use the knowledge learned in the source task to learn a good representation in the target task, such that the agent learns the target task more easily than learning from scratch.
Since we focus on developing a generic transfer mechanism, the base learner can be any DRL algorithms. We use $L_{\text{base}}$ to denote the loss function of the base learner. 
% which includes the loss of the policy network and the loss of the critic network.

% As in many representation learning works~\citep{jaderberg2016reinforcement,bellemare2019geometric,dabney2020the}, we use auxiliary tasks to regularize the learning process 
% \fh{let's make sure this sentence doesn't leave the impression that our method is not novel}.
% \fh{So, I think we should make it clear that the method is novel.}
% \fh{I feel we need to connect this idea better with the previous subsection of representation learning. How does the discussion of representation learning inspired our design.}
As motivated in Section~\ref{sec:representation}, we propose to learn policy-independent dynamics models for producing high-quality representations: (1) $\hat{P}$ which predicts the representation of the next state based on current state representation and action, and (2) $\hat{R}$ which predicts the immediate reward based on current state representation and action. 
For a batch of $N$ transition samples $\{o_i, a_i, o^\prime_i, r_i\}_{i=1}^N$, define the transition loss and the reward loss as:
\setlength\abovedisplayskip{0pt}
\setlength\belowdisplayskip{2pt}
\begin{equation}
    L_P(\phi,\hat{P}) = \frac{1}{N} \sum_{i=1}^N ( \hat{P}(\phi(o_i), a_i) - \bar{\phi}(o^\prime_i) )^2, \quad
    L_R(\phi,\hat{R}) = \frac{1}{N} \sum_{i=1}^N ( \hat{R}(\phi(o_i), a_i) - r_i )^2 \label{eq:model_loss} 
\end{equation}
where $\bar{\phi}(o^\prime_i)$ denotes the representation of the next state $o^\prime_i$ with stop gradients. 
In order to fit a more diverse state distribution, transition samples are drawn from an off-policy buffer, which stores shuffled past trajectories.
% Note that a similar model loss is commonly used in model-based RL~\citep{van2020plannable} and bisimulation works~\cite{zhang2020learning}, but our focus is to use the learned model as an auxiliary task and to transfer the model to the target task.

\begin{wrapfigure}{r}{0.45\textwidth}
\vspace{-1em}
  \centering
  \includegraphics[width=0.45\textwidth]{figs/model_new.pdf}
  \vspace{-1.5em}
  \caption{\small{The architecture of proposed method. $\hat{P}$ and $\hat{R}$ are learned in the source task, then transferred to the target task and fixed during training.}}
  \label{fig:model}
\vspace{-1em}
\end{wrapfigure}


The learning procedures for the source task and the target task are illustrated in Algorithm~\ref{alg:source} and Algorithm~\ref{alg:target}, respectively. Figure~\ref{fig:model} depicts the architecture of the learning model for both source and target tasks. 
% where the policy/value and the dynamics model share the same encoder. 
% $(o,a,o^\prime,r)$ is a transition sample. 
$z=\phi(o)$ and $z^\prime=\bar{\phi}(o^\prime)$ are the encoded observation and next observation.
Given the current encoding $z$ and the action $a$, the dynamics models $\hat{P}$ and $\hat{R}$ return the predicted next encoding $\hat{z}^\prime=\hat{P}(z, a)$ and predicted reward $\hat{r}=\hat{R}(z, a)$.
Then the transition loss is the mean squared error (MSE) between $z^\prime$ and $\hat{z}^\prime$ in a batch; the reward loss is the MSE between $r$ and $\hat{r}$ in a batch.
% For both the source and the target task, we use transition loss $L_P$ and reward loss $L_R$ to regularize the learned representation. 
% Whether to learn policy network or value network depends on the base DRL algorithm. For example, if DQN is used, then a Q-value head should be appended to the representation $z$.


\textbf{In the source task (Algorithm~\ref{alg:source}):} 
dynamics models $\hat{P}$ and $\hat{R}$ are learned by minimizing $L_P$ and $L_R$, which are computed based on a recent copy of encoder called stable encoder $\hat{\phi}\source$ (Line 5).
The computation of the stable encoder is to help the dynamics models converge, as the actual encoder $\phi\source$ changes at every step. Note that a stable copy of the network is widely used in many DRL algorithms (e.g. the target network in DQN), which can be directly regarded as $\hat{\phi}\source$ without maintaining an extra network. 
The actual encoder $\phi\source$ is regularized by the auxiliary dynamics models $\hat{P}$ and $\hat{R}$ (Line 6).

\textbf{In the target task (Algorithm~\ref{alg:target}):} 
dynamics model $\hat{P}$ and $\hat{R}$ are transferred from the source task and fixed during learning. Therefore, the learning of $\phi\target$ is regularized by static dynamics models, which leads to faster and more stable convergence than naively learning an auxiliary task.

\textbf{Relation and Difference with Model-based RL and Bisimulation Metrics.}
Learning a dynamics model is a common technique in model-based RL~\citep{kipf2019contrastive,grimm2020value}, whose goal is to learn an accurate world model and use the model for planning. The dynamics model could be learned on either raw observations or representations. In our framework, we also learn a dynamics model, but the model serves as an auxiliary task, and learning is still performed by the model-free base learner with $L_{\mathrm{base}}$. Bisimulation methods~\citep{castro2020scalable,zhang2020learning} aim to approximate the bisimulation distances among states by learning dynamics models, whereas we do not explicitly measure the distance among states. 
% Different from the above literature, our focus is to use the learned model as an auxiliary task and to transfer the model to the target task.
Note that we also do not require a reconstruction loss that is common in literature~\citep{lee2019stochastic}.



% \textbf{Source Task: Learning with Auxiliary Dynamics Models.}
% For the source task, we learn an encoder $\phi\source$ that maps the raw observation to a representation space, and a policy $\pi\source$ that maps the representations to action distributions, action values and etc. 
% A dynamics model $\hat{P}$ and a reward model $\hat{R}$ are learned based on the representation.  

% \paragraph{Target Task: Learning with Transferred Model Regularizers.}
% For the target task, we also learn an encoder $\phi\target$ and a policy $\pi\target$, but also enforce the encoder to match the dynamics models learned from the source task.
% \textit{Version 1: Pre-trained Encoder}
% The encoder is pre-trained using randomly generated trajectories (or max-entropy policy trajectories). 
% \textit{Version 2: Regularized Encoder}
% During the target task learning, the encoder is regularized by the dynamics learned in the source task.


% \paragraph{Proposed Tricks} In the deep RL version, we use the following tricks to further improve the performance.\\
% (1) Linear Model + Linear Policy/Value heads (in order to fit the linear model theory).\\
% (2) Let the target task encoder to fool a discriminator that distinguishes from the source representation and the target representation.


% \paragraph{Network Complexity and Representation Sufficiency}
% It is commonly believed that a good representation should linearly represent the policy values~\citep{barreto2016successor,bellemare2019geometric,dabney2020the}. That is, the policy/value head should be a single linear layer. However, as we discussed in Section~\ref{sec:representation}, being linearly sufficient for learning requires the representation to be sufficient for exponentially many policies, which is expensive to compute and hard to transfer. Instead, we encourage the representation to be sufficient for predicting the next state representation and the reward, which is relatively easy to achieve and transfer. As a result, the regularized representation is sufficient but not linearly sufficient for learning. Therefore, we suggest using a \textit{non-linear policy/value head} following the representation.
% In Section~\ref{sec:theory}, we elaborate the relationship between model sufficiency and learning sufficiency of representations, where we show a trade-off between model complexity and approximation complexity. 

\subsection{Theoretical Analysis: Benefits of Transferable Dynamics Model}
\label{sec:theory}

% In our algorithm, we let the encoder $\phi$ minimize $L_P$ and $L_R$ as auxiliary tasks. In this section, we analyze the theoretical insights behind this model-based auxiliary task.
The algorithms introduced in Section~\ref{sec:algo} consist of two designs: learning a latent dynamics model as an auxiliary task, and transferring the dynamics model to the target task.
In this section, we show theoretical justifications and practical advantages of our proposed method. 
We aim to answer the following two questions: \textit{(1) How does learning an auxiliary dynamics model help with representation learning? (2) Is the auxiliary dynamics model transferable?}
% in terms of both single-task learning and transfer learning. 
% All technical proofs are provided in Appendix~\ref{app:proofs}.

% For notation simplicity, in this section we assume that the original observation space $\states$ is finite with cardinality $n$. 
% and the transition dynamics $P$ is deterministic, i.e., $supp(P(o,a))=1$ where $supp(\cdot)$ is the support of a vector. 
% But our method can be easily generalized to environments with infinite observation spaces and stochastic transitions. 
% \fh{Should we discuss how to generalize somewhere later?}.
% Recall that $\rep\in\mathbb{R}^{n \times d}$ is a matrix with $\rep_{i,:}=\phi(o_i)$. 
% We let $\rep$ denote the representation matrix, where the $i$-th row of $\rep$ refers to the feature of the $i$-the observation.
% We use $\pact\in\mathbb{R}^{n\times n}$, $\ract\in\mathbb{R}^n$ to denote the transition matrix and the reward vector of all states under action $a$, respectively. That is, $\pact[i,j] = P(o_j|o_i,a)$, 
% % \fh{do you want to explain $\pact[i]$ used in Def 4?} 
% $\ract[i] = R(o_i,a)$, where $[i,j]$ refers to the entry in the $i$-th row and the $j$-th column.
% Note that the dynamics $(\pact, \ract)$ are policy-independent.
% The following definition describes the quality of a representation map through the lens of dynamics.

For notational simplicity, let $\pact$ and $\ract$ denote the transition and reward functions associated with action $a\in\actions$.
% Similarly, let $\apa,\ara$ denote the learned latent dynamics models associated with action $a$. 
Note that $\pact$ and $\ract$ are independent of any policy. 
We then define the sufficiency of a representation mapping w.r.t. dynamics models as below.
\begin{definition}[Policy-independent Model Sufficiency]
\label{def:model_suf}
% $\rep$ is linearly sufficient for a dynamics pair $(\pact, \ract)$  if there exists matrices $\{ \apa \in \mathbb{R}^{d\times d} \}_{a\in\actions}$ and vectors $\{ \ara\in\mathbb{R}^d \}_{a\in\actions}$ such that $\rep\hat{P} = P\rep, \rep \hat{R} = R$.  
% A representation mapping $\phi$ is sufficient for a dynamics pair $(\pact, \ract)$ if $\forall a \in \actions$ there exists functions $\apa: \mathbb{R}^d \to \mathbb{R}^d$ and $\ara: \mathbb{R}^d \to \mathbb{R}$ such that $\forall o_i\in\states$, $\apa(\phi(o_i))=\pact[i]\rep$, $\ara(\phi(o_i)) = \ract[i]$, where $\pact[i]:=\pact[i,:]$.
% Here $(\apa,\ara)$ are called latent dynamics models.
For an MDP $\mdp$, a representation mapping $\phi$ is sufficient for its dynamics $(\pact,\ract)_{a\in\actions}$ if $\forall a \in \actions$, there exists functions $\apa: \mathbb{R}^d \to \mathbb{R}^d$ and $\ara: \mathbb{R}^d \to \mathbb{R}$ such that $\forall o\in\states$, $\apa(\phi(o))=\mathbb{E}_{o^\prime\sim \pact(o)}[\phi(o^\prime)]$, $\ara(\phi(o)) = \ract(o)$.
\end{definition}
\vspace{-0.5em}
\textbf{Remarks.} 
(1) $\phi$ is exactly sufficient for dynamics $(\pact,\ract)_{a\in\actions}$ when the transition function $P$ is deterministic. 
(2) If $P$ is stochastic, but we have $\max_{o,a}\| \mathbb{E}_{o^\prime\sim \pact(o)}[\phi(o^\prime)] - \apa(\phi(o))\|\leq\epsilon_P$ and $\max_{o,a}|\ract(o)-\ara(\phi(o))|\leq\epsilon_R$, then $\phi$ is \textbf{$\bm{(\epsilon_P,\epsilon_R)}$-sufficient} for the dynamics of $\mdp$.
% \ac{is there an assumption that the dynamics model here is deterministic? edit: i see below it is stated as such. should maybe make this more explicit beforehand so as not to trip up reviewers.}

% \textbf{Model Sufficiency Induces Learning Sufficiency}\quad
%What is the relationship between the model sufficiency and the sufficiency for learning (Definition~\ref{def:suf_learn})? 
Next we show by Proposition~\ref{prop:nonlin_suf} and Theorem~\ref{thm:model_learn_error} that learning sufficiency can be achieved via ensuring model sufficiency.

\begin{proposition}[Learning Sufficiency Induced by Policy-independent Model Sufficiency]
\label{prop:nonlin_suf}
Consider an MDP $\mdp$ with deterministic transition function $P$ and reward function $R$. If $\phi$ is sufficient for $(\pact, \ract)_{a\in\actions}$, then it is sufficient (but not necessarily linearly sufficient) 
for learning in $\mdp$.
% \fh{Did you specify the relationship between the $\mdp$ and the $\pact, \ract$?}.
\end{proposition}
Proposition~\ref{prop:nonlin_suf} shows that, if the transition is deterministic and the model errors $L_P,L_R$ are zero, then $\phi$ is exactly sufficient for learning.
% More generally, Theorem~\ref{thm:model_learn_error} shows that if model errors are bounded, there exists an $\epsilon$ such that $\phi$ is $\epsilon$-sufficient for learning.
% Proposition~\ref{prop:nonlin_suf} proven in Appendix~\ref{proof:nonlin_suf} suggests that learning policy-independent dynamics models as auxiliary tasks, as we propose in Algorithm~\ref{alg:source} and~\ref{alg:target}.
% The proof of Proposition~\ref{prop:nonlin_suf} is in Appendix~\ref{proof:nonlin_suf}.
% It is important to note that the representation regularized by the policy-independent models may not be linearly sufficient for learning. Therefore, we set the policy head after the latent representation to have more than one layer to reduce approximation error.
More generally, if the transition function $P$ is not deterministic, and model fitting is not perfect, the learned representation can still be nearly sufficient for learning as characterized by Theorem~\ref{thm:model_learn_error} below, which is extended from a variant of the value difference bound derived by~\citet{gelada2019deepmdp}. 
Proposition~\ref{prop:nonlin_suf} and Theorem~\ref{thm:model_learn_error} justify that learning the latent dynamics model as an auxiliary task encourages the representation to be sufficient for learning. The model error $L_P$ and $L_R$ defined in Section~\ref{sec:algo} can indicate how good the representation is.
% shows a general guarantee for learning with model-regularized representation.
% that learning policy-independent dynamics models benefits representation learning.
\begin{theorem}
\label{thm:model_learn_error}
For an MDP $\mdp$, if representation mapping $\phi$ is $(\epsilon_P,\epsilon_R)$-sufficient for the dynamics of $\mdp$,
% satisfies
% $\max_{o\in\states,a\in\actions}|R(o,a) - \hat{R}_a(\phi(o)) | \leq \epsilon_R$ and
% $\max_{o\in\states,a\in\actions} \| \mathbb{E}_{o^\prime\sim P(\cdot|o,a)} [\phi(o^\prime)] - \hat{P}_a (\phi(o)) \|_2 \leq \epsilon_P$ for dynamics models $(\apa,\ara)_{a\in\actions}$, 
then approximate policy iteration with approximation operator $\apx$ starting from any initial policy $\pi_0\in\reppolicy$ satisfies 
% \setlength\abovedisplayskip{-2pt}
% \setlength\belowdisplayskip{2pt}
\vspace{-0.5em}
\begin{equation}
    \limsup_{k\to\infty} \|Q^* - Q^{\pi_k} \|_{\infty} \leq \frac{2\gamma^2}{(1-\gamma)^3}(\epsilon_R+\gamma\epsilon_P\lipsvalue).
\end{equation}
where $\lipsvalue$ is an upper bound of the value Lipschitz constant as defined in Appendix~\ref{app:repre}.
% where $\kappa_P$ and $\kappa_R$ are largest Lipschitz constants of $\apa$ and $\ara$ as defined in Appendix~\ref{proof:model_learn_error}.
\end{theorem}
% Theorem~\ref{thm:model_learn_error} is extended from a variant of the value difference bound derived by~\citet{gelada2019deepmdp}. Proposition~\ref{prop:nonlin_suf} and Theorem~\ref{thm:model_learn_error} both justify the effectiveness of learning dynamics models in Algorithm~\ref{alg:source} and \ref{alg:target}. Next, we discuss the effectiveness of our proposed method for knowledge transfer.

\textbf{Transferring Model to Get Better Representation in Target.} 
Although Proposition~\ref{prop:nonlin_suf} shows that learning auxiliary dynamics models benefits representation learning, finding the optimal solution is non-trivial since one still has to learn $\hat{P}$ and $\hat{R}$. Therefore, the main idea of our algorithm is to transfer the dynamics models $\hat{P},\hat{R}$ from the source task to the target task, to ease the learning in the target task. Theorem~\ref{thm:transfer_suf} below guarantees that transferring the dynamics models is feasible. Our experimental result in Section~\ref{sec:exp} verifies that learning with transferred and fixed dynamics models outperforms learning with randomly initialized dynamics models.
\begin{theorem}[Transferable Dynamics Models]
\label{thm:transfer_suf}
Consider a source task $\mdp\source$ and a target task $\mdp\target$ with deterministic transition functions. 
Suppose $\phi\source$ is sufficient for $(\pact\source, \ract\source)_{a\in\actions}$ with functions $\apa, \ara$, then there exists a representation $\phi\target$ satisfying
$\apa(\phi(o))=\mathbb{E}_{o^\prime\sim \pact\target(o)}[\phi(o^\prime)]$, $\ara(\phi(o)) = \ract\target(o)$, for all $o\in\states\target$, and $\phi\target$ is sufficient for learning in $\mdp\target$.
% $\apa(\phi(o_i))=\pact\target[i]\rep\target$, $\ara(\phi(o_i)) = \ract\target[i]$, $\forall o_i\in\states\target$.
% $\rep\target \apa = \pact\target \rep$ and $\rep\target \ara = \ract\target$.
\end{theorem}
Theorem~\ref{thm:transfer_suf} shows that the learned latent dynamics models $\hat{P},\hat{R}$ are transferable from the source task to the target task. 
For simplicity, Theorem~\ref{thm:transfer_suf} focuses on exact sufficiency as in Proposition~\ref{prop:nonlin_suf}, but it can be easily extended to $\epsilon$-sufficiency if combined with Theorem~\ref{thm:model_learn_error}.
Proofs for Proposition~\ref{prop:nonlin_suf}, Theorem~\ref{thm:model_learn_error} and Theorem~\ref{thm:transfer_suf} are all provided in Appendix~\ref{app:proofs}.
% The last lemma should be a theorem? I feel some of those lemmas above could be theorems as well, not sure, it may be distracting to have too many theorems.

% \textbf{Theory-inspired Algorithm Design.}
% It is commonly believed that a good representation should linearly represent the policy values~\citep{barreto2016successor,bellemare2019geometric,dabney2020the}. That is, the policy/value head should be a single linear layer. However, as we discussed in Section~\ref{sec:representation}, being linearly sufficient for learning requires the representation to be sufficient for exponentially many policies, which is expensive to compute and hard to transfer. Instead, we encourage the representation to be sufficient for predicting the next state representation and the reward, which is relatively easy to achieve and transfer. As a result, the regularized representation is sufficient but not linearly sufficient for learning. Therefore, we suggest using a \textit{non-linear policy/value head} following the representation.
% In Section~\ref{sec:theory}, we elaborate the relationship between model sufficiency and learning sufficiency of representations, where we show a trade-off between model complexity and approximation complexity. 


\textbf{Trade-off between Approximation Complexity and Representation Complexity.}
% \textbf{Why Non-linear Policy Head}\quad
% As discussed above, fitting policy-independent dynamics encourages the representation to be sufficient for learning, though not linearly sufficient. 
As suggested by Proposition~\ref{prop:nonlin_suf}, fitting policy-independent dynamics encourages the representation to be sufficient for learning, but not necessarily linearly sufficient. Therefore, we suggest using a \textit{non-linear policy/value head} following the representation to reduce the approximation error. 
Linear sufficiency can be achieved if $\phi$ is made linearly sufficient for $\ppi$ and $\rpi$ for all $\pi\in\reppolicy$, where $\ppi$ and $\rpi$ are transition and reward functions induced by policy $\pi$ (Proposition~\ref{prop:lin_suf}, Appendix~\ref{app:repre}).
% linear sufficiency for learning can be resulted from 
% Is it possible to learn linearly sufficient representations via auxiliary tasks? 
% A direct method is to fit $Q^\pi$ linearly for many representative $\pi$'s, as proposed by some prior works~\citep{bellemare2019geometric,dabney2020the}, but this \textcolor{red}{auxiliary task} is not transferable across observation spaces \fh{why?}, as policies have different input spaces between the source and the target tasks. \fh{I feel the auxiliary task is not very clearly explained. You mean the auxiliary task is based on the input space? What do you mean by auxiliary task? }
% In order to transfer knowledge, we should learn models in the latent representation space. Thus \fh{Why using thus here? the following doesn't seem to use latent space}, another method is to fit policy-dependent dynamics $\ppi$ and $\rpi$ for all $\pi$. As we discuss in Appendix~\ref{app:repre}, a linearly sufficient representation could be found with this auxiliary task. 
However, using this method for transfer learning is expensive in terms of both computation and memory, as it requires to learn $\ppi$ and $\rpi$ for many different $\pi$'s and store these models for transferring to the target task.
% \ac{what is meant by a "large number of latent dynamics models?"}
% Another way to achieve linear sufficiency is to directly fit $Q^\pi$ linearly for many representative $\pi$'s~\citep{bellemare2019geometric,dabney2020the}, but it is not obvious how to apply these methods to the transfer problem studied in this work.
% \fh{Where did you mention latent? Sorry I am quite confused about the writing of this paragraph}.
Therefore, there is a trade-off between approximation complexity and representation complexity. Learning a linearly sufficient representation reduces the complexity of the approximation operator. But it requires more complexity in the representation itself as it has to satisfy much more constraints. 
To develop a practical and efficient transfer method, we use a slightly more complex approximation operator (non-linear policy head) while keeping the auxiliary task simple and easy to transfer across tasks. Please see Appendix~\ref{app:repre} for more detailed discussion about linear sufficiency.

