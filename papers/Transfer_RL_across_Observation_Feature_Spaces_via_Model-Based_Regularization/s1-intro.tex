\section{Introduction}
\label{sec:intro}

% \ys{
% 1. motivate the transfer RL problem (1 sentence)
% 2. the need of transferring across observation representations, motivation example with figure
% 3. existing methods use inter-task mappings, prior knowledge about the target task -- our goal is to transfer without them
% 4. challenges (change could be drastic, and unknown to the source task)
% 5. introduce the idea: disentanglement and model-based regularization
% 6. contributions: identify the novel transfer problem; formulate the representation, relate model and the representation, theory-guided algorithm. experimental results.
% }
Deep Reinforcement Learning (DRL) has the potential to be used in many large-scale applications such as robotics, gaming and automotive.
In these real-life scenarios, it is an essential ability for agents to utilize the knowledge learned in past tasks to facilitate learning in unseen tasks, which is known as Transfer RL (TRL).
Most existing TRL works~\citep{taylor2009transfer,zhu2020transfer} focus on tasks with the same state-action space but different dynamics/reward.
% such that the source policy can adapt to the target task by learning both task-specific and task-agnostic skills.
% \fh{what do we mean to keep dynamics?} 
However, these approaches do not apply to the case where the observation space changes significantly. 
% without knowing a mapping between old observations and new observations. 
% while the dynamics remain similar. 
% which is often the case as humans modify how the agent observes the world (e.g., upgrading a sensor).

% motivation: why transfer rl
% Deep Reinforcement Learning (DRL) has the potential to be used in many large-scale applications which are under rapid evolution and development, such as robotics, gaming and automotive.
% Concretely, a rapidly evolving environment may be subject to significant changes in the transition dynamics, reward function, and observation space.
% Therefore, in order for DRL to be a viable solution, algorithms must be able to efficiently cope with evolving environment configurations. Transfer learning promises to address this 
% by providing efficient means of transferring knowledge from a learned task (\textit{source task}) to a related, new task (\textit{target task}).

%Therefore, it is crucial to improve the efficiency of training DRL agents by transferring knowledge from a learned task (\textit{source task}) to a related new task (\textit{target task}).  
% These real-life systems are usually under rapid evolution, but once the environment changes, the previous well-trained policy often does not work, then it usually takes days if not weeks to retrain a new policy in the new environment. 
% Therefore, in applications where the agent needs to frequently adapt to new changes in environment, the inefficiency of repeated policy retraining becomes a bottleneck in development.
% motivation: why transfer across observations
%  due to hardware upgrading, incremental development or curriculum design
Observation change is common in practice as in the following scenarios.
(1) Incremental environment development. RL is used to train non-player characters (NPC) in games~\citep{juliani2018unity}, which may be frequently updated. 
% However, the development of a game is incremental \fh{What do you mean by `but' development is incremental? maybe delete this sentence}.
When there are new scenes, characters, or obstacles added to the game, the agent's observation space will change accordingly.
(2) Hardware upgrade/replacement. For robots with sensory observations~\citep{bohez2017sensor}, the observation space could change (e.g. from text to audio, from lidar to camera) as the sensor changes.
(3) Restricted data access. In some RL applications~\citep{ganesh2019reinforcement}, agent observation contains sensitive data (e.g. inventory) which may become unavailable in the future due to data restrictions. 
In these cases, the learner may have to discard the old policy and train a new policy from scratch, as the policy has a significantly different input space, even though the underlying dynamics are similar. But training an RL policy from scratch can be expensive and unstable. Therefore, there is a crucial need for a technique that transfers knowledge across tasks with similar dynamics but different observation spaces.

% For example, the absolute positions of objects in a source task may be replaced by the relative positions between the agent and the objects in a target task.
% The change may even be drastic, as shown in a motivating example in Figure~\ref{fig:example}, the vector-based observation (the x-y coordinates of the agent and the goal locations) in a source task (e.g., in a simulator) may be changed to a much higher dimensional image-based observation (a top-down view of the whole maze) in a target-task (e.g., in a real maze environment). 
% For example, a previously observable property of the environment may be replaced by other quantities \fh{this sentence is a bit unclear}. 
% In the above cases, existing methods may have to discard the old policy and train a new policy from scratch which is inefficient in both time and sample complexity, even though the underlying dynamics are similar. 
Besides these existing common applications, there are more benefits of across-observation transfer. For example, observations in real-world environments are usually rich and redundant, so that directly learning a policy is hard and expensive. If we can transfer knowledge from low-dimensional and informative vector observations (usually available in a simulator) to richer observations, the learning efficiency can be significantly improved. Therefore, an effective transfer learning method enables many novel and interesting applications, such as curriculum learning via observation design.
% Another possible application of transfer learning across observation spaces is the adaptation from simulators to real-world environments. 
% In a simulator, the agent may have access to the ground-truth data (e.g. positions of objects), while in the real world, the agent is only given rich and redundant observations (e.g. images). Directly learning from image inputs is challenging, but the learning efficiency will be greatly improved if the agent is able to automatically transfer knowledge from vector observations to image observations. \fh{Why is this a separate paragraph rather than bullet point 4 of the paragraph above?}

\begin{figure}[!htbp]
\centering
    \includegraphics[width=0.9\textwidth]{figs/example.pdf}
\vspace{-2em}
\caption{\small{An example of the transfer problem with changed observation space.
The source-task agent observes the x-y coordinates of itself and the goal, while the target-task agent observes a top-down view/image of the whole maze. The two observation spaces are drastically different, but the two tasks are structurally similar. Our goal is to transfer knowledge from the source task to accelerate learning in the target task, without knowing or learning any inter-task mapping.% \ac{should we be higher level about the goal i.e. transfer knowledge from the source task to speed learning in the target task?}
}}
\label{fig:example}
\vspace{-1em}
\end{figure}

% The case where the observation space changes is rarely considered, or is considered under relatively strong assumptions, e.g. an inter-task mapping between two observation spaces is available.


% For example, if one has already trained a navigating robot, but later on the observing sensor is upgraded, which results in a different observation format. When the change is drastic, one usually needs to train a new deep RL policy from scratch. 
% However, in the above example of navigating robot, we note that the underlying physics (dynamics) of the environment does not change. 

% As a result, when the environment changes, one usually has to retrain a new policy from scratch.
% Environment changes includes observation changing, e.g., enlarging the size of observed images, action changing, e.g., adding a new action, and dynamics changing, e.g., altering the friction of the floor. 


In this paper, we aim to fill the gap and propose a new algorithm that can automatically transfer knowledge from the old environment to facilitate learning in a new environment with a (drastically) different observation space.
In order to meet more practical needs, we focus on the challenging setting where the observation change is: 
\textbf{(1) unpredictable} (there is no prior knowledge about how the observations change), 
\textbf{(2) drastic} (the source and target tasks have significantly different observation feature spaces, e.g., vector to image), and
\textbf{(3) irretrievable} (once the change happens, it is impossible to query the source task, so that the agent can not interact with both environments simultaneously).
Note that different from many prior works ~\citep{taylor2007transfer,mann2013directed}, we do not assume the knowledge of any inter-task mapping. That is, the agent does not know which new observation feature is corresponding the which old observation feature.
% or are designed for the addition or removal of individual observation features~\citep{raiman2019neural}.
% 

% \ac{capitals?}\\
% \textbf{(1) Unpredictable}: there is no prior knowledge about how the observations change. Many works~\citep{taylor2007transfer,mann2013directed} require an inter-task mapping as prior knowledge, but such a mapping might not be available or costly in practice;\\
% \textbf{(2) Drastic}: the source and target tasks have significantly different observation feature spaces. \citet{raiman2019neural} propose to selectively adjust the network weights when there are additional input features, which does not work when the change is drastic (e.g. vector to image);\\
% \textbf{(3) Irretrievable}: once the change happens, it is impossible to query the source task, so that the agent can not interact with both environments simultaneously. 
% \citet{gupta2017learning} allow two agents to simultaneously learn two tasks with an invariant feature space, but

% existing works
% There are some existing methods tackling the changes in the observation space. However, many works~\citep{taylor2007transfer,mann2013directed} require an inter-task mapping as the prior knowledge. 
% \citet{raiman2019neural} propose to selectively adjust the network weights when there are additional input features, which does not work when the change is drastic (e.g. vector to image).
% Domain adaptation~\citep{higgins2017darla} also considers the observation change in environment, but mainly focuses on the change of observation distribution (e.g. the target observation has more noise than the source observation), instead of the observation features or observation dimensionality. 

% \ys{explain: disentangle representation learning from policy/value learning}
% In this paper, we propose an algorithm that can transfer a learned policy to an unseen task when the observation space changes while the underlying dynamics stay the same.

% \ys{move to the 2nd paragraph}
% A motivating example and the main idea of this work are depicted in Figure~\ref{fig:example}, where two agents are in the same location while observing different inputs. In this case, the source task and the target task are different only in how the states are observed and represented by the agent. 
To remedy the above challenges and achieve knowledge transfer, we make a key observation that, if only the observation features change, the source and target tasks share the same latent space and dynamics
% \ac{italicize?}
(e.g. in Figure~\ref{fig:example}, $\states\source$ and $\states\target$ can be associated to the same latent state).
Therefore, we first disentangle representation learning from policy learning, and then accelerate the target-task agent by regularizing the representation learning process with the latent dynamics model learned in the source task. We show by theoretical analysis and empirical evaluation that the target task can be learned more efficiently with our proposed transfer learning method than from scratch.
% Representation learning aims to learn a ``good'' encoder that benefits the learning of policy/value, while policy learning aims to learn a policy based on the representation generated by the encoder. 
% In DRL, the representation and the policy are jointly learned, but the quality of the representation may be poor~\citep{dabney2020the}, which makes it hard for agent to find an optimal policy.

% In this paper, we first formally define ``what is a good representation'' and analyze the sufficient conditions for a representation being good. 
% Then, based on theoretical insights, we propose a novel algorithm that automatically transfers knowledge across observation representations with an observation-independent dynamics model. 
% More specifically, we learn an auxiliary dynamics model in the source task, and transfer the dynamics model to the target task to regularize the learned representation, which improves the learning efficiency and stability.

\textbf{Summary of Contributions.} 
(1) To the best of our knowledge, we are the first to discuss the transfer problem where the source and target tasks have drastically different observation feature spaces, and there is no prior knowledge of an inter-task mapping.
(2) We theoretically characterize what constitutes a ``good representation'' and analyze the sufficient conditions the representation should satisfy.
(3) Theoretical analysis shows that a model-based regularizer enables efficient representation learning in the target task. Based on this, we propose a novel algorithm that automatically transfers knowledge across observation representations.
(4) Experiments in 7 environments show that our proposed algorithm significantly improves the learning performance of RL agents in the target task.
% \ys{(1) identify (2) theory (3) algorithm (4) results}
% (1) We propose an algorithm that transfers knowledge across observation spaces. To the best of our knowledge, we are the first to identify and propose a solution to this transfer problem where there is no prior knowledge about the target observation space, and the target observation space is drastically different from the source.
% (2) We theoretically characterize the sufficient conditions for a representation function to be good for learning an RL task, and associate the representation quality with model-based regularization.
% (3) Empirical study in a wide range of environments shows that our proposed algorithm effectively transfers knowledge from the source to the target, and significantly improves the learning performance of basic RL agents in the target task.

