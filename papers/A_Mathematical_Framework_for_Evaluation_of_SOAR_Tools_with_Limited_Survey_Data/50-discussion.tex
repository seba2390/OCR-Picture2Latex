\section{Discussion}
\vspace{-4mm}
In this work, we (1) provide a summary of features operators value in a SOAR tool, and (2) develop a method for down-selection when survey data is limited. Our multi-faceted approach for downselection is specifically applicable here, for progressing a subset of tools for secondary testing. This approach was centered around analysis of survey results collected from SOC operators after they viewed demonstration and overview videos on the SOAR tools. The goal of the survey was to limit thorough testing to only tools that contained many of the features that SOC operators require. 

Prior to data collection, we ran simulations to obtain an estimate for how much data and how many participants we needed to have statistically valid comparisons. Because not every operator rated every tool, machine learning was used to fill in the missing data and generate a fully populated dataset. Care was taken to address possible demographic impact, such as operator occupation and years of experience. 

We note that the two most important features operators are looking for in a SOAR tool are (1) its ability to automate common tasks and (2) functionality of playbooks. More than half of the participants voted that playbooks were the most important aspect of a SOAR tool, followed by task automation, ticketing, and ranking of alerts in a clear second, third, and fourth order, respectively. This survey, even with its limitations, provides a starting point for SOAR tool vendors to focus their efforts on aspects that are most beneficial to SOC function.

Another survey aspect that affected our analysis of operator preferences were the discrepancies between how an operator rated tools vs ranked tools. For example, in many instances operators would rate Tool 0 at a 5/5 and Tool 1 at a 3/5, but then would rank Tool 1 better than Tool 0. Due to the numerous discrepancies between ranking and rating we implemented the PageRank analysis algorithm to develop a scoring metric for the ratings. This algorithm allowed us to predict which node is the most preferred even with the user discrepancies.

Because participants viewed videos created by SOAR tool vendors rather than using the tools directly, it is possible that their opinions were influenced by either video quality or prior knowledge of a particular tool.
We asked participants to provide their opinion of video quality and their prior experience with each tool in our survey, and we discuss correlations between these factors and our results in Section~\ref{correlation}. 
It is also possible that their opinion of the tool would change if they had the opportunity to use it in an operational context, which is the next phase of our research. However, the largest limitation to our study was the number of participants. We recognize that in an ideal world we would have had more participants, however given the information we found about how many participants would provide what confidence, our sponsor determined they were satisfied with nineteen for this phase of down-selecting. 

Based on this analysis, we are equipped to identify a subset of these 11 tools that will be thoroughly tested. Operators will use these tools in several realistic scenarios and will be asked to complete a survey after regarding their experience. Furthermore specific performance metrics will be collected to measure the improvement to efficiency and effectiveness by usage of SOAR tools in a SOC. While this framework was developed in the context of SOC survey responses and downselecting a large sample, the impacts of this study are far-reaching. In general the framework we present provides designers of user studies with the ability to quantify the statistical power of their analyses based on their sample size, and furthermore provides a reliable method of populating missing user data. If there are multiple methods of evaluation, such as scores and ranking, we demonstrate a novel method of reconciling differences between the two for a more clear interpretation and meaningful results.
