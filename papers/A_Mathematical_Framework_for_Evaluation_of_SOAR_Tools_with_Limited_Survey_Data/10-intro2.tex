\section{Introduction and Background}
\vspace{-4mm}
\label{sec:intro} 
The term \textit{security operations center} (SOC) refers to the subteam of an organization's IT department tasked with maintaining the network's cyber health---the confidentiality, integrity, and availability of the enterprise's data and systems.
SOCs are now equipped with a widespread collection of data from a vast array of sensors feeding logs, alerts, and raw data to a security information and event management system (SIEM). 
SIEMs and back-end infrastructure generally provide SOCs customizable, real-time dashboards and rapid data query. 
However, the process of identifying incidents, responding appropriately, and documenting findings remains to be done by the operators. The SIEM, along with the many other SOC tools, can only provide limited awareness, leading to extended threat detection and response times and reduced ability to prevent or quickly mitigate an attack \citep{Network, islam_babar_nepal_2019}. 

% \todo{The Introduction section should present to the reader the organization of the rest of the paper}
%\todo{Does similar previous work exist in the bibliography? Is the cybersecurity industry currently adopting evaluation methodologies and/or related user acceptance assessment procedures?. If this is the case, it would be important to emphasise the proposal's strengths against these. }
% \todo{It would also be interesting to know whether previous studies perceive similar patterns of acceptance from the end-user perspective.}
%\todo{It is possible to estimate some kind of decision bias in the review board? Note that close organizations and experts tend to share a similar vision on their operational needs). }
%\todo{The study focuses on user acceptance. It would be interesting to combine such a view with objective technical parameters, such as performance tests, efficiency, fault tolerance, resilience, etc. This could go from reinforcing the quality of the assessment of the solutions analysed, to highlighting points for improvement in the study process (for example, to what extent has the user been "convinced" by the video demonstration of the virtues of each solution, or which operational aspects are most valued with respect to the technical capabilities that enable them?}
%\todo{its strengths and weaknesses are barely discussed in relation to previous work, which gives rise to the following suggestions.}
%\todo{The specific contributions of this research should be better reflected in the introductory section.}
% \todo{It is not completely clear the rationale behind the selection of demographic attributes and the impact they had in the overall assessment. At a glance, and based on the questionnaire presented, only three were considered (Questions 1 to 3). Question 4 is rather an instruction by the way.}
% \todo{ Although the authors claim no similar SOAR assessment methodology is found in the literature, authors are encouraged to cite evaluation approaches outside thorough cyber tools testing, such as those grounded in sentiment analysis or in operator's quality expectations.}
%\todo{the main downside of this research is the lack of a common baseline to profile the quality requirements for the videos provided by the SOAR vendors. Authors are encouraged to extend more on this.}


% This leaves them with a tedious, manual effort often comprised of ill-defined workflows, navigating disparate systems and a sea of data, and is proving vastly inefficient. %Because SOAR tools must interact with each SOC's unique data feeds and systems and be configured to systematize and automate SOC-specific procedures, the adoption of a SOAR tool is costly in the training of operators, monetary cost, and technological debt.

% It is no surprise that improving efficiency largely goes hand-in-hand with subsidizing human operator duties with machine-based tasks.

% We look for SOAR tools to help improve SOC efficiency by mitigating some of the costs of a false positive. The false positive rates that SOC analysts have to manage in a typical SIEM environment are extraordinarily high and impose a heavy burden on operators to identify which are credible threats. 
% Because SOAR tools are marketed as doing so many distinct things, direct comparison of these commercially available tools is highly nontrivial. Such a study requires SOC operators to use them in a hands-on fashion. This is a complicated scientific effort that requires the establishment of a high-fidelity testing environment with metrics to gauge gains in efficiency and effectiveness. Additionally, sufficient time must be allocated to the SOC operators for both familiarization with the tool and the completion of a rigorous evaluation of the tools.
Further, SOC efficiency suffers due to time spent documenting findings, collecting evidence across an array of sources, and completing other administrative-type procedures \citep{BREWER20198}. Security orchestration, automation, and response (SOAR) tools---the term coined by Gartner in 2017 \citep{BREWER20198}---are the newest generation of software tools designed to enable SOC operators to more efficiently and uniformly detect and address cybersecurity threats. 
% These tools are a joint application of both security orchestration and automation. 
While orchestration and automation are often marketed together, they do have distinct functions and can be distinguished \citep{islam_babar_nepal_2019} as follows: \textit{orchestration} specifically refers to the integration of separate tools with different functions into a single platform to streamline and accelerate the investigation of a threat; \textit{automation} refers to reducing the manual effort required by SOC operators during investigation and threat response phase \citep{Microsoft}. 
% Typically, automation will follow as a result of orchestration \citep{islam_babar_nepal_2019}.
SOAR tools aim to help by automating parts of an investigation and helping SOC operators prioritize alerts to investigate. 
A SOAR tool has the following defining capabilities: 
(1) ingests a wide variety of SOC data, 
(2) assists in prioritizing, organizing, and displaying the data to the users, 
(3) allows customizable workflows or ``playbooks'' to standardize SOC procedures, 
(4) provides automation to expedite the SOC's procedures, 
(5) integrates with a ticketing system, and 
(6) facilitates collaboration of operators, potentially in disparate geographic locations or networks. 
%\citep{gartner_inc}. 
% In some cases, the gap between SIEM environments and SOAR tools is more a matter of automation. For example, an AI-based SIEM that uses deep learning to dwindle the vast pool of would-be alerts into a more manageable set of alerts most likely to be true positives has been proposed \citep{AI_SOAR}. The ``threat intelligence'' capacity stores context about each incoming threat and whether it was deemed to be true/false positive in a repository that can be queried. 
% If the threat is determined to be true, responses are executed automatically via playbook instructions \citep{AI_SOAR}.
The capabilities of a SOAR tool are observed with the integration of a SOAR mechanism to secure energy microgrids---wherein the tool collects and contextualizes security data from multiple sources in the microgrid, performs an overall sweep of the system to identify present vulnerabilities, and initialize or engage a response to detected threats \citep{microgrid}.

SOAR tools are a transformational and centerpiece tool for a SOC promising measurable gains in effectiveness and efficiency; 
as such, they require substantial investments. 
In addition to monetary costs, configuration depends on standardizing and codifying workflows, and incurring technological and organizational debt. 
Hence, efforts to find the right SOAR tool for a SOC are warranted, yet because SOAR tools are so new, there is little research assessing their usability or the degree to which they improve SOC operators' ability to respond to threats. 
This work will provide an in-progress report on the first-ever user study to assess and compare user preferences of SOAR tools with the aim of downselecting to a smaller number of tools to be extensively tested. % usability and SOC user preferences of SOAR tools.
Our goal is to provide scientific assistance to an organization to winnow the wide variety of SOAR tools to the best one for the organization's needs. 
%In Vast et al. (2021), the authors propose an AI-based SIEM that uses deep learning to dwindle the vast pool of would-be alerts into a more manageable set of alerts most likely to be true positives. 

% \subsubsection{Our contributions.}
A first step for evaluating SOAR tools, and the focus of this work, is down-sampling the variety of tools to a subset worthy of the costly hands-on evaluation without discarding a potentially desired tool. 
% The focus of this work is to describe our mathematically-driven approach to equipping the sponsor organization with data needed to eliminating SOAR tools for hands-on evaluation.
We describe both our experimental design methodology and anonymized results---as a prerequisite, SOAR vendors agreed to providing licenses and support for this study in exchange for anonymity of their results and participation---of actual SOC operators evaluating 11 market-leading SOAR tools based on vendor-supplied technological overviews. Previous work has used game theory to aid in decisions of mid-size enterprises on cybersecurity tools \cite{decision_support}, or conducted user studies questioning users why they do (not) employ specific cybersecurity measures \cite{use_non_use}. Yet none of this work addresses how users perceive current market leading tools that are available to them.

% While we see other work that uses strictly theory for small to medium enterprises to make decisions on cybersecurity tools (not like, real evaluations of tools, just game theory stuff) \cite{decision_support}, as well as work that asks users why they do or do not employ specific cybersecurity measures \cite{use_non_use}, we have yet to see (1) what users want in SOAR tools (2) how they perceive current technology to perform in those areas and (3) what our continuation of this work will do, which is combine the objective performance of SOAR tools with user perception to make a specific recommendation.


% This describes the ``first half" of a larger, in-progress study that will culminate in the aforementioned hands-on tests of the fewer selected tools. 
% Our approach addresses each design concern mathematically with a goal of equipping the sponsor organization to make data-driven decisions with multiple views of the data. 
% Our contributions can be dichotomized into specific findings relevant to SOAR tools and reusable, novel data-analytic approaches to design questions and results analysis. 

In terms of understanding usability of SOAR tools, this work aids in understanding what defining capabilities are most desired by SOC operators and how this affects their preferences. 
Our unique pipeline provides a method for narrowing down to only the tools that are most preferred by SOC operators. 
This pipeline's many unique and modular components could be applicable to future user studies, in particular: 
statistical simulations to quantify confidence of results under varying number of participants;
a novel algorithm for optimizing participant assignments; 
a framework for optimizing combinations and variants of multi-criteria recommender system (for predicting missing ratings) to the data at hand that outperforms the previous versions; 
an evaluation of supervised regressors for predicting overall ratings from aspect ratings is provided (and verifies that this is a needed step as it greatly out-performs predictions from the recommender system); 
many methods for ranking the tools based on the data are provided side-by-side including a novel application of PageRank to convert ratings data to ranking data; 
statistical analyses that investigate correlations present. 
To assist the community in reusing any of these methods, code will be released \href{https://github.com/noremsa/SurveyAnalysisFramework}{here} once open-source copyright is gained. 

In summation, this work will present a novel pipeline of ingesting sparse data and using a mathematically sound method to fully populate the matrix of data. Novel methods are then applied to create multiple views of data along with the statistical relevance of demographic factors. This paper will fill a gap in knowledge where we start with a survey of what the users of these tools actually want, and will proceed to an in depth user study of what the tools deliver. We first present our methods, beginning with study design and progressing through our algorithms and machine learning techniques for handling missing data and predicting overall ratings. We then detail our use of PageRank for rank aggregation, along with statistical methods to analyze demographic impact on user responses. 
%Thus, this work fills a gap where we start with a survey of what the users of these tools actually want, and will proceed to an in depth user study of what the tools deliver.
% In our experimental design, we address questions of how many operators, how to assign SOAR tool reviews to operators, how to address missing ratings, and how to leverage all received data in order to rank the 11 tools for next-step testing.
% However, because of constraints on operator availability, we assigned each operator only a subset for evaluation. As such, not every operator will complete the survey for every tool. 
% Thus, to confidently downselect the tools, we utilize multicriteria recommender systems to predict how users would rate aspects of tools they did not review, based on how similarly they reviewed tools that other operators provided feedback for. Next, we used machine learning to predict each SOC operators overall rating of each tool. Finally, we used Page Rank algorithm to find an overall ranking of the tools.
% More specifically, while realistic test environments were implemented for a hands-on study, 
% we established a systematic method to downselect from a wide range of SOAR tools to the most preferred few was developed to ensure resources in future testing focus on the tools which best met the SOC operators' needs.
% Job responsibilities of operators are dependent on SOC and the tier of the operator. As a tier 1 analyst, tasks might include reviewing blacklisted IP communications and daily reports, responding and triaging alerts from security information and event management (SIEM) tools, and responding to phishing emails. A tier 2 analyst might be tasked with analyzing emerging threat reports, developing security dashboards, and responding to incidents. Some of the metrics used for SOC analysts could include remediation of vulnerabilities, emails investigated, perimeters defended, and intrusion detection systems monitored. While some of these tasks seem intrinsically human, some of them are ripe for automation with a SOAR tool. 
%SOAR tools function similar to incident tracking tools and enable cybersecurity threats to be efficient, without sacrificing the quality, of responses. For our experiment, we requested that SOAR tools submitting have the following capabilities:
% \begin{enumerate}
% \item ingest a wide variety of logs/alerts
% \item rank and display logs/alerts. This may involve correlating/grouping events.
% \item provide playbooks/workflows (decision trees that guide analyst through the correct set of steps)
% \item automate repetitive tasks, such as steps of a playbook or ticketing actions
% \item integrate with a ticketing system and fill out/append data to tickets
% \item facilitate collaboration across SOC operators, possibly across networks, and both synchronously and asynchronously
% \end{enumerate}
% Based on this information, we determined specific aspects on which operators could evaluate these SOAR tools. 
% Prior to SOAR tool evaluation, we tasked the operators to rank which of these aspects are most important to them, with the anticipation that they would favor tools that performed well on these aspects. 