\section{Results}
\vspace{-4mm}
\subsection{Results from Raw Data}
\vspace{-2mm}
Survey results were collected from four different SOCs, for a total of 19 SOC operators and 158 reviews (10-17 reviews per tool). The survey had two sections: evaluation of what users wanted most out of a SOAR tool (\textit{aspect ratings}), evaluation of how users scored each of their assigned SOAR tools on these aspects, and how they ranked them in order of preference (\textit{tool ratings and rankings).} 
\subsubsection{Tool results}
Each SOC operator watched a tool overview video and a tool demonstration video. Following completion of the video reviews, the operators completed our survey that asked questions about each  aspect of the tool, provided an overall tool ranking (1: most preferred), and then rated (scored) all of the tools (1: lowest score).

% \todo{put this mapping of short names to aspects in the appendix? it didn't actually fix the spacing so maybe put it back in?}
%Ability to prepopulate alert and logging data into tickets (Ticketing); Ability to rank / score alerts for prioritization (Ranking); Ability for multiple analysts to work simultaneously on an investigation (Collaboration); Ability to automate common tasks (Automation); Playbooks or workflows that are easy to create, configure, share, and combine (Playbooks); Ability to ingest custom logging / alert formats (Ingestion)
\begin{wrapfigure}[13]{r}{0.5\textwidth}
    \vspace{-10mm}
    \centering
    \includegraphics[scale = 0.45]{fig/task_ranking.png}
    \captionsetup{font=scriptsize}
    \captionsetup{width=\textwidth}
    \vspace{-10pt}
    \caption{Each aspect's rank of importance from 1 (very important) to 6 (not important)} 
    \vspace{-10pt}
    \label{fig:taskranking}
\end{wrapfigure}
Before investigating individual tools, we asked each operator to rank six aspects, or tasks that a SOAR tool could perform, in order of descending importance. In Figure \ref{fig:taskranking}, we plot how many times each tool was ranked in each position. In this grouped bar chart, we see that playbooks are ranked first by ten operators, indicating that operators prioritize playbooks and workflows that are easy to manage. In second place, most operators voted that automating common tasks was important, followed by pre-populating tickets in third and ranking alerts in fourth.  Fifth and sixth places are less clear, but we can conclude the last two places are reserved for collaboration ability and ingestion of data from various sources.

\subsubsection{Aspect Ratings.}
Before investigating individual tools, we asked each operator to rank six aspects, or tasks that a SOAR tool could perform, in order of descending importance. In Figure \ref{fig:taskranking}, we plot how many times each tool was ranked in each position. In this grouped bar chart, 
we see that playbooks are ranked first by ten operators, indicating that operators prioritize playbooks and workflows that are easy to manage. In second place, most operators voted that automating common tasks was important, followed by pre-populating tickets in third and ranking alerts in fourth.  Fifth and sixth places are less clear, but we can conclude the last two places are reserved for collaboration ability and ingestion of data from various sources.

\begin{wrapfigure}[17]{l}{0.65\textwidth}
    \vspace{-25pt}
    %\centering
    \includegraphics[scale = 0.32]{fig/ToolAspects.png}
    \captionsetup{font = scriptsize}
    \vspace{-10pt}
    \caption{Summary of all of the raw survey scores. Each tool is on the X-axis, and the score is on the Y-axis. The unique colors represent a specific aspect of the survey.} 
    %\vspace{-12pt}
    \label{fig: RawScore}
\end{wrapfigure} 
% \subsubsection{Tool results}
% Each SOC operator watched a tool overview video and a tool demonstration video. Following completion of the video reviews, the operators completed our survey that asked questions about each  aspect of the tool, provided an overall tool ranking (1: most preferred), and then rated (scored) all of the tools (1: lowest score).
% \begin{wrapfigure}[18]{r!}{0.65\textwidth}
%     %\vspace{-20pt}
%     %\centering
%     \includegraphics[scale = 0.32]{fig/ToolAspects.png}
%     \captionsetup{font = scriptsize}
%     %\vspace{-10pt}
%     \caption{Summary of all of the raw survey scores. Each tool is on the X-axis, and the score is on the Y-axis. The unique shapes represent a specific aspect of the survey.} 
%     \vspace{-12pt}
%     \label{fig: RawScore}
% \end{wrapfigure} 

In Figure \ref{fig: RawScore}, we note that Tool 6 is scored the highest on average with a mean rating of 4.36 out of 5. Contrarily, Tool 7 is rated the lowest on average with a mean rating of 3.27.  Operators ranked playbook management as the most important aspect (Figure \ref{fig:taskranking}) and the tools received an average score of 3.56 on this aspect. Tool 9 performed the best on their playbook aspect, and scored a 4.07, while Tool 10 performed the worst.  Operators rank automation as the second most important feature of a SOAR tool, and we found that on average our tools received a rating of 3.75. We again see Tool 9 perform the best, and Tool 7 perform the worst.

\begin{wrapfigure}[13]{r}{0.45\textwidth}
    \vspace{-13mm}
    \centering
    \includegraphics[scale = 0.3]{fig/RawPR.png}
    \captionsetup{font = scriptsize}
    \captionsetup{width=0.8\textwidth}
   % \vspace{-10pt}
    \caption{PageRank graphical network from raw data.}
   % \vspace{-10pt}
    \label{fig:RawPR}
\end{wrapfigure}
We performed a PageRank analysis on the tool rankings to identify the most preferred tool based on the user's rankings alone, given the seemingly inconsistent ratings and rankings of the tools. The directed PageRank graph (Figure \ref{fig:RawPR}) is based on user rankings, and we note that Tool 6 has the most inward weights, which indicates that most users ranked this tool as the best. Analogously, we see Tool 0 has the most outward weights, indicating that most users ranked this tool last.
\vspace{-2mm}
\subsection{Results from Populated Data}
\vspace{-2mm}
The following results are derived from the populated data, or the data for which we have filled in all the missing aspect ratings and used these to predict the missing overall ratings. With the populated data, we have a total of 197 complete tool reviews. 

\subsubsection{Predicted Responses}
 In Figure \ref{fig:toolEx} we highlight how our predicted responses compare the user defined responses in this single example of Tool 0. Analogous 
 plots are available for each tool. In Figure \ref{fig:toolEx} (left), the green region is the user-defined ratings for the six aspects and the overall score of the tool. The red region is the predicted ratings for the overall score and aspects our algorithm  
 \vspace{-10pt}
  \begin{figure}[H]
    \centering
    \includegraphics[scale = 0.45]{fig/Tool0results_anon.png}
    \captionsetup{font = scriptsize}
    \caption{Tool 0 score summary based on user defined results and predicted results. In the left panel, the distribution of the user defined results (green) and the predicted results (red). In the right panel, comparison of the results (green/red dots, respectively) with the average scores across all tools (grey box plot).}
    \label{fig:toolEx}
\end{figure}
\vspace{-8mm}
 \noindent filled in for missing user responses. Importantly, our predicted values fall well within in the middle quartiles, indicating that our predictions are not skewing the overall tool ratings.
 % \todo{do we mean quartiles here? not familiar with quarterlies}
 In Figure \ref{fig:toolEx} (right), we compare the results of all of the tools to the responses for the individual tool. The green/red dots are the user-defined/predicted scores.
 
\vspace{-2mm}
\begin{wraptable}[20]{l}{0.4\textwidth}
\centering
\setlength \extrarowheight{4pt}
\begin{tabular}{l c}
\toprule
{\color[HTML]{000000} \textbf{Model}}  & {\color[HTML]{000000} \textbf{MSE}} \\ \hline
SGD & 0.2733 \\ 
Bayesian Ridge    & 0.2735\\ 
Kernel Ridge      & 0.2736 \\ 
Linear Regression            & 0.2762 \\ 
Support Vector    & 0.3567 \\ 
Random Forest     & 0.3786  \\ 
Gradient Boost    & 0.3887 \\ 
CatBoost          & 0.4375  \\ 
AK Modeling & 0.7379  \\ 
Elastic Net       & 0.7623 \\ 
\bottomrule
\end{tabular}
\captionsetup{font = scriptsize}
%\vspace{10pt}
\caption{Ten machine learning algorithms compared based on Cross Validation Mean Squared Error.}
%\vspace{-10pt}
\label{tab:ml_scores}
\end{wraptable}
%\vspace{5mm}
After having a complete user profile, we used 10 machine learning regression algorithms to predict an overall tool score (Table \ref{tab:ml_scores}). Because Stochastic Gradient Descent Regression had the highest accuracy, we used this algorithm to complete the missing values in the overall scores.

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{fig/Tool0results_anon.png}
%     \caption{Overall score after filling in missing data.}
%     \label{fig:toolEx}
% \end{figure}

%We comparing the results of all the tools, by looking at the median of the overall score.
% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{fig/MLresults_anon.png}
%     \caption{Overall score after filling in missing data.}
%     \label{fig:MLresults}
% \end{figure}
As previously mentioned, some users \textit{rank} tools differently then how they have \textit{rated} them, and we implement PageRank to account for these discrepancies (see fig \ref{fig:MLresults}). As we did on the raw data with missing values, we create the same directed graph for the populated data to identify the most preferred tool using only rankings. Following the pipeline we now have a complete picture of user overall scores of a tool and how that translates to user tool rankings. When calculating the overall rankings with our predicted data, we assume users rank tools based on the overall score they give a tool. If a user gives 2 tools the same overall score we then deflect to the initial tool ranking. As with the raw data, we find Tool 6 to be the 

\begin{wrapfigure}[10]{r}{0.5\textwidth}
    \vspace{-30pt}
    \centering
    \includegraphics[scale = 0.27]{fig/MLPR.png}
    \captionsetup{font = scriptsize}
    \vspace{-10pt}
    \caption{PageRank graphical network from populated data based on ML algorithms.}
    %\vspace{-10pt}
    \label{fig:MLresults}
\end{wrapfigure}
most preferred tool and Tool 7 to be the least preferred tool using the populated data. 
% \vspace{-20pt}

\subsubsection{Statistical Analysis of Demographic Impact}\label{correlation}
In this section we present the results related to user demographics and their correlation to overall tool rating. We selected these demographics--- years of experience, occupation, and tool familiarity ---, along with one factor pertaining to video quality, because these demographics are most likely to influence tool ratings and rankings. First, we note in Figure \ref{fig:exp} that there is no relationship between a tool's rating and the years of experience a user has. Similarly, we find that a user's occupation has no impact on tool rating, with no discernible preference for one tool over another by users of specific occupation. As such, we need not consider the effects of these factors moving forward.

However, we did identify two factors that did correlate to how a tool was rated (Figure \ref{fig:sig}). The first factor we found that influences tool rating is the familiarity a user has with a tool. We found that tools were generally rated higher by users when the user had used the tool before. The second of these was perceived quality of the submitted video. In this case, tools with higher quality videos were subsequently rated higher. It is not necessarily true that a low quality video is related to a tool's performance, and caution must be taken to ensure that the best tools are selected to move to Phase 2. Similarly, we need to account for the fact that if an operator is familiar with a tool then they likely will rate it higher.

\begin{figure}
    \vspace{-15pt}
    \centering
    \includegraphics[scale = 0.4]{fig/exp_occ_anon.png}
    \captionsetup{font = scriptsize}
    %\vspace{-10pt}
    \caption{There is no relationship between operator years of experience or occupation and how a tool was rated. This confirms that there is no decision bias based on experience or occupation, and even operators with similar backgrounds have different visions for SOCs.}
    \label{fig:exp}
\end{figure}

\begin{wrapfigure}[15]{t!}{0.65\textwidth}
    \vspace{-18mm}
    \centering
    \includegraphics[scale = 0.27]{fig/fam_vq2_barplot_anon.png}
    \captionsetup{font = scriptsize}
    %\vspace{15pt}
    \caption{In the left panel we see there is a relationship between overall score and user familiarity. In the right panel we see a relationship between video quality and overall score.}
    %\vspace{-25pt}
    \label{fig:sig}
\end{wrapfigure}

\vspace{-2mm}
\subsection{Overall Results}
\vspace{-2mm}
We have two sets of data, the raw data with missing values and the populated data, and we have two ways of evaluating tool preferences, ratings and rankings. As previously mentioned, there were some discrepancies in how a user rated the tool and how a user ranked the same tool. Here, we present the results of four analyses we implemented to determine tool preferences (Figure \ref{fig: Heatmap} and Table \ref{tab:Overall Scores}). Note that the first two columns are from the raw data, and the last two columns are from the populated data. 

We note that in every analysis, Tool 7 was the bottom performer, and for

\begin{wrapfigure}[17]{l}{0.58\textwidth}
    \vspace{-6mm}
    \centering
    \includegraphics[scale = 0.5]{fig/heatmap_aiatac3_phase1.png}
    \captionsetup{font = scriptsize}
   % \captionsetup{width=0.8\textwidth}
    \vspace{-10pt}
    \caption{Heat map summary of all four methods used to report or derive overall ratings of the tools. Raw score: average of user-defined ratings; PR: average of overall ratings derived from PageRank algorithm on the raw data; ML: average of overall ratings derived from machine learning predictions on populated data; ML + PR: average of overall ratings derived from machine learning and PageRank algorithm on populated data.}
    %\vspace{-10pt}
    \label{fig: Heatmap}
\end{wrapfigure}
\noindent three out of four analyses, we note that Tool 2 is the the second to last performer. Similarly, Tool 6 is the top performer in three out of four cases, and only dropping to a lowest preference of third place in the PageRank analysis on raw data. It is worth noting, too, that for three of the four analyses, we have the same tools in the top three places: Tool 0, Tool 6, and Tool 9. The middle placements shuffle around considerably, but the top performers remain consistent. 
% \begin{table}
% \vspace{-20pt}
% \centering
% \setlength\extrarowheight{3pt}
% \begin{tabular}{m{2.95cm} m{2.75cm} m{2.75cm} m{2.75cm}}
% \toprule
% % >{\columncolor[HTML]{FFFFFF}}c |
% % >{\columncolor[HTML]{FFFFFF}}c |
% % >{\columncolor[HTML]{FFFFFF}}c |
% % >{\columncolor[HTML]{FFFFFF}}c c}
% \textbf{Raw Score} $[1-5]$ & \textbf{PR} $[0-1]$ & \textbf{ML} $[1-5]$ & \textbf{ML + PR} $[0-1]$\\
% %   {\color[HTML]{000000} \textbf{\begin{tabular}[c]{@{}l@{}} PR $[0-1]$ \end{tabular}}}&
% %   {\color[HTML]{000000} \textbf{\begin{tabular}[c]{@{}l@{}}ML $[1-5]$ \end{tabular}}} &
% %   {\color[HTML]{000000} \textbf{\begin{tabular}[c]{@{}l@{}}ML + PR $[0-1]$ \end{tabular}}}
% \hline
% Tool 6 (4.363)  & Tool 0 (.341) & Tool 6 (4.23)  & Tool 6 (.207) \\ 
% Tool 9 (4.071) & Tool 1 (.114)    & Tool 9 (4.06) & Tool 9 (.192) \\ 
% Tool 0 (4.071)  & Tool 6 (.097)  & Tool 0 (4.04) & Tool 0 (.146)  \\ 
% Tool 1 (3.813) & Tool 9 (.085) & Tool 5 (3.77)   & Tool 1 (.070)  \\ 
% Tool 5 (3.750) & Tool 3 (.057)  & Tool 4 (3.75)  & Tool 5 (.061)   \\ 
% Tool 4 (3.636)  & Tool 2 (.052) & Tool 1 (3.75)  & Tool 3 (.055)  \\ 
% Tool 3 (3.571)  & Tool 4 (.048)  & Tool 3 (3.66) & Tool 4 (.054)  \\ 
% Tool 2 (3.438)  & Tool 5 (.043)  & Tool 2 (3.47) & Tool 2 (.046)  \\
% Tool 7 (3.273)  & Tool 7 (.042) & Tool 7 (3.44) & Tool 7 (.046)  \\ 
% \bottomrule
% \end{tabular}
% \captionsetup{font = scriptsize}
% \caption{Four methods used to report or derive overall ratings of the tools. Raw score: average of user-defined ratings; PR: average of overall ratings derived from PageRank algorithm on the raw data; ML: average of overall ratings derived from machine learning predictions on populated data; ML + PR: average of overall ratings derived from machine learning and PageRank algorithm on populated data.}
% \label{tab:Overall Scores}
% \end{table}