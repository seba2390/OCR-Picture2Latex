\section{Methodology}
\vspace{-4mm}
In total, 11 SOAR tools are included in this study.
Before the study, operators were asked to provide an ordered ranking of the defining capabilities (listed in the SOAR definition above) in order of importance. 
Participants watched two vendor-prepared videos, one that gives of their tool and one that provides a demonstration. We provided general guidelines regarding the videos, wherein the overview must include information on the technical approach of the tool (architecture, deployment, algorithms, etc.) as well as the novelty of the tool. We requested that the demonstration video provide an introduction to the platform with examples of users viewing events, using playbooks, collaborating, along with capabilities to automatically populate tickets and orchestrate multiple incidents. Participants then provided per-capability Likert scale ratings (1 to 5), an overall tool rating, and ranked all tools viewed. 
 %Specifically, we defined a SOAR tool as having the following capabilities and evaluate the effectiveness of each:
% \begin{enumerate}
% \item ingest a wide variety of logs/alerts
% \item rank and display logs/alerts. This may involve correlating/grouping events.
% \item provide playbooks/workflows (decision trees that guide analyst through the correct set of steps)
% \item automate repetitive tasks, such as steps of a playbook or ticketing actions
% \item integrate with a ticketing system and fill out/append data to tickets
% \item facilitate collaboration across SOC operators, possibly across networks, and both synchronously and asynchronously
% \end{enumerate}
\vspace{-2mm}
\subsection{Data Collection}
\vspace{-2mm}
\subsubsection{Survey Design.}
\label{sec:survey-design}
Prior to the survey in which the operators evaluated the tools in this study, we collected information from the operators about which defining capabilities of SOAR tools are most important to them. 
Four SOCs participated in this preliminary conversation and informed the questions we asked in our Internal Review Board \footnote{The IRB works to ensure the rights of participants in any human subject study are fully protected} approved full survey.
The capabilities in question included: the ability to rank/sort alerts so that high priority alerts are emphasized, ability to automate common tasks, easy of playbook creation and modification, ability to provide a unified experience across geographic locations, ability to ingest disparate data sources, and ability to pre-populate alerts and tickets with additional context. 
These same aspects were the criteria by which the operators evaluated the SOAR tools. For each question, answers followed the 1-5 Likert scale, with an additional option for ``Can't Tell." 

Before the set of SOAR tool videos and surveys were administered, a set of demographic questions were posed to each participant including how long they had been in their role, what their role was, the level of familiarity they had with each specific SOAR tool, and finally, asking them to rank by importance the defining capabilities of a SOAR tool. 

After viewing all SOAR tool videos and completing the per-tool surveys, users were asked to rank the tools they had reviewed. The survey was provided online via a secured website to protect the sensitive nature of both SOAR tool privacy videos as well as information provided by SOC analysts. 
The full survey can be found in Table \ref{tab:Survey}. 

\subsubsection{How many participants to recruit?} 
% \todo{open source the code that goes with this estimate - not a todo for the actual paper so I'm commenting it out} 
\begin{table}[!b]
 \centering
\resizebox{\textwidth}{!}{%
\setlength\extrarowheight{4pt}
\begin{tabular}{p{1.3cm}lcccccc}
\toprule
\textbf{\#reviews/pair:} & & \textbf{10} & \textbf{15} & \textbf{25} & \textbf{30} & \textbf{35} & \textbf{40} \\ 
\midrule
\multirow{4}*{\rotatebox{90}{Different}} 
 & $z$ test & (0.88, 0.96) & (0.89, 0.99) & (0.95, 1.0) & (0.97, 1.0) & (0.98, 1.0) & (0.98, 1.0) \\
 & $\chi^2$ test & (0.04, 0.23) & (0.06, 0.37) & (0.17, 0.71) & (0.24, 0.81) & (0.29, 0.88) & (0.34, 0.91) \\
 & Bin. test & (0.36, 0.68) & (0.42, 0.8) & (0.57, 0.93) & (0.63, 0.97) & (0.66, 0.97) & (0.71, 0.99) \\
\midrule
\multirow{4}*{\rotatebox{90}{Same}}
& $t$-test & 0.04 & 0.05 & 0.05 & 0.03 & 0.05 & 0.06 \\
& $z$-test & (0.73, 0.82) & (0.79, 0.86) & (0.81, 0.89) & (0.83, 0.89) & (0.86, 0.91) & (0.86, 0.91) \\
& $\chi^2$-test & (0.0, 0.02) & (0.0, 0.01) & (0.01, 0.03) & (0.01, 0.01) & (0.01, 0.02) & (0.02, 0.02) \\
& Bin.-test & (0.1, 0.18) & (0.13, 0.18) & (0.19, 0.19) & (0.2, 0.21) & (0.2, 0.17) & (0.23, 0.21) \\
\bottomrule
\end{tabular}} 
 \captionsetup{font = scriptsize}
\caption{For each simulated number of participants, the table reports the percent of the simulations for which the hypothesis test confirmed the two tool's data were different. For example, a result of 0.67 implies that test provided at least 95\% confidence that the tool 2 rating was different from the tool 1 rating in 67\% of the simulations. For the $z$-, $\chi^2$-, and Binomial tests, Likert values $\{1, \dots, 5\}$ are converted to binary $\{0,1\}$ using thresholds 2.5, then 3.5. Both results are given in the form of (\%\{thresh = 2.5\}, \%\{thresh = 3.5\}). 
Since these tests were testing the null hypothesis that the ratings were sampled from the same distribution, higher (lower) percentages in the top (bottom) half when the distributions were different imply high (low) performance by the test, respectively.}
\label{tab:sim_table}
\end{table}
In general, unless the differences between the groups are rather extreme, small sample sizes do not yield enough statistical power. We begin this study by choosing an appropriate sample size to ensure validity of the results. We design and run a simple (two-tool, single-rating) simulation to quantify the sensitivity of statistical significance to number of participants. 
Although our actual user study will include several tools and ratings, this simple exercise will be sufficient to provide quantifiable reasoning about the number of participants to target. 

For the simulation, we consider two scenarios in which a pair of tools are rated on the  Likert scale: (1) a pair of tools are rated differently (tool 2 preferred to tool 1) and (2) a pair of tools are rated the same (no preference for tool 1 over tool 2, or vice versa).
We sample $m = 5, 10, \dots, 40$ participants' overall ratings of the two tools---one overall rating sampled per user per tool--from two distributions over sample space $\{1, 2, \dots, 5\}$. We then compare results of different hypothesis tests that assess whether the ratings are sampled from the same distribution.

First, we examine the number ($m$) of pairwise tool reviews needed to have confidence that tool 2 is preferred over tool 1 given distribution means of $\mu = 3.65$ and $\mu = 2.93$, and variances of $\sigma^2 = 1.17$ and $\sigma^2 = 0.923$ for tool 2 and tool 1, respectively. 
Second, we examine the number of reviews needed to tell with confidence that tool 2 is preferred equally to tool 1 by using the same distribution ($\mu = 3.65, \sigma^2 = 1.17$) for both. 
In both scenarios, we run the simulation 100 times and for each compute a two-sided $t$ test. We use Welch's $t$ test assuming variances are not equal for the first scenario. 

Next we convert the Likert data to binary data using a threshold of 2.5 such that ($1,2 \mapsto 0; 3,4,5 \mapsto 1$), and with a threshold of 3.5 where ($1,2,3 \mapsto 0; 4,5 \mapsto 1$). 
We compute the $z$-score on difference of means, a $\chi^2$ test with Yates' correction on the difference in proportions of 0 and 1s, and two binomial tests (first hypothesis is tool 2 is sampled from tool 1's distribution, second is vice-versa) for each. 
The statistical tests in this section were designed based on Loveland \cite{Loveland} and applications of Sauro \& Lewis \cite{Sauro_Lewis_2012}. 
 
Our results (Table \ref{tab:sim_table}) confirm that more reviews provide higher, or at least negligibly worse, percentages of correct conclusions in all cases. 
Our results show that the $z$ test and Binomial test under both thresholds are poor at identifying when tools are the same, whereas the $\chi^2$ test with threshold of 2.5 is poor at telling when they are different. 
However, the $t$ test has good performance in both scenarios, as does the $\chi^2$ test with threshold of 3.5, so we use these results for our target number. 
% These results suggest that with at least 30 reviews for each pair of tools, we can confidently identify if the tools are different 76-81\% of the time using this test, and confidently identify when there is no discernible preference 97-99\% of the time.
These estimates are based only on the specific distributions used in the two scenarios (which may not match real-world data we obtain), but are reasonable assumptions to provide a quantitative approach to reasoning how many operators are needed. 

We presented these statistical results to the sponsor organization allowing them to quantifiably reason about the balance between their operator's time and statistical power of the results desired. Note that there are 11 total tools, and thus [11 choose 2] = 55 unique pairs of tools, and secondly that each participant will be asked to rate eight of 11 tools to respect their time, yielding [8 choose 2] = 28 pairwise reviews. Thus, 1,650 desired total desired pairwise reviews divided by 28 pairwise reviews per participant yields a target of 59 participants. After being presented with this information along with Table \ref{tab:sim_table} our sponsoring organization decided that nineteen participants was sufficient. 
% Statistical power, or the ability to detect differences between groups, is dependent on sample size. In general, unless the differences between the groups are rather extreme, small sample sizes do not yield enough statistical power and these analyses are fraught with Type II errors---in our case a Type II error is misinterpreting that two tools are preferred equally by participants. 
% Thus, when beginning a study in which hypothesis testing will be performed, caution must be taken to choose an appropriate sample size to ensure validity of the results. 
% As such, we design and run a simple (two-tool, single-rating) simulation to quantify the sensitivity of statistical significance on participant size. 
% Although our actual user study will include several tools and ratings, this simple exercise will be sufficient to provide quantifiable reasoning about the number of participants to target. 
% For the simulation, we consider two scenarios in which a pair of tools are rated on the ${1, 2, \dots, 5}$ Likert scale: (1) a pair of tools are rated differently (tool 2 preferred to tool 1) and (2) a pair of tools are rated the same (no preference for tool 1 over tool 2, or vice versa).
% We sample $m = 5, 10, \dots, 40$ participants' overall ratings of the two tools---one overall rating sampled per user per tool--from two distributions over sample space $\{1, 2, \dots, 5\}$. We then compare results of hypothesis tests that assess whether the ratings are sampled from the same distribution.
% First, we examine the number ($m$) of pairwise tool reviews, or tool reviews performed by the same ``user'', needed to have confidence that tool 2 is preferred over tool 1 given distribution means of $\mu = 3.65$ and $\mu = 2.93$, and variances of $\sigma^2 = 1.17$ and $\sigma^2 = 0.923$ for tool 2 and tool 1, respectively. 
% Second, we examine the number of reviews needed to tell with confidence that tool 2 is preferred equally to tool 1 by using the same distribution ($\mu = 3.65, \sigma^2 = 1.17$) for both. 
% In both scenarios, we run the simulation 100 times and for each compute a two-sided $t$ test (we use Welch's $t$ test, i.e., assuming variances are not equal, for the first scenario only). 
% Next we convert the Likert data to binary data first using a threshold of 2.5 such that ($1,2 \mapsto 0; 3,4,5 \mapsto 1$), and then with a threshold of 3.5 where ($1,2,3 \mapsto 0; 4,5 \mapsto 1$). 
% We compute the $z$-score on difference of means, a $\chi^2$ test with Yates' correction on the difference in proportions of 0 and 1s, and two binomial tests (first hypothesis is tool 2 is sampled from tool 1's distribution, second is vice-versa) for each. 
% The statistical tests in this section were designed based on rigorous treatments by Loveland \cite{Loveland} and applications of Sauro \& Lewis \cite{Sauro_Lewis_2012}. 
% Our results (Table \ref{tab:sim_table}) overwhelmingly confirm that more reviews provide higher, or at least negligibly worse, percentages of correct conclusions in all cases. 
% Our results show that the $z$ test and Binomial test under both thresholds are poor at identifying when tools are the same, whereas the $\chi^2$ test with threshold of 2.5 is poor at telling when they are different. 
% Fortunately, the $t$-test has good performance in both scenarios, as does the $\chi^2$ test with threshold of 3.5, so we glean our target number of pairwise tool reviews from their results. 
% Given the assumptions, these results suggest that with at least 30 reviews for each pair of tools, we can confidently identify if the tools are different 76-81\% of the time using this test, and confidently identify when there is no discernible preference 97-99\% of the time. 
% Rigorously, this estimate is based only on the specific distributions used in the two scenarios (which may not match real-world data we obtain), yet, these are reasonable assumptions used not to provide perfectly accurate statistical power, but to provide a quantitative approach to reasoning about how many operators to recruit. 
% Finally, we must map the above information to a number of participants. 
% First note that there are 11 total tools, and thus 55 unique pairs of tools. 
% For each pair we target 30 ``pair reviews''---a single participant reviewing both in a pair--- for a desired total 1,650 pair reviews. 
% To respect the time commitments of the participants, we requested that they each rate eight of the 11 tools, though it was encouraged that they rate all 11 should they have the time to do so. 
% Assuming each operator will provide pairwise reviews of 8 tools, we obtain [8 choose 2] = 28 of the total 55 pairs of tools. Thus, 1,650 desired total pairwise reviews divided by 28 pairwise reviews per participant yields a target of \textit{59 participants that should be recruited for this study.}
\subsubsection{How to assign reviews to each participant?} 
Since we only will require 8 of 11 tools to be reviewed by each participant, the problem of which 8 tools to assign to each participant arises because the assignment of tools to reviewers affects how many pair reviews we obtain. 
% As an example, consider if participants A and B both review tools (1,2,3). We obtain 2 pairwise reviews for pairs (1,2), (1,3), (2,3), whereas, if instead reviewer B reviews tools (1,2,4) we have two pairwise review for tool pair (1,2), but only one for pairs (1,3), (1,4), (2,3), (2,4). 
We seek to maximize the number of reviews for every pair of tools given a fixed number of participants. 
Further, the algorithm must accommodate more/less participants than is desired to accommodate optimistic/realistic participation. 

Note that independent of the assignment, for $n$ total tools, $m$ participants, and $l$ reviews per participant there will be an average of $\mu := m \times [l$ choose $2]/[n$ choose $2]$ ratings of each pair of tools. 
Our assignment algorithm seeks to find $m$ different $l$-tuples of tools so that
each pair of tools occurs in as close to $\mu$ of the tuples as possible (each pair is assigned to exactly $\mu$ participants). 
For our case ($m = 59, l = 8, n = 11)$ we seek $\mu = 59 \times 28/55 \sim 30$ reviews for each pair. 
% To revisit the example above, if $n = 4$ tools, $l = 3$ reviews per participant, the assignment (1,2,3), (1,2,4), (2,3,4) provides $\mu = 2$ pair reviews for all [4 choose 2] pairs. Unfortunately, the problem of finding an optimal assignment becomes much more difficult as $m$ and $l$ grow! 
% The problem for $l=3$ but growing $n$ is well studied e.g, see Steiner Triples\footnote{\url{https://en.wikipedia.org/wiki/Steiner_system}} and many related publications, e.g. \cite{Cole_1913}.

For each $l$-tuple, we enumerate the $[l$ choose $2]$ pairs in the $l$-tuple, and for each pair, we track the number of times that tuple has already occurred. 
Initially all counts are 0, and we define the score for the $l-$tuple as the sum of these tallies. 
While the set of assignments is less than $m$, the algorithm sorts all $l-$tuples by score, and picks the next participant's assignment uniformly at random from those $l-$tuples with minimal score, then updates the tallies and scores of all $l$-tuples. 
Once an assignment is given (a list of $m$ total $l$-tuples), we define the assignment error as the average absolute difference of the number of pairwise assignments from $\mu$. 
Although this algorithm may not find an optimal assignment, it is usually close.%; it is sufficiently fast to run multiple times ($\sim 0.1$s/run with a naive implementation on a laptop); and it is stochastic. 
% So it finds different and very-near-optimal assignments some percentage of the time. 
We set $m$ larger than the desired number of users, to optimistically accommodate over-recruitment, and run the algorithm $100$ times (10s), keeping only the best assignment. 
Since the recruitment in practice yielded only 19 participants, we note that because the algorithm is greedy, using the first 19 assignments is also close to optimal. 
In our case of $n = 11, m = 59, l = 8$, we have $\mu = 30.0\overline{36}$, and the resulting assignment produced 6 pairs with 29, 41 pairs with 30, and 8 pairs with 31 reviews for an average error of $\sim0.28$. 
%\subsubsection{Data Collection}
%Due to limited operator availability we used [Bobby's method to find we needed XX operators] ADD MORE CONTENT HERE>
%We developed a survey for rating and ranking the SOAR tools based on the videos that were submitted. Since each user was assigned eight of the 11 tools to evaluate, we did not have responses from every user for every tool, making it difficult to do a fair comparison across all tools. To facilitate this, we evaluated a metric of similarity on both a user level and a tool level, and used this to how a user would rate the aspects of tools they they did not review. Once we calculated the aspect ratings from all users for all tools, we then trained several machine learning models [TABLE \#] on these aspect ratings to predict the overall rating on a scale of 1-5 that a user would assign to a tool they did not review. After determining an overall rating for each tool and user pair, we use that information along with each users initial \emph{ranking} of the tools (users were tasked with ranking the tools they reviewed in order of preference) to create a PageRank [CITATION NEEDED?] that determines a single, overall ranking of how much users liked the tools relative to each other. Lastly, prior to interpretation of results and discussion of which tools would progress to Phase 2, statistical analyses were conducted to evaluate the role that user demographics could play in how they rated the tools. \\

\subsubsection{Sentiment Analysis}
After watching the videos for each tool, participants were asked to complete the free response question, ``Is there anything else about this tool that you would like to share?" 
The problem for analyzing this feedback is it is text, i.e., not numerical. 
In order to convert text feedback into a numerical rating, our algorithm takes the average of two sentiment analyzers, VADER\footnote{\url{https://www.nltk.org/_modules/nltk/sentiment/vader.html}} \cite{hutto2014vader}, which provides a polarity score based on heuristics leveraging sentiWordnet \cite{baccianella2010sentiwordnet}, and roBERTa\footnote{\url{https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment}} \cite{barbieri2020tweeteval}, a sentiment analyzer that uses BERT \cite{devlin2019bert} and is trained on sentiment-labeled tweets to produce a polarity score.
The average of these scores provides a polarity (in $[-1,1]$), which we map to $[1,5]$ Likert scale.
% via an affine map (scale then offset) followed by the ceiling function to map into our desired Likert set $\{1,2,...,5\}$. 
Finally, we included a check for similarity of the VADER and roBERTa scores, manually inspecting if they differed more than .5, which never occurred herein. 
Overall, the free responses are parsed into strings and converted to a Likert value using sentiment analysis, which can be analyzed similar to the other numeric feedback. 
% Notably, to choose the average of VADER and roBERTa, we manually tested a set of sentiment analyzers on a set of tool-feedback comments. 
%\add{@bobby can you add a sentence or two describing what this was used for?}
%\todo{Include some example of text comments paired with scores, i think maybe two or three comments with polarity scores from both algorithms, mean, conversion to 1-5 scale would be a nice table to include. could include in appendix} 
\vspace{-2mm}
\subsection{Predicting Missing Ratings}
\vspace{-2mm}
Since operators recruited for this survey were assigned a subset of the tools to review, this means that we have missing data since every operator did not complete the survey for each tool. 
This section provides a linear progression of previous research that informs our approach followed by our algorithm for filling in missing values to populate a complete dataset. 
Our contributions include providing a simple Bayesian technique for computing similarities dependent on unseen ratings and a method to optimally weight multiple predicted ratings, which, at least on our data, outperforms previous methods. 
Consider a vector, $\vec{r}(u, t, :)$, that comprises the ratings assigned by user $u$ to tool $t$; i.e., $\{\vec{r}(u,t,i)\}_{u,t,i}$ is a tensor, $\vec{r}$ with the third dimension representing the ratings.

\subsubsection{Relevant Recommender System Literature.} 
Breese et al. \cite{breese1998empirical} considers the single-criteria problem where $\vec{r}(u,t, :) = r(u,t)$ is a scalar, equiv. $r$ is a matrix. Unknown values are defined as follows, 
\begin{equation}
 \label{eq:breese}
 \vec{r}(u,t,:): = \frac{1}{\sum_{v\in \text{Users}} \s(u,v) } \sum_{v\in \text{Users}} \s(u,v) \vec{r}(v,t,:)
\end{equation}
where $\s(u,v)$ is a similarity measure of users $u$ and $v$ computed with a standard similarity measure (e.g, cosine, inverse Euclidean, Pearson correlation) applied to the vector of ratings from $u$ and $v$ on the set of items both users rated. 
% This can be conceptualized as an expectation where likelihood is proportional user similarity. 
% More generally, the fundamental concept put forward (that we leverage heavily) is called collaborative filtering---that a user's ratings are predicted based on (in this case proportional to) ratings of those of other users with similar ratings. 

Adomavicius \& Kwon (AK) \cite{adomavicius2007new} extend this framework to multi-criteria ratings where $\vec{r}(u,t)$ is a vector with $\vec{r}(u,t,n)$ user $u$'s overall rating of tool $t$ on the $n^{th}$ aspect. 
Setting a distance on rating vectors, $d_R(\vec{r}(u,t,:), \vec{r}(v,s,:))$, %($\ell^p$ and Mahalanobis distance are tested), 
AK defines the distance between two users, $u, v$, as \\
$
 d_{U}(u,v) = \sum_{t\in T_{u,v}} d_R(\vec{r}(u,t), \vec{r}(v,t,:)) /|T_{u,v}|,
$
where $T_{u,v}:= \{$tools $t$ rated by both $u$ and $v$\}. The similarity of two users is simply an inverse function of their distance such that $\s(u,v):= 1/(1+d_{U}(u,v))$. 
Notably, the general framework is symmetric in users and items; hence, item similarity could just as easily produce predicted missing values. 
Finally, for each missing aspect rating ($j = 1, ..., n-1$), the output $\vec{r}(u,t,j)$ is predicted
%\todo{n-1?} 
according to Eg. \ref{eq:breese}, supervised learning techniques are suggested for learning $\vec{r}(u,t,n)$ from the aspect ratings $\{\vec{r}(u,t,j): j = 1, ..., n-1\}.$
%\todo{this is confusing}
% Both AK and Breese et al. replaced $r(w,t)$ with $r(w,t)-\overline{r_w}$, where $\overline{r_w}$ is the mean of user $w$'s ratings, in Eq. \ref{eq:breese}. 
% By predicting the variation from the mean, one allows for users whose mean ratings trends high/low.
% \begin{itemize}
% \item (Ratings distance) $d_R(r(u,t), r(v,s)) := \|r(u,t) - r(v,s)\|_p$ for $p = 1, 2, \infty$ (Euclidean, Manhattan, Chebyshev), or Mahalanobis Distance, i.e., $(r(u,t)-r(v,t))^t C (r(u,t) - r(v,t)) $ with $C$ denoting the covariance of $r(\cdot, t) $. 
% \item (User distance) $d_U(u,v): = \sum_{t\in I_{u,v}} d_R(r(u,t), r(v,t)) /|I_{u,v}|$, with $I_{u,v}:= \{$tools $t$ rated by both $u$ and $v$\}. 
% \item (User similarity) $\s(u,v) := 1/(1+d_{U}(u,v))$ 
% \end{itemize}
% two methods above are referred to as , as opposed to a progression of 
 The progression of the research literature diverged from these ``instance-based'' methods to develop ``model-based'' methods, e.g., 
 \cite{hofmann1999latent, si2003flexible, sahoo2012research}, which seek Bayesian network models that include latent variables designed to encode clusters of similar users and of items.
Results of Sahoo et al. \cite{sahoo2012research} conclude that model-based methods excel (in precision-recall balance and in precision-rank balance) when $\vec{r}$ is sparse (common, e.g., in online marketplaces with a huge inventory of items), whereas, the instance-based method of AK excels
%(specifically with $\ell^\infty$ ratings distance)\footnote{Through our testing we can provide intuition for why the $\ell^\infty$ ratings norm excels---in this situation, two users are similar ($\s(u,v)$ small) iff \textit{all} co-rated items are rated similarly. 
% Thus, predictions are almost exclusively based on those of other users with nearly identical preferences.} 
for dense $\vec{r}$, which is the case in this study. 

\subsubsection{Predicting Aspect Ratings.}
For each user, $u$, and for each tool, $t$, we have a vector of numeric responses $\vec{r}(u,t,:)$ of length eight, with the first seven entries as the aspect ratings--- the six questions on the SOAR tool's capabilities and the numerically-converted text comments field---and the last entry the overall rating by user $u$ for tool $t$.
Hence, $\vec{r}$ is a stack of eight $19\times11$ matrices. 
As each user was assigned a minimum of eight tools (of 11 total), on average we expect 3 tools $\times$ 8 ratings missing, leaving a whole vector $\vec{r}(u,t,:)$ empty. 
In addition to empty entries due to how tools were assigned, some entries were empty due to an operator leaving a question blank for a tool that they were assigned to evaluate. 
In all there were approximately one third missing values. 
Following the results of Sahoo et al. \cite{sahoo2012research}, we use the AK workflow with tailored modifications. 

From our data we define and compute three different similarities. Each produces predictions for all unknown values following Eq. \ref{eq:breese}, and we learn an optimal convex combination of the three as our prediction. 
The first two similarities are simply the user similarity $\s_U(u,v)$ and tool similarity $\s_T(s,t)$ from the ratings matrix.
To compute these, we first need a distance on the rating vectors. 
We use $\ell^p$ distance and test four distances, $p = 0, 1, 2, \infty$, where $p=0$ denotes counting the number of unequal entries in the two input vectors.
For our implementation, we define 
$\s_U(u,v) := \exp{(-\| \vec{r}(u,:,:) - \vec{r}(v,:,:)\|_p)}$, and 
$\s_I(s,t) := \exp{(-\| \vec{r}(:,s,:) - \vec{r}(:,t,:)\|_p)}$. 

When computing $\s_U$, the original work of AK ignored items that were not rated by both users. 
%This exhibits the stereotypical ``frequentist'' problem in practice, namely that two users with only a few co-rated items that have identical ratings (low evidence for similarity) receive maximal similarity, whereas two users with many similar but non-identical co-rated items (high evidence for similarity) can receive a much lower similarity. 
We tested both the naive rating distances against a Bayesian version. 
To compute the Bayesian distance between two vectors that may be missing values, we simply marginalize over a uniform distribution on the set of all possible missing values, (uniform on $\{1,...,5\}$).
Upon training we will have eight ratings distances to consider parameterized by $p \in \{0, 1, 2, \infty\}$ and each using the naive or Bayesian approach. 


Recall from Section \ref{sec:survey-design} our survey asked each participant to rank the aspects of a SOAR tool in advance of seeing and rating any of the tools.
This is valuable information for understanding each user's preferences, and we take this into account by providing a second user similarity (third similarity in total) from this data, which in turn provides a prediction of unknown ratings. 
We simply define $\s_{U_{rank}}(u,v)= (1 + \texttt{kendalltau}(u,v) )/2$ where the function \texttt{kendalltau} computes the Kendall Tau correlation \cite{knight1966computer} of the two users' aspect rankings. 
As $\text{kendalltau}(u,v) \in [-1,1]$ this similarity achieves its minimum of -1 with opposite rankings input, and maximum of 1 with identical rankings input. 

%Now armed with these three similarities, we use Eq. \ref{eq:breese} to predict ratings from each via Eq. \ref{eq:breese}. 
Let $\vec{r}_U, \vec{r}_I,$ and $\vec{r}_{U_{rank}}$ denote the populated tensors with previously missing values now predictions from $\s_U, \s_T, \s_{U_{rank}}$, respectively. Define the unknown ratings as $\vec{r}: = (1-a-b) \vec{r}_U + b \vec{r}_I + a \vec{r}_{U_{rank}}$, where $a \in [0,1]$, and $b \in [a, 1]$ are weight parameters to be learned. 
To learn the parameters, we grid search over $a, b, p$ and across using naive vs. Bayesian ratings distances.
For each set of parameters, we compute the macro-averaged error over a 20-fold cross validation to find the most accurate combination. 
We use 20-fold cross validation so that each fold has only 5\% known but hidden values used for testing.%---since accuracy of the predictions rests heavily on the non-missing values, using a larger set of folds (and therefore a smaller set of held-out data) increases the fidelity of the test to the actual situation. 

Our optimal parameters were found to use the Bayesian ratings distance computation with $p=1$, $a = 0.2$, and $b = 0.2$, which exhibited the (lowest) average error of 0.682. 
Since we used a grid search, the previous methods (non-Bayesian distances, using only a single similarity) are included in the results, and hence our advancements to provide greater accuracy than previous methods. 

\subsubsection{Predicting overall ratings.}
While overall ratings were included in the predictions above, we suspect (as AK \cite{adomavicius2007new} suggests) that overall ratings are more accurately predicted from the seven aspect ratings (for that tool by that user). % than by the collaborative filtering approach. 
To this end, we test a wide variety of supervised machine learning algorithms for regressing the overall ratings from the corresponding learning algorithms on a held out test set. 

After creating fully populated ratings for each aspect of each tool for each user, we first take the data that has a user-given overall ranking. We use this populated data to train and test ten machine learning regression algorithms. We then do a five-fold cross-validation on each model to determine which has the smallest error. We use the results from the model that produced the lowest average mean squared error (MSE) across the five folds. The model with the lowest MSE is then trained on all the available user given rankings. We then use the predicted aspect ratings to predict the overall rating.
%[TABLE \#] also shows that while we can predict the overall rating using the same technique that we use for the aspects, predicting the aspects and then using machine learning results in a significantly (?) lower average mean squared error, hence moving forward with this method. 
\vspace{-2mm}
\subsection{PageRank}
\vspace{-2mm}
Using the predicted overall rating, we developed a directed graph. Each vertex of the directed graph represents a tool. The edges between each node are drawn with an arrow, where the arrow points to higher rated tools based on each user. For example, if a user rated Tool A: 5; Tool B: 3; Tool C: 1, there would be directed edges as follows:
$C \rightarrow B, \quad
C \rightarrow A, \quad
B \rightarrow A. $
In cases where a user rates tools with the same number, no edges are drawn between those tools that would indicate which one is higher rated. In the case of duplicate links, the edges become weighted. A single edge has a weight of 1, if the directed edge is duplicated then 1 is added to the weight for every duplicate. If the converse directed edges is added, then 1 is deducted from the weight of the edge. This method was particularly applicable because we considering filling in missing data in a pairwise fashion, and this algorithm compares the tools pairwise.

After developing a weighted directed graph, we use the PageRank algorithm to measure the importance of each node \cite{page1999pagerank}. To implement the PageRank algorithm we used the NetworkX PageRank link analysis toolbox \cite{hagberg2008exploring}. 
\vspace{-2mm}
\subsection{Statistical Analyses}
\vspace{-2mm}
When we interpret results, we need to consider them in the context of certain demographics if it is shown that these factors have an impact on how operators are rating these SOAR tools. Through these univariate and multivariate analyses, we sought to determine the strength of four relationships: 
\begin{enumerate}
\item Tool rating and operator experience (in years)
\item Tool rating and operator occupation (security operator or other)
\item Tool rating and operator familiarity with the tool
\item Tool rating and perceived video quality by the operator
\end{enumerate}
We employed three tools to conduct these analyses: linear regression, Kruskal-Wallis Test, and multivariate analysis of variance (MANOVA). Each of these methods are described briefly below.

\subsubsection{Linear Regression.}
For each tool, we regressed its overall rating onto years of experience the operator has. After a line is fit to the data, Wald's Chi-Squared Test is to determine whether years of experience has an impact on the rating of the tool. Here, the null hypothesis is that the slope of the line that we fit to the data is zero, indicating that there is no relationship at all between years of experience and tool rating. Based on a 5\% error rate, a sufficiently small $p$-value ($<$ 0.05) from Wald's Chi-Squared Test indicates that the slope of the best fit line is \textit{not} zero and there is likely a relationship between the variables. 
\subsubsection{Kruskal-Wallis.}
In our analysis of the impact of tool familiarity on tool rating, we sought to address the question ``Do users assign higher ratings to tools with which they have more familiarity?'' Familiarity had five categories that users could mark for the tools they reviewed: \textit{Currently use it often, Used it at least once, Used it often in the past, Heard of it, Never heard of it}. 

For this analysis, we use a non-parametric Kruskal-Wallis test to compare the median overall tool scores of each of these five groups. All tool scores from every group are put into a single vector and sorted in ascending order. The tools are then ranked by their position in order from 1...$n$, where $n$ is the number of observations. For each of the five group, a sum of the ranks from each of its observations are calculated. These sums, along with the sample size and number of groups, are used to calculate an $H$ statistic. The $H$ statistic then gets compared to a critical chi-squared value at a certain error rate. Should the $H$ statistic be greater than the critical value, the null hypothesis (``the medians of these five groups are the same'') is rejected in favor of the alternate hypothesis (``the medians of the five groups are not the same'').

A Kruskal-Wallis Test was also performed to answer the question ``Does perceived video quality impact tool ratings?'' After watching the overview and demonstration videos about the tools, users specified whether they felt the video was \textit{Great, Okay, Terrible}. 

\subsubsection{MANOVA.}
We use a MANOVA test to determine whether a user's occupation impacts how they rated tools, and whether certain occupations preferred specific tools. MANOVA is useful for testing the effects that one explanatory variable (occupation) has on two or more dependent variables (ratings of tools 1-11) and compares the means of multiple dependent variables across two or groups. 
% \subsubsection{Relevant Previous Recommender System Works} 
% Breese et al. \cite{breese1998empirical} considers the single-criteria problem ($r(u,t)$ is a single number). 
% Unknown values are defined as follows, 
% \begin{equation}
% \label{eq:breese}
% r(u,t): = \frac{1}{\sum_{v\in \text{Users}} \s(u,v) } \sum_{v\in \text{Users}} \s(u,v) r(v,t)
% \end{equation}
% where $\s(u,v)$ is a similarity measure of users $u$ and $v$ computed with a standard similarity measure (e.g, cosine, inverse Euclidean, Pearson correlation) applied to the vector of ratings from $u$ and $v$ on the set of items both users rated. 
% % This can be conceptualized as an expectation where likelihood is proportional user similarity. 
% % More generally, the fundamental concept put forward (that we leverage heavily) is called collaborative filtering---that a user's ratings are predicted based on (in this case proportional to) ratings of those of other users with similar ratings. 
% Adomavicius \& Kwon (AK) \cite{adomavicius2007new} extend this framework to multi-criteria ratings ($r(u,t)$ is a vector with $r(u,t,n)$ user $u$'s overall rating of item $t$ and $r(u,t,j)$ the $j-$th aspect rating for $j = 1, ..., n-1$). 
% Setting a distance on rating vectors, $d_R(r(u,t), r(v,s))$, %($\ell^p$ and Mahalanobis distance are tested), 
% AK can in turn define the distance between two users, $u,v$, $\sum_{t\in I_{u,v}} d_R(r(u,t), r(v,t)) /|I_{u,v}|$, with $I_{u,v}:= \{$tools $t$ rated by both $u$ and $v$\}. 
% The similarity of two users is simply an inverse function of their distance ($\s(u,v):= 1/(1+d_{U}(u,v))$ was tested). 
% Notably, the general framework is symmetric in users and items; hence, item similarity could just as easily produce predicted missing values. 
% Finally, for each missing aspect rating ($j = 1, ..., n-1$) $r(u,t,j)$ is predicted according to Eg. \ref{eq:breese}, and supervised learning techniques are suggested for learning $r(u,t,n)$ from the aspect ratings $\{r(u,t,j): j = 1, ..., n-1\}.$
% % Both AK and Breese et al. replaced $r(w,t)$ with $r(w,t)-\overline{r_w}$, where $\overline{r_w}$ is the mean of user $w$'s ratings, in Eq. \ref{eq:breese}. 
% % By predicting the variation from the mean, one allows for users whose mean ratings trends high/low.
% % \begin{itemize}
% % \item (Ratings distance) $d_R(r(u,t), r(v,s)) := \|r(u,t) - r(v,s)\|_p$ for $p = 1, 2, \infty$ (Euclidean, Manhattan, Chebyshev), or Mahalanobis Distance, i.e., $(r(u,t)-r(v,t))^t C (r(u,t) - r(v,t)) $ with $C$ denoting the covariance of $r(\cdot, t) $. 
% % \item (User distance) $d_U(u,v): = \sum_{t\in I_{u,v}} d_R(r(u,t), r(v,t)) /|I_{u,v}|$, with $I_{u,v}:= \{$tools $t$ rated by both $u$ and $v$\}. 
% % \item (User similarity) $\s(u,v) := 1/(1+d_{U}(u,v))$ 
% % \end{itemize}
% % two methods above are referred to as , as opposed to a progression of 
% The progression of the research literature, diverged from these ``instance-based'' methods to develop ``model-based'' methods, e.g., 
% \cite{hofmann1999latent, si2003flexible, sahoo2012research}, which seek Bayesian network models that include latent variables designed to encode clusters of similar users and of items.
% Results of Sahoo et al. \cite{sahoo2012research} conclude that model-based methods excel (in precision-recall balance and in precision-rank balance) when $r$ is sparse (common, e.g., in online marketplaces with a huge inventory of items), whereas, the instance-based method of AK excels
% %(specifically with $\ell^\infty$ ratings distance)\footnote{Through our testing we can provide intuition for why the $\ell^\infty$ ratings norm excels---in this situation, two users are similar ($\s(u,v)$ small) iff \textit{all} co-rated items are rated similarly. 
% % Thus, predictions are almost exclusively based on those of other users with nearly identical preferences.} 
% for dense $r$ (our situation). 