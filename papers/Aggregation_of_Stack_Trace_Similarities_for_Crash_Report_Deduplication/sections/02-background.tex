\section{Background}\label{sec:background}

\subsection{Grouping Stack Traces}

There are various approaches for grouping stack traces that use different techniques, from edit distance to deep learning methods.
In this section, we describe the key ones.
It should be noted right away that most methods assign the incoming stack trace to the group that contains the most similar stack trace and therefore such approaches mainly differ in the way of calculating the similarity between stack traces.

Modani et al.~\cite{modani} use algorithms on strings to calculate stack trace similarity.
The authors present three similarity measures: edit distance, Longest Common Subsequence (LCS), and prefix match.
These algorithms are variants of classical string matching algorithms, with some modifications for a specific task of stack trace similarity.
In addition, the authors propose approaches for identifying uninformative functions (\textit{e.g.}, redundant recursion) and their further removal.

Lerch and Mezini~\cite{lerch} propose to use the approach based on the term frequency and the inverse document frequency (TF-IDF)~\cite{tfidf}.
In their work, each stack trace is tokenized, and the similarity value for the incoming stack trace $q$ and the given stack trace $d$ is calculated according to the following formula:
\begin{gather*}
    \mathrm{similarity}(q, d) = \sum_{t \in q} \mathrm{tf}_d(t) \cdot \mathrm{idf}(t)^2.
\end{gather*}
The advantage of the proposed approach is the speed of learning and the ease of implementation.

Sabor et al.~\cite{durfex} developed DURFEX, a technique that substitutes function names with the names of the packages in which they are defined to calculate the stack trace similarity, and then segments the resulting stack traces into N-grams of variable length.
This approach significantly reduces the amount of vocabulary used. This technique works with the JVM languages.

In our previous work, we proposed TraceSim~\cite{tracesim}, an approach for determining the similarity between two stack traces, which combines TF-IDF and string distance.
To obtain the values of hyperparameters used in the calculation of local and global frame weights, we formulate an optimization problem and use machine learning approaches to solve it.

Later, building on the ideas of processing stack traces, we proposed a new approach called S3M~\cite{s3m}.
S3M uses a Siamese neural network with biLSTM~\cite{lstm, bilstm} encoder to build the embeddings of stack traces and determine their similarity.
As a classifier, two fully-connected layers with ReLU activation are used. This is one of the first works in this field that uses deep learning methods.

Unlike previous works, Kim et al.~\cite{crash_graphs} proposed an approach called CrashGraphs that differs from most modern solutions described above in that it finds the similarity directly between the incoming stack trace and the groups themselves.
To do this, they represent the stack trace and the group in the form of a graph.
Firstly, all stack traces in the group are divided into pairs of consecutive calls.
For example, the group of two stack traces $ABC$ and $ACD$ will be parsed into pairs of frames $A \rightarrow B$, $B \rightarrow C$, $A \rightarrow C$, and $C \rightarrow D$.
Then, an oriented graph is built from these edges, and the similarity of a stack trace to the group is calculated by finding their sub-graph similarity. 
The main advantage of this approach is that we immediately get the similarity between the stack trace and the group, avoiding counting the similarity between all the individual stack traces.
In addition, such a representation of the group allows us to take into account its various features: composition, size, and uniqueness of stack traces.

The methods considered earlier determine the similarity value between the incoming stack trace and a certain group of stack traces as the similarity value between the incoming stack trace and the most similar stack trace in this group.
The main disadvantage of this technique is that finding this one most similar stack trace still requires calculating \textit{all} the similarity values for all stack traces, and this information is then discarded.
At the same time, the calculated similarity values can become a new source of information, which can improve the quality of many algorithms.
In addition, none of the considered works uses the information about the time of occurrence of stack traces in any way, which can also become a new source of information for future methods.

In this work, we aim to combine the advantages of the proposed approaches, create an \ag based on them that uses the time of occurrence of stack traces, and evaluate its usefulness on the real-world data from JetBrains, a large software company.

\subsection{Aggregating the Information with $k$-NN}
\label{sec:knn}

Note that placing the incoming stack trace into the group that contains the most similar stack trace is similar to the approach of the $k$-Nearest Neighbors algorithm ($k$-NN) with $k=1$. Let us briefly recall the idea of the $k$-NN approach.

Let $D$ be a set of labeled objects with labels from a certain set $Y$.
For a new object $x$, it is necessary to determine whether it belongs to a particular class.
The choice of a class is carried out according to the following decision function:
\begin{gather*}
    h(x, D) = \arg\max_{y \in Y}\sum_{x_i\in D}[y_i = y]w(x_i, x),\\
    w(x_i, x) = \begin{cases}1, \text{if $x_i$ is in the $k$ nearest neighbours of $x$}\\ 0, \text{otherwise}\end{cases}.
\end{gather*}
In the classical version of $k$-NN, the incoming object is assigned to the class whose objects were the most present among the $k$ closest to the new object.
If we take $k$ equal to 1, then we get an approach similar to choosing the group that contains the most similar stack trace.

The advantage of methods based on the idea of $k$-NN is that they do not require the presence of a metric space, with the main requirement being the ability to determine the distance (in our case, similarity) between a pair of objects.
Thus, the idea of $k$-NN can be transferred to the problem of grouping stack traces. 
In this case, taking into account multiple closest stack traces among the available groups will mean considering not only the single closest stack trace, but also other similiarity values in the groups.

A common modification to a basic $k$-NN approach is the \textit{weighted} $k$-NN approach.
The idea is that each object from the $k$ nearest neighbours is included in the decision rule with a certain non-zero weight.
A common approach to determining weight is to use kernel functions.
A lot of different kernel functions have been used in research: uniform, triangle, epanechnikov~\cite{epanechnikov}, quartic, triweight, gaussian, cosine, tricube~\cite{tricube}, logistic, sigmoid, silverman~\cite{silverman}.
These functions weight objects differently depending on their distance.

In addition to classical approaches, there are works that offer new types of weighting objects based on the distance.
Hyukjun et al.~\cite{k_conditional} propose the approach which calculates the distance between a new instance and the $k$-th nearest neighbor from each class, estimates posterior probabilities of class memberships using the distances, and assigns the instance to the class with the largest posterior. 
Ekin et al.~\cite{distance_based} propose several methods for weighting distances.
For example, in the \textit{Adjusted Weighting Method}, the distance to a certain class $Y_j$ is defined as the sum of the distances to the $\gamma$ nearest neighbours in this class with some decreasing weight $w$:
\begin{gather*}
    h(x, Y_j) = \sum_{i=1}^{\gamma}w^i \cdot d_i
\end{gather*}

Overall, $k$-NN-based approaches allow us to take into account the similarity to all stack traces in the groups.
To make sure that the proposed \ag is necessary, and that the same performance cannot be obtained by simply using a $k$-NN-based approach, we evaluate them also.