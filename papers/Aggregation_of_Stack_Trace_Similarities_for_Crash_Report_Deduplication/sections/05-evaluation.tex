\section{Evaluation}\label{sec:evaluation}

To test the applicability of our approach in the real world, we evaluated it not only on the open-source data, but also collected a dataset from JetBrains, a large software engineering company.
Specifically, we formulated three research questions:

\begin{questions}[leftmargin=1.3cm]
    \item Does the proposed \ag improve the performance of existing similarity models?
    \item Does the proposed \ag work better than the $k$-NN-based approach?
    \item Which features are the most important for the performance of the \ag?
\end{questions}

In this section, we describe how the experiments were set up, what data and metrics were used, as well as discuss the results of our experiments.

\subsection{Experimental setup}

Our goal is to compare several baseline similarity models from the literature with the \ag that employs them to consider all stack traces in the group instead of just the nearest one.
We evaluated five different similarity models described before: Modani et al.~\cite{modani}, Lerch and Mezini~\cite{lerch}, DURFEX~\cite{durfex}, TraceSim~\cite{tracesim}, and S3M~\cite{s3m}.
Additionally, we used CrashGraphs~\cite{crash_graphs} as a baseline, however, it cannot be aggregated since it does not calculate the similarity between individual stack traces.

It is necessary for all the models to see the same amount of information during training.
The data has a temporal component, used to construct some features, so it is very important for the train set to be located before the test set in time.

\begin{figure}[htbp]
\centering
    \includegraphics[width=\columnwidth]{figures/data.pdf}
    \centering
    \vspace{-0.4cm}
    \caption{The way the data was split.}
    \label{fig:data}
\end{figure}

The proposed way of splitting the data is shown in~\Cref{fig:data}.
All the models are validated on the $\mathrm{\textbf{Validation}}$ data span and tested on the $\mathrm{\textbf{Test}}$ data span, which contains the latest stack traces.
Each baseline similarity model is trained on the older data $\mathrm{\textbf{Train}}$.
The same data is then split into two intervals~--- $\mathrm{\textbf{Train}_\textbf{Sim}}$ and $\mathrm{\textbf{Train}_\textbf{Agg}}$.
On the $\mathrm{\textbf{Train}_\textbf{Sim}}$, the same similarity model is trained, with the help of which the similarity values on $\mathrm{\textbf{Train}_\textbf{Agg}}$ are obtained for training the \ag.
This way, the comparison is fair, the training for the \ag is merely divided into training the employed similarity model and training the aggregation itself.

\subsection{Data}

We carried out our experiments on two different datasets.
The first one is an open-source NetBeans dataset from our previous work~\cite{s3m}.
At the same time, we build our approach to be used in production, so we collected a new dataset from the proprietary data of JetBrains, a large software engineering company.
We plan to release this dataset upon acceptance to facilitate the further development of new approaches. 

\begin{table}[h]
\centering
\caption{The details of splitting the data into train, \\ validation (\textit{Val.}), and test sets.}
\begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{\textbf{Measure}} & 
\multicolumn{3}{c}{\textbf{NetBeans}} & \multicolumn{3}{c}{\textbf{JetBrains}}
\\ \cmidrule(lr){2-4}\cmidrule(lr){5-7} 
& \multicolumn{1}{c}{Train} & \multicolumn{1}{c}{Val.} & \multicolumn{1}{c}{Test} & \multicolumn{1}{c}{Train} & \multicolumn{1}{c}{Val.} & \multicolumn{1}{c}{Test} 

\\ \midrule

Groups &
30,975 & 1,695 & 6,084 & 6,356 & 1,077 & 1,731                          
\\ 

Reports & 
39,417 & 1,973 & 7,792 & 205,284 & 5,243 & 25,647                         
\\ 

Days & 
4,200 & 140 & 700 & 124 & 7 & 28                          
\\ \bottomrule
\end{tabular}


\label{tab:datasets}

\end{table}

\Cref{tab:datasets} shows the breakdown of datasets into train, validation, and test sets.
We split $\mathrm{\textbf{Train}}$ into $\mathrm{\textbf{Train}_\textbf{Sim}}$ and $\mathrm{\textbf{Train}_\textbf{Agg}}$ at the ratio of 20:1 for the NetBeans dataset and 9:1 for the JetBrains dataset.
This table also shows a strong difference between the datasets.
The NetBeans dataset generated an average of $7.7$ groups per day while the JetBrains dataset generated an average of $57.6$ groups per day.
Each group in the NetBeans dataset contains an average of $1.3$ reports, while for JetBrains, it is $25.8$ reports.
In short, the JetBrains data is less \textit{sparse} than the NetBeans data.
This diversity makes our data valuable for further research. Let us now describe each dataset a bit more.

\subsubsection{NetBeans dataset}
This dataset is open-source and was collected in our previous work~\cite{s3m}. NetBeans is an integrated development environment written in Java, the dataset includes their crash reports submitted before $2016$. Originally, the users attached files with a description of the failures that occurred, so the approach proposed by Lerch and Mezini~\cite{lerch} was used to generate crash reports by extracting stack traces from the attached files and description field using regular expressions.

\subsubsection{JetBrains dataset}\label{sec:jb_dataset}
JetBrains has its own system for handling automatically generated error reports. This system collects error reports from various projects written in JVM languages (Java, Kotlin, Scala), which helps to unify the work with them due to the homogeneous nature of the data. When a new report arrives, it is pre-processed. The following information is extracted from it: the product version, the point in time when the error occurred, the information about the system, and the stack trace itself.

New groups are formed as follows. When a new stack trace arrives, the Lerch and Mezini's similarity model~\cite{lerch} determines the stack trace's similarity to each of the groups and assigns the stack trace to the corresponding group. If the maximum similarity value is less than a certain threshold, then the corresponding stack trace is shown to the company developer who is experienced in this software component, and they themselves make a decision whether to define this stack trace in one of the existing groups or create a new one. To make a decision, the developer is shown a ranked list of groups.
Thus, data labeling is automatically performed whenever the developer has manually formed a new group or determined the stack trace into one of the existing groups. When working with our data, we do it in a similar way.

However, the usage of complex models in real-life large-scale projects introduces certain difficulties. Since the amount of data is really large, calculating all similarity values becomes very difficult, unacceptably long for production. For this reason, we use an additional \textit{filtration}. It works as follows.

Using the Lerch and Mezini's similarity model~\cite{lerch}, we find similarity values between the incoming stack trace and all stack traces in all groups. This model is very straightforward and fast, and can work the quickest. Then, having sorted the obtained values of similarity, we begin to select most similar stack traces until there are \textit{N} unique groups. This approach allows us to reduce the number of groups and reports used when inferencing the models. In our experiments, we used $N = 1000$. When choosing the value of \textit{N}, we were guided by the performance of the entire system (the smaller the value of \textit{N}, the faster the inference of the models goes) and the value of the $\mathrm{RR@N}$ metric, which is obtained using the similarity model. ``Heavier'' similarity models, for example, deep learning-based ones, can then be used on this filtered subset of data much faster. Evaluating models with filtering is crucial for their use in industrial systems.

Finally, it should be noted that the company's system has the following important property. If a group has not been updated for more than two months ($62$ days), then it is automatically considered closed for new error reports. That is, groups that have not been updated in the last $62$ days are not considered for a newly arrived stack trace. Thus, this feature must be taken into account when carrying out experiments.

\subsection{Performance Metrics}

\input{sections/05-evaluation-table}

We used the Mean Reciprocal Rank ($\mathrm{MRR}$)~\cite{mrr} and the Recall Rate ($\mathrm{RR@k}$)~\cite{rr} as comparison metrics. The choice of these metrics is based on research from previous works~\cite{lerch,durfex,irving}. The Mean Reciprocal Rank (MRR) reflects the ranking quality of the entire list, as it calculates the average reciprocal position of the correct item:

\begin{gather}
    \mathrm{MRR} = \frac{1}{|Q|}\sum\limits_{i=1}^{|Q|}\frac{1}{\mathrm{rank}_i}.
\end{gather}

That is, for each incoming stack trace $\in Q$, we get a ranked list of groups and find the position of the only correct answer in it.
If the stack trace was the first in the group, then there will be no correct answer in the list and its position can be considered equal to infinity.
After that, we take the average over all queries from the reciprocal values of the positions of the correct elements.
In the absence of a valid element, the inverse value will be zero.

Another important metric, \textit{Recall Rate at the first $k$ positions} ($\mathrm{RR} @ k$) counts the proportion of cases when the correct group was among the first $k$ options:

\begin{gather}
    \mathrm{RR}@k = \frac{1}{|Q|}\sum\limits_{i=1}^{|Q|}[\mathrm{rank}_i \leq k].
\end{gather}

This metric is more interpretable, which helps to better predict the future behavior of the system.
However, it only counts the first $k$ elements of the list, ignoring the rest.
The $\mathrm{RR} @ 1$ metric is especially important for us, since the automatic report processing system selects exactly one closest group and places the report there.
Accordingly, for such a system, the main quality criterion will be precisely the accuracy in the first position, since all the others will not be taken into account.
In addition, we are interested in the $\mathrm{RR} @ 5$ and $\mathrm{RR} @ 10$ metrics, since in the case of manual group selection, the developer is shown the top relevant groups.

\subsection{Training}

Since we are solving the ranking problem, we used RankNet Loss~\cite{ranknet} to train the \ag. For every positive example, we pick ten random negative examples as a negative sampling, which allows the model to better distinguish between similar stack traces.
Initially, the feature coefficients are randomly initialized. We used the Adam optimizer~\cite{adam} with the learning rate of $1\mathrm{e}{-3}$ and the weight decay of $1\mathrm{e}{-3}$.

\subsection{$k$-NN-based Approach}

To test the usefulness of the proposed \ag and make sure that it is necessary, we also implemented an alternative, simpler version of aggregating the data from different stack traces in the group.
We use the $k$-NN algorithm described in~\Cref{sec:knn} to take into account the distances to all the stack traces in all groups.

Since most approaches based on the idea of $k$-NN use distance values between objects, it is first necessary to transform the obtained similarity values in such a way that they satisfy the following property: the larger the similarity value, the smaller the distance value.
Thus, the more similar stack traces are, the closer they will be located to each other.
To comply with the required property, we used the following transformation.
Consider the similarity values $\{s_i\}_{i=1}^{M}$ from a given stack trace to all stack traces from all groups.
Then, the new values of the distances will be determined using the following formula:
\begin{gather}
    d_i = \max_{j=1,\ldots,M}\{s_j\} - s_i,\ \forall i \in 1,\ldots,M. 
    \label{distance_from_similarity}
\end{gather}

Some $k$-NN approaches use a support from 0 to 1, that is, if the distance is greater than or equal to 1, then the weight of the object will be zero. To take this into account, all distances obtained through Equation~\ref{distance_from_similarity} are divided by the distance to ($k$~+~1)-th neighbor.

Note that the distances obtained by Equation~\ref{distance_from_similarity} satisfy the requirement: the distance to the most similar stack trace will be minimal, and the less similar the stack traces are, the greater the resulting distance value will be.

In our experiments, we used weighted $k$-NN and evaluated the following eleven kernel functions: uniform, triangle, epanechnikov, quartic, triweight, gaussian, cosine, tricube, logistic, sigmoid, and silverman, as well as the apporaches proposed by Hyukjun et al.~\cite{k_conditional} and Ekin et al.~\cite{distance_based} For both datasets and all the tested similarity models, we tested these weighting methods and values of $k$ from 1 to 15, since larger values of $k$ always demonstrated worse performance. 

\subsection{Results}

\begin{figure*}[htbp]
\centering
    \includegraphics[width=\textwidth]{figures/heatmap.pdf}
    \centering
    \vspace{-0.4cm}
    \caption{The heatmap of feature importance for various models on the both datasets.}
    \label{fig:jb_features}
\end{figure*}

Let us now describe the results of our experiments and answer the posed research questions.

\subsubsection{RQ1: Aggregation Model vs. Baselines}

In the first research question, we wanted to see how the proposed \ag improves the performance of different similarity models. The results of the experiments are shown in~\Cref{tab:results}. For all four metrics (MRR, $\mathrm{RR} @ 1$, $\mathrm{RR} @ 5$, $\mathrm{RR} @ 10$), the column $\Delta_{A}$ shows the improvement of the \ag over the given baseline model.
Our \ag showed the largest improvement for the approach of Lerch and Mezini~\cite{lerch}: using the \ag improves the quality metric $\mathrm{RR} @ 1$ by $15$ percentage points on the NetBeans dataset and by $8$ percentage points on the JetBrains dataset. At the same time, the highest value in the $\mathrm{RR} @ 1$ metric overall was obtained with the aggregation of similarities obtained using S3M ($0.47$ and $0.86$ on the NetBeans dataset and the JetBrains dataset, respectively). Overall, it can be seen that using the \ag allows us to significantly improve the performance of all the models.

It can also be seen that the improvement is smaller on the new dataset than on the open-source NetBeans dataset. This difference in the influence demonstrates its importance for further research: improving the aggregation or developing brand new methods both require representative industrial data.

Finally, it should be mentioned that the filtration described in Section~\ref{sec:jb_dataset} greatly speeds up the processing of large amounts of data in the real run in production. We tested the approach on a machine with 8 Intel Xeon CPUs @ 2.30 GHz and 60 GB of RAM, and the report processing speed increased up to $200$ times.

\observation{According to our results, the proposed Aggreation Model increases the quality of the considered similarity models up to $8$ percentage points on the JetBrains dataset and up to $15$ percentage points on the open-source NetBeans dataset. The approach is very simple to implement in a real-world project and can significantly increase the quality of the employed models.}

\subsubsection{RQ2: Aggregation Model vs. $k$-NN}

In the second research question, we studied how the performance of our \ag compares with $k$-NN based approaches.
The results of applying $k$-NN-based approaches are also presented in \Cref{tab:results}. For each similarity model, the table shows the $k$-NN configuration that showed the best results (the kernel function and $k$).
The column $\Delta_{K}$ shows the improvement of the best $k$-NN model over the given baseline, thus, to compare our \ag and the $k$-NN-based approach, one needs to compare columns $\Delta_{A}$ and $\Delta_{K}$. 

The increase in the $\mathrm{RR} @ 1$ metric when using $k$-NN-based approaches is not as significant as in the case of the \ag. 
On the open-source NetBeans data, the maximum increase in the $\mathrm{RR} @ 1$ metric turned out to be $4$ percentage points for the S3M~\cite{s3m} model, while in the case of the \ag, the maximum increase in the $\mathrm{RR} @ 1$ metric turned out to be $15$ percentage points for the Lerch and Mezini~\cite{lerch} model.
As for the JetBrains data, the $k$-NN-based approaches demonstrated even worse results.
The maximum increase in the $\mathrm{RR} @ 1$ metric turned out to be $2$ percentage points for the TraceSim~\cite{tracesim} model, while in the case of the \ag, the maximum increase in the $\mathrm{RR}@1$ metric was $8$ percentage points for the Lerch and Mezini~\cite{lerch} model.
Overall, the best-performing $k$-NN-based models never outperformed the \ag.

\observation{The results of applying the $k$-NN-based approach are inferior in quality to the \ag for all the similarity models except for TraceSim.
This indicates the usefulness of the aggregation and the importance of taking into account the temporal aspect of stack traces.}

\subsubsection{RQ3: Feature Importance}
Finally, in the third research question, we assessed the importance of the features used in the \ag. Since we used standard scaling, and all the features have the same scale, we can analyze the coefficients of the used linear model. \Cref{fig:jb_features} shows the coefficients of each of the features listed in~\Cref{table:features-description} for each similarity model, trained for both datasets.

First of all, it can be seen that for all similarity models, the feature \textit{first maximum} (\textit{i.e.}, the similarity to the most similar stack trace) has the largest coefficient on both datasets. This confirms the importance of its use and explains the good results of baseline similarity models.

It can also be seen that for all similarity models, the following is true: the feature \textit{weighted similarity histogram bin \#12} has the largest coefficient out of all the bins of the weighted similarity histogram on the JetBrains dataset.
This is also true for almost all similarity models on the open-source NetBeans data.
Bin \#12 contains the most similar stack traces, including the \textit{first maximum}, and the more similar the stack traces are, the greater the score the \ag gives to this group.

Finally, it can be seen that most of the coefficients of the \textit{weights histogram} features have negative values. 
This is consistent with our assumption that the more stack traces there are that are far removed in time, and the longer the group has not been updated, the less likely it is that the stack trace should be assigned to this group. 
Thus, the \ag introduces some penalty to the final similarity value.

\vspace{0.1cm}

\observation{Expectedly, the \ag gives the greatest preference to the \textit{first maximum} feature, which is the feature that is used in virtually all existing methods.
However, the increase in performance and the analysis show that the model uses all the information given to it.}

\vspace{0.1cm}