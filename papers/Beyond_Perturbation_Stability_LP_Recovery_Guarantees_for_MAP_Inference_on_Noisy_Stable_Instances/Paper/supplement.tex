\onecolumn
\aistatssupptitle{Beyond Perturbation Stability: Supplementary Material}

\appendix

\section{Preliminaries Details}\label{sec:prelim_details}

\begin{claim}\label{claim:UMLnonnegative}
For Uniform Metric Labeling, we can assume $c(u,i) \ge 0$ and $w(u,v) > 0$ without loss of generality.
\end{claim}
\begin{proof}
For problem instances where some node costs are strictly negative, let $c_{\min}$ be the minimum value among all the node costs. Consider a new problem instance where we keep the edge costs the same, but set $c'(u,i) = c(u,i) + \abs{c_{\min}}$ for all $u \in V$ and $i \in [k]$. This new problem instance has all non-negative node costs, and the optimization problem is equivalent, because we added the same constant for all solutions. This reformulation also does not affect the $(2,1)$-expansion stability or $(2,1,\psi)$-expansion stability of the instance.

Likewise, for problem instances where some edge weights are $0$, let $E_0$ be the set of all edges with $0$ edge weight. Consider a new problem instance with $E' = E \setminus E_0$, with $w(u,v)$ unchanged for $(u,v) \in E\setminus E_0$, and identical node costs. The MAP optimization problem remains the same, and the new instance $((V,E'),c,w)$ is equivalent: it has the same MAP solution, and satisfies the stability definitions if and only if the original instance does as well.
\end{proof}

\Lstarclaim*
\begin{proof}[Proof of Claim \ref{claim:Lstar}]
Recall the local LP:
\begin{alignat}{2}
  \mindot_{x}\sum_{u\in V}&\sum_i c(u,i)x_u(i) + \sum_{(u,v) \in E}&&w(u,v)\sum_{i\ne j}x_{uv}(i,j)\\
  \text{subject to:}& \sum_{i}x_u(i) = 1 &&\forall\ u\in V\label{con:norm}\\
                    & \sum_{i}x_{uv}(i,j) = x_v(j)&& \forall\ (u,v)\in E,\ j\in [k]\label{con:marg1}\\
                   & \sum_{j}x_{uv}(i,j) = x_u(i)&& \forall\ (u,v)\in E,\ i\in [k]\label{con:marg2}\\
                   & x_{uv}(i,j) \in [0,1]&&\forall\ (u,v),\ (i,j)\label{con:edge-normalize}\\
                   & x_{u}(i) \in [0,1]&& \forall\ u,\ i.
\end{alignat}
The feasible region defined by the above constraints is $L(G)$. $L^*(G) \subseteq L(G)$ is the set of points that satisfy the additional constraint that $x_{uv}(i,i) = \min(x_u(i), x_v(i))$ for all $(u,v) \in E$ and $i \in [k]$. For any feasible node variable assignments $\{x_u\}$, $L^*(G)$ is not empty: a simple flow argument\footnote{For an edge $(u,v)$, consider the bipartite graph $\tilde{G} = ((U,V), E)$, where $|U| = |V| = k$. We let $x_u(i)$ represent the supply at node $i$ in $U$, and let $x_v(i)$ represent the demand at node $j$ in $V$. Because $x_u$ and $x_v$ are both feasible, the total supply equals the total demand. $E$ contains all edges between $U$ and $V$, so we can send flow from $i\in U$ to $j \in V$ for any $(i,j)$ pair. Let $x_{uv}(i,j)$ represent this flow, and set $x_{uv}(i,i) = \min(x_u(i), x_v(i))$. For every $i$, this either satisfies the demand at node $V_i$ or exhausts the supply at node $U_i$. In each case, we can remove that satisfied/exhausted node from the graph. After this choice of $x_u(i,i)$, the total remaining supply equals the total remaining demand ($\sum_i x_u(i) - \min(x_u(i),x_v(i)) = \sum_i x_v(i) - \min(x_u(i),x_v(i))$), all supplies and demands are nonnegative, and the remaining graph $\tilde{G}'$ is a complete bipartite graph (over fewer nodes). This implies that the flow constraints \eqref{con:marg1}, \eqref{con:marg2}, \eqref{con:edge-normalize} are still feasible.}
implies that the constraints \eqref{con:marg1}, \eqref{con:marg2}, and \eqref{con:edge-normalize} are always satisfiable even when we set $x_{uv}(i,i) = \min(x_u(i), x_v(i))$. 
For all integer feasible solutions in $L(G)$, notice that $x_{uv}(i,j) = 1$ if $x_u(i) = 1$ and $x_v(j) = 1$ or $0$ otherwise. Therefore, all integer solutions satisfy this additional constraint.
Consider a $\theta$ where all edge weights are strictly positive. 
If $x$ minimizes $\dot{\theta}{x}$, $x$ must pay the minimum edge cost consistent with its node variables $x_u(i)$.
So if we fix the $x_u(i)$ portion of $x$, we know that the edge variables $x_{uv}$ of $x$ are a solution to:
\[ \min_{x \in L(G)} \sum_{(u,v) \in E}w(u,v)\sum_{i\ne j}x_{uv}(i,j). \]
Notice that since we have fixed the node variables $x_u(i)$, there is no interaction between the $x_{uv}$ variables across different edges. 
So we can minimize this objective by minimizing each individual term $w(u,v)\sum_{i\ne j}x_{uv}(i,j)$. 
Since $w_{uv} > 0$ for all edges, we need to minimize $\sum_{i\ne j}x_{uv}(i,j)$. 
Notice that for every edge $(u,v) \in E$, we get that $\sum_{i} \sum_{j} x_{uv}(i,j) = 1$ by substituting $x_u(i)$ in constraint~\ref{con:norm} with $\sum_{j} x_{uv}(i,j)$ from constraint~\ref{con:marg2}. 
Therefore $\sum_{i\ne j} x_{uv}(i,j) = 1 - \sum_{i}x_{uv}(i,i)$. 
Thus, minimizing $\sum_{i\ne j}x_{uv}(i,j)$ is  the same as maximizing $\sum_{i}x_{uv}(i,i)$. 
And the maximizing choice for $x_{uv}(i,i) = \min (x_u(i), x_v(i))$ due to constraints~\ref{con:marg1} and \ref{con:marg2}.
\end{proof}

\section{Expansion Stability details}\label{sec:expansion-stability_details}

\begin{figure}[tb]
  \centering
\begin{subfigure}{.5\linewidth}
  \centering
  \scalebox{1.0}{  
  \tikzstyle{vertex}=[circle, draw=black, very thick, minimum size=5mm]
  \tikzstyle{edge} = [draw=black, line width=1]
  \tikzstyle{weight} = [font=\normalsize]
  \begin{tikzpicture}[scale=2,auto,swap]
    \foreach \pos /\name in {{(0,0)}/u,{(1,0)}/w,{(0.5,0.75)}/v}
    \node[vertex](\name) at \pos{$\name$};
    \foreach \source /\dest /\weight in {u/w/1+\epsilon}
    \path[edge] (\source) -- node[weight] {$\weight$} (\dest);
    \foreach \source /\dest /\weight/\pos in {u/v/1+\epsilon/{above left}, v/w/1+\epsilon/{above right}}
    \path[edge] (\source) -- node[weight, \pos] {$\weight$} (\dest);
  \end{tikzpicture}
  }
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
  \centering
  \scalebox{1.0}{
\begin{tabular}{|l|ccc|}
\hline
\multicolumn{1}{|c|}{\textbf{Node}} & \multicolumn{3}{|c|}{\textbf{Costs}} \\
\hline
u & .5 & $\infty$        & $\infty$      \\
\hline
v & 1 & 0 & $\infty$\\
\hline
w & 1 & $\infty$ & 0\\
\hline
\end{tabular}
}
\end{subfigure}
  \caption{$(2,1)$-expansion stable instance that is not $(2,1)$-stable. In the original instance (shown left), the optimal solution labels each vertex with label 1, for an objective of $2.5$. The adversarial $(2,1)$-perturbation for this instance replaces all the edge weights of $1+\epsilon$ with $(1+\epsilon)/2$. In this perturbed instance, the optimal solution labels $(u,v,w) \rightarrow (1,2,3)$. This has a node cost of 0.5 and an edge cost of $(3+3\epsilon)/2$, for a total of $2+3\epsilon/2 < 2.5$. Since the original solution is not optimal in the perturbed instance, this instance is not $(2,1)$-perturbation stable. However, note that the only expansions of the original solution (which had all label 1) that have non-infinite objective are $(u,v,w) \rightarrow (1,2,1)$ and $(u,v,w) \rightarrow (1,1,3)$. These each have objective $2.5 + \epsilon$, which is strictly greater than the perturbed objective of the original solution. Therefore, this instance is $(2,1)$-expansion stable.}
\label{fig:counter1-apdx}
\end{figure}

\begin{claim}\label{exp_worst}
An instance $(G,w,c)$ is $(2,1)$-expansion stable iff the MAP solution $\bx$ is strictly better than all its expansions in the adversarial perturbation $\theta_{adv}$. That is, for all $x \in \calE_{\bx}, \dot{\theta_{adv}}{x} > \dot{\theta_{adv}}{\bx}$ where $\theta_{adv}$ has the same node costs $c$ but has weights $
    w_{adv}(u,v) = \begin{cases}
    \frac{1}{2}w(u,v) & \bx(u) = 
    \bx(v)\\
    w(u,v) & \bx(u) \ne \bx(v).
\end{cases}
$
\end{claim}
\begin{proof}
Consider $\theta' = (c,w')$, any valid $(2,1)$-perturbation of $\theta = (c,w)$ i.e. for every edge $(u,v) \in E, \frac{w(u,v)}{2} \leq w'(u,v) \leq w(u,v)$. For any valid labeling $x$, let $E_x$ represent the edges cut by $x$. Then, for any $x$ which is an expansion of $\bx$ i.e. $x \in \calE_{\bx}$,
\begin{align*}
    \dot{\theta'}{x} - \dot{\theta'}{\bx} &= \sum_{u \in V} c(u,x(u)) - c(u,\bx(u)) + \sum_{(u,v) \in E_x} w'(u,v) - \sum_{(u,v) \in E_{\bx}} w'(u,v)
    \\&= \sum_{u \in V} c(u,x(u)) - c(u,\bx(u)) + \sum_{(u,v) \in E_x \setminus E_{\bx}} w'(u,v) - \sum_{(u,v) \in E_{\bx}\setminus E_x} w'(u,v)
    \\&= \dot{\theta_{adv}}{x} - \dot{\theta_{adv}}{\bx} + \sum_{(u,v) \in E_x \setminus E_{\bx}} w'(u,v) -w_{adv}(u,v)  + \sum_{(u,v) \in E_{\bx}\setminus E_x} w_{adv} - w'(u,v)
    \\&= \dot{\theta_{adv}}{x} - \dot{\theta_{adv}}{\bx} + \sum_{(u,v) \in E_x \setminus E_{\bx}} w'(u,v) -\frac{w(u,v)}{2}  + \sum_{(u,v) \in E_{\bx}\setminus E_x} w(u,v) - w'(u,v)
\end{align*}
Since $w'$ is a valid $(2,1)$-perturbation, $w'(u,v) \ge w(u,v)/2$ and $w'(u,v) \le w(u,v)$. Therefore, for any valid $(2,1)$-perturbation $\theta'$, we have 
\[
\dot{\theta'}{x} - \dot{\theta'}{\bx} \geq \dot{\theta_{adv}}{x} - \dot{\theta_{adv}}{\bx}.
\]

If the instance is $(2,1)$-expansion stable, then certainly $\dot{\theta_{adv}}{x} > \dot{\theta_{adv}}{\bx}$ for all $x\in \calE^{\bx}$, since $\theta_{adv}$ is a valid $(2,1)$-perturbation of $\theta$. If the instance is not $(2,1)$-expansion stable, there exists a $\theta'$ and an $x\in \calE^{\bx}$ for which $\dot{\theta'}{x} - \dot{\theta'}{\bx} \le 0$. But the above inequality then implies that $\dot{\theta_{adv}}{x} - \dot{\theta_{adv}}{\bx} \le 0$ as well. This gives both directions.
\end{proof}

This claim shows that to check whether an instance is $(2,1)$-expansion stable, it is sufficient to check that the MAP solution is strictly better than all its expansions in the adversarial perturbation $\theta_{adv}$. We don't need to verify that this condition is satisfied in \textit{every} valid $(2,1)$-perturbation. Because the optimal expansion of $\bx$ in the instance with objective $\theta_{adv}$ can be computed efficiently, this claim also implies that $(2,1)$-expansion stability can be efficiently checked once the MAP solution $\bx$ is known.

\begin{claim}\label{exp_weak}
$(2,1)$-expansion stability is strictly weaker than $(2,1)$-perturbation stability. 
\end{claim}
\begin{proof}
Figure \ref{fig:counter1} gives an instance of uniform metric labeling that is $(2,1)$-expansion stable but not $(2,1)$-perturbation stable. Here, $0 < \epsilon < 1/3$.
\end{proof}

\lptes*
\begin{proof}
First, we note that for any $x \in L^*(G)$, the objective value of the local LP can be written in a form that depends only on the node variables $x_V$. The objective term corresponding to the edges
\begin{align*}
    \sum_{(u,v) \in E}w(u,v)\sum_{i\ne j}x_{uv}(i,j) &= \sum_{(u,v) \in E}w(u,v)\rbr{\sum_{i,j}x_{uv}(i,j) - \sum_{i}x_{uv}(i,i)}
    \\&= \sum_{(u,v) \in E}w(u,v)\rbr{1 - \sum_{i}x_{uv}(i,i)} = \sum_{(u,v) \in E}w(u,v)\rbr{1 - \sum_{i}\min(x_u(i),x_v(i))}
    \\&= \sum_{(u,v) \in E}w(u,v)\rbr{1 - \sum_{i}\rbr{\frac{x_u(i) + x_v(i)}{2} - \frac{\abs{x_u(i) - x_v(i)}}{2}}}
    \\&= \sum_{(u,v) \in E}w(u,v)\rbr{\frac{1}{2}\sum_{i}\abs{x_u(i) - x_v(i)}}
\end{align*}

Here we used the definition of $L^*(G)$ and the facts that $\sum_i x_u(i) = 1$ for all $(u,i)$ and $\sum_{j}x_{uv}(i,j) = x_u(i)$ for all $(u,v)\in E,\ i\in [k]$.

Thus, for any $x \in L^*(G)$, the objective of the local LP can be written as \[\sum_{u\in V}\sum_i c(u,i)x_u(i) + \sum_{(u,v) \in E}w(u,v)d(u,v)\]
where $d(u,v) \coloneqq \frac{1}{2}\sum_{i}\abs{x_u(i) - x_v(i)}$. This is the objective function of another LP relaxation for uniform metric labeling called the ``metric LP'', which is equivalent to the local LP \citep{archer2004approximate}. Note that both $\bar{x}$ and $\hx$ are in $L^*(G)$ by Claim \ref{claim:Lstar}. Therefore, the objective function can be written in the above form for both of them.

In the next section, we introduce a rounding algorithm and prove some guarantees for the random solutions $h$ output by it. We then use these guarantees to show an upper bound on the expected cost of these random solutions in a perturbed instance of the problem. Finally, we use this upper bound to prove that $\hx = \bx$.

\subsection{$\epsilon$-close rounding:}
Given any feasible solution $x \in L(G)$ and a valid labeling $\bar{x}$, we construct a related feasible solution $x'$ which is $\epsilon$-close to $\bar{x}$ in the $\ell_{\infty}$-norm i.e. $\norm{x' - \bar{x}}_{\infty} \leq \epsilon$:
\begin{equation}\label{eqn:eps-close}
    x' = \epsilon x + (1-\epsilon)\bar{x},
\end{equation}
where $\epsilon < 1/k$ and we have identified the labeling $\bar{x}:V\to\mathcal{L}$ with its corresponding vertex of the marginal polytope (a vector in $\{0,1\}^{nk+mk^2}$). We consider the following rounding algorithm applied to $x'$, which is a modified version of the $\epsilon$-close rounding algorithm used in \citet{LanSonVij18}:
\begin{algorithm}[H]
   \caption{$\epsilon$-close rounding}
   \label{alg:rounding_algorithm}
\begin{algorithmic}[1]
   \STATE Choose $i \in \{1,\dots,k\}$ uniformly at random.
   \STATE Choose $r \in (0,1/k)$ uniformly at random.
   \STATE Initialize labeling $h: V\to [k]$.
   \FOR{each $u \in V$}
        \IF {$x'_u(i) > r$}
            \STATE Set $h(u) = i$.
        \ELSE
            \STATE Set $h(u) = \bx(u)$
        \ENDIF 
   \ENDFOR
   \STATE \textbf{Return} $h$
\end{algorithmic}
\end{algorithm}

\begin{lemma}[Rounding guarantees]
\label{lemma:rounding-guar}
Given any $x'$ constructed using \eqref{eqn:eps-close}, the labeling $h$ output by Algorithm \ref{alg:rounding_algorithm} satisfies the following guarantees:
\begin{align*}
    \prob{h(u) = i} &= x'_u(i) &\forall\ u\in V, i\in [k]\\
    \prob{h(u) \ne h(v)} &\le 2d(u,v) &\forall\ (u,v) \in E : \bx(u) = \bx(v)\\
    \prob{h(u) = h(v)} &= (1-d(u,v)) &\forall\ (u,v) \in E : \bx(u) \ne \bx(v),
\end{align*}
where $d(u,v) = \frac{1}{2}\sum_i|x'_u(i) - x'_v(i)|$ is the edge separation of the constructed feasible point $x'$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:rounding-guar} (rounding guarantees)]
First, fix $u\in V$ and a label $i\ne \bx(u)$. We output $h(u) = i$ precisely when $i$ is chosen and $0 < r < x'_u(i)$, which occurs with probability $\frac{1}{k}\frac{x'_u(i)}{1/k} = x'_u(i)$ (we used here that $x'_u(i) \le \epsilon < 1/k$ for all $i\ne \bx(u)$). Now we output $h(u)=\bx(u)$ with probability $1-\sum_{j\ne \bx(u)}\prob{h(u) = j} = 1-\sum_{j\ne \bx(u)}x'_u(j) = x'_u(\bx(u))$, since $\sum_{i}x'_u(i) = 1$. This proves the first guarantee.

For the second, consider an edge $(u,v)$ not cut by $\bx$, so $\bx(u) = \bx(v)$. Then $(u,v)$ is cut by $h$ when some $i \ne \bx(u)$ is chosen and $r$ falls between $x'_u(i)$ and $x'_v(i)$. This occurs with probability \[\frac{1}{k}\sum_{i\ne \bx(u)}\frac{\max(x'_u(i), x'_v(i))-\min(x'_u(i), x'_v(i))}{1/k} = \sum_{i\ne \bx(u)} |x'_u(i) - x'_v(i)| \le 2d(u,v).\]

Finally, consider an edge $(u,v)$ cut by $\bx$, so that $\bx(u) \ne \bx(v)$. Here $h(u) = h(v)$ if some $i,r$ are chosen with $r < \min(x'_u(i), x'_v(i))$. We have $r < \min(x'_u(i), x'_v(i))$ with probability $\frac{\min(x'_u(i), x'_v(i))}{1/k}$. Note that this is still valid if $i=\bx(u)$ or $i=\bx(v)$, since only one of those equalities can hold.
So we get \[ \frac{1}{k}
\sum_{i} \frac{\min(x'_u(i),x'_v(i))}{1/k}  = \frac{1}{2}\left(\sum_i x'_u(i) + x'_v(i) - |x'_u(i) - x'_v(i)|\right) = 1 - d(u,v),
\]
where we used again that $\sum_i x'_u(i) = 1$.
\end{proof}
Given these rounding guarantees, we can relate the expected cost difference between $h$ and $\bx$ in a perturbation of the original instance to the cost difference between $x$ and $\bx$ in the original instance. We are only interested in the case when $x \in L^*(G)$ and so the objective function $f(x) = \sum_{u\in V}\sum_i c(u,i)x_u(i) + \sum_{(u,v) \in E}w(u,v)d(u,v)$.

\subsection{Using the rounding guarantees}
\begin{lemma}\label{lem:Ahupp}
Given an integer solution $\bx$, a feasible LP solution $x \in L^*(G)$, and a random output $h$ of Algorithm \ref{alg:rounding_algorithm} on an input $x' = \epsilon x + (1-\epsilon)\bar{x}$, define
\[
    w'(u,v) = \begin{cases}
    \frac{1}{2}w(u,v) & \bx(u) = 
    \bx(v)\\
    w(u,v) & \bx(u) \ne \bx(v)
\end{cases}
\]
and let $f'(y) = \sum_{u\in V}\sum_i c(u,i)y_u(i) + \sum_{(u,v) \in E}w'(u,v)d(y,u,v)$ be the objective in the instance with the original costs, but using weights $w'$. Here $d(y,u,v) = \frac{1}{2}\sum_i|y_u(i) - y_v(i)|$. Let $A_h \coloneqq f'(h) - f'(\bx)$ be the difference in this perturbed objective between $h$ and $\bx$. Then,
\[
 \E[A_h] = \E{[f'(h) - f'(\bx)]} \le \epsilon \cdot \rbr{f(x) - f(\bx)}.
\]
\end{lemma}
\begin{proof}
\begin{align*}
\E{[f'(h) - f'(\bx)]} &= \sum_{u\in V}\sum_{i}c(u,i)\prob{h(u) = i} - \sum_{u\in V}c(u,\bx(u)) + \sum_{uv : \bx(u) = \bx(v)} w'(u,v)\prob{h(u) \ne h(v)} \\
\qquad\qquad &- \sum_{uv : \bx(u) \neq \bx(v)} w'(u,v)\prob{h(u) = h(v)}\\
&= \sum_u\sum_ic(u,i)x'_u(i) - \sum_uc(u,\bx(u)) + \sum_{uv: \bx(u)=\bx(v)}2w'(u,v)d(x',u,v) \\
&- \sum_{uv : \bx(u) \ne \bx(v)}w'(u,v)(1-d(x',u,v))\\
&= \sum_u\sum_ic(u,i)x'_u(i) - \sum_uc(u,\bx(u)) + \sum_{uv\in E}w(u,v)d(x',u,v)- \sum_{uv : \bx(u) \ne \bx(v)}w(u,v)\\
&= f(x') - f(\bx).
\end{align*}
where the second-to-last equality used the definition of $w'$ (note that $w'$ is identical to the worst-case perturbation $w_{adv}$ for $\bx$). Because $f$ is convex (in particular, $d(x,u,v)$ is convex in $x$), we have $f(x') \le \epsilon f(x) + (1-\epsilon)f(\bx)$. Therefore,
\[
\E{[f'(h) - f'(\bx)]} \le \epsilon f(x) + (1-\epsilon)f(\bx) - f(\bx) = \epsilon(f(x) - f(\bx)),
\]
which is what we wanted.
\end{proof}

\subsection{Final proof of Theorem~\ref{thm:lptes}:}\label{sec:finlpt}
%We prove Theorem~\ref{thm:lptes} using Lemma~\ref{lem:Ah}.
Suppose the local LP solution $\hx$ is not the same as the MAP solution $\bx$ i.e. $\hx \neq \bx$. Consider $x' = \epsilon\hat{x} + (1-\epsilon)\bar{x}$ where $0< \epsilon < 1/k$ (see equation~\eqref{eqn:eps-close}).
Let $h$ be the random integer solution output by using Algorithm~\ref{alg:rounding_algorithm} on $x'$. By Lemma~\ref{lem:Ahupp}, we have 
\[ \E{[f'(h) - f'(\bx)]} \leq \epsilon \cdot \rbr{f(\hx) - f(\bx)} \]
%Here $f'$ is the worst-case $(2,1)$-perturbation for the MAP solution $\bx$.

We note that any solution $h$ that we get from rounding $x'$ is either $\bx$ or an expansion move of $\bx$. This is because we pick only a single label $i$ in step 1 of Algorithm~\ref{alg:rounding_algorithm} and label all vertices $u$ either $i$ or $\bx(u)$. 
Therefore, for the $i$ picked in step 1, $h$ is an $i$-expansion of $\bx$ if $h \neq \bx$.
\begin{align*}
    \E{[f'(h) - f'(\bx)]} &= \E{[f'(h) - f'(\bx)|h\neq \bx]}\ \Pr[h \neq \bx] + \E{[f'(h) - f'(\bx)|h = \bx]}\ \Pr[h = \bx]
    \\&= \E{[f'(h) - f'(\bx)|h\neq \bx]}\ \Pr[h \neq \bx]
\end{align*}
Since $(G,c,w)$ is a $(2,1)$-expansion stable instance, we know that $f'(h) > f'(\bx)$ when $h \neq \bx$ since all $h$ in the support of the rounding (other than $\bx$) are expansion moves of $\bx$ and we get $f'$ by a valid $(2,1)$-perturbation of $(G,c,w)$.
Therefore, $\E{[f'(h) - f'(\bx)|h\neq \bx]} > 0$. We also have that $\Pr[h \neq \bx] > 0$ since we assumed that $\hx \neq \bx$. Therefore, $\E{[f'(h) - f'(\bx)]} > 0$. But we know that $f(\hx) - f(\bx) \leq 0$ since $\hx$ is the minimizer of $f(x)$ among all feasible $x \in L(G)$. So Lemma \ref{lem:Ahupp} implies $\E{[f'(h) - f'(\bx)]} \le 0$. Thus we have a contradiction and so the local LP solution $\hx$ has to be the same as the MAP solution $\bx$.

\iffalse
First, we introduce a different LP relaxation for uniform metric labeling, which we will refer to as the \emph{metric} LP relaxation.
\begin{alignat}{2}
  \minimize_{x}\sum_{u\in V}&\sum_i c(u,i)x_u(i) + \sum_{(u,v) \in E}w(u,v)&&d(u,v)\\
  \text{subject to:}& \sum_{i}x_u(i) = 1 &&\forall\ u\in V\\
                    & d(u,v) \ge \frac{1}{2}\sum_i|x_u(i) - x_v(i)| && \forall (u,v),\\
                   & x_{u}(i) \in [0,1]&& \forall\ u,\ i.
\end{alignat}

This is similar to the local LP. The node variables $x_u(i)$ still correspond to fractional labelings. Instead of using edge variables $x_{uv}(i,j)$, the metric relaxation simplifies by noting that for the Potts model, for fixed node variables $x_u$, $x_v$, the \emph{optimal} edge cost paid by any edge variables $x_{uv}(i,j)$ that are consistent with $x_u$ and $x_v$ is precisely $\frac{1}{2}\sum_i|x_u(i) - x_v(i)|$.

Let $\hat{x}^m$ be the solution of the metric LP relaxation. We prove that the node variables $\hat{x}^m_u$ exactly match the node variables $\bar{x}_u$ of the MAP solution.
This implies the pairwise node variables $\hat{x}_u$ are integral and equal to $\bar{x}_u$, because otherwise the solution to the metric LP would not be unique. 
This also implies the pairwise edge variables are equal to $\bar{x}_{uv}$---only one setting of $x_{uv}(i,j)$ is consistent with \emph{integral} $x_u$ and $x_v$.
This argument implies we can focus on showing that the metric LP solution is unique and equal to $\bar{x}_u$.
For the rest of the proof, we let $\hat{x}$ refer to the solution to the \emph{metric} LP. 

Let $x' = \epsilon\hat{x} + (1-\epsilon)\bar{x}$ be a convex combination of the MAP solution $\bar{x}$ and the metric LP solution $\hat{x}$. 
\fi

\end{proof}

\section{Stability and Curvature around MAP solution: details}\label{sec:stable-curvature_details}
\curvature*

Here, we provide two proofs for this theorem, one deals directly with the local LP relaxation and the other uses the dual of the relaxation. The dual proof is more general than the primal proof as it works for all $x \in L(G)$, not just for those in $L^*(G)$.

\subsection{Primal-based proof}
\begin{proof}
For any $x \in L^*(G)$, consider a feasible solution $x'$ which is $\epsilon$-close to $\bx$ constructed using Equation~\ref{eqn:eps-close} i.e. $x' = \epsilon x + (1-\epsilon)\bar{x}$. Let $h$ be the random solution output by Algorithm~\ref{alg:rounding_algorithm} on $x'$.

\begin{lemma}[Bound for $\E{[B_h]}$]\label{lem:Bh}
For any $h$ in the support of the rounding of $x' = \epsilon x + (1-\epsilon)\bar{x}$, let us define $B_h$ to be the number of vertices which it labels differently from $\bx$. In other words, it is the number of vertices which are misclassified by $h$ i.e. $B_h \coloneqq \sum_{u \in V} \indicator[h(u) \ne \bx(u)]$. Then, 
\[ \E{[B_h]} = \epsilon \sum_{u \in V} \frac{1}{2} \norm{x_u - \bx_u}_1 \]
\end{lemma}
\begin{proof}
\begin{align*}
    \E[B_h] &= \sum_{u \in V} \E[\indicator[h(u) \neq \bx(u)]] = \sum_{u \in V} \prob{h(u) \neq \bx(u)} = \sum_{u \in V} 1 - \prob{h(u) = \bx(u)} 
    \\&= \sum_{u \in V} 1 - x'_u(\bx(u)) = \sum_{u \in V} 1 - \rbr{\epsilon x_u(\bx(u)) + (1-\epsilon)} = \sum_{u \in V} \epsilon \rbr{1 -  x_u(\bx(u))}
    \\&= \epsilon \sum_{u \in V} \frac{1}{2} \rbr{1 - x_u(\bx(u)) + \sum_{i \neq \bx(u)} x_u(i) } = \epsilon \sum_{u \in V} \frac{1}{2} \norm{x_u - \bx_u}_1
\end{align*}
Here, we used the fact that for all $u \in V, \bx_u(\bx(u)) = 1 \text{ and } \bx_u(i) = 0\ \forall\ i \neq \bx(u)$ and since $x$ is a feasible solution to the LP, it satisfies $\sum_{i \ne \bx(u)} x_u(i) = 1 - x_u(\bx(u))$ for all $u \in V$.
\end{proof}

\begin{lemma}[Lower bound for $A_h$ using $(2,1,\psi)$-expansion stability]\label{lem:Ahlow}
If $(G,w,c)$ is a $(2,1,\psi)$-expansion stable instance, then for any $h$ in the support of the rounding of $x' = \epsilon x + (1-\epsilon)\bar{x}$,
 \[A_h \ge \psi \cdot B_h\]
where $A_h = f'(h) - f'(\bx)$ and $f'$ is the objective in the instance $(G,c,w')$ where $w'$ is the worst $(2,1)$ perturbation for $\bx$ i.e. \[
    w'(u,v) = \begin{cases}
    \frac{1}{2}w(u,v) & \bx(u) = 
    \bx(v)\\
    w(u,v) & \bx(u) \ne \bx(v)
\end{cases}
\]
\end{lemma}

\begin{proof}
Note that $A_h$ here is the same as the one defined for Lemma~\ref{lem:Ahupp}.
Since the instance $(G,c,w)$ is $(2,1,\psi)$-expansion stable, we know that $(G,c',w)$ should be $(2,1)$-expansion stable for all $c'$ such that $c \leq c' \leq c + \psi \cdot \mathbf{1}$. Consider the worst $c'$ for $\bx$ i.e.  $c'(u,i) = \begin{cases}
    c(u,i) + \psi & i = \bx(u)\\
    c(u,i) & i \ne \bx(u)
\end{cases}$. 
Let $f''$ be the objective in the instance $(G,c',w')$. As discussed in section~\ref{sec:finlpt}, we know that any $h \neq \bx$ in the support of the rounding is an expansion move of $\bx$. Therefore, for any $h \neq \bx$ in the support of the rounding of $x'$,

\begin{align*}
    & f''(h) - f''(\bx) > 0 \implies \sum_{u \in V} c'(u,h(u)) - c'(u,\bx(u))  + \sum_{(u,v):h(u)\neq h(v)} w'(u,v) - \sum_{(u,v):\bx(u)\neq \bx(v)} w'(u,v) > 0
    \\&\implies \sum_{u \in V} c(u,h(u)) - \rbr{c(u,\bx(u)) + \psi \cdot \indicator[h(u) \ne \bx(u)] } + \sum_{(u,v):h(u)\neq h(v)} w'(u,v) - \sum_{(u,v):\bx(u)\neq \bx(v)} w'(u,v) > 0
    \\&\implies f'(h) - f'(g) > \psi \cdot \sum_{u \in V} \indicator[h(u) \ne \bx(u)] \implies A_h > \psi \cdot B_h.
\end{align*}
This is true for all $h \neq \bx$ in the support of the rounding of $x'$. When $h = \bx$, we have $A_h = B_h = 0$. Therefore for all $h$ in the support of the rounding of $x'$, we have that $A_h \geq \psi \cdot B_h$.  \end{proof}

\subsection{Final proof of Theorem~\ref{thm:curvature}:}
We use the Lemmas \ref{lem:Ahupp}(upper bound for $\E[A_h]$), \ref{lem:Ahlow}(lower bound for $A_h$), and \ref{lem:Bh}(bound for $\E[B_h]$) to prove Theorem~\ref{thm:curvature}. For all $h$ in the support of rounding of $x$, $A_h \geq \psi \cdot B_h$. Also, 
\[ \E[A_h] \leq \epsilon \rbr{f(x) - f(\bx)},\ \E[B_h] = \epsilon \sum_{u \in V} \frac{1}{2} \norm{x_u - \bx_u}_1  \]

Suppose that $\norm{x - \bx}_1 > \tau \cdot \rbr{f(x) - f(\bx)}$. Then,

\[ \frac{\E[A_h]}{\E[B_h]} \leq \frac{f(x) - f(\bx)}{\sum_{u \in V} \frac{1}{2} \norm{x_u - \bx_u}_1} < \frac{2}{\tau} \]

But since $A_h \geq \psi \cdot B_h$ for every $h$ in the rounding of $x$, we get that $\dfrac{\E[A_h]}{\E[B_h]} \geq \psi$.
\\Setting $\tau = \frac{2}{\psi}$, we get a contradiction and thus we get, \[ \frac{1}{2}\norm{x - \bx}_1 \leq \frac{1}{\psi}\cdot \rbr{f(x) - f(\bx)} = \frac{1}{\psi} \cdot \rbr{\dot{\theta}{x}-\dot{\theta}{\bx}} \]
\end{proof}


\deviation*
\begin{proof}[Proof of Corollary \ref{cor:deviation}]
First, we note that for the nearby stable instance, the MAP and the local LP solutions are the same due to Theorem~\ref{thm:lptes}. Therefore, for any feasible solution $x \in L(G)$, $ \dot{\bar{\theta}}{x} \ge \dot{\bar{\theta}}{\bx}$. In particular, this implies that $\dot{\bar{\theta}}{\hx} \ge \dot{\bar{\theta}}{\bx}$ and $\dot{\bar{\theta}}{\hx^{MAP}} \ge \dot{\bar{\theta}}{\bx}$ since $\hx, \hx^{MAP}$ are also feasible solutions. Remember that we defined $d(\hat{\theta},\bar{\theta}) \coloneqq \sup_{x \in L^*(G)} \abs{ \dot{\hat{\theta}}{x}  - \dot{\bar{\theta}}{x}}$. Therefore,
\[ \dot{\bar{\theta}}{\hx} \leq \dot{\hat{\theta}}{\hx} + d(\hat{\theta},\bar{\theta}) \leq \dot{\hat{\theta}}{\bx} + d(\hat{\theta},\bar{\theta}) \leq \dot{\bar{\theta}}{\bx} + 2d(\hat{\theta},\bar{\theta}). \]
The first and third inequalities hold due to the definition of $d(\hat{\theta},\bar{\theta})$. The second inequality follows from the fact that $\hx$ is the minimizer for $\dot{\hat{\theta}}{x}$ among $x \in L(G)$. Thus, $0 \leq \dot{\bar{\theta}}{\hx} - \dot{\bar{\theta}}{\bx} \leq 2d(\hat{\theta},\bar{\theta})$. From Theorem~\ref{thm:curvature}, we get
$\frac{1}{2}\norm{\hx_V - \bx_V}_1 \le \frac{2d(\hat{\theta},\bar{\theta})}{\psi}$. Thus,
\begin{align*}
    \frac{1}{2}\norm{\hx_V - \hx_V^{MAP}}_1 &\leq \frac{1}{2}\norm{\hx_V - \bx_V}_1 + \frac{1}{2}\norm{\hx_V^{MAP} - \bx_V}_1 \\&\leq \frac{2d(\hat\theta,\bar{\theta})}{\psi} + \frac{1}{2}\norm{\hx_V^{MAP} - \bx_V}_1. 
\end{align*}
\end{proof}

\subsection{Dual-based proof}%of Theorem \ref{thm:curvature}
Here we provide an alternate proof of the curvature result using the dual of the local LP relaxation.
First, we show that the curvature bound is related to the \emph{dual margin} of the instance. 
Then we show that $(2,1,\psi)$-expansion stability implies that the dual margin is at least $\psi$.
Throughout this section, we assume the local LP solution $\hat{x}$ is unique and integral (as guaranteed, for example, by $(2,1)$-expansion stability), so $\hat{x} = \bar{x}$.

Relaxing the
local LP's marginalization constraints in both directions for each edge, we obtain
the following Lagrangian for the local LP:
\[
L(\delta,x) = \sum_{u}\sum_i\left(\theta_u(i) + \sum_{v \in N(u)}\delta_{uv}(i)\right)x_u(i) + \sum_{uv}\sum_{ij}\left(\theta_{uv}(i,j) - \delta_{uv}(i) - \delta_{vu}(j)\right)x_{uv}(i,j)
\]
where each $x_u$ is constrained to be in the $(k-1)$-dimensional simplex, and each $x_{uv}$ the $k^2-1$-dimensional simplex (i.e., the normalization constraints remain). There are no constraints on the dual variables $\delta$. Observe that for any $\delta$ and any primal-feasible $x$, $L(\delta,x) = \langle \theta, x\rangle$. This gives rise to the \emph{reparametrization} view: for a fixed $\delta$, define $\theta^{\delta}_u(i) = \theta_u(i) + \sum_{v\in N(u)}\delta_{uv}(i)$, and $\theta^{\delta}_{uv}(i,j) = \theta_{uv}(i,j) - \delta_{uv}(i) - \delta_{vu}(j)$. Then $L(\delta, x) = \langle \theta^{\delta}, x\rangle$. This will allow us to define equivalent primal problems with simpler structure than the original. $L(\delta,x)$ also gives the dual function:
\[
D(\delta) = \min_x L(\delta,x) = \sum_{u}\min_{i}\left(\theta_u(i) + \sum_{v \in N(u)}\delta_{uv}(i)\right) + \sum_{uv}\min_{i,j}\left(\theta_{uv}(i,j) - \delta_{uv}(i) - \delta_{vu}(j)\right).
\]
A dual point $\delta$ is a dual \emph{solution} if $\delta \in \argmax_{\delta'} D(\delta')$. Theorem \ref{thm:lptes} implies that the local LP has a unique, integral solution when the instance is $(2,1,\psi)$-expansion stable. 
\citet[Theorem 1.3]{sontag2011introduction} show that this implies the existence of a dual solution $\delta^*$ that is \emph{locally decodable} at all nodes $u$: for each $u$, $\argmin_i \theta^{\delta^*}_u(i)$ is unique, and moreover, the edge and node dual subproblems agree:
\begin{equation}
\label{eqn:dual-subproblems-agree}
\left(\argmin_i \theta^{\delta^*}_u(i), \argmin_j \theta^{\delta^*}_v(j)\right) \in \argmin_{i,j}\theta_{uv}^{\delta^*}(i,j).
\end{equation}
In this case, the primal solution defined by ``decoding'' $\delta^*$, $x(u) = \argmin_i \theta^{\delta^*}_u(i)$, is the MAP solution \citep{sontag2011introduction}.

For locally decodable $\delta^*$, we define the \emph{node margin} $\psi_u(\delta^*)$ at a node $u$ as: \[
\psi_u(\delta^*) = \min_{i\ne \argmin_j \theta^{\delta^*}(j)} \theta^{\delta^*}(i) - \min_j \theta^{\delta^*}(j).
\]
This is the difference between the optimal reparametrized node cost at $u$ and the next-smallest cost.
Local decodability of $\delta$ is the property that $\psi_u(\delta) > 0$ for every $u$. 

Together with \eqref{eqn:dual-subproblems-agree}, the following lemma implies that we need only consider locally decodable dual solutions where the optimal primal solution pays zero edge cost.
\begin{lemma}[Dual edge ``removal'']\label{lemma:edge-removal}
  Given a locally decodable dual solution $\delta$, we can transform
  it to a locally decodable dual solution $\delta'$ that satisfies
  $\min_{i,j}\theta_{uv}^{\delta'}(i,j) = 0$ and has the same (additive) margin at every node.
\end{lemma}
\begin{proof}
  Fix an edge $(u,v)$, and consider any pair $i^*,j^*$ in $\argmin_{i,j}\theta_{uv}^{\delta}(i,j)$. Put $\theta^\delta_{uv}(i^*,j^*) = \theta_{uv}(i^*,j^*) + \epsilon$ for $\epsilon \in \mathbb{R}$. Now define $\delta'_{uv}(i) = \delta_{uv}(i) - \epsilon$ for all $i$ (or, equivalently, $\delta'_{vu}(j) = \delta_{vu}(j) - \epsilon$ for all $j$). Because we changed $\theta^{\delta}_u(i)$ by a constant for each $i$, local decodability is preserved and the additive \emph{margin of local decodability} is not changed. We incurred a change of $+\epsilon$ in the dual objective of $\delta$ from the edge term $\min_{i,j} \theta_{uv}^{\delta'}(i,j)$, and a $-\epsilon$ in the objective from the decrease in the node term $\min_i \theta^{\delta'}_u(i)$, so $\delta'$ is still optimal. We can repeat this process for every edge $(u,v)$.
\end{proof}
Lemma \ref{lemma:edge-removal} implies that when $(x^*,\delta^*)$ is a
pair of primal/dual optima and $\delta^*$ is locally decodable, we can assume that $L(x^*,\delta^*) = \sum_u\theta^{\delta^*}_u(x^*_u)$, where we overload notation to define $x^*_u$ to be the label for which $x^*_u(i) = 1$. That is, the primal optimum pays no edge cost in the problem reparametrized by the dual opt $\delta^*$. Finally, Lemma \ref{lemma:edge-removal} implies that we can always assume that $\theta_{uv}^{\delta^*}(i,j) \ge 0$ for all $(u,v),\ (i,j)$. Therefore, if there is any locally decodable dual solution, and the primal LP solution is integral and unique, we may assume there exists a locally decodable dual solution $\delta$ such that $\bar{x}(u) = \argmin_i\theta^{\delta}(i)$, $(\bx(u), \bx(v)) \in \argmin_{i,j}\theta^{\delta}_{uv}(i,j)$, $\theta^{\delta}_{uv}(\bx(u),\bx(v)) = 0$, and $\theta^{\delta}_{uv}(i,j) \ge 0$.

\begin{lemma}[Dual margin implies curvature around $\bx$]
For an instance with objective $\theta$ and MAP solution $\bx$, assume there exists a locally decodable dual solution $\delta$ such that $\bar{x}(u) = \argmin_i\theta^{\delta}(i)$, $(\bx(u), \bx(v)) \in \argmin_{i,j}\theta^{\delta}_{uv}(i,j)$, $\theta^{\delta}_{uv}(\bx(u),\bx(v)) = 0$, and $\theta^{\delta}_{uv}(i,j) \ge 0$. Additionally, let $\psi(\delta) = \min_u\psi_u(\delta)$ be the smallest node margin. Note that $\psi(\delta) > 0$ because $\delta$ is locally decodable.
Then for any $x\in L(G)$,
\[
\frac{1}{2}||x_V-\bx_V||_1 \le \frac{\dot{\theta}{x-\bx}}{\psi(\delta)}
\]
\end{lemma}
\begin{proof}
  Let $\Delta = \dot{\theta}{x - \bx}$. 
  Since $x$ and $\bx$ are both primal-feasible, we have $L(x,\delta) = \dot{\theta}{x}$ and $L(\bx, \delta) = \dot{\theta}{\bx}$. Therefore,
  \begin{equation}\label{eqn:lagrangian-ineq}
  L(x,\delta) = L(\bx,\delta) + \Delta.
  \end{equation}
  Because $\theta^{\delta}(\bx(u),\bx(v)) = 0$ for all $(u,v)$, we have
  \[
  L(\bx,\delta) = \sum_u \theta_u^{\delta}(\bx(u)).
  \]
  Additionally, because $\theta^{\delta}_{uv}(i,j) \ge 0$,
  \[
  L(x,\delta) = \sum_u\sum_i\theta_u^{\delta}(i)x_u(i) + \sum_{uv}\sum_{ij}\theta^{\delta}_{uv}(i,j)x_{uv}(i,j) \ge \sum_u\sum_i\theta_u^{\delta}(i)x_u(i).
  \]
  Combining the above two inequalities with \eqref{eqn:lagrangian-ineq} gives:
  \begin{equation}\label{eqn:node-ineq}
  \sum_u\sum_i\theta_u^{\delta}(i)x_u(i)  \le \sum_u \theta_u^{\delta}(\bx(u)) + \Delta
  \end{equation}
  Because $\delta$ is locally decodable to $\bx$, and the smallest node margin is equal to $\psi(\delta)$, we have that for every $u$,
  $\theta^{\delta}_u(\bx(u)) + \psi(\delta) \le \theta^{\delta}_u(i)$ for all $i\ne \bx(u)$.
  The margin condition implies:
  \[
  \sum_u\theta^{\delta}_u(\bx(u))x_u(\bx(u)) + \sum_u\sum_{i\ne \bx(u)}(\theta^{\delta}_u(\bx(u)) + \psi(\delta))x_u(i) < \sum_u\sum_i\theta_u^{\delta}(i)x_u(i),
  \]
  and simplifying using $\sum_ix_u(i) = 1$ gives:
  \[
  \sum_u\theta^{\delta}_u(\bx(u)) + \psi(\delta)\sum_u\sum_{i\ne \bx(u)}x_u(i) < \sum_u\sum_i\theta_u^{\delta}(i)x_u(i).
  \]
  Plugging in to \eqref{eqn:node-ineq} gives:
  \[
  \sum_u\sum_{i\ne \bx(u)}x_u(i) < \frac{\Delta}{\psi(\delta)}.
  \]
  The left-hand-side is precisely $||x_V-\bx_V||_1 / 2$.
\end{proof}
Now we show that $(2,1,\psi)$-expansion stability implies that there exists a locally decodable dual solution $\delta$ with dual margin $\psi(\delta) \ge \psi$.

\begin{lemma}[$(2,1,\psi)$-expansion stability gives a lower bound on dual margin]
Let $(G,c,w)$ be a $(2,1,\psi)$-expansion stable instance with $\psi > 0$. Then there exists a locally decodable dual solution $\delta$ with dual margin $\psi(\delta) \ge \psi$.
\end{lemma}
\begin{proof}
Define new costs $c_{\psi}$ as
\[
c_\psi(u,i) = \begin{cases}
c(u,i) + \psi & \bx(u) = i\\
c(u,i) & \text{otherwise.}
\end{cases}
\]
By definition, the instance $(G,c_{\psi},w)$ is $(2,1)$-expansion stable (see Definition \ref{def:expansion-stability}). Theorem \ref{thm:lptes} implies the pairwise LP solution is unique and integral on $(G,c_{\psi},w)$. This implies there exists a dual solution $\delta^0$ that is locally decodable to $\bx$. The only guarantee on the dual margin of $\delta^0$ is that $\psi(\delta^0) > 0$. But note that $\delta^0$ is also an optimal dual solution for $(G,c,w)$, since its objective in that instance is the same as the objective of $\bx$. But in that instance, the dual margin at every node is at least $\psi$, because $c_{\psi}(u,\bx(u)) - c(u,\bx(u)) = \psi$. So $\psi(\delta) \ge \psi$.
\end{proof}

These two lemmas directly imply Theorem \ref{thm:curvature}. This dual proof is slightly more general than the primal proof, since the curvature result applies to any $x\in L(G)$.




\section{Details for Generative model}\label{sec:random-model_details}

\begin{definition}[sub-Gaussians and $(b,\sigma)$-truncated sub-Gaussians] \label{def:subG}
Suppose $b \in \R, \sigma \in \R_+$.
A random variable $X$ with mean $\mu$ is sub-Gaussian with parameter $\sigma$ if and only if  $\E[e^{\lambda(X-\mu)}]\le \exp(\lambda^2 \sigma^2/2)$ for all $\lambda \in \R$.
The random variable $X$ is $(b,\sigma)$-truncated sub-Gaussian if and only if $X$ is supported in $(b,\infty)$ and $X$ is sub-Gaussian with parameter $\sigma$. 
\end{definition}

We remark that the above definition captures many well-studied families of bounded random variables e.g., Rademacher distributions, uniform distributions on an interval etc. We remark that a bounded random variable supported on $[-M,M]$ is also sub-Gaussian with parameter $M$. However in our setting, it needs to be truncated only on negative side, and the bound $M$ will be much larger than the variance parameter $\sigma$; the bound is solely to ensure non-negativity of edge costs. A canonical example to keep in mind is a truncated Gaussian distribution.%\anote{What parameters?}  
We use the following standard large deviations bound for sums of sub-Gaussian random variables (for details, refer to Thm 2.6.2 from \citet{vershynin_hdp}). Given independent r.v.s $X_1, X_2, \dots, X_n$, with $X_i$ drawn from a sub-Gaussian with parameter $\sigma_i$ we have for $\mu=\sum_{i=1}^n \E[X_i]$ and $\sigma^2 = \sum_{i=1}^n \sigma_i^2$,
\begin{equation}\label{eq:deviationbound}
    \Pr\Big[ \Big| \sum_{i=1}^n X_i - \mu \Big| \ge t \Big] \le 2\exp\Big(-\frac{t^2}{2\sigma^2} \Big).
\end{equation} 

\dswhp*

\begin{proof}
As discussed in section~\ref{sec:expansion-stability_details}, for any $x \in L^*(G)$, the objective of the local LP can be written as \[\sum_{u\in V}\sum_i c(u,i)x_u(i) + \sum_{(u,v) \in E}w(u,v)d(u,v)\]
where $d(u,v) \coloneqq \frac{1}{2}\sum_{i}\abs{x_u(i) - x_v(i)}$. Let $\hat f(x) \coloneqq \dot{\hat{\theta}}{x}, \bar{f}(x) \coloneqq \dot{\bar{\theta}}{x}$. Then,

\begin{align*}
\abs{\dot{\hat{\theta}}{x}  - \dot{\bar{\theta}}{x}} &= \abs{\hat{f}(x) - \bar{f}(x)} = \Big|\sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i) + \sum_{(u,v) \in E} \tw(u,v)d(u,v)\Big|
\end{align*}

For any feasible LP solution $x$, consider the following rounding algorithm $\calR$: 

\begin{algorithm}[H]
   \caption{$\calR$ rounding}
   \label{alg:R_rounding}
\begin{algorithmic}[1]
   \FOR{each $i \in \calL$}
        \STATE Choose $r_i \in (0,1)$ uniformly at random.
        \FOR{each $u \in V$}
            \IF {$x_u(i) > r_i$}
                \STATE $\calR(x)_u(i) = 1$.
            \ELSE
                \STATE $\calR(x)_u(i) = 0$
            \ENDIF 
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}

Then, we have
\begin{align*}
\E[\hat f(\calR(x)) - \bar{f}(\calR(x))] &= \sum_{u \in V} \sum_{i\in L} \tc(u,i) \E[\indicator[\calR(x)_u(i) = 1]] + \sum_{(u,v) \in E} \frac{\tw(u,v)}{2}\sum_{i} \E[\indicator[\calR(x)_u(i) \neq \calR(x)_v(i)]]
\\&= \sum_{u \in V} \sum_{i\in L} \tc(u,i) \Pr[x_u(i) > r_i] + \sum_{(u,v) \in E} \frac{\tw(u,v)}{2}\sum_{i} \Pr[\min\rbr{x_u(i), x_v(i)} \leq r_i < \max\rbr{x_u(i),x_v(i)}]
\\&= \sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i) + \sum_{(u,v) \in E} \frac{\tw(u,v)}{2}\sum_{i} \abs{x_u(i) - x_v(i)}
\\&= \sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i) + \sum_{(u,v) \in E} \tw(u,v)d(u,v) = \hat f(x) - \bar{f}(x) 
\end{align*}

Therefore,
\[\sup_{x \in L^*(G)}\abs{\hat f(x) - \bar{f}(x)} = \sup_{x \in L^*(G)} \abs{\E[\hat f(\calR(x)) - \bar{f}(\calR(x))] } \leq \sup_{\hat x_V \in \{0,1\}^{nk}} \abs{\hat{f}(\hat x_V) - \bar{f}(\hat x_V)}\]

Note that for all $x \in L^*(G)$, $\hat{f}(x)$ and $\bar{f}(x)$ only depend on the portion of $x$ restricted to the vertices i.e. $x_V$. This is why we only need to look at $\hat{x}_v \in \{0,1\}^{nk}$ for the last inequality.

For any fixed $\hat x_V \in \{0,1\}^{nk}$, since $\tw(u,v), \tc(u,i)$ are all mean $0$ and sub-Gaussian with parameters $\gamma_{u,v}, \sigma_{u,i}$, we have for any $t > 0$,
\[ \Pr \left[\abs{\hat f(\hat x_V) - \bar{f}(\hat x)} > t\right] \leq 2\exp\left(\frac{-t^2}{2\rbr{\sum_{u,i}\sigma_{u,i}^2 + k^2/4\sum_{uv}\gamma_{u,v}^2}}\right) \]

Taking $t = c\sqrt{nk}\sqrt{\sum_{u,i}\sigma_{u,i}^2 + k^2/4\sum_{uv}\gamma_{u,v}^2}$, we get that for any fixed $\hat x_V \in \{0,1\}^{nk}$,

\[ \Pr \left[\abs{\hat f(\hat x) - \bar{f}(\hat x)} > t\right] \leq 2\exp\left(-c^2 nk\right) \]

Taking a union bound over $\{0,1\}^{nk}$, we get that

\[ \Pr \left[ \sup_{\hat x_V \in \{0,1\}^{nk}}  \abs{\hat f(\hat x) - \bar{f}(\hat x)} > t\right] \leq 2\exp\left(nk\rbr{\log 2 - c^2}\right) \]
\end{proof}

Here, $c$ needs to be greater than $\sqrt{\ln 2} \approx 0.83$ to get a high probability guarantee.

\mapreg*
\begin{proof}
From Theorem~\ref{thm:apmap}, we have that, with high probability over the random noise \[ \frac{1}{2}\norm{\hx_V - \bar{x}_V}_1 \le \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\sum_{u,i}\sigma_{u,i}^2 + \frac{k^2}{4}\sum_{uv}\gamma_{u,v}^2} \]
In this setting, this leads to 
\[ \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\sum_{u,i}\sigma_{u,i}^2 + \frac{k^2}{4}\sum_{uv}\gamma_{u,v}^2} = \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\rho nk\sigma^2 + \eta \frac{k^2}{4} \frac{nd}{2} \gamma^2}
    = \frac{2cnk\sqrt{\rho \sigma^2 + \frac{\eta dk}{8} \gamma^2}}{\psi} \]
since $\abs{V} = n, \abs{L} = k, \text{ and } \abs{E} = \frac{nd}{2}$.

\iffalse
For any $x \in L^*(G)$,
\begin{align*}
    \hat f(x) - \bar{f}(x) &= \sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i) + \sum_{(u,v) \in E} \tw(u,v)d(u,v)
    \\&= \frac{1}{2} \rbr{\sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i) + 2\sum_{(u,v) \in E} \tw(u,v)d(u,v)} + \frac{1}{2} \sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i)
    \\&= \frac{1}{2}\E[\hat{f}(\calR(x)) - \bar{f}(\calR(x))] + \frac{1}{2} \sum_{u \in V} \sum_{i\in L} \tc(u,i) x_u(i)
\end{align*}
Therefore,
\begin{align*}
\sup_{x \in L^*(G)}\abs{\hat f(x) - \bar{f}(x)} &= \sup_{x \in L^*(G)} \Big| \frac{1}{2} \E[\hat f(\calR(x)) - \bar{f}(\calR(x))] + \frac{1}{2}\sum_{u \in V} \sum_{i \in L} \tc(u,i)x_u(i) \Big|
\\&\leq \sup_{x \in L^*(G)} \Big| \frac{1}{2} \E[\hat f(\calR(x)) - \bar{f}(\calR(x))]\Big| + \sup_{x \in L^*(G)} \Big|\frac{1}{2}\sum_{u \in V} \sum_{i \in L} \tc(u,i)x_u(i) \Big|
\\&\leq \sup_{\hat x_V \in \{0,1\}^{nk}} \Big| \frac{1}{2} \rbr{\hat{f}(\hat x_V) - \bar{f}(\hat x_V)} \Big| + \sup_{\hat x_V \in \{0,1\}^{nk}} \Big|\frac{1}{2}\sum_{u \in V} \sum_{i \in L} \tc(u,i)\hat x_u(i) \Big|
\end{align*}

Note that for all $x \in L^*(G)$, $\hat{f}(x)$ and $\bar{f}(x)$ only depend on the portion of $x$ restricted to the vertices i.e. $x_V$. This is why we only need to look at $\hat{x}_v \in \{0,1\}^{nk}$ for the last inequality.

For any fixed $\hat x_V \in \{0,1\}^{nk}$, since $\tw(u,v), \tc(u,i)$ are all mean $0$ and sub-Gaussian with parameters $\gamma_{u,v}, \sigma_{u,i}$, we have for any $t > 0$,
\[ \Pr \left[\Big| \frac{1}{2} \rbr{\hat f(\hat x_V) - \bar{f}(\hat x)} \Big| > t\right] \leq 2\exp\left(\frac{-2t^2}{\sum_{u,i}\sigma_{u,i}^2 + k^2\sum_{uv}\gamma_{u,v}^2}\right) \]

Taking $t = c\sqrt{nk}\sqrt{\sum_{u,i}\sigma_{u,i}^2 + k^2\sum_{uv}\gamma_{u,v}^2}$, we get that for any fixed $\hat x_V \in \{0,1\}^{nk}$,

\begin{align*}
    \Pr \left[\Big| \frac{1}{2} \rbr{\hat f(\hat x) - \bar{f}(\hat x)} \Big| > t\right] &\leq 2\exp\left(-2c^2 nk\right)
    \\\Pr \left[\Big|\frac{1}{2}\sum_{u \in V} \sum_{i \in L} \tc(u,i)\hat x_u(i) \Big| > t\right] &\leq 2\exp\left(-2c^2 nk\right)
\end{align*}

We get the second bound from the fact that its sub-Gaussian parameter can only be smaller than the sub-Gaussian parameter of the first bound.

Taking a union bound over $\{0,1\}^{nk}$, we get that

\begin{align*}
    \Pr \left[ \sup_{\hat x_V \in \{0,1\}^{nk}}  \Big| \frac{1}{2} \rbr{\hat f(\hat x) - \bar{f}(\hat x)} \Big| > t\right] &\leq 2\exp\left(nk\rbr{\log 2 - 2c^2}\right)
    \\\Pr \left[\sup_{\hat x_V \in \{0,1\}^{nk}} \Big|\frac{1}{2}\sum_{u \in V} \sum_{i \in L} \tc(u,i)\hat x_u(i) \Big| > t\right] &\leq 2\exp\left(nk\rbr{\log 2 - 2c^2}\right)
\end{align*}

Taking another union bound for these two terms, we get 
\[ \Pr \left[\sup_{x \in L^*(G)}\abs{\hat f(x) - \bar{f}(x)} > 2c\sqrt{nk}\sqrt{\sum_{u,i}\sigma_{u,i}^2 + k^2\sum_{uv}\gamma_{u,v}^2} \right] \leq 4\exp\left(nk\rbr{\log 2 - 2c^2}\right) \]
\end{proof}

\mapreg*
\begin{proof}
From Theorem~\ref{thm:apmap}, we have that, with high probability over the random noise \[ \frac{1}{2}\norm{\hx_V - \bar{x}_V}_1 \le \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\sum_{u,i}\sigma_{u,i}^2 + k^2\sum_{uv}\gamma_{u,v}^2} \]
In this setting, this leads to 
\[ \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\sum_{u,i}\sigma_{u,i}^2 + k^2\sum_{uv}\gamma_{u,v}^2} = \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\rho nk\sigma^2 + \eta k^2 \frac{nd}{2} \gamma^2}
    = \frac{2cnk\sqrt{\rho \sigma^2 + \frac{\eta dk}{2} \gamma^2}}{\psi} \]
since $\abs{V} = n, \abs{L} = k, \text{ and } \abs{E} = \frac{nd}{2}$.
\fi


\end{proof}

\section{Algorithm for finding nearby stable instances details }
\label{sec:algorithm_details}
Let $\bx$ be a MAP solution, and let $\calE^{\bx}$ be the set of expansions of $\bx$.
We prove that an instance is $(2,1,\psi)$-expansion stable if and only if $\dot{\theta_{adv}}{\bx} \le \dot{\theta_{adv}}{x}$ for all $x \in \calE^{\bx}$. In other words, it is sufficient to check for stability in the \emph{adversarial} perturbation for $\bx$. 
This proves that we need not check every possible perturbation when finding a $(2,1,\psi)$-expansion stable instance.
\begin{claim}\label{claim:adv-enough}
Let $(G,c,w)$ be an instance of uniform metric labeling with MAP solution $\bx$. Define:
\begin{equation*}
    w_{adv}(u,v) = \begin{cases}
    \frac{1}{2}w_{uv} & \bx(u) = \bx(v)\\
    w(u,v) & \bx(u) \ne \bx(v)
    \end{cases}
\end{equation*}
\begin{equation*}
    c_{adv}(u,i) = \begin{cases}
        c(u,i) + \psi & \bx(u) = i\\
        c(u,i) & \text{otherwise.}
    \end{cases}
\end{equation*}
Let $\theta_{adv}$ be the objective vector in the instance $(G,c_{adv},w_{adv})$. Then
\[
\dot{\theta'}{\bx} \le \dot{\theta'}{x}
\]
for all $(2,1,\psi)$-perturbations $\theta'$ of $\theta$ and all $x\in \calE^{\bx}$ if and only if:
\[
\dot{\theta_{adv}}{\bx} \le \dot{\theta_{adv}}{x}
\]
for all $x\in \calE^{\bx}$.
\end{claim}
\begin{proof}
    The proof is analogous to that of Claim \ref{exp_worst}. If the instance is $(2,1,\psi)$-expansion stable, then $\dot{\theta'}{x} - \dot{\theta'}{\bx} \ge 0$ for all $(2,1,\psi)$-perturbations $\theta'$ and all expansions $x$ of $\bx$. Because $\theta_{adv}$ is a valid $(2,1,\psi)$-perturbation, this gives one direction. For the other, note that if the instance is not $(2,1,\psi)$-expansion stable, there exists a $\theta'$ and an $x\in\calE^{\bx}$ for which $\dot{\theta'}{x} - \dot{\theta'}{\bx} < 0$. A direct computation shows that $\dot{\theta'}{x} - \dot{\theta'}{\bx} \ge \dot{\theta_{adv}}{x} - \dot{\theta_{adv}}{\bx}$ for all $(2,1,\psi)$-perturbations $\theta'$ of $\theta$. Then we have $\dot{\theta_{adv}}{x} - \dot{\theta_{adv}}{\bx} < 0$.
\end{proof}
This claim justifies \eqref{eqn:alg}, which only enforces that $\bx$ is at least as good as all of its expansions in $\theta_{adv}$. The following claim implies that there is always a feasible point of \eqref{eqn:alg} that makes modifications of bounded size to $c$ and $w$.
\begin{claim}
Consider an instance $(G,c,w)$ with a unique MAP solution $\bx$. Let $w'$ be defined as
\begin{equation*}
    w'(u,v) = \begin{cases}
    w(u,v) & \bx(u) \ne \bx(v)\\
    2w(u,v) & \bx(u) = \bx(v),
    \end{cases}
\end{equation*}
and let $c'$ be defined as
\begin{equation*}
c'(u,i) = \begin{cases}c(u,i) - \psi & \bx(u) = i\\
c(u,i) & \bx(u) \ne i.\end{cases}
\end{equation*}
Then the instance $(G,c',w')$ is $(2,1,\psi)$-expansion stable with MAP solution $\bx$.
\end{claim}
\begin{proof}[Proof (sketch)]
The original MAP solution $\bx$ is also the MAP solution to $(G,c',w')$. Then the original instance $(G,c,w)$ is obtained from $(G,c',w')$ by performing the adversarial $(2,1,\psi)$-perturbation for $\bx$ (see Claim \ref{claim:adv-enough}). Because $\bx$ was the unique MAP solution to this instance, it has better objective than all of its expansions. Therefore, $(G,c',w')$ is $(2,1,\psi)$-expansion stable, by Claim \ref{claim:adv-enough}.
\end{proof}
$(G,c',w')$ is a ``nearby'' stable instance to $(G,c,w)$, but it requires changes to quite a few edges---every edge that is not cut by $\bx$---and changes the node costs of every vertex. Surprisingly, the stable instances we found in Section \ref{sec:experiments} were much closer than $(G,c',w')$---that is, only sparse changes were required to transform the observed instance $(G,c,w)$ to a $(2,1,\psi)$-expansion stable instance.

\section{Experiment details}\label{sec:experiments_details}
In this section, we give more details for the numerical examples for which we evaluate our curvature bound from Theorem \ref{thm:curvature}. We studied instances for stereo vision, where the input is two images taken from slightly offset locations, and the desired output is the disparity of each pixel location between the two images (this disparity is inversely proportional to the depth of that pixel). We used the models from \citet{tappen2003comparison} on three images from the Middlebury stereo dataset \citep{scharstein2002taxonomy}. In this model, $G$ is a grid graph with one node corresponding to each pixel in one of the images (say, the one taken from the left), the costs $c(u,i)$ are set using the Birchfield-Tomasi matching costs \citep{birchfield1998pixel}, and the edge weights $w(u,v)$ are set as:
\[
w(u,v) = \begin{cases}
P \times s & |I(u) - I(v)| < T\\
s & \text{otherwise}.
\end{cases}
\]
Here $I(u)$ is the intensity of one of the images (again, say the left image) at pixel location $u$, and we set $(P,T,s) = (2,50,4)$. This is a Potts model. The {\tt tsukuba}, {\tt cones}, and {\tt venus} images were {\tt 120 x 150}, {\tt 125 x 150}, and {\tt 383 x 434}, respectively. These models had $k=7$, $k=5$, and $k=5$, respectively.

To generate Table \ref{tbl:boundtable}, we ran the algorithm in \eqref{eqn:alg} using Gurobi \citep{gurobi} for the L1 distance. For each observed instance $(G,\hc,\hw)$, this output a nearby $(2,1,\psi)$-stable instance $(G,\bc,\bw)$. In all of our experiments, we used $\psi=1$. Additionally, we always set the target MAP solution $x^t$ in \eqref{eqn:alg} to be equal to the observed MAP solution $\hx^{MAP}$. To evaluate our recovery bound, we compared the objective of the observed LP solution $\hat{x}$ to the $\hx^{MAP}$ in $(G,\bc,\bw)$. That is, if $\bar{\theta}$ is the objective for $(G,\bc,\bw)$, we computed $\dot{\bar{\theta}}{\hx} - \dot{\bar{\theta}}{\hx^{MAP}} = \dot{\bar{\theta}}{\hx} - \dot{\bar{\theta}}{\bx}$, where the second equality is because we set the target solution $x^t$ to be equal to $\hx^{MAP}$, so $\hx^{MAP} = \bx$. Because $\psi=1$, the difference between these two objectives is precisely the value of our curvature bound. In particular, Theorem \ref{thm:curvature} guarantees that \[
\frac{1}{2n}||\hx_V - \hx^{MAP}_V||_1 \le \frac{1}{n}\left(\dot{\bar{\theta}}{\hx} - \dot{\bar{\theta}}{\hx^{MAP}}\right).
\]
The right-hand-side is shown for these instances in the ``Recovery error bound'' column of Table \ref{tbl:boundtable}, and the true value of $\frac{1}{2n}||\hx_V - \hx^{MAP}_V||_1$ (i.e., the true recovery error) is shown in the identically titled column of Table \ref{tbl:boundtable}.
On these instances, $\frac{1}{n}\left(\dot{\bar{\theta}}{\hx} - \dot{\bar{\theta}}{\hx^{MAP}}\right)$ is close to 0, so our curvature bound ``explains'' a large portion of $\hx$'s recovery of $\hx^{MAP}$. These instances are close to $(2,1,\psi)$-stable instances where $\hx$ and $\hx^{MAP}$ have close objective, and this implies by Theorem \ref{thm:curvature} that $\hx$ approximately recovers $\hx^{MAP}$. 

However, this result relies on a property of the LP solution $\hx$: that it has good objective in the stable instance discovered by the procedure \eqref{eqn:alg}. Compare this to Corollary \ref{cor:deviation}, which only depends on properties of the observed instance $\hat{\theta}$ and the stable instance $\bar{\theta}$ (in particular, some notion of ``distance'' between them). Given an observed instance $\hat{\theta}$ and stable instance $\bar{\theta}$, we can try to compute $d(\bar{\theta}, \hat{\theta})$ from Corollary \ref{cor:deviation} to give a bound that does not depend on $\hat{x}$. Unfortunately, this distance can be large, leading to a bound that can be vacuous (i.e., normalized Hamming recovery $> 1$). The following refinement of Corollary \ref{cor:deviation} gives much tighter bounds.

\begin{restatable}[LP solution is good if there is a nearby stable instance, refined]{corollary}{deviation2}\label{cor:deviation2}
Let $\hx^{MAP}$ and $\hx$ be the MAP and local LP solutions to an observed instance $\obsins$. Also, let $\bar{x}$ be the MAP solution for a latent $(2,1,\psi)$-expansion stable instance $\stabins$. If $\hat{\theta} = (\hat{c},\hat{w})$ and $\bar{\theta} = (\bar{c}, \bar{w})$, define
\[
d(\bar{\theta}, \hat{\theta}) \coloneqq \sup_{x \in L^*(G)} \dot{\bar{\theta}}{x}  - \dot{\bar{\theta}}{\bx} - (\dot{\hat{\theta}}{x}  - \dot{\hat{\theta}}{\bx}).
\]
Note that while we still use the name $d(\cdot, \cdot)$, evoking a metric, $d$ is not symmetric. Then
\[ \frac{1}{2}\norm{\hx_V - \hx^{MAP}_V}_1 \le \frac{d(\bar{\theta}, \hat{\theta})}{\psi} + \frac{1}{2}\norm{\hx_V^{MAP} - \bx_V}_1. \]
\end{restatable}
\begin{proof}
Note:
\begin{align*}
    \frac{1}{2}\norm{\hx_V - \hx_V^{MAP}}_1 &\leq \frac{1}{2}\norm{\hx_V - \bx_V}_1 + \frac{1}{2}\norm{\hx_V^{MAP} - \bx_V}_1.
\end{align*}
By the definition of $d$, for any $x \in L^*(G)$, 
\[\dot{\bar{\theta}}{x}  - \dot{\bar{\theta}}{\bx} \le d(\bar{\theta},\hat{\theta}) + (\dot{\hat{\theta}}{x}  - \dot{\hat{\theta}}{\bx}). \]
Now if we set $x = \hx$, the LP solution to the observed instance, we have $\dot{\hat{\theta}}{\hx}  - \dot{\hat{\theta}}{\bx}) \le 0$, so 
\[
\dot{\bar{\theta}}{\hx}  - \dot{\bar{\theta}}{\bx} \le d(\bar{\theta},\hat{\theta}).
\]
Theorem \ref{thm:curvature} then implies $\frac{1}{2}\norm{\hx_V - \bx_V}_1 \le d(\bar{\theta}, \hat{\theta})/\psi$, which gives the claim.
\end{proof}
Given an observed instance $\hat{\theta}$ and a $(2,1,\psi)$-expansion stable instance $\bar{\theta}$ output by \eqref{eqn:alg} with $\bar{x}^{MAP}$ = $\hat{x}^{MAP}$, we can upper bound $d(\bar{\theta}, \hat{\theta})$ by computing 
\[
d_{up}(\bar{\theta}, \hat{\theta}) \coloneqq \sup_{x \in L(G)} \dot{\bar{\theta}}{x}  - \dot{\bar{\theta}}{\bx} - (\dot{\hat{\theta}}{x}  - \dot{\hat{\theta}}{\bx}),
\]
which is a linear program in $x$ because we relaxed $L^*(G)$ to $L(G)$. Corollary \ref{cor:deviation2} then implies that the recovery error of $\hat{x}$ is at most $d_{up}(\bar{\theta},\hat{\theta}) / \psi$, which we can compute. Table \ref{tbl:boundtable2} shows the results of this procedure on two of the same instances from Table \ref{tbl:boundtable} in the ``Unconditional bound'' column. While the values of this bound are much larger than the ``Curvature bound'' of Theorem 5.2, they are much more theoretically appealing, since they only depend on the difference between $\hat{\theta}$ and $\bar{\theta}$ rather than on a property of the LP solution $\hat{x}$ to $\hat{\theta}$. For Table \ref{tbl:boundtable2}, we did a grid search for $\psi$ over $\{1,\ldots, 10\}$; $\psi=4$ gave the optimal unconditional bound for both instances. The difference in $\psi$ explains the  slight differences between the other columns of Tables \ref{tbl:boundtable} and \ref{tbl:boundtable2}.

\begin{table*}[ht]
     \centering
     \caption{Results from the output of \eqref{eqn:alg} on two stereo vision instances. Curvature bound shows the bound obtained from Theorem \ref{thm:curvature}, which depends on the observed LP solution $\hat{x}$. Unconditional bound shows the bound from the refined version of Corollary \ref{cor:deviation}, which depends \emph{only} on the observed instance and the stable instance. This ``unconditional'' bound explains a reasonably large fraction of the LP solution's recovery for these instances: because the instance is close to a stable instance, the LP solution approximate recovers the MAP solution.}
     \begin{tabular}{lccccc}
          Instance & Costs changed & Weights changed & Curvature bound & Unconditional bound & $||\hat{x}_V - \hat{x}^{MAP}_V||_1/2n$ \\
          \toprule
          ${\tt tsukuba}$ & 4.9\% & 2.8\% & 0.0173 & 0.4878 & 0.0027\\
          ${\tt cones}$ & 2.81\% & 2.31\% & 0.0137 & 0.2819 & 0.0022\\
          \bottomrule
     \end{tabular}
     \label{tbl:boundtable2}
 \end{table*}