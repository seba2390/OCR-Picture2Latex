\iffalse
\documentclass{article}
\usepackage[round]{natbib}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath,amssymb, amsthm}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\title{Solving stable instances corrupted with noise: dual notes}

\newcommand{\argmax}{\mathop{\mathrm{arg\,max}{}}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}{}}}
\newcommand{\cI}{\mathcal{I}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\maketitle
\fi

\section{Background}
We are given a graph $G = (V,E)$, with $|V| = n$, $|E| = m = nd$ for
average degree $d$ (we will assume $d$ relatively large). Fix a
constant $k$. A \emph{labeling} of $G$ is a map $g: V\to [k]$.
For each labeling $g$, define the indicator functions
\[
\mathbb{I}_{u;i}(g) = \begin{cases}
  1 & g(u) = i\\
  0 & \mbox{otherwise}.
\end{cases}\qquad
\mathbb{I}_{uv;ij}(g) = \begin{cases}
  1 & g(u) = i, g(v) = j\\
  0 & \mbox{otherwise}.
  \end{cases}
\]
Each labeling $g$ thus defines a point $\phi(g)$ in $\{0,1\}^{nk + mk^2}$.
The \emph{marginal polytope} $M(G)$ is defined as the convex hull of all $\phi(g)$:
\[
M(G) \triangleq \mbox{conv}\left(\{\phi(g)\ |\ g: V\to L\}\right).
\]
Denote the coordinates of a point in $x\in M(G)$ as $x_u(i), x_{uv}(i,j)$,
with $u \in V$, $uv \in E$, and $i$ and $j$ in $[k]$.
The MAP inference problem can be written as a linear optimization over $M(G)$:
\[
x^* = \argmin_{x\in M(G)} \langle\theta, x\rangle
\]
This polytope often lacks an efficient description, and optimizing a linear function over it is NP-hard in general. A typical relaxation to the MAP inference problem is to solve:
\[
x^* = \argmin_{x \in L(G)} \langle\theta, x\rangle,
\]
where $L(G)$ is the \emph{local polytope} defined by the constraints:
\begin{align*}
  \sum_{j} x_{uv}(i,j) = x_u(i) &\qquad \forall (u,v) \in E, i \in [k]\\
  \sum_{i} x_{uv}(i,j) = x_v(j) &\qquad \forall (u,v) \in E, j \in [k]\\
  \sum_{i} x_{u}(i) = 1 &\qquad \forall u \in V\\
  \sum_{ij} x_{uv}(i,j) = 1 &\qquad \forall (u,v) \in E\\
  x_{uv}(i,j) \in [0,1] &\qquad \forall (u,v) \in E, (i,j) \in [k]\times [k]\\
  x_{u}(i) \in [0,1] &\qquad \forall u\in V,\ i\in [k].
\end{align*}
The first two sets of constraints are called \emph{marginalization
  constraints}, and ensure the edge variables $x_{uv}(i,j)$ locally
match the ``marginals'' $x_u(i)$ and $x_v(j)$. The second two sets are \emph{normalization} constraints that ensure the $x_u$ and $x_{uv}$ variables sum to 1. Note that there are some redundant constraints in this formulation. Relaxing the
marginalization constraints in both directions for each edge, we obtain
the following Lagrangian:
\[
L(\delta,x) = \sum_{u}\sum_i\left(\theta_u(i) + \sum_{v \in N(u)}\delta_{uv}(i)\right)x_u(i) + \sum_{uv}\sum_{ij}\left(\theta_{uv}(i,j) - \delta_{uv}(i) - \delta_{vu}(j)\right)x_{uv}(i,j)
\]
where each $x_u$ is constrained to be in the $(k-1)$-dimensional simplex, and each $x_{uv}$ the $k^2-1$-dimensional simplex (i.e., the normalization constraints remain). Observe that for any $\delta$ and any primal-feasible $x$, $L(\delta,x) = \langle \theta, x\rangle$. This gives rise to the \emph{reparametrization} view: for a fixed $\delta$, define $\theta^{\delta}_u(i) = \theta_u(i) + \sum_{v\in N(u)}\delta_{uv}(i)$, and $\theta^{\delta}_{uv}(i,j) = \theta_{uv}(i,j) - \delta_{uv}(i) - \delta_{vu}(j)$. Then $L(\delta, x) = \langle \theta^{\delta}, x\rangle$. This will allow us to define equivalent primal problems with simpler structure than the original. $L(\delta,x)$ also gives the dual function:
\[
D(\delta) = \min_x L(\delta,x) = \sum_{u}\min_{x_u}\left(\theta_u(i) + \sum_{v \in N(u)}\delta_{uv}(i)\right) + \sum_{uv}\min_{x_{uv}}\left(\theta_{uv}(i,j) - \delta_{uv}(i) - \delta_{vu}(j)\right)x_{uv}(i,j).
\]
Many solvers for the relaxed primal problem instead solve:
\[
\max_{\delta} D(\delta).
\]
\citet{LSV18} prove that if the instance $\mathcal{I}_\theta$ is $(2,1)$-stable, the primal LP has a unique, integer-valued solution. \citet{sontag2011introduction} show that this implies the existence of a dual solution $\delta^*$ that is \emph{locally decodable} at all nodes $u$: for each $u$, $\argmin_i \theta^{\delta^*}_u(i)$ is unique.

\begin{lemma}[Dual edge removal]\label{lemma:edge-removal}
  Given a locally decodable dual solution $\delta$, we can transform
  it to a locally decodable dual solution $\delta'$ that satisfies
  $\min_{i,j}\theta_{uv}^{\delta'}(i,j) = 0$ and has the same (additive) margin at every node.
\end{lemma}
\begin{proof}
  Fix an edge $(u,v)$, and consider any pair $i^*,j^*$ in $\argmin_{i,j}\theta_{uv}^{\delta}(i,j)$. Put $\theta^\delta_{uv}(i^*,j^*) = \theta_{uv}(i^*,j^*) + \epsilon$ for $\epsilon \in \mathbb{R}$. Now define $\delta'_{uv}(i) = \delta_{uv}(i) - \epsilon$ for all $i$ (or, equivalently, $\delta'_{vu}(j) = \delta_{vu}(j) - \epsilon$ for all $j$). Because we changed $\theta^{\delta}_u(i)$ by a constant for each $i$, local decodability is preserved. We incurred a change of $+\epsilon$ in the dual objective of $\delta$ from the edge term $\min_{i,j} \theta_{uv}^{\delta'}(i,j)$, and a $-\epsilon$ in the objective from the decrease in the node term $\min_i \theta^{\delta'}_u(i)$, so $\delta'$ is still optimal. We can repeat this process for every edge $(u,v)$.
\end{proof}
Lemma \ref{lemma:edge-removal} implies that when $(x^*,\delta^*)$ is a
pair of primal/dual optima and $\delta^*$ is locally decodable, we can assume that $L(x^*,\delta^*) = \sum_u\theta^{\delta^*}_u(x^*_u)$, where we overload notation to define $x^*_u$ to be the label for which $x^*_u(i) = 1$. That is, the primal optimum pays no edge cost in the problem reparametrized by the dual opt $\delta^*$. Finally, Lemma \ref{lemma:edge-removal} implies that we can always assume that $\theta_{uv}^{\delta^*}(i,j) \ge 0$ for all $(u,v),\ (i,j)$.

\section{Stable + noise}
%In this section we prove the following theorem for approximate MAP
%recovery on stable instances corrupted with noise.

\begin{lemma}[Hamming distance bound]\label{lem:hamming}
  An instance  $\cI_\theta$ where there exists an (optimal) dual
  solution $\delta$ that is locally decodable at every node has an integral primal solution $x^*$; we set $x^*_u = \argmax_i
  x^*_u(i)$.
  Assume $\cI_\theta$ has the following additional property: $\delta$ is locally decodable by a \emph{margin} $\eta$. That is, for every $u \in V$ and every $i \ne x^*_u$,
  \[
  \theta_u^{\delta}(i) > \theta_u^{\delta}(x^*_u) + \eta.
  \]  
  Then if we have a feasible primal point $x$ satisfying
  \[
  \langle \theta, x^*\rangle \le \langle \theta, x\rangle \le \langle \theta, x^*\rangle + \Delta,
  \]
  The Hamming distance between $x$ and $x^*$ is bounded by:
  \[
  \frac{1}{2}||x-x^*||_1 \le \frac{\Delta}{\eta}.
  \]
\end{lemma}
\begin{proof}
  Since $x$ and $x^*$ are both primal-feasible, we have:
  \begin{equation}\label{eqn:lagrangian-ineq}
  L(x,\delta) \le L(x^*,\delta) + \Delta.
  \end{equation}
  Lemma 1.1 implies that
  \[
  L(x^*,\delta) = \sum_u \theta_u^{\delta}(x^*_u).
  \]
  Because Lemma 1.1 also implies that $\theta^{\delta}_{uv}(i,j) \ge 0$, we also have that
  \[
  L(x,\delta) = \sum_u\sum_i\theta_u^{\delta}(i)x_u(i) + \sum_{uv}\sum_{ij}\theta^{\delta}_{uv}(i,j)x_{uv}(i,j) \ge \sum_u\sum_i\theta_u^{\delta}(i)x_u(i).
  \]
  Combining with \eqref{eqn:lagrangian-ineq} gives:
  \begin{equation}\label{eqn:node-ineq}
  \sum_u\sum_i\theta_u^{\delta}(i)x_u(i)  \le \sum_u \theta_u^{\delta}(x^*_u) + \Delta
  \end{equation}
  The margin condition implies:
  \[
  \sum_u\theta^{\delta}_u(x^*_u)x_u(x^*_u) + \sum_u\sum_{i\ne x^*_u}(\theta^{\delta}_u(x^*_u) + \eta)x_u(i) < \sum_u\sum_i\theta_u^{\delta}(i)x_u(i),
  \]
  and simplifying using $\sum_ix_u(i) = 1$ gives:
  \[
  \sum_u\theta^{\delta}_u(x^*_u) + \eta\sum_u\sum_{i\ne x^*_u}x_u(i) < \sum_u\sum_i\theta_u^{\delta}(i)x_u(i).
  \]
  Plugging in to \eqref{eqn:node-ineq} gives:
  \[
  \sum_u\sum_{i\ne x^*_u}x_u(i) < \frac{\Delta}{\eta}.
  \]
  The left-hand-side is precisely $|x-x^*|_1 / 2$.
\end{proof}

\paragraph{Metric relaxation.}
We now introduce another relaxation for the \emph{uniform} metric
labeling problem that will be more convenient than the local
relaxation. Let $L^m(G)$ be the set defined by the following constraints:
\begin{align*}
  \sum_{i} x_{u}(i) = 1 &\qquad \forall u \in V\\
  x_{uv}(i) = \frac{1}{2}|x_u(i) - x_v(i)| &\qquad \forall (u,v) \in E,\ \forall i \in [k]\\
  x_{u}(i) \in [0,1] &\qquad \forall u\in V,\ i\in [k].
\end{align*}
Compared with the local polytope $L(G)$, the edge variables $x_{uv}$ now live in the $(k-1)$-dimensional simplex.
The metric and local relaxations are known to have the same optima for uniform metric labeling:
\[
\min_{x\in L^m(G)} \sum_{u}\sum_i \theta_u(i) x_u(i) + \sum_{uv\in E}w_{uv}\sum_i x_{uv}(i) = \min_{x\in L(G)} \sum_{u}\sum_i \theta_u(i) x_u(i) + \sum_{uv\in E}w_{uv}\sum_{ij} \mathbb{I}[i\ne j]x_{uv}(i,j),
\]
and the solutions to the two relaxations agree when resticted to the
$x_u$ coordinates. The nonlinear constraint in the definition of
$L^m(G)$ can be linearized when solving this optimization problem.

\begin{lemma}[Deviation bound]\label{lem:deviationbound}
Consider uniform metric labeling, so $\theta_{uv}(i,j) = w_{uv}\mathbb{I}[i\ne j]$. Instead of observing $\theta$, assume we observe a noisy objective vector $\hat\theta$, where $\hat\theta_u(i) = \theta_u(i)$ and $\hat\theta_{uv}(i,j) = (w_{uv} + n_{uv})\mathbb{I}[i\ne j]$. Here the $n_{uv}$'s are mutually independent, subgaussian random variables with mean 0, parameter $\sigma_{uv}$ and support $[-w_{uv},\infty)$. Then with probability $1-o(1)$ over the generation of $\hat\theta$,
  \[
  \sup_{x\in L^m(G)}\left|\langle\theta - \hat\theta, x\rangle\right| \le ck^{3/2}\sqrt{n\sum_{uv}\sigma_{uv}^2}.
  \]
\end{lemma}
\begin{proof}
   Define a random function $\mathcal{R}: L^m(G)\to \{0,1\}^{nk}$ by choosing $r \in (0,1)^k$ uniformly at random. Call $\mathcal{R}(x)$ by $\hat x$, and for each $(u,i) \in V \times [k]$, if $x_u(i) > r_i$, set $\hat x_u(i) = 1$. Otherwise, set $\hat x_u(i) = 0$. For a point $\hat x \in \{0,1\}^{nk}$, define $\hat d_{uv} = \sum_i \mathbb{I}[\hat x_u(i) \ne \hat x_v(i)] = |\hat x_u - \hat x_v|_1$, and let
  \[
  f(\hat x) = \sum_{uv}n_{uv}\hat d_{uv}.
  \]
  Then
  \[
  \E_r[f(\hat x)] = \sum_{uv}n_{uv}\E[\hat d_{uv}] = \sum_{uv}n_{uv}\sum_i\P[\hat x_u(i) \ne \hat x_v(i)] = \sum_{uv}n_{uv}\sum_i|x_u(i) - x_v(i)| = 2\langle \theta - \hat \theta, x\rangle,
  \]
  So
  \[
  \sup_{x\in L^m(G)} \left|\langle \theta - \hat \theta, x\rangle\right| = \sup_{x\in L^m(G)} \left|\frac{1}{2} \mathbb{E}_r[f(\mathcal{R}(x))]\right| \le \sup_{\hat x\in \{0,1\}^{nk}} \left|\frac{1}{2}\sum_{uv}n_{uv}\hat d_{uv}\right| \le \sup_{\hat x\in \{0,1\}^{nk}} \left|\frac{k}{2}\sum_{uv}n_{uv}\mathbb{I}[\hat x_u \ne \hat x_v]\right|,
  \]
  where $\hat x_u$ is the vector with coordinates $\hat x_u(i)$. Now it remains to upper bound
  \[
  \sup_{\hat x\in \{0,1\}^{nk}} \left|\frac{k}{2}\sum_{uv}n_{uv}\mathbb{I}[\hat x_u \ne \hat x_v]\right|
  \]
  with high probability over $n_{uv}$.
  
  Fix $\hat x \in \{0,1\}^{nk}$. By the subgaussian conditions on $n_{uv}$, we have the following concentration
  bound:
  \[
  \P\left[\frac{k}{2}\left|\sum_{uv}n_{uv}\mathbb{I}[\hat x_u \ne \hat x_v]\right| > t\right] \le 2\exp\left(\frac{-2t^2}{k^2\sum_{uv}\sigma_{uv}^2}\right)
  \]
  Let $t = ck^{3/2}\sqrt{n\sum_{uv}\sigma_{uv}^2}$ for some constant $c$. Then 
    \[
  \P\left[\frac{k}{2}\left|\sum_{uv}n_{uv}\mathbb{I}[\hat x_u \ne \hat x_v]\right| > t\right] \le 2\exp\left(-2c^2nk\right).
  \]
  Union bounding over $\{0,1\}^{nk}$, we get
  \[
  \P\left[\sup_{\hat x\in \{0,1\}^{nk}} \left|\frac{k}{2}\sum_{uv}n_{uv}\mathbb{I}[\hat x_u \ne \hat x_v]\right| > ck^{3/2}\sqrt{n\sum_{uv}\sigma_{uv}^2}\right] \le 2\exp(nk(\log 2 -2c^2)),
  \]
  which is $o(1)$ for $c \ge 0.59$.
\end{proof}
\begin{lemma}[Objective bound]\label{lem:objective-bound}
Given an instance $\theta$ of uniform metric labeling, we observe an instance $\hat\theta$ generated as in Lemma \ref{lem:deviationbound}, then solve the LP:
\[
x = \argmin_{x' \in L(G)} \langle\hat\theta, x'\rangle.
\]
Let $x^*$ be a solution to the denoised LP:
\[
x^* = \argmin_{x' \in L(G)} \langle\theta, x'\rangle.
\]
Then the noisy solution $x$ satisfies
\[
\langle \theta, x \rangle \le \langle \theta, x^* \rangle + 2ck^{3/2}n\sigma\sqrt{d},
\]
with probability $1-o(1)$,
where the constants are as in Lemma \ref{lem:deviationbound}.
\end{lemma}
\begin{proof}
Let $\Delta = ck^{3/2}n\sigma\sqrt{d}$. Two applications of Lemma \ref{lem:deviationbound} and the optimality of $x$ for the noisy instance $\hat\theta$ imply:
\[
\langle \theta, x\rangle \le \langle \hat \theta, x\rangle + \Delta \le \langle \hat \theta, x^*\rangle + \Delta \le \langle \theta, x^*\rangle + 2\Delta.
\]
\end{proof}
\begin{theorem}\label{thm:solclose}
Consider a uniform metric labeling instance with objective vector $\theta$ whose dual solution $\delta$ is locally decodable with margin $\eta$ (e.g., a (2,1)-stable instance implies $\eta > 0$). Instead of observing $\theta$, assume we observe a noisy objective vector $\hat\theta$, where $\hat\theta_u(i) = \theta_u(i)$ and $\hat\theta_{uv}(i,j) = (w_{uv} + n_{uv})\mathbb{I}[i\ne j]$. Here the $n_{uv}$'s are mutually independent, subgaussian random variables with mean 0, parameter $\sigma_{uv}$ and domain $[-w_{uv},\infty)$. Let $x$ be the optimal LP solution to the observed instance:
\[
x = \argmin_{x' \in L(G)}\langle\hat\theta, x'\rangle
\]
and let $x^*$ be the optimal solution to the original instance:
\[
x^* = \argmin_{x' \in L(G)}\langle\theta, x'\rangle.
\]
Note that $x^*$ is integral.
Then with probability $1-o(1)$ over the generation of $\hat\theta$,
\[
\frac{1}{2}\sum_u |x_u - x^*_u|_1 \le \frac{2ck^{3/2}n\sigma\sqrt{d}}{\eta}.
\]
Here $d$ is the maximum degree (e.g., $4$ for a grid) and $\sigma$ is the maximum of $\sigma_{uv}$'s.
\end{theorem}
\begin{proof}
Lemma \ref{lem:objective-bound} implies that 
\[
\langle \theta, x\rangle \le \langle \theta, x^*\rangle + 2ck^{3/2}n\sigma\sqrt{d},
\]
with probability $1-o(1)$,
and Lemma \ref{lem:hamming} bounds the Hamming distance.
\end{proof}
\subsection{Computing the optimal dual margin}
If there exists one locally decodable dual solution where the primal pays 0 edge cost, there typically exists many, since the dual solution can be modified to trade margin between neighboring nodes without compromising optimality. But the Hamming distance bound in Theorem \ref{thm:solclose} depends on a particular value of $\eta$: the minimum across all the nodes $u$ of the dual margin at that node. How can we find $\delta$ so that this value is as large as possible?

For each node $u$ and each $j\ne x^*_u$, define $\eta_{uj}(\delta) = \theta^{\delta}_u(j) - \theta^{\delta}_u(x^*_u)$. I.e., $\eta_{uj}(\delta)$ is the cost margin at $u$ between label $j$ and the optimal primal label $x^*_u$ when the potentials are reparametrized by the dual solution $\delta$. Then define $\eta_{u}(\delta) = \min_j \eta_{uj}(\delta)$. Since $\eta_{uj}(\delta)$ is a linear function of $\delta$, $\eta_{u}$ is a concave function of $\delta$. Likewise, let $\eta(\delta) = \min_u \eta_u(\delta)$. This is also a concave function of $\delta$.
Now let $LD(G,\theta)$ be the convex set defined by the constraints:
\begin{align*}
  \eta_{uj} > 0 &\qquad \forall u\in V,\ j \ne x^*_u\\
  \theta^{\delta}_{uv}(i,j) \ge 0 &\qquad \forall (u,v) \in E, (i,j) \in [k]\times [k]\\
   \theta^{\delta}_{uv}(x^*_u,x^*_v) = 0 &\qquad \forall (u,v) \in E.
\end{align*}
The first constraint ensures that any $\delta \in LD(G,\theta)$ is locally decodable to $x^*$ (and thus, optimal). The latter two constraints ensure that the optimal primal solution pays 0 edge cost, which is necessary for the proof of Lemma \ref{lem:hamming}. All the constraints are linear in $\delta$. So any $\delta \in LD(G,\theta)$ has a corresponding $\eta(\delta)$ that can be used in Theorem \ref{thm:solclose}. We can find the best one by solving:
\[
\eta^* = \max_{\delta \in LD(G,\theta)}\eta(\delta).
\]
This is a concave maximization problem over a polytope defined by $n(k-1) + m(k^2+1)$ constraints, so we can solve it efficiently. Lemma \ref{lemma:edge-removal} guarantees that $LD(G,\theta)$ is nonempty as long as there exists a locally decodable dual solution. In the next section, we examine structural conditions on $G$ and $\theta$ that imply the solution to this optimization problem is large.

\subsection{What conditions imply dual margin?}
\subsubsection{Node stability}
\begin{theorem}
Assume an instance $(G,\theta)$ of uniform metric labeling is $(2,1)$-stable even after the node costs of the optimal solution $x^*$ are each increased by up to $\eta$ (for a total increase of $n\cdot\eta$). Then there is dual margin $\eta$.
\end{theorem}
\subsubsection{Anchor nodes}

%\bibliographystyle{apalike}
%\bibliography{ref}
%\end{document}
