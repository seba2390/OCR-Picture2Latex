\section{Introduction}
In this work, we study the MAP inference problem in the \emph{ferromagnetic Potts model}, which is also known as uniform metric labeling \citep{KleinbergTardos02}. Given a graph $G=(V,E)$, this problem is:
\begin{align*}
  \minimize_{x: V\to [k]}\sum_{u\in V}c(u, x(u)) + \smashoperator{\sum_{(u,v) \in E}}w(u,v)\indicator[x(u) \ne x(v)].
\end{align*}
Here we are optimizing over \emph{labelings} $x: V\to [k]$ where $ [k] = \{1,2,\dots,k\}$. 
The objective is comprised of ``node costs'' $c: V\times [k] \to \mathbb{R}$, and ``edge weights'' $w: E \to \mathbb{R}_{> 0}$; a labeling $x$ pays the cost $c(u,i)$ when it labels node $u$ with label $i$ and pays $w(u,v)$ on edge $(u,v)$ when it labels $u$ and $v$ differently.
This problem is NP-hard for variable $k \ge 3$ \citep{KleinbergTardos02} even when the graph $G$ is planar \citep{dahlhaus1992complexity}.
However, there are several efficient and empirically successful approximation algorithms for the MAP inference problem---such as TRW \citep{wainwright2005map} and MPLP \citep{globerson2008fixing}---that are related in some way to the \emph{local LP relaxation}, which is also sometimes called the {\em pairwise LP} \cite{wainwright2008graphical, chekuri2001approximation}.  
This LP relaxation returns an approximate MAP solution for most problem instances. However, when the parameters of these models are learned so as to enable good structured prediction, often the LP relaxation exactly or almost exactly recovers the MAP solution  \citep{meshi2019train}.
The connection between the LP relaxation and commonly used approximate MAP inference algorithms then leads to the following compelling question, which is of great practical relevance for understanding the ``tightness'' of the LP solution (informally, how close the LP solution is to the MAP solution). 

{\em Can we explain the exceptional performance of the local LP relaxation in recovering the MAP solution in practice?}

\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{Paper/fig/stablefig3.pdf}
\caption{Left: prior work \citep{LanSonVij18} showed that a \emph{stable instance} can be exactly solved efficiently. Colors indicate the label of each vertex in the MAP solution $x^*$. On stable instances, solving the LP relaxation (represented by the arrow) recovers the MAP solution. However, real-world instances are \emph{not} suitably stable for this result to apply in practice \citep{LanSonVij19}. Right: in this work, we show that solving the LP relaxation on a (slightly) \emph{corrupted} stable instance (corruptions shown as bold edges) \emph{approximately} recovers the original MAP solution. This is true even if the corruption changes the MAP solution (as in the bottom example). In other words, we prove that ``easy'' instances are still approximately easy even after small perturbations.}
\label{fig:main-idea}
\end{figure*}

Several works have studied different conditions that imply the local relaxation or related relaxations are tight \citep[e.g.,][]{kolmogorov2005optimality, wainwright2008graphical, thapper2012power, weller2016tightness,rowland2017conditions}.
Recent work on tightness of the local relaxation has focused on a class of several related conditions known as \emph{perturbation stability}.
Intuitively, an instance is perturbation stable if the solution $x^*$ to the MAP inference problem is unique, and moreover, $x^*$ is the unique solution even when the edge weights $w$ are multiplicatively perturbed by a certain adversarial amount \cite{BLstable}.
This structural assumption about the instance $(G,c,w)$ captures the intuition that, on ``real-world'' instances, the ground-truth solution is stable and does not change much when the weights are slightly perturbed.

For constants $\beta, \gamma \geq 1$, we say that $w'$ is a $(\beta,\gamma)$-perturbation of the weights $w$ if $\frac{1}{\beta}\cdot w(u,v) \le w'(u,v) \le \gamma \cdot w(u,v)$ for all $(u,v)\in E$. 
Suppose $x^*$ is the unique MAP solution to the instance $(G,c,w)$. Then, we say $(G,c,w)$ is a $(\beta,\gamma)$-stable instance if $x^*$ is also the unique MAP solution to every instance $(G,c,w')$ where $w'$ is a $(\beta,\gamma)$-perturbation of $w$.
 \citet{LanSonVij18} showed that when $(G,c,w)$ is $(2,1)$-stable, the solution to the local LP relaxation is {\em persistent} i.e., the LP solution exactly recovers the MAP solution $x^*$.

While theoretically interesting, $(2,1)$-stability is a strict condition that is unlikely to be satisfied in practice: the solution $x^*$ is not allowed to change \emph{at all} when the weights are perturbed. No real-world instances have yet been shown to be $(2,1)$-stable \citep{LanSonVij19}. Moreover, the LP relaxation is also not persistent on most of those instances. However, the solution of the local LP relaxation is still {\em nearly persistent} i.e., the LP solution is very close to the MAP solution $x^*$ (see Definition~\ref{def:hammingerror} for a formal definition). 
Those examples made it clear that theory must go beyond perturbation stability to explain this phenomenon of near-persistence that is prevalent in practice~\citep[see e.g.,][]{Sontagthesis, shekhovtsov2017maximum}.

{\em Why is the LP relaxation nearly persistent on MAP inference instances in practice?}

There are several theoretical frameworks to explain exactness or tightness of LP relaxations, such as total unimodularity, submodularity \citep{kolmogorov2005optimality}, and perturbation stability \citep{LanSonVij18, LanSonVij19}, as well as structural assumptions about the graph $G$ \citep{wainwright2008graphical}, or combined assumptions about $G$ and the form of the objective function \citep{weller2016tightness, rowland2017conditions}. However, these frameworks can not be used to prove near-persistence.

% \citet{LanSonVij19} proved that the linear program \emph{partially} recovers $x^*$ on large sub-portions (blocks) that are stable, and demonstrated that such large stable blocks are indeed present in many examples from computer vision. However, the required {\em block stability} condition in turn depends on certain quantities related to the LP dual. This is unsatisfactory since this does not explain when and why such instances are likely to arise in practice. Developing new techniques and frameworks to reason about near-persistence presents a technical challenge that may also be of interest in other settings. \fxnote{move to related work}

Figure \ref{fig:main-idea} (informally) shows our main result. The left side depicts the previous result of \citet{LanSonVij18}: if the instance is $(2,1)$-stable (a fairly strong structural assumption), the LP relaxation exactly recovers the full solution $x^*$. This result is limited because real-world instances have been shown to not satisfy $(2,1)$-stability \citep{LanSonVij19}. The right side shows our main result: if the instance is a \emph{slightly corrupted} $(2,1)$-stable instance, the LP relaxation still \emph{approximately} recovers the solution $x^*$ to the stable instance.

Intuitively, we may expect a real-world instance to be ``close'' to a stable instance (i.e., to be a ``corrupted stable'' instance, as in Figure \ref{fig:main-idea}) even if the instance itself is not stable. We design an algorithm to check whether this is the case. We find that on several real examples, sparse and small-norm changes to the instance make it appropriately stable for our theorems to apply. In other words, we certify that these real instances are close to stable instances. For these instances, our theoretical results explain why the LP relaxation approximately recovers the MAP solution.

More formally, we assume that there is some \emph{latent} stable instance $\stabins$, and that the observed instance $\obsins$ is a noisy version of $\stabins$ that is close to it.
Let $\hx$ be the solution to the local LP on the observed instance $\obsins$, and let $\bx$ be the (unknown) MAP solution on the unseen stable instance $(G,\bar{c},\bar{w})$. We prove that under certain conditions, the LP solution $\hx$ is {\em nearly persistent} i.e., the Hamming error $\norm{\hx - \bx}_1$ is small (see Definition~\ref{def:hammingerror}). In other words, the local LP solution to the observed instance approximately recovers the latent integral solution $\bx$.

We complement this by studying a natural generative model that generates noisy stable instances which, with high probability, satisfy the above conditions for {\em near persistency}. In other words, the observed instance $\obsins$ is obtained by random perturbations to the latent stable instance $\stabins$, and the LP relaxation approximately recovers the MAP solution to the latent instance with high probability.



Our theoretical results imply that the local LP approximately recovers the MAP solution when the observed instance is close to a stable instance. Our empirical results suggest that real-world instances are very close to stable instances. These results together suggest a new explanation for the near-persistency of the solution of the local LP relaxation for MAP inference in practice. To prove these results and derive our algorithm for finding a ``close-by'' stable instance, we make several novel technical contributions, which we outline below.


\paragraph{Technical contributions.}
\begin{itemize}
    \item In Section \ref{sec:expansion-stability}, we generalize the $(2,1)$-stability result of \citet{LanSonVij18} to work under a much weaker assumption, which we call $(2,1)$-expansion stability. That is, we prove the local LP is tight on $(2,1)$-expansion stable instances. 
    Additionally, given the instance's MAP solution, $(2,1)$-expansion stability is efficiently checkable. To the best of our knowledge, most other perturbation stability assumptions are not known to satisfy this desirable property. This generalization is crucial for the efficiency of our algorithm for finding stable instances that are close to a given observed instance.
    \item In Section \ref{sec:stable-curvature}, we give a simple extension of $(2,1)$-expansion stability called $(2,1,\psi)$-expansion stability. We prove it implies a ``curvature'' result around the MAP solution $\bx$. On instances that satisfy this condition, if a labeling $\hat{x}$ is close in objective value to $\bx$, it must also be close in the solution space. This result lets us translate between objective gap and Hamming distance.
    \item In Section \ref{sec:random-model}, we study a natural generative model where the observed instance is generated from an arbitrary {\em latent stable} instance by random (sub-Gaussian) perturbations to the costs and weights. We prove that, with high probability, every feasible LP solution takes close objective values on the latent and observed instances. The proof uses a rounding algorithm for metric labeling in a novel way to obtain stronger guarantees. When combined with our other results, this proves that when the latent instance is $(2,1,\psi)$-expansion stable, the LP solution is nearly persistent on the observed instance with high probability. These results suggest a theoretical explanation for the phenomenon of near-persistence of the LP solution in practice. 
    \item We design an efficient algorithm for finding $(2,1,\psi)$-expansion stable instances that are ``close'' to a given instance $\obsins$ in Section \ref{sec:algorithm}. To the best of our knowledge, this is the first algorithm for finding close-by stable instances, and is also an efficient algorithm for checking $(2,1,\psi)$-expansion stability. This algorithm allows us to check whether real-world instances can plausibly be considered ``corrupted stable'' instances as shown in Figure \ref{fig:main-idea}.
    \item We run our algorithm on several real-world instances of MAP inference in Section \ref{sec:experiments}, and find that the observed instances $\obsins$ often admit close-by $(2,1,\psi)$-stable instances $\stabins$. 
    Moreover, we find that the local LP solution $\hat{x}$ typically has very close objective to $\bx$ in $\stabins$.
    Our curvature result for $(2,1,\psi)$-stable instances thus gives an explanation for the tightness of the local LP relaxation on $\obsins$.
\end{itemize}

\section{Related work}
\label{sec:related}
\paragraph{Perturbation stability.}

Several works have given recovery guarantees for the local LP relaxation on perturbation stable instances of uniform metric labeling \citep{LanSonVij18, LanSonVij19} and for similar problems \citep{makarychev2014bilu, AngMakMak17}.

\citet{LanSonVij19} give partial recovery guarantees for the local LP when parts (\textit{blocks}) of the observed instance satisfy a stability-like condition, and they showed that practical instances have blocks that satisfy their condition. However, the required {\em block stability} condition in turn depends on certain quantities related to the LP dual. This is unsatisfactory since this does not explain when and why such instances are likely to arise in practice.
For a more extensive treatment of the subject, we refer the reader to the ``Perturbation Resilience'' chapter from \citet{roughgarden_2021}.

\paragraph{Easy instances corrupted with noise.}
Our random noise model is similar to several planted average-case models like stochastic block models (SBMs) considered in the context of problems like community detection, correlation clustering and partitioning \citep[see e.g.,][]{Mcsherry,Abbesurvey, GRSY15}. Instances generated from these models can also be seen as the result of random noise injected into an instance with a nice block structure that is easy to solve. 
Several works give exact recovery and approximate recovery guarantees for semidefinite programming (SDP) relaxations for such models in different parameter regimes~\cite{Abbesurvey,guedon2016community}. 
In our model however, we start with an {\em arbitrary} stable instance as opposed to an instance with a block structure (which is trivial to solve). 
Moreover, we are unaware of such analysis in the context of linear programs.
Please see Section~\ref{sec:random-model} for a more detailed comparison. 
To the best of our knowledge, we are the first to study instances generated from random perturbations to stable instances.

\paragraph{Partial optimality algorithms.} Several works have developed fast algorithms for identifying parts of the MAP assignment. These algorithms output an approximate solution $\hat{x}$ and a set of vertices where $\hat{x}$ provably agrees with the MAP solution $x^*$ \citep[e.g.,][]{kovtun2003partial, shekhovtsov2013exact,swoboda2016partial,shekhovtsov2017maximum}. Like these works, our results also prove that an approximate solution $\hat{x}$ has small error $|\hat{x} - x^*|$. However, these previous approaches are more concerned with designing fast algorithms for finding such $\hat{x}$. In contrast, we focus on giving structural conditions that explain why a \emph{particular} $\hat{x}$ (the solution to the local LP relaxation) often approximately recovers $x^*$. Our algorithm in Section \ref{sec:algorithm} is thus not meant as an efficient method for certifying that $|\hat{x} - x^*|$ is small, but rather as a method for checking whether our structural condition (that the observed instance is close to a stable instance) is satisfied in practice.
