\section{Generative model for noisy stable instances}\label{sec:random-model}
In the previous section, we showed that if an instance $(G, \hc, \hw)$ is ``close'' to a $(2,1,\psi)$-expansion stable instance $(G,\bc, \bw)$ (i.e., the LP solution $\hat{x}$ to $(G, \hc, \hw)$ has good objective in  $(G,\bc, \bw)$), then $\norm{\hx - \bx}$ is small, where $\bx$ is the MAP solution to the stable instance. In this section, we give a natural generative model for $(G, \hc, \hw)$, based on randomly corrupting $(G,\bc,\bw)$, in which $\hat{x}$ has good objective in $(G,\bc,\bw)$ with high probability. Together with the curvature result from the previous section (Theorem \ref{thm:curvature}), this implies that the LP relaxation, run on the noisy instances $(G,\hc,\hw)$, approximately recovers $\bx$ with high probability.



We now describe our generative model for the problem instances, which starts with an arbitrary stable instance and perturbs it with random additive perturbations to the edge costs and node costs (of potentially varying magnitudes). 
The random perturbations reflect possible uncertainty in the edge costs and node costs of the Markov random field.
We will assume the random noise comes from any distribution that is sub-Gaussian\footnote{All of the results that follow can also be generalized to sub-exponential random variables; however for convenience, we restrict our attention to sub-Gaussians.}. However, there is a small technicality: the edge costs need to be positive (node costs can be negative). For this reason we will consider truncated sub-Gaussian random variables for the noise for the edge weights. We define sub-Gaussian and truncated sub-Gaussian random variables in Appendix~\ref{sec:random-model_details}.

\paragraph{Generative Model:}
We start with an instance $(G,\bar{c},\bar{w})$ that is $(2, 1, \psi)$-expansion stable, and perturb the edge costs and node costs independently. Given any instance $(G,\bar{c},\bar{w})$, an instance $(G,\hat{c},\hat{w})$ from the model is generated as follows:

\begin{enumerate}[noitemsep,nolistsep]
    \item For all node-label pairs $(u,i),\ \hat{c}(u,i) = \bar{c}(u,i) + \tc(u,i)$, where $\tc(u,i)$ is sub-Gaussian with mean $0$ and parameter $\sigma_{u,i}$.
    \item For all edges $(u,v),\ \hat{w}(u,v) = \bar{w}(u,v) + \tw(u,v)$, where $\tw(u,v)$ is an independent r.v. that is $(-w(u,v),\gamma_{u,v})$-truncated sub-Gaussian with mean $0$.
    \item $(G, \hat{c}, \hat{w})$ is the observed instance.
\end{enumerate}

By the definition of our model, the edge weights $\hat{w}(u,v) \ge 0$ for all $(u,v) \in E$. The parameters of the model are the unperturbed instance $(G,\bar{c},\bar{w})$, and the noise parameters $\set{\gamma_{u,v},\sigma_{u,i}}_{u,v \in V, i \in [k]}$. 
On the one hand, the above model captures a natural average-case model for the problem. For a fixed ground-truth solution $x^*: V \to [k]$, consider the stable instance $(H,c,w)$ where $w^*_{uv}=2$ for all $u,v$ in the same cluster (i.e., $x^*(u) = x^*(v)$) and $w^*_{uv}=1$ otherwise; and with $c^*(u,i)=1$ if $x^*(u)=i$, and $c^*(u,i)=1+\psi$ otherwise. The above noisy stable model with stable instance $(H,c,w)$ generates instances that are reminiscent of (stochastic) block models, with additional node costs. On the other hand, the above model is much more general, since we can start with {\em any} stable instance $(G,c,w)$.   

With high probability over the random corruptions of our stable instance, the local LP on the corrupted instance approximately recovers the MAP solution $\bar{x}$ of the stable instance. The key step in the proof of this theorem is showing that, with high probability, the observed instance is close to the latent stable instance in the metric we defined earlier.

\begin{restatable}[$d(\hat{\theta},\bar{\theta})$ is small w.h.p. ]{lemma}{dswhp}\label{lem:dswhp}
There exists a universal constant $c < 1$ such that for any instance in the above model, with probability at least $1-o(1)$, 
\begin{equation*}
    \smashoperator{\sup_{x \in L^*(G)}} \abs{\dot{\hat{\theta}}{x}  - \dot{\bar{\theta}}{x}}\le c \sqrt{nk} \sqrt{\sum_{u,i}\sigma_{u,i}^2 + \frac{k^2}{4}\sum_{uv}\gamma_{u,v}^2}
\end{equation*}
\end{restatable}

\begin{proof}[Proof (sketch)]
For any \emph{fixed} $x \in L^*(G)$, we can show that $\abs{\dot{\hat{\theta}}{x}  - \dot{\bar{\theta}}{x}}$ is small w.h.p. using a standard large deviations bound for sums of sub-Gaussian random variables. The main technical challenge is in showing that the supremum over all feasible solutions is small w.h.p. The standard approach is to perform a union bound over an $\epsilon$-net of feasible LP solutions in $L^*$. However, this gives a loose bound. Instead, we upper bound the supremum by using a rounding algorithm for LP solutions in $L^*(G)$, and union bound only over the discrete solutions output by the rounding algorithm. This gives significant improvements over the standard approach; for example, in a $d$-regular graph with equal variance parameter $\gamma_{uv}$, this saves a factor of $\sqrt{d}$ apart from logarithmic factors in $n$.
\end{proof}

We defer the details to Appendix~\ref{sec:random-model_details}. 
The above proof technique that uses a rounding algorithm to provide a deviation bound for a continuous relaxation is similar to the analysis of SDP relaxations for average-case problems~\citep[see e.g.,][]{MMVfas,guedon2016community}. 
The above lemma, when combined with Theorem~\ref{thm:curvature} gives the following guarantee. 

\begin{restatable}[LP solution is nearly persistent]{theorem}{apmap}\label{thm:apmap}
Let $\hx$ be the local LP solution to the observed instance $\obsins$ and $\bar{x}$ be the MAP solution to the latent $(2,1,\psi)$-expansion stable instance $\stabins$. With high probability over the random noise, \[ \frac{1}{2}\norm{\hx_V - \bar{x}_V}_1 \le \frac{2}{\psi} \cdot c\sqrt{nk} \cdot \sqrt{\sum_{u,i}\sigma_{u,i}^2 + k^2\sum_{uv}\gamma_{u,v}^2}\]

\end{restatable}
\begin{proof}
We know that for any feasible solution $x \in L(G), \dot{\bar{\theta}}{x} \ge \dot{\bar{\theta}}{\bx}$. Therefore, $\dot{\bar{\theta}}{\hx} \ge \dot{\bar{\theta}}{\bx}$. Remember that we defined $d(\hat{\theta}, \bar{\theta})$ as $\sup_{x \in L^*(G)} \abs{ \dot{\hat{\theta}}{x}  - \dot{\bar{\theta}}{x}}$. Since $\hx$ and $\bx$ are both points in $L^*(G)$,
\begin{align*}
\dot{\bar{\theta}}{\hx} \leq \dot{\hat{\theta}}{\hx} + d(\hat{\theta},\bar{\theta}) \leq \dot{\hat{\theta}}{\bx} &+ d(\hat{\theta},\bar{\theta}) \\
&\leq \dot{\bar{\theta}}{\bx} + 2d(\hat{\theta},\bar{\theta})
\end{align*}
The first and third inequalities follow from the definition of $d(\hat{\theta},\bar{\theta})$. The second inequality follows from the fact that $\hat{x}$ is the minimizer of $\dot{\bar{\theta}}{x}$ over all $x \in L(G)$.
Therefore,$0 \le \dot{\bar{\theta}}{\hx} - \dot{\bar{\theta}}{\bx} \leq 2d(\hat{\theta},\bar{\theta})$. Using this in Theorem~\ref{thm:curvature}, we get $\frac{1}{2}\norm{\hx - \bar{x}}_1 \le \frac{2d(\hat{\theta},\bar{\theta})}{\psi}$. Lemma~\ref{lem:dswhp} then gives an upper bound on $d(\hat{\theta},\bar{\theta})$ that holds w.h.p.
\end{proof}

For a $d$-regular graph in the uniform setting, we get the following useful corollary:
\begin{restatable}[MAP solution recovery for regular graphs ]{corollary}{mapreg}\label{cor:mapreg}
Suppose we have a $d$-regular graph $G$ with $\gamma_{u,v}^2 = \gamma^2$ for all edges $(u,v)$, and $\sigma_{u,i}^2 = \sigma^2$ for all vertex-label pairs $(u,i)$. Also, suppose only a fraction $\rho$ of the vertices and $\eta$ of the edges are subject to the noise. With high probability over the random noise, \[ \frac{\norm{\hx_V - \bar{x_V}}_1}{2n} \le \frac{2ck\sqrt{\rho \sigma^2 + \frac{\eta dk}{8}\gamma^2}}{\psi}\]
\end{restatable}
Note that when $\hat{x}$ is an integer solution, the left-hand-side is the fraction of vertices misclassified by $\hat{x}$.

