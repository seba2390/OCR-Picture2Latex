
\section{Experiments}

In this section, we conduct a series of comprehensive experiments to validate the effectiveness of our method. Below we first present our experimental analysis and ablative study on Visual Genome~\cite{krishna2017visual} dataset (in Sec.~\ref{lab:vg_exp}), then our results on the Open Images V4/V6~\cite{OpenImages} (In Sec.~\ref{lab:oi_exp}). 
In each dataset, we first introduce the implementation details of our method and then report comparisons of quantitative results in detail.

%On each dataset, we first introduce the experiment configurations and then compare with previous state-of-the-art methods to validate our method,
%followed by ablation studies to validate the components in our framework. Finally, we visualize several qualitative results to demonstrate the capabilities of our model. 

% First part is the experiment configurations(Sec.\ref{subsec:exp_setup}).
% The experiment results are composed of the quantitative results(Sec.\ref{subsec:quant_res}) and quantitative results(Sec.\ref{subsec:qual_res}) two part.

% % We evaluate our method on three SGG public benchmarks datasets: Visual Genome, OpenImage datsets V4 and V6.

% We introduce the datasets we used and corresponding evaluation protocol, the implementation details of our methods.





% For the quantitative results, we will show the comparison between the other methods(Sec.\ref{subsec:sota_comp}) and ablative study for our methods(Sec.\ref{subsec:abla_sty}).

\begin{figure}
    \centering
    \includegraphics[width=7.6cm]{imgs/vg_longtail_part_perf.png}
    \caption{\textbf{The per-group results on VG dataset (SGGen task).}} 
    \label{fig:vg_longtail_part_perf} 
    \vspace{-0.4cm}
\end{figure} 


\subsection{Visual Genome} \label{lab:vg_exp}

% \noindent\textbf{4.1.1~Experiments Configurations}
\subsubsection{Experiments Configurations}

\paragraph{Dataset Details}  
For Visual Genome~\cite{krishna2017visual} dataset, we take the same split protocol as~\cite{xu_scene_2017,zellers_neural_2017}
% where 62,723 images are used for training, 26,446 for test, and 5,000 images sampled from the training set for validation. 
The most frequent 150 object categories and 50 predicates are adopted for evaluation. Following the similar protocol in~\cite{liu_largescale_2019}, we divided the categories into three disjoint groups according to the instance number in training split: \textit{head}~(more than 10k), \textit{body}~(0.5k $\sim $ 10k), \textit{tail}~(less than 0.5k), more details are reported in Suppl.

\vspace{-0.4cm}

% we follow the most popular spilt proposed by Xu, et al.\cite{xu_scene_2017}, which contains 108077 images in total, and 150 object categories and 50 predicate categories. 
% We takes the same split protocol with Zellers\cite{zellers_neural_2017}, The images of dataset has been divided into the training set and test set by 7:3.  For validation set, the 5,000 images are taken from the training set for hyper-parameter tuning.
\paragraph{Evaluation Protocol}
We evaluate the model on three sub-tasks as\cite{xu_scene_2017,zellers_neural_2017}:
1) predicate classification (PredCls), 
2) scene graph classification (SGCls),
3) scene graph generation (SGGen, also denote as SGDet).
Following the previous works~\cite{chen_knowledge-embedded_2019, tang_learning_2018,tang_unbiased_2020,lin_gps-net_2020} that concentrate on the long-tail distribution of entity and predicate categories on Visual Genome,
we takes \textbf{recall@K(R@K)} and \textbf{mean recall@K (mR@K)} as evaluation metrics, and also report the mR@K on each long-tail category groups~(\textit{head}, \textit{body} and \textit{tail}).

\vspace{-0.4cm}
%To demonstrate the efficacy of our model on long-tailed datasets, we borrow the the evaluation metrics on the long-tailed recognition tasks.
%Specifically, we divided the categories into three groups \textbf{Head}, \textbf{Body}, \textbf{Tail} respectively according to the instance frequency, 
%and compute the mR@K in each categories set. 
%In our partition, the predicate categories whose frequency is above the 10k are included in \textbf{Head}, those between 0.5k and 10k belong to \textbf{Body},
%and others are in \textbf{Tail} group.
\paragraph{Implementation Details}
Similar to Tang~\cite{tang_unbiased_2020}, we adopt ResNeXt-101-FPN~\cite{xie2017aggregated} as backbone network and Faster-RCNN~\cite{ren_faster_2015} as object detector, 
whose model parameters are kept frozen in model training.
% We follow the same setup as the Tang\cite{tang_unbiased_2020}, which is Faster-RCNN ResNeXt-101 FPN detector and freezed in relationships detection training phrase as entities proposals generation.
%For the proposal generation network, we take the 80 entities proposals and Top-3000 relationship proposals for each images.
In our resampling strategy, we set the repeat factor $t$=0.07, instances drop rate $\gamma_d$=0.7, and weight of fusion the entities features $\rho$=-5.
The $\alpha, \beta$ are initialized as 2.2 and 0.025 respectively.

% \paragraph{Implementation Detail}\label{subsec:impl_detail}
% We follow same setup with the Tang\cite{tang_unbiased_2020}, which is Faster-RCNN ResNeXt-101 FPN detector and freezed in relationships detection training phrase as entities proposals generation.
% For the post-process of entities proposal in Scene graph detection, we take the 80 entities proposals and top 3000 relationship proposals for each images.
% In our resampling strategy, we set the repeat factor $T$ as 0.07 and $\gamma_d = 0.7$ instances drop rate to obtain the performance shown in paper.
% The $\alpha, \beta$ are initialized as 2.2 and 0.025 respectively, the $\rho$ for skip connections, we set as the $-5$

% \noindent\textbf{4.1.2~Comparisons with State-of-the-Art Methods}

\subsubsection{Comparisons with State-of-the-Art Methods} \label{subsec:sota_comp}

%1) The methods that explicitly encode the context information: Motifs\cite{zellers_neural_2017},MSDN\cite{li_scene_2017}, VCTree \cite{tang_learning_2018}, Graph-RCNN\cite{yang_graph_2018}, GPS-Net\cite{lin_gps-net_2020}, 
%2) Causal inference method of overcome the strong bias of longtail distribution: Unbiased\cite{tang_unbiased_2020}.
%We also compare the method with resampling strategies.
%3) The comparable simple baseline implemented by us, which has a similar model structure with\cite{zhang_graphical_2019} but without any graph module for context modeling.
%We present the quantitative results in Table.\ref{overall_table} for comparison.
% For fair comparison with previous works, we reimplement Motifs~\cite{zellers_neural_2017} and GPS-Net~\cite{lin_gps-net_2020} with instance-level resampling strategy~\cite{gupta_lvis:_2019}. 
% As shown in Tab.~\ref{overall_table}, our method achieves \textbf{30.4} and \textbf{32.9} on mR@50 and mR@100 on PredCls sub-task, 
% outperforming the previous SOTA Unbiased[38] with a signifiant marge of \textbf{5.0} and \textbf{4.2}, respectively. 
% For SGCls and SGGen sub-tasks, BGNN outperforms all the previous works on mR@50/100, 
% and achieves the comparable performance on R@50/100, which demonstrate 
% the effectiveness of our methods.

% For fair comparison with previous works, we reimplement Motifs~\cite{zellers_neural_2017} and GPS-Net~\cite{lin_gps-net_2020} with instance-level resampling strategy~\cite{gupta_lvis:_2019}. 

As shown in Tab.~\ref{overall_table}, BGNN achieves the state of the art in all three sub-tasks (PredCls, SGCls, SGGen) evaluated by mR@50/100 on X-101-FPN backbone, and outperforms Unbiased~\cite{tang_unbiased_2020} with a significant margin of \textbf{5.0} and \textbf{4.2} on PredCls. 
Besides, BGNN shows a comparable performance with previous SOTA in SGCls and SGGen sub-tasks on R@50/100, which demonstrate the effectiveness of our methods.

% \rev{
On the VGG16 backbone, we achieves comparable result with the state-of-the-art, PCPL~\cite{yan_pcpl_2020} on the SGCls and PredCls sub-tasks by mR@50/100.
However, BGNN outperforms the PCPL on SGGen sub-tasks by a large margin, which shows that our proposed BGNN is capable of handling challenging SGGen task with more noisy proposals.
% }


%First, we compare our method with the SOTA method by the categories balanced metric for this task, mean recall (mRecall@K).
%shows the comparison results between our methods and other SOTA methods and relevant methods on the Visual Genome dataset with the same codebase and entities detector. 
%First, we compare our method with the SOTA method by the categories balanced metric for this task, mean recall (mRecall@K).
%For fairness, we both compare our method with other methods that applied resampling or not by different groups.
%As shown in the table, our full method obtains the SOTA on mRecall@K metrics on three sub-tasks.
%We also maintain the comparable performance on classical overall Recall@K metric with the current SOTA methods, the specific performance number can be seen in the supplementary and ablation study.
% Our methods without resampling also achieve the same comparable performance with the SOTA methods
% 这里是否需要介绍ours- 的性能？

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{imgs/vg_longtail_dist.png}
%     \caption{The bar plot of the each categories instances frequency on Visual Genome training set. The red, blue, and green represents the head, body and tail categories sets respectively.}
%     \label{fig:vg_longtail_part}
% \end{figure}
% Moreover, we compare our method with recent work~\cite{lin_gps-net_2020, tang_unbiased_2020} on scene graph detection task in each category group for analysis. For the Fig~\ref{fig:vg_longtail_part_perf}, we find BGNN significantly outperform the prior works on overall mean recall and tail recall, which indicates our method can solve the biased scene graph generation effectively.
Moreover, as shown in Fig.~\ref{fig:vg_longtail_part_perf}, we compute the mean recall on each long-tail category groups in SGGen sub-task and find BGNN significantly outperforms the prior works~\cite{lin_gps-net_2020, tang_unbiased_2020} on \textit{tail} group, as a result it achieves the highest mean recall over all categories.
% \rev{
% move the visualization to suppl.
We provide more visualization of our results and comparisons in the Suppl.
% }
%For the performance comparison between the different long-tail parts, as shown in figure 
%\ref{fig:vg_longtail_part_perf}. We can see that our method obtain the SOTA performance on body and tail categories and maintain a comparable performance on head category set. 


\begin{table}[]
    \begin{center}
        \resizebox{0.45\textwidth}{!}{
            \begin{tabular}{ccc|cc|ccc}
            \toprule
            \multicolumn{3}{c|}{\textbf{Module}} & \multicolumn{5}{c}{\textbf{SGGen}}                               \\ \midrule
            \textbf{B}&\textbf{C}&\textbf{M}& \textbf{mR@100} & \textbf{R@100}  & \textbf{Head} & \textbf{Body} & \textbf{Tail} \\ \midrule
            \xmark&\xmark&\xmark & 9.7 &34.1 & 32.1& 9.3  & 3.0 \\\midrule
           \cmark &\xmark&\xmark &10.5 &34.8 &  32.4 & 10.7 & 4.5   \\ 
           \cmark &\cmark&\xmark & 11.7&35.3 &  33.6 & 11.4  & 5.2  \\
           \cmark &\cmark&\cmark & \textbf{12.6}&\textbf{35.8} & \textbf{34.0} &  \textbf{12.9} & \textbf{6.0}  \\
           \midrule
          \bottomrule   
        \end{tabular}
        }
    \end{center}
    \caption{\textbf{Ablation study of model structure.} \textbf{B}: bipartite graph neural network with plain message passing; \textbf{C}:confidence-aware message propagation mechanism;\textbf{M}: multi-sage refinement}
    \label{struct_ablation} 
\end{table} 


\begin{table}[]
    \begin{center}
    \resizebox{0.44\textwidth}{!}{
        \begin{tabular}{c|cc|ccc}
        \toprule
       \multirow{2}{*}{\textbf{Resample}} & \multicolumn{5}{c}{\textbf{SGGen}}     \\ 
       \cmidrule{2-6}
            &\textbf{mR@100} & \textbf{R@100}  & \textbf{Head} & \textbf{Body} & \textbf{Tail} \\ \midrule
        None 
            & 9.7 &  36.1   &   34.2   &  9.9  & 2.6\\
        RFS\cite{gupta_lvis:_2019} 
            & 10.7 &  34.6  &   33.2   & 9.7  &  3.5 \\ % need more to compare?
        \textbf{BLS} 
            & \textbf{12.6} & \textbf{35.8}   & \textbf{34.0} & \textbf{ 12.9} & \textbf{6.0} \\ 
        \bottomrule
        \end{tabular}
    }
    \caption{
        \textbf{The ablation for the resampling strategy.} RFS: repeat factor sampling; BLS: bi-level data sampling.} 
    \label{tab:resampling}
    \vspace{-0.4cm}
\end{center}
\end{table}




\subsubsection{Ablation Study}
\label{subsec:abla_sty}
% \noindent \textbf{4.1.3~Ablation Study}

% 一个表格对应一个 paragraphs
% We perform several ablation experiments to analysis the effectiveness of our method components.
% The results are shown in Tab~\ref{struct_ablation} \ref{tab:resampling} \ref{tab:iter_num}.

\paragraph{Model Components}
% Tab~\ref{struct_ablation} summarizes the results of our ablation experiments, in which we compare our full model with several partial model settings. From the table, we find the naive one-stage bipartite graph neural network can achieve \textbf{10.5} performance gain compared with the baseline(9.7). Moreover, the confidence-aware message propagation promotes the mean recall to \textbf{11.7} with a margin at 1.2. Finally, multiple stages can further refine the results to \textbf{12.6}.
We first evaluate the importance of each component in our BGNN. 
As shown in Tab.~\ref{struct_ablation}, we incrementally add one component to the plain baseline to validate their effectiveness.
We observe the plain message propagation(without adaptive confidence-aware mechanism) on bipartite graph neural network improves the baseline with \textbf{8}\% and achieves \textbf{10.5} on mR@100.
Besides, the confidence-aware message propagation mechanism promotes the performance to \textbf{11.7},
and multiple stages refinement further improves the final results to \textbf{12.6}.

%From the table, we nd the learnable magnitude could achieve a signicant improve- m ent com pared w ith baseline and the learnable m argin is also able to achieve com petitive results at 49.3% , w hich demonstrate the effectiveness of our models

%First, we compare the single-layer STANet with the multi-
%layer model in Table 1 (last two rows). We can see that the multi-layer STANet further promotes the accuracies in both few-shot settings. While the improvement seems mild, it demonstrates that more STA layers can refine the image representations
\vspace{-4mm}
\paragraph{Re-sampling Strategies}
We compare the widely-used repeat factor sampling~\cite{gupta_lvis:_2019} with BLS for validating the proposed bi-level data sampling strategy~(BLS).
As shown in in Tab.~\ref{tab:resampling}, we find our proposed BLS outperforms the baseline and RFS with a large margin, especially on \textit{body} and \textit{tail} categories.
Besides, unlike other methods, BLS maintains the performance on head categories. This indicates our sampling can balance the prediction on all category groups.

\vspace{-4mm}
\paragraph{Stages and Iterations of BGNN}
% Using one stage of BGNN or one iteration of message propagation always limits the visual representation. 
To investigate the multiple stage refinement and iterative context encoding in BGNN, we incrementally apply several sets of hyper-parameters on number of stages $N_t$ and iterations $N_i$ in model design.
% We conduct an ablation study to explore the influence of multiple iterations and multiple stages and report the performance in Tab~\ref{tab:iter_num}.
The quantitative results in Tab.~\ref{tab:iter_num} indicate the message propagation with 3 iterations achieves the best performance in one-stage BGNN.
Furthermore, by freezing the iteration number $N_i$ as 3, we find the performance increases with more stages, and will saturate when $N_t$=3.
% For the multi-stage refinement, the three groups' performance improves sustainedly and finally saturates when the stage number reaches 4.


%%%%%%%%%%%%move to suppl.%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%

% =======
% To better understand the BGNN, we visualize the generated scene graphs in Visual Genome dataset. 
% As shown in Fig.~\ref{fig:qualitative},
% our model is eligible to cope with the large intra-class variations, which is illustrated in Fig~\ref{fig:qualitative}. 
% Compared with GPS-Net, our model can enhance the modeling capability with confidence-aware message propagation, 
% which has a significant improvement on tail categories by utilizing the noise-less scene context and better 
% data-sampling strategy.
% >>>>>>> 06b31dc752e8888d528bb151884d736725a5a807


\begin{table}[]
    \centering
    \resizebox{7.2cm}{!}{
    \begin{tabular}{cc|cc|ccc}
    \toprule
          &        & \multicolumn{5}{c}{\textbf{SGGen}}                        \\ 
    \midrule
    $N_t$ & $N_i$ &  \textbf{mR@100} & \textbf{R@100} & \textbf{Head} & \textbf{Body} & \textbf{Tail} \\
    \midrule
    1 & 1 &   10.0 & 35.3 &  33.5  &  9.3  &  4.0  \\
    1 & 2 &  10.5 & 35.5 &  34.0  &  9.4    &  4.2  \\
    1 & 3 &  \textbf{10.8} & \textbf{35.3} &  \textbf{33.8}  &  \textbf{10.6}  &  \textbf{4.6}  \\\midrule 
    % 1 & 3   &  35.6  &   11.2  &  34.0  &  10.4  &  4.6    \\
    % 2 & 3   &  35.7  &   11.9  &  33.6  & 10.8   &  5.0   \\
    % 3 & 3   &  35.8  &  12.6   &  34.0  &  12.9  &  5.6    \\ 
    1 & 3   &  10.8 & 35.3 &  33.8  &  10.6  &  4.6  \\
    2 & 3   &  11.1 & 35.6 &  34.0  &  11.3  &  5.3   \\
    3 & 3   &  \textbf{12.6} & \textbf{35.8} &  \textbf{34.0}  &  \textbf{12.9}  &  \textbf{6.0}    \\ 
    4 & 3   &  12.5 & 35.2 &  34.4  &  12.2  &  5.7    \\
    % 6  & 3 & 4   &  10.7  &  12.5   &  34.4  &  12.2  &  5.7    \\ 
    \bottomrule
\end{tabular}
    }
    \caption{\textbf{The ablation for the different graph iteration parameters.} 
    The ablation of different iteration number for iterative refinement models in our methods. }
    \label{tab:iter_num} 
    \vspace{-0.3cm}

\end{table}







%As shown in figure \ref{fig:qualitative}, we visualize the results of scene graph generation from the Visual Genome dataset, by comparing our method with GPS-Net, to 

%, and using 3 stagerank adjacency matrices can achieve 1.4 APbox and 1.2 APbox gain compared to the baseline. However, we note that inserting such mixture LatentGNN module into multiple stages achieves little gain, possibly due to the global context encoded by the multi-stage model.

%There are two iterations based modules (Bipartite GNN, Multi-stage refinement) in our method. In table \ref{tab:iter_num} we shows difference of chooses iterations number for message propagation $N_i$ and refinement stages $N_t$, respectively.
%As shown in the upper part of table \ref{tab:iter_num}(line 1-3), by fixing the iteration number graph stage number as one, we can see that the two or three iterations might be the proper iterations number for our bipartite graph neural.
%We further show the performance comparison between the different number of graph refinement ?stage.
%The lower part of table \ref{tab:iter_num} shows by fixing the graph iterations number as 3, and we gradually increase refinement stage, the performance on three categories sets improves and finally saturates when the number of refinement stage reached 4.


%We ablatively show the effectiveness of model design on the Visual Genome dataset.
%The incremental manner is used to show how each part of the model impacts performance.
%As shown in Table \ref{struct_ablation}, the left-hand side of each character represents the different modules in our method. We start from the simple baseline model we have referred to above.

%\textbf{Bipartite Graph Neural Network}(\textbf{B}): 
%By comparison between lines 1-2 in table \ref{struct_ablation}, we can see that the bipartite graph model contributes to the critical performance gain in the low-frequency category.

%\textbf{Adaptive Confidence-aware Message Propagation}(\textbf{A}):
%Based on the bipartite graph model, we further add the novel confidence aware adaptive message propagation mechanism, to show the effectiveness of this module.
%By comparing the line 2-3 in table \ref{struct_ablation}, it shows that with better topology for context encoding, we further obtain the performance improvement on three category sets.
%
%
%\textbf{Multi-stage Graph Refinement}(\textbf{M}):
%We further show the effectiveness of the multi-stage graph topology refinement.
%By comparing with line 3-4 in table \ref{struct_ablation}, we can see that the multistage refinement bring an anther boost on low-frequency categories set, especially body categories.
%
%\textbf{Instance Based Resampling Strategy}:
%As shown in table \ref{tab:resampling}, we compared our model in two resampling strategies and without the resampling strategy. 
%For lines 1-2, the results show the LVIS resampling obtain visual improvements on the tail categories set. For line 2-3, by comparing the LVIS resampling strategy with ours, it shows our design further improve the performance on body and tail categories set.
%

%\textbf{The Number of iteration}:



% \begin{table}[]
%     \begin{center}
%         \resizebox{0.45\textwidth}{!}{
%             \begin{tabular}{cccc|lcccc}
%             \toprule
%             \multicolumn{4}{c|}{module} & \multicolumn{5}{c}{SG Det}                               \\ \midrule
%             \textbf{B}&\textbf{H}&\textbf{S}&\textbf{D}& R@100 & mR@100 & head & body & tail \\ \midrule
%             \xmark&\xmark&\xmark&\xmark &\multicolumn{1}{c}{34.1} & 9.7  & 32.1& 9.3  & 3.0 \\
%            \cmark &\xmark&\xmark&\xmark &\multicolumn{1}{c}{34.8} & 11.8 & 32.4 & 11.5 & 5.5    \\ 
%            \cmark &\cmark&\xmark&\xmark &\multicolumn{1}{c}{33.9} & 11.6 & 30.7 & 11.8 & 5.4     \\
%            \cmark &\cmark&\xmark&\cmark &\multicolumn{1}{c}{34.4}  &12.4& 30.6 & 12.6 & 6.7 \\
%            \cmark &\xmark&\cmark&\xmark& \multicolumn{1}{c}{35.3} & 10.8& 33.6 & 10.4  & 4.6  \\
%            \cmark &\xmark&\cmark&\cmark & \multicolumn{1}{c}{35.8} & 12.6& 34.0 &  12.9 & 6.0 \\
%            \midrule
%           \bottomrule   
%         \end{tabular}
%         }
%     \end{center}
%     \caption{The ablation for the different model structure. The each characters of left hand represent the different module add into the whole model's pipeline. }
%     \label{struct_ablation}
% \end{table} 



% \begin{table}[]
%     \centering
%     \resizebox{0.45\textwidth}{!}{
%     \begin{tabular}{c|cc|cccc|ccc}
%     \toprule
%        &           &             & \multicolumn{7}{c}{SG Det}                        \\ 
%     \midrule
%     id & $N_g$ & $N_r$ & R@50 & R@100 & mR@50 & mR@100 & head & body & tail \\
%     \midrule
%     3  & 3 & 1 &  30.4   &   35.3    &  9.1   &  10.8   &  33.8  &  10.6  &  4.6  \\
%     4  & 3 & 2 &  30.1   &   35.65   &  9.33  &  11.13  &  34.0  &  11.3  &  5.3   \\
%     1  & 1 & 3 &  31.5   &    35.7   &  9.3   &   11.2  &  34.0  &  10.8  &  5.0    \\
%     2  & 2 & 3 &  30.4   &   35.3    &  10.0  &   11.9  &  33.6  &  10.4  &  4.6   \\
%     5  & 3 & 3 &  30.1   &   35.7    &  10.7  &  12.6   &  33.5  &  12.9  &  5.6    \\ 
%     6  & 3 & 4 &  30.6   &   35.2    &  10.7  &  12.5   &  34.4  &  12.2  &  5.7    \\ 
%     \midrule
%     \bottomrule
% \end{tabular}
%     }
%     \caption{The ablation for the different graph iteration parameters.}
% \end{table}

%  the overall&mrecall@K performance table
% \begin{table*}[!ht]
%     \begin{center}
%         \resizebox{\textwidth}{!}{
%             \begin{tabular}{c|c|cccccccccccccc}
%                 \toprule
%                  & & \multicolumn{4}{c}{Predicate Classification}       & \multicolumn{4}{c}{Scene Graph Classification}     & \multicolumn{4}{c}{Scene Graph Detection} \\ \midrule
                
%                 B & \multicolumn{1}{c|}{Models}   & R@50 & R@100 & mR@50 & \multicolumn{1}{c|}{mR@100} & R@50 & R@100 & mR@50 & \multicolumn{1}{c|}{mR@100} & R@50    & R@100    & mR@50    & mR@100    \\ \hline
%                 \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}} \rotatebox{90}{ VGG16 } \end{tabular}}  & \multicolumn{1}{c|}{Motifs\cite{zellers_neural_2017} \cite{tang_learning_2018}} &  65.2  &  67.1 &14.0 & \multicolumn{1}{c|}{15.3} & 35.8 & 36.5 &  7.7  & \multicolumn{1}{c|}{8.2}    & 27.2 & 30.3 & 5.7  &  6.6     \\  % VGG16 results
%                 & \multicolumn{1}{c|}{Graph-RCNN \cite{yang_graph_2018}}   & 54.2  & 59.1& - & \multicolumn{1}{c|}{-} & 29.6 & 31.6 & - & \multicolumn{1}{c|}{-}& 11.4& 13.7& - & -   \\    
%                 & \multicolumn{1}{c|}{FREQ \cite{zellers_neural_2017}\cite{tang_learning_2018}}& 60.6  & 62.2& 13.0 & \multicolumn{1}{c|}{16.0} & 32.3 & 32.9 & 7.2 & \multicolumn{1}{c|}{8.5}& 26.2& 30.1& 6.1 & 7.1   \\              
%                 & \multicolumn{1}{c|}{VCTree\cite{tang_learning_2018}}   & 66.4 &  68.1& 17.9 & \multicolumn{1}{c|}{19.4} & 38.1 & 38.8 &  10.1 & \multicolumn{1}{c|}{10.8}   & 27.9 & 31.3 & 6.9  &  8.0     \\
%                 &  \multicolumn{1}{c|}{KERN \cite{chen_knowledge-embedded_2019}}    & 65.8 &  67.6 &  17.7 & \multicolumn{1}{c|}{19.2}& 36.7 & 37.4 &  9.40 & \multicolumn{1}{c|}{10.0}   & 27.1 & 29.8 & 6.4  &  7.3     \\
%                 & \multicolumn{1}{c|}{GPS-Net\cite{lin_gps-net_2020}}  &  69.7& 69.7 &  - & \multicolumn{1}{c|}{22.8} & 39.2 & 40.1 &   -   & \multicolumn{1}{c|}{ 12.6}  & 28.4 & 31.7 &  -   &  9.8     \\ \midrule
%                 \multirow{11}{*}{\begin{tabular}[c]{@{}c@{}}\rotatebox{90}{ ResNeXt-101-FPN } \end{tabular}}  
%                 &  \multicolumn{1}{c|}{RelDN \cite{zhang_graphical_2019}} &68.7 & 68.8  &- & \multicolumn{1}{c|}{-}& 38.9 & 38.9& - & \multicolumn{1}{c|}{-}  & 31.0  & 36.7 & - &  -  \\ \cline{2-14}  % Res101 results
%                 &  \multicolumn{1}{c|}{Baseline} &64.8 & 66.7  &15.8  & \multicolumn{1}{c|}{17.2}& 38.1 & 39.3& 9.3 & \multicolumn{1}{c|}{9.61}    & 31.4  & 35.9 & 6.0 &  7.3     \\ % Res101 results
%                 &  \multicolumn{1}{c|}{Motifs\cite{tang_unbiased_2020}}& 66.8 & 67.9 & 14.6& \multicolumn{1}{c|}{15.8} & 39.0& 39.8 & 8.0  & \multicolumn{1}{c|}{8.5} & 32.1  & 36.9 & 5.5 &  6.8     \\
%                 &  \multicolumn{1}{c|}{VCTree\cite{tang_unbiased_2020}}&65.5 &  67.4& 15.4 & \multicolumn{1}{c|}{16.6} & 38.9& 39.8 &  7.4 & \multicolumn{1}{c|}{7.9} & 31.8  & 36.1 & 6.6 &  7.7     \\
%                 &  \multicolumn{1}{c|}{GPS-Net$\dagger$}  & 65.2  & 67.1 &  15.2& \multicolumn{1}{c|}{16.6}  & 37.7 & 39.2  & 8.5   & \multicolumn{1}{c|}{9.13}   & 31.4  & 36.0 & 6.7 &  8.3     \\
%                 &  \multicolumn{1}{c|}{Graph-RCNN$\dagger$}  &      &       &       & \multicolumn{1}{c|}{}& 37.7 & 39.2  & 8.5   & \multicolumn{1}{c|}{9.13}   & 28.6  & 32.2 & 5.8 &  6.6     \\
%                 & \multicolumn{1}{c|}{MSDN$\dagger$ \cite{li_scene_2017}}   & 65.2  & 68.0 & 15.9 & \multicolumn{1}{c|}{17.5} & 38.3 & 39.4 & 9.3 & \multicolumn{1}{c|}{9.7}& 31.0  & 35.3 & 6.5 &  7.9  \\         
%                 &  \multicolumn{1}{c|}{Motifs$^{*}$\cite{tang_unbiased_2020}} & 64.4 &  66.7 & 18.5 & \multicolumn{1}{c|}{20.0}    & 37.9 & 38.8  & 11.1  & \multicolumn{1}{c|}{11.8}  & 30.5 & 35.4 & 8.2 &  9.7     \\
%                 &  \multicolumn{1}{c|}{Ours$^{-}$}   & 65.5 &  67.2 & 17.4 & \multicolumn{1}{c|}{18.8}  & 38.5& 39.9 &  9.9 & \multicolumn{1}{c|}{10.6}  & 31.3  & 36.1 & 8.2 &  9.7     \\ 
%                 &  \multicolumn{1}{c|}{Unbiased\cite{tang_unbiased_2020}}  & 50.7&  54.9 & 25.3 & \multicolumn{1}{c|}{28.4}   & 25.6& 28.5 & 12.9  & \multicolumn{1}{c|}{14.8}  & 18.7  & 22.6 & 8.6 &  10.2    \\
%                 &  \multicolumn{1}{c|}{\textbf{Ours}}    & 58.2 &  60.3 & 30.4 & \multicolumn{1}{c|}{32.9}& 37.4& 38.6 & 13.3 & \multicolumn{1}{c|}{15.0}  & 30.1 & 35.8 & 10.7&  12.6\more{(+2.4)}   \\ \midrule
%             \bottomrule
%             \end{tabular}
%         }
%     \end{center}
% \caption{The SGG performance on Recall@K and mRecall@K on graph constraints setting, the models that implemented by us in our codebase are denoted by subscript $\dagger$. }
% \label{overall_table}
% \end{table*}


\begin{table}[]
    \centering
    \resizebox{0.47\textwidth}{!}{
        \begin{tabular}{l|l|cc|cc|c}
            \toprule 
            \multirow{2}{*}{\textbf{D }} & \multirow{2}{*}{\textbf{Models}}  &\multirow{2}{*}{\textbf{mR@50}}&  \multirow{2}{*}{\textbf{R@50}} &\multicolumn{2}{c|}{\textbf{wmAP}} & \multirow{2}{*}{\textbf{score}\scriptsize{wtd}}   \\ 
                            \cmidrule{5-6}
                            &  &  &   & \textbf{rel}& \textbf{phr} &   \\ 
                            \midrule
        \multirow{6}{*}{\rotatebox{90}{V4}} 
                            & RelDN\cite{lin_gps-net_2020}  
                                        & -    & 74.94  & 35.54 & 38.52 & 44.61     \\
                            & GPS-Net\cite{lin_gps-net_2020}  
                                        & -    & 77.27  & 38.78 & 40.15 & 47.03     \\  \cmidrule{2-7} 
                            & RelDN $^\dagger$ & 70.40  & 75.66 & 36.13 & 39.91 & 45.21     \\
                            & GPS-Net$^\dagger$  & 69.50  & 74.65  & 35.02 & 39.40 & 44.70     \\
                            \cmidrule{2-7} 
                            & \textbf{BGNN} & \textbf{72.11} & 75.46 & 37.76 & 41.70 & 46.87    \\ 
                            % & \textbf{Ours}     & 74.91 & 75.18  & 35.82 & 39.55 & 45.44    \\ 
                            \midrule
        \multirow{10}{*}{\rotatebox{90}{V6}} 
                            & RelDN $^\dagger$  & 33.98 & 73.08 & 32.16 & 33.39 & 40.84   \\
                            & RelDN $^{\dagger*}$ & 37.20 & 75.34 & 33.21 & 34.31 & 41.97 \\
                            & VCTree$^\dagger$  &  33.91 & 74.08 & 34.16 & 33.11 & 40.21   \\
                            & G-RCNN$^\dagger$  &34.04& 74.51 & 33.15 & 34.21 & 41.84   \\
                            & Motifs$^\dagger$ & 32.68 &71.63& 29.91 &31.59   & 38.93    \\
                            & Unbiased$^\dagger$ & 35.47 & 69.30 & 30.74  & 32.80  & 39.27   \\ 
                            & GPS-Net$^\dagger$ & 35.26 & 74.81 & 32.85&33.98 & 41.69 \\
                            & GPS-Net$^{\dagger*}$ & 38.93 &74.74 & 32.77  &33.87 & 41.60    \\ \cmidrule{2-7} 
                            % & \textbf{Ours$^-$}  & 38.11 & 75.04  & 33.65 & 34.81 & 42.39    \\  
                            & \textbf{BGNN}    & \textbf{40.45} & 74.98  & 33.51    & 34.15   & \textbf{42.06}     \\ 
                            \bottomrule
        \end{tabular}
    }
    \caption{\textbf{The Performance table of Open Images Dataset.} 
    $*$ denotes the resampling~\cite{gupta_lvis:_2019} is applied for this model.
    $\dagger$ denote results reproduced with the authors' code. } 
    \label{tab:openimage}
    \vspace{-0.4cm}
\end{table}


\subsection{Open Images} \label{lab:oi_exp}

\subsubsection{Experiments Setting}
% \noindent \textbf{4.2.1~Experiments Setting} 

\paragraph{Dataset Details.} 

The \textbf{Open Images} dataset~\cite{OpenImages} is a large-scale dataset proposed by Google recently. Compared with Visual Genome dataset, it has a superior annotation quality for the scene graph generation. In this work, we conduct experiments on Open Images  V4\&V6, we follow the similar data processing and evaluation protocols in~\cite{zhang_graphical_2019, OpenImages, lin_gps-net_2020}. 

% The details of dataset specs are reported in Suppl.
%%%%%%%move to suppl.%%%%%%%%%%
The Open Images V4 is introduced as a benchmark for scene graph generation by~\cite{zhang_graphical_2019} and~\cite{lin_gps-net_2020}, which has 53,953 and 3,234 images for the train and val sets, 57 objects categories, and 9 predicate categories in total.
The Open Images V6 has 126,368 images used for training, 1813 and 5322 images for validation and test, respectively, with 301 object categories and 31 predicate categories. 
This dataset has a comparable amount of semantics categories with the VG.
%%%%%%%%%%%%%%%%%

\vspace{-4mm}
\paragraph{Evaluation Protocol}\label{subsec:eval_prot}
For Open Images  V4\&V6, we follow the same data processing and evaluation protocols in~\cite{zhang_graphical_2019, OpenImages, lin_gps-net_2020}. 
The mean Recall@50 (mR@50), Recall@50 (R@50), weighted mean AP of relationships ( $\text{wmAP}_{rel}$), and weighted mean AP of phrase ($\text{wmAP}_{phr}$) are used as evaluation metrics. 
Following standard evaluation metrics of Open Images refer to~\cite{OpenImages, zhang_graphical_2019, lin_gps-net_2020}, the weight metric $\text{score}_{wtd}$ is computed as: $\text{score}_{wtd} = 0.2 \times \text{R}@50 +0.4 \times \text{wmAP}_{rel} + 0.4 \times \text{wmAP}_{phr}$.
Besides, we also report mRecall@K like Visual Genome as a balanced metric for comprehensive comparison.

% the $\text{wmAP}_{rel}$ evaluates the AP of the predicted triplet where both the subject and object boxes have an IoU of at least 0.5 with ground truth. 
% The $\text{wmAP}_{phr}$ evaluates the union area of the subject and object boxes.
% Since the mAP and recall is easy to be dominated by the high frequency category, the weighted mAP is scaling by the frequency ration of each categories in val set, that can be view as a balanced metric for each categories. 

% \begin{align}

% \end{align} 
%$$. 
\subsubsection{Quantitative Results}
% \noindent \textbf{4.2.2~Quantitative Results}


Performance on these two datasets are reported in Tab.~\ref{tab:openimage}. 
For Open Images V4, which includes only 9 predicate categories in total, make it constrained to explore the biased scene graph generation tasks. 
Our method can still achieve competitive results on weighted metric $\text{score}$, and outperform the previous work on mean recall with a significant margin.
For Open Images V6 with 31 predicate categories,
we reimplement several recent works~\cite{lin_gps-net_2020,tang_unbiased_2020, tang_learning_2018, zellers_neural_2017, yang_graph_2018} for fair comparison. As shown in Tab.~\ref{tab:openimage}, our method achieves the state-of-the-art performance on mean recall and competitive results on weighted metric $\text{score}$. Results on both datasets demonstrate the efficacy of our approach.
%
%The upper part of the table shows the comparison between our method and other methods also test on OpenImage V4. 
%The results show that we achieve comparable performance with the previous methods.
%For OpenImages V6, since there are few works test on this dataset, we both compare their methods with/without resampling strategy with our methods for fair benchmark the methods are implemented in our codebase.
%The results show that we achieve the SOTA performance on standard OpenImages evaluation metrics and the mean recall metric.


% \begin{table}
%     \begin{center}
%     \resizebox{0.9\textwidth}{!}{
%     \begin{tabular}{l|ccccc|cccccc}
%     \hline
%             & \multicolumn{5}{c}{Open Images V4}   & \multicolumn{5}{c}{Open Images V6}            \\ \hline
%     \textbf{Model}     & mR@50 & R@50 & wmAP$_{rel}$ & wmAP$_{phr}$ & score$_{wtd}$ & mR@50 & R@50 & wmAP$_{rel}$ & wmAP$_{phr}$ & score$_{wtd}$ \\ \hline
%     RelDN      & -    & 74.94  & 35.54 & 38.52 & 44.61    &   -    &   -   &     -     &      -    &    -       \\
%     GPS-Net\cite{lin_gps-net_2020}   
%                & -    & 77.27  & 38.78 & 40.15 & 47.03    &   -    &   -   &     -     &      -    &    -       \\
%     GPS-Net$^\dagger$     & 69.50 & 74.65& 35.02 & 39.40 & 44.70 & 35.26 & 74.81 & 32.85 & 33.98 & 41.69 \\
%     GPS-Net$^{\dagger *}$ & 74.65 & 69.5 & 35.02 & 39.40 & 44.70 & 38.93 &74.74 & 32.77  &33.87 & 41.60 \\
%     Baseline              & 67.47 & 75.43 & 35.98 & 39.92 & 45.45  & 33.98 & 73.08 & 32.16 & 33.39 & 40.84       \\
%     Baseline $^*$         & 72.40 & 75.67 & 36.7 & 40.49 & 46.01   & 37.20 & 75.34 & 33.21 & 34.31 & 41.97  \\
%     VCTree                & - & - & -    & -    & -   &  33.91 & 74.08 & 34.16 & 33.11 & 40.21   \\
%     Unbiased              & - & - & -    & -    & -  & 35.47 & 69.30 & 30.74  & 32.80  & 39.27   \\
%     Motifs                & - & - & -    & -    & -  & 32.68 &71.63& 29.91 &31.59   & 38.93     \\
%     \textbf{BGNN}         & 74.91 & 75.18  & 35.82 & 39.55 & 45.44  & 40.45 & 74.98  & 32.99  & 34.15   & 41.85         \\ 
%     \midrule \bottomrule
%     \end{tabular}
%     }
% \end{center}
%     \caption{The Performance table of Open Images Dataset. The models' names that are denoted by subscript $*$ are additionally associated with the resampling strategy. The models implemented by us in our codebase are denoted by subscript $\dagger$.}
%     \label{tab:openimage}
% \end{table}

% wmAP$_{rel}$ & wmAP$_{phr}






