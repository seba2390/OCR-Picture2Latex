\section{Our Method}
% what we do,
In this work, we mainly proposed a more effective graph message passing pipeline of GNN for scene graph generation, which boost the performance of recognizing the adjust the topology of graph and gating dynamically and iteratively with the awareness of relatedness information of relationship proposals. 

% from challenge to insight
In previous works, the GNN with the message passing mechanism is widely used in the scene graph generation tasks for aggregating the context information(e.g. co-occurrence, similarity.) on proposals scene graph. To obtain the more discriminative feature representation for relationship recognition.

However, due to special properties of scene graph/visual relationships, the noise during the message passing is cannot be ignored.  
The ground-truth relationships are much sparse than relationships proposals predicted by model, there are massive number of negative connections on proposal scene graph. Besides, the relationship is naturally high variance due to its combination properties. 

The previous works apply the plain GNN for aggregating the context information based on feature extracted by VGG16 backbone. 
However, with the better detection and backbone features(ResNet-101 FPN), the initial representation is much better than before.
To achieve better performance, the more efficient features augmentation is required.
This trivial way of apply GNN is not inefficient aggregate the context information, can only improve the performance marginally compared with the plain baseline. 

% our insight and pipeline
According the principle of the visual relationships, the context information of visual relationships are mainly provided by the adjacent relationships. 
Those connections can be mark as "relatedness", which is the status of whether has any relationship between the entities and their neighbors entities.

In this work, we proposed a dynamic GNN message passing model, that explicitly utilize the relatedness information as proxy, and iteratively refine the optimize the topology of message passing. 
Specifically, the model perceive the relatedness information for modify topology of message passing dynamically, by reduce the un-necessary connections by adjust the topology of graph by a course to fine manner(hard to soft). 
In this way, the less search space during the training can provide more efficient parameters learning. In inference time, it also reduce the noise of system, which lead to more effective features augmenting. 
Further more, the model is works iteratively as a closed loop system, the model will iteratively produce the better relatedness information on refined features. 

% how we introduce our method
In Section 3.1 Overview, we formally introduce the input and output of SGG task and our new proposed model, and the pipeline of our method at module level.
Then we introduce the model we proposed, called relatedness-aware GMSP in section 3.2.
In section 3.3, we introduce the iterative mechanism between the relatedness aware module and GMSP module. 
In section 3.4, we introduce the design of the relatedness module.

\subsection{Overview}


\begin{figure*}
    \centering
    \includegraphics[width=16cm]{imgs/rel_aware_overview.pdf}
    \caption{}
    \label{pipeline_overview}
\end{figure*}
% problem formulations
First, we formally introduce the input and output of the scene graph generation task. 
Give a images $\mathcal{I}$, the model need to predict the scene graph of this image, which is the a set of visual relationship between the objects.
Here we formulate the scene graph as $\mathcal{G}=\{\mathcal{O}, \mathcal{R}\}$, $\mathcal{O} = \{o_i\}_{i=1}^{N}$ is the set of objects in the images, and a set of relationships $\mathcal{R}$ between them. 
The relationship $\mathcal{R} = \{r_k\}_{k=1}^{M}$ is the triplet $<o_i, p_{k}, o_j>$. 
Specifically, it is composed by subject entities $o_i$, object entities $o_j$ and the predicate $p_k$ between them. The categories of entities and predicates are in the $C_{ent}$ and $C_{rel}$ respectively.


% how we do, big modules and pipeline
As shown in fig.\ref{pipeline_overview}, our method can be summarized as the following steps: 
(1) Build up the proposal scene graph: detect and pair the entities, and extract the relationship features according to the proposal scene graph.
(3) Compute the "relatedness" for relationship proposals by Relatedness module.
(4) Feature augmentation by relatedness aware message passing on proposal scene graph.
(5) Predict the relatedness score on feature refined by graph, and iteratively apply message passing according to the refined relatedness score.

Our method follows the wildly used bottom-up scene graph generation pipeline. 
The main focus of our work is to build a more efficient feature refined model for scene graph generation utilizes the relatedness information.

First, we obtain the $N$ entities proposal by the off-the-shelf object detection model(Faster-RCNN, ResNet101-FPN) for each entity $o_i$, and extract its representation $\mathbf{x}_i \in \mathbb{R}^{D_e}$ (or $\mathbf{X} \in \mathbb{R}^{N \times D_e}$ for whole all entities) including visual, semantic and geometry following the baseline of Motifs\cite{zellers_neural_2017}.
Based on the entities, the $M$ relationship proposals $\mathcal{R}'$ are produced by fully connected pairing the entities $r_k = <\mathbf{e}_i, \mathbf{e}_j>$. 
The representation for relationship proposals $\mathbf{u}_k \in \mathbb{R}^{D_u}$(or $\mathbf{U} \in \mathbb{R}^{M \times D_r}$ for all predicates proposals of image) to represent the visual and geometry features. 
Those features for the relationship proposals are input for our proposed method. Below we will describe the details of our model architecture.



% detail of obtain the entities and predicates features

% In this module, our goal is to provide the proposal relationships $\mathcal{R'}$ and scene graph proposal $\mathcal{G'}$ build on it. 
% , we can get its bounding box $\mathbf{b}_i, \mathbf{b}_i = [x_1, y_1, x_2, y_2]$, categories $c_{i}^{ent}$ from $C_{ent}$ entities categories, and categories probability distribution $\mathbf{p}^e_i \in \mathbb{R}^{C_{ent}}$ predict by classifier.
% From those $N$ entities, we can pair them by a fully connected manner to get the relationships proposal $\mathcal{R'}$.
% Then we extract the feature representation from the relationship proposals, it mainly composed by three parts, visual feature of entities and predicates, geometry and semantic features of entities pairs, similar to the previous work \cite{zhang_graphical_2019,zellers_neural_2017, tang_learning_2018}.

% Specifically, the features for proposal relationships are entities features $\mathbf{x}_i \in \mathbb{R}^{2048}$ and predicates features $\mathbf{u}_i \in \mathbb{R}^{4096}$ for each relationship proposals triplets. 
% For the entities feature is generated from the its visual feature $\mathbf{v}_i$ from the ROI-align on visual feature of image, and it bounding box information $\mathbf{b}_i$, with the semantic feature by weighted sum of pre-trained word embedding by classification distribution $\mathbf{e}_i = \mathbf{E} \cdot \mathbf{p}^e_i$. To represent this process in formal ways, is shown at eq.\ref{entities_feature}, the $[\cdot ;\cdot ;\cdot]$ is concatenating operation.
% \begin{equation}
% \label{entities_feature}
% \mathbf{x}_i = F_{en}([\mathbf{v}_i;F_{geo}(\mathbf{b}_i);\mathbf{e}_i])
% \end{equation}

% For the predicates features$\mathbf{u}_{ij}$, are generated for the visual feature $\mathbf{v}^u_{ij}$ is ROI-aligned from the feature of image by the union box of two entities bounding boxes, the geometry features: $[\mathbf{b}_i;\mathbf{b}_j]$ of two entities boxes coordination. As shown in eq.\ref{predicate_feature}
% \begin{equation}
% \label{predicate_feature}
% \mathbf{u}_{k} = F_{pre}([\mathbf{v}^u_k;F_{geo}([\mathbf{b}_i;\mathbf{b}_j])])
% \end{equation}

% Hence it is a massive number of relationships, we will filter them in the first stage by ranking according to their quality. The quality of relationship proposals is defined by the multiplication of two entities classification scores and relatedness score predict by the 1st stage relatedness module. This quality score represents the confidence of two entities and how they related to each other.
% In this way, for each relationship proposals $R'$, we can have such a representation $<\mathbf{e}_i, \mathbf{u}_{k}, \mathbf{e}_j>$, this representation will be input for following relatedness module and GMSP module. 


\subsection{Relatedness-aware Graph Message Passing}
% 在这个section主要去介绍 RA GMSP的若干输入，包括 relationship proposal 以及他们的representation
In this section, we will introduce the relatedness aware GMSP model detailed.
Specifically, our method has three main parts: 
a) The relatedness module predicts the relatedness information for each relationship proposals. b) The relationships aware message passing module take the relatedness information and dynamically adjust the topology of message passing process. c) The iterative refinement process between the two modules for better performance.

%from general MSP to the sgg MSP with relatedness denoising
% general MSP formulate
\subsubsection{The General Graph Message Passing}
We start from the generic graph message passing pipeline for scene graph generation based on the previous works to introduce our method. 
The high-level definition of the the this feature aggregating model is shown in eq. \ref{gen_GMPS_refine}. The model is a GNN, work as a non-linear function $\mathcal{M}$ that takes the relationship proposals features $\mathbf{X},~ \mathbf{U}$, and refine along the graph topology $G$.
The $\mathbf{\hat{X}},~ \mathbf{\hat{U}}$ is the refined features,
\begin{align}
    \mathbf{\hat{X}},~ \mathbf{\hat{U}}  &= \mathcal{M}(\mathbf{X},~ \mathbf{U}, G) 
    \label{gen_GMPS_refine} 
\end{align}

Specifically, the GNN model is a message passing/aggregating process.
The general message passing of graph neural network can be formulated as eq.\ref{gen_GMSP1}, \ref{gen_GMSP2}. We aggregate the representation from the neighbor nodes $\mathbf{S} \in \mathbb{R}^{N \times D_2}$ after a non-linear transformation with weight $\mathbf{W}_x \in \mathbb{R}^{D_2 \times D_1}$ to target nodes, by the adjacency matrix $\mathbf{A} \in \mathbb{R}^{N \times M}$.
The $\mathbf{A}$ is generated according to the graph topology $G=\{\mathcal{E}, \mathcal{V}\}$ and feature representations $\mathbf{T}^{t}, \mathbf{S}$ by a distance function $d(\cdot)$ as shown in eq.\ref{gen_GMSP1}
The initial representation $\mathbf{T}^{t} \in \mathbb{R}^{M \times D_1}$ is updated by the weighted aggregation with the weight matrix $\mathbf{A}$ as shown in eq.\ref{gen_GMSP2}.
The aggregation weight matrix $\mathbf{A} \in \mathcal{R}^{M \times N}$ is calculated by the distance function $d(\cdot)$ along the adjacency of $\mathcal{E}$. 

\begin{align}
    \mathbf{T}^{t+1} &= \mathbf{T}^{t} + \mathbf{A} \cdot \sigma (\mathbf{S} \cdot  \mathbf{W_s}  + \mathbf{b_s}) \label{gen_GMSP1} \\
    \mathbf{A} &= d(\mathcal{E}, \mathbf{X}^{t}, \mathbf{S}) \label{gen_GMSP2}
\end{align}

In message passing process of the scene graph generation, there could be different nodes types as the $\mathbf{S}$ and $\mathbf{T}^{t}$. Different nodes types can lead to different information flow of message passing.
From a general GMPS model on scene graph generation, the information flow of message passing on proposal scene graph could have 3 types: (1)predicates to entities, (2)entities to predicates, (3)entities to entities. (predicate to predicate without condition on entities is quite of non-sence). 

This message passing process is applied on the proposal scene graph $G=\{\mathcal{E}, \mathcal{V}\}$. 
In the proposal scene graph, there have multiple nodes and topology connections during the message passing. 
Specifically, there are two types of nodes(representation):$\mathcal{V}_e$ entities representation, $\mathcal{V}_p$ predicates representation; three types of edges: $\mathcal{E}_{p\rightarrow e}$ predicates to entities, $\mathcal{E}_{e\rightarrow p}$ predicates to entities, $\mathcal{E}_{e\rightarrow e}$ predicates to entities.
The message passing process will be applied to nodes, according to the the topology defined by three kinds of edges between them. The eq.\ref{gen_GMSP_pe1, gen_GMSP_pe2} shows the one of topology that aggregate the predicate features $\mathbf{U}$ to entities features $\mathbf{X}$, with connections edges $E_{p\rightarrow e}$.
\begin{align}
    \mathbf{X}^{t+1} &= \mathbf{X}^{t} + \mathbf{A} \cdot \sigma (\mathbf{U} \cdot  \mathbf{W}_{p\rightarrow e}  + \mathbf{b}_{p\rightarrow e}) \label{gen_GMSP_pe1} \\
    \mathbf{A} &= d(\mathcal{E}_{p\rightarrow e}, \mathbf{X}^{t}, \mathbf{U})
\label{gen_GMSP_pe2}
\end{align}

\begin{figure*}
    \centering
    \includegraphics[width=16cm]{imgs/relation-aware-pipeline.pdf}
    \caption{} 
    \label{}
\end{figure*}


\subsubsection{Relatedness-awared Graph Message Passing}
According to the previous introduce, we know that the proposal scene graph has multiple negative connections that may introduce the massive noise to the model.
Instead of message passing on the initial proposal scene graph, the utilize the relatedness to dynamically control the information flows of each entities and predicates nodes.

This process can be viewed as a two stage process, the first stage, hard topology refinement, and the second stage, relatedness aware message aggregation.
In first stage, there is a function $\mathcal{F}$ that take the relatedness scores $\mathbf{P}^{prop} \in \mathbb{R}^{M \times C_{rel}}$ to adjust the topology structure $G$ by remove unnecessary edges and nodes, to generate a more clean proposal scene graph $\hat{G}$ for following message passing process by remove the unnecessary topology connections $\mathcal{E}$ from initial graph.
The relatedness scores $\mathbf{P}^{prop} $ is predicted by the relatedness module, which written as function $\mathcal{P}$ in eq. \ref{graph_clean}, the details of relatedness modules will introduce in section3.3 .
\begin{align}
    \hat{G} &= \mathcal{F}(G, \mathbf{P}^{prop})  \\
    \mathbf{P}^{prop} &= \mathcal{P}(G, \mathbf{X}, \mathbf{U})
    \label{graph_clean}
\end{align}


In second stage, aggregate weight matrix $\mathbf{A}_r \in \mathbb{R}^{N \times M}$ is computed from the new graph structure $\hat{G}$. 
Further more, relatedness information along the edges $\mathcal{E}$ is a additional input for the distance function $d_{rel}(\cdot)$ that provides a relatedness for better gating. as shown in eq.
\ref{rel_GMSP_relness1} \ref{rel_GMSP_relness2}
\begin{align}
    \mathbf{T}^{t+1} &= \mathbf{T}^{t} + \mathbf{A_r} \cdot \sigma (\mathbf{S} \cdot  \mathbf{W_s}  + \mathbf{b_s})  \label{rel_GMSP_relness1}  \\
    \mathbf{A}_r &= d(\hat{G}, \mathbf{X}^{t}, \mathbf{S}, \mathbf{P}^{prop}) 
\label{rel_GMSP_relness2} 
\end{align}


% stage one
\paragraph{Graph Topology Adjustments}
In stage one, a filtering process is applied on $G$ by $F$, with the relatedness score  $s_{k}$ of each relationships proposals.
The goal of this filtering process is to reduce the negative connections from the initial topology $\mathcal{E}$, for a more clean graph.
% ranking
For each proposal relationships, we can get a scaler $s_{k}$ that represents the relatedness confidence of $k$th relationship proposal.
% The For the $s_{k}$, we take the max probability from the distribution as the relatedness score.
% $$
% s_{k} = max_{i\in C_{rel}}(\mathbf{p}^{prop}_{k,i})
% $$
% Intuitively, we have multiple templates to match with the input feature, if one of template matched, we can say this relationship proposal is "related".

For the $N_r$ relationships proposal, we can get a set of relatedness score $\mathbf{s} \in \mathbb{R}^{M}$ for each relationship proposal $r_k$. 
Those relationships can be the edges for GMSP $\mathcal{E}_{p\rightarrow e}$ predicates to entities, $\mathcal{E}_{e\rightarrow p}$ predicates to entities.
The first stage filtering function $F$ ranks the relationships proposal according relatedness scores $\mathbf{s}$. 
With such ranking results, we remove the incontinence edges from the $\mathcal{E}_{p\rightarrow e}$, which introduce massive noise into the GMSP system.

represent this process in formal ways, the $\mathcal{E}_{p\rightarrow e}$ is represented as adjacency matrix $\mathbf{E}_{p\rightarrow e} \in \{0,1\}^{M \times N}$, $F$ produces the $\mathbf{\hat{E}}_{p\rightarrow e} $ after filtering. The 1st filtering $F$ actually is a gating function the suppress connection that below the threshold $\tau$ as a learnable parameter.

\textcolor{red}{TODO: how to formulate the stage-wise function, with the such a learnable gating parameters. }
\begin{align}
    \mathbf{\hat{E}}_{p\rightarrow e} = set~topN~rel\_prop~edges~to~1(\mathcal{E}_{p\rightarrow e}, S)
\end{align}
With such refinement, we can filter the three types of edges by such ranking strategy. 
Along the assumption, we can get the cleaner and smaller proposals spaces, which provides the better training efficacy with the smaller space space for optimization, and less noise during the inference phrase. 

\paragraph{Relatedness Aware Message Aggregation}
Based on the new topology filtered by the first stage, we design a new message passing model that takes the relatedness information as additional input to compute the aggregation matrix $A_{rel}$. 
In each connection in $\mathcal{E}$, we can use the distance function $d(\cdot)$ to compute a aggregation weight. 
As shown in eq.\ref{relness_gating}, this is an example of the gating weight $\alpha_{k\rightarrow i}^{gating}$ between the $k$th connection between the $i$th entities from the aggregation matrix $A_{rel}$. 

In our proposed method, the distance/gating values is compute from multiple information source.
As shown in eq.\ref{relness_gating}, besides the gating values $d_{ki}$ computed by visual representation of $k$th predicate and $i$th entity representation, the weight computed from relatedness probability score set $d_{ki}^{rel\_prob}$ and relatedness score $s^{relness}_{k}$, as summarized by the learnable weight $\gamma, \beta$ as a linear combination for more flexibility. 

\begin{align}
    \alpha^{gating}_{k \rightarrow i} 
    &= d(\hat{\mathcal{E}}_{p \rightarrow e}, \mathbf{x}_{i}, \mathbf{u}_k, \mathbf{p}^{prop}) \\
    &= \sigma(d_{ki}^{feat} + \gamma * d_{ki}^{rel\_prob} + \beta * s^{relness}_{k}) 
    \label{relness_gating}
\end{align}

The detail of how those weights compute from the initial input. The $\sigma$ is the non-linear function sigmoid.
\begin{align}
    d_{ki}^{feat} &= W_x \cdot \mathbf{x}_i * W_u \cdot \mathbf{u}_{k} \\
    d_{ki}^{rel\_prob} &= W_p \cdot \mathbf{p}^{prop}_{k} \\
    s_{relness} &= log(s_{k})
\end{align}

% For (1) information flow, the initial MSP process is:
% \begin{align}
%     h_{u \rightarrow i} &=  \sum_{r_{im} \in \mathcal{R'},  } \alpha^{gating}_{im \rightarrow i} * \mathbf{u_{im}} \\
%     & + \sum_{r_{ni} \in \mathcal{R'}, } \alpha^{gating}_{ni \rightarrow i} * \mathbf{u_{ni}}  \\
%     \alpha^{gating}_{ni \rightarrow i} &= sigmoid(FC_{u2e}(FC_e(\mathbf{e}_i) * FC_u(\mathbf{u}_{ni}))) \\
%     \mathbf{e}_i^{t+1} &= \mathbf{e}_pi^{t} + \lambda h_{u \rightarrow i}
% \end{align}


% proposal relationships $\mathcal{R'}$ to proposal relationships $\mathcal{R'}_{vaild}$
% Second, the the relatedness prediction prob $\mathbf{p}^{prop}_{ij}$ will be the auxiliary input for calculate the $\alpha^{gating}_{ij \rightarrow i}$
% Third, the relatedness score $s_{ij}$ will be the additional gating weight with the $\alpha^{gating}_{ij \rightarrow i}$ with the learnable weight $\gamma, \beta$

\subsubsection{Iterative refinements}
In this section, we introduce the iterative refinement process between the relatedness module and message passing module. 
After the message passing, the initial representation has been refined. In this way, the predictor can take more discriminative features for better prediction performance. 
With the better relatedness prediction, we can also get better topology refinement during the message passing. 
Hence, two models can iteratively refine as a close-loop system. So we use formulate such iterative refinement between the relatedness model and the message passing model, the topology of message passing will refine according to the features augmented in last stage.
\begin{align}
    \mathbf{\hat{X}},~ \mathbf{\hat{U}}  &= \mathcal{M}(\mathbf{X},~ \mathbf{U}, \hat{G}^t) \\
    \mathbf{P}^{t+1} &= \mathcal{P}(G, \mathbf{\hat{X}},~ \mathbf{\hat{U}}) \\
    \hat{G}^{t+1} &= \mathcal{F}(G, \mathbf{P}^{t+1})
\label{rel_GMSP_relness2} 
\end{align}



\subsection{Relatedness Module}
% high level description of modules' input output, function
% intuition of module design and 
The goal of relatedness module is mainly predict the "relatedness score" for each relationship proposals, which are used for following relationship-Aware graph message passing. 
The previous methods proposed a binary classifier that takes the entities pairs' features, e.g. visual features or geometry features of bounding boxes. 
However, due to the large variance of relationship, we find that such plain binary classifier is not sufficient for dividing the related and not related. We propose a another relatedness module can be viewed as multi-templates binary classifier.

\begin{align}
    \mathbf{P}^{prop} &= \sigma (W_{rel\_m} \cdot \mathbf{H}^{prop\_rel}  + \mathbf{b}_{rel\_m} ) 
\label{rel_module}
\end{align}
% specific inference process
Instead of using a binary classifier predictor, we use a such multi-templates relatedness module.
It has similar structure with standard multi-label classifier, the weight of relatedness model is $W_{rel\_m} \in \mathbb{R}^{(C_{rel} + 1) \times D_r}$. Intuitively, we have $N+1$ types of templates(foreground categories plus one background), to matching with the relationship hidden representation $\mathbf{h}_{k}^{prop\_rel}$ for the $k$th relationship proposals from $R'$ . 
We can get a probability distribution on category space $\mathbf{P}^{prop} \in \mathbb{R}^{N_r \times (C_{rel} + 1)}$ for each relationship proposals.
% 
\begin{align}
    \mathbf{h}_{k}^{prop\_rel}& = \sigma(( \mathbf{h}^{e}_{ij} + \mathbf{h}^{u}_k) \cdot \mathbf{W}_{rel\_h}   + \mathbf{b}_{rel\_h}) \label{rel_module_trans1} \\
    \mathbf{h}^{e}_{ij} &= \sigma([\mathbf{x}_i; \mathbf{x}_j] \cdot \mathbf{W}_{rel\_he}+ \mathbf{b}_{rel\_he})  \label{rel_module_trans2} \\
    \mathbf{h}^{u}_k &= \sigma(\mathbf{u_{k}} \cdot \mathbf{W}_{rel\_hu} + \mathbf{b}_{rel\_hu})
\label{rel_module_trans3}
\end{align}
% 
The hidden representation of $k$th relationship proposal $\mathbf{h}_{k}^{prop\_rel} \in \mathbb{R}^{D_r}$ ($ \mathbf{H}^{prop\_rel} \in \mathbb{R}^{N_r \times D_r}$ for all $N_r$ relationship proposals) and transformed from the representation of initial relationship proposals $\mathbf{X}, \mathbf{U}$ by several non-linear transformation as shown in eq.(\ref{rel_module_trans1})(\ref{rel_module_trans2})(\ref{rel_module_trans3}).

% training process
During the training, a binary cross entropy loss $L_{relness}$ are used for supervise the those $C_{rel} + 1$ templates.
\begin{align}
L_{relness} = \frac{1}{M}\sum_k^{M}\sum_i^{C_{rel} + 1} -y_{k,i} \cdot log(\mathbf{p}_{k,i}^{prop})
\end{align}
This module will be trained on all relationships annotations from the Visual Genome dataset.

% the outputs of this module for following module
The relatedness module will output the matching distribution $\mathbf{p}^{prop}_{k}$ for each relationship proposals as "relationship-aware" information for guiding the GMSP in our Relationship-Aware Graph Message Passing module.






With cleaned graph $\hat{G}$
\subsection{Resampling Strategy}
