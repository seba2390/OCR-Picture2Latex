\section{Related Works}

\paragraph{Scene Graph Generation.}
Traditional methods in scene graph generation typically utilize graph-based context-modeling strategies to learn discriminative representation for node and edge prediction. Most of them either focus on the graph structure design or leveraging scene context via various message propagation mechanisms. 

Several types of graph structure have been proposed for context modeling in literature. A popular idea is to model the context based on a sequential model (\textit{e.g.}, LSTM)~\cite{zellers_neural_2017} or a fully-connected graph~\cite{xu_scene_2017,dai_drnet_2017, li_scene_2017,yin_zoom-net:_2018,woo_linknet:_2018, wang_exploring_2019, lin_gps-net_2020}. 
In addition, recent works~\cite{tang_learning_2018, wang2020tackling, yang2019auto, qi_attentive_2018} explore sparse graph structures, which are either associated with the downstream tasks (\textit{e.g.} VQA) or built by trimming the relationship proposals according to the category or geometry information of subject-object pairs. However, these works often rely on their specific designs based on the downstream tasks, which limits the flexibility of their representations. 

%\rev{
%Prior works propose different graph structure(or \textit{topology}) for context modeling. A popular idea is modeling the context based on sequential model(\textit{e.g.}, LSTM)~\cite{zellers_neural_2017} or fully-connected graph~\cite{xu_scene_2017,dai_drnet_2017, li_scene_2017,yin_zoom-net:_2018,woo_linknet:_2018, wang_exploring_2019, lin_gps-net_2020}. 
%In addition, recent works~\cite{tang_learning_2018, wang2020tackling, yang2019auto, qi_attentive_2018} explore a sparse graph structure by trim the relationship proposals or entities connections according to the categories and geometry information, or downstream tasks(e.g. VQA).
%However, these works typically rely on the specific design, which limits their flexibility and discriminative power.
%}

% Another direction 
%There are also have different designs. Tang~\cite{tang_learning_2018}, Wang~\cite{wang_sketching_2020} proposed the tree structure, which is associated with the downstream tasks(VQA, visual saliency).
%Yang~\cite{yang_graph_2018}, Qi~\cite{qi_attentive_2018} proposed a spared graph by trim the relationship proposals according to the categories or geometry information of subject-object pairs.
%Instead of using the strong assumption for constructing the graph, we proposed a confidence-aware message propagation mechanism, which adaptively controls the information flow during the message propagation for more flexibility to modeling the scene graph context.


Another direction aims to incorporate context information into existing deep ConvNet models by exploring different message propagation mechanisms. A common strategy is to perform message passing between the entities proposals~\cite{zellers_neural_2017, tang_learning_2018, wang_sketching_2020, woo_linknet:_2018, qi_attentive_2018, wang_exploring_2019, lin_gps-net_2020, chen_knowledge-embedded_2019}, while the other aggregates the contextual information between the entities and predicates~\cite{xu_scene_2017, li_scene_2017, dai_drnet_2017, li_factorizable_2018, yin_zoom-net:_2018, yang_graph_2018, wang_exploring_2019, cong_nodis_nodate}, which also produces effective scene graph representations.

%shows that the message propagation between the entities and predicates is also effective.
%mainly adopt a entities representation.
%Most previous studies shed light on the different graph structure design for message propagation for better context modeling for the scene graph generation.
%Some methods explore the different message propagation direction between the entities and predicates.
%The ~\cite{zellers_neural_2017, tang_learning_2018, wang_sketching_2020, woo_linknet:_2018, qi_attentive_2018, lin_gps-net_2020, chen_knowledge-embedded_2019} proposed different graph that propagates the message only between the entities representation.
%The others ~\cite{xu_scene_2017, li_scene_2017, dai_drnet_2017, li_factorizable_2018, yin_zoom-net:_2018, yang_graph_2018, wang_exploring_2019, cong_nodis_nodate} shows that the message propagation between the entities and predicates is also effective.
%We shared a similar idea with the latter one. We formulate the context propagation between the entities and predicates as a bipartite graph neural network. 

Our work considers both message passing and inferring network connectivity in a single framework. In particular, we develop a generic Bipartite Graph Neural Network (BGNN) to effectively model the context of the entity and predicate proposals, and an adaptive message propagation to compute a more flexible representation. The previous SGG models~\cite{li_scene_2017, xu_scene_2017, li_factorizable_2018, yang_graph_2018} can be considered as special cases of the BGNN.


%The community has also explored the different topology for context modeling, the common idea is use the linear~\cite{zellers_neural_2017,cong_nodis_nodate} or fully connected~\cite{xu_scene_2017,dai_drnet_2017, li_scene_2017,yin_zoom-net:_2018,woo_linknet:_2018, lin_gps-net_2020} structure. 
%There are also have different designs. Tang~\cite{tang_learning_2018}, Wang~\cite{wang_sketching_2020} proposed the tree structure, which is associated with the downstream tasks(VQA, visual saliency).
%Yang~\cite{yang_graph_2018}, Qi~\cite{qi_attentive_2018} proposed a spared graph by trim the relationship proposals according to the categories or geometry information of subject-object pairs.
%Instead of using the strong assumption for constructing the graph, we proposed a confidence-aware message propagation mechanism, which adaptively controls the information flow during the message propagation for more flexibility to modeling the scene graph context.

% They proposed different structure of graph based networks and features aggregation strategies to encode the context information between the entities and predicates proposals  pairs(e.g. \cite{xu_scene_2017,  zellers_neural_2017, tang_learning_2018, wang_sketching_2020, , , , , , chen_knowledge-embedded_2019, , }).
% The Xu, et al.\cite{xu_scene_2017} uses the two subgraphs of entities and predicates, and exchanges the information between two graph, 
% the Li, et al.\cite{li_scene_2017} further aggregating features from the region representation for images caption to scene graph nodes representation.
% Chen,et al.\cite{chen_knowledge-embedded_2019} proposed categories level graph, which weighted by statistical correlations weight.
% Lin, et al. \cite{lin_gps-net_2020} proposed a fully connected entities graph, and utilize the predicates representation for indicates the direction and weighting for feature aggregation between the nodes.
% The graph structure of those methods for encoding are fully connected, and treat all connected equally, due to the sparsity of the visual relationships, it could introduce many negative sampling into the model.

% Some methods are not formulate the context structure as a fully connected graph,
% Zellers, et al. \cite{zellers_neural_2017} proposed a stacked LSTM to encode the context between the entities,
% Tang, et al.\cite{tang_learning_2018} constructs the entities of scene graph into tree-structured with jointly supervision for downstream task dependent validity(VQA).
% The Khademi, et al. \cite{khademi_deep_2020} proposed a Deep Q-learning model to prune the entities nodes from scene graph 
% Yang\cite{yang_graph_2018} design a relationship proposal network that prune the relationship proposals by ranking their relatedness score.
% Qi, et al. \cite{qi_attentive_2018} and Wang, et al. \cite{wang_sketching_2020} proposed to compose the graph structure from entities by their geometry relationship or visual saliency between the the entities respectively.
% The linear or tree structure topology contains that trim the neighborhood connection with too strict constraints, limit the flexibility of model to encode the context information. 
% Or utilizing the external linguistic and structural knowledge to augment the visual representations of relationship proposals \cite{lu_visual_2016, peyre_detecting_2019, gu_scene_2019, zareian_bridging_2020, zareian_learning_2020} \rev{maybe not to cite such knowledege based method, since we don't use}
\vspace{-4mm}
\paragraph{Long-tail Visual Recognition}
Previous works in visual recognition typically utilize re-balancing strategies to alleviate biased prediction caused by long-tail distributions.
These re-balancing strategies include dataset resampling to achieve balanced class prior~\cite{chawla_smote_2002, drummond_why_2003, shen_relay_2016, mahajan_exploring_2018}, and loss re-weighting based on instance frequency or hard-example mining~\cite{cao_learning_2019, cui_class-balanced_2019, khan_cost_2017, tan_equalization_2020, lin_focal_2017, ssd_liu_2015, li2020overcoming, lu_gridrcnn_2018}. 
Recently, \cite{hu_learning_2020,gupta_lvis:_2019} propose an instance-level re-sampling strategy for the tasks of object detection and instance segmentation.
Other approaches also explore knowledge transfer learning from head categories for long-tail classification~\cite{liu_largescale_2019, gidaris_dynamic_2018} or develop the two-stage learning scheme~\cite{zhou_bbn_nodate, kang2019decoupling}. 
However, it is non-trivial to apply a naive re-balancing strategy for scene graph generation.
%Previous works try either the instance-level replay strategy~\cite{hu_learning_2020} or images-level resampling method~LVIS\cite{gupta_lvis:_2019} for object detection, however, which is sub-optimal for the scene graph generation due to the entity-pair association.
We propose a \textit{bi-level data sampling strategy} by combing the \textit{image-level over-sampling}~\cite{gupta_lvis:_2019} and \textit{instance-level under-sampling}~\cite{hu_learning_2020}  to achieve a better trade-off between the head and tail categories.


%\rev{
%Previous works either utilize the re-balancing ideas (e.g. resampling \cite{chawla_smote_2002, drummond_why_2003, shen_relay_2016, mahajan_exploring_2018}, loss re-weighting \cite{cao_learning_2019, cui_class-balanced_2019, khan_cost_2017, tan_equalization_2020, lin_focal_2017, ssd_liu_2015, li2020overcoming, lu_gridrcnn_2018}) or explore the knowledge transfer for head categories, to alleviate biased prediction caused by long-tail distribution.
%% Along the direction of  data re-sampling, researchers have proposed to resample the dataset toleverage the balanced class prior~.  
%% These methods consists of: over-sampling minority categories~\cite{chawla_smote_2002} or under-sampling majority categories~\cite{drummond_why_2003}. 
%% Along the direction of data re-sampling, the key idea is over-sampling minority categories~\cite{chawla_smote_2002} or under-sampling majority categories~\cite{drummond_why_2003}.
%Recently \cite{hu_learning_2020,gupta_lvis:_2019} propose the instance-level re-sampling strategy for the instance related tasks (\textit{e.g.} object detection, instance segmentation).
%}


For the task of scene graph generation, several strategies have proposed to tackle the intrinsic long-tail problem. Some researchers propose novel loss designs by leveraging the semantic constraints of scene graph~\cite{knyazev_graph_2020, lin_gps-net_2020, yan_pcpl_2020, wang2020tackling}.  Others develop new graph structure encoding the context~\cite{tang_learning_2018, chen_knowledge-embedded_2019, lin_gps-net_2020} or introduce external commonsense and linguistic knowledge~\cite{zareian_bridging_2020, zareian_learning_2020, peyre2019detecting} for better representation learning. Recently, Tang \etal~\cite{tang_unbiased_2020} proposes an unbiased inference method by formulating the recognition process as a causal model. In this work, we aim to improve the context modeling for tail-categories by design an novel graph network and message propagation mechanism.


%% 几个 BMVC和 ACMMM的paper 
%The Tang~\cite{tang_unbiased_2020} proposes an unbiased inference method by formulating the recognition process as a causal model.
%%From the perspective of improving the context modeling,
%The Zareian~\cite{zareian_bridging_2020, zareian_learning_2020}, Peyre~\cite{peyre2019detecting} show the external commonsense and linguistic knowledge are also effective for discriminative representation learning. 
%The Tang~\cite{tang_learning_2018}, Chen~\cite{chen_knowledge-embedded_2019}, Lin~\cite{lin_gps-net_2020}, design the new graph structure encoding the context.
%In this work, we further improving the context modeling for tail-categories by design an novel graph network and message propagation mechanism.

%The Tang~\cite{tang_learning_2018}, Chen~\cite{chen_knowledge-embedded_2019}, and Lin~\cite{lin_gps-net_2020}, design the new graph structure encoding the context by associating with downstream task(VQA), or statistical priors. 
%Sharing a similar perspective of improving the context modeling, we further push direction forward by design an effective context modeling method with bipartite graph structure and
% The Chen, et al. \cite{chen_scene_2019} designed automatic methods to generate the annotation from the unlabeled data. 

% powerful semantic context.


%There also have many works that tackle the long-tail distribution in scene graph generation from different perspectives. 
%The Knyazev~\cite{knyazev_graph_2020} and Lin~\cite{lin_gps-net_2020}, Yan~\cite{yan_pcpl_2020} , Wang~\cite{wang2020tackling} propose different loss design.
%The Tang \cite{tang_unbiased_2020} proposed an unbiased inference method by formulating the recognition process as a causal model.
%From the perspective of improving the context modeling,
%The Zareian~\cite{zareian_bridging_2020, zareian_learning_2020} and Peyre~\cite{peyre2019detecting} shows the external commonsense/linguistic knowledge is powerful semantic context.
% The Tang~\cite{tang_learning_2018}, Chen~\cite{chen_knowledge-embedded_2019}, Lin~\cite{lin_gps-net_2020}, designed the new graph structure encoding the context by associating with downstream task(VQA), or statistical priors.
%Sharing a similar perspective of improving the context modeling, we further push direction forward by design an effective context modeling method with bipartite graph structure and
% The Chen, et al. \cite{chen_scene_2019} designed automatic methods to generate the annotation from the unlabeled data. 

% The Lin, et al.\cite{lin_gps-net_2020} use a softened relationship categories frequency to adjust the probability distribution of relationship prediction.
% The community has explored to design an more effective context modeling methods for recogning low frequency categories.
% The Chen, et al.\cite{chen_knowledge-embedded_2019} proposed graph neural network utilize the statistical correlations as aggregation weighting for encoding the categories level context for enhance the ability of recognizing the low frequency categories.
% Tang, et al.\cite{tang_learning_2018} proposed a tree structure graph for encoding the context between entities with jointly supervision of downstream VQA task.

\begin{figure*}[th]
	\centering
	\includegraphics[width=15cm]{imgs/main_figure.pdf}
	\caption{
		\textbf{Illustration of overall pipeline of our BGNN model.}
		%    The framework of our method. First, the proposal generation module produces the relationship proposals, which include their geometry information and visual representation.
		\textbf{RCE} denotes the relationship confidence estimation module. 
		\textbf{CMP} denotes the confidence-aware message propagation model.
		\textbf{SG Predictor} is the scene graph predictor for the final prediction.
		%    (RCE) estimation the confidence score of relationship proposals.
		%    The adaptive bipartite graph neural network(Bipartite GNN) takes the confidence score to conduct the adaptive message passing on relationship proposals.
		%    The refinement process will iterate multiple times, for each stage of graph refinement, the refined representations of proposals are fused with the initial representation by skip connection.
		%    Finally, the scene graph predictor classifies the predicates and entities and converts the bipartite graph to the scene graph. 
	}\label{pipeline_overview}
	\vspace{-0.4cm}
\end{figure*}

% 括号和citation和文字之间 加一个空格
\vspace{-4mm}
\paragraph{Graph Neural Network}
Our work is also related to learning deep networks on graph-structure data. The Graph Neural Network (GNN) is first proposed by~\cite{scarselli_graph_2009}, which is a powerful method for handling the non-Euclidean data. 
%The community has proposed many structures and algorithms based on the GNN. 
Previous works have explored different graph structures (e.g. directed graph~\cite{kampffmeyer_rethinking_2019}, heterogeneous graph~\cite{zhang_deep_2018, wang_heterogeneous_2019} ) and aggregation mechanism (e.g. convolutional ~\cite{kipf_semi-supervised_2017, hamilton_inductive_2018}, attention~\cite{li_gated_2017, wang_non-local_2017, zhang_latentgnn:_2019, vaswani_attention_2017}) for various tasks. Several recent efforts attempt to improve the quality of message propagation in GNNs.
%Improving the quality of message propagation by designing an information flow control-based mechanism is becoming popular.
Hou \etal~\cite{hou_measuring_2020} proposes a context-surrounding GNN framework to measure the quality of neighborhood aggregation.
Xu \etal~\cite{xu_dynamically_2020} introduces a dynamical subgraph construction by applying a graphical attention mechanism conditioned on input queries. 
%Wang~\cite{wang_camp_2019} proposed a matching-based method for adaptively control the information flow for message passing across modalities for text-image retrieval.
In this work, we introduce a novel bipartite graph network to learn a robust context-aware feature representation for the predicates.
% and a more balanced predicate classifier.


% Another interesting way is to directly intervene the prediction. The Tang, et al.\cite{tang_unbiased_2020} proposed a unbiased inference method by formulating the recognition process as a unbiased causality model.

% The Lin, et al.\cite{lin_gps-net_2020} use a softened relationship categories frequency to adjust the probability distribution of relationship prediction.
% The community has explored to design an more effective context modeling methods for recogning low frequency categories.
% The Chen, et al.\cite{chen_knowledge-embedded_2019} proposed graph neural network utilize the statistical correlations as aggregation weighting for encoding the categories level context for enhance the ability of recognizing the low frequency categories.
% Tang, et al.\cite{tang_learning_2018} proposed a tree structure graph for encoding the context between entities with jointly supervision of downstream VQA task.
% The Lin, et al.\cite{lin_gps-net_2020} proposed a novel direction awared message passing method for modeling context between the entities of scene graph.

% Sharing the similar perspective with Tang, et al.\cite{tang_learning_2018}, Chen, et al.\cite{chen_knowledge-embedded_2019} and Lin, et al.\cite{lin_gps-net_2020}, we proposed a effective bipartite graph neural to encode the context information, by aggregating the representation between the entities and predicates with the adaptive message passing mechanism, we can achieve better context model for produce better visual representation for low frequency relationships.
% Besides design for effective feature representation, the anther most direct and effective method to tackle with the longtail distribution is apply the resampling on training data.

% However, the most of resampling strategies \cite{chawla_smote_2002, drummond_why_2003, shen_relay_2016, mahajan_exploring_2018} are based on image levels repeating, which is not effective enough for balancing the data distribution of instances based task. 
% Inspired by the Instance-level Data Balanced Replay strategy from Hu, et al.\cite{hu_learning_2020}, we further the instance based resampling strategy for scene graph generation task, and efficiently rebalancing the data distribution for out model to achieve a better balanced performance.


