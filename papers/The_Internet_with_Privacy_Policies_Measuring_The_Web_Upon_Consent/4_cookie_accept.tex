\section{\TOOL design and testing}
\label{sec:metho}

We explicitly engineer \TOOL to fully automate the visit to websites and collect statistics. The key element of \TOOL is its ability to identify the presence of a Consent Banner and automatically accept privacy policies. We aim at a practical and effective approach to accept privacy policies through the offered button. %As previously said, most users will indeed be nudged in this direction, being the opt-out options often made cumbersome on purpose\cite{bauer2021you, hausner2021dark, CookieBenchmarkStudy}.

To illustrate \TOOL operation, consider again Figure~\ref{fig:cookie_accept_example}. A large Consent Banner appears on the first visit, and the user shall click on the ``Got it'' button to access the webpage content. \TOOL has to locate this button and click on it automatically. As a result, the website starts loading advertisements and contacting trackers in the background. We refer to these two types of visits as \BEFORE and \AFTER in the remainder of the paper.

We implement \TOOL using the Selenium browser automation tool~\cite{avasarala2014selenium}, the de-facto standard for browser automation, using Google Chrome as browser. Given a target URL, \TOOL carries out the following tasks:
\begin{enumerate}
    \item It navigates to the URL with a fresh browser profile, i.e., with an empty cache and cookie storage. This makes the visit the equivalent of a \BEFORE to the website.
    \item It inspects the Document Object Model (DOM) of the rendered webpage to find a possible \emph{Accept-button} in a Consent Banner. For this, it matches a list of keywords on the text of each node of the DOM. We identify an \emph{Accept-button} if we exactly match any of these keywords. For robustness, we remove leading/trailing/repeated blank characters and the match is performed ignoring the case. We do not use stemming, lemmatization or other techniques for text processing given the specificity of the words to match and the need to support multiple languages.
    \item If \TOOL finds the \emph{Accept-button}, it clicks on the corresponding DOM element (typically a \texttt{<button>}, \texttt{<href>} or \texttt{<span>} element) to accept the privacy policy and logs the success acceptance.
%    \item \TOOL then revisits the URL to collect statistics about the \AFTER experience.
\end{enumerate}

In the beginning, we built \TOOL to look for accept buttons through CSS selectors combined with keywords as done in~\cite{vallina2019tales} and popular add-ons. However, we soon observed that this methodology was too fragile as the use of selectors is strongly CMP-specific and highly customizable by webmasters. The keyword-based approach eases the generalization of the solution. Considering complexity, \TOOL adds marginal overhead to the time required to visit a webpage. Only for very complex webpages, iterating through all DOM elements may require some time, but this is still less than the time needed to load and render the webpage by the browser. 

During each visit, \TOOL stores metadata regarding the whole process in a JSON log file. It includes details on all HTTP transactions and installed cookies. Moreover, it optionally takes screenshots of the webpage during the various phases to allow manual verification.

\TOOL is highly customizable and offers the user various features. It lets the user customize the declared \texttt{User-Agent} and browser language (in the \texttt{Accept-Language} headers). Important to our analysis, it can be configured to run a:
\begin{itemize}
    \item \emph{Warm-up visit}: to populate the browser cache.
    \item \BEFORE: to collect statistics on the webpage before accepting the privacy policy, as a Naive Crawler would do.
    \item \AFTER: to collect statistics on the webpage as it appears after accepting the privacy policy (if an \emph{Accept-button} is found).
    \item \INTERNAL: to a number of webpages of the same website, randomly choosing among the internal links.\footnote{We define internal links as those having the same Fully Qualified Domain Name as the visited website.} We visit internal pages both if \TOOL finds the \emph{Accept-button} and if it does not.
\end{itemize}

For each page visit, \TOOL collect several metadata. Considering QoE metrics, here we focus on the Page Load Time, or \emph{OnLoad} time~\cite{da2018narrowing}. It allows us to compare the webpage rendering performance with and without privacy policy acceptance. It is simpler and faster to compute than the SpeedIndex~\cite{speedindex}, allowing large scale measurements. Notice that we neglect metrics that are not affected by the presence of a Consent Banner, such as the Time-to-first-byte (TTFB). 

Notice that the \AFTER visit can only occur with a warm browser cache in real cases since the browser would have first to complete the \BEFORE visit.
To fairly compare a \BEFORE and \AFTER, in our experiments we run a preliminary \emph{Warm-up visit} before the \BEFORE to fill the browser cache. This lets us appreciate the eventual extra time to load additional components and fairly compare the \emph{OnLoad} on the two visits with the hot cache. Alternatively, \TOOL can erase the HTTP cache and clean the socket pool upon each visit to compare webpage performance with a cold cache. 

\TOOL follows possible redirects during the visits and cases when a script triggers a reload of the webpage. This allows us to manage cases in which the consent banner is hosted on a separate specific landing page than the actual website home page. At last, to limit the impact of random delay due to webpage download and rendering, \TOOL uses quite conservative timeouts before eventually abort the visit. In detail, the DOM inspection starts 5 seconds after the \emph{OnLoad} event. While this clearly slows down the visit of multiple webpages, it maximizes the success rate.

To allow large-scale measurement campaigns, we containerize \TOOL using the Docker container engine~\cite{docker}. In the containerized version, we use Google Chrome version 89 in headless mode and force it to use a standard \texttt{User-Agent} instead of the pre-defined \texttt{ChromeHeadless}.\footnote{The containerized version is available on Docker Hub as \emph{martino90/priv-accept}.}

\subsection{Keyword Selection and Validation}
\label{sec:keywords}

At the core of \TOOL there is the list of keywords to be matched against the webpage content to localize the clickable DOM element for accepting the privacy policy. We thoroughly build this list manually in an iterative way. To handle different languages, we build a list that includes keywords for each country we are interested in. For this work, we focus on 5 European countries, namely France, Germany, Italy, Spain, UK\footnote{In January 2021 UK has enforced the UK GDPR, with practically identical requirements.}, plus the US -- which we use as an example of a large, extra-EU country were privacy laws are in force. For each country, we pick the most popular websites according to the Similarweb lists~\cite{similarweb}, a website-ranking service analogous to Alexa.

\subsubsection{First Round - keyword extraction from top websites}

In the first round, for each of the $5$ countries, we consider the top-200 websites that have a Consent Banner. We randomly choose half of these websites and manually visit them (from Europe) to extract the accept keyword. In total, we visit $500$ websites and identify $186$ unique keywords. We next instruct \TOOL to visit the other half of websites and let it accept the privacy policy, if found. For those where it fails ($233$ cases), we manually visit them to check i) if they have a Consent Banner, and ii) eventually to extract new keywords. With this, we identify $36$ new keywords, $222$ in total. During these steps, we also check that the tool correctly accepts the policy.

\subsubsection{Second Round - testing and keyword increase}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/cookieaccept_validation_third_round.pdf}
    \caption{Validation results of \TOOL over 200 randomly picked websites per country. Upon two rounds of keyword selection, \TOOL 92\%-95\% accurate. }
    \label{fig:validation}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/cookieaccept_keywords_freq_abs_alt.pdf}
    \caption{Frequency of the \TOOL keywords, with indication of the coverage at different points. The top-98 keywords already cover 95\% of websites.}
    \label{fig:keywords}
\end{figure}

To evaluate the accuracy of \TOOL in the wild, we next consider $200$ new random websites for each country from the Similarweb lists, $1000$ websites in total. We let \TOOL visit them and manually check the subset of $448$ websites for which \TOOL did not find (and accepted) a privacy policy. We depict the results in Figure~\ref{fig:validation}. \TOOL can accept the privacy policy in more than half of websites, independently from the language. In $6-14\%$ of cases, we find 36 new keywords -- that we promptly add to our list. Interestingly, we find a non-negligible portion of websites ($26-30\%$) that do not present any Consent Banner. At last, \TOOL fails to accept privacy in only $5-8\%$ of cases. Investigating further, this is due to some non-standard behavior of the webpage when accessed in headless mode. For instance, some websites present a CAPTCHA when they detect an automated visit; other websites return a blank webpage. This is a common problem for any crawler-based measurement study~\cite{vastel2020fp}. For completeness, cases of \emph{False Positives} -- i.e., \TOOL clicking on a wrong DOM element -- are possible, although we have not observed any in our manual validation tests. 

At the end of the keyword list building phases, we collect a total of $258 (186+36+36)$ keywords obtained by manually visiting $1181 (500+233+448)$ websites, covering 6 languages.\footnote{In Spain, some websites are in Catalan, rather than in Spanish.} In Figure~\ref{fig:keywords}, we show the distribution of keyword appearance frequency across the entire set of $12\,277$ Similarweb websites (see Section~\ref{sec:dataset} for details on this list). The most common keyword is the string ``Ok''. Red dots indicate the portion of websites covered by the top-$N$ keywords -- i.e., the coverage of the top-$N$ words. The top keywords are very common (note the logarithmic scale on the $y$-axis), with the top-$10$ that cover half of the websites. The top-$98$ keywords cover $95\%$ of the websites, while the remaining appear less than $10$ times each in the whole website set. Clearly, we expect the list of keywords to naturally grow as the tail of the Figure~\ref{fig:keywords} suggests. Notice indeed that more than 80 keywords have been found on a single website. Curiously, we find complex strings like ``I'm fine with this'' or ``Alle auswählen, weiterlesen und unsere arbeit unterstützen''.\footnote{Which translates to ``Select all, keep reading and support our work''.}



\subsection{\TOOL vs. Consent-O-Matic}
\label{sec:ca_vs_com}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/cookieaccept_consentomatic_random.pdf}
    \caption{Privacy policy acceptance rate of \TOOL and Consent-O-Matic on 100 websites per country. \TOOL can find and accept Consent Banners on twice as many websites as Consent-O-Matic.}
    \label{fig:ca_vs_com}
\end{figure}

We compare the effectiveness of \TOOL with Consent-O-Matic, the most mature browser plugin designed to offer/deny consent to privacy policies automatically. Unlike our tool, Consent-O-Matic exploits the presence of popular Consent Management Platforms (CMP), services that take care of the management of users' choices on behalf of the website. At the time of writing, Consent-O-Matic allows managing Consent Banners for 35 CMPs. To gauge its performance, we visit the top-100 most popular websites with a Consent Banner for the 5 countries using a Chrome browser with the Consent-O-Matic plugin enabled. Consent-O-Matic accepts the privacy policies in less than 35\% of websites with Consent Banner, and as little as 17\% and 20\% for websites in Italy and UK, respectively. Here \TOOL accepts the privacy policies on all websites by construction.

We then run a second experiment considering another set of 100 websites randomly picked from the Similarweb per country lists. We visit each website with \TOOL and a Consent-O-Matic-enabled browser. Figure~\ref{fig:ca_vs_com} summarizes the comparison. \TOOL accepts the privacy policies in more than 50\% of websites, more than twice the success rate of Consent-O-Matic. These results are in line with those of Figure~\ref{fig:validation}. The remaining websites may not have a Consent Banner, fail to load, or use an unknown keyword. This testifies that the customization of Consent Banners makes it difficult to engineer a generic and simple solution. The keyword-based strategy results more robust than the CMP-based approach (with similar complexity in curating the lists).



\subsection{Dataset and Tracker list}
\label{sec:dataset}

In the following, we use \TOOL to check the impact of using \TOOL when doing large web measurement experiments. We targets a large set of websites popular in France, Germany, Italy, Spain and US, using a test server located in our university campus. For each of the $6$ countries, we use the Similarweb lists to select the top-100 websites from 24 different categories -- see Figure~\ref{fig:ca_category}. These are the top-level unique categories listed in the Similarweb page~\cite{similarwebcategories}. In total, we include $12\,277$ unique websites to visit (as the lists in different countries partially overlap). When visiting websites of a given country, we set the \texttt{Accept-Language} header to indicate the appropriate locale and country language. This behavior can be configured in the \TOOL configuration to allow further experimentation.

We run \TOOL on a single high-end server running 16 parallel instances to speed up the crawl. We instrument it to run a \emph{test sequence}, which consists in a \emph{Warm-up visit}, \BEFORE and \AFTER to the landing page, followed by \INTERNAL to 5 randomly chosen internal pages -- previous studies indeed show that internal and landing pages have different properties~\cite{aqeel2020on}. For each website, we repeat the test sequence $5$ times, randomizing the order of websites to visit in each repetition.  Our main experimental campaign took place for two weeks on April 2021.

We run additional measurement campaigns to investigate specific aspects. To understand whether Consent Banners appear or have a different impact depending on the visitor location, we repeat the above experiments using servers located in the US, Brazil and Japan. We use Amazon Web Services to deploy on-demand servers on the desired availability zone. Here, we aim to check if websites behave differently based on the location of the visitors. Since we are using cloud servers, targeted websites may wrongly recognise the test machines as not regular users and located them in a generic or wrong country. While we cannot check this, we verified that the two most popular commercial IP location databases (IP2Location\footnote{\url{https://www.ip2location.com/}} and MaxMind\footnote{\url{https://www.maxmind.com/}}) map the IP addresses of our crawlers to the correct country. 

To offer a view on a larger number of websites, we visit the top-100\,000 websites according to the Tranco list~\cite{pochat2018tranco}. Unfortunately, the Tranco list does not offer a per-category and per-country rank. We run two separate test sequences: with warm caches, doing (i) \emph{Warm-up visit}, (ii) \BEFORE, and (iii) \AFTER. And with cold caches, (i) \BEFORE, (ii) erase HTTP cache and clean socket pool and (iii) \AFTER. Following this procedure, we ensure a fair comparison between \BEFORE and \AFTER in the two scenarios. Recall that \TOOL allows one to generate any combination of test sequence with warm/cold cache.

To observe how the presence of trackers changes, we rely on publicly-available lists provided by Whotracksme~\cite{whotracksme} (a tracking-related open-data provider), EasyPrivacy~\cite{easyprivacy} (one of the lists at the core of AdBlock tracker-blocking strategy) and AdGuard~\cite{adguard} (a popular ad-blocking tool). For robustness, we merge the three lists and consider as a potential tracker any third-party domain that appear in at least two lists. In total, we obtain $1\,497$ domains that we consider tracking services.\footnote{In the following, we identify them with their \emph{second-level domain name} -- i.e., a hostname truncated after the second label. We handle the case of two-label country code TLDs such as \texttt{co.uk}.} We finally record the presence of a tracker during a visit if the webpage embeds an object from a tracking domain, and the latter installs a cookie with a lifetime longer than one month~\cite{trevisan20194} -- commonly referred to as \textit{profiling cookie}. As such, we divide the HTTP transactions carried out during a visit in: 
\begin{itemize}
    \item First-Party: objects from the same domain of the target webpage.
    \item Third-Party: objects from a different domain than the target webpage.
    \item Trackers: objects from a Third-Party that is a tracking domain and sets a profiling cookie.
\end{itemize}