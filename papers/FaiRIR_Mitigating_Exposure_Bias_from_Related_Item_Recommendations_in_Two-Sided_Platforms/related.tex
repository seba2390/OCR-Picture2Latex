\vspace{-2mm}
\section{Background and Related Work}
We review prior works on related item recommendations, 
followed by %a discussion on 
the recent works on algorithmic fairness % in general, and on 
especially in the domain of recommendation systems.

\vspace{1mm}
%\subsection{Related item recommendations}
\noindent \textbf{Related item recommendations:} 
RIR systems recommend items %in the context of 
for a source item 
(typically, the item a user is consuming at a point in time) based on their relatedness to the source.
Evaluating relatedness between a pair of items is central to item-based collaborative filtering recommendations~\cite{desrosiers2011comprehensive,linden2003amazon}. 
Multiple prior works have proposed different approaches for identifying relatedness. 
For example, some approaches use user-rating matrices
~\cite{desrosiers2011comprehensive,sarwar2001item}, which may be factorized using matrix decomposition techniques like SVD~\cite{sarwar2000application}. 
Other approaches rely on implicit %activity 
information such as clicks or co-purchases %to determine relatedness~
\cite{barkan2016item2vec,hu2008collaborative}. 
%Typically, 

RIR algorithms are used in %different domains~\cite{amatriain2016past}, and in 
multiple two-sided platforms, with some platform-specific enhancements. %such as in 
For instance, Amazon uses RIRs in %the context of e-commerce
its product pages~\cite{linden2003amazon,smith2017two}, 
YouTube recommends videos through their `Up next' recommendations~\cite{covington2016deep}, 
Netflix uses RIR for recommending movies~\cite{gomez2016netflix}. %, and so on.
%\noindent 
In this work, we attempt to cover different types of relatedness %from one of 
considering each of the above categories, by examining %relatedness from 
user-item ratings (rating-SVD), and users' activity information (item2vec). %and an online recommendation platform (IMDb).

\vspace{2mm}
\noindent \textbf{Algorithmic bias and fairness:} 
Recently, there have been extensive research on algorithmic bias and fairness across multiple disciplines %on algorithmic bias and discrimination
~\cite{barocas2016big}. % and the references therein). 
To %do away with such
tackle any inadvertent consequences of algorithmic decisions,
many recent studies have considered fairness from mainly two perspectives: 
(1)~{\it individual fairness}, which requires similarly deserving candidates should be treated similarly~\cite{zemel2013learning,lahoti2019ifair}, and 
(2)~{\it group fairness}, requiring different social salient groups should % the disadvantaged group 
be treated similarly % to the advantaged group or the entire population
~\cite{pedreshi2008discrimination, pedreschi2009measuring}. 
While some studies focus on detecting
the discrimination (e.g.,~\cite{angwin2016machine,pedreshi2008discrimination}),
others suggest 
mitigation strategies by proposing fairness-aware
algorithms (e.g.,~\cite{lahoti2019ifair,zafar2017fairness,zehlike2017fa}).

Few recent works have considered group and individual fairness in {\it personalized recommendations}~\cite{yao2017beyond,edizel2019fairecsys,geyik2019fairness, patro2020incremental, patro2020fairrec} where the goal is to ensure 
that the recommendations do not discriminate against socially salient groups or individuals. 
To our knowledge, ours is the first attempt to consider {\it individual fairness in related item recommendations}, where we %systematically develop 
propose a novel algorithm {\it FaiRIR}, that attempts to provide the desired level of exposure to different items. 
%that various items should get during recommendations. 