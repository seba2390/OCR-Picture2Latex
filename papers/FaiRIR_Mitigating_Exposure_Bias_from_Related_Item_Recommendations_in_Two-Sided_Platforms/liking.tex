%\subsubsection{Liking}
\vspace{1mm}
\noindent 
\textbf{(1) Users' propensity to like the recommended items}:
When a source item $i$ recommends an item $j$,
intuitively, the utility of the recommendation is high if the recommended item $j$ is `liked' by most users who had also liked the source item $i$. 
To this end, we consider a user to have liked a given movie / item if (s)he gives a rating/score of more than 3.5 (out of 5) i.e., a score of more than 70\%. Our choice is influenced by studies claiming 3.5--4.5 (out of 5) being the sweet spot~\cite{WomplyHow, MetacriticHow} in five star rating systems. 
Note that, since all users who had liked $i$ need not consume or rate item $j$, we need to consider only those users who liked $i$ and rated the recommended item $j$. %Given the way standard recommendation datasets are, we almost always know which user rated which item and by how much score. 
For the item $i$, let $R_i$ and $L_i$ be the set of users who rated and liked the item respectively ($L_i$ is a subset of $R_i$). 
For a source item $i$ and a recommended item $j$ (from $i$), we evaluate the \textbf{like overlap} as $\frac{|L_i \cap L_j|}{|L_i \cap R_j|}$, i.e., out of all users who liked item $i$ and rated item $j$, what fraction of users actually liked (rated highly) $j$.
Specifically, for each pair of items $(i,j)$ where $j$ has been recommended for $i$, we compute the like overlap between $i$ and $j$, and then take the mean across all such pairs.% \todo{I have changed the expression. check the expression and explanation for like overlap -- SG}\todo{Is there any reference which indicates that a rating above 3.5 is an indication of `liking' of the user? I mean why not 4 or 4.5? --AM}

\vspace{1 mm}
\noindent \textbf{Results:}  Table~\ref{Tab:relatedness-of-recs} (`Like overlap' column) and Table~\ref{Tab:UserSatisfaction-of-recs-Amazon} (all columns, for different $\beta$ values) show the mean {\it like overlap} for the various RIR algorithms, for the MovieLens and Amazon datasets respectively. 
For the MovieLens data (Table~\ref{Tab:relatedness-of-recs} last column), while using rating-SVD, the average {\it like overlap} reduces for FaiRIR$_{rl}$ (as compared to the original algorithm); however, there is practically no reduction in like overlap for FaiRIR$_{sim}$ and FaiRIR$_{nbr}$.  
Interestingly, while using item2vec, {\it FaiRIR$_{sim}$ and FaiRIR$_{nbr}$} ({\it like overlap = 0.59}) {\it have outperformed even the original algorithm} ({\it like overlap = 0.56}). 
For Amazon dataset (Table~\ref{Tab:UserSatisfaction-of-recs-Amazon}), for both rating-SVD and item2vec algorithms, all the FaiRIR algorithms have performed comparably or even better than the original algorithms. We repeated this experiment with 4 and 4.5 thresholds for {\it like overlap} and found qualitatively similar observations (omitted for brevity). These results show the efficacy of the proposed approaches to preserve utility of the recommendation to end users. However, one can argue that such high scores might be an artifact of the underlying desiredness assumption while designing the algorithm esp. when $\beta = 0$ i.e., a completely meritocratic distribution. Hence, analysis on different $\beta$ values are of utmost importance.% for deriving some insights.

%\vspace{1mm}
\noindent
\textbf{Analysis on multiple desired exposure distribution}: Table~\ref{Tab:UserSatisfaction-of-recs-Amazon} shows the {\it like overlap} for various desired exposure distributions (i.e., for various values of $\beta$) for the Amazon dataset. 
%The interpretation of the results is the same as discussed above -- 
We see that the performance of all the FaiRIR interventions in preserving utility toward users is very stable and is agnostic of the underlying desired distribution (similar results for MovieLens dataset are omitted for brevity).