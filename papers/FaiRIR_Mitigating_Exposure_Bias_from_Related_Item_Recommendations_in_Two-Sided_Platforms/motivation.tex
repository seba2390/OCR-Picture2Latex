
\section{Limitations of existing RIRs}
\label{sec: motivation}


Figure~\ref{fig:recopipeline} shows a schematic block diagram of the methods involved in building a model of {\it item-item relatedness}, (also called an \textit{item model}), which is then utilized
to make the recommendations at scale~\cite{smith2017two,nikolakopoulos2019recwalk}.
Item models 
capture the similarity %and/or proximity 
between items based on either user-item interaction logs and/or item attributes~\cite{sarwar2001item, ning2011slim, desrosiers2011comprehensive}. From these logs, %(user-item) logs, 
first a latent space representation of each item is obtained. 
Then, the similarity between pairs of items are computed from the latent representations, 
and $k$ most similar (related) items to a given item are generated. In case of RIRs, this item model is used to recommend items that are `related' or `similar' to an item.  
Next, we explore two popular RIR algorithms, %and a real-world online recommendation system (IMDb), 
and check whether their recommendations %adhere to meritocratic fairness.
induce disproportionate exposure to items.%exposure bias.


\vspace{-2 mm}
\subsection{Two popular RIR algorithms}\label{sec: RIRAlgos}
As a proof of concept, we consider the following two popular algorithms for generating related item recommendations -- (i)~rating-SVD, and (ii)~item2vec (detailed below). 
We choose these two algorithms because they cover some of the most common techniques for RIRs~\cite{yao2018judging}.
These algorithms take as input %the user-item interaction logs, in the form of 
a {\it user-item rating matrix} $M$ whose $(i,j)$-th entry gives the rating that the user $i$ gave to the item $j$. 
%These algorithms produce the related item recommendations based on some underlying notion of relatedness. 
First, these algorithms learn a latent space representation of different items (from $M$). 
To generate recommendations for a seed item $i$, the algorithms then generate top-$k$ neighbours of $i$, by ranking items based on their similarity with $i$ in the latent space. 


\vspace{1 mm}
$\bullet$ \textbf{rating-SVD} applies Singular Value Decomposition (SVD)~\cite{sarwar2000application} to the user-item rating matrix $M$, and uses cosine-similarity for similarity evaluation%scores between item pairs %(that are represented as vectors in the latent space)
~\cite{sarwar2001item}. %We choose this strategy due to the popularity of item-based collaborative filtering algorithms in the recommendation literature. 
The underlying notion of relatedness between two items $i$ and $j$ can be described as `people who liked item $i$ are likely to like item $j$'. 
Following the setup in~\cite{yao2018judging}, we implement SVD with 128 dimensions. As a pre-processing step, we perform mean subtraction on the input and normalise each row of the final representations to a unit vector.

\vspace{1 mm}
$\bullet$ \textbf{item2vec}~\cite{barkan2016item2vec} is a replication of word2vec~\cite{mikolov2013distributed} representation learning%, a neural network-based algorithm that achieves state-of-the-art results in linguistic tasks
. item2vec substitutes items for words, and tries to find out co-occurrence patterns in user consumption. 
The underlying notion of relatedness among items can be defined as `people who consumed item $i$ are likely to consume item $j$ in close temporal proximity'. 
Following the setup in~\cite{yao2018judging}, we train the algorithm for 100 epochs by setting negative sampling to 15 and dimension of the output vector to 128. 

%\vspace{-2 mm}
%\noindent
\subsection{Datasets for experiments} 
We performed our experiments on %MovieLens 1M and 
MovieLens datasets (1M and 10M)~\cite{harper2016movielens} and Amazon review dataset~\cite{he2016ups}, that are well-known benchmarks for recommendation tasks. 

\vspace{1 mm}
\noindent
\textbf{MovieLens datasets}: These datasets provide ratings and browsing logs of users;
the ratings come from real MovieLens users. 
The ratings range from $[0.5, 5]$ in half-star increments. 
Experiments on both the MovieLens 1M and 10M datasets yield qualitatively similar insights. %and results
%. Hence, for brevity, w
Hence, we report results only on the MovieLens 10M dataset which contains $10,000,054$ ratings from $71,567 $ different users about $10,677$ distinct movies. 

\vspace{1 mm}
\noindent
\textbf{Amazon review dataset}: The Amazon product review dataset released by He \textit{et al.} \cite{he2016ups} comprises customer reviews and ratings for different Amazon products. For the purpose of this paper, we used the 5-core cellphone and accessories dataset. This dataset contains 194,439 reviews of 27,879 users for 10,429 different products, where each user and item has at least 5 reviews.

\vspace{1 mm}
\noindent
We applied the rating-SVD and item2vec algorithms on %the MovieLens and the Amazon review 
these datasets to find the top-$k$ RIRs for each item, and then created the RIN (see Figure~\ref{Fig:RINCreation}) 
to measure the observed exposures for each algorithm. 
%The observed exposures of the items, according to a certain RIR algorithm, are computed based on this RIN, as described in Section~\ref{sec: EstOexp}.


\begin{figure}[tb]
	\centering
	\begin{subfigure}{0.43\columnwidth}
		\centering
		\includegraphics[width=\textwidth, height=3.0cm]{figures/Lorenz_Amazon.pdf}
		\caption{Amazon product review}
		\label{fig: AmazonExp}
	\end{subfigure}%
	\begin{subfigure}{0.43\columnwidth}
		\centering
		\includegraphics[width=\textwidth, height=3.0cm]{figures/Lorenz_ML.pdf}
		\caption{MovieLens }%on MovieLens}
		\label{fig: MLExp}
	\end{subfigure}
	
	\vspace*{-2mm}
	\caption{{\bf Lorenz plot showing the skew in observed exposure of items due to two state-of-the-art RIR algorithms over two real world recommendation datasets. 
	}}
	\label{fig: Lorenz}
	\vspace*{-5mm}
\end{figure}


\subsection{Skew in exposure of items due to RIRs}
\label{sub:existing-rir-bias}
We applied rating-SVD and item2vec algorithms on MovieLens and Amazon product review datasets (described above) to find the top-$k$ recommendations for each item. We experimented with different $k$ values, and the results were qualitatively similar in all cases. Hence, we report the results for the top-$10$ related item recommendations.

The existing RIR algorithms perform very well as regards to finding related items for items. However, the exposure that different items get is found to be heavily skewed. Figure~\ref{fig: Lorenz} shows the cumulative proportion of exposure distribution on the $y$-axis, and $x-$axis shows the cumulative proportion of items in the increasing order of their exposure (from left to right). The horizontal line (in black) corresponds to 25\% of the entire exposure and the corresponding vertical line denotes the percentage of items accounting for it.
Figure~\ref{fig: Lorenz}(a) shows that the top $25\%$ of the items with most exposure in the Amazon cellphone and accessories dataset account for $75\%$ of the entire exposure. Thus, a small fraction of the item-set gets very high fraction of the entire exposure; put differently, there is very low item-space coverage.

\vspace{1 mm}
\noindent
\textbf{Are the top items deserving of the exposure?} Based on the observation of this skewed exposure distribution, a plausible question can be raised: are those $25 \%$ items of very high quality? If so, one may argue that such items, having better quality than others, deserve more exposure than others, i.e., gap in quality can explain this skew in the exposure distribution. To further dwell on this particular line of argument, we investigated both the quality distribution and exposure distribution together. 


There can be many measures for quality,
based on domain experts' opinion (e.g., critical reviews of movies), public opinion (e.g., ratings given by consumers on e-commerce sites), awards won by movies, and so on. Specifically, in this work, we assume the {\it average user-rating} of an item as the quantification of its quality. We further normalize the quality distribution so that the quality of all items add up to $1$. Note that we understand the limitations of using average user ratings as a quality measure; however, we believe that such ratings in most cases are a reflection of an item's perceived quality for a given user.

\noindent
\textbf{Observations}: A striking observation we made is the large gap between the quality and exposure of different items. 
Only 6\% -- 7\% of the total number of items in the Amazon dataset have comparable quality and exposure, with a stark disparity between quality and exposure for a large majority of items. 
\begin{figure}[tb]
	\centering
	\begin{subfigure}{0.45\columnwidth}
		\centering
		\includegraphics[width=\textwidth, height=3.0cm]{figures/VanillaSVDAmazon.pdf}
		\caption{Amazon product review}
		\label{fig: IMDbExp}
	\end{subfigure}%
	\begin{subfigure}{0.45\columnwidth}
		\centering
		\includegraphics[width=\textwidth, height=3.0cm]{figures/VanillaSVD.pdf}
		\caption{MovieLens }%on MovieLens}
		\label{fig: SVDExp}
	\end{subfigure}
	
	\vspace*{-2mm}
	\caption{{\bf (color online) Scatter plot of log of observed exposure and average user rating of different items in Amazon product review, and MovieLens dataset (using rating-SVD). The red curve shows the log plot of the quality distribution.}}
	\label{fig: Exp}
	\vspace*{-5 mm}
\end{figure}
Figure~\ref{fig: Exp} shows scatter plots for ranking-SVD RIRs on Amazon review (Figure~\ref{fig: Exp}(a)) and MovieLens (Figure~\ref{fig: Exp}(b)) datasets. 
In these figures, the $x$-axis is the average user rating (quality) of different items and $y$-axis is the log of their observed exposures. The red curve in each figure is the log plot of the quality distribution, while the green dots show the observed exposure of the corresponding items. Note that many items having low user-ratings (in the range $[1, 2]$) enjoy significant observed exposure, while many high quality items are deprived of the same. Similar trends are found for item2vec on both the datasets (results omitted for brevity).

\begin{table}[tb]
	\noindent
	\scriptsize
	\centering
	\begin{tabular}{ |p{1.3cm}|p{2.5cm}|p{4.0cm}| }
		\hline
		{\bf RIN} & {\bf High-quality items} & {\bf Low-quality recommended items} \\
		\hline
		%\multicolumn{2}{|c|}{{\bf MovieLens}} \\
		%\hline
		%The Insider (7.8) & Slaughterhouse Rock (3.9) \\
		%\hline
		\multirow{2}{*}{Amazon } & Plantronics Windsmart Headset (4.3) & Plantronics Headset Charging Cradle for Voyager (2.2) \\
		\cline{2-3}
		& Motorola H385 Bluetooth Headset (5.0) & Motorola Hands-Free Headset (2.9) \\
		\hline
		\multirow{2}{*}{MovieLens} & Rear Window (8.5) & Meatballs part II (3.7)\\
		\cline{2-3}
		& 12 Angry Men (8.9) & Stop! Or My Mom Will Shoot (4.2) \\
		\hline
	\end{tabular}	
	\caption{{\bf Examples of low-quality items that get over-exposed being recommended by high-quality items. Values within () are average user-ratings.}}
	\label{Tab: examplesRIN}
	\vspace{-3 mm}
\end{table}


We observe that RIR links often lead to poor-quality movies from high-quality ones,
resulting in higher exposure of unworthy items, potentially at the cost of other more worthy items. The reason behind such phenomenon can be formation of recommendation links (due to the underlying relatedness algorithm). Note that if a poor-quality item gets recommended on the web-page of a popular high-quality item, it may be bestowed some exposure simply due to the merit of the high quality item from which it is getting recommended. For instance, many customers will organically end up at the item page of the high quality item and in turn, they well get recommended to the said low quality items increasing their accessibility and visibility. This phenomenon may get reinforced over time as many customers would end up exploring both the items in the same session of exploration~\cite{dash2021umpire, dash2019network}.

Table~\ref{Tab: examplesRIN} shows a few illustrative examples of the above phenomenon.
%in both the related item recommendation algorithms as well as IMDb.
For example, a high-quality item `Motorola H385 Bluetooth Headset' (avg. user rating 4.9) recommends the relatively low-quality item `Motorola Hands-free Headset' (avg. user rating 2.9) in both rating-SVD and item2vec recommendations (note that there is a quality drop of almost $40\%$). 
Similarly, for the MovieLens dataset, several high-quality movies having user-ratings higher than $8.0$ recommend movies of much lower user ratings (according to ranking-SVD). Note that, the average user ratings for movies are obtained from their corresponding IMDb pages.



Table~\ref{Tab: examplesDistortion} shows a few illustrative examples of the amount of distortion in exposure of some items introduced by RIR algorithms. %As can be seen, 
Many high quality items, e.g., `Samsung Galaxy S5 SM-G900H', etc. have been severely deprived of exposure at the cost of significant exposure of items like `Google Nexus Wireless Charger', etc.


Given the huge inventory size of online platforms, RIR systems are one of the primary tools through which users explore the universe of items. Thus, these systems play a significant role in deciding %to a large extent 
how much 
{\it exposure} (or visibility) different items receive. Hence, an unjust exposure distribution can have detrimental repercussions for the producers of these items and their livelihood. Being keyed to relatedness, these algorithms inadvertently overlook these important aspects of different stakeholders in the business cycle. In section~\ref{sec: mitigation} we suggest three different intervention mechanisms (\textbf{FaiRIR}) to appropriately adjust the skew in the exposure of items.


\begin{table}[tb]
	\noindent
	\scriptsize
	\centering
	\begin{tabular}{ |p {4 cm}|p {4 cm}| }
		\hline
		\bf High ratings, deprived of exposure & \bf Low ratings, with high exposure \\
		\hline
		\multicolumn{2}{|c|}{\textbf{Amazon Cellphones and Accessories}}\\
		\hline
		Samsung 3.5mm Stereo Headset for Galaxy S4 (4.2, $\frac{1}{10}$ times)& Google Nexus Wireless Charger from Google (1.2, 10 times)\\
		\hline
		%Breaking Bad (9.5, $\frac{1}{58}$ times)& The Adventure Club (5, 60 times) \\
		%\hline
		TechPro Galaxy S4 ShatterProof Premium Tempered Glass Screen Protector (4.83, $\frac{1}{8}$ times) & Sony Ericsson Liveview watch micro display for Android Devices (1.4, 9 times)\\
		\hline
		Samsung Galaxy S5 SM-G900H (4.8, $\frac{1}{7} $ times) & HTC One Screen Protector, Spigen Steinheil GLAS.t SLIM Premium Tempered Glass (1.35, 9 times) \\
		\hline
		\multicolumn{2}{|c|}{\textbf{MovieLens}}\\
		\hline
		Bittersweet Motel (8.0, $\frac{1}{11} $ times) & Diebinnen (5.5, $17$ times)\\
		\hline
		Men With Guns (7.6, $\frac{1}{10} $ times) & Slaughterhouse Rock (3.9, $10$ times)\\
		\hline
		%Galaxy Quest (7.3, $\frac{1}{12} $ times) & Das Superweib (4.1, $20$ times)\\
		%\hline
		Braveheart (8.3, $\frac{1}{12} $ times) & 
		Get Over it (5.7, $9$ times)\\
		\hline		
	\end{tabular}	
	\caption{{\bf High-quality (low-quality) items that are differently exposed. Values within () denote average user-ratings and the ratio of observed exposure to normalized quality.}}
	\label{Tab: examplesDistortion}
	\vspace{-8 mm}
\end{table}