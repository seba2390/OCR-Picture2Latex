\section{Experimental Evaluation}

\subsection{Simulation Tool}
We use a simulation tool written in Python to replay and simulate
historical data. From historical data we can observe the coverage
contributions of every cell to every pixel. Then we can use this
information to reproduce the effects of turning \textsf{ON} or
\textsf{OFF} a given cell. In this way we can compute the values
required by Definition~\ref{def:optimal}. The traffic demand is based
on historical data with the additional assumption that the demand is
uniformly distributed over all pixels affected by
cell. Figure~\ref{fig:simulatorclasses} shows the overall architecture
of the network simulator.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.7]{class-diagram.png}
  \caption{Simulator architecture}
  \label{fig:simulatorclasses}
\end{figure}


\subsection{\stratego Controller}
\label{def:strategocontroller}

The main contribution of our work is to synthesize a near-optimal
strategy in accordance to Definition~\ref{def:optimal}. Toward this
goal we use the tool \stratego{}~\cite{uppaal}. The tool developed at
Aalborg University and used to facilitate generation and optimization
of strategies for abstract games with stochastic and real-time
aspects. The tool uses simulation-based statistical machine learning
methods.


Figure~\ref{fig:strategocontroller} shows the \stratego model for a
stochastic hybrid game $\G_{\nrCells}$. Solid arrows correspond to
controllable actions where as dashed arrows correspond to environment
actions. At every simulation step and for every cell the controller
has the choice to set \textsf{ON} or \textsf{OFF} the given cell,
indicated by the command \uppUpdate{actions[cellId]=1} or
\uppUpdate{actions[cellId]=0}. Once actions on cells have been chosen
the environment executes its actions, this is done by calling a
external C library with the command
\uppUpdate{do\_sim\_step(stepSize,eps)}.  This function return a real
value which is then accumulated in the variable \uppVar{reward}. These
steps are then executed until the short time horizon $h$ has been
reached. \stratego will perform a number of simulations and used
machine learning techniques to find the controllable actions which
optimize the variable \uppVar{reward}. Once the learning is complete
\stratego returns the near-optimal strategy which is then implemented
in the simulator (or the real world).


\begin{figure}[t]
  \centering
  \includegraphics[scale=0.7]{strategoController}
  \caption{\stratego Controller}
  \label{fig:strategocontroller}
\end{figure}




\subsection{Simulation Scenarios and Controllers}

As a proof of concept we have chosen to perform a simulation of 1 day
in the following two geographical locations in Aalborg, Denmark:

\begin{itemize}
\item City Syd with 39 cells and 2687 pixels   
\item Frydendal - Nørre Tranders  with 107 cells and 6138 pixels
\end{itemize}  

In our experiments we have used the following controllers:

%\subsubsection{Controllers}
\begin{itemize}
\item \emph{ALLON} all cells always ON
\item \stratego as described in Definition~\ref{def:strategocontroller}
\end{itemize}

Table~\ref{tab:results} shows the results of the different controllers
in the different scenarios. The columns energy, penalty and reward
correspond to Definition~\ref{def:optimal}. The values on column
energy are computing using a linear function on historical data and a
constant for a cost per megabit. 

We observe that while having no penalty, the \stratego controller is
able to save about 10\% energy on a single day. Concerning controller
\stratego, the computation time for strategies about 22 and 31 hours
for City Syd and Frydendal - Nørre Tranders on 16 cores. This means
that given sufficient hardware resources, the scenarios could be
controlled in real time. This is because using Online Strategy
Synthesis (c.f.\ Section~\ref{onlineStrategySynthesis}) will give a
time window of up to 60 minutes to compute the next near optimal
strategy.

\begin{table}[t]
  \centering
  \begin{tabular}{|l||c|c|c||c|c|c|}
    \hline
    \multirow{2}{*}{Scenario}  &  \multicolumn{3}{c||}{\emph{ALLON}}
    &  \multicolumn{3}{c|}{\stratego\footnote{}}
    \\\cline{2-7}
                               & Energy & Penalty & Reward M. & Energy & Penalty & Reward M.
    \\ \hline
    City Syd
                               & 3473&0&150  & 3191&0&141 
    \\ \hline\hline
    \parbox{3cm}{Frydendal \\ Nørre Tranders}
                               & 10115 & 0 & 436   & 9347&0&394

    \\ \hline
  \end{tabular}    
  
  \caption{Experimental results}
  \label{tab:results}
\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
