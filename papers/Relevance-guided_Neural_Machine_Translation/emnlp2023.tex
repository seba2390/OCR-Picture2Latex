% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{lipsum,graphicx,subcaption}
%\captionsetup[subfigure]{labelformat=simple,labelsep=colon}
%\renewcommand{\thesubfigure}{fig\arabic{subfigure}}
%\newcolumntype{D}{>{\centering\arraybackslash}m{0.08\linewidth}}
%\usepackage{subfig}
%\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{mathtools}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{comment}
%\usepackage{graphicx}
\usepackage{babel,blindtext}%,multicol}
%\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{rotating}
\newcommand{\isidora}[1]{{\color{red}{\small\bf\sf [Isidora: #1]}}}
\DeclareMathOperator{\Tr}{Tr}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Relevance-guided Neural Machine Translation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Isidora Chara Tourni \\
  Boston University\\
  \texttt{isidora@bu.edu} \\
  \And
  Derry Wijaya \\
  Boston University \\
  \texttt{wijaya@bu.edu}}

\begin{document}
\maketitle
%\blindtext
\begin{abstract}
With the advent of the Transformer architecture, Neural Machine Translation (NMT) results have shown great improvement lately. 
However, results in low-resource conditions still lag behind in both bilingual and multilingual setups, due to the limited amount of available monolingual and/or parallel data; hence, the need for methods addressing data scarcity in an efficient, and explainable way, is eminent. We propose an explainability-based training approach for NMT, applied in Unsupervised and Supervised model training, for translation of three languages of varying resources, French, Gujarati, Kazakh, to and from English. Our results show our method can be promising, particularly when training in low-resource conditions, outperforming simple training baselines;  though the improvement is marginal, it sets the ground for further exploration of the approach and the parameters, and its extension to other languages. 
\end{abstract}

\section{Introduction}
%contribution, problem, evaluation
%task, sota, flaw in sota, our idea, proof it works
Unsupervised Neural Machine Translation (UNMT) has seen remarkable progress in recent years, with a very large number of methods proposed aiming to NMT when parallel data are few or non-existent for certain language pairs \citep{artetxe2017unsupervised, lample2018phrase, conneau2017word, wang2021advances,lample2019cross,song2019mass,liu2020multilingual,
marchisio2020does,kim2020and,lample2017unsupervised,artetxe2019effective,garcia2020multilingual,su2019unsupervised, nguyen2022refining}.
Training techniques such as Back-Translation \citep{sennrich2015improving} and Auto-Encoding have been widely studied, in order to efficiently train NMT models under those data scarcity conditions to obtain high quality translation results. However, there is little work in enhancing Neural Machine Translation (NMT) models with utilizing explainability of the model in order to improve quality of the output. We propose a method, based on Layer-wise Relevance Propagation (LRP), which leverages the contribution of the input tokens to the output, to boost NMT performance. Our results show LRP may be beneficial during model training for NMT output improvement, particularly in low-resource conditions and for specific well defined model setups.

%in an effort to create efficient and trustworthy Neural Machine Translation models of excellent performance which do not rely on the existence of parallel data. 

%Unsupervised Neural Machine Translation aims to make NMT work in the absence of parallel data. Most common approaches have focused on cross- or multilingual initialization of a language model either through an alignment of monolingual embeddings, \citep{artetxe2017unsupervised, lample2017unsupervised,lample2018phrase,conneau2017word} or by model pretraining and fine-tuning
%\citep{lample2019cross,song2019mass,liu2020multilingual}. Back Translation \citep{sennrich2015improving} is a model training technique used in UNMT which translates monolingual data between languages, with the goal of creating pseudo-parallel corpora to train the NMT model with \citep{artetxe2017unsupervised,lample2017unsupervised}.
%\citet{marchisio2022systematic} are the first to systematically examine the naturalness and diversity of the UNTM output, comparing it to similar quality human translations, and proposing a way to leverage UNMT to improve a classical supervised NMT system.

%We examine the application of an LRP-based interpretability approach during training to improve NMT performance, exploring how this affects previously examined metrics, and if there a setup where this way of training could be useful.

%However, there has been little effort on analyzing, apart from the quality of the output, the model behavior during UNMT, and on comparing Supervised and Unsupervised NMT results regarding the models' inner workings and the effects of various setups on hypotheses and generated translations' quality, monotonicity and semantic similarity, as well as model robustness and consistency.

\begin{comment}
We are conducting an analysis and comparison of Unsupervised, fully Supervised and joint Unsupervised and Supervised Neural Machine Translation approaches for two very diverse languages, French and Gujarati, translating to and from English. Extending previous work \citep{voita2020analyzing,voita2021language}, we research into the existence of different stages in Neural Machine Translation, analyze source and target sentence tokens' contributions in each training stage to the NMT result by calculating their relevance (using Layerwise Relevance Propagation - LRP by \citet{bach2015pixel}) to the output, evaluate the quality and word alignment of the generated translations, and Robustness and Consistency of our model to perturbed inputs, and 
\end{comment}

%Our paper follows up closely on the work of \citet{voita2020analyzing,voita2021language,marchisio2022systematic}, 
%Our paper and in summary examines if we use Layerwise Relevance Propagation (LRP) during training, and how does this affect the previously examined metrics? Is there a setup where this way of training could be useful?

\begin{comment}
\begin{itemize}
\item Do the distinct stages of transformer-based NMT analyzed in previous works exist in Unsupervised, Supervised, and joint Supervised \& Unsupervised NMT? %Analysis based on LRP calculation. 
\item How do output quality metrics, such as BLEU, word alignment metrics, such as Translation Edit Rate (TER) \citep{snover2006study} and Fuzzy Reordering Score (FRS) \citep{talbot2011lightweight}, semantic similarity metrics, measured by Ratio Margin-based Similarity Score (RMSS) \citep{artetxe2018margin} and Frechet Bert Distance (FBD) \citep{xiang2021assessing}, as well as source and target sentences' token contributions to the NMT output behave across the aforementioned stages?
%\item Are certain metricsâ€™ values better in intermediate stages/model checkpoints? Can I use those checkpoints for data/model selection, avoiding training the model to convergence? 
\item How Robust and Consistent are the NMT models throughout training?
\item Can we use Layerwise Relevance Propagation (LRP) during training, and how does this affect the previously examined metrics? Is there a setup where this way of training could be useful?
\end{itemize}
\end{comment}

\begin{comment}
Our focus is not on outperforming NMT state-of-the-art results, but rather on training bilingual Unsupervised and Supervised Neural Machine Translation models and examining their behavior as described above, from and to English, comparing our results for a high- and a low-resource language. Our findings confirm the existence of NMT stages regardless of the level of training supervision, and show that using Back-Translation in Unsupervised/joint Supervised and Unsupervised training setups leads to translations more similar to source sentences in terms of word order, regardless of the language, yet more semantically distant. We see that models trained in Unsupervised or joint Supervised and Unsupervised setups tend to show higher robustness and consistency, and can more easily recover from sentence perturbations, compared to models trained with parallel data only. We also observe that in reduced/low training data experiments, there is a heavy reliance on the source sentence for generating translations. 
\end{comment}


\section{Related Work}
%\subsection*{Unsupervised Neural Machine Translation (UNMT)}
%Unsupervised Neural Machine Translation aims to make NMT work in the absence of parallel data. Most common approaches have focused on cross- or multilingual initialization of a language model either through an alignment of monolingual embeddings, \citep{artetxe2017unsupervised, lample2017unsupervised,lample2018phrase,conneau2017word} or by model pretraining and fine-tuning
%\citep{lample2019cross,song2019mass,liu2020multilingual}. Back Translation \citep{sennrich2015improving} is a model training technique used in UNMT which translates monolingual data between languages, with the goal of creating pseudo-parallel corpora to train the NMT model with \citep{artetxe2017unsupervised,lample2017unsupervised}.
%\citet{marchisio2022systematic} are the first to systematically examine the naturalness and diversity of the UNTM output, comparing it to similar quality human translations, and proposing a way to leverage UNMT to improve a classical supervised NMT system.


\begin{comment}
In more recent works,
\citet{liu2022flow} introduce a flow-adapter architecture for UNMT, to separately model the distributions of source and target languages, and convert one to the other through a simple transformation, allowing for better language-specific representations and improved results in various benchmarks.
\citet{he2022bridging} identify a style and content gap between back-translated data and natural source sentences used in training and inference in an UNMT model, and attempt to mitigate this during training. %by introducing a self-training approach resembling inference.
\citet{garcia2020multilingual} take a step into expanding the paradigm to multilingual UNMT, and propose a probabilistic framework that incorporates and combines existing supervised and zero-shot approaches for NMT, as well as addresses scenaria either when the existing parallel data contains only one of the languages, or when there exists no parallel data at all.
\citet{garcia2020harnessing} use synthetic data created with offline BT to improve multilingual unsupervised MT in the En-xx direction for low resource languages xx, for which NMT usually performs significantly better than in the xx-En direction. %They split training into 3 stages, pre-training (using MASS), synthetic data creation through BT, and BT and cross-translation.
Lastly, \citet{nguyen2022refining} attempt to alleviate the curse of multilinguality in low-resource UNMT, introducing a method that disentangles languages in a multilingual UNMT model, allowing it to focus only on the target NMT task.
\end{comment}

%\citet{wang2021advances}
\subsection*{Layer-wise Relevance Propagation (LRP)}
LRP %measures relevance of the input components, or the neurons of a network, to the next layers' output 
was introduced by \citet{bach2015pixel}, measuring the contribution of the input components, or the neurons of a network, to the next layer's output. Due to its nature, it is directly applicable to layer-wise architectures, and we extend its usage to the Transformer architecture, measuring the relevance of source and target sentences' tokens to the NMT output during training.
%\isidora{TODO add further LRP definition and analysis}
%\citet{wu2021explaining} use LRP as an attribution method, among others, for sequence classification tasks with BERT \citep{devlin2018bert}, examining the metric's validity and robustness in producing explanations for sentiment analysis. We extend the usage of this metric to the Transformer architecture, and measure the relevance of source and target sentences to the NMT output, during evaluation and training.


\begin{comment}
\subsection*{Neural Machine Translation analysis}
\citet{voita2020analyzing} examine the source and target sentences' tokens' relative contributions to text generation during NMT, adapting the Layerwise Relevance Propagation (LRP) score to a Transformer model, and experimenting with different training objectives, training data amounts and types of target sentence prefixes, and their effect on NMT output quality and monotonicity. Following up to that work,
\citet{voita2021language} conduct a thorough analysis into the different stages of NMT, drawing parallels to the distinct stages identified in standard SMT. Their findings include decomposing NMT into three phases, targetside language modeling, word-by-word translation, and sentence reordering, and using the key learning advantages of each stage for selecting the best teacher models to improve non-autoregressive NMT.  We attempt to examine and identify whether those stages exist in UNMT. 
%\isidora{add other papers doing NMT analysis}
\end{comment}

%\subsection*{Exposure bias mitigation}
\subsection*{Explanations \& Explanation-guided training}

Several previous works outline and summarize the findings of explainability and interpetability- related research in NLP \citep{belinkov-etal-2020-interpretability,sun2021interpreting,tenney2020language,madsen2021post,danilevsky2020survey,qian2021xnlp}.
\citet{weber2022beyond} provide a systematic review of explainable AI methods employed in improving various properties of Machine Learning models, such as performance, convergence, robustness, reasoning, efficiency and equality. Of these, of particular interest, and the focus of our work, are those that along with measuring feature importance and distinguishing relevant from irrelevant features, are utilized to augment the intermediate learned features, and improve model performance or reasoning \citep{anders2022finding,sun2021explanation,zunino2021excitation,fukui2019attention,zhou2016learning,mitsuhara2019embedding,schiller2019relevance,zunino2021explainable}. 

In our paper, we modify the approach of
\citet{sun2021explanation}, that proposes a model-agnostic LRP-guided training method for few shot classification, to improve model generalization to new classes, extending it to a transformer-based masked language model for NMT. Every intermediate feature representation f\textsubscript{p} is weighted (multiplied) by its relevance R(f\textsubscript{p}), with respect to the feature processing output, normalized in [-1,1]. The model is then trained on a loss function taking into account both predictions, $p$ and $p_{lpr}$ and given by the following formula
\begin{equation}
{L' = \xi * L (y,p) + \lambda * L (y, p\textsubscript{lrp})}\end{equation}where ${\xi}$, ${\lambda}$
are positive scalars. In this way, the features more relevant to the prediction are emphasized, while the less relevant ones are downscaled.
Other recent works utilize LRP for improving model performance in the medical domain
\citep{sefcik2021improving}, mitigating the influence of language
bias for image captioning models \citep{sun2022explain}.


\begin{comment}
\subsection*{Robustness}
Various works examine Robustness in NLP \citep{yu2022measuring,zhang2022interpreting,wang2021measure,zhang2022improving, la2022king,chen2022can,zhao2022certified,wang2020cat}, measuring NLP models' performance against perturbed or unseen input, and suggesting ways to alleviate its effect. Specifically for NMT, 
\citet{niu2020evaluating} propose two metrics, Robustness and Consistency to measure sensitivity of a model to input perturbations. % using those in evaluating models employing subword regularization. 
\citet{sun2020robust} are the first to examine robustness in Unsupervised NMT, categorizing text noise into word and word-order noise, evaluating its influence in UNMT and introducing adversarial training methods to mitigate it.
In \citet{cheng2019robust}, the authors propose a two stage method based on adversarial source and target examples used in the model in an attack and defence alternating approach, to improve model robustness.
\citet{fujii2020phemt} introduce a Japanese to English dataset to examine the robustness of NMT systems when certain linguistic phenomena appear in text. Finally \citet{li2019improving} suggest various data augmentation techniques to extend existing corpora used to evaluate model robustness.
\end{comment}

%\section{Method}

\section{Method \& Experiments}

%Languages: french, gujarati
%amounts of data: 22k (for fr, gu, so)
%52k (for fr, so)
%1m, 2.5m, 5m, all (for fr)

\subsection{Model \& Translation Quality Evaluation}
In our experiments we use a 6-layers 8-heads encoder-decoder transformer-based model, XLM \citep{lample2019cross}, following the training configurations and hyperparameters suggested by the authors. We use Byte Pair Encoding \citep{sennrich-etal-2016-neural} to extract a 60k vocabulary, and have an embedding layer size of 1024, a dropout value and an attention layer dropout value of 0.1, and a sequence length of 256. We measure the quality of the Language Model (LM) with perplexity, and quality of the NMT output with BLEU \citep{papineni2002bleu}, both used as training stopping criteria, when there is no improvement over 10 epochs. All further parameter values are provided in the Appendix.
We first pre-train a Language Model in each language with the MLM objective, which is then used to initialize the encoder and decoder of the NMT model. We further train a NMT model, 
using backtranslation (BT) and denoising auto-encoding (AE) with the monolingual data used for LM pretraining for UNMT, the Machine Translation (MT) objective for the Supervised NMT model, and BT+MT for the joint Unsupervised and Supervised approach. 


\subsection{Datasets}
The languages we work with are English, French, Gujarati, Kazakh, and we're translating in all English-centric directions, English--French (En--Fr), French--English (Fr--En), English--Gujarati (En--Gu), Gujarati--English (Gu--En), English--Kazakh (En--Kk), Kazakh--English (Kk--En). For English and French, we use 5 million News Crawl 2007-2008 monolingual sentences for each language, and 23 million WMT14 parallel sentences. 
For Gujarati, we have 1.4 million sentences and for Kazakh we have 9.5M monolingual sentences, collected for both languages from Wikipedia, WMT 2018, 2019 and Leipzig Corpora (2016)\footnote{https://wortschatz.unileipzig.de/en/download/}. %For Somali, we have 1.97M Wikipedia and Leipzig Corpora sentences. 
As parallel data, we have 22k and 132k from the WMT 2019 News Translation Task\footnote{http://data.statmt.org/news-crawl/} for Gu--En and Kk--En.%, respectively, and 22k sentences from LORELEI \citep{tracey-etal-2019-corpus} for So--En.
As development and test sets, we use newstest2013 and newstest2014, respectively, for En--Fr and Fr--En, WMT19 for En--Gu and Gu--En and En--Kk and Kk--En.%, and LORELEI sentences for En--So, So--En.
%\footnote{http://habit-project.eu/wiki/SetOfEthiopianWebCorpora}.

%Since 2019, WMT produces newstest sets with
%only source-original text and human translations
%on the target side to mitigate known issues when
%translating and evaluating on target-original data
\begin{table*}[!htbp]
\centering
\small
\begin{tabular}{lcccc|c|ccc|c}
\toprule
&&& \textbf{En--Fr} & &&  & \textbf{Fr--En} &  \\\hline
Parameters' Set Values &  & v1 & v2 & v3 &Regular& v1 & v2 & v3 &Regular \\\hline
\midrule
22k  &&&&&&&&& \\
\midrule
%Regular MT &&&&&31.12 &&&&30.63   &&&&1.04   &&&&2.65 \\
%Regular BT+AE+MT &  &&&&34.54 &&&&34.02  &&&&1.16   &&&&2.19 \\\hline
MT  &&29.9&24.67&30.24&31.12&30.12&26.79 &\textbf{30.52}&30.63 \\%&2&2.18&\textbf{2.19}&1.04&0.69&0.7 &\textbf{0.73}&2.65 
BT+AE+MT &&30.01&28.62&\textbf{32.15}&34.54&29.94&29.87&\textbf{33.03}&34.02 \\%&0.76&\textbf{1.36}&0.89&1.16 &0.71&\textbf{1.06}&0.67&2.19
\midrule
\midrule
1m  & &&&&&&&\\\hline
%Regular MT &&&&&41.25 &&&&41.33   &&&&-   &&&&- \\
%Regular BT+AE+MT &  &&&&40.37 &&&&40.4  &&&&-   &&&&- \\\hline
MT & &\textbf{39.12}& 38.12&35.44 &41.25& \textbf{39.2}&38.43&36.04&41.33%&-&-&-&-&-&-&-&-
\\
BT+AE+MT & & \textbf{37.58}& 37.22& 36.89&40.37& 37.66& \textbf{37.95}& 37.38&40.4%&-&-&-&-&-&-&-&-
\\
\midrule
\midrule
2.5m  &&&&&&&&\\\hline
%Regular MT &&&&&40.46 &&&&40.71   &&&&-   &&&&- \\
%Regular BT+AE+MT &  &&&&39.88 &&&&39.62  &&&&-   &&&&- \\\hline
MT &  &\textbf{39.06}&36.02  &35.57&40.46&\textbf{39.11} &36.28&36.09&40.71%&-&-&-&-&-&-&-&-
\\
BT+AE+MT &  &37.68 & 36.21& \textbf{37.74} &39.88&40.71&37.48 &36.66& \textbf{38.15}%&-&-&-&-&-&-&-&-
\\
\midrule
\midrule
5m &&&&&&&&\\\hline
%Regular MT &&&&&41.52 &&&&41.18   &&&&-   &&&&- \\
%Regular BT+AE+MT &  &&&&40.89 &&&&40.8  &&&&-   &&&&- \\\hline
MT  &&\textbf{39.21} &37.55&39&41.52&\textbf{39.33} &38.01&39.2&41.18%&-&-&-&-&-&-&-&-
\\
BT+AE+MT &&30.71&36.7&\textbf{37.48}&40.89&32.02 &37.03&\textbf{37.67}&40.8%&-&-&-&-&-&-&-&- 
\\
\midrule
\midrule
{\emph{Other methods}} &&&&&45.9 &  &&&-\\
\bottomrule
\end{tabular}
\caption{BLEU scores for Supervised, and Unsupervised + Supervised NMT Layerwise Relevance Propagation-guided experiments, for En--Fr, Fr--En. \emph{AE}, \emph{BT} and \emph{MT} stand for Auto-Encoding loss, Back Translation loss and Machine Translation loss, respectively. Test and validation sets are from newstest2013-14 for French. State of the art results (\textit{Other methods}) for En--Fr come from \url{http://www.deepl.com/press.html}, \url{http://nlpprogress.com/english/machine_translation.html}.}
\vspace{-2mm}
\label{table:bleu_results_lrp_train_fr}
\vspace{-0.5em}
\end{table*}

\subsection{Layer-wise Relevance Propagation (LRP)}
%As previously explained in \citet{voita2020analyzing}, calculation of the Layer-wise Relevance Propagation score in an encoder-decoder Transformer architecture could at first seem confusing due to the non-clear layered nature of the model. 

We follow the method proposed by \citet{voita2020analyzing} in calculating LRP in an encoder-decoder Transformer architecture. The Relevance Score is first propagated inversely through the decoder and then the encoder, up to the input layer of the architecture. The conservation principle only holds across all processed tokens, and the score is defined as relevance of the input neurons to the top-1 logit predicted by the Transformer model, and the sum of the input neurons' relevance is regarded as the token contribution.
\begin{comment}
considering the Relevance score to be propagated first inversely through the decoder and then the encoder, up to the input layer of the architecture, and without assuming that the conservation principle holds here per layer, but only across all tokens processed by the model. LRP score is then defined as the relevance of the input neurons to the top-1 logit predicted by the Transformer model, and the sum of the input neurons' relevance is regarded as the token contribution.
\end{comment}
The total source and target sentence contributions to the result are defined as the summation of the Relevance of tokens in the source sentence, ${x}$ and that of those in the target sentence, ${y}$, at generation step t.
%\begin{comment}
\begin{equation}
R_t(source)  = \sum_{i}^{}{x_i} ,
\\\\  \hspace{3mm}
R_t(target) =  \sum_{j=1}^{t-1}{y_j}
\end{equation}
%\end{comment}
At every step t, Relevance of the source and target sentences follow the conservation principle, summing up to 1.
%$R_t(source) + R_t(target) = 1$. At step 1, we also have $R_1(source) = 1$, $R_1(target) = 0$. 
Moreover, for every target token past the currently generated one, Relevance Score is 0.



%\subsection{Exposure Bias}

%\subsection{Translationese}

\begin{table*}[!htbp]
\small
\centering
\begin{tabular}{lcccc|c|ccc|c}
\toprule
&&& \textbf{en--gu} &&&& \textbf{gu--en} && \\\hline
% \cmidrule{4-5} \cmidrule{8-9}
\midrule
Parameters' Set Values &  & v1 & v2 & v3 &Regular& v1 & v2 & v3 &Regular \\\hline
22k &&&&&\\
%Regular MT &&&&&31.12 &&&&30.63   &&&&1.04   &&&&2.65 \\
%Regular BT+AE+MT &  &&&&34.54 &&&&34.02  &&&&1.16   &&&&2.19 \\\hline
\midrule
MT && 2 & 2.18 &\textbf{2.19}&1.04&0.69&0.7 &\textbf{0.73}&2.65 \\
BT+AE+MT &&0.76&\textbf{1.36}&0.89&1.16 &0.71&\textbf{1.06}&0.67&2.19\\\hline
\midrule
%\midrule
{\emph{Other methods}} &&&&&0.1&  &&&0.3\\
\bottomrule
\end{tabular}

\caption{BLEU scores for Supervised, and Unsupervised + Supervised NMT Layerwise Relevance Propagation-guided experiments, for En--Gu, Gu--En. \emph{AE}, \emph{BT} and \emph{MT} stand for Auto-Encoding loss, Back Translation loss and Machine Translation loss, respectively. Test and validation sets are from WMT19 for Gujarati. State of the art results (\textit{Other methods}) can be found in \url{https://github.com/google-research/bert/blob/master/multilingual.md}. }
\vspace{-2mm}
\label{table:bleu_results_lrp_train_gu}
\vspace{-0.5em}
\end{table*}

\begin{table*}[!htbp]
\small
\centering
\begin{tabular}{lcccc|c|ccc|c}
\toprule
&& & \textbf{en--kk} &&&& \textbf{kk--en} && \\\hline
Parameters' Set Values &  & v1 & v2 & v3 &Regular& v1 & v2 & v3 &Regular \\\hline
\midrule
22k  &  && &&&   &&\\
\midrule
%Regular MT &&&&&31.12 &&&&30.63   &&&&1.04   &&&&2.65 \\
%Regular BT+AE+MT &  &&&&34.54 &&&&34.02  &&&&1.16   &&&&2.19 \\\hline
MT  &&2.1&3&\textbf{3.2}&2.4&2.2&2.1&\textbf{2.7}&2.6 \\
BT+AE+MT &&1.6&\textbf{3}&2.3&2.8&2.1&\textbf{2.6}&2.&2.9\\
\midrule
\midrule
132k  &&&&&&&&\\
%Regular MT &&&&&41.25 &&&&41.33   &&&&-   &&&&- \\
%Regular BT+AE+MT &  &&&&40.37 &&&&40.4  &&&&-   &&&&- \\\hline
MT &&4.8&5.6&5.3&5.2&6.8&\textbf{8.5}&8.4&8\\
BT+AE+MT &&5.2&\textbf{6.8}&6.4&6.6&8.7&\textbf{9.4}&9.2&8.9\\
\midrule
\midrule
{\emph{Other methods}} &&&&&2.5\footnote{https://www.deepl.com/press.html} &  &&&7.4\\
\bottomrule
\end{tabular}

\caption{BLEU scores for Supervised, and Unsupervised + Supervised NMT Layerwise Relevance Propagation-guided experiments, for En--Kk, Kk--En. \emph{AE}, \emph{BT} and \emph{MT} stand for Auto-Encoding loss, Back Translation loss and Machine Translation loss, respectively. Test and validation sets are from WMT19. State of the art results (\textit{Other methods}) can be found in \url{https://github.com/google-research/bert/blob/master/multilingual.md}.}
\vspace{-2mm}
\label{table:bleu_results_lrp_train_kk}
\vspace{-0.5em}
\end{table*}


\subsection{LRP-weighted training}
Following \citet{sun2021explanation}, we attempt to utilize LRP contributions during training, and examine performance. In our case, the representation of every intermediate source or target token $x_{i}$, with Relevance Score $R_t(x_i)$, is reweighted by its score at each layer, and included in a new loss term, $L\textsubscript{ce} (y,x_{i})$. The formula takes the following form
\begin{equation}
L' = \xi * L (y,p(x_i)) + \lambda * L (y, p({R_t}(x_i))\end{equation},where $p(x_i)$ and $p({R_t}(x_i))$ are the model prediction and the explanation-guided prediction, respectively.
The loss then is the weighted sum of the previous and the new terms, each weighted by parameters $\xi$, $\lambda$ respectively, for which we experiment with three sets of values:
\begin{equation}
\xi, \lambda = \{v\textsubscript{1} = \{1,0.5\}, v\textsubscript{2} = \{0,1\}, v\textsubscript{3} = \{1,1\}\}.
\end{equation}
In the first layer we only weigh the word embedding of the token.
We hypothesize that in this way, the tokens with a higher contribution to the NMT result are enhanced, while the effect of the ones contributing less is reduced.



%LRP-weighted AL\\

\section{Results \& Discussion}
In Tables \ref{table:bleu_results_lrp_train_fr}, 
\ref{table:bleu_results_lrp_train_gu},
\ref{table:bleu_results_lrp_train_kk}, we present our results for LRP-guided training, in certain low- and high-resource Semi-Supervised and Supervised experiments, for all languages and directions, providing the regular NMT model results as our baselines.

We see that for En--Fr and Fr--En NMT, in Table \ref{table:bleu_results_lrp_train_fr}, the method fails to outperform baseline NMT results in all cases. The model translation quality is usually on par with baselines, and state-of-the art results on high scale experiments, and small differences in BLEU scores in the range of 0.1-0.5 can be considered negligible. 
Among the three hypermarameter settings, choosing v1 for training seems to outperform the other two in the majority of experiments under a MT-only setting. 
Results could indicate unsuitability of the method in high-resource settings; the original method was after all proposed in a few-shot classification context, hence we also seek more promising results in low-resource NMT experiments, examined below.

A different model behavior is observed for all cases but one in English to Gujarati NMT, in Table \ref{table:bleu_results_lrp_train_gu} when either the MT-only or BT-AE-MT objectives are used in training. This is an interesting finding - we can hypothesize that LRP-guided training might be more useful when translating into highly complex morphological languages such as Gujarati; Results marginally outperform previous state-of-the-art approaches, however more research including other languages and potentially other parameter values is required to verify that observation.

Encouraging is also the case of LRP-guided training in En--Kk and Kk--En NMT. In a large number of settings, training with Relevance guidance improves NMT BLEU scores significantly compared to our regular models' results. More specifically, we see improvement in both low- (22k) and mid-resource (132k) experiments, with v2-parameterized models to outperform our baselines in the majority of cases. Also, all experiments we are able to perform better than the state of the art current result, hence relevance guidance shows potential again in NMT experiments where few parallel data are available.

%\ref{table:bleu_results_lrp_train_fr_gu}
%\ref{table:bleu_results_lrp_train_kk}
%\include{table_lrp_train.tex}
%\include{table_lrp_gu.tex}
%\include{relevanceNMT/table_lrp_train_sideways}


%\begin{sidewaystable}[htbp]

%\end{sidewaystable}





%\begin{sidewaystable}[htbp]

%\end{sidewaystable}



\begin{comment}
%\include{relevanceNMT/table_res}
\begin{table}[ht!]
\small
\centering
%\resizebox{\textwidth}{}
\begin{tabular}{lrrrrrr}
\toprule
\textbf{Method}  & \textbf{En--Fr}  & \textbf{Fr--En} &\textbf{En--Gu} & \textbf{Gu--En}  \\ 
\hline
\multicolumn{4}{l}{\emph{Other methods}} \\ \hline
 &45.9\footnote{https://www.deepl.com/press.html}   &-  &15.9 \footnote{Kuwanto, Garry, et al. "Low-Resource Machine Translation Training Curriculum Fit for Low-Resource Languages." arXiv preprint arXiv:2103.13272 (2021)}  &13.2  \\ \hline
\hline
\multicolumn{4}{l}{Our method} \\ \hline
BT+AE    &21.76   &21.69&0.4 &0.46  \\ \hline\hline
Parallel data: 22k&&&\\\hline
MT    & 31.12 &30.63 & 1.04 &\textbf{2.65} \\ 
BT+AE+MT  & 34.54  & 34.02& \textbf{1.16} & 2.19 \\ \hline\hline
1m&&&\\\hline
MT   &41.25&41.33& - & -  \\
BT+AE+MT   &40.37&40.4& - & - \\\hline\hline
2.5m&&&&\\\hline
MT   &40.46&40.71& - & -  \\
BT+AE+MT   &39.88&39.62& - & - \\\hline\hline
5m&&&&\\\hline
MT   & 41.52  & 41.18& - & -  \\ 
BT+AE+MT   & 40.89  & 40.8& - & -\\\hline\hline
23m&&&&\\\hline
MT   &\textbf{41.75}&\textbf{41.41}& - & -  \\
BT+AE+MT   &41.29&40.99& - & - \\\hline
\end{tabular}
%\vspace*{-0.5em}
\bottomrule
\vspace*{0.5em}
\caption{BLEU scores for Unsupervised, Supervised, and Unsupervised and Supervised NMT experiments, for En - Gu, Gu - En, En - Fr, Fr - En. \emph{AE}, \emph{BT} and \emph{MT} stand for Masked Language Model, Auto-Encoding loss, Back Translation loss and Machine Translation loss, respectively. Test and validation sets are from WMT19 for Gujarati, and newstest2013-14 for French.}
\vspace{-2mm}
\label{table:bleu_results}
\vspace{-0.5em}
\end{table}

\end{comment}

\begin{comment}
\begin{figure*}[t!]
%\begin{multicols}{2}
\begin{subfigure}{\columnwidth}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/En--Fr/en_fr_bleu.png}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/Fr--En/fr_en_bleu.png}
\caption{}
\label{fig:bleu_over_batches_fr}
\end{subfigure}
%\end{figure*}
%\begin{figure*}[t!]
%\begin{multicols}{2}
\hfill 
\begin{subfigure}{\columnwidth}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/En--Gu/en_gu_bleu.png}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/Gu--En/gu_en_bleu.png}
\caption{}
\label{fig:bleu_over_batches_gu}
\end{subfigure}
\caption{\label{fig:bleu_over_batches}BLEU scores for (a) En - Fr, Fr - En, (b) En - Gu and Gu - En experiments over the number of batches}
\end{figure*}

Table \ref{table:bleu_results} shows NMT BLEU scores for our Unsupervised, Supervised, and Unsupervised and Supervised experiments, for En - Gu, Gu - En, En - Fr, Fr - En. We provide the state-of-the art results for the sake of consistency, though our goal is not to outperform the best NMT models for those languages.
\end{comment}
%\include{relevanceNMT/table_bleu_mtnt.tex}

\begin{comment}
\begin{multicols}{4}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Fr/en_fr_ppl.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Fr--En/fr_en_ppl.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Gu/en_gu_ppl.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Gu--En/gu_en_ppl.png}
\end{multicols}\caption{\label{fig:ppl_over_batches}Perplexity (PPL) scores for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\end{comment}

%\ BLEU scores for each experiment are given in Table \ref{table:bleu_results}.


\begin{comment}
\begin{figure*}[tp]
%\begin{multicols}{2}
\begin{subfigure}{\columnwidth}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/En--Fr/en_fr_frs.png}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/Fr--En/fr_en_frs.png}
\caption{}
\label{frs_over_batches_fr}
\end{subfigure}
\hfill 
\begin{subfigure}{\columnwidth}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/En--Gu/en_gu_frs.png}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/Gu--En/gu_en_frs.png}
\caption{}
\label{frs_over_batches_gu}
\end{subfigure}
\caption{\label{fig:frs_over_batches}Fuzzy Reordering Scores (FRS) between reference sentences and generated translations, for (a) En - Fr, Fr - En, (b) En - Gu and Gu - En experiments over the number of batches}
%\end{figure*}
%\hspace{-100mm}
%\begin{figure*}[t]
%\begin{multicols}{2}
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/En--Fr/en_fr-diff_frs.png}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/Fr--En/fr_en-diff_frs.png}
\caption{}
\label{frs_diff_over_batches_fr}
\end{subfigure}
\hfill 
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/En--Gu/en_gu-diff_frs.png}
\includegraphics[width=.49\columnwidth]{relevanceNMT/figures/Gu--En/gu_en-diff_frs.png}
\caption{}
\label{frs_diff_over_batches_gu}
\end{subfigure}
\caption{\label{fig:frs_diff_over_batches}Fuzzy Reordering Scores (FRS) between source sentences and generated translations for (a) En - Fr, Fr - En, (b) En - Gu and Gu - En experiments over the number of batches}
\end{figure*}
\end{comment}

\begin{comment}
\begin{figure*}[t!]
\begin{multicols}{4}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Fr/en_fr_frs.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Fr--En/fr_en_frs.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Gu/en_gu_frs.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Gu--En/gu_en_frs.png}
\end{multicols}\caption{\label{fig:frs_over_batches}Fuzzy Reordering Scores (FRS) between reference sentences and generated translations, for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\begin{multicols}{4}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Fr/en_fr-diff_frs.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Fr--En/fr_en-diff_frs.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Gu/en_gu-diff_frs.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Gu--En/gu_en-diff_frs.png}
\end{multicols}\caption{\label{fig:frs_over_batches}Fuzzy Reordering Scores (FRS) between source sentences and generated translations for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\end{figure*}
\end{comment}

\begin{comment}
In Figure \ref{fig:frs_over_batches} we show the Fuzzy Reordering Score (FRS) between generated translations and references over the number of batches processed by the model during training. For all En - Fr, Fr - En experiments, we observe a large fluctuation yet a small and gradual FRS increase and then, in the majority of cases, a small decrease throughout training. A higher FRS indicates a more monotonic alignment, so we start from non-monotonic alignments in the first translation stage, reach maximum FRS values in the second stage of training - hence highly aligned translation sentences and references - and see a slight decrease in the third training stage until model convergence.
Using parallel data for training (MT, BT-MT experiments) produces the most monotonic alignments, while BT-only models show the lowest FRS scores, indicating that we have the least identical reorderings between the reference and translation sentences in those cases. This applies to both high- and low-resource setups.

On the other hand, in En - Gu, Gu - En, 
BT and BT-MT experiments show high and steady FRS values compared to MT-only experiments, signaling that between languages with a more complicated and non-monotonic alignment, such as English and Gujarati, the usage of Back-Translation seems to help produce translations more aligned to the reference. In all experiments, we begin with having highly monotonic alignments.

In Figure \ref{fig:frs_diff_over_batches} we show the average Fuzzy Reordering Scores (FRS) between source sentences and generated translations. In contrast to previous results, for En - Fr, Fr - En it is surprising to see that FRS is relatively stable in all cases across training, and also that BT, BT-MT experiments tend to show the highest values, implying highly monotonic alignments; when Back-Translation is used, the generated translations tend to be closer to source sentences in terms of word order. On the other hand, scores are significantly lower in MT only experiments; source and translation alignments are less monotonic when models are trained with parallel data alone.
Results are similar in  En - Gu, Gu - En; Fuzzy Reordering scores show small fluctuation but a rather stable behavior, with BT, BT-MT training setups to have scores almost close to 1; we have an almost perfect alignment between source sentences and generated translations when Back-Translation is used.

\begin{figure*}[t!]
\begin{multicols}{4}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Fr/en_fr_ter.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Fr--En/fr_en_ter.png}\\
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Gu/en_gu_ter.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Gu--En/gu_en_ter.png}
\end{multicols}\caption{\label{fig:ter_over_batches}Translation Edit Rate (TER) between reference sentences and generated translations for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\begin{multicols}{4}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Fr/en_fr-diff_ter.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Fr--En/fr_en-diff_ter.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/En--Gu/en_gu-diff_ter.png}
    \includegraphics[width=4.3cm]{relevanceNMT/figures/Gu--En/gu_en-diff_ter.png}
\end{multicols}\caption{\label{fig:ter_diff_over_batches}Translation Edit Rate (TER) between source sentences and generated translations for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\end{figure*}

We furthermore examine the Translation Edit Rate (TER) scores between produced translations and references (Figure \ref{fig:ter_over_batches}). In En - Fr and Fr - En, low TER for MT, BT-MT experiments means more monotonic alignments between references and produced translations, in contrast to higher TER in the low-resource and BT-only experiments.
We also observe that TER gradually decreases over training for all models, which is expected as the translations generated at the end of training have a higher resemblance to the human translations.

Results are different for En - Gu and Gu - En; TER is significantly low in BT- only and BT-MT models, but rather high, and increasing in the MT-only model, with generated translations in the former case highly resembling references. Back-Translation produces more monotonic to the reference translations in the case of a language diverse in terms of script, morphological complexity and word order from English.

In Figure \ref{fig:ter_diff_over_batches} we show the average Translation Edit Rate (TER) scores during training between source sentences and generated translations. TER is stable or slightly increases during training, hence translations structurally resemble the source sentence throughout the entire training process (especially in MT experiments). For En - Fr and Fr - En, BT and BT-MT show the lowest TER values, hence with Back-Translation, generated sequences and source sentences tend to have a high monotonicity. A similar behavior is exhibited in the En - Gu and Gu - En results, where BT, BT-MT experiments show a lower TER and hence higher and more monotonic alignment of translations and source sentences, in contrast to MT-only setup. 

From calculating and representing FRS and TER values for all experiments, we can deduce that for En - Fr and Fr - En, generated translations have higher monotonicity to references in MT, BT-MT and lower in BT-only experiments, but higher monotonicity to source sentences in BT, BT-MT and lower in MT experiments. Hence, using parallel data in training leads to better translation to hypothesis alignment, while Back-Translation leads to better translation to source sentence alignment, with alignment in both cases meaning higher structural similarity and closer following the word order of the other sentence.


\begin{figure*}[htb]
\subfloat[]{%
     \includegraphics[width=.24\textwidth]{relevanceNMT/figures/En--Fr/en_fr_rms.png}}\hfill
    \subfloat[]{%
     \includegraphics[width=.24\textwidth]{relevanceNMT/figures/Fr--En/fr_en_rms.png}}\hfill
     \subfloat[]{%
     \includegraphics[width=.24\textwidth]{relevanceNMT/figures/En--Gu/en_gu_rms.png}}\hfill
     \subfloat[]{%
   \includegraphics[width=.24\textwidth]{relevanceNMT/figures/Gu--En/gu_en_rms.png}}\\
\caption{Ratio Margin-based Similarity Score (RMSS) between reference sentences and generated translations for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\label{fig:rms_over_batches}

 \subfloat[]{%
    \includegraphics[width=.24\textwidth]{relevanceNMT/figures/En--Fr/en_fr-diff_rms.png}}\hfill
     \subfloat[]{%
     \includegraphics[width=.24\textwidth]{relevanceNMT/figures/Fr--En/fr_en-diff_rms.png}}\hfill
      \subfloat[]{%
 \includegraphics[width=.24\textwidth]{relevanceNMT/figures/En--Gu/en_gu-diff_rms.png}}\hfill
  \subfloat[]{%
 \includegraphics[width=.24\textwidth]{relevanceNMT/figures/Gu--En/gu_en-diff_rms.png}}\\
\caption{Ratio Margin-based Similarity Score (RMSS)  between source sentences and generated translations for En - Fr, Fr - En, En - Gu and Gu - En experiments over the number of batches}
\label{fig:rms_diff_over_batches}
\end{figure*}

\end{comment}

\begin{comment}
\subsubsection*{Contributions throughout training}

We examine the average source contribution, the entropy of source contributions and the entropy of target contributions during training for all our experiments. Our main observations confirm the findings of \citet{voita2020analyzing,voita2021language}, that changes in sentence contributions are not necessarily monotonic   
to the result, and can help distinguish different stages in the training process, as well as identify the balance between source and target sequences' relevance to the result in various training setups and objectives (BT, MT, BT-MT) (Figures \ref{fig:contrib1},\ref{fig:contrib2},\ref{fig:contrib3},\ref{fig:contrib4}).

For En - Fr and Fr - En NMT models trained with 23 million parallel sentences (Figures \ref{fig:contrib1},\ref{fig:contrib2}), average source sentence contributions drop at the very beginning of training, while contributions are lowest in both directions in MT, and slightly higher in BT, BT-MT experiments; using only parallel (natural) data during training, average source contributions are lower \citep{voita2020analyzing} and the model relies more on the target prefix for sequence generation, while using back-translated data seems to boost the influence of source sentence to the result. 
We also notice that the average contributions are mostly stable or slightly decrease as training progresses, and the source sentence becomes less important in generating the target sequence.
%Different from that, we see that the average contributions in BT-MT with 22k parallel data gradually increase before decreasing. 
Observing the behavior of the models trained with less data (MT-22K, BT-MT-22K), contributions are high, which is no surprise, given that due to the lack of data the relevance of the source sentence tokens to the generated target sentence is high. 
%Experiments using Back-Translation on top of the MT objective show lower contributions here in contrast to the full data setting. \isidora{why?} 

What is most interesting, is that the entropy of the source contributions is high for MT-only experiments (MT, MT-22k), but low for experiments involving Back-Translation (BT, BT-MT, BT-MT-22k), and relatively stable in all other cases. Hence, source contributions are more focused in all MT-only experiments, and the model is more confident in choosing the important source tokens, while, on the other hand, the model in BT-only and BT-MT experiments requires broader source context for the target sequence generation, hence entropy of contributions is high. That applies to both evaluation directions. We also observe that in setups involving the MT objective, the training process converges significantly faster than in all other experiments which use Back-Translation alone, or along with parallel data in training.

This is supported by studying the entropy of the target contributions; in both En - Fr and Fr - En directions, target entropy is more focused during the first part of training. Then, we notice either a small (BT, MT-22k, BT-MT-22k) or a larger (MT, BT-MT) increase, which gradually evens out as the model converges. Experiments with a small amount of training data, and/or Back-Translation (BT, BT-MT, BT-MT-22k, MT-22k) have significantly lower entropy contributions than MT-only, with Back-Translation contributing to the model having higher confidence in choosing the target tokens generated. On the contrary, in French to English NMT, combined experiments (BT-MT, BT-MT-22k) seem to have the highest, hence less focused target contributions; translating into English is more challenging in those cases requiring more target context for sequence generation.

Contributions' patterns are not similar for En - Gu and Gu - En models (Figures \ref{fig:contrib4}, \ref{fig:contrib5}).
The average source contributions in experiments involving the MT objective are always higher than those with BT (BT, BT-MT experiments), implying that using natural parallel data during training forces the model to rely on source tokens more heavily. The average source contributions, are lowest in the BT-only experiment, hence there is highest target sentence reliance for generation.

Patterns in entropy of source contributions resemble the ones in En - Fr and Fr - En experiments. Entropy is low in MT-only, meaning that training with parallel data only increases model confidence in selecting the important source tokens for target generation, while entropy in BT, BT-MT experiments is similarly high.
Examining the entropy of target contributions, we notice an increase and high values in MT-only experiments in both directions, which validates our hypothesis that source contributions are more focused in these cases (hence target ones are less), while the entropy in experiments involving Back-Translation (BT, BT-MT) is lower.

Looking for differences between evaluation directions, En - Gu contributions in MT- and BT-MT experiments are similar to those in the En - Fr low resource experiments (MT-22k, BT-MT-22k), in contrast to training in the other direction.

\begin{table*}[ht]
\small
\begin{tabular}{l  B lll|lll|lll|llll}
\toprule
&&& \textbf{En--Fr} & &  & \textbf{Fr--En} & & & \textbf{En--Gu} &&& \textbf{Gu--En} & \\\hline
 &  & BLEU & R & C & BLEU & R & C & BLEU & R    & C & BLEU & R    & C \\\hline
BT  &&&&&&&&&&&&&\\
\textbullet \hspace{0.1cm} original &&\textbf{22.6}&-&-&\textbf{21.78}&-&-&0.31&-&-&0.36&-&-\\
\textbullet \hspace{0.1cm} misspelling &&14.77&0.65&\textbf{17.82}&16.86&\textbf{0.77}&\textbf{16.52}&\textbf{2.49
}&0.03&\textbf{1.59}&\textbf{3.27}&0.08&\textbf{1.33}\\
\textbullet \hspace{0.1cm} case-changing &&14.87&\textbf{0.66}&13.34&14.56&0.66&11.3&1.22&\textbf{0.93}&0.45&0.91&\textbf{0.52}&0.65\\\hline \hline
22k  &  && &&&   &&&   &&&   & \\\hline
MT  &&&&&&&&&&&&&\\
\textbullet \hspace{0.1cm} original &&\textbf{31.12}&-&-&\textbf{30.63}&-&-&\textbf{2.51}&-&-&\textbf{0.77}&-&-\\
\textbullet \hspace{0.1cm} misspelling &&30.22&\textbf{0.97}&\textbf{26.03}&30.4&\textbf{0.99}&\textbf{31.24}&0.05&\textbf{0.01}&0&0.23&0.29&0.3\\
\textbullet \hspace{0.1cm} case-changing &&20.01&0.64&22.13&23.34&0.76&24.93&0&0&0&0.33&\textbf{0.42}&\textbf{0.47}\\\hline
BT+AE+MT &&&&&&&&&&&&&\\
\textbullet \hspace{0.1cm} original &&\textbf{34.42}&-&-&\textbf{33.87}&-&-&\textbf{1.08}&-&-&2.19&-&-\\
\textbullet \hspace{0.1cm} misspelling &&31.79&\textbf{0.92}&\textbf{28.18}&32.62&\textbf{0.96}&\textbf{33.44}&0.72&0.66&\textbf{0.79}&\textbf{3}&0.36&\textbf{2.37}\\
\textbullet \hspace{0.1cm} case-changing &&23.06&0.66&22.52&27&0.79&24.22&0.9&\textbf{0.83}&0.6&2.12&\textbf{0.96}&1.88\\\hline \hline
23m &&&&&&&&&&&&&\\\hline
MT  &&&&&&&&&&&&&\\
\textbullet \hspace{0.1cm} original &&\textbf{41.84}&-&-&\textbf{41.41}&-&-&-&-&-&-&-&-\\
\textbullet \hspace{0.1cm} misspelling &&40.16&\textbf{0.96}&\textbf{35.36}&40.63&\textbf{0.98}&\textbf{41.5}&-&-&-&-&-&-\\
\textbullet \hspace{0.1cm} case-changing &&27.26&0.65&29.68&30.08&0.72&30.79&-&-&-&-&-&-\\\hline
BT+AE+MT  &&&&&&&&&&&&&\\
\textbullet \hspace{0.1cm} original &&\textbf{42.63}&-&-&\textbf{42.37}&-&-&-&-&-&-&-&-\\
\textbullet \hspace{0.1cm} misspelling &&39.71&\textbf{0.93}&\textbf{35.04}&40.72&\textbf{0.96}&\textbf{41.49}&-&-&-&-&-&-\\
\textbullet \hspace{0.1cm} case-changing &&28.12&0.65&27.31&33.58&0.79&30.33&-&-&-&-&-&-\\
\end{tabular}
\bottomrule
\caption{BLEU scores, Robustness (R) and Consistency (C) values for Unsupervised (BT), Supervised (MT), and Unsupervised + Supervised (BT+AE+MT) NMT experiments, for the converged model (best model checkpoint) for En - Fr, Fr - En, En - Gu, Gu - En.  Test and validation sets are from WMT19 for Gujarati, and newstest 2013-14 for French, and are perturbed following the method suggested in \citet{niu2020evaluating} for misspelling and case-changing.}
\vspace{-2mm}
\label{table:bleu_robust_consist}
\vspace{-0.5em}
\end{table*}
\end{comment}

\begin{comment}
\subsubsection*{Semantic Similarity}
The values of RMSS between the generated translations, and the reference sentences or source sentences, respectively, for all Language Pairs (En - Fr, Fr - En, En - Gu, Gu - En) can be seen in Figures \ref{fig:rms_over_batches}, \ref{fig:rms_diff_over_batches}. In the former case, MT-only and BT-MT experiments have high RMSS values in En - Fr, Fr - En, hence we have a higher semantic similarity of reference and generated sentences, than in BT-only or in reduced dataset experiments, which slightly increases throughout training. On the contrary, in En - Gu and Gu - En, generated translations in MT experiments are less similar to the reference sentences, and most similar in BT-only experiments, for which we observe the highest RMSS.
Source sentences show high semantic similarity to the generated translations in MT-only experiments, followed by reduced-data model training results, outperforming BT-only or BT-MT models, in En - Fr and Fr - En; in the first direction, RMSS is very similar across models, while in the latter, behavior of the model in different setups is significantly more distinct.  For En - Gu and Gu - En, BT-only experiments show highest semantic similarity between source sentences and translations, though in this case, especially in the second direction, RMSS in BT-MT is significantly high and comparable to RMSS in BT models.
Hence we can conclude, that for En - Fr and Fr - En, BT almost always reduces output and references' or source sentences' similarity, while the opposite happens in En - Gu and Gu - En, where BT seems to lead to a highly similar to reference or to source sentence output.
\end{comment}

\begin{comment}
\subsubsection* {Robustness and Consistency}
In Table \ref{table:bleu_robust_consist},  we present our models' Robustness and Consistency scores in all directions and training setups, when sentences are perturbed in two different ways, \textbf{misspelling} and \textbf{case-changing}. Examples of sentences and their perturbations can be seen in Table \ref{table:examples_perturbed} in the Appendix (Â§\ref{sec:appendix}).

Robustness takes values in [0,1].
En - Fr and Fr - En NMT models are highly robust in MT, BT-MT setups, regardless of the amount of parallel data used in training, and especially when test sets are misspeled, and significantly less robust when they're trained with Back-Translation only. On the contrary, En - Gu and Gu - En models are highly robust when Back-Translation is used in training, either alone or with the MT objective (BT, BT-MT experiments), and on test sets perturbed by case-changing.
That is a very interesting result indicating that for highly morphological complex languages such as Gujarati, Back-Translation may help boost model robustness compared to simply training a model with parallel data only - meaning the model can more easily recover from test sentence perturbations.

Measuring consistency, we attempt to evaluate similarity of regular and perturbed sentences' translations. In En - Fr and Fr - En NMT models, the patterns we notice are very similar to those found for Robustness: models are highly consistent in MT, BT-MT experiments, primarily when test sentences are misspelled, with their consistency capability increasing according to the amount of parallel data used in training. Back-Translation, when used alone in training, does not seem to help.

Similarly, model Consistency behavior patterns for En - Gu, Gu - En follow those of Robustness, with Back-Translation in BT, BT-MT experiments considerably making models more consistent, and highlighting its usefulness for such languages and training setups.

In Figures \ref{fig:robust_over_batches_fr} and \ref{fig:robust_over_batches_gu} we show model Robustness scores on the perturbed sentences generated translations for all directions, perturbations, and setups throughout the entire training process. For En - Fr, Fr - En, our model is consistently robust, but what is interesting to observe is that Robustness values are relatively stable as the model processes more and more tokens, for misspelled sentences, and slightly fluctuating for sentences modified with a case-changing perturbation.
Robustness' relative stability across training can be seen in En - Gu and Gu - En NMT model figures.
We can make similar observations for model consistency in all training setups and directions, as seen in Figures \ref{fig:consist_over_batches_fr} and 
\ref{fig:consist_over_batches_gu}. 
Hence, we can conclude that our NMT models' robustness and consistency change very little during training, regardless of the setup, the languages and translation direction, though they depend highly on the training objectives, where Back-Translation and usage of parallel data are high influence factors depending on the language morphological complexity and the amount of data used in training.

\end{comment}




\begin{comment}
%\include{relevanceNMT/table_robust_consist.tex}
%\include{relevanceNMT/rms_plots.tex}
\include{relevanceNMT/robustness_consistency_plots.tex}
%\include{relevanceNMT/table_lrp_train}
\include{relevanceNMT/entropy_figures}
%\section{Discussion}
\end{comment}

\section{Conclusions}
We perform a series of Semi-Supervised and Supervised Neural Machine Translation experiments, using an explainability-based metric, namely Relevance-guided propagation, during training; we leverage the measure of influence of the input and intermediate layer outputs to the NMT result, in an attempt to improve NMT for three quite different languages, lying in both high- and low- resource data regimes. Our results, though showing marginal and very small improvements, indicate that Layerwise-relevance propagation shows potential in boosting NMT quality when training in small data scenarios. Further exploration of the method, different model hyperparameter setups, and expansion of our method to other languages is strongly recommended as a next step to identify the efficiency and robustness of the proposed method.
%\section{Bib\TeX{} Files}
\section{Limitations}
Training a large Neural Machine Translation model from scratch is a hard task computationally, 
and employing LRP-guidance during training significantly raises training time, the amount and usage of required computational resources, and the complexity of the training process, calling for more efficient training solutions, in terms of memory distribution of the model and parallelization. These factors constitute the limitations of our approach, and allowed us to launch a small number of experiments, hence addressing those factors and expanding to more languages, in more efficient training and computational ways, is a strong requirement for further generalization of the method.

\section{Ethics Statement}
Several ethical concerns ought to be addressed when working with large language models regarding quality, toxicity and bias related to their training process and output \cite{10.1145/3442188.3445922,chowdhery2022palm,brown2020language},of which the authors of the paper are aware in their work.

%\newpage
\begin{comment}
\section*{Acknowledgements}
\end{comment}
\newpage
% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

%\section{Appendix}
%\label{sec:appendix}
%\appendix

\begin{comment}
\subsubsection*{Source Sentence Contributions}

We examine the entire source sentence contribution to each target sentence token, for the first and last model checkpoint in each experiment, for all experiments and both En - Fr, Fr - En and En - Gu, Gu - En directions (Figures \ref{fig:contrib5},\ref{fig:contrib6},\ref{fig:contrib9}). 
We also plot and compare each source sentence's token contribution to the entire target sentence, again for the first and last model checkpoint, for all experiments and translation directions (Figures \ref{fig:contrib7}, \ref{fig:contrib8}, \ref{fig:contrib10}).

Confirming \citet{voita2020analyzing}, during the target sentence generation, the source contributions, and the influence of the source to the result decreases, meaning the model relies more on the generated target sequence instead of the source sentence. We notice that results vary very little across BT, MT and BT-MT experiments, and are also comparable between first and last model checkpoints and evaluation directions, indicating that the source contribution to each generated target token does not change much when the training objective changes, or throughout training. 

%The contributions are higher in the first compared to the last checkpoint in each experiment, indicating that as training progresses, the target tokens' generation relies less and less on the source sentence.
%We also observe that contributions are slightly lower in the French to English vs English to French translation direction, indicating perhaps that English sentence tokens' generation is easier for the model and does not depend much on the source sentence, than when we're translating into French.

For each source sentence token's contribution to the entire target sentence (Figures \ref{fig:contrib7}, \ref{fig:contrib8}), source tokens at later positions influence more the generated translation than tokens in earlier ones; the model stops relying on the source after it has seen the majority of the source sentence tokens. Again we notice no significant differences across BT- or MT-only or BT-MT experiments, and also between training directions, and very subtle differences between the first and last checkpoint. Value of source contributions is higher in the experiments using Back-Translation, indicating higher reliance of the generated target sequence on the source sentence.

In En - Gu and Gu - En experiments, contributions of the source sentence to each target token decrease more gradually, and source contributions are significantly higher when generating the first part of the target sentence; source influences the target tokens generation more when generating the first part of the sentence, rather than the last part of the target sequence.

Examining the contribution of each source sentence token to the entire target sentence, we notice a gradual increase of relevance across the source sentence tokens in all experiments and directions, and a steep drop over the last tokens. This indicates a monotonically increasing reliance of the target sentence generated to the source sentence tokens being processed. In MT and BT-MT experiments, there is no significant difference between first and last checkpoints in each experiment, in both directions; that is different in the BT-only experiment, in which the last model checkpoint contributions of the later part of the source sentence slightly outperform those of the first one, in both training directions.

\end{comment}

\begin{comment}
\subsubsection*{Entropy of Source Sentence Contributions}

We show the entropy of the source contributions over each target sentence token position (Figures \ref{fig:contrib11}, \ref{fig:contrib12}, \ref{fig:contrib13}). For En - Fr and Fr - En, there are very little differences across training setups (BT, MT, BT-MT, MT-22k, BT-MT-22k), the entropy is highest when the first target token is produced, drops gradually up until 2/3 of the target sentence is generated, and then increases until the entire sentence has been created.
For En - Gu and Gu - En the patterns are similar. We see no major differences between the first and last model checkpoint, since in almost all experiments, in both directions, the respective plots overlap, and we notice a slight shift of the Fr - En and Gu - En plots compared to the En - Fr and En - Gu respectively, showing that more context is needed to complete a longer part of the target sequence in the latter case.


\end{comment}
\begin{comment}
%\subsubsection*{Frechet Bert Distance}
%Figures \ref{fig:fbd_over_batches}, \ref{fig:fbd_diff_over_batches} show the values of FBD throughout training, between either the reference or the source sentences and their translations.

\end{comment}

\begin{comment}
\subsubsection*{Average LRP contributions' heatmaps from source to target sentence tokens}
In out attempt to further analyze and understand the source to target contributions between source and target sentence tokens' we plot their average LRP contributions' heatmaps, seen in
Figures \ref{fig:heatmaps_rev_en_fr_bt} -
%\ref{fig:heatmaps_rev_en_fr_mt},
%\ref{fig:heatmaps_rev_en_fr_bt_mt},
%\ref{fig:heatmaps_rev_en_gu_bt}, 
%\ref{fig:heatmaps_rev_en_gu_mt}, 
\ref{fig:heatmaps_rev_en_gu_bt_mt}, 
for all languages and translation directions and setups.

Examining Figure \ref{fig:heatmaps_rev_en_fr_bt}, we see a high contribution of all source sentence tokens to the first target sentence token, yet a small contribution to all other tokens; we notice a slight difference in the last model checkpoint in En - Fr NMT, where we have high contributions to specific intermediate target tokens (eg 20, 22, 30, 32, 62, 66, 80, 92) - further examination of examples is needed though in this case to identify why and if tokens in these positions have different characteristics that justify our observations.
In Figure \ref{fig:heatmaps_rev_en_fr_mt}, it's interesting to observe that every source sentence token in French contributes heavily to the first target sentence token in English, with a slightly increasing contribution throughout the entire evaluation process, in both translation directions. Contribution to all other target sentence tokens is small and stable throughout generation.
Lastly, the model behavior in BT-MT experiments, as seen in Figure \ref{fig:heatmaps_rev_en_fr_bt_mt}, is significantly different, as we inspect high contributions of source to a large number of target sentence tokens in En - Fr. In both directions, we still notice a very heavy reliance of the first target sentence token to all source sentence tokens.

In reduced training data MT experiments, presented in Figure \ref{fig:heatmaps_rev_en_fr_mt_22k}, we observe a high contribution from all source sentence tokens to the first target sentence token, in both training directions. What differs from the full-data MT experiments is that source sentence contributions to all subsequent target sentence tokens are high; this is expected, as with less training data there is higher reliance on the source sentence for tokens' generation. BT-MT experiments results' are seen in Figure  \ref{fig:heatmaps_rev_en_fr_bt_mt_22k}; In En - Fr, patterns are similar, as the contribution from all source sentence tokens to subsequent target sentence tokes is high - highest to the first target token - while in Fr - En only the highest target sentence token shows a heavy reliance on all source sentence tokens.


For En - Gu and Gu - En, in Figures \ref{fig:heatmaps_rev_en_gu_bt}, \ref{fig:heatmaps_rev_en_gu_mt}, \ref{fig:heatmaps_rev_en_gu_bt_mt}, contributions are high from source sentence tokens to subsequent target sentence tokens in all experiments; the small amount of training data makes the model rely more on the entire source sentence for each target token generation. %We observe a higher relevance of certain target tokens to the source sentence, eg tokens 16, 22 in En - Gu MT. 
\end{comment}


\begin{comment}
\include{relevanceNMT/src_tgtk}
\include{relevanceNMT/entropy_tok_figures}

%\include{relevanceNMT/fbd_plots.tex}
\include{relevanceNMT/heatmaps_en_fr.tex}
\include{relevanceNMT/heatmaps_en_gu.tex}


\include{examples_fr.tex}
\include{examples_gu.tex}
\include{examples_perturbed.tex}
\end{comment}
\end{document}
