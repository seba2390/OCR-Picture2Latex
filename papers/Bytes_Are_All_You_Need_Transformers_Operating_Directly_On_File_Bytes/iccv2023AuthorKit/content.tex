\newcommand{\autorefsubfig}[2]{\hyperref[#1]{Figure~\ref*{#1}#2}}

%%%%%%%%% BODY TEXT
\section{Introduction}
%\the\columnwidth % 237.135 points, 3.297 inches
%\the\textwidth % 496.85625 points, 6.90 inches
\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{figures/teaser_v2.pdf}
    \caption{An overview of our ByteFormer (BF) compared to standard inference with DeiT \cite{deit}. (A): File bytes are read from disk and converted to an RGB tensor using a standard image decoder. A patch embedding creates tokens from the RGB representation. (B): File bytes on disk are directly used as tokens, and projected into learned embeddings. (C): Similar to (B), but we apply an obfuscation function $\phi$. (D): We  capture a privacy-preserving representation with a custom camera, and perform token embedding from this representation.
    }
    \label{fig:teaser}
\end{figure*}

Deep learning inference usually involves explicit modeling of the input modality. For example, Vision Transformers (ViTs) \cite{vit} explicitly model the 2D spatial structure of images by encoding image patches into vectors. Similarly, audio inference often involves computing spectral features (such as MFCCs \cite{mfcc}) to pass into a network \cite{ast,bcresnet}. When a user wants to perform inference on a file stored on disk (e.g. a JPEG image file or an MP3 audio file), the user must first decode the file into a modality-specific representation (e.g. an RGB tensor or MFCCs), as in \autorefsubfig{fig:teaser}{a}.

The practice of decoding inputs into a modality-specific representation has two main drawbacks. First, it requires hand-crafting an input representation and a model stem for each input modality. Recent works such as PerceiverIO \cite{perceiverio} and UnifiedIO \cite{unifiedio} have shown that Transformer backbones can be used for a variety of different tasks. However, these methods still require modality-specific input preprocessing. For instance, PerceiverIO decodes image files into $[H\times W, C]$ tensors before passing them into the network. Other modalities input to PerceiverIO are processed into different forms. We hypothesize that it's possible to remove all modality-specific input preprocessing by performing inference directly on file bytes.

The second drawback of decoding inputs into a modality-specific representation is that it reduces privacy by exposing the data being analyzed. Consider the case of a smart-home device that performs inference on RGB images. If an adversary accesses this model input, the user's privacy might be compromised. We hypothesize that inference can instead be performed on privacy-preserving inputs.

To address these drawbacks, we note that a common property of many input modalities is that they can be stored as file bytes. Thus, we use file bytes (without any decoding) as inputs to our model at inference time (\autorefsubfig{fig:teaser}{b}). We use a modified Transformer \cite{transformer} architecture for our model, given their ability to handle a variety of modalities \cite{perceiverio,unifiedio} and variable-length inputs. We call our model ByteFormer.

We demonstrate the efficacy of ByteFormer on ImageNet \cite{imagenet} classification, achieving $77.33\%$ accuracy on files stored in the TIFF format. Our model uses transformer backbone hyperparameters chosen in DeiT-Ti \cite{deit} (which achieves $72.2\%$ accuracy on RGB inputs). We also demonstrate strong results on PNG and JPEG files. Additionally, we demonstrate that our classification model can achieve $95.8\%$ accuracy on Speech Commands v2 \cite{speechcommands}, comparable to state-of-the-art ($98.7\%$) \cite{bcresnet}, \textit{without any architecture changes or hyperparameter tuning}.

Because ByteFormer can handle a variety of input representations, we can also use it to operate on privacy-preserving inputs. We demonstrate that we can remap input byte values using a permutation function $\phi: [0, 255] \to [0, 255]$ (\autorefsubfig{fig:teaser}{c}) to obfuscate inputs without losing accuracy. Although this does not guarantee cryptography-level security, we demonstrate how this method can be used as a building block for obfuscating inputs to a learning system.

Stronger privacy can be obtained by performing inference with ByteFormer on a partially-formed image (\autorefsubfig{fig:teaser}{d}). We demonstrate that ByteFormer is capable of training on images with $90\%$ of the pixels masked while still achieving $71.35\%$ accuracy on ImageNet \cite{imagenet}. ByteFormer does not require information about the specific location of unmasked pixels. Our representation passed to our model maintains privacy by avoiding a standard image capture.

In summary, our contributions are: (1) we develop ByteFormer, a model capable of performing inference on file bytes. (2) We show that ByteFormer achieves strong performance on a variety of image and audio file encodings, without the need for architectural changes or hyperparameter tuning. (3) We demonstrate application of ByteFormer to privacy-preserving inputs. (4) We analyze properties of ByteFormers trained to classify images and audio directly from file bytes. (5) We will release our code at \url{https://github.com/apple/ml-cvnets/tree/main/examples/byteformer}.

\section{Related Work} \label{sec:related-work}
% To our knowledge, we are the first to explore models that directly consume file bytes without requiring modality-specific processing. Related works have unified backbone architectures on a variety of modalities, but still require modality-specific stems. We discuss these and other related works below.

\textbf{Architectures With Multimodal Inputs:} A few methods have explored the idea of feeding different input modalities into the same network for processing. Perceiver IO \cite{perceiverio} demonstrates that a Transformer \cite{transformer} architecture with cross-attention input can be used for a variety of different tasks. Their method differs from ours because their inputs are processed with modality-specific preprocessing. For example, images are loaded into a $[H \times W, C]$ buffer, which differs from the model's treatment of text. By contrast, our method can classify images directly from file bytes. To our knowledge, we are the first to explore models that directly consume file bytes without modality-specific processing.

Other recent works that model multiple modalities with a single model or a single embedding space (but still require input-specific processing) include Unified IO \cite{unifiedio}, CLIP \cite{clip}, and LQAE \cite{lqae}.

\textbf{Alternate Image Input Representations:}
Previous works have explored using alternate input representations for images. In \cite{fasterfromjpeg}, the authors perform partial JPEG decoding, stopping when Discrete Cosine Transform \cite{dct} coefficients are formed. They modify CNNs \cite{resnet} to ingest this new representation. In \cite{rgbnomore}, a similar method is used with Transformers. Our work differs in that we perform no decoding of file bytes at inference time.

\textbf{Privacy-Preserving Inference:} We demonstrate applications of our model to privacy-preserving inference. A few works have examined secure inference in Multi-Party Communication (MPC) settings \cite{securenn,cryptflow,cryptflow2,secure-featurization,orca}. The focus of these works is to perform inference securely on a remote machine using decrypted data on a local machine. We differ from these methods in that our privacy-preserving systems add a layer of privacy to the data on a local machine, and inference is performed locally. Thus, our approach complementary to theirs.

\textbf{Compressive Sensing:} Our privacy-preserving camera is inspired by works in compressive sensing \cite{compressive-sensing}. Related works use image input masking with a single-pixel camera to capture an image over multiple exposures with different masks \cite{single-pixel-imaging,deep-learning-single-pixel}. Instead, we experiment with a single masked capture on a multi-pixel camera.

\section{Overview of Common File Encodings} \label{sec:background}
When performing inference with a standard model, the choice of file encoding is irrelevant. For example, it doesn't matter whether an image is stored as a JPEG or PNG file if it will be decoded into an RGB tensor. By contrast, ByteFormer performs inference on file bytes. In this case, the choice of file encoding matters. This section provides an overview of  common file encodings for images (\autoref{sec:image-encodings}) and audio (\autoref{sec:audio-encodings}). File encoding methods typically contain a large number of optional settings that influence the resulting file bytes. We use default settings provided by {\tt PIL} \cite{pil} or {\tt scipy} \cite{scipy} software packages unless otherwise stated.

\subsection{Image File Encodings} \label{sec:image-encodings}
\textbf{fHWC:} We use fHWC as an abbreviation for ``flattened tensors in height, width, channel order.'' It refers to uint8 image bytes stored in HWC order without any file headers. It is not common to store images in this way, since they cannot be decoded without pre-existing knowledge of their height and width. This serves as a strong baseline.

\textbf{fCHW:} This format is similar to fHWC, but images are stored in ``CHW'' order.

\textbf{TIFF:} The TIFF file encoding \cite{tiff} allows for many custom configurations. For our experimentation, we use the default settings provided by PIL. This results in a format similar to fHWC, but with the addition of TIFF image headers describing configuration options and the image size.
%The TIFF file encoding allows for many custom configurations. Images can be stored in raw form, or in compressed form, leveraging JPEG, LZW, or CCITT compression. Images can be grouped into strips of multiple rows, which are compressed separately from each other. For our experimentation, we focus storing uncompressed RGB values in a single strip. This file encoding is similar to fHWC, but includes image headers which allow standard software to load file bytes into an RGB tensor for visualization.

\textbf{PNG:} The PNG format \cite{png} contains headers describing PNG configuration options, followed by rows of image data stored in ``IDAT'' chunks. Each IDAT chunk contains a byte describing the filtering method used for that row's data. The filtering method applies an offset to the row's data based on neighboring pixel values. Thus, our PNG file contains rows of RGB data, with offsets applied, interrupted by occasional bytes that contain file encoding settings. We do not use the optional {\tt zlib} compression that PNG allows.

\textbf{JPEG:}
JPEG \cite{jpeg} encodes images by applying a series of transformations to compress the image before serialization. The RGB image is converted to YCbCr, then downsampled in the chroma channels, then passed through a Discrete Cosine Transform \cite{dct}, then quantized using coefficients determined by the JPEG quality factor. The quality factor determines the level of compression, with $100$ denoting no compression due to quantization, and lower values indicating stronger compression. After quantization, the coefficients are encoded via a run-length encoding, followed by a Huffman encoding \cite{huffman-encoding}. Note that Huffman codes are not byte-aligned, e.g. they can cross byte boundaries. We expect this to make our modeling task more difficult.

\subsection{Audio File Encodings} \label{sec:audio-encodings}
\textbf{WAV:} The WAV file encoding \cite{wav} stores audio signals represented as a sequence of amplitudes. We use single-channel (mono) audio files. The most common configuration options are the bit depth and the frequency. The bit depth corresponds to the precision with which amplitude values are stored. We experiment with a variety of bit depths, storing audio with 8-bit unsigned integers, 16-bit integers, 32-bit integers, and 32-bit floating-point values. The frequency corresponds to how often amplitude values are chosen. We use 16 kHz, a standard choice for audio \cite{speechcommands}.

\textbf{MP3:} MP3 \cite{mp3} uses a perceptual compression method that removes portions of audio that are difficult for humans to detect. The remaining portions are recorded in frequency space. An MP3 file contains a series of frames. Each frame contains a header with encoding settings, followed by the encoded signal in frequency space. We use standard settings for MP3 provided by the {\tt pydub} \cite{pydub} software package. We expect MP3 encodings to be more difficult to handle than WAV files due to the compression applied.

\section{Methods} \label{sec:methods}
First, we discuss our method for performing inference on file bytes (\autoref{sec:methods-inference}). Then, we discuss how to use our method with image obfuscation techniques to enable privacy-preserving inference (\autoref{sec:methods-obfuscation}). Finally, we discuss how to use our method with a privacy-preserving camera to perform inference without constructing full images (\autoref{sec:methods-privacy-preserving-camera}).

\begin{figure}
    \centering
    \includegraphics{figures/model_arch.pdf}
    \caption{An overview of ByteFormer. We map byte values to learned vectors using a learned token embedding. Next, we apply a Conv1D to reduce the token dimension. Finally, we apply a transformer with shifted window attention and downsampling.}
    \label{fig:model}
\end{figure}

\subsection{Inference on File Bytes} \label{sec:methods-inference}
\subsubsection{Preprocessing} \label{sec:methods-preprocessing}
Some of our file encodings such as TIFF are not frequently used in machine learning datasets. To allow for comparisons on a single dataset across a variety of file encodings, we must re-encode files with different file encodings.

At training time, we decode the file (e.g. read the contents into an RGB tensor in the case of images), then perform standard training augmentation (e.g. random cropping in the case of images), then save the result in the desired file encoding. We find that standard training augmentation is important for model accuracy. Thus, our \textit{training} method is implicitly dependent on the input modality due to our augmentation.

At \textit{inference} time, we do not need knowledge of the input modality. We only need to ensure that our model inputs use the correct file encoding. For example, for TIFF experiments on ImageNet, we precompute $224\times 224$ crops of the validation images and save them in the TIFF format. Such preprocessing is only necessary because the ImageNet validation set is not already stored in the desired format.

\begin{table}[]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule[1.5pt]
         \textbf{Model} & \textbf{Data format}  & $\mathbb{E}[S]$ & $\mathbb{E}[L_t]$ & \textbf{Top-1} \\
         \midrule[1.25pt]
          DeiT-Ti & RGB Tensor & $3 \times 224 \times 224$ & 196 & 72.2 \\
          DeiT-Ti$^\star$ & RGB Tensor & $3 \times 224 \times 224$ & 196 & 74.35 \\
          \midrule
          \multirow{5}{*}{BF-Ti (Ours)} & fHWC & 150528 & 9407 & 77.06 \\
           & fCHW & 150528 & 9407 & 74.65 \\
           & TIFF & 150668 & 9415 & 77.33 \\
           & PNG & 150864 & 9428 & 74.94 \\
           & JPEG & 48564 & 12140 & 65.92 \\
          \bottomrule[1.5pt]
    \end{tabular}
    }
    \caption{ImageNet Top-1 accuracy of ByteFormer Tiny (BF-Ti) using various file encodings, compared to DeiT-Ti. $\mathbb{E}[S]$ denotes the input shape, and $\mathbb{E}[L_t]$ denotes the token length passed to the transformer backbone. ($^\star$) denotes our implementation of DeiT-Ti. We set BF-Ti's Conv1D kernel size to $k=32$ for all experiments except JPEG ($k=8$).}
    \label{table:accuracies}
\end{table}

\subsubsection{ByteFormer}
We describe our ByteFormer model for inference on file bytes. An overview of our model is given in \autoref{fig:model}. The main challenge in using file bytes as inputs is the long sequence lengths. In \autoref{table:accuracies}, we observe that input sizes $\mathbb{E}[S]$ for various file encodings can exceed $150,000$ tokens. As described below, we use strided Conv1D and shifted window attention \cite{swintransformer} to handle long sequence lengths.

The first step of our model is to use a learned token embedding with a vocabulary size of $256$ (corresponding to $2^8$ unique byte values) to produce embeddings. This choice allows our model to handle a variety of input modalities.

The next step of our model is to perform a Conv1D to reduce the sequence length. Our intuition for choosing Conv1D is that neighboring file bytes often contain related information. Reducing our sequence length with Conv1D greatly improves memory usage. In \autoref{table:accuracies}, $\mathbb{E}[L_t]$ denotes the input size to our Transformer, which is significantly smaller than $\mathbb{E}[S]$. Typically, we set our kernel size $k=32$. Our stride is always $k/2$.

Next, we add positional embeddings to the token embeddings, then pass our embeddings to a Transformer. We choose Transformer size parameters to match the $12$-layer DeiT-Ti \cite{deit} architecture with embedding dimension $192$. We call this particular version of our architecture ByteFormer Tiny (BF-Ti). To compensate for our long sequence length ($9417$ for TIFF, compared to $196$ for DeiT-Ti), we use shifted window attention \cite{swintransformer} to limit the attention window size $w$, alleviating the quadratic complexity of attention layers on sequence length. We also add down-sampling layers to halve the sequence length, as in \cite{swintransformer}. We add them after transformer blocks 0, 1, 3, 5, 7, and 9. After passing our tokens through the transformer, we average the embeddings across the sequence dimension.

\subsection{Inference on Obfuscated Inputs} \label{sec:methods-obfuscation}
ByteFormer is designed to perform inference on file encodings without converting them into a standard input representation (e.g. an RGB tensor in the case of images). Therefore, we explore whether ByteFormer can be used for inference on privacy-preserving representations that obfuscate information about the underlying data (\autorefsubfig{fig:teaser}{c}).

Consider a permutation $\phi: \{0, 1, 2, \ldots, 255\} \to \{0, 1, 2, \ldots, 255\}$. Let $\tau$ denote a token embedding, and let $f_\theta$ denote the subsequent transformer. It's easy to see that, for a given $\phi$, there exists a $\tau_{\phi^{-1}}$ such that $\tau_{\phi^{-1}}(\phi(x)) = \tau(x)$. $\tau_{\phi^{-1}}$ is simply a copy of $\tau$ with embedding vectors reassigned to different indices. Thus, $f(\tau_{\phi^{-1}}(\phi(x))) = f(\tau(x))$. The implication of this statement is that our network $f_\theta$ can operate on re-encoded inputs $\phi(x)$ \textit{without requiring any retraining} as long as the network's token embedding $\tau$ is reordered to $\tau_{\phi^{-1}}$.

To take advantage of this property, we choose a permutation $\phi$ at random before training. All training and inference inputs are remapped using $\phi$. We optionally apply uniform noise before applying $\phi$. Without uniform noise, $\phi$ can be applied to a standard ByteFormer without retraining (as explained above). However, we find uniform noise helpful in obfuscating regions of constant color in our experiments.

More generally, we can use more sophisticated methods for altering input representations. As our method can handle highly nonlinear JPEG encodings, we expect it to perform well on a variety of alternative encodings that an outside observer might not be able to easily guess. How secure are such methods against an adversary? This analysis depends on the threat model used. For example, if an adversary has access to a large number of encoded samples $\phi(x)$, analysis of byte statistics might suggest that strings of common bits correspond to patches of blue sky in images. The adversary's task is made more difficult given certain file encodings (e.g. the highly nonlinear JPEG encoding). We do not make strong claims regarding the level of security provided by different choices of $\phi$. Secure systems should be designed and analyzed by security researchers. Instead, we simply suggest that decoupling the input representation from the model can lead to new possibilities for building more secure systems.

\subsection{Privacy-Preserving Camera} \label{sec:methods-privacy-preserving-camera}
We describe another application of ByteFormer to privacy-preserving inference (\autorefsubfig{fig:teaser}{d}). In this scenario, a custom camera captures a non-standard, privacy-preserving representation to allow for inference without building a full RGB image. This custom representation could take a variety of forms. In our experimentation, we consider a hypothetical camera that masks out a large fraction of its pixel channels. The camera stores the remaining unmasked pixel channels in an array without retaining the coordinates of pixel channels on the image sensor. In this scenario, an adversary could not obtain a faithful reconstruction of the input image. Even if the adversary could guess pixel channel locations, the low resolution of captured data prevents the adversary from recovering a high-fidelity image.

\section{Experiments} \label{sec:experiments}
We evaluate ByteFormer on 1000-way classification on ImageNet \cite{imagenet}. We also evaluate 12-way audio keyword classification (including ``background'' and ``unknown'' classes) of 1-second audio clips sampled at $16$ khz using Speech Commands v2 \cite{speechcommands}. For all experiments, ByteFormer's backbone uses hyperparameters that match DeiT-Ti \cite{deit}. We refer to this architecture as BF-Ti.

We train using CVNets \cite{cvnets}. For ImageNet, we use batch size $48$ on 8 NVIDIA A100 GPU machines. At training time, we use random resized cropping, random horizontal flipping, RandAugment \cite{randaugment}, and RandomErase \cite{random-erase} before storing the image in the desired file encoding (\autoref{sec:methods-preprocessing}). We train with AdamW \cite{adamw} with weight decay $0.05$, and a cosine annealing learning rate schedule from $0.001$ to $0.00002$, with $7500$ warmup iterations.

We train our Speech Commands v2 with MixUp \cite{mixup}, noise augmentation, and time shifting augmentation, as in \cite{conv-mixer}. Our training and architecture hyperparameters match our ImageNet experiments. We train these models on 4 NVidia A100 GPU machines.

For ImageNet experiments, we report Top-1 accuracy of models trained with exponential moving average of weights with momentum $0.0001$, which typically increased accuracy by roughly $0.25\%$. For Speech Commands V2, we found EMA to sometimes increase and sometimes decrease accuracy, so we omit it.
\begin{table}
% NOTE: These are EMA numbers.
\centering
\begin{tabular}{ ccccc }
  \toprule[1.5pt]
  $q$ & $w$ & $k$ & $\mathbb{E}[S]$ & \textbf{Top-1} \\ 
  \midrule[1.25pt] 
  100 & 128 & 32 & 48564 & 60.86\\ 
  100 & 128 & 16 & 48564 & 64.86\\ 
  100 & 128 & 8 & 48564 & 65.92 \\
  \midrule
  60 & 128 & 32 & 8436  & 31.8 \\ 
  60 & 128 & 16  & 8436 & 50.11\\
  60 & 128 & 8  & 8436 & 56.26\\ 
  60 & 128 & 4  & 8436 & 62.52 \\ 
  \midrule
  60 & 32 & 32  & 8436 & 37.23\\ 
  60 & 32 & 16  & 8436 & 50.24\\ 
  60 & 32 & 8  & 8436 & 56.74\\
  60 & 32 & 4  & 8436 & 59.52 \\ 
  \bottomrule[1.5pt]
\end{tabular}
\caption{
  ImageNet Top-1 accuracy for ByteFormer Tiny (BF-Ti) for different JPEG quality factors $q$, window sizes $w$, and convolutional kernel sizes $k$. $\mathbb{E}[S]$ denotes the expected shape of the inputs during validation.
}
\label{table:jpeg-wk}
\end{table}
\subsection{ImageNet File Encodings}
\autoref{table:accuracies} summarizes results for a variety of file encodings on the ImageNet dataset. For BF-Ti, we use $w=128$ and $k=32$ for all models except JPEG, for which we find $k=8$ to perform better. Our method surpasses DeiT-Ti accuracies for TIFF, PNG, fCHW, and fHWC encodings.

We find training on JPEG to be more difficult. This is likely due to the highly nonlinear and variable-length JPEG encoding. We investigate the influence of our model's kernel size $k$ on JPEG accuracy in \autoref{table:jpeg-wk}. We find that reducing $k$ from its default value of $32$ increases accuracy. Since JPEG images have a smaller token length than TIFF or PNG, they are likely less compressible. To further explore this, we investigate two settings for JPEG quality factor in \autoref{table:jpeg-wk}. We find that lower quality factors result in lower token lengths, thus reducing $k$ improves accuracy. We also try reducing $w$, but accuracy does not improve.

We present our method's computational efficiency compared to related works in \autoref{sec:performance}.

\begin{table}

\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccccc}
    \toprule[1.5pt]
  \textbf{Model} & \textbf{Input} & $w$ & $k$ & $\mathbb{E}[S]$ & \textbf{Top-1} \\
  \midrule[1.25pt]
  BC-ResNet-8 & log Mel & - & - & $40 \times 98$ & 98.70  \\
  \midrule
  \multirow{2}{*}{BF-Ti (Ours)} & \multirow{2}{*}{W-FP32} & 128 & 32 & 64058 & 95.80  \\
  &  & 128 & 16 & 64058 & 95.51 \\
  \midrule
  \multirow{2}{*}{BF-Ti (Ours)} & \multirow{2}{*}{W-INT32} & 128 & 32 & 64044 & 94.90  \\
  & & 128 & 16 & 64044 & 95.27  \\
  \midrule
  \multirow{3}{*}{BF-Ti (Ours)} & \multirow{3}{*}{W-INT16} & 128 & 32 & 32044& 94.81  \\
   &  & 128 & 16 & 32044 & 95.51 \\
  &  & 128 & 8 & 32044 & 95.13 \\
  \midrule
  \multirow{4}{*}{BF-Ti (Ours)} & \multirow{4}{*}{W-UINT8} & 128 & 32 & 16044 & 92.28 \\
   &  & 128 & 16 & 16044 & 94.39 \\
   &  & 128 & 8 & 16044 & 94.81  \\ 
   &  & 128 & 4 & 16044 & 93.99  \\ 
  \midrule
  \multirow{4}{*}{BF-Ti (Ours)} & \multirow{4}{*}{MP3} & 128 & 8 & 3465 & 88.39  \\ 
   &  & 128 & 4 & 3465 & 88.00  \\ 
   &  & 32 & 8 & 3465 & 88.69  \\ 
   &  & 32 & 4 & 3465 & 89.19 \\
  \bottomrule
\end{tabular}
}
\caption{
  Results for audio classification with BF-Ti on the Speech Commands v2 dataset. ``W-'' denotes WAV files with the given bit width. $\mathbb{E}[S]$ denotes the shape of network inputs.
}
\label{table:sc-accuracies}
\end{table}


\subsection{Speech Commands v2 File Encodings}
Results for audio classification on Speech Commands v2 \cite{speechcommands} are given in \autoref{table:sc-accuracies}. BF-Ti achieves accuracies of up to $95.51\%$ on WAV files, comparable to the state-of-the-art method BC-ResNet-8 \cite{bcresnet}. Note that BC-ResNet-8 is specifically designed for audio processing. By contrast, we performed no parameter tuning relative to our ImageNet training recipe (besides ablating choices of $w$ and $k$). Our best-performing model has $w=128$ and $k=32$. Our model performs best on floating-point values. In this case, since each 32-bit floating-point value in the audio signal will be encoded as 4 file bytes, each audio sample will be represented by 4 neighboring tokens before our Conv1D.

We investigate the influence of $k$ on model accuracy. In general, the optimal $k$ decreases when the expected number of input tokens decreases. This matches our observations in ImageNet JPEG experiments. For MP3 files, we observed that $k=32$ resulted in unstable models due to the drastic reduction in token length. For MP3, we additionally experiment with $w=32$, but it does not improve results.

\begin{table}
\centering
\begin{tabular}{lcc}
    \toprule[1.5pt]
    \multirow{2}{*}{\textbf{Noise level}} &  \multicolumn{2}{c}{\textbf{Model}} \\
    \cmidrule[1.25pt]{2-3}
    & \textbf{DeiT-Ti} & \textbf{BF-Ti} \\
    \midrule[1.25pt]
    None & 51.61 & \textbf{77.39} \\
    $\mathbb{U}[-5, 5]$ & 50.77 & \textbf{77.27} \\
    $\mathbb{U}[-10, 10]$ & 49.50 & \textbf{77.17} \\
    $\mathbb{U}[-20, 20]$ & 43.84 & \textbf{76.31} \\
    \bottomrule[1.5pt]
\end{tabular}
\caption{
ImageNet Top-1 results for obfuscation with $\phi$. We show results with no noise, and with uniform noise in $[-a, a]$ added. We use the fHWC encoding.
}
\label{table:obfuscation-accuracy}
\end{table}

\begin{figure}
    \centering
    \includegraphics{figures/byte_scrambled_images.pdf}
    \caption{
        A sample image from the ImageNet validation set, with uniform noise applied (top row), and with byte remapping $\phi$ additionally applied (bottom row).
    }
    \label{fig:obfuscation-visualization}
\end{figure}

\subsection{Image Obfuscation}
Results for our image obfuscation method (\autoref{sec:methods-obfuscation}) on ImageNet are summarized in \autoref{table:obfuscation-accuracy}. After obtaining our fHWC encoding, we apply a randomly chosen obfuscation function $\phi$.

Examples of obfuscated images are shown in \autoref{fig:obfuscation-visualization}. We observe that byte remapping retains shape information. A region of the image that is dominated by a single pixel value will continue to be dominated by a new (remapped) pixel value. To alleviate this, we add noise from a uniform distribution $\mathbb{U}[-a, a]$ sampled from $-a$ to $a$ (inclusive) to each pixel channel independently, then compute the result modulo $256$. Afterwards, we apply $\phi$. This prevents regions of constant pixel value from being remapped to a single value. As shown in \autoref{fig:obfuscation-visualization}, the upper right corner of the image becomes less recognizable as noise from progressively larger ranges is used. In \autoref{table:obfuscation-accuracy}, we observe that our method is resilient to this transformation, but DeiT is not.

\subsection{Privacy Preserving Camera} \label{sec:experiments-privacy-preserving-camera}
\autoref{table:privacy-preserving-camera-results} summarizes our results for our privacy-preserving camera (\autoref{sec:methods-privacy-preserving-camera}). We emulate the camera setup by masking pixel channels of ImageNet images at random, then storing unmasked pixels in a buffer (in fHWC order) and passing that buffer into our network. For these experiments, we cannot provide DeiT-Ti baselines because DeiT-Ti is not capable of ingesting pixel values without any indication of their placement in the image.

In \autoref{fig:privacy-preserving-camera-visualization}, we show masked inputs before the unmasked pixels are rasterized. At $10\%$ pixel retention, the content of image is hard to visually perceive \textit{even though active pixels are placed side-by-side in a new buffer}. Even if an adversary correctly guessed the positions of unmasked pixel channels in the original image, the adversary could not former a high-fidelity image. As shown in \autoref{table:privacy-preserving-camera-results}, our accuracy at $10\%$ pixel retention is $71.35\%$, comparable to the original DeiT-Ti model operating on non-privacy-preserving (unmasked) images.

Note that this privacy-preserving technique can be combined with the byte remapping technique (\autoref{sec:methods-obfuscation}) to further obfuscate network inputs.

\begin{table}
\centering
%\begin{tabular}{ c | c | c | c | c | c | c }
\begin{tabular}{ ccccccc }
\toprule[1.5pt]
  \textbf{Kept} & 75\% & 50\% & 25\% & 10\% & 5\% & 3\% \\
    \textbf{k} & 32 & 16 & 8 & 4 & 4 & 4 \\
  \textbf{Top-1} & 74.77 & 75.36 & 74.04 & 71.35 & 68.11 & 64.15 \\ 
  \bottomrule
\end{tabular}
\caption{
ImageNet Top-1 accuracy for our privacy-preserving camera experiment with BF-Ti when the given fraction of pixel channels are kept.
}
\label{table:privacy-preserving-camera-results}
\end{table}

\begin{figure}
    \centering
    \includegraphics{figures/mae_images.pdf}
    \caption{
        An ImageNet validation image captured by our hypothetical privacy-preserving camera in which the given fraction of pixel channels are kept. Note that positions of retained pixel channels is discarded by the camera. To make visualization possible, we include the positional information implicitly by placing unmasked pixels in the correct position.
    }
    \label{fig:privacy-preserving-camera-visualization}
\end{figure}

\begin{table}
\centering
\begin{tabular}{ lc }
\toprule[1.5pt]
  \textbf{Attention} & \textbf{Top-1} \\ \midrule[1.25pt]
  Full & OOM \\
  Window & 77.33 \\
  Bag & 75.20 \\
  \bottomrule[1.5pt]
\end{tabular}
\caption{
ImageNet Top-1 accuracy of BF-Ti with different types of attention. We run out of memory with full attention.}
\label{table:attention}
\end{table}

\begin{figure}
    \centering
    \includegraphics{figures/attention_singlecol_v2.pdf}
    \caption{
    Illustration of the types of attention we experiment with. Bag attention is computed in two stages. First, individual bags compute attention. Then, attention is computed across bags.
    }
    \label{fig:attention}
\end{figure}

\begin{table}
\centering
%\begin{tabular}{ c | c }
\begin{tabular}{ lc }
\toprule[1.5pt]
  \textbf{Augmentation} & \textbf{Top-1} \\ 
  \midrule[1.25pt]
  Random Shuffle & 3.06 \\
  Stride & 5.64 \\
  Window Shuffle & 18.14 \\
  Cyclic & 60.97 \\
  Reverse & 61.23 \\
  Baseline & 60.81 \\ 
  \bottomrule[1.5pt]
\end{tabular}
\caption{
  Ablation showing the Top-1 ImageNet accuracy of BF-Ti on JPEG images ($k=32$, quality factor $100$). See text for details.
}
\label{table:ablation}
\end{table}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{figures/token_positional.pdf}
    \caption{
    $|x \cdot y| / (||x|| \cdot ||y||)$ for pairs $x, y$ of token embeddings (top row) and positional embeddings (bottom row) learned by BF-Ti. We show results for various file encodings on ImageNet (IN) and Speech Commands (SC).
    }
    \label{fig:embeddings}
\end{figure*}

\section{Analysis} \label{sec:analysis}

\textbf{Alternate Attention Methods:} We study three state-of-the-art  self-attention methods for handling long sequence lengths in Transformers: (1) full self-attention \cite{transformer, vit, li2022exploring} where each token attends on every other token, (2) shifted window attention \cite{swintransformer,beltagy2020longformer} where tokens are divided into local windows and each local window computes self-attention independently, and (3) bag (or hierarchical) attention \cite{hatnet,chen2022scaling} where tokens are broken up into bags and each bag computes intra-bag self-attention. Inter-bag self-attention is then computed on the resultant output. These different methods are visualized in \autoref{fig:attention} while results on TIFF data are summarized in \autoref{table:attention}. We choose TIFF for these experiments because of its long sequence length (\autoref{table:accuracies}). We find window attention to outperform bag attention. Note that full attention cannot be run due to its $\mathcal{O}(n^2)$ dependence on sequence length $n$. In our main experiments, we used shifted window attention.

\textbf{Effect of Byte Ordering:} To better understand ByteFormer's behavior, we ask, \textit{does ByteFormer simply learn byte frequencies, or is the byte ordering relevant?} In \autoref{table:ablation}, we apply a series of augmentations during training and validation. We focus on the case of JPEG compression at quality factor $100$ with our standard kernel size $k=32$. Each augmentation modifies the byte order of the inputs in some way. In {\tt random shuffle}, we randomly reorder the bytes during training and validation. The order is redrawn every iteration. This severely degrades accuracy. Next, we perform a strided sampling with stride size $1024$ (e.g. $[0, 1024, 2048, \ldots, 1, 1025, 2049, \ldots]$). This slightly improves accuracy over the previous method by improving byte order consistency. Next, we experiment with {\tt window shuffle}, in which the bytes from each window of size $1024$ are consistently permuted. This increases accuracy to $18.14\%$. Next we experiment with a {\tt cyclic} shift in which the second half of the image bytes are moved to the beginning. Accuracy matches the baseline (unaltered JPEG bytes) closely. Similarly, {\tt reverse}, in which the byte order is reversed, preserves locality well and matches the baseline. We find that our model is sensitive to locality, and does not only learn byte frequencies.

\textbf{Learned Token Embeddings:} We study the token embeddings learned by ByteFormer. These embeddings are used to project file bytes into vector representations. In \autoref{fig:embeddings} (top row), we observe the absolute value of the cosine distance $|x \cdot y |/ (||x|| \cdot ||y||)$ between each pair of token embeddings $x$, $y$ on a variety of file encodings. We choose this metric to highlight the difference between (anti-)correlated embeddings (bright patches) and uncorrelated embeddings (dark patches). The pattern varies substantially across input encodings and tasks. In TIFF, PNG, and fCHW, we observe a bright band off of the diagonal, corresponding to high correlation between bytes and their neighbors. This matches our expectations, since replacing a byte with its neighbor only slightly alters the image. This does not hold for JPEG due to the Huffman encoding step. We also observe that the correlation between token embeddings in the float32 encoding of Speech Commands is generally weak. We believe this occurs because the float32 audio amplitude value is split across four bytes in the file encoding, weakening the association between byte values and amplitudes.

\paragraph{Learned position embeddings} We visualize the absolute value of the cosine distance between the first 256 positional embeddings learned by ByteFormer in \autoref{fig:embeddings} (bottom row). For JPEG, we see a strong band of highly uncorrelated values at early positions, corresponding to the file header. Later positions demonstrate interesting patterns that may arise due to the Huffman encodings crossing byte boundaries. In TIFF, a small band of highly uncorrelated values is visible early on, corresponding to the header (which is shorter than in the JPEG case).

\section{Limitations}
The accuracy of ByteFormer depends on the file encoding chosen. As shown in \autoref{sec:experiments}, choosing JPEG over TIFF results in a reduction of accuracy on ImageNet. Adding invariance to file encodings is future work.

As discussed in \autoref{sec:methods-obfuscation}, our choice of $\phi$ for our obfuscation method does not provide cryptography-level security against an attacker with access to a large set of model inputs. We view this method as a building block for security experts to design thoroughly analyzed, secure systems.

Finally, our method has only been evaluated on classification for images and audio. Experimenting with other domains (video, text) and tasks that require fine-grained localization (detection, segmentation) is exciting future work.

\section{Conclusion}
We present ByteFormer, a model that consumes only bytes and does not explicitly model the input modality. We show that it achieves strong performance on image and audio classification without hyperparameter tuning or architecture modifications. We show how ByteFormer can be used in conjunction with image obfuscation techniques with little or no loss in accuracy. We also demonstrate how ByteFormer can be incorporated into a privacy-preserving camera to enable inference without forming a full image at capture time.