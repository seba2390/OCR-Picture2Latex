\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv_rebuttal}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{booktabs}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{comment}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\iccvPaperID{8996} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand*\colourcheck[1]{%
  \expandafter\newcommand\csname #1check\endcsname{\textcolor{#1}{\ding{52}}}%
}
\colourcheck{green}
\colourcheck{yellow}

\newcommand*\colourx[1]{%
  \expandafter\newcommand\csname #1x\endcsname{\textcolor{#1}{\ding{56}}}
}
\colourx{red}
\begin{document}



%%%%%%%%% TITLE - PLEASE UPDATE
%\title{Bytes Are All You Need}  % **** Enter the paper title here

%\maketitle
\thispagestyle{empty}

% TODO encourage JPEG reflections

\textbf{R1,R2,R3 (Baselines+Perf):} Our goal is to learn \emph{directly from bytes without domain-specific modeling} (and demonstrate privacy-preserving applications). This  is not expected to outperform the efficiency/accuracy trade-off of domain-specific modeling. But we agree perf numbers help contextualize performance. Our main comparison was with DeiT-Ti because our backbone hyperparameters follow DeiT-Ti. We add more comparisons (\autoref{table:perf}). BF-Ti delivers competitive performance to Perceiver [14], but with less compute (and it enables privacy). Other methods are uni-modal (image-only), so comparisons are not entirely fair.

For SpeechCommandsv2, we will add KWT-3 \footnote{\url{https://arxiv.org/pdf/2104.00769v3.pdf}}, a state-of-the-art Transformer method (5.36M params, 98.6\% acc). % Perceiver [14] and RGB No More [30] don't use this dataset so we can't compare.

\textbf{R2,R4 (Multimodal):} Our focus is uni-modal privacy-preserving inference. To motivate future work, we try joint image/audio classification (\autoref{table:multimodal}). Without class balancing, image accuracy is unchanged, and audio is slightly reduced. With balancing, audio accuracy recovers.

\textbf{R1 (Additional Modalities):} Our focus is proof-of-concept for modeling bytes directly (with privacy applications). Additional modalities is future work.

% Some experiment results are difficult to understand. For example, in Table.1, why the fCHW format is largely surpassed by the fHWC format? Why BF-Ti can achieve better results compared to DeiT? In Table.4, why DeiT-Ti is so bad in the case without any noise?
\textbf{R1 (Discussion):} \textbf{fHWC vs. fCHW}: The spatial locality imparted by channels-last byte ordering allows full pixel values to appear in the same attention window. \textbf{BF-Ti vs. DeiT}: Our model adds parameters (\autoref{table:perf}). Size and accuracy fall between DeiT-Ti and DeiT-S. \textbf{Table 4}: Obfuscation destroys the patterns that patch-based vision transformers exploit (Fig 3, bottom left).

\textbf{R1 (Cartoon eval of Sec 5.3):} We evaluate ImageNet models on ImageNetSketch (with no fine-tuning). BF-Ti outperforms DeiT-Ti by $+0.76\%$. BF-Ti U[-20, 20] outperforms DeiT-Ti by $+0.67\%$. Thus, our method is competitive with the baseline.
% 20.16\%, BF-Ti: 20.92\%, BF-Ti U[-20, 20]: 20.83\%.

\begin{table}[]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccccc}
        \toprule[1.5pt]
         \textbf{Model} & M & $\mathbb{E}[L_t]$ &\textbf{Top-1} & Sec & P (M) & F (B) & Im/s \\
         \midrule
          MobileNetv3 Large & \redx &  N/A & 75.1 & - & 5.43 & 0.22 & 9615 \\
          ResNet-50 & \redx & N/A & 78.12 & - & 25.55 & 4.017 & 3488 \\
         \midrule[1.25pt]
          DeiT-S p16 & \redx & 196 & 78.62 & 336 & 22.05 & 4.61 & 3594 \\ % n9mki96m4d
          DeiT-Ti p=16 & \redx & 196 & 73.20 & 334 & 5.72 & 1.26 & 6885 \\
          DeiT-Ti p=14 & \redx & 256 & 74.62 & 331 & 5.69 & 1.70 & 4970 \\
          DeiT-Ti p=8  & \redx & 784 & 77.44 & 824 & 5.72 & 7.06 & 1243 \\ % nvpyhg3ggh
          RGB No More DeiT-Ti & \redx & 196 & 75.1 & - & 5.72 & 1.26 & 6885 \\
          \midrule
          Perceiver (learned pos)   & \yellowcheck & N/A  & 67.6 & - & 55.9 & 62.3 & -\\
          Perceiver IO (learned pos) & \yellowcheck & N/A & 72.7 & - & 62.3 & 407 & -\\ 
          Perceiver (conv) & \yellowcheck & N/A & 77.4 & - & 42.1 & 367 & -\\
          Perceiver IO (conv) & \yellowcheck & N/A & 82.1 & - & 48.6 & 369 & -\\
          \midrule
          BF-Ti k=32& \greencheck & 9415 & 77.27 & 1314 & 8.82 & 23.74 & 373 \\ % c5bw66inem
          BF-Ti k=32 -C & \greencheck & 9415 & 74.54 & 1122 & 7.64 & 12.63 & 370 \\ %xafcypsdgj
          BF-Ti k=32 -C -NPE & \greencheck & 9415 & 68.42 & 1121 & 5.83 & 12.63 & 372 \\ % 2fgirt9zmt
          \midrule
          BF-Ti k=4 f0.05        & \greencheck & 3762 & 67.53 & 368 & 6.70 & 5.70 & 1687 \\ % wne342kzhv
   %BF-Ti k=4 f0.05 -C -NE        & \greencheck & 3762 & 45.59 & 351 & 5.83 & 5.14 & 1721 \\ % jj7uzxmcc3
          BF-Ti k=4 f=0.1         & \greencheck & 7524 & 71.26 & 580 & 7.42 & 11.07 & 875 \\ % eisxkgu6pq
          %BF-Ti k=4 f=0.1 -C -NE  & \greencheck & 7524 & 51.01 & 563 & 5.83 & 9.97 & 894 \\ % 7bu99fb78v
          BF-Ti k=8 f=0.25        & \greencheck & 9407 & 73.65 & 769 & 7.93 & 15.40 & 634 \\ % jx4h7rtc8d
          %BF-Ti k=8 f=0.25 -C -NE & \greencheck & 9407 & 58.21 & 732 & 5.83 & 12.63 & 645 \\ % xzuetndyqs 
          \bottomrule[1.5pt]
    \end{tabular}
    }
    \caption{ImageNet Top-1 accuracy. \textbf{M}: whether the model accepts various modalities (\redx: No. \yellowcheck: Yes, but with modality-specific modeling. \greencheck: Yes). \textbf{$\mathbb{E}[L_t]$}: length of token inputs to transformer (after Conv1D for BF-Ti. Note, Perceiver feeds inputs through cross-attention, so this concept doesn't directly apply). \textbf{Sec}: Train epoch time (only reported for models we train to avoid hardware differences). \textbf{P (M)}: Number of parameters (millions). \textbf{F (B)}: Number of flops (billions). \textbf{Im/s}: Throughput (images/sec) on A100 80GB. ``-'' means ``not reported''. For DeiT, $p$ is patch size. $p \le 4$ is unfeasible (epochs took hours or days). For BF-Ti, $k$ is conv kernel size, and $f$ indicates fraction of retained pixels for privacy-preserving camera experiments (Section 5.4). ``-C'' indicates replacement of Conv1D with a windowed mean. ``-NPE'' indicates no positional embedding (ablation).
    \vspace{-0.1cm}
}
    \label{table:perf}
\end{table}

\begin{table}[]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule[1.5pt]
         \textbf{Balanced?} & Finished? & Top-1 & IN Top-1 & SC2 Top-1 \\
         \midrule
          No & Yes & 75.12 & 77.47 & 85.71 \\
          Yes & No (Ep 199/300) & 74.93 & 73.16 & 95.43 \\
          \bottomrule[1.5pt]
    \end{tabular}
    }
    \caption{
    Joint image and audio classification (TIFF and WAV-FP32). \textbf{Top-1}: The top-1 accuracy on 1012-way joint classification (image+audio). \textbf{IN Top-1}: The top-1 on 1000-way image classification. \textbf{SC2 Top-1}: The top-1 on 12-way audio classification. Note, the IN training set is 33x as large as SC2. We optionally add class balancing (replicating Speech Commands v2 by 33x; it's still training).
}
    \label{table:multimodal}
    \vspace{-0.3cm}
\end{table}

\textbf{R2 (Contribution):} Our contribution is not model design, it's training directly on bytes (with privacy applications). Additionally, dealing with massive token lengths (150k, 10x more than LongFormer) required much experimentation. Hyperparameter settings of window attention, placement of downsampling layers, the new addition of Conv1D downsampling, etc. are valuable contributions.

\textbf{R3 (Elaborate on Processing):} All files are stored on disk as a sequence of bytes. We pass these bytes directly to a token embedding layer (e.g. torch.nn.Embedding in PyTorch). See Figure 2. There's no additional processing. % All bytes (headers, RGB values, etc.) are simply passed to the embedding. The network automatically learns patterns directly from the file bytes. Amazingly, our method seems to ignore irrelevant information in file headers (Figure 6).% In short, our approach is similar to language modeling, but file bytes (rather than pieces of works) serve as the tokens.

\textbf{R4 (Ablation: Token Embedding):} Compared to DeiT-Ti, we add token embeddings, but we also remove patch embeddings. Our token embedding (256*192) is the same size as DeiT-Ti's patch embedding (16*16*192), so this change doesn't affect model size (no ablation needed). Instead we ablate removal of the positional embedding (\autoref{table:perf}).

\textbf{R4 (Ablation: Token Length):}
Added DeiT with longer token length (e.g. lower patch size) in \autoref{table:perf}. See also Table 2 and 3 in paper. Halving $k$ results in doubling the input length to the transformer backbone.

\textbf{R4 (Privacy):} (1) If an attacker can access your camera-equipped smarthome device, they can watch a live video feed of your home. In the privacy-preserving camera case (Section 5.4), they cannot capture high-fidelity images (full images aren't captured). You can run on-device models with reduced risk. (2) Data can be stored on secure hardware (for example, a secure enclave\footnote{\url{https://www.theiphonewiki.com/wiki/Secure_Enclave_Processor}}) to protect it even if a system is compromised. If the data needs to be processed outside the enclave (e.g. due to enclave memory limits), it must leave the secure enclave. Obfuscation (Section 5.3) will help disguise it if it's being sent to a model outside the enclave.


% \footnote{\url{https://opaque.co/blog/what-are-secure-enclaves/}} 


\end{document}