\section{Appendix}
\paragraph{Table of Contents}\label{para:toc}
\begin{enumerate}
    \item \nameref{appendix:algo}
    \item \nameref{appendix:imp}
    \item \nameref{appendix:hash}
    \item \nameref{appendix:misc-results}
    \item \nameref{appendix:dipper}
    \item \nameref{appendix:models}
    \item \nameref{appendix:bit-acc-as-metric}
    \item \nameref{appendix:detection-analysis}
    \item \nameref{appendix:max-multi-approx}
    \item \nameref{appendix:other-zwb}
    \item \nameref{appendix:tables}
    \item \nameref{appendix:generation samples}
\end{enumerate}

\subsection{Decoding Algorithm}\label{appendix:algo}
\input{float/algo1}

\clearpage
\subsection{Implementation, Hardware, Code Details}\label{appendix:imp}
We follow \cite{kirchenbauer2023watermark} in most experimental settings. 
For the hashing scheme in the main paper, we use LeftHash scheme with context window $h=1$. In the appendix, we provide results for the SelfHash scheme. For further discussions regarding the hash scheme see Appendix \ref{appendix:hash}.
To generate sequences with the desired token length $T$, we generate with the max token set as $T$. Then we filter out the watermarked and non-watermarked sequences that have token lengths under $T_\text{low}=T-\tau$. We set $\tau$=25, except for the LFQA dataset, which was set to $\tau$=50 as it has instructions that state to generate answers with 200-300 words. For generation, we use sampling with a temperature of 0.7. For each bit-width, a new set of generations had to be made as the length of the message differed. 

We used \texttt{float16} for all our models during generation. Our experiment was run on a single NVIDIA A100. For T=250, generating around 500 watermarked and non-watermarked samples took approximately 200 minutes for the left hash scheme. When using the self-hash scheme, this took significantly longer ($\sim$ 550 minutes). Our implementation is based on the official codebase of \cite{kirchenbauer2023watermark}: \url{https://github.com/jwkirchenbauer/lm-watermarking}. We will be releasing our code to reproduce our experiments.


\subsection{Discussion on the Hashing Scheme}\label{appendix:hash}
The hashing scheme for generating the seed plays a significant role in watermarking. For our MPAC, the hashing scheme is employed once for position allocation and once for permuting the vocabulary list. Here, we discuss some implications of the design choices. 

To recap, the function $f(X_{t-h:t-1})$ is used to hash to $h$ most recent tokens before generating the $t^\text{th}$ token. Following the terminology of \citet{kirchenbauer2023reliability}, LeftHash takes the leftmost token, while SelfHash is determined in a slightly more complex way that is dependent on the $t^\text{th}$ token (see Algorithm 1 of \citet{kirchenbauer2023reliability}). The context width and the hashing scheme determine robustness and quality (diversity) trade-offs. For our experiments, we use the two configurations (LeftHash with $h$=1 and SelfHash with $h$=4) proposed in the previous work found to be effective in the two aspects without further fine-tuning. 

As expected by the trade-off, the perplexity was slightly higher for LeftHash compared to SelfHash (5.1 vs. 4.9 on average for 250 tokens), while P-SP was at the same level. One clear distinction between the two schemes was the encoding time latency. As SelfHash iteratively searches for tokens, this took significantly longer than the LeftHash scheme, which had nearly no overhead compared to no watermarking (\cref{appendix:imp} and Table \ref{tab:mic}). In addition, we observed that the sampled positions were not uniform for LeftHash with $h=1$ as shown in Tab. \ref{tab:hashing-scheme-pos-ratio}) due to the reduced diversity of the tokens in the context width. Despite this, the multi-bit performance was similar for the two schemes (Table \ref{tab:fixedT} and \ref{tab:bpt}). A possible direction for improvement may be using different hashing schemes for position allocation (more robust) and vocabulary partitioning (more quality-focused). 



\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \multicolumn{5}{c}{\textbf{Ratio Sampled Position (Sorted)}} \\ 
        LeftHash ($h$=1) & 0.319	& 0.251 & 0.235 & 0.195 \\
        SelfHash ($h$=4) & 0.264    & 0.257 & 0.242 & 0.238  \\ 
        \bottomrule
    \end{tabular}
    \caption{Ratio of the sampled position for $b$=8,$r$=4 (four positions total) for the two hashing schemes for position allocation. 
}  \label{tab:hashing-scheme-pos-ratio}
\end{table}



\subsection{More Results: List Decoding, Latency, Feedback}\label{appendix:misc-results}
\textbf{List decoding and Latency}
We show absolute accuracy gained using confidence-based list decoding ($|L|$=16) compared with random decoding. We further compare the encoding and decoding latency for sequences with $\sim 250$ tokens using a single Nvidia A100 when using an additive left hash scheme with context width 1. The results are in Table \ref{tab:mic}.The latency \textit{does not} proportionally increase with message bit length, making it scalable to long messages. When using an efficient hashing scheme watermarking has a negligible increase in both encoding and decoding compared to vanilla generation, which requires 7.9 seconds and 0.09 seconds, respectively. 


\begin{wraptable}{R}{0.4\textwidth}
    \begin{tabular}{c|ccc}
    \toprule

    \multicolumn{4}{c}{Bit Accuracy} \\ 
        $\delta$    & 0.5  &  1  &  2 \\
    \toprule
    No feedback & .626  &  .766  & .948  \\  
    $\tilde \delta=\delta$ + 1 & .769 & .860 & .960  \\
    \bottomrule
\end{tabular}
\caption{Results for using feedback for adapting bias on T=100,b=8}
\label{tab:adaptive-bias}
\end{wraptable}

\begin{table}
\centering
\begin{tabular}{c|cccc}
    \toprule

    \multicolumn{5}{c}{Accuracy Gained} \\ 
    &  8b &  16b  &  24b & 32b \\
    \toprule
    Confidence-based list & 1.1\%  &  3.7\%  & 6.0\% & 5.6\%  \\  
    Random list & 0.6\% & 0.4\% & 0.5\% &  0.3\% \\
    \bottomrule
\end{tabular}
\begin{tabular}{c|ccccc}
    \multicolumn{6}{c}{Latency (seconds$\slash$250 tokens )} \\
    & 0b & 8b &  16b  &  24b & 32b \\
    \toprule 
    Encoding (7.9) & 8.19 & 7.98 & 8.01 & 7.96 & 8.24  \\
    Decoding (.09) & .08 & .09 & .09 & .09 & .10 \\
    \bottomrule
\end{tabular}
\caption{Comparison of absolute improvement in bit accuracy when using confidence-based list decoding and random list.}\label{tab:mic}
\end{table}



\noindent \textbf{Message Correction with Feedback}
Here we provide some preliminary results of taking advantage of feedback during message encoding. One simple scheme is adapting the magnitude of the bias so that when the message is not correctly encoded, we enlarge the bias. Concretely, for $0 \leq t \leq T$ that is allocated to position $p$, if the current max colorlist does not match the actual message content, i.e. $\mathbf{m}[p] \neq \text{argmax}_j \mathbf{W}[j]$, we use a larger bias $\tilde \delta > \delta$. The results in Fig. \ref{tab:adaptive-bias} show that all lead to increase in the multi-bit accuracy. However, we observed this came at a degradation in text quality mesaured by automatic metrics. We leave finding better methodology as a future work. 


\begin{figure}
    \centering
    \includegraphics{fig/model-size.pdf}
    \caption{Multi-bit performance across datasets and model sizes.}
    \label{fig:model-dataset-ablation}
\end{figure}




\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \multicolumn{5}{c}{\textbf{Bit Acc. after Paraphrasing with DIPPER}} \\ 
        \multicolumn{1}{c}{Bit-width}  & 8 & 16 & 24 & 32 \\ \hline
        Best Prediction   &  .922 (.13) & .825 (.12) & .778 (.12) & .736 (.10) \\
        16-List Decoded   &  .982 (.05) & .924 (.08) & .864 (.10) & .801 (.09) \\
        \bottomrule
    \end{tabular}
    \caption{Robustness under paraphrasing using DIPPER (Lexical diveristy=20)}  \label{tab:dipper}
\end{table}

\begin{table}[t]
    \centering
    \begin{tabular}{c|c|cccc}
        \toprule
        &  & \multicolumn{4}{c}{DIPPER} \\
        & GPT-3.5 & Lex.=20 & Lex.=40 & Lex.=60 & \makecell{Lex.=60\\Ordering=60} \\
        \toprule
        P-SP  &  .815	&.933	&.897	& .844	&.827 \\
        \small Absolute Change in $\#$ of Words  & 36 & 13 & 16 & 19 & 20\\
        Bit Acc. & .733 &	.922 &	.849 &	.757 &	.719 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of the two paraphrasing method on text quality.}  \label{tab:dipper-semantic}
\end{table}



\begin{figure}
    \centering
    \includegraphics{fig/auc-at-t-cp.pdf}
    \caption{AUC vs. number of tokens observed when corrupted with copy-paste attack for 8-bit message.}
    \label{fig:auc-at-t-cp}
\end{figure}


\subsection{More on Robustness: Other Attacks, Detection}\label{appendix:dipper} 
vWe also test our watermark against DIPPER \citep{krishna2023paraphrasing}, which is a specialized paraphrasing model. DIPPER is parameterized by two scalers, which control lexical diversity and token order diversity. We first present the results across bit-width with lexical diversity of 20 (out of 100). We see that the watermark fares considerably better than using GPT-3 attack in Table \ref{tab:dipper}.  

To see the magnitude of semantic drift of the two paraphrasing methods, we compute the P-SP between the original watermarked text and its paraphrased counterpart. We also compute the absolute change in the number of words. Table \ref{tab:dipper-semantic} demonstrates that paraphrasing using GPT-3.5 changes the semantic and the number of words greater than the setting used in Table \ref{tab:dipper}, which may explain why the multi-bit watermark performance is lower for GPT-3.5. When we control the diversity parameters of DIPPER, this is able to degrade the watermark performance as well as GPT-3.5.

Some other forms of possible attacks considered in the literature are word substitution, insertion, and deletion. Word substition is very similar to the copy-paste attack considered in the main paper. Our watermark scheme is also robust to partial insertion and deletion of words as MPAC relies on the local context to synchronize the positions of the message and ordering of the vocabulary. 

\noindent\textbf{Robustness of zero-bit Watermark} Here we provide results for the detection performance under corrptuion. We use the copy-paste attack with the attack percentage ranges of \{10\%, 20\%, 30\%, 40\%, 50\%\} and compare the AUC vs. number of tokens observed curve similar to Fig. \ref{fig:detection}. While the detectability is noticeably affected, the final AUC is recovered to a large degree only after observing 250 tokens. In order of the attack strength, the final AUC's are .992, .987, ,980, ,971, .942, respectively.  For the zero-bit counterpart, all the scores are over .990.



\subsection{Ablations on Datasets and Model Sizes}\label{appendix:models} 
We show additional results on other datasets and model sizes in Fig. \ref{fig:model-dataset-ablation}. C4 news-like subset is the dataset we used for our main experiment. "Long-form Question-Answering" (LFQA) is a dataset curated by \citet{krishna2023paraphrasing} on the Reddit’s “Explain Like I’m Five” (ELI5) forum. The Essays dataset comprises paris of instructions and essays~\citep{essay}. Wikitext~\citep{merity2016pointer} comprises Wikipedia article. We use the `wikitext-2' subset. For LFQA, we use the finetuned version, LLaMA-2-Chat, specialized for chats as they explicitly have questions or instructions as prompts.

It is apparent that the watermark performance is affected by the text distribution. When the entropy of the vocabulary distribution is low (low diversity), there is little room for encoding the message with a fixed bias, which has been observed in zero-bit watermarking as well where the watermark performance suffers for low entropy text distributions such as coding~\citep{lee2023wrote, kirchenbauer2023reliability}. For our multi-bit case, this means the load capacity is inherently low for such text distributions. This is especially observed for LFQA, in which the model consistently starts the response by restating the question (e.g. \textit{"The reason for [Question] is $\dots$"}). Across the model scale, the trend is not as apparent although we found that the largest model consistently has a lower performance. This hints that the entropy of the vocabulary distribution is lower for the largest model, which might explain the higher text quality in general when we increase the model size. Larger models might have the capacity to form high-quality sequences even when the text distribution is altered by increasing the entropy via temperature or explicitly increasing the magnitude of the bias during watermarking. We leave this as a future work.  



\subsection{Metrics: Bit Accuracy, Text Quality}\label{appendix:bit-acc-as-metric}

\begin{wrapfigure}{R}{.4\textwidth}
    \includegraphics{fig/delta-vs-quality.pdf}
    \caption{Text quality vs. $\delta$ across bias@T=100,b=8}
    \label{fig:delta}
\end{wrapfigure}

\textbf{Text Quality Metrics} Using P-SP, we measure the semantic similarity between the human text and watermarked text given the same prompt. While human evaluation is considered to be the golden label, our main purpose is to show that our multi-bit watermarking does not degrade the quality compared to zero-bit watermarking. Moreover, the effect of watermarking on the text quality \textit{compared to no watermarking} show promising results in human evaluations when sufficiently large models are used for open-ended generation by \citealt{kirchenbauer2023reliability} (Appendix A.2 and A.9). Additionally, \citet{fernandez2023three} also demonstrate that watermarking does not lead to noticeable performance degradation even on benchmarks with non-ambiguous answers such as coding and math especially with sufficiently larger models, albeit at a small bias. We further show in Fig. \ref{fig:delta} the trade-off curve between bit accuracy and text quality. Analysis of text quality shows $\delta=2$ lies at a good trade-off point, and  In \ref{fig:delta}, the size indicates the magnitude of bias ($\{$1, 1.5 2, 3, 4, 5$\}$) and horizontal dashed lines indicate non-watermarked counterparts.




\noindent\textbf{Bit Accuracy for Multi-bit Watermark}
In our experiments, we used bit accuracy (error) as our metric for multi-bit watermark performance. This is a general metric that is independent of the downstream application or the encoding scheme.
However, computing the exact match of a message should be done dependent on the context.
To illustrate this, we start with some examples. First, consider the case where the encoding scheme to identify users is simply assigning a message to each user. Then, by embedding 4-bit message one can encode $2^{4}$ different users : $\mathbf{m}$=`0000' for Bob, $\mathbf{m}$=`0001' for Alice, and so on. For such a scenario, one might be interested in computing the exact match of the 4-bit message, also known as the packet error ratio.  While this encoding scheme enables tracing back to the exact users at low load capacity, this is extremely inflexible as it cannot handle influx or outflux of users. 


Conversely, one can turn to a more flexible encoding scheme by encoding each character. Using UTF-8, this requires 8 bits per character, which would mean 40 bits is required just for encoding 5 character user ID. For this scenario, one might be more interested in computing the packet error ratio of each character or the entire 40-bit message. A more realistic encoding scheme will be somewhere between the middle, which uses a more efficient representation, e.g. by merging often-used bytes as done in Byte pair encoding \citep{gage1994new}. Added with error correction codes such as the Reed-Solomon code~\citep{wicker1999reed}, this allows a more robust representation. Since focusing on a single type of encoding scheme -- and more fundamentally, what information to embed -- narrows down the potential applications, we present bit accuracy in our main experiments as done in previous works in the literature~\citep{zhu2018hidden, luo2020distortion, yang2022tracing, yoo2023robust, fernandez2023stable}. For T=250, the packet error ratio for the 8-bit message was 7.1\%, which is +5.7 \% higher than the bit error rate. With 16-list decoding, this is reduced to 2.4\%. 


Another metric considered in Table III of \citet{fernandez2023three} was combining the detection scheme and packet error ratio. In this scenario, they assume using an encoding scheme of assigning each user to a single message (first example) and computing the percentage of finding the exact user given a fixed false positive rate. At FPR=$1e^\text{-3}$ and using 8-bit message (256 users), we can correctly identify 92.6\% cases. Our true positive rate was computed by the setting used in Table \ref{tab:tpr}.


\begin{table}[t]
    \centering
    \begin{tabular}{c|ccccc}
        \toprule
        \multicolumn{6}{c}{\textbf{True Positive Rate}} \\ 
        \multicolumn{1}{c}{Bit-width} &  0 & 8 & 16 & 24 & 32 \\ \hline
        FPR=$1e^\text{-2}$ & 1 &	0.981 &	0.972&	0.957&	0.960\\
        FPR=$5e^\text{-3}$ & 1	&0.979	&0.958	&0.949	&0.921\\ 
        FPR=$1e^\text{-3}$ & 0.983&	0.975&	0.950	&0.931	&0.907 \\ 
        \bottomrule
    \end{tabular}
    \caption{True positive rate at a fixed false positive rate across bit-widths. We use $\sim 500$ positive sample and $\sim$1000 negative samples, so the minimum FPR threshold is $1e^\text{-3}$
    We only count the unique tokens following \citep{kirchenbauer2023watermark, fernandez2023three}. This has an effect of removing outlier human text samples that has exceptionally high scores.
}  \label{tab:tpr}
\end{table}


\subsection{Analysis on Watermark Detection}\label{appendix:detection-analysis}
Here we further analyze how bit-width of the message and radix affect detection performance. Our analysis stems from the observation that as we increase the bit-width the detection score for the non-watermarked text increases more rapidly than that of the watermarked text. Consequently, the difference in the two scores \textit{decreases} as larger bit-width is used, leading to reduced seperability. The results are in Fig. \ref{fig:z-at-t}. Notice that the difference between the scores of watermarked and non-watermarked texts decreases for larger bit-width.

To grasp a hint of what is going on, we do away with the language model and other complexities by modeling this only through statistical distributions. To recap, our detection statistic (Eq. \ref{eq:zero-bit-detection}) was computed by aggregating the number of tokens in each position of the message. Letting $C_i$ as the number of tokens in the colorlist for the position $i$, we can write the aggregated form as 
\begin{equation}
    C = C_0 + \cdots + C_{p-1} \distas{H_0} \text{Binomial}(T, \gamma)
\end{equation}




However, note that during decoding the ground truth message is unknown and thus, is predicted by taking the colorlist that has the max number of tokens. This is problematic when decoding for non-watermarked text as it biases the statistic to be higher when bit-width is increased. We let $W_i=[w_0, \dots, w_{r-1}]$ be the number of tokens in $r$ colorlists (strength of watermark) for position $i$. For a non-watermarked text, we can assume that this is a random variable with equal probability for each colorlist 
\begin{equation}
    W_i\distas{} \text{Multinomial}(n_i, [\gamma \cdots \gamma])
\end{equation}
where $n_i$ is the number of tokens allocated to position $i$. Our decoding method takes the maximum cell value of this, which makes itself a random variable:
\begin{equation}\label{eq:radix}
    W^\text{max}_i = \mathrm{max}(W_i) =  \mathrm{max}([w_0,\dots,w_{r-1}]).
\end{equation}
Our final statistic used for our detection score is the sum of this variable over the entire positions: 
\begin{equation}\label{eq:pos}
    W^\text{max} = \sum_i^{p} W^\text{max}_i
\end{equation}
We see that our statistic is dependent upon the number of candidates when selecting the maximum cell (i.e. radix) through Eq. \ref{eq:radix} and the number of positions (i.e. bit-width) through Eq. \ref{eq:pos}.


To verify the effect of bit-width and radix on the detection score, we compare the difference in the statistics for a uniform multinomial distribution, which signify non-watermarked text, and a multinomial distribution with a slightly modified probability $[\gamma + \epsilon, \gamma, \dots ,\gamma]$ to signify the added bias term for the watermarked distribution. We sample 1000 samples of $W^\text{max}$ and compute the difference in the detection scores for the two distributions. The results in Fig. \ref{fig:simulation} corroborate that an increase in bit-width / radix decreases the separability of the detection scores. 



\begin{figure}
    \centering
    \includegraphics{fig/z-at-t.pdf}
    \caption{The detection scores of non-watermarked texts, watermarked texts and their difference as a function of number of tokens observed. We see that the difference in the scores decreases as bit-width increases, leading to reduced seperability.}
    \label{fig:z-at-t}
\end{figure}



\begin{figure}[th!]
    \centering
    \includegraphics{fig/simulation-bit.pdf}
    \includegraphics{fig/simulation-radix.pdf}
    \caption{Simulation of the difference between (unormalized) scores for watermarked and non-watermarked multinomial distributions. Higher score signify higher seperability, hence higher detection performance. We use $\epsilon$=0.1. For right, we use $\gamma$=.125 to allow more radix.}\label{fig:simulation}
\end{figure}

In an attempt to overhaul this, we tried computing the likelihood of $W_i^\text{rm}$ before aggregating them using an approximation of \cite{levin1981representation} (More details in the next section). However, this only led to on par or slightly worse performance. This may be because $n_i$ is small for cases when $T$ is small compared to the length of the message. 
Other than this, some of the approaches we attempted were:
\begin{itemize}
    \item Computing test statistic per position or weighting the statistic of each position with $n_i$ before aggregating.
    \item Computing the p-value of the binomial random variables rather than using the normal approximation, i.e. regularized incomplete beta function.
    \item Computing the p-value under the null hypothesis that the distribution of the colorlists follows a uniform distribution, i.e. Chi-square Goodness of Fit test    
\end{itemize}
All the approaches either led to on-par or slightly worse results. 


\subsection{Approximating Max Multinomial Cell Distribution}\label{appendix:max-multi-approx}
We used the approximation of \cite{levin1981representation} for modeling the distribution of the maximum cell frequency. For completeness, we present the steps used for the approximation adapted to our case. For a multinomial distribution with sample size $N$ and probability vectors $[p_0, \dots, p_{r-1}]$, Let $a$ be the maximum cell value, then the cumulative distribution function of having a maximum value of $a$ can be approximated for any real number $s > 0$
\begin{equation}
    P(a) = \frac{N!}{s^{N}e^{-s}}\{\prod_i^{r-1} P(X_i \leq a)\} P(W=N)
\end{equation}
where $X_i\distas{}$Poisson($sp_i$) and $W=\sum_i^{r-1}=Y_i\distas{}$Truncated Poisson($sp_i$) with range $0,1,\dots,a$. Following Example 1 of \cite{levin1981representation}, we set $s=N$ and use Stirling's approximation for $N!$. We also approximate $W$ using the normal approximation to the Poisson distribution.



\subsection{Extension to Other Zero-bit Watermarking}\label{appendix:other-zwb}
\citet{openai-watermark} is another line of work in zero bit watermarking that modifies the sampling process by generating a secret vector $\mathbf{r}\in [0,1]^{|\mathcal{V}|}$ based on the random seed $s$. Given the original probability distribution $\mathbf{p}^{|\mathcal{V}|}$, the token with both large $p_v$ and $\mathbf{r}_v$ is favored by choosing  
\begin{equation}
    x = \text{argmax}_{v\in \mathcal{V}}\mathbf{r}_v^{\sfrac{1}{\mathbf{p}_v}}.
\end{equation}

We can adapt our position allocation method to this as well by preceding the above step with position allocation. Then, the secret key can be modified depending on the message content by the following rule:
\begin{equation}
    \mathbf{r} = 
        \begin{cases}
            \mathbf{r} & \text{if }\mathbf{m}[p] = 0  \\
            \mathbf{1} -\mathbf{r} & \text{if }\mathbf{m}[p] = 1 \\
        \end{cases}
\end{equation}
where $\mathbf{1}$ is a vector with 1 in all the elements. Analogous to favoring mutually exclusive colorlists, this allows favoring different tokens depending on the message content. At decoding time, we can similarly maintain a counter for each position for the two cases.  







\subsection{Tabular Results}\label{appendix:tables}
Here we present the numerical results for the experiments done in the main paper. Numbers in the parenthesis signify the standard deviation. 
\begin{itemize}
    \item Table \ref{tab:delta} $\leftrightarrow$ Figure \ref{fig:delta} show the relationship between $\delta$ vs. text quality and watermark strength.
    \item Table \ref{tab:clean-radix} $\leftrightarrow$ Figure \ref{fig:main-clean} left compare the different configurations of radix and colorlist proportion. 
    \item Table \ref{tab:fixedT} $\leftrightarrow$ Figure \ref{fig:main-clean} left show the multibit watermark performance on a fixed token length.
    \item Table \ref{tab:bpt} $\leftrightarrow$ Figure \ref{fig:main-clean} right show the multibit watermark performance on a fixed load capacity (bits per token). 
    \item Table \ref{tab:robustness-cp} $\leftrightarrow$ Figure \ref{fig:robust}a show the multibit watermark performance under copy-paste corruption. 
    \item Table \ref{tab:robustness-gpt} $\leftrightarrow$ Figure \ref{fig:robust}b show the multibit watermark performance under paraphrasing. 
\end{itemize}



\begin{table}[h]
\small
\centering
\begin{tabular}{c|ccccccc}
\toprule
$\delta$ & 0.5 & 1 & 1.5 & 2 & 3  & 4 & 5 \\\hline
Bit Acc.  & .626 (.19) & .766 (.18) & .887 (.15) & .947 (.11) & .982 (.08) & .993 (.05  & .995 (.05)  \\ 
P-SP (w/ reference) & .385 (.15) & .379 (.15) & .372 (.15) & .371 (.15  & .360 (.14) & .336 (.13) & .319 (.13)  \\
P-SP (w/ non-wm.) & .526 (.18) & .460 (.16) & .433 (.15) & .417 (.15) & .388 (.14) & .349 (.14) & .330 (.13)  \\
PPL & 4.41 (1.5) & 4.64 (1.8) & 5.01 (2.0) & 5.6 (2.0)  & 7.41 (2.7) & 10.3 (4.1) & 13.67 (5.9) \\ 
\bottomrule
\end{tabular}
\caption{Bit accuracy and text quality on embedding 8 bit-width message on T=250 across various magnitudes of bias $\delta$.} \label{tab:delta}
\end{table}


\begin{table}[h]
\centering
% \begin{adjustbox}{width=.5\columnwidth}
\begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Bit Accuracy @ T=250}} \\ 
    \toprule
    Bit &  8  &  16 &  24  &  32 \\
    \toprule
    $\gamma$=.25,$r$=4& .986 (.06) & .951 (.07) & .900 (.09)  & .871 (0.08) \\
    $\gamma$=.25,$r$=2& .966 (.07)  & .905 (.08) & .858 (.08) & 0.820 (.08) \\
    $\gamma$=.50,$r$=2 & .978 (.05)  & .922 (.07) & .875 (.08) & 0.849 (.07)   \\
    \bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Multibit watermark performance measured by bit accuracy for varying configurations of colorlist proportion and radix.}\label{tab:clean-radix}

\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{c|cccc}
    \toprule
    \multicolumn{5}{c}{\textbf{Bit Acc. @ T=250}} \\ 
    \toprule
    Bit &  8  &  16 &  24  &  32 \\
    \toprule
    LeftHash($h=1$) & .986 (0.06) & .951 (.07) & .900 (.09) & .871 (0.08)   \\
    SelfHash($h=4$) & .976 (.08)  & .905 (.08) & .895 (.09) & .862 (.09)    \\ 
    \bottomrule
\end{tabular}
\caption{Bit accuracy for two different hash schemes for a fixed token length.}\label{tab:fixedT}

% \begin{adjustbox}{width=.95\columnwidth}
\begin{tabular}{c|cccccc}
    \toprule
    \multicolumn{6}{c}{\textbf{Bit Acc. @ BPT=.064}} \\ 
    \toprule
    T &  63  & 125 &  250  &  500 &  1000 \\
    Bit & 4 &  8 & 16  & 32 & 64 \\
    \toprule
   LeftHash($h=1$) & .961 (.13)  & .958 (.09)   & .951 (.07)   & .913 (.08) & .846 (.09) \\
   SelfHash($h=4$) & .952 (.13)  & .953 (.10) & .945 (.08) & .911 (.08)  & .850 (.08) \\
    \bottomrule
\end{tabular}
% \end{adjustbox}
\caption{Bit accuracy for two different hash schemes for a fixed bits per token.}
\label{tab:bpt}
\vspace{-1mm}
\end{table}


\begin{table}[ht!]
\centering
\begin{adjustbox}{width=1.\columnwidth}
\begin{tabular}{cc|cccccc}
    \toprule
    \multicolumn{8}{c}{\textbf{Copy-paste Attack}} \\ 
    \multicolumn{2}{c}{Attack Strength} &  Clean  &  10\% &  20\%  &  30\% &  40\% & 50\% \\ \hline
    \multirow{2}{*}{8-bit} & Best & .986 (.06)  & .981 (.07) & 0.971 (.08) & .956 (.10) & .938 (.12) & .900 (.13) \\
     & +16-List & .997 (.02)  & .997 (.02) & .995 (.03)  & .993 (.03) & .991 (.04) & .980 (.05) \\
     \hline 
    \multirow{2}{*}{16-bit} & Best & .951 (.07)  & .939 (.08) & .918 (.09)  & .887 (.09) & .858 (.11) & .819 (.12) \\
     & +16-List & .988 (0.04) & .983 (.04) & .978 (.05)  & .964 (.06) & .947 (.07) & .918 (.08) \\
     \hline
    \multirow{2}{*}{24-bit} & Best & .899 (.09)  & .882 (.09) & .858 (.10)  & .830 (.10) & .797 (.11) & .755 (.11) \\
     & +16-List & .959 (.06)  & .944 (.06) & .927 (.08)  & .907 (.08) & .879 (.09) & .840 (.09) \\
     \hline
    \multirow{2}{*}{32-bit} & Best & .871 (.08)  & .851 (.09) & .828 (.09)  & .801 (.09) & .765 (.09) & .723 (.1)  \\
     & +16-List & .927 (.07)  & .910 (.08) & .888 (.08)  & .863 (.08) & .831 (.09) & .792 (.09) \\
    \bottomrule
\end{tabular}
\end{adjustbox}
\caption{Robustness when certain percentage of human text is mixed into the watermarked text.} \label{tab:robustness-cp}
\end{table}

\begin{table}[h!]
\centering
\begin{adjustbox}{width=0.6\columnwidth}
\begin{tabular}{cc|ccc}
    \toprule
    \multicolumn{5}{c}{\textbf{GPT-3.5 Paraphrasing}} \\ 
    \multicolumn{2}{c}{Token Length} &  250T  &  400T &  500T  \\ \hline
    \multirow{5}{*}{8-bit} 
     & Best & .733 (.19)  & .792 (.19) & .795 (.19)  \\
     & +2-List & .825 (.16)  & .874 (.15) & .875 (.15)   \\
     & +4-List & .856 (.14)  & .894 (.13) & .898 (.13)   \\
     & +8-List & .893 (.12)  & .924 (.11) & .928 (.11)   \\
     & +16-List & .911 (.10)  & .934 (.09) & .939 (.09)   \\
    \bottomrule
\end{tabular}
\end{adjustbox}

\begin{tabular}{p{0.6\textwidth}}
    \textbf{Prompt} \\ 
    “As an expert copy-editor, please rewrite the following text in your own voice while ensuring that the final output contains the same information as the original text and has roughly the same length. Please paraphrase all sentences and do not omit any crucial details. Additionally, please take care to provide any relevant information about public figures, organizations, or other entities mentioned in the text to avoid any potential misunderstandings or biases.” \\
    \bottomrule
\end{tabular}
\caption{Robustness when paraphrased with GPT-3.5.}\label{tab:robustness-gpt}
\end{table}


\clearpage 
\subsection{Generation Samples}\label{appendix:generation samples}

\begin{table}[h]
\centering
\small
\renewcommand*{\arraystretch}{2.0}
\begin{tabular}{p{.33\textwidth}|p{.3\textwidth}|p{.3\textwidth}}
\toprule
\multicolumn{1}{c}{Prompt} & \multicolumn{1}{c}{Non-Watermarked} & \multicolumn{1}{c}{Watermarked}  \\\midrule
A blind man in his 60s is searching for the young man who pulled him back from an approaching train and saved his life last Thursday, Nov. 9.
Mike Wyatt stood at the stairs to the Peoria train station in Aurora, Colorado, on Monday, looking for the young man who saved his life days earlier 
& 
``He told me he was 20 years old and he was going to college," Wyatt told reporters. ``He said he was going to be a doctor."
Wyatt, who is blind and has been for most of his life, was at the train station ready to catch a train to the airport when he slipped and almost fell in front of an oncoming train.[...]
& 
and has given him new life.
Inside the station, he spoke to the counter clerk, hoping the clerk might have information that might lead him to the young man, who jumped at the chance to help Wyatt when he saw he was in danger. But, he said, he found nothing.[...]
\\\hline
[...]
Shortly after the release of the lead single “Life’s About to Get Good,” it hit number one on the Billboard 200. Of course, fans were simply dying to see how their favourite country icon was doing, so
 &
 the album was released on CD, vinyl, and digital. But, Shania took things to the next level and released a deluxe version with added tracks. [...]
 &
 the song was released a few months earlier in June.
``OK so I am sooooo proud that my song Life's About to Get Good is now the number one song on Billboard country,” she tweeted back in June.[...]
 \\\hline
 [..] Amazon’s offices will be located within an already-established special tax district where a portion of the property tax revenue goes toward infrastructure improvements such as parks and wider sidewalks.
The incentive agreement says that half of any new revenue from that district starting in 202
 &
 3 would go to Amazon to help pay for employee benefits and transportation, but the county has said it would not be required to pay those benefits.[...]
 &
 7 would go to Amazon through a 15-year payment schedule. This amount would grow to more than 60 percent if the company exceeds 10.24 million square feet of office space. Amazon is expected to reach that level in 2027.[...] \\\hline

[...]
"Below the surface of the Sun a dynamo process is working creating magnetic field," Harra explained in an email interview. "When this becomes buoyant it can rise to the surface of the Sun,
&
and this is what we call a sunspot."
The new research shows that the initial explosion was actually a series of smaller blasts that combined to create the X-7 flare. 
&
and form coronal holes and sunspots. This happens in a fairly homogeneous area on the Sun, so there can be several sunspots with a single magnetic field underneath. \\\hline

[...]The merge listing the most important changes to Linux 3.8's sound subsystem includes some other changes to audio drivers.
The kernel now includes a driver for human interface devices (HIDs) that use I2C (1, 2 and others), using the "HID over I2C" protocol designed by Microsoft and implemented in WindowsÂ 
&
 7 and later versions of the operating system.
The kernel now has a driver for the Samsung Galaxy S III smartphone's touchscreen (1, 2 and others), and the rt2800usb driver, for the RaLink RT2800USB WLAN chip, now supports devices that have Bluetooth 3.0 (1, 2).[...]
&
 7 and Windows Vista. The drivers can read out data from HIDs and set the appropriate commands to them. An example of such a device is a BT-USB adapter.
The sound subsystem now supports two new, high-quality audio codecs (1, 2):[...] \\ 

\bottomrule
\end{tabular}
\vspace{1em}
\caption{Randomly sampled examples of watermarked texts on the C4 newslike subset with 100\% bit accuracy. Samples are truncated for readability.}
\label{table:prompts}
\end{table}

