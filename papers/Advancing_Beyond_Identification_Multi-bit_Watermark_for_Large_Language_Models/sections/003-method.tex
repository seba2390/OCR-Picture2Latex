

% Method 
We briefly review zero-bit watermarking introduced by \citet{kirchenbauer2023watermark} and elaborate on extending this method to multi-bit watermarking. Then, we analyze our framework from the lens of coding theory and introduce additional techniques for improving the watermark performance. 
\subsection{Zero-bit Watermarking \citep{kirchenbauer2023watermark}}\label{subsec:zwb}
A watermark is embedded by biasing the language model to output a certain subset of tokens. 
Given an autoregressive language model that predicts the next token with vocabulary $V$,
a subset of tokens is randomly selected from the vocabulary at each token step $t$ and forms a green list $\mathcal{G}_t$. The logit scores  $l_t \in \mathbb{R}^{|V|}$ are modified towards selecting the green-listed tokens in favor of the other tokens by adding a bias term $\delta$ to the logits in $\mathcal{G}_t$. Instead of fixing the greenlist using rule-based heuristics such as spelling or synonym variations \citep{he2022protecting}, the greenlist is selected (pseudo-)randomly at each time step to minimize a noticeable shift in text distributions. At each time step, a seed $s$ is outputted depending on the previous $h$ tokens using a pseudo-random function $f: \mathbb{N}^h \rightarrow \mathbb{N}$, and $s$ is used to sample $\mathcal{G}_t$ from $V$. 

At decoding, the greenlist can be recovered by using the same pesudo-random function $f$. The presence of a watermark is determined by counting the number of tokens in the greenlist. For a human-generated text that has no knowledge of the greenlist rule, a token will be from the greenlist with the probability $\gamma$, the proportion of the greenlist size compared to the entire vocabulary. Without the knowledge of the greenlist (null hypothesis), the number of tokens in the greenlist ($g$) follows a binomial distribution. \citep{kirchenbauer2023watermark} used the normal approximation to the binomial distribution to compute the $z$-statistics for a text with $T$ tokens: $z = \frac{g - \gamma T}{\sqrt{\gamma(1-\gamma)T}}$. 


\subsection{MPAC: Extending to Multi-bit Watermark}
\label{subsec:multibit}
We first present an overview of our method and further elaborate on the details in the subsequent section. The objective of multi-bit watermarking is to embed and extract a message $\mathbf{m} \in \Sigma^b$ where $\Sigma$ denotes the $r$-nary possible strings, or more commonly referred to as the alphabet. For a binary message, $\Sigma=\{0,1\}$.  We let $p \in \{0, \dots, b-1\}$ denote the position of the message and $\mathbf{m}[p] \in \{0, \dots, r-1\}$ the message content at that position. Hereafter, we use $[a]$ to denote the integer set $\{0, \dots, a-1\}$. 

Our proposed method \textbf{M}ulti-bit watermarking via \textbf{P}osition \textbf{A}llo\textbf{c}ation (MPAC) works by partitioning the tokens to message positions and enlarging the size of the alphabet through color-listing. First, notice that zero-bit watermarking can be viewed as watermarking a single bit of information stating the existence of a watermark ($\mathbf{m}$=0). In essence, each token generated by the language model is a signal in favor of the watermark (See Fig.~\ref{fig:multi-bit}-Right).


\begin{figure}[t]
    \vspace{-3mm}
    \centering
    \includegraphics[width=\textwidth]{fig/overview.png}
    \caption{An overview of our method MPAC. The number inside a token (e.g. $\boxed{p=1}$) denotes the allocated position, while the color signifies the message content at that position. At Step 1, a position is sampled prior to generating a token. Then at Step 2, the message at that position determines the token subsets to favor. \textit{Right:} Zero-bit watermarking can be viewed as a special case of multi-bit watermarking.} 
    \label{fig:multi-bit}
\vspace{-4mm}
\end{figure}

\noindent\textbf{Message Encoding}
In MPAC, we allocate the signals (tokens) into multiple positions. For instance, when the message content at a position is `0', we sample from the greenlist, while doing otherwise when the message is `1'. This allows encoding multi-bit messages of arbitrary length as long as the language model generates sufficient tokens. To further increase the bit capacity, we color the vocabulary set with $r$ ``colors" instead of using a single greenlist, increasing the alphabet to $\Sigma=[r]$. %\{0,\dots,r-1\}$. 
The number of colors can be determined by the greenlist proportion $\gamma$, i.e. $r = \lfloor \frac{1}{\gamma} \rfloor$. Thus, this allows encoding $r$ states into each token as opposed to encoding a binary state (whether the token is selected from the greenlist or not). Given a binary message of length $b$, the message is convereted to radix $r$ attaining $\mathbf{m}_r \in [r]^{\tilde b}$ where $\tilde b=\lceil \frac{b}{\log_2{r}} \rceil $. 
In Figure \ref{fig:multi-bit} Left, we illustrate the case of $r=4$ and $b=8$, where the 8-bit message is converted into radix $4$, resulting in an effective message length of 4 ($\tilde b=4$)\footnote{Hereafter, we use $b$ instead of $\tilde b$ to denote the effective message length (dimension of $\mathbf{m}_r$).}. At each token generation, the message content at the assigned position $p$ determines which colorlist to add $\delta$ to. If the message content is `0', the tokens from the first list (red in Fig. \ref{fig:multi-bit}) are favored. We discuss the design choices for allocating the positions in the next section.

Our method is extremely easy to implement over the zero-bit watermarking scheme. We highlight the steps in colors that are specific to ours. Given $t-1$ prefix tokens $X_{1:t-1}$, bit message $\mathbf{m}$, its r-radix converted  form $\mathbf{m}_r$, and pseudo-random function $f$, the $t^\text{th}$ token is generated by 







\begin{enumerate}[noitemsep]
    \item Compute hash of tokens $s=f(X_{t-h:t-1})$. Use $s$ as a seed for a random number generator.
    \item \hlc[cyan!40]{$p \leftarrow \texttt{sample}([b])$} \textcolor{blue}{\qquad \# $p$ is the position of the message.}
    \item \hlc[cyan!40]{$m \leftarrow \mathbf{m}_r[p]$} \textcolor{blue}{\qquad\qquad \ \ \# $m$ is the message content at position $p$.}
    \item Permute vocabulary $\mathcal{V}_t$ using $s$ as seed.
    \item \hlc[cyan!40]{Partition $\mathcal{V}_t=[\mathcal{C}_t^{0}, \cdots , \mathcal{C}_t^{r-1}]$ discarding remainders if any.}
    \item Add $\delta$ to token logits in $\mathcal{C}_t^{m}$.
\end{enumerate}

\noindent Note that zero-bit watermarking can be seen as a special case of embedding the same single bit message ($b=1$, $r=2$, and $\mathbf{m}=0$) as shown in Figure \ref{fig:multi-bit}-Right. 

\noindent \textbf{Message Decoding}
Given a watermarked language model output, we determine the position and which colorlist each token is from and increment the number of tokens in the colored lists. For instance, for the $t^\text{th}$ token with message position $p=i$ and the $j^{\text{th}}$ colorlist $\mathcal{C}_t^j$, we increment the counter $\mathbf{W}_i[j]$. After computing this on the entire text segment, we predict the message content by taking the colorlist with the most tokens for each position. A more detailed algorithm is shown in Algorithm \ref{alg:extraction} in Appendix \ref{appendix:algo}. 

MPAC encodes and decodes each bit position of the message independently, which brings a negligible increase in the computation as the message length is increased. This is in contrast with recent works~\citep{wang2023towards, fernandez2023three}\footnote{See Section 7.5 of \citealp{wang2023towards}
} that have to compute the likelihood of all the possible messages during decoding, which makes the embedding of long-length messages infeasible due to exponentially growing ($\bigO(2^b)$) computations during decoding without additional techniques to avoid this. In contrast, our algorithm can encode 64-bit messages ($2^{64}$ messages) as will be shown in the subsequent section.



\subsection{Design Choices through the Lens of Coding Theory}
Having set out the encoding and decoding algorithm of MPAC, we elaborate on the design choices and analyze what factors affect the performance using basic notions from coding theory adapted from \citet{cover1999elements}:  
\begin{itemize}
    \item Encoding function is a function $E: \mathcal{M} \rightarrow \mathcal{X}$  that maps the original message into longer, usually redundant string where $\mathcal{M} \subseteq [r]^b, \mathcal{X} \subseteq \Sigma^T$. The rate of $E$ is given by $\frac{b}{T} \log_2 r$ bits/symbol.
    \item  $p(y|x)$ is a noisy channel that models the transmission of the encoded message. 
    \item A channel's capacity is the upper bound of the rate of an encoding function in order for a reliable transmission.
   \item Decoding function is a function $D: \mathcal{Y} \rightarrow \mathcal{M}$ that recovers the original message from $y$.
\end{itemize}
We first simplify our setting to embedding a single-digit message ($b=1$), which does not lead to a loss of generality as MPAC encodes each position independently.  As noted earlier, each token of a language model is a signal for embedding the message ($m$) by repetitively sampling from the $m^{\text{th}}$ colorlist. Therefore, in MPAC our \textbf{encoding} function is a repetition code that maps a redundant message content $T$ (number of tokens) times. Our \textbf{channel} is the generation process of the language model, which stochastically transmits the encoded message by sampling from the vocabulary distribution that has been modified to favor the selected colorlist. The success probability of each transmission depends on the magnitude of the bias $\delta$, the entropy of the vocabulary distribution, and, more holistically, the text distribution. The \textbf{decoding} function is the rule set out in Sec.~\cref{subsec:multibit}, whereby the argmax of the colorlist is predicted as the message content, i.e. majority voting.  

% \begin{wrapfigure}{R}{.4\textwidth}
%     \includegraphics[width=.38\textwidth]{fig/position.pdf}
%     \subcaption{}\label{fig:position}
%     \includegraphics[width=.38\textwidth]{fig/snr.pdf}
%     \subcaption{}\label{fig:snr}
%     \caption{(a) Comparing robustness of the watermark (Clean bit error - corrupted bit error) for deterministic and pseudo-random position sampling schemes at T=250. (b) Relationship between bit accuracy and SNR for a fixed BPT.}
% \end{wrapfigure}


\noindent \textbf{Position Allocating}
Coming back to our original multi-bit message setting, the rate $\frac{b}{T} \log_2 r$ signifies that having more tokens leads to increased strength of the signal (i.e. lower rate) as more tokens are assigned to each position. Ideally, allocating tokens to each position should (1) equally favor the positions and (2) be robust to potential corruptions in the watermarked text. The first criterion can be easily achieved through a rule-based scheme such as sequentially allocating positions for every $k$ token or deterministically cycling through the positions. 
%The former is restricted to a scenario where the generator knows the number of total tokens in advance in order for it to uniformly allocate the signals. 
However, while these schemes may effectively retain the positions when the generated text is untouched, even a single insertion or deletion of a word will lead to burst errors during decoding. This makes them extremely fragile under realistic use cases where users paraphrase or edit the generated texts. 

\begin{figure}
\begin{minipage}[t]{.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig/position.pdf}
    \subcaption{}\label{fig:position}
\end{minipage}\hfill
\begin{minipage}[t]{.49\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{fig/snr.pdf}
    \subcaption{}\label{fig:snr}
\end{minipage}
\caption{(a) Comparing robustness of the watermark (Clean bit error - corrupted bit error) for deterministic (cyclic) and pseudo-random position sampling schemes at T=250. (b) Relationship between bit accuracy and SNR for a fixed BPT.}
%\label{fig:snr}
\end{figure}

%since hashing sufficiently large n-gram tokens guarantees diversity, hence uniformly sampling the positions in expectation. 
To remedy this, we use the hashing scheme that was used for permuting the colorlists: At each token step, we sample $p \in [b]$ seeded with $s$ that was generated from the pseudo-random function $f$. This allows enjoying the relative robustness of the hashing scheme towards attacks that alter the total length (e.g. paraphrasing) or mixing snippets of human text onto the watermarked text. This is illustrated in Fig. \ref{fig:position}: when 20\% of human texts are mixed into the watermarked texts, rule-based position allocation (cycle) almost falls to near-random chance (61\%, 55\%, 54\%, and 51\% across the bit-widths), while sampling positions via hashing maintains the watermark. 

At this point, one may assume that the ratio between the number of tokens and bit-width ($\frac{T}{b}$) determines the channel's capacity. We show in Fig. \ref{fig:snr} that this is not necessarily the case: The bit error rate increases as longer messages are embedded despite the same bits per token (BPT). This can be explained by the well-known theorem in channel coding that the noise of the signal~\citep{shannon1948mathematical} also affects the channel capacity. Signal-to-noise ($SNR=\frac{\mu}{\sigma}$) precisely measures this quantity where we define $\mu$ as the mean number of tokens per position and $\sigma$ as the standard deviation. Modeling this as a uniform multinomial distribution $P\distas{} \text{Multinomial}(T, [\frac{1}{b} \cdots \frac{1}{b}])$ for simplicity, we can get a rough estimate of SNR. Increasing both $b$ and $T$ at the same rate maintains $\mu = T/b$, but increases $\sigma = \sqrt{T(b-1)}/b$. Fig. \ref{fig:snr} displays the theoretical SNR (= $\sqrt{T/(b-1)}$, which better explains the empirical bit error rate. %Partitioning the vocabulary into colored lists at a given $\gamma$ has the effect of increasing SNR by a factor of $\log_2(r)$\footnote{One can easily show using the definitions of multinomial distribution that $\frac{\mu}{\sigma}\propto\frac{1}{\sqrt(b)}$}.




\noindent \textbf{List Decoding}
The linear nature with respect to the message length allows MPAC to output a \textit{list} of most likely messages without exhaustively considering all the possible messages. List decoding is a well-established field in coding theory that decodes a list of messages that are within a certain hamming distance~\citep{elias1991error, guruswami2008explicit, guruswami2004list}.
Inspired by this, we alter our decoding function to output candidate messages sorted by the level of confidence. Denoting the predicted message for position $i$ by $\hat{m}$, and the observed number of tokens in the colored list (strength of the watermark) by $w=\mathbf{W}_{i}[\hat{m}]$, the confidence of $\hat{m}$ should be higher if $w$ deviates from the expected mean under the null hypothesis that all colored lists are equally likely to be sampled. We define confidence at position $i$ as $c_i \propto {\text{Pr}(W^{\text{max}}_{i} \leq w | H_0)}$ where $W^\text{max}_i$ is the maximum cell value of $W_i\distas{H_0}\text{Multinomial}(T_i, [\gamma \cdots \gamma])$ where $T_i$ is the number of tokens assigned to position $i$. The distribution of $W^\text{max}_i$ is approximated using techniques from \citet{levin1981representation} (See Appendix \ref{appendix:max-multi-approx}).

Our algorithm can be parameterized by the confidence bound on each position:
\vspace{-1mm}
{\setlength{\leftmargini}{.5cm}   
\begin{itemize}
    \item Input: Best prediction $\hat{\mathbf{m}}$ found by majority voting via Alg. \ref{alg:extraction}, confidence bound $c_0$ 
    \vspace{-1mm}
    \item Output: $\hat{\mathbf{m}}_1,\cdots, \hat{\mathbf{m}}_{|\mathbb{L}|} \in \mathbb{L}$ whose predictions are altered for the positions with confidence under $c_0$ 
    \end{itemize}
}

Empirically, we determine $c_0$ by constraining $|\mathbb{L}|$. Note that since $\hat{\textbf{m}}$ is always the most confident message, we comprise $\mathbb{L}$ with the next confident messages. To do this, we greedily alter the positions with the lowest confidence to the colorlist with the second largest number of tokens. Note that this list decoding technique is not unique to ours and can be applied to other methods as long as the decoding stage is computationally feasible.

\noindent \textbf{Error Correcting with Feedback}
One key characteristic of our $p(y|x)$ is that we can instantly check whether the message was correctly transmitted by examining whether the sampled token is in the correct colorlist. This property resembles the settings of error correcting codes with feedback, in which the receiver can send feedback to the sender after receiving the message\citep{berlekamp1964block, gupta2023binary}. One can take advantage of this property by adapting the magnitude of the bias during encoding when the majority vote of a given position differs from the actual message. We show some preliminary results in Appendix \ref{appendix:misc-results}.  

We believe the simplicity of our multi-bit watermark scheme via position allocation makes it generalizable to other zero-bit watermark approaches. An example is provided in Appendix \ref{appendix:other-zwb}.


\subsection{Detecting Machine Text}\label{subsec:detection}
While we can use MPAC to decode the multi-bit watermark in conjunction with another detection mechanism, MPAC alone can detect human text from watermarked text just like zero-bit watermarking. The strength of our watermark can be determined by taking the maximum cell frequency of each position, which is modeled by the confidence $c_i$ at each position. However, we found that simply modeling the number of tokens in the argmax colorlist of position $i$ as a random variable $C_i \distas{H_0} \text{Binomial}(T_i, \gamma$) led to slightly better results where $T_i$ is the number of tokens assigned to position $i$. 
As $C_0, \dots, C_{b-1}$ are independent for a fixed set of trials ($T_i, \dots, T_{b-1}$) and have the same success probability parameter, the sum of these is a binomial random variable as well:
\begin{equation}\label{eq:zero-bit-detection}
    C = C_0 + \cdots + C_{b-1} \distas{H_0} \text{Binomial}(T, \gamma)
\end{equation}
where $T=T_{0}+\cdots+T_{b-1}$. This reduces to the same random variable used in zero-bit watermarking and we can compute the z-statistics from \cref{subsec:zwb}. 
Computing the total number of color-listed tokens is shown in Line 12 of Alg. \ref{alg:extraction}. More discussion regarding other possible statistics is outlined in the last section of Appendix \ref{appendix:detection-analysis}. 


