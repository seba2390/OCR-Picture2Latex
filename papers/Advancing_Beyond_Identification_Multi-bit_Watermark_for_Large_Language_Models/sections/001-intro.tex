% Intro
 How can we take a step further from merely identifying machine-generated text to proactively tackling misuses of large language models? The emergence of human-like language models has necessitated the development of various methods to detect machine-generated texts through techniques such as zero-shot detection, supervised training, watermarking, and more \citep{mitchell2023detectgpt, wang2023m4, kirchenbauer2023watermark, krishna2023paraphrasing}. 
These endeavors focus on the crucial task of \textit{identifying} machine-generated content, which serves as a pivotal step in mitigating the potential harm caused by such text. An illustrative instance where detection can play a significant role is in curbing students' academic misconduct, particularly in cheating on assignments within educational institutions. Properly identifying machine text can mitigate the negative impact on students' learning experiences by creating an environment that guarantees a fair assessment of assignments. While the potential harm is real, such misconduct does not call for more drastic measures such as legal actions against the adversary user. 


However, when it comes to more pernicious misuses of large language models, such as the dissemination of misinformation and propaganda on social media platforms, the stakes are considerably higher, potentially leading to the erosion of social trust~\citep{valenzuela2022downward}. 
Notable instances that exploited automated bots in the past include manipulating an election campaign~\citep{Badway2018}, spreading disinformation about the Russian invasion of Ukraine~\citep{pierri2023propaganda}, and promoting products through fake reviews~\citep{amazon-reviews}. With the rapid pace at which large language models are currently developed, similar threats will be automated in a much more rapid and delicate manner in the future. 

In these scenarios, merely identifying the machine-generated text may not suffice for the language model providers. Instead, the ability to trace back to the adversary user responsible for generating the content becomes pivotal in counteracting such misuses. By doing so, the API providers can take a precursory measure to ban these users from their systems. More importantly, this allows media and social platforms, along with API providers, to collaborate with law enforcement authorities and take more decisive actions. All in all,  watermarking the user information (or part thereof) can hold the adversary user accountable for potential harms facilitated through language model APIs without having to store user queries~\citep{krishna2023paraphrasing}, which would be prohibitively expensive and concern ordinary users who value privacy. Additionally, watermarking can enable language model providers to bind meta-data (e.g. model versions) for tracing the provenance of the language model output.


All this can be achieved by embedding multi-bit information. In this study, we demonstrate that this can be realized on top of the recently proposed zero-bit watermarking method~\citep{kirchenbauer2023watermark} in an extremely simple way without sacrificing the text quality. Zero-bit watermarking works by randomly favoring a ``greenlist" of tokens at each generation. The selection of a greenlist from the vocabulary set is determined by a random seed generated by a pseudo-random function. 

Our proposed method called \textbf{M}ulti-bit watermark via \textbf{P}osition \textbf{A}llo\textbf{c}ation (MPAC) first allocates each token pseudo-randomly onto a single position of the message to be embedded. Then the message content at the allocated position decides which subset of tokens to favor. We show that the zero bit watermarking method can be viewed as a special case of encoding the same single bit message. To increase load capacity, we can further partition the vocabulary into multiple ``colored" lists instead of a single green list, effectively encoding multiple states for every token. By formulating our method through coding theory, we analyze what factors affect the performance of our method and devise techniques for improvement. 



Since our method works on top of zero-bit watermarking, it leverages most of the advantages: (1) Multi-bit message can be extracted without access to the model parameters or the API, allowing other parties to extract the adversary information (e.g. timestamp, ID) if given access to the extraction algorithm. (2) It can be done on the fly without pretraining or finetuning the model and can embed and extract long messages ($\geq$ 32-bit) with negligible overhead. (3) The watermark is not fragile against realistic corruptions such as interleaving with human texts and paraphrasing, while maintaining the same text quality as its zero-bit counterpart. (4) Finally, our watermarking framework can distinguish between machine and human text while simultaneously embedding multi-bit information. Our experiments demonstrate that 8-bit messages can be embedded effectively in short text lengths ($\sim$100 tokens) with over 90\% bit accuracy. For text segments with $\sim$250 and $\sim$500 tokens, we can extract 16-bit and 32-bit messages, respectively, with over 90\% accuracy, demonstrating its real-world applicability in embedding information for tracing adversary users. 