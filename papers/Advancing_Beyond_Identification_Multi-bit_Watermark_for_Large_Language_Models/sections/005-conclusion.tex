\vspace{-2mm}
Our findings underscore the feasibility of embedding any information into the outputs of language models while having the capability to distinguish between machine text and human text. This unveils a novel prospect of counteracting high-stake misuse of large language models via API. Furthermore, our analysis rooted in coding theory opens up other avenues for technical improvements such as using feedback or fusing error correction codes into MPAC. One limitation of our approach is the reduced separability of machine and human text when embedding longer messages. Overhauling this limitation can be a major step towards deploying multi-bit watermark in the real world.


\section{Ethics Statement}
Watermarking is one of the technologies that can mitigate malicious use cases by being able to trace back to the malicious user. However, ordinary users may find the idea discomforting as it may give the sense that the API provider can know what outputs are fed to the individual users . This is not the case unless the content is published to the public by the user, which -- in many cases -- is already done in an environment where the user can be identified (e.g. social media). This is in contrast to storing individual users' queries in a database. All in all, identification of machine-generated texts and tracing their provenance can enhance the accountability of API access of large language models without breaching individual users' privacy.  
