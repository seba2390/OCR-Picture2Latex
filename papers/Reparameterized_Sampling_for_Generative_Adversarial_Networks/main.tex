% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
% \usepackage{amsthm}
\usepackage[makeroom]{cancel}
\usepackage[shortlabels]{enumitem}
\usepackage[misc]{ifsym}

% \usepackage[utf8]{inputenc}
% \usepackage[english]{babel}

% \usepackage{amsthm}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{lemma}[theorem]{Lemma}


\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%

% \usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}
\newtheorem{assumption}{Assumption}

\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{float}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\db}{\mathbf{b}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\p}{\mathbf{p}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\bflambda}{\bm{\lambda}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bfm}{\mathbf{m}}%\bm has been defined as a font
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bdelta}{\bm{\delta}}
\newcommand{\bvarepsilon}{\bm{\varepsilon}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\brho}{\bm{\rho}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bnu}{\bm{\nu}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bPi}{\bm{\Pi}}
\newcommand{\bGamma}{\bm{\Gamma}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bTheta}{\bm{\Theta}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\norm}[1]{\ensuremath{\left\| #1 \right\|}}
\newcommand{\trace}[1]{\ensuremath{\mathrm{tr}\left(#1\right)}}
\newcommand{\spn}[1]{\ensuremath{\mathrm{span}\left(#1\right)}}
\newcommand{\spark}{\mbox{spark}}
\newcommand{\card}{\mbox{card}}
\newcommand{\supp}{\mbox{supp}}
\newcommand{\grad}{\mbox{grad}}
\newcommand{\prox}{\mbox{prox}}
\renewcommand{\div}{\mbox{div}}
%\renewcommand{\span}{\mbox{span}}
\newcommand{\sign}{\mbox{sign}}
%\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\vol}{\mbox{vol}}
\renewcommand{\d}{\mbox{d }}
%\newcommand{\D}{\mbox{D}}
\renewcommand{\Pr}{\textup{\mbox{Pr}}}
\renewcommand{\vec}{\textup{\mbox{vec}}}
\newcommand{\<}{\left\langle}
\renewcommand{\>}{\right\rangle}
\newcommand{\lbar}{\left\|}
\newcommand{\rbar}{\right\|}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\red}{\textcolor[rgb]{1.00,0.00,0.00}}
\newcommand{\hongyang}{\textcolor[rgb]{0.00,1.00,0.00}}
\DeclareMathOperator*{\Ber}{Ber}
\DeclareMathOperator*{\Unif}{Unif}
\DeclareMathOperator*{\Range}{Range}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\rank}{rank} \DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\Prox}{Prox}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Diag}{Diag}
% \DeclareMathOperator*{\lim}{lim}
\newcommand{\dspint}{\displaystyle \int}
\newcommand{\dspfrac}{\displaystyle \frac}
\newcommand{\dd}{\mbox{d}}
\newcommand{\orthc}{\mathbf{orth}_c}
\newcommand{\orthr}{\mathbf{orth}_r}
% \newtheorem{theorem}{Theorem}
% \newtheorem{assumption}{Assumption}

\def\bpi{{\boldsymbol{\pi}}}



\begin{document}

\title{Reparameterized Sampling for Generative Adversarial Networks}

\author{Yifei Wang\inst{1}
\and
Yisen Wang\inst{2}\footnotemark[1](\Letter)
\and
Jiansheng Yang\inst{1}
\and
Zhouchen Lin\inst{2}
}
\authorrunning{Y. Wang et al.}

\institute{
School of Mathematical Sciences, Peking University, Beijing 100871, China
\email{yifei\_wang@pku.edu.cn}, % 
\email{yjs@math.pku.edu.cn}
\and 
Key Lab. of Machine Perception (MoE), School of EECS, Peking University, Beijing 100871, China \\
\email{yisen.wang@pku.edu.cn},
\email{zlin@pku.edu.cn}
}

\toctitle{Reparameterized Sampling for Generative Adversarial Networks}

\tocauthor{Yifei Wang, Yisen Wang, Jiansheng Yang, Zhouchen Lin }

\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}} 
\footnotetext[1]{Corresponding Author}

\begin{abstract}
Recently, sampling methods have been successfully applied to enhance the sample quality of Generative Adversarial Networks (GANs). However, in practice, they typically have poor sample efficiency because of the independent proposal sampling from the generator. 
In this work, we propose REP-GAN, a novel sampling method that allows general dependent proposals by REParameterizing the Markov chains
into the latent space of the generator.
Theoretically, we show that our reparameterized proposal admits a closed-form Metropolis-Hastings acceptance ratio.
Empirically, extensive experiments on synthetic and real datasets demonstrate that our REP-GAN largely improves the sample efficiency and obtains better sample quality simultaneously.
\keywords{Generative Adversarial Networks  \and Sampling \and Markov Chain Monte Carlo \and Reparameterization.}
\end{abstract}

\section{Introduction}

Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} have achieved a great success on generating realistic images in recent years \cite{karras2019style,brock2018large}. Unlike previous models that explicitly parameterize the data distribution, GANs rely on an alternative optimization between a generator and a discriminator to learn the data distribution implicitly. However, in practice, samples generated by GANs still suffer from problems such as mode collapse and bad artifacts. 

Recently, sampling methods have shown promising results on enhancing the sample quality of GANs by making use of 
the information in the discriminator.
In the alternative training scheme of GANs, the generator only performs a few updates for the inner loop and has not fully utilized the density ratio information estimated by the discriminator. Thus, after GAN training, the sampling methods propose to further utilize this information to bridge the gap between the generative distribution and the data distribution in a fine-grained manner.
For example, DRS \cite{azadi2019discriminator} applies rejection sampling, and MH-GAN \cite{turner2019metropolis} adopts Markov chain Monte Carlo (MCMC) sampling for the improved sample quality of GANs. Nevertheless, these methods still suffer a lot from the sample efficiency problem. For example, as will be shown in Section \ref{sec:experiments}, MH-GAN's average acceptance ratio on CIFAR10 can be lower than 5\%, which makes the Markov chains slow to mix. As MH-GAN adopts an \emph{independent} proposal $q$, i.e., $q(\bx'|\bx)=q(\bx')$, the difference between samples can be so large that the proposal gets rejected easily. 

\begin{figure}[t]
\centering
\includegraphics[width=.9\linewidth]{mcmc.pdf}
\caption{Illustration of REP-GAN's reparameterized proposal with two pairing Markov chains, one in the latent space $\cZ$, and the other in the sample space $\cX$. }
\label{fig:pairing-chains}
\end{figure}


To address this limitation, we propose to generalize the independent proposal to a general \emph{dependent} proposal $q(\bx'|\bx)$. To the end, the proposed sample can be a refinement of the previous one, which leads to a higher acceptance ratio and better sample quality. We can also balance between the exploration and exploitation of the Markov chains by tuning the step size. However, it is hard to design a proper dependent proposal in the high dimensional sample space $\cX$ because the energy landscape could be very complex \cite{neal2010MCMC}. 

Nevertheless, we notice that the generative distribution $p_g(\bx)$ of GANs is implicitly defined as the push-forward of the latent prior distribution $p_0(\bz)$, and designing proposals in the low dimensional latent space is generally much easier. Hence, GAN's latent variable structure motivates us to design a \emph{structured dependent proposal} with two pairing Markov chains, one in the sample space $\cX$ and the other in the latent space $\cZ$. As shown in Figure \ref{fig:pairing-chains}, given the current pairing samples $(\bz_k,\bx_k)$, we draw the next proposal $\bx'$ in a bottom-to-up way: 1) drawing a latent proposal $\bz'$ following $q(\bz'|\bz_k)$; 2) pushing it forward through the generator and getting the sample proposal $\bx'=G(\bz')$; 3) assigning $\bx_{k+1}=\bx'$ if the proposal $\bx'$ is accepted, otherwise $\bx_{k+1}=\bx_k$ if rejected. By utilizing the underlying structure of GANs, the proposed reparameterized sampler becomes more efficient in the low-dimensional latent space. 
We summarize our main contributions as follows:
\begin{itemize}
    \item We propose a structured dependent proposal of GANs, which reparameterizes the sample-level transition $\bx\to\bx'$ into the latent-level  $\bz\to\bz'$ with two pairing Markov chains. We prove that our reparameterized proposal admits
    a tractable acceptance criterion.
    \item Our proposed method, called REP-GAN, serves as a unified framework for the existing sampling methods of GANs. It provides a better balance between exploration and exploitation by the structured dependent proposal, and also corrects the bias of Markov chains by the acceptance-rejection step. 
    \item Empirical results demonstrate that REP-GAN achieves better image quality and much higher sample efficiency than the state-of-the-art methods on both synthetic and real datasets. 
\end{itemize}


\begin{table}[t]
    \caption{Comparison of sampling methods for GANs in terms of three effective sampling mechanisms. 
    }
    \label{tab:compare-sample-methods}
    \begin{center}
    \vspace{-0.1 in}
    \begin{tabular}{lccc}
    \toprule
    {\bf Method} & {\bf Rejection step~~} & {\bf Markov chain~~} & {\bf Latent gradient proposal} \\
    \midrule 
    GAN & \xmark & \xmark & \xmark \\
    DRS \cite{azadi2019discriminator} & \checkmark & \xmark & \xmark \\
    MH-GAN \cite{turner2019metropolis} & \checkmark & \checkmark & \xmark \\
    DDLS \cite{che2020your} & \xmark & \checkmark & \checkmark \\
    % DCD \cite{song2020discriminator} & ~~\xmark$^*$ & \checkmark & \checkmark \\
    REP-GAN (ours) & \checkmark & \checkmark & \checkmark \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\section{Related Work}

Although GANs are able to synthesize high-quality images, the minimax nature of GANs makes it quite unstable, which usually results in degraded sample quality. 
A vast literature has been developed to fix the problems of GANs ever since, including network modules \cite{miyato2018spectral}, training mechanisms \cite{metz2016unrolled} and  objectives \cite{arjovsky2017wasserstein}.


Moreover, there is another line of work using sampling methods to improve the sample quality of GANs. DRS \cite{azadi2019discriminator} firstly proposes to use rejection sampling. MH-GAN \cite{turner2019metropolis} instead uses the Metropolis-Hasting (MH) algorithm with an independent proposal. DDLS \cite{che2020your} and DCD \cite{song2020discriminator} apply  gradient-based proposals by viewing GAN as an energy-based model. Tanaka et al.~\cite{tanaka2019discriminator} proposes a similar gradient-based method named DOT from the perspective of optimal transport. 

Different from them, our REP-GAN introduces a structured dependent proposal through latent reparameterization, and includes all three effective sampling mechanisms, the Markov Chain Monte Carlo method, the acceptance-rejection step, and the latent gradient-based proposal, to further improve the sample efficiency. As shown in Table \ref{tab:compare-sample-methods}, many existing works are special cases of our REP-GAN. 

Our method also belongs to the part of the literature that combine MCMC and neural networks for better sample quality. Previously, some works combine variational autoencoders \cite{kingma2013auto} and MCMC to bridge the amorization gap \cite{salimans2015markov,hoffman2017learning,li2017approximate}, while others directly learn a neural proposal function for MCMC \cite{song2017nice,levy2017generalizing,wang2018meta}. Our work instead reparameterizes the high-dimensional sample-level transition into a simpler low-dimensional latent space via the learned generator network. 

\section{Background}
GANs model the data distribution $p_d(\bx)$ implicitly with a generator $G: \cZ\to\cX$ mapping from a low-dimensional latent space $\cZ$ to a high-dimensional sample space $\cX$,
\begin{equation}
    \bx=G(\bz), \quad \bz\sim p_0(\bz),
\end{equation}
where the sample $\bx$ follows the generative distribution $p_g(\bx)$ and the latent variable $\bz$ follows the prior distribution $p_0(\bz)$, e.g., a standard normal distribution $\cN(\bzero,\bI)$. In GANs, a discriminator  $D:\cX\to[0,1]$ is learned to distinguish samples from $p_d(\bx)$ and $p_g(\bx)$ in an adversarial way
\begin{equation}
    \min_G\max_D\bbE_{\bx\sim p_d(\bx)}\log(D(\bx))+\bbE_{\bz\sim p_0(\bz)}\log(1-D(G(\bz))).
\end{equation}
\cite{goodfellow2014generative} point out that an optimal discriminator $D$ implies the density ratio between the data and generative distributions 
\begin{equation}
D(\mathbf{x})=\frac{p_{d}(\mathbf{x})}{p_{d}(\mathbf{x})+p_{g}(\mathbf{x})} ~~\Rightarrow ~~ \frac{p_d(\bx)}{p_g(\bx)}=\frac{1}{D(\bx)^{-1}-1}.
\label{eq:optimal-D}
\end{equation}

Markov Chain Monte Carlo (MCMC) refers to a kind of sampling methods that draw a chain of samples $\bx_{1:K}\in\cX^K$ from a target distribution $p_t(\bx)$. We denote the initial distribution as $p^x_0(\bx)$ and the proposal distribution as $q(\bx^\prime|\bx_k)$. With the Metropolis-Hastings (MH) algorithm, we accept the proposal $\bx^\prime\sim q(\bx'|\bx_k)$ with probability
\begin{equation}
\alpha\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_t\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}|\bx^\prime\right)}{p_t\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right) \in[0,1].   
\label{eq:mh-accept} 
\end{equation}
If $\bx^{\prime}$ is accepted, $\bx_{k+1}=\bx^{\prime}$, otherwise $\bx_{k+1}=\bx_{k}$. Under mild assumptions, the Markov chain is guaranteed to converge to $p_t(\bx)$ as $K\to\infty$. In practice, the sample efficiency of MCMC crucially depends on the proposal distribution to trade off between exploration and exploitation. 

\section{The Proposed REP-GAN}
\label{sec:emhgan}
In this section, we first review MH-GAN and point out the limitations. We then propose our structured dependent proposal to overcome these obstacles, and finally discuss its theoretical properties as well as practical implementations.

\subsection{From Independent Proposal to Dependent Proposal}

MH-GAN \cite{turner2019metropolis} first proposes to improve GAN sampling with MCMC. Specifically, given a perfect discriminator $D$ and a decent (but imperfect) generator $G$ after training, they take the data distribution $p_d(\bx)$ as the target distribution and use the generator distribution $p_g(\bx)$ as an independent proposal
\begin{equation}
\mathbf{x}^{\prime} \sim q\left(\mathbf{x}^{\prime} | \mathbf{x}_{k}\right)=q\left(\mathbf{x}^{\prime}\right)=p_g(\bx').
\label{eq:independence-proposal}
\end{equation}
With the MH criterion (Eqn. \eqref{eq:mh-accept}) and the density ratio (Eqn. \eqref{eq:optimal-D}), we should accept $\bx'$
with probability
\begin{equation}
\alpha_{\rm MH}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}\right)}{p_d\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}\right)}\right)=\min \left(1, \frac{D\left(\mathbf{x}_{k}\right)^{-1}-1}{D\left(\mathbf{x}^{\prime}\right)^{-1}-1}\right).
\label{eq:mhgan-alpha}
\end{equation}
However, to achieve tractability, MH-GAN adopts an independent proposal $q(\bx^\prime)$ with poor sample efficiency. As the proposed sample $\bx'$ is independent of the current sample $\bx_k$, the difference between the two samples can be so large that it results in a very low acceptance probability. Consequently, samples can be trapped in the same place for a long time, leading to a very slow mixing of the chain. 

A natural solution is to take a \textit{dependent} proposal $q(\bx'|\bx_k)$ that will propose a sample $\bx'$ close to the current one $\bx_k$, which is more likely to be accepted. 
Nevertheless, the problem of such a dependent proposal is that its MH acceptance criterion
\begin{equation}
    \alpha_{\rm DEP}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right),
    \label{eq:dependent-proposal-acceptance}
\end{equation}
is generally intractable because the data density $p_d(\bx)$ is unknown. Besides, it is hard to design a proper dependent proposal $q(\bx'|\bx_k)$ in the high dimensional sample space $\cX$ with complex landscape. These obstacles prevent us from adopting a dependent proposal that is more suitable for MCMC.

\subsection{A Tractable Structured Dependent Proposal with Reparameterized Markov Chains}
As discussed above, the major difficulty of a general dependent proposal $q(\bx'|\bx_k)$ is to compute the MH criterion. We show that it can be made tractable by considering an additional pairing Markov chain in the latent space.

As we know, samples of GANs lie in a low-dimensional manifold induced by the push-forward of the latent variable \cite{arjovsky2017wasserstein}. Suppose that at the $k$-th step of the Markov chain, we have a GAN sample $\bx_k$ with latent $\bz_k$. Instead of drawing a sample $\bx'$ directly from a sample-level proposal distribution $q(\bx'|\bx_k)$, we first draw a latent proposal $\bz'$ from a dependent latent proposal distribution $q(\bz'|\bz_k)$. Afterward, we push the latent $\bz'$ forward through the generator and get the output $\bx'$ as our sample proposal. 

As illustrated in Figure \ref{fig:pairing-chains}, our bottom-to-up proposal relies on the transition reparameterization with two pairing Markov chains in the sample space $\cX$ and the latent space $\cZ$. Hence we call it a REP (reparameterized) proposal. Through a learned generator, we transport the transition $\bx_k\to\bx'$ in the high dimensional space $\cX$ into the low dimensional space $\cZ$, $\bz_k\to\bz'$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. For example, the latent target distribution is nearly standard normal when the generator is nearly perfect.
In fact, under mild conditions, the REP proposal distribution $q_{\rm REP}(\bx'|\bx_k)$ and the latent proposal distribution $q(\bz'|\bz_k)$ are tied with the following change of variables \cite{gemici2016normalizing,ben1999change}
\begin{equation}
    \log q_{\rm REP}(\bx'|\bx_k)= \log q(\bx'|\bz_k)
    =\log q(\bz'|\bz_k) - \frac{1}{2}\log\det J_{\bz'}^\top J_{\bz'},
    \label{eq:cmc-proposal-change-of-variable}
\end{equation}
where $J_\bz$ denotes the Jacobian matrix of the push-forward $G$ at $\bz$, i.e., $\left[J_\bz\right]_{ij} = \partial\,\bx_i/\partial\,\bz_j, \bx=G(\bz)$.

Nevertheless, it remains unclear whether we can perform the MH test to decide the acceptance of the proposal $\bx'$. Note that a general dependent proposal distribution does not meet a tractable MH acceptance criterion (Eqn. \eqref{eq:dependent-proposal-acceptance}). Perhaps surprisingly, 
it can be shown that with our structured REP proposal, 
the MH acceptance criterion is tractable for general latent proposals $q(\bz'|\bz_k)$. 

\begin{theorem}
\label{thm:emh}
Consider a Markov chain of GAN samples $\bx_{1:K}$ with initial distribution $p_g(\bx)$. For step $k+1$, we accept our REP proposal $\bx'\sim q_{\rm REP}(\bx'|\bx_k)$ with probability
\begin{equation}
\alpha_{\rm REP}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)
=\min \left(1,~\frac{p_0(\bz')q(\bz_k|\bz')}{p_0(\bz_k)q(\bz'|\bz_k)}\cdot
\frac{D(\bx_k)^{-1}-1}{D(\bx')^{-1}-1}\right),
\label{eq:cmc-mh-acceptance}
\end{equation}
i.e. let $\bx_{k+1}=\bx'$ if $\bx'$ is accepted and $\bx_{k+1}=\bx_k$ otherwise. Further assume the chain is irreducible, aperiodic and not transient. Then, according to the Metropolis-Hastings algorithm, the stationary distribution of this Markov chain is the data distribution $p_d(\bx)$ \cite{gelman2013bayesian}.
\label{thm:main}
\end{theorem}

\begin{proof}
Note that similar to Eqn \eqref{eq:cmc-proposal-change-of-variable},  we also have the change of variables between $p_g(\bx)$ and $p_0(\bz)$,
\begin{equation}
\log p_g(\bx)\rvert_{\bx=G(\bz)}=\log p_0(\bz) - \frac{1}{2} \log \det J_{\bz}^\top J_{\bz}.
\label{eq:generator-change-of-variable}
\end{equation}
According to \cite{gelman2013bayesian}, the assumptions that the chain is irreducible, aperiodic, and not transient make sure that the chain has a unique stationary distribution, and the MH algorithm ensures that this stationary distribution equals to the target distribution $p_d(\bx)$.
Thus we only need to show that the MH criterion in Eqn. \eqref{eq:cmc-mh-acceptance} holds. Together with Eqn. \eqref{eq:optimal-D}, \eqref{eq:dependent-proposal-acceptance} and \eqref{eq:cmc-proposal-change-of-variable}, we have
\begin{equation}
\begin{aligned}
\alpha_{\rm REP}(\bx',\bx_k)
&=\frac{p_d\left(\mathbf{x}^{\prime}\right)  q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right)  q\left(\mathbf{x}^{\prime}|\bx_k\right)}
=\frac{{p_d\left(\mathbf{x}^{\prime}\right)}  q(\bz_k|\bz')
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
{p_g(\bx_k)}p_g(\bx')}{{p_d\left(\mathbf{x}_{k}\right)}  q(\bz'|\bz_k)
\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}
{p_g(\bx')}p_g(\bx_k)}\\
&=\frac{q(\bz_k|\bz'){\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}}p_0(\bz'){\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}}(D(\bx_k)^{-1}-1)}
{q(\bz'|\bz_k){\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}}p_0(\bz_k){\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}}(D(\bx')^{-1}-1)}\\
&=\frac{p_0(\bz')q(\bz_k|\bz')(D(\bx_k)^{-1}-1)}{p_0(\bz_k)q(\bz'|\bz_k)(D(\bx')^{-1}-1)},
\end{aligned}
\label{eq:dcgan-cmc-alpha}
\end{equation}
which is the acceptance ratio as desired. Q.E.D.
\end{proof}

The theorem above demonstrates the following favorable properties of our method:
\begin{itemize}
\item The discriminator score ratio is the same as $\alpha_{\rm MH}(\bx',\bx_k)$, but MH-GAN is restricted to a specific independent proposal. Our method instead works for any latent proposal $q(\bz'|\bz_k)$. When we take $q(\bz'|\bz_k)=p_0(\bz')$, our method reduces to MH-GAN. 
\item Compared to $\alpha_{\rm DEP}(\bx',\bx_k)$ of a general dependent proposal (Eqn. \eqref{eq:dependent-proposal-acceptance}), the unknown data distributions terms are successfully cancelled in the reparameterized acceptance criterion.
\item The reparameterized MH acceptance criterion becomes tractable as it only involves the latent priors, the latent proposal distributions, and the discriminator scores.
\end{itemize}

Combining the REP proposal $q_{\rm REP}(\bx'|\bx_k)$ and its tractable MH criterion $\alpha_{\rm REP}(\bx',\bx_k)$, we have developed a novel sampling method for GANs, coined as REP-GAN. See Appendix \ref{algo:REP-GAN} for a detailed description.
Moreover, our method can serve as a general approximate inference technique for Bayesian models by bridging MCMC and GANs. Previous works \cite{marzouk2016introduction,titsias2017learning,hoffman2019neutra} also propose to avoid the bad geometry of a complex probability measure by reparameterizing the Markov transitions into a simpler measure. However, these methods are limited to explicit invertible mappings without dimensionality reduction. With this work, we are the first to show that it is also tractable to conduct such model-based reparameterization with implicit models like GANs. 



\subsection{A Practical Implementation}
\label{sec:proposal}
REP-GAN enables us to utilize the vast literature of existing MCMC algorithms \cite{neal2010MCMC} to design dependent proposals for GANs. We take Langevin Monte Carlo (LMC) as an example.
As an Euler-Maruyama discretization of the Langevin dynamics, LMC updates the Markov chain with
\begin{equation}
    \bx_{k+1}=\bx_k+\frac{\tau}{2}\nabla_{\bx}\log p_t(\bx_k)+\sqrt{\tau}\cdot\bvarepsilon,~~\bvarepsilon\sim\cN(\bzero,\bI),
    \label{eqn:langevin-steps}
\end{equation}
for a target distribution $p_t(\bx)$. Compared to MH-GAN, LMC utilizes the gradient information to explore the energy landscape more efficiently. However, if we directly take the (unknown) data distribution $p_d(\bx)$ as the target distribution $p_t(\bx)$, LMC does not meet a tractable update rule. 

As discussed above, the reparameterization of REP-GAN makes it easier to design transitions in the low-dimensional latent space. Hence, we instead propose to use LMC for the latent Markov chain.
We assume that the data distribution also lies in the low-dimensional manifold induced by the generator, i.e., 
$\operatorname{Supp}\left(p_d\right)\subset\operatorname{Im}(G)$. This implies that the data distribution $p_d(\bx)$ also has a pairing distribution in the latent space, denoted as $p_t(\bz)$. They are tied with the change of variables
\begin{equation}
\log p_d(\bx)\rvert_{\bx=G(\bz)}=\log p_t(\bz) - \frac{1}{2} \log\det\, J_{\bz}^\top J_{\bz},
\end{equation}
Taking $p_t(\bz)$ as the (unknown) target distribution of the latent Markov chain, we have the following Latent LMC (L2MC) proposal 
\begin{equation}
\begin{aligned}
\bz'&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log p_t(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log\frac{p_t(\bz_k)
% \left|\frac{\partial\,\bx}{\partial\,\bz}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
}{p_0(\bz_k)
% \left|\frac{\partial\,\bx}{\partial\,\bz}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
}+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log\frac{p_d(\bx_k)}{p_g(\bx_k)}+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz_k-\frac{\tau}{2}\nabla_{\bz}\log(D^{-1}(\bx_k)-1)+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon, \quad\bvarepsilon\sim\cN(\bzero,\bI),
\end{aligned}
\label{eq:langevin}
\end{equation}
where $\bx_k=G(\bz_k)$. As we can see, L2MC is made tractable by our structured dependent proposal with pairing Markov chains. DDLS \cite{che2020your} proposes a similar Langevin proposal by formalizing GANs as an implicit energy-based model, while here we provide a straightforward derivation through reparameterization. Our major difference to DDLS is that REP-GAN also includes a tractable MH correction step (Eqn. \eqref{eq:cmc-mh-acceptance}), which accounts for the numerical errors introduced by the discretization in Eqn. \eqref{eqn:langevin-steps} and ensures that detailed balance holds. 

We give a detailed description of the algorithm procedure of our REP-GAN in Algorithm \ref{algo:REP-GAN}.



\subsection{Extension to WGAN}
Our method can also be extended to other kinds of GAN, like Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein}.
The WGAN objective is
\begin{align}
\min_G\max_D\,\mathbb{E}_{\bx\sim p_{d}(\bx)}[D(\bx)]-\mathbb{E}_{\bx\sim p_{g}(\bx)}[D(\bx)],
\end{align}
where $D:\cX\to\bbR$ is restricted to be a Lipschitz function. Under certain conditions, WGAN also implies an approximate estimation of the density ratio \cite{che2020your},
\begin{equation}
D(\bx)\approx\log\frac{p_d(\bx)}{p_g(\bx)}+\text{const}\quad\Rightarrow\quad\frac{p_d(\bx)}{p_g(\bx)}\approx\exp(D(\bx))\cdot \text{const}.
\end{equation}
Following the same derivations as in Eqn. \eqref{eq:dcgan-cmc-alpha} and \eqref{eq:langevin}, we will have the WGAN version of REP-GAN. Specifically, with $\bx_k=G(\bz_k)$, the L2MC proposal follows
\begin{equation}
\bz'=\bz_k+\frac{\tau}{2}\nabla_{\bz}D(\bx_k)+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon, \quad\bvarepsilon\sim\cN(\bzero,\bI),
\label{eq:wgan-proposal}
\end{equation}
and the MH acceptance criterion is 
\begin{equation}
\alpha_{REP-W}\left(\mathbf{x}^{\prime},\mathbf{x}_{k}\right)=\min \left(1,~ \frac{q(\bz_k|\bz')p_0(\bz')}{q(\bz'|\bz_k)p_0(\bz_k)}\cdot\frac{\exp\left(D(\bx')\right)}{\exp\left(D(\bx_k)\right)}\right).
\end{equation}

\begin{algorithm}[t]
\textbf{Input:} trained GAN with (calibrated) discriminator $D$ and generator $G$, Markov chain length $K$, latent prior distribution $p_0(\bz)$, latent proposal distribution $q(\bz'|\bz_k)$;\\
\textbf{Output:} an improved GAN sample $\bx_K$;
\begin{algorithmic}
\STATE Draw an initial sample $\bx_1$: 1) draw initial latent $\bz_1\sim p_0(\bz)$ and 2) push forward $\bx_1=G(\bz_1)$;
\FOR{each step $k\in[1,K-1]$}
\STATE Draw a REP proposal $\bx'\sim q_{\rm REP}(\bx'|\bx_k)$: 1) draw a latent proposal $\bz'\sim q(\bz'|\bz_k)$, and 2) push forward $\bx'=G(\bz')$;
\STATE Calculate the MH acceptance criterion $\alpha_{\rm REP}(\bx_k,\bx')$ following Eqn. \eqref{eq:cmc-mh-acceptance};
\STATE Decide the acceptance of $\bx'$ with probability $\alpha_{\rm REP}(\bx_k,\bx')$;
\IF{$\bx'$ is accepted}
\STATE{Let $\bx_{k+1}=\bx', \bz_{k+1}=\bz'$}
\ELSE
\STATE{Let $\bx_{k+1}=\bx_k,\bz_{k+1}=\bz_k$}
\ENDIF
\ENDFOR
\end{algorithmic}
\caption{GAN sampling with Reparameterized Markov chains (REP-GAN)}
\label{algo:REP-GAN}
\end{algorithm}



\section{Experiments}
\label{sec:experiments}
We evaluate our method  on two synthetic datasets and two real-world image datasets as follows.

\subsection{Manifold Dataset}
Following DOT \cite{tanaka2019discriminator} and DDLS \cite{che2020your}, we apply REP-GAN to the Swiss Roll dataset, where data samples lie on a Swiss roll manifold in the two-dimensional space. We construct the dataset by scikit-learn with 100,000 samples, and train a WGAN with the same architecture as DOT and DDLS, where both the generator and discriminator are fully connected neural networks with leaky ReLU nonlinearities. We optimize the model using the Adam optimizer, with learning rate $0.0001$. After training, we draw 1,000 samples with different sampling methods. Following previous practice, we initialize a Markov chain with a GAN sample, run it for $K=100$ steps, and collect the last example for evaluation.

As shown in Figure \ref{fig:swissroll}, with appropriate step size ($\tau=0.01$), the gradient-based methods (DDLS and REP-GAN) outperform independent proposals (DRS and MH-GAN) by a large margin, while DDLS is more discontinuous on shape compared to REP-GAN. In DDLS, when the step size becomes too large ($\tau=0.1,1$), the numerical error of the Langevin dynamics becomes so large that the chain either collapses or diverges. In contrast, those bad proposals are rejected by the MH correction steps of REP-GAN, which prevents the misbehavior of the Markov chain.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{swiss-roll.pdf}
    \caption{Visualization of samples with different sampling methods on the Swiss Roll dataset. Here tau denotes the Langevin step size in Eqn. \eqref{eq:wgan-proposal}.}
    \label{fig:swissroll}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{25Gaussians-mark.pdf}
    \caption{Visualization of samples with different sampling methods on the 25-Gaussians dataset. Here $\tau$ denotes the Langevin step size in Eqn. \eqref{eq:wgan-proposal}.}
    \label{fig:25gaussians}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{more-modes.pdf}
    \caption{Visualization of the mixture-of-Gaussian experiments with 9x9 (1st row) and 13x13 (2nd row) modes with proper step size $\tau=0.01$. True data points are shown in grey (in background), and generated points are shown in blue. }
    \label{fig:more-modes}
\end{figure}


\subsection{Multi-modal Dataset}

As GANs are known to suffer from the mode collapse problem \cite{goodfellow2016nips}, we also compare different GAN sampling methods in terms of modeling multi-modal distributions. Specifically, we consider the 25-Gaussians dataset that is widely discussed in previous work \cite{azadi2019discriminator,turner2019metropolis,che2020your}. The dataset is generated by a mixture of twenty-five two-dimensional isotropic Gaussian distributions with variance 0.01, and means separated by 1, arranged in a grid. We train a small GAN with the standard WGAN-GP objective following the setup in  \cite{tanaka2019discriminator}. After training, we draw 1,000 samples with different sampling methods. 

As shown in Figure \ref{fig:25gaussians}, compared to MH-GAN, the gradient-based methods (DDLS and ours) produce much better samples close to the data distribution with proper step size ($\tau=0.01$). Comparing DDLS and our REP-GAN, we can notice that DDLS tends to concentrate so much on the mode centers that its standard deviation can be even smaller than the data distribution. Instead, our method preserves more sample diversity while concentrating on the mode centers. This difference becomes more obvious as the step size $\tau$ becomes larger. When $\tau=0.1$, as marked with blue circles, DDLS samples become so concentrated that some modes are even missed. When $\tau=1$, DDLS samples diverge far beyond the $5\times5$ grid. 
In comparison, our REP-GAN is more stable because the MH correction steps account for the numerical errors caused by large $\tau$.

These distinctions also become even more obvious when we scale to more modes. As shown in Figure \ref{fig:more-modes}, we also compare them w.r.t. mixture of Gaussians with $9\times9$ and $13\times13$ modes, respectively. Under the more challenging scenarios, we can see that the gradient-based methods still consistently outperforms MH-GAN. Besides, our REP-GAN has a more clear advantage over DDLS. Specifically, for $9\times9$ modes, our REP-GAN produces samples that are less noisy, while preserving all the modes. For $13\times13$ modes, DDLS makes a critical mistake that it drops one of the modes. As discussed above, we believe this is because DDLS has a bias towards regions with high probability, while ignoring the diversity of the distribution. In comparison, REP-GAN effectively prevents such bias by the MH correction steps.



\subsection{Real-world Image Dataset}

Following MH-GAN \cite{turner2019metropolis}, we conduct experiments on two real-world image datasets, CIFAR-10 and CelebA, for two models, DCGAN \cite{radford2015unsupervised} and WGAN \cite{arjovsky2017wasserstein}. We adopt the DCGAN generator and discriminator networks as our backbone networks.
Following the conventional evaluation protocol, we initialize each Markov chain with a GAN sample, run it for 640 steps, and take the last sample for evaluation. We collect 50,000 samples to evaluate the Inception Score\footnote{For fair comparison, our training and evaluation follows the the official code of MH-GAN \cite{turner2019metropolis}: \url{https://github.com/uber-research/metropolis-hastings-gans}} \cite{salimans2016improved}. 
The step size $\tau$ of our L2MC proposal is $0.01$ on CIFAR-10 and $0.1$ on CelebA. We calibrate the discriminator with Logistic Regression as in \cite{turner2019metropolis}. 


\begin{table}[t]\centering
    \caption{Inception Scores of different sampling methods on CIFAR-10 and CelebA, with the DCGAN and WGAN backbones.}
    \label{tab:cifar-10-incep-results}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} &  \multicolumn{2}{c}{CIFAR-10} &  \multicolumn{2}{c}{CelebA} \\
    & DCGAN & WGAN & DCGAN & WGAN \\
    \midrule
    GAN  & 3.219 & 3.740  & 2.332 & 2.788 \\
    DRS \cite{azadi2019discriminator} & 3.073 & 3.137 & 2.869 & 2.861 \\
    MH-GAN \cite{turner2019metropolis}  & 3.225 & 3.851 & \textbf{3.106} & 2.889 \\
    DDLS \cite{che2020your} & 3.152 & 3.547 & 2.534 & 2.862 \\    
    REP-GAN (ours)  & \textbf{3.541} & \textbf{4.035} & 2.686 & \textbf{2.943} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[t]\centering
    \caption{Average Inception Score (a) and acceptance ratio (b) vs. training epochs with DCGAN on CIFAR-10.}
    \label{tab:training-epochs}
\begin{subtable}[t]{\linewidth}\centering
    \caption{Inception Score (mean $\pm$ std)}
    \label{tab:training-epochs-inception}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        Epoch & 20 & 21 & 22 & 23 & 24 \\
        \midrule
        GAN ~& 2.482 $\pm$ 0.027 ~& 3.836 $\pm$ 0.046 ~& 3.154 $\pm$ 0.014 ~& 3.383 $\pm$ 0.046 ~& 3.219 $\pm$ 0.036 \\
        MH-GAN ~& 2.356 $\pm$ 0.023 ~& 3.891 $\pm$ 0.040 ~& 3.278 $\pm$ 0.033 ~& 3.458 $\pm$ 0.029 ~& 3.225 $\pm$ 0.029 \\
        DDLS ~& 2.419 $\pm$ 0.021 ~& 3.332 $\pm$ 0.025 ~& 2.996 $\pm$ 0.035 ~& 3.255 $\pm$ 0.045 ~& 3.152 $\pm$ 0.028 \\
        REP-GAN ~& \textbf{2.487} $\pm$ 0.019 ~& \textbf{3.954} $\pm$ 0.046 ~& \textbf{3.294} $\pm$ 0.030 ~& \textbf{3.534} $\pm$ 0.035 ~& \textbf{3.541} $\pm$ 0.038 \\
        \bottomrule
    \end{tabular}    
    }
\end{subtable}
\begin{subtable}[t]{\linewidth}\centering
    \caption{Average Acceptance Ratio (mean $\pm$ std)}
    \label{tab:training-epochs-acceptance}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        Epoch & 20 & 21 & 22 & 23 & 24 \\
        \midrule
        MH-GAN ~& 0.028 $\pm$ 0.143 ~& 0.053 $\pm$ 0.188 ~& 0.060 $\pm$ 0.199 ~& 0.021 $\pm$ 0.126 ~& 0.027 $\pm$ 0.141 \\
        REP-GAN ~& \textbf{0.435} $\pm$ 0.384 ~& \textbf{0.350} $\pm$ 0.380 ~& \textbf{0.287} $\pm$ 0.365 ~& \textbf{0.208} $\pm$ 0.335 ~& \textbf{0.471} $\pm$ 0.384  \\
        \bottomrule
    \end{tabular}    
    }
\end{subtable}

\end{table}

\begin{table}[t]\centering
    \caption{Ablation study of our REP-GAN with 
    Inception Scores (IS) and acceptance ratios on CIFAR-10 with two backbone models, DCGAN and WGAN.
    }
    \label{tab:ablation-study}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method~} & \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN}  \\
    &  Accept Ratio & IS & Accept Ratio & IS \\
    \midrule
    REP-GAN & \textbf{0.447} $\pm$ 0.384 ~& \textbf{3.541} $\pm$ 0.038  ~& \textbf{0.205} $\pm$ 0.330 ~& \textbf{4.035} $\pm$ 0.036 \\
    REP-GAN w/o REP proposal~~ & 0.027 $\pm$ 0.141 & 3.225 $\pm$ 0.029 & 0.027 $\pm$ 0.141 & 3.851 $\pm$ 0.044 \\
    REP-GAN w/o MH rejection & -  & 3.152 $\pm$ 0.028 & -  & 3.547 $\pm$ 0.029 \\
    \bottomrule
    \end{tabular}
    }

\end{table}


From Table \ref{tab:cifar-10-incep-results}, we can see our method  outperforms the state-of-the-art sampling methods in most cases. 
In Table \ref{tab:training-epochs}, we also present the average Inception Score and acceptance ratio during the training process.
As shown in Table \ref{tab:training-epochs-inception}, our REP-GAN can still outperform previous sampling methods consistently and significantly. Besides, in Table \ref{tab:training-epochs-acceptance}, we find that the average acceptance ratio of MH-GAN is lower than 0.05 in most cases, which is extremely low. While with our reparameterized dependent proposal, REP-GAN achieves an acceptance ratio between 0.2 and 0.5, which is known to be a relatively good tradeoff for MCMC algorithms.

\subsection{Algorithmic Analysis}

\paragraph{\textbf{Ablation Study}} We conduct an ablation study of the proposed sampling algorithm, REP-GAN, and  the results are shown in Table \ref{tab:ablation-study}.  We can see that without our proposed reparameterized (REP) proposal, the acceptance ratio is very small (with an independent proposal instead). Consequently, the sample quality degrades significantly. Also, we can find that the MH correction step also matters a lot, without which the sample quality of Langevin sampling becomes even worse than the independent proposal. The ablation study shows the necessity of both REP proposal and MH rejection steps in the design of our REP-GAN.


\begin{figure}[t]\centering
\includegraphics[width=\linewidth]{chains-red-new.pdf}
\caption{The first 15 steps of two Markov chains with the same initial samples, generated by 
MH-GAN (top), DDLS (middle), and REP-GAN (bottom).}
\label{fig:sampled-chain}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{proposals_lr1.png}
    \caption{Visualization of 5 Markov chains of our REP proposals (i.e., REP-GAN without the MH rejection steps) with a large step size ($\tau=1$).}
    \label{fig:proposals}
\end{figure}

\paragraph{\textbf{Markov Chain Visualization}} In Figure \ref{fig:sampled-chain}, we demonstrate two Markov chains sampled with different methods.  We can see that 
MH-GAN is often trapped in the same place because of the independent proposals. DDLS and REP-GAN instead gradually refine the samples with gradient steps. In addition, compared the gradient-based methods, we can see that the MH rejection steps of REP-GAN help avoid some bad artifacts in the images. For example, in the camel-like images marked in red, the body of the camel is separated in the sample of DDLS (middle) while it is not in the sample of REP-GAN (bottom). 
Note that, the evaluation protocol only needs the last step of the chain, thus we prefer a small step size that finetunes the initial samples for better sample quality. As shown in Figure \ref{fig:proposals}, our REP proposal can also produce very diverse images with a large step size. 



\paragraph{\textbf{Computation Overhead}} We also compare the computation cost of the gradient-based sampling methods, DDLS and REP-GAN. They take 88.94 s and 88.85s, respectively, hence the difference is negligible. Without the MH-step, our method takes 87.62s, meaning that the additional MH-step only costs 1.4\% computation overhead, which is also negligible, but it brings a significant improvement of sample quality as shown in Table \ref{tab:ablation-study}. 





\section{Conclusion}
In this paper, we have proposed a novel method, REP-GAN, to improve the sampling of GAN. We devise a structured dependent proposal that reparameterizes the sample-level transition of GAN into the latent-level transition. More importantly, we first prove that this general proposal admits a tractable MH criterion. 
Experiments show our method does not only improve sample efficiency but also demonstrate state-of-the-art sample quality on benchmark datasets over existing sampling methods.


\section*{Acknowledgement}
Yisen Wang is supported by the National Natural Science Foundation of China under Grant No. 62006153 and Project
2020BD006 supported by PKU-Baidu Fund. Jiansheng Yang is supported by the National Science Foundation of
China under Grant No. 11961141007. Zhouchen
Lin is supported by the National Natural Science Foundation of China (Grant No.s 61625301 and 61731018), Project 2020BD006 supported by PKU-Baidu Fund, Major Scientific Research Project of Zhejiang Lab (Grant No.s
2019KB0AC01 and 2019KB0AB02), and Beijing Academy
of Artificial Intelligence. 




\begin{thebibliography}{10}
  \providecommand{\url}[1]{\texttt{#1}}
  \providecommand{\urlprefix}{URL }
  \providecommand{\doi}[1]{https://doi.org/#1}
  
  \bibitem{arjovsky2017wasserstein}
  Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein {GAN}. ICML  (2017)


  \bibitem{azadi2019discriminator}
  Azadi, S., Olsson, C., Darrell, T., Goodfellow, I., Odena, A.: Discriminator
    rejection sampling. ICLR  (2019)
  
  \bibitem{ben1999change}
  Ben-Israel, A.: The change-of-variables formula using matrix volume. SIAM
    Journal on Matrix Analysis and Applications  \textbf{21}(1),  300--312 (1999)
  
  \bibitem{brock2018large}
  Brock, A., Donahue, J., Simonyan, K.: Large scale {GAN} training for high
    fidelity natural image synthesis. ICLR  (2019)
  
  \bibitem{che2020your}
  Che, T., Zhang, R., Sohl-Dickstein, J., Larochelle, H., Paull, L., Cao, Y.,
    Bengio, Y.: Your {GAN} is secretly an energy-based model and you should use
    discriminator driven latent sampling. ICML  (2020)
  
  \bibitem{gelman2013bayesian}
  Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B.:
    Bayesian data analysis. CRC press (2013)
  
  \bibitem{gemici2016normalizing}
  Gemici, M.C., Rezende, D., Mohamed, S.: Normalizing flows on {Riemannian}
    manifolds. arXiv preprint arXiv:1611.02304  (2016)
  
  \bibitem{goodfellow2016nips}
  Goodfellow, I.: {NIPS} 2016 tutorial: Generative adversarial networks. arXiv
    preprint arXiv:1701.00160  (2016)
  
  \bibitem{goodfellow2014generative}
  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., Courville, A., Bengio, Y.: Generative adversarial nets. NeurIPS  (2014)
  
  \bibitem{hoffman2019neutra}
  Hoffman, M., Sountsov, P., Dillon, J.V., Langmore, I., Tran, D., Vasudevan, S.:
    Neutra-lizing bad geometry in {Hamiltonian Monte Carlo} using neural
    transport. arXiv preprint arXiv:1903.03704  (2019)
  
  \bibitem{hoffman2017learning}
  Hoffman, M.D.: Learning deep latent {Gaussian} models with {Markov chain Monte
    Carlo}. In: ICML (2017)
  
  \bibitem{karras2019style}
  Karras, T., Laine, S., Aila, T.: A style-based generator architecture for
    generative adversarial networks. NeurIPS  (2019)
  
  \bibitem{kingma2013auto}
  Kingma, D.P., Welling, M.: Auto-encoding variational bayes. ICLR  (2014)
  
  \bibitem{levy2017generalizing}
  Levy, D., Hoffman, M.D., Sohl-Dickstein, J.: Generalizing {Hamiltonian Monte
    Carlo} with neural networks. ICLR  (2018)
  
  \bibitem{li2017approximate}
  Li, Y., Turner, R.E., Liu, Q.: Approximate inference with amortised {MCMC}.
    arXiv preprint arXiv:1702.08343  (2017)
  
  \bibitem{marzouk2016introduction}
  Marzouk, Y., Moselhy, T., Parno, M., Spantini, A.: An introduction to sampling
    via measure transport. arXiv preprint arXiv:1602.05023  (2016)
  
  \bibitem{metz2016unrolled}
  Metz, L., Poole, B., Pfau, D., Sohl-Dickstein, J.: Unrolled generative
    adversarial networks. ICLR  (2017)
  
  \bibitem{miyato2018spectral}
  Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for
    generative adversarial networks. ICLR  (2018)
  
  \bibitem{neal2010MCMC}
  Neal, R.M., et~al.: {MCMC} using {Hamiltonian} dynamics. Handbook of markov
    chain monte carlo  \textbf{54},  113--162 (2010)
  
  \bibitem{radford2015unsupervised}
  Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with
    deep convolutional generative adversarial networks. arXiv preprint
    arXiv:1511.06434  (2015)
  
  \bibitem{salimans2016improved}
  Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:
    Improved techniques for training {GANs}. NeurIPS  (2016)
  
  \bibitem{salimans2015markov}
  Salimans, T., Kingma, D., Welling, M.: {Markov chain Monte Carlo} and
    variational inference: Bridging the gap. In: ICML (2015)
  
  \bibitem{song2017nice}
  Song, J., Zhao, S., Ermon, S.: {A-NICE-MC}: Adversarial training for {MCMC}.
    NeurIPS  (2017)
  
  \bibitem{song2020discriminator}
  Song, Y., Ye, Q., Xu, M., Liu, T.Y.: Discriminator contrastive divergence:
    semi-amortized generative modeling by exploring energy of the discriminator.
    arXiv preprint arXiv:2004.01704  (2020)
  
  \bibitem{tanaka2019discriminator}
  Tanaka, A.: Discriminator optimal transport. NeurIPS  (2019)
  
  \bibitem{tang2020lessons}
  Tang, S.: Lessons learned from the training of {GANs} on artificial datasets. IEEE Access (2020)
    
  \bibitem{titsias2017learning}
  Titsias, M.K.: Learning model reparametrizations: implicit variational
    inference by fitting {MCMC} distributions. arXiv preprint arXiv:1708.01529
    (2017)
  
  \bibitem{turner2019metropolis}
  Turner, R., Hung, J., Saatci, Y., Yosinski, J.: {Metropolis-Hastings}
    generative adversarial networks. ICML  (2019)
  
  \bibitem{wang2018meta}
  Wang, T., Wu, Y., Moore, D., Russell, S.J.: Meta-learning {MCMC} proposals. In:
    NeurIPS (2018)
  
  \end{thebibliography}  

  \bibliographystyle{splncs04}



\appendix
\section{Appendix}

\subsection{Assumptions and Implications}

Note that our method needs a few assumptions on the models for our analysis to hold. Here we state them explicitly and discuss their applicability and potential impacts.

\begin{assumption}
The generator mapping $G:\bbR^n\to\bbR^m (n<m)$ is injective, and its Jacobian matrix $\left[\frac{\partial\,G(\bz)}{\partial\,\bz}\right]$ of size $m\times n$, has full column rank for all $\bz\in\bbR^n$.
\end{assumption}

For the change of variables in Eqn. 11 and 13 to hold, according to \cite{ben1999change}, we need the mapping to be injective and its Jaobian should have full column rank. A mild sufficient condition for injectivity is that the generator only contains (non-degenerate) affine layers and injective non-linearities, like LeakyReLU. It is not hard to show that such a condition also implies the full rankness of the Jacobian. In fact, this architecture has already been found to benefit GANs and achieved state-of-the-art results \cite{tang2020lessons}. The affine layers here are also likely to be non-degenerate because their weights are randomly initialized and typically will not degenerate in practice during the training of GANs.


\begin{assumption}
The discriminator $D$ offers a perfect estimate the density ratio between the generative distribution $p_g(\bx)$ and the data distribution $p_d(\bx)$ as in Eqn. 3.
\end{assumption}

This is a common, critical, but less practical assumption among the existing sampling methods of GANs. It is unlikely to hold exactly in practice, because during the alternative training of GANs, the generator is also changing all the time, and the a few updates of the discriminator cannot fully learn the corresponding density ratio. Nevertheless, we think it can capture a certain extent information of density ratio which explains why the sampling methods can consistently improve over the baseline at each epoch. 

From our understanding, the estimated density ratio is enough to push the generator better but not able to bring it up to the data distribution. This could be the reason why the Inception scores obtained by the sampling methods, can improve over the baselines but cannot reach up to that of real data and fully close the gap, even with very long run of the Markov chains.

Hence, there is still much room for improvement. To list a few, one can develop mechanisms that bring more accurate density ratio estimate, or relax the assumptions for the method to hold, or establishing estimation error bounds. Overall, we believe GANs offer an interesting alternative scenario for the development of sampling methods.


\setcounter{table}{4}
\setcounter{figure}{5}

\begin{table}[t]\centering
    \caption{Frchet Inception Distance (FID) of different MCMC sampling methods on CIFAR-10 and CelebA.}
    \label{tab:cifar-10-fid-results}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} &  \multicolumn{2}{c}{CIFAR-10} &  \multicolumn{2}{c}{CelebA} \\
    & DCGAN & WGAN & DCGAN & WGAN \\
    \midrule
GAN    & 100.363 & 153.683 & 227.892 & 207.545 \\
MH-GAN \cite{turner2019metropolis} & 100.167 & 143.426 & {\bf 227.233} & 207.143 \\
DDLS \cite{che2020your}   & 145.981 & 193.558 & 269.840 & 232.522 \\
REP-GAN (ours)   & {\bf 99.798}  & {\bf 143.322} & 230.748 & {\bf 207.053} \\    
    \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{celeba-chains.pdf}
    \caption{Visualization of the Markov chains of MH-GAN (top), DDLS (middle), and REP-GAN (bottom) on CelebA with WGAN backbone.}
    \label{fig:celeba-chains}
\end{figure}


\subsection{Additional Empirical Results}
Here we list some additional empirical results of our methods.

\paragraph{\textbf{Frchet Inception Distance}}
We additionally report the comparison of Frchet Inception Distance (FID) in Table \ref{tab:cifar-10-fid-results}. 
We can see the ranks are consistent with the Inception scores in Table 2 and our method is superior in most cases.


\paragraph{\textbf{Markov Chain Visualization on CelebA}}
We demonstrate two Markov chains on CelebA with different MCMC sampling methods of WGAN in Figure \ref{fig:celeba-chains}. We can see that on CelebA, the acceptance ratio of MH-GAN becomes much higher than that on CIFAR-10. Nevertheless, the sample quality is still relatively low. In comparison, the gradient-based method can gradually refine the samples with Langevin steps, and our REP-GAN can alleviate image artifacts with MH correction steps.



\end{document}
