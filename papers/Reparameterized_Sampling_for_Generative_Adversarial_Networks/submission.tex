% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsthm}
\usepackage[makeroom]{cancel}
\usepackage[shortlabels]{enumitem}
\usepackage[misc]{ifsym}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%

% \usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}

\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{float}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\db}{\mathbf{b}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\p}{\mathbf{p}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\bflambda}{\bm{\lambda}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bfm}{\mathbf{m}}%\bm has been defined as a font
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bdelta}{\bm{\delta}}
\newcommand{\bvarepsilon}{\bm{\varepsilon}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\brho}{\bm{\rho}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bnu}{\bm{\nu}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bPi}{\bm{\Pi}}
\newcommand{\bGamma}{\bm{\Gamma}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bTheta}{\bm{\Theta}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\norm}[1]{\ensuremath{\left\| #1 \right\|}}
\newcommand{\trace}[1]{\ensuremath{\mathrm{tr}\left(#1\right)}}
\newcommand{\spn}[1]{\ensuremath{\mathrm{span}\left(#1\right)}}
\newcommand{\spark}{\mbox{spark}}
\newcommand{\card}{\mbox{card}}
\newcommand{\supp}{\mbox{supp}}
\newcommand{\grad}{\mbox{grad}}
\newcommand{\prox}{\mbox{prox}}
\renewcommand{\div}{\mbox{div}}
%\renewcommand{\span}{\mbox{span}}
\newcommand{\sign}{\mbox{sign}}
%\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\vol}{\mbox{vol}}
\renewcommand{\d}{\mbox{d }}
%\newcommand{\D}{\mbox{D}}
\renewcommand{\Pr}{\textup{\mbox{Pr}}}
\renewcommand{\vec}{\textup{\mbox{vec}}}
\newcommand{\<}{\left\langle}
\renewcommand{\>}{\right\rangle}
\newcommand{\lbar}{\left\|}
\newcommand{\rbar}{\right\|}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\red}{\textcolor[rgb]{1.00,0.00,0.00}}
\newcommand{\hongyang}{\textcolor[rgb]{0.00,1.00,0.00}}
\DeclareMathOperator*{\Ber}{Ber}
\DeclareMathOperator*{\Unif}{Unif}
\DeclareMathOperator*{\Range}{Range}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\rank}{rank} \DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\Prox}{Prox}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Diag}{Diag}
% \DeclareMathOperator*{\lim}{lim}
\newcommand{\dspint}{\displaystyle \int}
\newcommand{\dspfrac}{\displaystyle \frac}
\newcommand{\dd}{\mbox{d}}
\newcommand{\orthc}{\mathbf{orth}_c}
\newcommand{\orthr}{\mathbf{orth}_r}
% \newtheorem{theorem}{Theorem}
% \newtheorem{assumption}{Assumption}

\def\bpi{{\boldsymbol{\pi}}}
\usepackage{filecontents}

% \usepackage{fancyhdr}
% \chead{GAN Sampling with Reparameterized Markov Chains}




\begin{filecontents}{main.bib}
@article{li2017approximate,
  title={Approximate inference with amortised mcmc},
  author={Li, Yingzhen and Turner, Richard E and Liu, Qiang},
  journal={arXiv preprint arXiv:1702.08343},
  year={2017}
}

@inproceedings{hoffman2017learning,
  title={Learning deep latent Gaussian models with Markov chain Monte Carlo},
  author={Hoffman, Matthew D},
  booktitle={ICML},
  year={2017}
}

@inproceedings{salimans2015markov,
  title={Markov chain monte carlo and variational inference: Bridging the gap},
  author={Salimans, Tim and Kingma, Diederik and Welling, Max},
  booktitle={ICML},
  year={2015}
}

@article{roberts1996exponential,
  title={Exponential convergence of Langevin distributions and their discrete approximations},
  author={Roberts, Gareth O and Tweedie, Richard L and others},
  journal={Bernoulli},
  volume={2},
  number={4},
  pages={341--363},
  year={1996},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@book{gelman2013bayesian,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year={2013},
  publisher={CRC press}
}

@article{salimans2016improved,
  title={Improved techniques for training {GANs}},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={NeurIPS},
  year={2016}
}

@article{tang2020lessons,
  title={Lessons Learned From the Training of GANs on Artificial Datasets},
  author={Tang, Shichang},
  journal={IEEE Access},
  volume={8},
  pages={165044--165055},
  year={2020},
  publisher={IEEE}
}

@article{neal2010MCMC,
  title={{MCMC} using {Hamiltonian} dynamics},
  author={Neal, Radford M and others},
  journal={Handbook of markov chain monte carlo},
  volume={54},
  pages={113-162},
  year={2010}
}
@article{ben1999change,
  title={The change-of-variables formula using matrix volume},
  author={Ben-Israel, Adi},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={21},
  number={1},
  pages={300--312},
  year={1999},
  publisher={SIAM}
}

@inproceedings{wang2018meta,
  title={Meta-learning MCMC proposals},
  author={Wang, Tongzhou and Wu, Yi and Moore, Dave and Russell, Stuart J},
  booktitle={NeurIPS},
  year={2018}
}



@article{gemici2016normalizing,
  title={Normalizing flows on {Riemannian} manifolds},
  author={Gemici, Mevlana C and Rezende, Danilo and Mohamed, Shakir},
  journal={arXiv preprint arXiv:1611.02304},
  year={2016}
}

@article{lucic2018gans,
  title={Are {GANs} created equal? a large-scale study},
  author={Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
  journal={NeurIPS},  
  year={2018}
}

@article{levy2017generalizing,
  title={Generalizing {Hamiltonian Monte Carlo} with Neural Networks},
  author={Levy, Daniel and Hoffman, Matthew D and Sohl-Dickstein, Jascha},
  journal={ICLR},
  year={2018}
}

@article{song2017nice,
  title={{A-NICE-MC}: Adversarial training for {MCMC}},
  author={Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
  journal={NeurIPS},  
  year={2017}
}


@article{hu2017unifying,
  title={On unifying deep generative models},
  author={Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric P},
  journal={ICLR},
  year={2018}
}


@article{hinton2002training,
  title={Training products of experts by minimizing Contrastive Divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  volume={14},
  number={8},
  pages={1771--1800},
  year={2002},
  publisher={MIT Press}
}

@article{levine2001implementations,
  title={Implementations of the Monte Carlo {EM} algorithm},
  author={Levine, Richard A and Casella, George},
  journal={Journal of Computational and Graphical Statistics},
  volume={10},
  number={3},
  pages={422--439},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{neal1992connectionist,
  title={Connectionist learning of belief networks},
  author={Neal, Radford M},
  journal={Artificial intelligence},
  volume={56},
  number={1},
  pages={71--113},
  year={1992},
  publisher={Elsevier}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={ICLR},
  year={2014}
}

@article{lin2018pacgan,
  title={{PACGAN}: The power of two samples in generative adversarial networks},
  author={Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
  journal={NeurIPS},
  year={2018}
}

@article{metz2016unrolled,
  title={Unrolled generative adversarial networks},
  author={Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  journal={ICLR},
  year={2017}
}

@article{song2020discriminator,
  title={Discriminator contrastive divergence: semi-Amortized generative modeling by exploring energy of the discriminator},
  author={Song, Yuxuan and Ye, Qiwei and Xu, Minkai and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2004.01704},
  year={2020}
}

@article{tanaka2019discriminator,
  title={Discriminator optimal transport},
  author={Tanaka, Akinori},
  journal={NeurIPS},
  year={2019}
}

@article{kingma2018glow,
  title={Glow: Generative flow with invertible 1x1 convolutions},
  author={Kingma, Durk P and Dhariwal, Prafulla},
  journal={NeurIPS},
  year={2018}
}

@article{dinh2016density,
  title={Density estimation using real {NVP}},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={ICLR},
  year={2017}
}

@article{miyato2018spectral,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={ICLR},
  year={2018}
}

@article{arjovsky2017wasserstein,
  title={Wasserstein {GAN}},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={ICML},
  year={2017}
}

@article{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  journal={NeurIPS},
  year={2019}
}

@article{brock2018large,
  title={Large scale {GAN} training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={ICLR},
  year={2019}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={NeurIPS},
  year={2014}
}

@article{azadi2019discriminator,
  title={Discriminator rejection sampling},
  author={Azadi, Samaneh and Olsson, Catherine and Darrell, Trevor and Goodfellow, Ian and Odena, Augustus},
  journal={ICLR},
  year={2019}
}

@article{turner2019metropolis,
  title={{Metropolis-Hastings} generative adversarial networks},
  author={Turner, Ryan and Hung, Jane and Saatci, Yunus and Yosinski, Jason},
  journal={ICML},
  year={2019}
}

@article{hoffman2019neutra,
  title={NeuTra-lizing bad geometry in {Hamiltonian Monte Carlo} using neural transport},
  author={Hoffman, Matthew and Sountsov, Pavel and Dillon, Joshua V and Langmore, Ian and Tran, Dustin and Vasudevan, Srinivas},
  journal={arXiv preprint arXiv:1903.03704},
  year={2019}
}

@article{marzouk2016introduction,
  title={An introduction to sampling via measure transport},
  author={Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  journal={arXiv preprint arXiv:1602.05023},
  year={2016}
}

@article{titsias2017learning,
  title={Learning model reparametrizations: implicit variational inference by fitting {MCMC} distributions},
  author={Titsias, Michalis K},
  journal={arXiv preprint arXiv:1708.01529},
  year={2017}
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}

@article{gulrajani2017improved,
  title={Improved training of {Wasserstein} {GANs}},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={NeurIPS},
  year={2017}
}

@article{che2020your,
  title={Your {GAN} is secretly an energy-based model and you should use discriminator driven latent sampling},
  author={Che, Tong and Zhang, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
  journal={ICML},
  year={2020}
}
\end{filecontents}


\begin{document}

% \title{Efficient Sampling for Generative Adversarial Networks with Reparameterized Markov Chains}
\title{Reparameterized Sampling for Generative Adversarial Networks}


% \title{Contribution Title\thanks{Supported by organization x.}}
% /%
% \titlerunning{Reparameterized Sampling for Generative Adversarial Networks}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Yifei Wang\inst{1}
% \orcidID{0000-1111-2222-3333} 
\and
Yisen Wang\inst{2}(\Letter)
% \orcidID{1111-2222-3333-4444} 
\and
Jiansheng Yang\inst{1}
% \orcidID{2222--3333-4444-5555}
\and
Zhouchen Lin\inst{2}
}
%
\authorrunning{Y. Wang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

\institute{
School of Mathematical Sciences, Peking University, Beijing 100871, China
% \email{\{abc,lncs\}@uni-heidelberg.de}
\email{yifei\_wang@pku.edu.cn}, % 
\email{yjs@math.pku.edu.cn}
\and 
Key Lab. of Machine Perception (MoE), School of EECS, Peking University, Beijing 100871, China \\
% Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germanyx
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{yifei\_wang@pku.edu.cn}
\email{yisen.wang@pku.edu.cn},
\email{zlin@pku.edu.cn}
}

\toctitle{Reparameterized Sampling for Generative Adversarial Networks}

\tocauthor{Yifei Wang, Yisen Wang, Jiansheng Yang, Zhouchen Lin }

\maketitle

\begin{abstract}
Recently, sampling methods have been successfully applied to enhance the sample quality of Generative Adversarial Networks (GANs). However, in practice, they typically have poor sample efficiency because of the independent proposal sampling from the generator. 
In this work, we propose REP-GAN, a novel sampling method that allows general dependent proposals by REParameterizing the Markov chains
into the latent space of the generator.
Theoretically, we show that our reparameterized proposal admits a closed-form Metropolis-Hastings acceptance ratio.
Empirically, extensive experiments on synthetic and real datasets demonstrate that our REP-GAN largely improves the sample efficiency and obtains better sample quality simultaneously.
\keywords{Generative Adversarial Networks  \and Sampling \and Markov Chain Monte Carlo \and Reparameterization.}
\end{abstract}

\section{Introduction}

Generative Adversarial Networks (GANs) \cite{goodfellow2014generative} have achieved a great success on generating realistic images in recent years \cite{karras2019style,brock2018large}. Unlike previous models that explicitly parameterize the data distribution, GANs rely on an alternative optimization between a generator and a discriminator to learn the data distribution implicitly. However, in practice, samples generated by GANs still suffer from problems such as mode collapse and bad artifacts. 
% Various variants have been proposed to alleviate these issues \cite{lucic2018gans}.



% With an optimal discriminator, the generative distribution is pushed towards the data distribution by minimizing their Jensen-Shannon (JS) divergence. 
Recently, sampling methods have shown promising results on enhancing the sample quality of GANs by making use of 
the information in the discriminator.
% Conventionally, after GAN training, the discriminator is discarded, and only the generator is left for image generation. 
% While the sampling methods utilize the density ratio information (implied by the discriminator) to close the gap between the generative distribution and the data distribution.
In the alternative training scheme of GANs, the generator only performs a few updates for the inner loop and has not fully utilized the density ratio information estimated by the discriminator. Thus, after GAN training, the sampling methods propose to further utilize this information to bridge the gap between the generative distribution and the data distribution in a fine-grained manner.
For example, DRS \cite{azadi2019discriminator} applies rejection sampling, and MH-GAN \cite{turner2019metropolis} adopts Markov chain Monte Carlo (MCMC) sampling for the improved sample quality of GANs. Nevertheless, these methods still suffer a lot from the sample efficiency problem. For example, as will be shown in Section \ref{sec:experiments}, MH-GAN's average acceptance ratio on CIFAR10 can be lower than 5\%, which makes the Markov chains slow to mix. As MH-GAN adopts an \emph{independent} proposal $q$, i.e., $q(\bx'|\bx)=q(\bx')$, the difference between samples can be so large that the proposal gets rejected easily. 

\begin{figure}[t]
\centering
\includegraphics[width=.9\linewidth]{mcmc.pdf}
\caption{Illustration of REP-GAN's reparameterized proposal with two pairing Markov chains, one in the latent space $\cZ$, and the other in the sample space $\cX$. }
\label{fig:pairing-chains}
\end{figure}


To address this limitation, we propose to generalize the independent proposal to a general \emph{dependent} proposal $q(\bx'|\bx)$. To the end, the proposed sample can be a refinement of the previous one, which leads to a higher acceptance ratio and better sample quality. We can also balance between the exploration and exploitation of the Markov chains by tuning the step size. However, it is hard to design a proper dependent proposal in the high dimensional sample space $\cX$ because the energy landscape could be very complex \cite{neal2010MCMC}. 

Nevertheless, we notice that the generative distribution $p_g(\bx)$ of GANs is implicitly defined as the push-forward of the latent prior distribution $p_0(\bz)$, and designing proposals in the low dimensional latent space is generally much easier. Hence, GAN's latent variable structure motivates us to design a \emph{structured dependent proposal} with two pairing Markov chains, one in the sample space $\cX$ and the other in the latent space $\cZ$. As shown in Figure \ref{fig:pairing-chains}, given the current pairing samples $(\bz_k,\bx_k)$, we draw the next proposal $\bx'$ in a bottom-to-up way: 1) drawing a latent proposal $\bz'$ following $q(\bz'|\bz_k)$; 2) pushing it forward through the generator and getting the sample proposal $\bx'=G(\bz')$; 3) assigning $\bx_{k+1}=\bx'$ if the proposal $\bx'$ is accepted, otherwise rejected $\bx_{k+1}=\bx_k$. By utilizing the underlying structure of GANs, the proposed reparameterized sampler becomes more efficient in the low-dimensional latent space. 
We summarize our main contributions as follows:
\begin{itemize}
    \item We propose a structured dependent proposal of GANs, which reparameterizes the sample-level transition $\bx\to\bx'$ into the latent-level  $\bz\to\bz'$ with two pairing Markov chains. We prove that our reparameterized proposal admits
    % under certain regularity conditions, it converges to the true data distribution with 
    a tractable acceptance criterion.
    \item Our proposed method, called REP-GAN, serves as a unified framework for the existing sampling methods of GANs. It provides a better balance between exploration and exploitation by the structured dependent proposal, and also corrects the bias of Markov chains by the acceptance-rejection step. 
    % under certain regularity conditions.
    % It includes all three effective mechanisms for the sampling of GANs, the latent-level transition, the Markov chain
    \item Empirical results demonstrate that REP-GAN achieves better image quality and much higher sample efficiency than the state-of-the-art methods on both synthetic and real datasets. 
    % The structured dependent proposal and the MH rejection step both help a lot for the final performance.
\end{itemize}

% Another major contribution of this work is that we first prove that this structured dependent proposal of GANs admits a closed-form Metropolis-Hastings (MH) acceptance criterion for general latent proposals. When the latent Markov chain adopts an independent proposal, our MH criterion reduces to that of MH-GAN \cite{turner2019metropolis}. When we take a Langevin latent proposal and omit the MH test, our method then reduces to DDLS \cite{che2020your}. As a result, MH-GAN and DDLS are both special cases of our framework. Our methods are more sample efficient than MH-GAN because of the dependent proposal, and more unbiased than DDLS \cite{che2020your} with the MH test for bias correction. See Table \ref{tab:compare-sample-methods} for a clear comparison. Empirical results also demonstrate our proposed REP-GAN achieves better image quality and much higher sample efficiency than state-of-the-art methods. 

\begin{table}[t]
    \caption{Comparison of sampling methods for GANs in terms of three effective sampling mechanisms. 
    % The table lists of GANs. Our REP-GAN includes all three mechanisms, and the other previous methods can be seen as our ablated special cases.
    }
    \label{tab:compare-sample-methods}
    \begin{center}
    \vspace{-0.1 in}
    \begin{tabular}{lccc}
    \toprule
    {\bf Method} & {\bf Rejection step~~} & {\bf Markov chain~~} & {\bf Latent gradient proposal} \\
    \midrule 
    GAN & \xmark & \xmark & \xmark \\
    DRS \cite{azadi2019discriminator} & \checkmark & \xmark & \xmark \\
    MH-GAN \cite{turner2019metropolis} & \checkmark & \checkmark & \xmark \\
    DDLS \cite{che2020your} & \xmark & \checkmark & \checkmark \\
    % DCD \cite{song2020discriminator} & ~~\xmark$^*$ & \checkmark & \checkmark \\
    REP-GAN (ours) & \checkmark & \checkmark & \checkmark \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\section{Related Work}

Although GANs are able to synthesize high-quality images, the minimax nature of GANs makes it quite unstable, which usually results in degraded sample quality. 
A vast literature has been developed to fix the problems of GANs ever since, including network modules \cite{miyato2018spectral}, training mechanisms \cite{metz2016unrolled} and  objectives \cite{arjovsky2017wasserstein}.

% A series of works like DCGAN \cite{radford2015unsupervised} and SNGAN \cite{miyato2018spectral} propose more effective network modules for GANs. Methods like Unrolled GAN \cite{metz2016unrolled} and PACGAN \cite{lin2018pacgan}, try to fix mode collapse with new training mechanisms. Other methods like WGAN \cite{arjovsky2017wasserstein} and WGAN-GP \cite{gulrajani2017improved} instead propose alternative objectives for better training stability.

Moreover, there is another line of work using sampling methods to improve the sample quality of GANs. DRS \cite{azadi2019discriminator} firstly proposes to use rejection sampling. MH-GAN \cite{turner2019metropolis} instead uses the Metropolis-Hasting (MH) algorithm with an independent proposal. DDLS \cite{che2020your} and DCD \cite{song2020discriminator} apply  gradient-based proposals by viewing GAN as an energy-based model. \cite{tanaka2019discriminator} proposes a similar gradient-based method named DOT from the perspective of optimal transport. 

Different from them, our REP-GAN introduces a structured dependent proposal through latent reparameterization, and includes all three effective sampling mechanisms, the Markov Chain Monte Carlo method, the acceptance-rejection step, and the latent gradient-based proposal, to further improve the sample efficiency. As shown in Table \ref{tab:compare-sample-methods}, many existing works are special cases of our REP-GAN. 

Our method also belongs to the thread of works that combine MCMC and neural networks for better sample quality. Previously, some works combine variational autoencoders \cite{kingma2013auto} and MCMC to bridge the amorization gap \cite{salimans2015markov,hoffman2017learning,li2017approximate}, while others directly learn a neural proposal function for MCMC \cite{song2017nice,levy2017generalizing,wang2018meta}. Our work instead reparameterizes the high-dimensional sample-level transition into a simpler low-dimensional latent space via the learned generator network. 
% which shares the same spirit with previous works?? 


% A more closely related approach to ours is to reparameterize a complex probabilistic measure into a simpler measure via neural networks in MCMC \cite{titsias2017learning,hoffman2019neutra}. 
% w have a close connection to 
% also have a close connection to the literature that bridges Variational Inference and MCMC. 

% Our MH criterion reduces to that of MH-GAN \cite{turner2019metropolis} when the latent Markov chain adopts an independent proposal. When we take the latent  Langevin proposal and ignores the MH rejection step, our method then reduces to DDLS \cite{che2020your}. 

% In fact, sampling methods have long stood as a classical method for the learning and inference of various probabilistic models, with classic examples including Sigmoid Belief Network \cite{neal1992connectionist}, Restricted Boltzmann Machine

% In fact, GANs can be considered as a special kind of variational inference \cite{hu2017unifying}. The marriage between variational inference and sampling methods has a long history in learning probabilistic models, to list a few, Monte Carlo EM \cite{levine2001implementations}, Contrastive Divergence \cite{hinton2002training}, etc. In the era of deep learning, \cite{salimans2015markov} and \cite{hoffman2017learning} develop methods to leverage MCMC to improve the posterior of VAE \cite{kingma2013auto}, while \cite{song2017nice} and \cite{levy2017generalizing} adopt neural networks for better proposals in MCMC. 

% density ratio

\section{Background}
% In this section, we briefly introduce the notations and backgrounds.

% \paragraph{GAN}
GANs model the data distribution $p_d(\bx)$ implicitly with a generator $G: \cZ\to\cX$ mapping from a low-dimensional latent space $\cZ$ to a high-dimensional sample space $\cX$,
% $G:\bbR^m\to\bbR^n (m<n)$,
\begin{equation}
    \bx=G(\bz), \quad \bz\sim p_0(\bz),
\end{equation}
where the sample $\bx$ follows the generative distribution $p_g(\bx)$ and the latent variable $\bz$ follows the prior distribution $p_0(\bz)$, e.g., a standard normal distribution $\cN(\bzero,\bI)$. In GANs, a discriminator  $D:\cX\to[0,1]$ is learned to distinguish samples from $p_d(\bx)$ and $p_g(\bx)$ in an adversarial way
\begin{equation}
    \min_G\max_D\bbE_{\bx\sim p_d(\bx)}\log(D(\bx))+\bbE_{\bz\sim p_0(\bz)}\log(1-D(G(\bz))).
\end{equation}
\cite{goodfellow2014generative} point out that an optimal discriminator $D$ implies the density ratio between the data and generative distributions 
\begin{equation}
D(\mathbf{x})=\frac{p_{d}(\mathbf{x})}{p_{d}(\mathbf{x})+p_{g}(\mathbf{x})} ~~\Rightarrow ~~ \frac{p_d(\bx)}{p_g(\bx)}=\frac{1}{D(\bx)^{-1}-1}.
\label{eq:optimal-D}
\end{equation}

% \paragraph{MCMC}
% \label{sec:mcmc}
Markov Chain Monte Carlo (MCMC) refers to a kind of sampling methods that draw a chain of samples $\bx_{1:K}\in\cX^K$ from a target distribution $p_t(\bx)$. We denote the initial distribution as $p_0(\bx)$ and the proposal distribution as $q(\bx^\prime|\bx_k)$. With the Metropolis-Hastings (MH) algorithm, we accept the proposal $\bx^\prime\sim q(\bx'|\bx_k)$ with probability
\begin{equation}
\alpha\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_t\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}|\bx^\prime\right)}{p_t\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right) \in[0,1].   
\label{eq:mh-accept} 
\end{equation}
If $\bx^{\prime}$ is accepted, $\bx_{k+1}=\bx^{\prime}$, otherwise $\bx_{k+1}=\bx_{k}$. Under mild assumptions, the Markov chain is guaranteed to converge to $p_t(\bx)$ as $K\to\infty$. In practice, the sample efficiency of MCMC crucially depends on the proposal distribution to trade off between exploration and exploitation. 

\section{The Proposed REP-GAN}
\label{sec:emhgan}
% For GANs, typically, the discriminator is thrown away after training, and only the generator is kept for generating samples. The idea behind sampling methods is to utilize the density ratio information in the discriminator to further enhance the generator. 
In this section, we first review MH-GAN and point out the limitations. We then propose our structured dependent proposal to overcome these obstacles, and finally discuss its theoretical properties as well as practical implementations.

\subsection{From Independent Proposal to Dependent Proposal}

MH-GAN \cite{turner2019metropolis} first proposes to improve GAN sampling with MCMC. Specifically, given a perfect discriminator $D$ and a descent (but imperfect) generator $G$ after training, they take the data distribution $p_d(\bx)$ as the target distribution and use the generator distribution $p_g(\bx)$ as an independent proposal
\begin{equation}
\mathbf{x}^{\prime} \sim q\left(\mathbf{x}^{\prime} | \mathbf{x}_{k}\right)=q\left(\mathbf{x}^{\prime}\right)=p_g(\bx').
\label{eq:independence-proposal}
\end{equation}
With the MH criterion (Eqn. \eqref{eq:mh-accept}) and the density ratio (Eqn. \eqref{eq:optimal-D}), we should accept $\bx'$
with probability
\begin{equation}
\alpha_{\rm MH}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}\right)}{p_d\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}\right)}\right)=\min \left(1, \frac{D\left(\mathbf{x}_{k}\right)^{-1}-1}{D\left(\mathbf{x}^{\prime}\right)^{-1}-1}\right).
\label{eq:mhgan-alpha}
\end{equation}
% As a result, the Markov chain is guaranteed to converge to the true data distribution $p_d(\bx)$ under certain regularity conditions.
% Because $\alpha_{\rm MH}(\bx',\bx_k)$ only involves the discriminator scores, it is computationally tractable. 
% According to the MH algorithm, the resulting Markov chains $\{\bx_{1:K}\}_{\rm MH}$ will marginally converge to the data distribution $p_d(\bx)$. 
% Empirical results show that MH-GAN help improve the sample quality of GAN.
However, to achieve tractability, MH-GAN adopts an independent proposal $q(\bx^\prime)$ with poor sample efficiency. As the proposed sample $\bx'$ is independent of the current sample $\bx_k$, the difference between the two samples can be so large that it results in a very low acceptance probability. Consequently, samples can be trapped in the same place for a long time, leading to a very slow mixing of the chain. 

A natural solution is to take a \textit{dependent} proposal $q(\bx'|\bx_k)$ that will propose a sample $\bx'$ close to the current one $\bx_k$, which is more likely to be accepted. 
% and lead to a reasonable acceptance ratio. 
Nevertheless, the problem of such a dependent proposal is that its MH acceptance criterion
\begin{equation}
    \alpha_{\rm DEP}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right),
    \label{eq:dependent-proposal-acceptance}
\end{equation}
is generally intractable because the data density $p_d(\bx)$ is unknown. Besides, it is hard to design a proper dependent proposal $q(\bx'|\bx_k)$ in the high dimensional sample space $\cX$ with complex landscape. These obstacles prevent us from adopting a dependent proposal that is more suitable for MCMC.

\subsection{A Tractable Structured Dependent Proposal with Reparameterized Markov Chains}
As discussed above, the major difficulty of a general dependent proposal $q(\bx'|\bx_k)$ is to compute the MH criterion. We show that it can be made tractable by considering an additional pairing Markov chain in the latent space.

As we know, samples of GANs lie in a low-dimensional manifold induced by the push-forward of the latent variable. Suppose that at the $k$-th step of the Markov chain, we have a GAN sample $\bx_k$ with latent $\bz_k$. Instead of drawing a sample $\bx'$ directly from a sample-level proposal distribution $q(\bx'|\bx_k)$, we first draw a latent proposal $\bz'$ from a dependent latent proposal distribution $q(\bz'|\bz_k)$. Afterward, we push the latent $\bz'$ forward through the generator and get the output $\bx'$ as our sample proposal. 
% In summary, we have
% \begin{equation}
%     1)\,\bx_k=G(\bz_k);
%     ~~2)~\bz'\sim q(\bz'|\bz_k);
%     ~~3)~\bx'=G(\bz').
% \end{equation}    

As illustrated in Figure \ref{fig:pairing-chains}, our bottom-to-up proposal relies on the transition reparameterization with two pairing Markov chains in the sample space $\cX$ and the latent space $\cZ$. Hence we call it a REP (reparameterized) proposal. Through a learned generator, we transport the transition $\bx_k\to\bx'$ in the high dimensional space $\cX$ into the low dimensional space $\cZ$, $\bz_k\to\bz'$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. For example, the latent target distribution is nearly standard normal when the generator is nearly perfect.
%  we can reparameterized the REP proposal distribution as follows
% Note that for a push-forward mapping $G:\bbR^m\to\bbR^n (m<n)$.
In fact, under mild conditions, the REP proposal distribution $q_{\rm REP}(\bx'|\bx_k)$ and the latent proposal distribution $q(\bz'|\bz_k)$ are tied with the following change of variables \cite{gemici2016normalizing,ben1999change}
\begin{equation}
    \log q_{\rm REP}(\bx'|\bx_k)= \log q(\bx'|\bz_k)
    =\log q(\bz'|\bz_k) - \frac{1}{2}\log\det J_{\bz'}^\top J_{\bz'},
    \label{eq:cmc-proposal-change-of-variable}
\end{equation}
where $J_\bz$ denotes the Jacobian matrix of the push-forward $G$ at $\bz$, i.e., $\left[J_\bz\right]_{ij} = \partial\,\bx_i/\partial\,\bz_j, \bx=G(\bz)$.

% In this way, through a learned generator, we transport the transition in the high dimensional space $\cX$ into the low dimensional space $\cZ$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. Previous works \cite{titsias2017learning,hoffman2019neutra} also develop similar sampling methods with invertible models that have explicit density, like flows \cite{dinh2016density,kingma2018glow}. In this work, we first prove that it is also tractable to conduct such model-based reparameterization \cite{marzouk2016introduction} for implicit models like GANs. 

Nevertheless, it remains unclear whether we can perform the MH test to decide the acceptance of the proposal $\bx'$. Note that a general dependent proposal distribution does not meet a tractable MH acceptance criterion (Eqn. \eqref{eq:dependent-proposal-acceptance}). Perhaps surprisingly, 
% another favorable property of our REP proposal is that its 
it can be shown that with our structured REP proposal, 
the MH acceptance criterion is tractable for general latent proposals $q(\bz'|\bz_k)$. 

\begin{theorem}
\label{thm:emh}
Consider a Markov chain of GAN samples $\bx_{1:K}$ with initial distribution $p_g(\bx)$. For step $k+1$, we accept our REP proposal $\bx'\sim q_{\rm REP}(\bx'|\bx_k)$ with probability
\begin{equation}
\alpha_{\rm REP}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)
% =\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right)  q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right)  q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right)
% =\min \left(1, \frac{p_0(\bz')q(\bz_k|\bz')(D(\bx_k)^{-1}-1)}{p_0(\bz_k)q(\bz'|\bz_k)(D(\bx')^{-1}-1)}\right),
=\min \left(1,~\frac{p_0(\bz')q(\bz_k|\bz')}{p_0(\bz_k)q(\bz'|\bz_k)}\cdot
\frac{D(\bx_k)^{-1}-1}{D(\bx')^{-1}-1}\right),
\label{eq:cmc-mh-acceptance}
\end{equation}
i.e. let $\bx_{k+1}=\bx'$ if $\bx'$ is accepted and $\bx_{k+1}=\bx_k$ otherwise. Further assume the chain is irreducible, aperiodic and not transient. Then, according to the Metropolis-Hastings algorithm, the stationary distribution of this Markov chain is the data distribution $p_d(\bx)$ \cite{gelman2013bayesian}.
\label{thm:main}
\end{theorem}

\begin{proof}
Note that similar to Eqn \eqref{eq:cmc-proposal-change-of-variable},  we also have the change of variables between $p_g(\bx)$ and $p_0(\bz)$,
\begin{equation}
\log p_g(\bx)\rvert_{\bx=G(\bz)}=\log p_0(\bz) - \frac{1}{2} \log \det J_{\bz}^\top J_{\bz}.
\label{eq:generator-change-of-variable}
\end{equation}
According to \cite{gelman2013bayesian}, the assumptions that the chain is irreducible, aperiodic, and not transient make sure that the chain has a unique stationary distribution, and the MH algorithm ensures that this stationary distribution equals to the target distribution $p_d(\bx)$.
Thus we only need to show that the MH criterion in Eqn. \eqref{eq:cmc-mh-acceptance} holds. Together with Eqn. \eqref{eq:optimal-D}, \eqref{eq:dependent-proposal-acceptance} and \eqref{eq:cmc-proposal-change-of-variable}, we have
\begin{equation}
\begin{aligned}
\alpha_{\rm REP}(\bx',\bx_k)
&=\frac{p_d\left(\mathbf{x}^{\prime}\right)  q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right)  q\left(\mathbf{x}^{\prime}|\bx_k\right)}
=\frac{{p_d\left(\mathbf{x}^{\prime}\right)}  q(\bz_k|\bz')
% \left|\frac{\partial\,\bx_k}{\partial\, \bz_k}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
{p_g(\bx_k)}p_g(\bx')}{{p_d\left(\mathbf{x}_{k}\right)}  q(\bz'|\bz_k)
\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}
% \left|\frac{\partial\,\bx'}{\partial\, \bz'}\right|^{-1}p_g(\bx_k)
{p_g(\bx')}p_g(\bx_k)}\\
&=\frac{q(\bz_k|\bz'){\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}}p_0(\bz'){\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}}(D(\bx_k)^{-1}-1)}
{q(\bz'|\bz_k){\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}}p_0(\bz_k){\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}}(D(\bx')^{-1}-1)}\\
&=\frac{p_0(\bz')q(\bz_k|\bz')(D(\bx_k)^{-1}-1)}{p_0(\bz_k)q(\bz'|\bz_k)(D(\bx')^{-1}-1)}.
\end{aligned}
\label{eq:dcgan-cmc-alpha}
\end{equation}
Hence the proof is completed.
\end{proof}

The theorem above demonstrates the following favorable properties of our method:
\begin{itemize}
\item The discriminator score ratio is the same as $\alpha_{\rm MH}(\bx',\bx_k)$, but MH-GAN is restricted to a specific independent proposal. Our method instead works for any latent proposal $q(\bz'|\bz_k)$. When we take $q(\bz'|\bz_k)=p_0(\bz')$, our method reduces to MH-GAN. 
\item Compared to $\alpha_{\rm DEP}(\bx',\bx_k)$ of a general dependent proposal (Eqn. \eqref{eq:dependent-proposal-acceptance}), the unknown data distributions terms are successfully cancelled in the reparameterized acceptance criterion.
%  During the reparameterization, the unknown Jacobian terms are also canceled. 
\item The reparameterized MH acceptance criterion becomes tractable as it only involves the latent priors, the latent proposal distributions, and the discriminator scores.
\end{itemize}

Combining the REP proposal $q_{\rm REP}(\bx'|\bx_k)$ and its tractable MH criterion $\alpha_{\rm REP}(\bx',\bx_k)$, we have developed a novel sampling method for GANs, coined as REP-GAN. See Appendix \ref{algo:REP-GAN} for a detailed description.
% Recently, researchers have developed an interest to bridge MCMC algorithms and neural networks for better sample efficiency. 
Moreover, our method can serve as a general approximate inference technique for Bayesian models by bridging MCMC and GANs. Previous works \cite{marzouk2016introduction,titsias2017learning,hoffman2019neutra} also propose to avoid the bad geometry of a complex probability measure by reparameterizing the Markov transitions into a simpler measure. However, these methods are limited to explicit invertible mappings without dimensionality reduction.
In our work, we first show that it is also tractable to conduct such model-based reparameterization with implicit models like GANs. 

% A more distantly related approach is to directly learn a neural proposal \cite{song2017nice,levy2017generalizing,wang2018meta}.


% \subsection{Transition Reparameterization} 
% According to the law of the unconscious statistician, 
% our Markov chain in the sample space $\cX$ can be reparameterized as a latent Markov Chain followed by push-forward at each step. 
% Specifically, given a pairing pair $(\bz_k,\bx_k)$ in the two chains, the transition $\bx_k\to\bx'$ following $q_{\rm REP}(\bx'|\bx_k)$ is equivalent to that we firstly transit from $\bz_k$ to $\bz'$ in the latent chain following $q(\bz'|\bz_k)$, and then push it forward and get $\bx'=G(\bz')$ through the generator $G$. 

% In this way, through a learned generator, we transport the transition in the high dimensional space $\cX$ into the low dimensional space $\cZ$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. Previous works \cite{titsias2017learning,hoffman2019neutra} also develop similar sampling methods with invertible models that have explicit density, like flows \cite{dinh2016density,kingma2018glow}. In this work, we first prove that it is also tractable to conduct such model-based reparameterization \cite{marzouk2016introduction} for implicit models like GANs. 



\subsection{A Practical Implementation}
\label{sec:proposal}
REP-GAN enables us to utilize the vast literature of existing MCMC algorithms \cite{neal2010MCMC} to design dependent proposals for GANs. We take Langevin Monte Carlo (LMC) as an example.
As an Euler-Maruyama discretization of the Langevin dynamics, LMC updates the Markov chain with
\begin{equation}
    \bx_{k+1}=\bx_k+\frac{\tau}{2}\nabla_{\bx}\log p_t(\bx_k)+\sqrt{\tau}\cdot\bvarepsilon,~~\bvarepsilon\sim\cN(\bzero,\bI),
\end{equation}
for a target distribution $p_t(\bx)$. Compared to MH-GAN, LMC utilizes the gradient information to explore the energy landscape more efficiently. However, if we directly take the (unknown) data distribution $p_d(\bx)$ as the target distribution $p_t(\bx)$, LMC does not meet a tractable update rule. 

As discussed above, the reparameterization of REP-GAN makes it easier to design transitions in the low-dimensional latent space. Hence, we instead propose to use LMC for the latent Markov chain.
We assume that the data distribution also lies in the low-dimensional manifold induced by the generator, i.e., 
$\operatorname{Supp}\left(p_d\right)\subset\operatorname{Im}(G)$. This implies that the data distribution $p_d(\bx)$ also has a pairing distribution in the latent space, denoted as $p_t(\bz)$. They are tied with the change of variables
\begin{equation}
\log p_d(\bx)\rvert_{\bx=G(\bz)}=\log p_t(\bz) - \frac{1}{2} \log\det\, J_{\bz}^\top J_{\bz},
\end{equation}
Taking $p_t(\bz)$ as the (unknown) target distribution of the latent Markov chain, we have the following Latent LMC (L2MC) proposal 
\begin{equation}
\begin{aligned}
\bz'&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log p_t(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log\frac{p_t(\bz_k)
% \left|\frac{\partial\,\bx}{\partial\,\bz}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
}{p_0(\bz_k)
% \left|\frac{\partial\,\bx}{\partial\,\bz}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
}+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz+\frac{\tau}{2}\nabla_{\bz}\log\frac{p_d(\bx_k)}{p_g(\bx_k)}+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz-\frac{\tau}{2}\nabla_{\bz}\log(D^{-1}(\bx_k)-1)+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon, \quad\bvarepsilon\sim\cN(\bzero,\bI),
\end{aligned}
\label{eq:langevin}
\end{equation}
where $\bx_k=G(\bz_k)$. As we can see, L2MC is made tractable by our structured dependent proposal with pairing Markov chains. DDLS \cite{che2020your} proposes a similar Langevin proposal by formalizing GANs as an implicit energy-based model, while here we provide a straightforward derivation through reparameterization. Our major difference to DDLS is that REP-GAN also includes a tractable MH correction step (Eqn. \eqref{eq:cmc-mh-acceptance}), which accounts for the numerical errors introduced by the  discretization and ensures that detailed balance holds. 

% \subsection{Algorithm Procedure}
We give a detailed description of the algorithm procedure of our REP-GAN in Algorithm \ref{algo:REP-GAN}.



% Experiments show that it is crucial for the final performance.
%  Note that our method is general, and any off-the-shelf MCMC algorithms can be adopted as the latent proposal methods.

% \subsection{Limitations}
% With the additional MH correction step, our REP-GAN has some extra computational overhead. In practice, it takes about 1.2x time than DDLS \cite{che2020your} with only Langevin Monte Carlo. 
% Another limitation of our method is that its superior performance does not generalize to some variants of GAN objectives, like hinge loss \cite{miyato2018spectral}. In practice, we find the additional MH step causes performance drop with these models. We suspect this is because their discriminators do not explicitly estimate the density ratio like GAN. And this bias may get amplified in the MH correction step, which relies crucially on the estimation of density ratio. A more concrete analysis of this phenomenon remains an open problem for future work.
\subsection{Extension to WGAN}
Our method can also be extended to other kinds of GAN, like Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein}.
The WGAN objective is
\begin{align}
\min_G\max_D\,\mathbb{E}_{\bx\sim p_{d}(\bx)}[D(\bx)]-\mathbb{E}_{\bx\sim p_{g}(\bx)}[D(\bx)],
\end{align}
where $D:\cX\to\bbR$ is restricted to be a Lipschitz function. Under certain conditions, WGAN also implies an approximate estimation of the density ratio \cite{che2020your},
\begin{equation}
D(\bx)\approx\log\frac{p_d(\bx)}{p_g(\bx)}+\text{const}\quad\Rightarrow\quad\frac{p_d(\bx)}{p_g(\bx)}\approx\exp(D(\bx))\cdot \text{const}.
\end{equation}
Following the same derivations as in Eqn. \eqref{eq:dcgan-cmc-alpha} and \eqref{eq:langevin}, we will have the WGAN version of REP-GAN. Specifically, with $\bx_k=G(\bz_k)$, the L2MC proposal follows
\begin{equation}
\bz'=\bz_k+\frac{\tau}{2}\nabla_{\bz}D(\bx_k)+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon, \quad\bvarepsilon\sim\cN(\bzero,\bI),
\label{eq:wgan-proposal}
\end{equation}
and the MH acceptance criterion is 
\begin{equation}
\alpha_{REP-W}\left(\mathbf{x}^{\prime},\mathbf{x}_{k}\right)=\min \left(1,~ \frac{q(\bz_k|\bz')p_0(\bz')}{q(\bz'|\bz_k)p_0(\bz_k)}\cdot\frac{\exp\left(D(\bx')\right)}{\exp\left(D(\bx_k)\right)}\right).
\end{equation}
% For this we name it as REP-WGAN.

\begin{algorithm}[t]
\textbf{Input:} trained GAN with (calibrated) discriminator $D$ and generator $G$, Markov chain length $K$, latent prior distribution $p_0(\bz)$, latent proposal distribution $q(\bz'|\bz_k)$;\\
\textbf{Output:} an improved GAN sample $\bx_K$;
\begin{algorithmic}
% \STATE (Optional) Calibrate the discriminator;
\STATE Draw an initial sample $\bx_1$: 1) draw initial latent $\bz_1\sim p_0(\bz)$ and 2) push forward $\bx_1=G(\bz_1)$;
\FOR{each step $k\in[1,K-1]$}
\STATE Draw a REP proposal $\bx'\sim q_{\rm REP}(\bx'|\bx_k)$: 1) draw a latent proposal $\bz'\sim q(\bz'|\bz_k)$, and 2) push forward $\bx'=G(\bz')$;
\STATE Calculate the MH acceptance criterion $\alpha_{\rm REP}(\bx_k,\bx')$ following Eqn. \eqref{eq:cmc-mh-acceptance};
\STATE Decide the acceptance of $\bx'$ with probability $\alpha_{\rm REP}(\bx_k,\bx')$;
\IF{$\bx'$ is accepted}
\STATE{Let $\bx_{k+1}=\bx', \bz_{k+1}=\bz'$}
\ELSE
\STATE{Let $\bx_{k+1}=\bx_k,\bz_{k+1}=\bz_k$}
\ENDIF
\ENDFOR
\end{algorithmic}
\caption{GAN sampling with Reparameterized Markov chains (REP-GAN)}
\label{algo:REP-GAN}
\end{algorithm}


% \begin{table}[t]\centering
% \caption{Inception Scores and acceptance ratios on CIFAR-10 for DCGAN and WGAN. Computed with 50,000 samples per checkpoint, and averaged over five adjacent checkpoints. }
% \label{tab:cifar-10-incep-results}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Method} &  \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN} \\
% &  Acceptance & Inception Score &  Acceptance & Inception Score  \\
% % \multirow{1}{*}{Method} & Accept & Inception Score \\
% \midrule
% GAN & - & 3.769 & - & 3.557    \\
% % DRS \cite{turner2019metropolis} & 1.2 & 0.02 \\
% MH-GAN \cite{turner2019metropolis} & 0.033 & 3.735 & 0.084 & 3.509 \\
% DDLS \cite{che2020your}  & -  & 3.312 & -  & 3.500  \\
% REP-GAN (ours)  & \textbf{0.363} & \textbf{3.820}  & \textbf{0.447} & \textbf{3.598} \\
% \bottomrule
% \end{tabular}
% \end{table}


\section{Experiments}
\label{sec:experiments}
We evaluate our method  on two synthetic datasets and two real-world image datasets as follows.

% We show our empirical results both on synthetic and real image datasets.

% \subsection{Swill Roll Dataset}
\subsection{Manifold Dataset}
Following DOT \cite{tanaka2019discriminator} and DDLS \cite{che2020your}, we apply REP-GAN to the Swiss Roll dataset, where data samples lie on a Swiss roll manifold in the two-dimensional space. We construct the dataset by scikit-learn with 100,000 samples, and train a WGAN as in \cite{tanaka2019discriminator}, where both the generator and discriminator are fully connected neural networks with leaky ReLU nonlinearities. We optimize the model using the Adam optimizer, with $\alpha = 0.0001, \beta_1 = 0.5, \beta_2 = 0.9$.
After training, we draw 1,000 samples with different sampling methods. Following previous practice, we initialize a Markov chain with a GAN sample, run it for $K=100$ steps, and collect the last example for evaluation.

As shown in Figure \ref{fig:swissroll}, with appropriate step size ($\tau=0.01$), the gradient-based methods (DDLS and REP-GAN) outperform independent proposals (DRS and MH-GAN) by a large margin, while DDLS is more discontinuous on shape compared to REP-GAN. In DDLS, when the step size becomes too large ($\tau=0.1,1$), the numerical error of the Langevin dynamics becomes so large that the chain either collapses or diverges. In contrast, those bad proposals are rejected by the MH correction steps of REP-GAN, which prevents the misbehavior of the Markov chain.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{swiss-roll.pdf}
    \caption{Visualization of samples with different sampling methods on the Swiss Roll dataset. Here tau denotes the Langevin step size in Eqn. \eqref{eq:wgan-proposal}.}
    \label{fig:swissroll}
\end{figure}

% \subsection{Experimental Details}

% \subsubsection{Swiss Roll Dataset}

% \subsection{CIFAR-10 and CelebA}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{25Gaussians-mark.pdf}
    \caption{Visualization of samples with different sampling methods on the 25-Gaussians dataset. Here $\tau$ denotes the Langevin step size in Eqn. \eqref{eq:wgan-proposal}.}
    \label{fig:25gaussians}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{more-modes.pdf}
    \caption{Visualization of the mixture-of-Gaussian experiments with 9x9 (1st row) and 13x13 (2nd row) modes with proper step size $\tau=0.01$. True data points are shown in grey (in background), and generated points are shown in blue. }
    \label{fig:more-modes}
\end{figure}


\subsection{Multi-modal Dataset}

As GANs are known to suffer from the mode collapse problem, we also compare different GAN sampling methods in terms of modeling multi-modal distributions. Specifically, we consider the 25-Gaussians dataset that is widely discussed in previous work \cite{azadi2019discriminator,turner2019metropolis,che2020your}. The dataset is generated by a mixture of twenty-five two-dimensional isotropic Gaussian distributions with variance 0.01, and means separated by 1, arranged in a grid. We train a small GAN with the standard WGAN-GP objective following the setup in  \cite{tanaka2019discriminator}. After training, we draw 1,000 samples with different sampling methods. 

As shown in Figure \ref{fig:25gaussians}, compared to MH-GAN, the gradient-based methods (DDLS and ours) produce much better samples close to the data distribution with proper step size ($\tau=0.01$). Comparing DDLS and our REP-GAN, we can notice that DDLS tends to concentrate so much on the mode centers that its standard deviation can be even smaller than the data distribution. Instead, our method preserves more sample diversity while concentrating on the mode centers. This difference becomes more obvious as the step size $\tau$ becomes larger. When $\tau=0.1$, as marked with blue circles, DDLS samples become so concentrated that some modes are even missed. When $\tau=1$, DDLS samples diverge far beyond the $5\times5$ grid. 
In comparison, our REP-GAN is more stable because the MH correction steps account for the numerical errors caused by large $\tau$.

These distinctions also become even more obvious when we scale to more modes. As shown in Figure \ref{fig:more-modes}, we also compare them w.r.t. mixture of Gaussians with $9\times9$ and $13\times13$ modes, respectively. Under the more challenging scenarios, we can see that the gradient-based methods still consistently outperforms MH-GAN. Besides, our REP-GAN has a more clear advantage over DDLS. Specifically, for $9\times9$ modes, our REP-GAN produces samples that are less noisy, while preserving all the modes. For $9\times9$ modes, DDLS makes a critical mistake that it drops one of the modes. As discussed above, we believe this is because DDLS has a bias towards regions with high probability, while ignoring the diversity of the distribution. In comparison, REP-GAN effectively prevents such bias by the MH correction steps.



\subsection{Real-world Image Dataset}

Following MH-GAN \cite{turner2019metropolis}, we conduct experiments on two real-world image datasets, CIFAR-10 and CelebA, for two models, DCGAN \cite{radford2015unsupervised} and WGAN \cite{arjovsky2017wasserstein}. 
% We utilize the official implementation of MH-GAN
% \footnote{\url{https://github.com/uber-research/metropolis-hastings-GAN}}
% and train models with the same hyperparameters. 
% Our baselines include the vanilla GAN, DRS \cite{azadi2019discriminator}, MH-GAN \cite{turner2019metropolis}, DDLS \cite{che2020your}, and our REP-GAN. 
% For a fair comparison of MCMC methods, 
Following the conventional evaluation protocol, we initialize each Markov chain with a GAN sample, run it for 640 steps, and take the last sample for evaluation. We collect 50,000 samples to evaluate the Inception Score\footnote{For fair comparison, our training and evaluation follows the the official code of MH-GAN \cite{turner2019metropolis}: \url{https://github.com/uber-research/metropolis-hastings-gans}} \cite{salimans2016improved}. 
% each Markov chain starts from a GAN sample and runs 640 steps, and we only take the last sample for evaluation. 
The step size $\tau$ of our L2MC proposal is $0.01$ on CIFAR-10 and $0.1$ on CelebA. We calibrate the discriminator with Logistic Regression as in \cite{turner2019metropolis}. 
% and draw 50,000 samples to compute the Inception Score \cite{salimans2016improved}. 


% As Figure \ref{fig:is-epochs} shows, the training of GAN is very unstable, and the performance varies a lot from epoch to epoch. For a fair evaluation of the sampling methods, we average the reported metrics over five adjacent checkpoints with the best vanilla Inception Scores (IS) on average. 
% We believe in such cases, the discriminator is more likely to be nearly optimal, which is the common assumption of the sampling methods. 


\begin{table}[t]\centering
    \caption{Inception Scores of different sampling methods on CIFAR-10 and CelebA, with the DCGAN and WGAN backbones.}
    \label{tab:cifar-10-incep-results}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} &  \multicolumn{2}{c}{CIFAR-10} &  \multicolumn{2}{c}{CelebA} \\
    & DCGAN & WGAN & DCGAN & WGAN \\
    \midrule
    GAN  & 3.219 & 3.740  & 2.332 & 2.788 \\
    DRS \cite{azadi2019discriminator} & 3.073 & 3.137 & 2.869 & 2.861 \\
    MH-GAN \cite{turner2019metropolis}  & 3.225 & 3.851 & \textbf{3.106} & 2.889 \\
    DDLS \cite{che2020your} & 3.152 & 3.547 & 2.534 & 2.862 \\    
    REP-GAN (ours)  & \textbf{3.541} & \textbf{4.035} & 2.686 & \textbf{2.943} \\
    \bottomrule
    \end{tabular}
\end{table}


% \begin{figure}[t]\centering
%     \includegraphics[width=\linewidth]{is_epoch.pdf}
%     \label{fig:is-epochs}
% \end{figure}


\begin{table}[t]\centering
    \caption{Average Inception Score (a) and acceptance ratio (b) vs. training epochs with DCGAN on CIFAR-10.}
    \label{tab:training-epochs}
\begin{subtable}[t]{\linewidth}\centering
    \caption{Inception Score (mean $\pm$ std)}
    \label{tab:training-epochs-inception}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        Epoch & 20 & 21 & 22 & 23 & 24 \\
        \midrule
        GAN ~& 2.482 $\pm$ 0.027 ~& 3.836 $\pm$ 0.046 ~& 3.154 $\pm$ 0.014 ~& 3.383 $\pm$ 0.046 ~& 3.219 $\pm$ 0.036 \\
        MH-GAN ~& 2.356 $\pm$ 0.023 ~& 3.891 $\pm$ 0.040 ~& 3.278 $\pm$ 0.033 ~& 3.458 $\pm$ 0.029 ~& 3.225 $\pm$ 0.029 \\
        DDLS ~& 2.419 $\pm$ 0.021 ~& 3.332 $\pm$ 0.025 ~& 2.996 $\pm$ 0.035 ~& 3.255 $\pm$ 0.045 ~& 3.152 $\pm$ 0.028 \\
        REP-GAN ~& \textbf{2.487} $\pm$ 0.019 ~& \textbf{3.954} $\pm$ 0.046 ~& \textbf{3.294} $\pm$ 0.030 ~& \textbf{3.534} $\pm$ 0.035 ~& \textbf{3.541} $\pm$ 0.038 \\
        \bottomrule
    \end{tabular}    
    }
\end{subtable}
\begin{subtable}[t]{\linewidth}\centering
    \caption{Average Acceptance Ratio (mean $\pm$ std)}
    \label{tab:training-epochs-acceptance}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        Epoch & 20 & 21 & 22 & 23 & 24 \\
        \midrule
        MH-GAN ~& 0.028 $\pm$ 0.143 ~& 0.053 $\pm$ 0.188 ~& 0.060 $\pm$ 0.199 ~& 0.021 $\pm$ 0.126 ~& 0.027 $\pm$ 0.141 \\
        REP-GAN ~& \textbf{0.435} $\pm$ 0.384 ~& \textbf{0.350} $\pm$ 0.380 ~& \textbf{0.287} $\pm$ 0.365 ~& \textbf{0.208} $\pm$ 0.335 ~& \textbf{0.471} $\pm$ 0.384  \\
        \bottomrule
    \end{tabular}    
    }
\end{subtable}


\end{table}

\begin{table}[t]\centering
    \caption{Ablation study of our REP-GAN with 
    Inception Scores (IS) and acceptance ratios on CIFAR-10 with two backbone models, DCGAN and WGAN.
    % Independent  refers to the independent proposal of MH-GAN. REP refers to our REP proposal. MH denotes the MH rejection step of the corresponding sampler. 
    % Our REP-GAN is a combination of the REP proposal and the MH rejection step.
    }
    \label{tab:ablation-study}
    % \begin{tabular}{ll|cc}
    % \toprule
    % Proposal & MH  &  Accept & IS \\
    % \midrule
    % Reparameterized & \checkmark & \textbf{0.363}  & 3.820 \\
    % Reparameterized  & \xmark & -  & 3.312  \\
    % Independent & \checkmark & 0.033 & 3.735 \\
    % Independent & \xmark & - & 3.769   \\
    % \bottomrule
    % \end{tabular}

    % \begin{tabular}{lccccc}
    %     \toprule
    %     \multirow{2}{*}{Proposal~} & \multirow{2}{*}{~\shortstack{MH Rejection}~} & \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN}  \\
    %     &  & Accept & IS & Accept & IS \\
    %     \midrule
    %     Independent  & \xmark & - & 3.769 & - & 3.557 \\
    %     % DRS \citep{turner2019metropolis} & 1.2 & 0.02 \\
    %     Reparameterized & \xmark & -  & 3.312 & -  & 3.500 \\
    %     Independent  & \checkmark & 0.033 & 3.735 & 0.084 & 3.509 \\
    %     Reparameterized & \checkmark & \textbf{0.363} & \textbf{3.820}  & \textbf{0.447} & \textbf{3.598} \\
    %     \bottomrule
    % \end{tabular}    
    
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{Method~} & \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN}  \\
    &  Accept Ratio & IS & Accept Ratio & IS \\
    \midrule
    REP-GAN & \textbf{0.447} $\pm$ 0.384 ~& \textbf{3.541} $\pm$ 0.038  ~& \textbf{0.205} $\pm$ 0.330 ~& \textbf{4.035} $\pm$ 0.036 \\
    REP-GAN w/o REP proposal~~ & 0.027 $\pm$ 0.141 & 3.225 $\pm$ 0.029 & 0.027 $\pm$ 0.141 & 3.851 $\pm$ 0.044 \\
    REP-GAN w/o MH rejection & -  & 3.152 $\pm$ 0.028 & -  & 3.547 $\pm$ 0.029 \\
    \bottomrule
    \end{tabular}
    }
    
    % \begin{tabular}{c|c|cccc|cccc}
    %     \toprule
    %     Method & \multicolumn{2}{c}{Proposal} & \multicolumn{2}{c}{MH Rejection} \\
    %     & \multicolumn{2}{c}{Proposal} & \multicolumn{2}{c}{MH Rejection} \\
        
        
    %     \bottomrule
    % \end{tabular}    

\end{table}

% As the training process of GANs is known to be very unstable, we also demonstrate 

From Table \ref{tab:cifar-10-incep-results}, we can see our method  outperforms the state-of-the-art sampling methods in most cases. 
In Table \ref{tab:training-epochs}, we also present the average Inception Score and acceptance ratio during the training process.
As shown in Table \ref{tab:training-epochs-inception}, our REP-GAN can still outperform previous sampling methods consistently and significantly. Besides, in Table \ref{tab:training-epochs-acceptance}, we find that the average acceptance ratio of MH-GAN is lower than 0.05 in most cases, which is extremely low. While with our reparameterized dependent proposal, REP-GAN achieves an acceptance ratio around 0.4, which is known to be a relatively good tradeoff for MCMC algorithms.
% REP-GAN and MH-GAN outperform DDLS by a large margin, showing that the MH correction steps also matter on real-world datasets. In most cases, REP-GAN has higher Inception Scores and much higher acceptance ratios than MH-GAN, indicating the superiority of our structured dependent proposal. 
% We also notice that the acceptance ratio goes down as the training continues. We suspect this is because the distribution landscape becomes complex and a constant sampling step size will produce more distinct samples that are more likely to get rejected.

\subsection{Algorithmic Analysis}

\paragraph{\textbf{Ablation Study}} We conduct an ablation study of the proposed sampling algorithm, REP-GAN, and  the results are shown in Table \ref{tab:ablation-study}.  We can see that without our proposed reparameterized (REP) proposal, the acceptance ratio is very small (with an independent proposal instead). Consequently, the sample quality degrades significantly. Also, we can find that the MH correction step also matters a lot, without which the sample quality of Langevin sampling becomes even worse than the independent proposal. The ablation study shows the necessity of both REP proposal and MH rejection steps in the design of our REP-GAN.
% Meanwhile, the acceptance becomes very small on CIFAR-10 without the dependent REP proposal. 
% As a result, our REP-GAN (REP+MH) is the only setup that consistently improves over the baseline and obtains the best Inception Score on each dataset. The only exception is DCGAN on CelebA, where the independent proposal outperforms our REP proposal with a higher acceptance ratio. We believe that this is because the human face samples of CelebA are very alike among each other, such that independent samples from the generator can also be easily accepted. Nevertheless, the acceptance ratio of the independent proposal can be much lower on datasets with diverse sources of images, like CIFAR-10.


\begin{figure}[t]\centering
\includegraphics[width=\linewidth]{15chains-red.pdf}
% \begin{figure}[t]
    % \centering
% \includegraphics[width=\linewidth]{celeba-chains.pdf}
% \caption{Visualization of the Markov chains of MH-GAN (top), DDLS (middle), and REP-GAN (bottom) on CelebA with WGAN backbone.}
% \label{fig:celeba-chains}
% \end{figure}
\caption{The first 15 steps of two Markov chains with the same initial samples, generated by 
% MH-GAN, DDLS, REP-GAN (top to bottom).
% with the same initial samples of DCGAN on CIFAR-10.
MH-GAN (top), DDLS (middle), and REP-GAN (bottom).}
\label{fig:sampled-chain}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{proposals_lr1.png}
    \caption{Visualization of 5 Markov chains of our REP proposals (i.e., REP-GAN without the MH rejection steps) with a large step size ($\tau=1$).}
    \label{fig:proposals}
\end{figure}

% It can be seen that in most cases, our MCMC is superior with much better sample efficiency. 
\paragraph{\textbf{Markov Chain Visualization}} In Figure \ref{fig:sampled-chain}, we demonstrate two Markov chains sampled with different methods.  We can see that 
% It can be seen that on real-world datasets, REP-GAN also explores the energy landscape more effectively with gradient-based proposals, and rejects bad artifacts with the MH correction steps.
MH-GAN is often trapped in the same place because of the independent proposals. DDLS and REP-GAN instead gradually refine the samples with gradient steps. In addition, compared the gradient-based methods, we can see that the MH rejection steps of REP-GAN help avoid some bad artifacts in the images. For example, in the camel-like images marked in red, the body of the camel is separated in the sample of DDLS (middle) while it is not in the sample of REP-GAN (bottom). 
Note that, the evaluation protocol only needs the last step of the chain, thus we prefer a small step size that finetunes the initial samples for better sample quality. As shown in Figure \ref{fig:proposals}, our REP proposal can also produce very diverse images with a large step size. 

% As shown in Figure \ref{fig:proposals}, our gradient-based proposal can also produce very diverse examples with large step size (e.g., $\tau=1$ for CIFAR-10), just like the independent proposal of MH-GAN, and the chain won’t appear to be stuck. As is mentioned in Sec. 5.2, we evaluate our methods following the conventional setting of the sampling methods of GANs (as in MH-GAN, DDLS) by running a chain for 640 steps, and taking only the sample of the last step for evaluation. In this test scenario, because we only need the last example, we do not need the chain to have diversity among its different steps. Instead, we mainly use the chain to finetune the initial sample with a relatively small step size (e.g., $\tau=0.01$ for CIFAR-10) for better sample quality as in Figure \ref{fig:sampled-chain} and \ref{fig:celeba-chains}. 



\paragraph{\textbf{Computation Overhead}} We also compare the computation cost of the gradient-based sampling methods, DDLS and REP-GAN. They take 88.94s and 88.85s, respectively, hence the difference is negligible. Without the MH-step, our method takes 87.62s, meaning that the additional MH-step only costs 1.4\% computation overhead, which is also negligible, but it brings a significant improvement of sample quality as shown in Table \ref{tab:ablation-study}. 



% \subsection{Additional Empirical Results}
% Here we list some additional empirical results of our methods.

% \textbf{Fréchet Inception Distance (FID).} We additionally report the comparison of Fréchet Inception Distance (FID) in Table \ref{tab:cifar-10-fid-results}. Because previous works do not report FID on these benchmarks, we report our re-implementation results instead. We can see the ranks are consistent with the Inception scores in Table \ref{tab:cifar-10-incep-results} and our method is superior in most cases.

% \textbf{Markov chain visualization on CelebA.} We demonstrate two Markov chains on CelebA with different MCMC sampling methods of WGAN in Figure \ref{fig:celeba-chains}. We can see that on CelebA, the acceptance ratio of MH-GAN becomes much higher than that on CIFAR-10. Nevertheless, the sample quality is still relatively low. In comparison, the gradient-based method can gradually refine the samples with Langevin steps, and our REP-GAN can alleviate image artifacts with MH correction steps.

% \begin{table}[t]\centering
%     \caption{Fréchet Inception Distance (FID) of different MCMC sampling methods on CIFAR-10 and CelebA based on our re-implementation.}
%     \label{tab:cifar-10-fid-results}
%     \begin{tabular}{lcccc}
%     \toprule
%     \multirow{2}{*}{Method} &  \multicolumn{2}{c}{CIFAR-10} &  \multicolumn{2}{c}{CelebA} \\
%     & DCGAN & WGAN & DCGAN & WGAN \\
%     \midrule
% GAN    & 100.363 & 153.683 & 227.892 & 207.545 \\
% MH-GAN \cite{turner2019metropolis} & 100.167 & 143.426 & {\bf 227.233} & 207.143 \\
% DDLS \cite{che2020your}   & 145.981 & 193.558 & 269.840 & 232.522 \\
% REP-GAN (ours)   & {\bf 99.798}  & {\bf 143.322} & 230.748 & {\bf 207.053} \\    
% %     GAN   & 2.879 & 3.073  & 2.332 & 2.788 \\
% %     MH-GAN \cite{turner2019metropolis}  & 3.379 & 3.305 & \textbf{3.106} & 2.889 \\
% %     DDLS \cite{che2020your} & 3.518 & 3.547 & 2.534 & 2.862 \\    
% % REP-GAN (ours)  & \textbf{3.851} & \textbf{4.035} & 2.686 & \textbf{2.943} \\
%     \bottomrule
%     \end{tabular}
% \end{table}



% \begin{table}[t]
%     \caption{Comparison of computation cost (measured in seconds) of gradient-based MCMC sampling methods of GANs. We report the total time to sample a batch of 500 samples with DCGAN on a NVIDIA 1080 Ti GPU. We initialize the chain with GAN samples and run each chain for 640 steps.}
%     \label{tab:compare-computation-time}
%     \begin{center}
%     % \vspace{-0.1 in}
%     \begin{tabular}{cccc}
%     \toprule
%     {\bf DDLS} & {\bf REP-GAN (ours)} & {\bf REP-GAN w/o MH correction} \\
%     \midrule
%     88.94s & 88.85s & 87.62s \\
%     \bottomrule
%     \end{tabular}
%     \end{center}
% \end{table}



\section{Conclusion}
In this paper, we have proposed a novel method, REP-GAN, to improve the sampling of GAN. We devise a structured dependent proposal that reparameterizes the sample-level transition of GAN into the latent-level transition. More importantly, we first prove that this general proposal admits a tractable MH criterion. 
Experiments show our method does not only improve sample efficiency but also demonstrate state-of-the-art sample quality on benchmark datasets over existing sampling methods.

% \bibliographystyle{splncs04}
% \bibliography{main}

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%

\newpage

\bibliographystyle{splncs04}
\bibliography{main}
\end{document}
