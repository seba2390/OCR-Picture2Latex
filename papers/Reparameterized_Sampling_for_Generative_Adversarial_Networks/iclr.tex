\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% \input{math_commands.tex}

% \usepackage{hyperref}
% \usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsthm}
\usepackage[makeroom]{cancel}
\usepackage[shortlabels]{enumitem}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{color}

\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{float}

% \usepackage[ruled,vlined]{algorithm2e}

\def \CC {\textcolor{red}}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\db}{\mathbf{b}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\p}{\mathbf{p}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\bflambda}{\bm{\lambda}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bi}{\mathbf{i}}
\newcommand{\bj}{\mathbf{j}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bl}{\mathbf{l}}
\newcommand{\bfm}{\mathbf{m}}%\bm has been defined as a font
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bo}{\mathbf{o}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bdelta}{\bm{\delta}}
\newcommand{\bvarepsilon}{\bm{\varepsilon}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpi}{\bm{\pi}}
\newcommand{\brho}{\bm{\rho}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bxi}{\bm{\xi}}
\newcommand{\bnu}{\bm{\nu}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPhi}{\bm{\Phi}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bPi}{\bm{\Pi}}
\newcommand{\bGamma}{\bm{\Gamma}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bTheta}{\bm{\Theta}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\bone}{\mathbf{1}}
\newcommand{\norm}[1]{\ensuremath{\left\| #1 \right\|}}
\newcommand{\trace}[1]{\ensuremath{\mathrm{tr}\left(#1\right)}}
\newcommand{\spn}[1]{\ensuremath{\mathrm{span}\left(#1\right)}}
\newcommand{\spark}{\mbox{spark}}
\newcommand{\card}{\mbox{card}}
\newcommand{\supp}{\mbox{supp}}
\newcommand{\grad}{\mbox{grad}}
\newcommand{\prox}{\mbox{prox}}
\renewcommand{\div}{\mbox{div}}
%\renewcommand{\span}{\mbox{span}}
\newcommand{\sign}{\mbox{sign}}
%\newcommand{\E}{\mbox{E}}
\newcommand{\Var}{\mbox{Var}}
\newcommand{\Cov}{\mbox{Cov}}
\newcommand{\vol}{\mbox{vol}}
\renewcommand{\d}{\mbox{d }}
%\newcommand{\D}{\mbox{D}}
\renewcommand{\Pr}{\textup{\mbox{Pr}}}
\renewcommand{\vec}{\textup{\mbox{vec}}}
\newcommand{\<}{\left\langle}
\renewcommand{\>}{\right\rangle}
\newcommand{\lbar}{\left\|}
\newcommand{\rbar}{\right\|}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}
\newcommand{\red}{\textcolor[rgb]{1.00,0.00,0.00}}
\newcommand{\hongyang}{\textcolor[rgb]{0.00,1.00,0.00}}
\DeclareMathOperator*{\Ber}{Ber}
\DeclareMathOperator*{\Unif}{Unif}
\DeclareMathOperator*{\Range}{Range}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\rank}{rank} \DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\Prox}{Prox}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\Diag}{Diag}
% \DeclareMathOperator*{\lim}{lim}
\newcommand{\dspint}{\displaystyle \int}
\newcommand{\dspfrac}{\displaystyle \frac}
\newcommand{\dd}{\mbox{d}}
\newcommand{\orthc}{\mathbf{orth}_c}
\newcommand{\orthr}{\mathbf{orth}_r}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}

\def\bpi{{\boldsymbol{\pi}}}

\title{Efficient Sampling for Generative Adversarial Networks with Reparameterized Markov Chains}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Recently, sampling methods have been successfully applied to enhance the sample quality of Generative Adversarial Networks (GANs). However, in practice, they typically have poor sample efficiency because of the independent proposal sampling from the generator. 
% In this work, we propose REP-GAN, a novel sampling method with dependent proposal, which is further reparametrized through a pairing Markov Chain (CMC) in the latent space of the generator. 
In this work, we propose REP-GAN, a novel sampling method that allows general dependent proposals by REParameterizing the Markov chains
% with tractable dependent proposals reparameterized 
into the latent space of the generator.
% Theoretically, we show that given a perfect discriminator, and certain regularity conditions, our proposed method guarantees convergence to the true data distribution. 
Theoretically, we show that our reparameterized proposal admits a closed-form Metropolis-Hastings acceptance ratio.
Empirically, extensive experiments on synthetic and real datasets demonstrate that our REP-GAN largely improves the sample efficiency and obtains better sample quality simultaneously.
\end{abstract}

\section{Introduction}

Generative Adversarial Networks (GANs) \citep{goodfellow2014generative} have achieved a great success on generating realistic images in recent years \citep{karras2019style,brock2018large}. Unlike previous models that explicitly parameterize the data distribution, GANs rely on an alternative optimization between a generator and a discriminator to learn the data distribution implicitly. However, in practice, samples generated by GANs still suffer from problems such as mode collapse and bad artifacts. 
% Various variants have been proposed to alleviate these issues \citep{lucic2018gans}.



% With an optimal discriminator, the generative distribution is pushed towards the data distribution by minimizing their Jensen-Shannon (JS) divergence. 
Recently, sampling methods have shown promising results on enhancing the sample quality of GANs by making use of 
the information in the discriminator.
% Conventionally, after GAN training, the discriminator is discarded, and only the generator is left for image generation. 
% While the sampling methods utilize the density ratio information (implied by the discriminator) to close the gap between the generative distribution and the data distribution.
In the alternative training scheme of GANs, the generator only performs a few updates for the inner loop and has not fully utilized the density ratio information estimated by the discriminator. Thus, after GAN training, the sampling methods propose to further utilize this information to bridge the gap between the generative distribution and the data distribution in a fine-grained manner.
For example, DRS \citep{azadi2019discriminator} applies rejection sampling, and MH-GAN \citep{turner2019metropolis} adopts Markov chain Monte Carlo (MCMC) sampling for the improved sample quality of GANs. Nevertheless, these methods still suffer a lot from the sample efficiency problem. For example, as will be shown in Section \ref{sec:experiments}, MH-GAN's average acceptance ratio on CIFAR10 can be lower than 5\%, which makes the Markov chains slow to mix. As MH-GAN adopts an \emph{independent} proposal $q$, i.e., $q(\bx'|\bx)=q(\bx')$, the difference between samples can be so large that the proposal gets rejected easily. 

To address this limitation, we propose to generalize the independent proposal to a general \emph{dependent} proposal $q(\bx'|\bx)$. To the end, the proposed sample can be a refinement of the previous one, which leads to a higher acceptance ratio and better sample quality. We can also balance between the exploration and exploitation of the Markov chains by tuning the step size. However, it is hard to design a proper dependent proposal in the high dimensional sample space $\cX$ because the energy landscape could be very complex \citep{neal2010MCMC}. 

Nevertheless, we notice that the generative distribution $p_g(\bx)$ of GANs is implicitly defined as the push-forward of the latent prior distribution $p_0(\bz)$, and designing proposals in the low dimensional latent space is generally much easier. Hence, GAN's latent variable structure motivates us to design a \emph{structured dependent proposal} with two pairing Markov chains, one in the sample space $\cX$ and the other in the latent space $\cZ$. As shown in Figure \ref{fig:pairing-chains}, given the current pairing samples $(\bz_k,\bx_k)$, we draw the next proposal $\bx'$ in a bottom-to-up way: 1) drawing a latent proposal $\bz'$ following $q(\bz'|\bz_k)$; 2) pushing it forward through the generator and getting the sample proposal $\bx'=G(\bz')$; 3) assigning $\bx_{k+1}=\bx'$ if the proposal $\bx'$ is accepted, otherwise rejected $\bx_{k+1}=\bx_k$. By utilizing the underlying structure of GANs, the proposed reparameterized sampler becomes more efficient in the low-dimensional latent space. 
We summarize our main contributions as follows:
\begin{itemize}
    \item We propose a structured dependent proposal of GANs, which reparameterizes the sample-level transition $\bx\to\bx'$ into the latent-level  $\bz\to\bz'$ with two pairing Markov chains. We prove that our reparameterized proposal admits
    % under certain regularity conditions, it converges to the true data distribution with 
    a tractable acceptance criterion.
    \item Our proposed method, called REP-GAN, serves as a unified framework for the existing sampling methods of GANs. It provides a better balance between exploration and exploitation by the structured dependent proposal, and also corrects the bias of Markov chains by the acceptance-rejection step. 
    % under certain regularity conditions.
    % It includes all three effective mechanisms for the sampling of GANs, the latent-level transition, the Markov chain
    \item Empirical results demonstrate that REP-GAN achieves better image quality and much higher sample efficiency than the state-of-the-art methods on both synthetic and real datasets. 
    % The structured dependent proposal and the MH rejection step both help a lot for the final performance.
\end{itemize}

% Another major contribution of this work is that we first prove that this structured dependent proposal of GANs admits a closed-form Metropolis-Hastings (MH) acceptance criterion for general latent proposals. When the latent Markov chain adopts an independent proposal, our MH criterion reduces to that of MH-GAN \citep{turner2019metropolis}. When we take a Langevin latent proposal and omit the MH test, our method then reduces to DDLS \citep{che2020your}. As a result, MH-GAN and DDLS are both special cases of our framework. Our methods are more sample efficient than MH-GAN because of the dependent proposal, and more unbiased than DDLS \cite{che2020your} with the MH test for bias correction. See Table \ref{tab:compare-sample-methods} for a clear comparison. Empirical results also demonstrate our proposed REP-GAN achieves better image quality and much higher sample efficiency than state-of-the-art methods. 


\begin{figure}[t]
\centering
\includegraphics[width=.9\linewidth]{mcmc.pdf}
\caption{Illustration of REP-GAN's reparameterized dependent proposal with two pairing Markov chains, one in the latent space $\cZ$, and the other in the sample space $\cX$. }
\label{fig:pairing-chains}
\end{figure}


\section{Related Work}

Although GANs are able to synthesize high-quality images, the minimax nature of GANs makes it quite unstable, which usually results in degraded sample quality. 
A vast literature has been developed to fix the problems of GANs ever since, including novel network modules \citep{miyato2018spectral}, training mechanism \citep{metz2016unrolled}, and alternative objectives \citep{arjovsky2017wasserstein}.

% A series of works like DCGAN \citep{radford2015unsupervised} and SNGAN \citep{miyato2018spectral} propose more effective network modules for GANs. Methods like Unrolled GAN \citep{metz2016unrolled} and PACGAN \citep{lin2018pacgan}, try to fix mode collapse with new training mechanisms. Other methods like WGAN \citep{arjovsky2017wasserstein} and WGAN-GP \citep{gulrajani2017improved} instead propose alternative objectives for better training stability.

Moreover, there is another line of work using sampling methods to improve the sample quality of GANs. DRS \citep{azadi2019discriminator} firstly proposes to use rejection sampling. MH-GAN \citep{turner2019metropolis} instead uses the Metropolis-Hasting (MH) algorithm with an independent proposal. DDLS \citep{che2020your} and DCD \citep{song2020discriminator} apply  gradient-based proposals by viewing GAN as an energy-based model. \cite{tanaka2019discriminator} proposes a similar gradient-based method named DOT from the perspective of optimal transport. 


\begin{table}[!htbp]
    \caption{Comparison of sampling methods for GANs in terms of three effective sampling mechanisms. 
    % The table lists of GANs. Our REP-GAN includes all three mechanisms, and the other previous methods can be seen as our ablated special cases.
    }
    \label{tab:compare-sample-methods}
    \begin{center}
    \vspace{-0.1 in}
    \begin{tabular}{lccc}
    \toprule
    {\bf Method} & {\bf Rejection step} & {\bf Markov chain} & {\bf Latent gradient proposal} \\
    \midrule 
    GAN & \xmark & \xmark & \xmark \\
    DRS \citep{azadi2019discriminator} & \checkmark & \xmark & \xmark \\
    MH-GAN \citep{turner2019metropolis} & \checkmark & \checkmark & \xmark \\
    DDLS \citep{che2020your} & \xmark & \checkmark & \checkmark \\
    % DCD \citep{song2020discriminator} & ~~\xmark$^*$ & \checkmark & \checkmark \\
    REP-GAN (ours) & \checkmark & \checkmark & \checkmark \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

Different from them, our REP-GAN introduces a structured dependent proposal through latent reparameterization, and includes all three effective sampling mechanisms, the Markov Chain Monte Carlo method, the acceptance-rejection step, and the latent gradient-based proposal, to further improve the sample efficiency. As shown in Table \ref{tab:compare-sample-methods}, many existing works are special cases of our REP-GAN. 


Our method also belongs to the thread of works that combine MCMC and neural networks for better sample quality. Previously, some works combine variational autoencoders \citep{kingma2013auto} and MCMC to bridge the amorization gap \citep{salimans2015markov,hoffman2017learning,li2017approximate}, while others directly learn a neural proposal function for MCMC \citep{song2017nice,levy2017generalizing,wang2018meta}. Our work instead reparameterizes the high-dimensional sample-level transition into a simpler low-dimensional latent space via the learned generator network. 
% which shares the same spirit with previous works?? 


% A more closely related approach to ours is to reparameterize a complex probabilistic measure into a simpler measure via neural networks in MCMC \citep{titsias2017learning,hoffman2019neutra}. 
% w have a close connection to 
% also have a close connection to the literature that bridges Variational Inference and MCMC. 

% Our MH criterion reduces to that of MH-GAN \citep{turner2019metropolis} when the latent Markov chain adopts an independent proposal. When we take the latent  Langevin proposal and ignores the MH rejection step, our method then reduces to DDLS \citep{che2020your}. 

% In fact, sampling methods have long stood as a classical method for the learning and inference of various probabilistic models, with classic examples including Sigmoid Belief Network \cite{neal1992connectionist}, Restricted Boltzmann Machine

% In fact, GANs can be considered as a special kind of variational inference \citep{hu2017unifying}. The marriage between variational inference and sampling methods has a long history in learning probabilistic models, to list a few, Monte Carlo EM \citep{levine2001implementations}, Contrastive Divergence \citep{hinton2002training}, etc. In the era of deep learning, \cite{salimans2015markov} and \cite{hoffman2017learning} develop methods to leverage MCMC to improve the posterior of VAE \citep{kingma2013auto}, while \cite{song2017nice} and \cite{levy2017generalizing} adopt neural networks for better proposals in MCMC. 

% density ratio

\section{Background}
% In this section, we briefly introduce the notations and backgrounds.

\subsection{GAN}

GAN models the data distribution $p_d(\bx)$ implicitly with a generator $G: \cZ\to\cX$ mapping from a low-dimensional latent space $\cZ$ to a high-dimensional sample space $\cX$,
% $G:\bbR^m\to\bbR^n (m<n)$,
\begin{equation}
    \bx=G(\bz), \quad \bz\sim p_0(\bz),
\end{equation}
where the sample $\bx$ follows the generative distribution $p_g(\bx)$ and the latent variable $\bz$ follows the prior distribution $p_0(\bz)$, e.g., a standard normal distribution $\cN(\bzero,\bI)$. In GAN, a discriminator  $D:\cX\to[0,1]$ is learned to distinguish samples from $p_d(\bx)$ and $p_g(\bx)$ in an adversarial way
\begin{equation}
    \min_G\max_D\bbE_{\bx\sim p_d(\bx)}\log(D(\bx))+\bbE_{\bz\sim p_0(\bz)}\log(1-D(G(\bz))).
\end{equation}
\cite{goodfellow2014generative} point out that an optimal discriminator $D$ implies the density ratio between the data and generative distributions 
\begin{equation}
D(\mathbf{x})=\frac{p_{d}(\mathbf{x})}{p_{d}(\mathbf{x})+p_{g}(\mathbf{x})} ~~\Rightarrow ~~ \frac{p_d(\bx)}{p_g(\bx)}=\frac{1}{D(\bx)^{-1}-1}.
\label{eq:optimal-D}
\end{equation}

\subsection{MCMC}
\label{sec:mcmc}
Markov Chain Monte Carlo (MCMC) refers to a kind of sampling methods that draw a chain of samples $\bx_{1:K}\in\cX^K$ from a target distribution $p_t(\bx)$. We denote the initial distribution as $p_0(\bx)$ and the proposal distribution as $q(\bx^\prime|\bx_k)$. With the Metropolis-Hastings (MH) algorithm, we accept the proposal $\bx^\prime\sim q(\bx'|\bx_k)$ with probability
\begin{equation}
\alpha\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_t\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}|\bx^\prime\right)}{p_t\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right) \in[0,1].   
\label{eq:mh-accept} 
\end{equation}
If $\bx^{\prime}$ is accepted, $\bx_{k+1}=\bx^{\prime}$, otherwise $\bx_{k+1}=\bx_{k}$. Under mild assumptions, the Markov chain is guaranteed to converge to $p_t(\bx)$ as $K\to\infty$. In practice, the sample efficiency of MCMC crucially depends on the proposal distribution to trade off between exploration and exploitation. 

\section{The Proposed REP-GAN}
\label{sec:emhgan}
% For GANs, typically, the discriminator is thrown away after training, and only the generator is kept for generating samples. The idea behind sampling methods is to utilize the density ratio information in the discriminator to further enhance the generator. 
In this section, we first review MH-GAN and point out the limitations. We then propose our structured dependent proposal to overcome these obstacles, and finally discuss its theoretical properties as well as practical implementations.

\subsection{From Independent Proposal to Dependent Proposal}

MH-GAN \citep{turner2019metropolis} first proposes to improve GAN sampling with MCMC. Specifically, given a perfect discriminator $D$ and a descent (but imperfect) generator $G$ after training, they take the data distribution $p_d(\bx)$ as the target distribution and use the generator distribution $p_g(\bx)$ as an independent proposal
\begin{equation}
\mathbf{x}^{\prime} \sim q\left(\mathbf{x}^{\prime} | \mathbf{x}_{k}\right)=q\left(\mathbf{x}^{\prime}\right)=p_g(\bx').
\label{eq:independence-proposal}
\end{equation}
With the MH criterion (Eqn. \eqref{eq:mh-accept}) and the density ratio (Eqn. \eqref{eq:optimal-D}), we should accept $\bx'$
with probability
\begin{equation}
\alpha_{\rm MH}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}\right)}{p_d\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}\right)}\right)=\min \left(1, \frac{D\left(\mathbf{x}_{k}\right)^{-1}-1}{D\left(\mathbf{x}^{\prime}\right)^{-1}-1}\right).
\label{eq:mhgan-alpha}
\end{equation}
% As a result, the Markov chain is guaranteed to converge to the true data distribution $p_d(\bx)$ under certain regularity conditions.
% Because $\alpha_{\rm MH}(\bx',\bx_k)$ only involves the discriminator scores, it is computationally tractable. 
% According to the MH algorithm, the resulting Markov chains $\{\bx_{1:K}\}_{\rm MH}$ will marginally converge to the data distribution $p_d(\bx)$. 
% Empirical results show that MH-GAN help improve the sample quality of GAN.
However, to achieve tractability, MH-GAN adopts an independent proposal $q(\bx^\prime)$ with poor sample efficiency. As the proposed sample $\bx'$ is independent of the current sample $\bx_k$, the difference between the two samples can be so large that it results in a very low acceptance probability. Consequently, samples can be trapped in the same place for a long time, leading to a very slow mixing of the chain. 

A natural solution is to take a \textit{dependent} proposal $q(\bx'|\bx_k)$ that will propose a sample $\bx'$ close to the current one $\bx_k$, which is more likely to be accepted. 
% and lead to a reasonable acceptance ratio. 
Nevertheless, the problem of such a dependent proposal is that its MH acceptance criterion
\begin{equation}
    \alpha_{\rm DEP}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)=\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right) q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right) q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right),
    \label{eq:dependent-proposal-acceptance}
\end{equation}
is generally intractable because the data density $p_d(\bx)$ is unknown. Besides, it is hard to design a proper dependent proposal $q(\bx'|\bx_k)$ in the high dimensional sample space $\cX$ with complex landscape. These obstacles prevent us from adopting a dependent proposal that is more suitable for MCMC.

\subsection{A Tractable Structured Dependent Proposal with Reparameterized Markov Chains}
As discussed above, the major difficulty of a general dependent proposal $q(\bx'|\bx_k)$ is to compute the MH criterion. We show that it can be made tractable by considering an additional pairing Markov chain in the latent space.

As we know, samples of GANs lie in a low-dimensional manifold induced by the push-forward of the latent variable. Suppose that at the $k$-th step of the Markov chain, we have a GAN sample $\bx_k$ with latent $\bz_k$. Instead of drawing a sample $\bx'$ directly from a sample-level proposal distribution $q(\bx'|\bx_k)$, we first draw a latent proposal $\bz'$ from a dependent latent proposal distribution $q(\bz'|\bz_k)$. Afterward, we push the latent $\bz'$ forward through the generator and get the output $\bx'$ as our sample proposal. 
% In summary, we have
% \begin{equation}
%     1)\,\bx_k=G(\bz_k);
%     ~~2)~\bz'\sim q(\bz'|\bz_k);
%     ~~3)~\bx'=G(\bz').
% \end{equation}    

As illustrated in Figure \ref{fig:pairing-chains}, our bottom-to-up proposal relies on the transition reparameterization with two pairing Markov chains in the sample space $\cX$ and the latent space $\cZ$. Hence we call it a REP (reparameterized) proposal. Through a learned generator, we transport the transition $\bx_k\to\bx'$ in the high dimensional space $\cX$ into the low dimensional space $\cZ$, $\bz_k\to\bz'$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. For example, the latent target distribution is nearly standard normal when the generator is nearly perfect.
%  we can reparameterized the REP proposal distribution as follows
% Note that for a push-forward mapping $G:\bbR^m\to\bbR^n (m<n)$.
In fact, under mild conditions, the REP proposal distribution $q_{\rm REP}(\bx'|\bx_k)$ and the latent proposal distribution $q(\bz'|\bz_k)$ are tied with the following change of variables \citep{gemici2016normalizing,ben1999change}
\begin{equation}
    \log q_{\rm REP}(\bx'|\bx_k)= \log q(\bx'|\bz_k)
    =\log q(\bz'|\bz_k) - \frac{1}{2}\log\det J_{\bz'}^\top J_{\bz'},
    \label{eq:cmc-proposal-change-of-variable}
\end{equation}
where $J_\bz$ denotes the Jacobian matrix of the push-forward $G$ at $\bz$, i.e., $\left[J_\bz\right]_{ij} = \partial\,\bx_i/\partial\,\bz_j, \bx=G(\bz)$.

% In this way, through a learned generator, we transport the transition in the high dimensional space $\cX$ into the low dimensional space $\cZ$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. Previous works \citep{titsias2017learning,hoffman2019neutra} also develop similar sampling methods with invertible models that have explicit density, like flows \citep{dinh2016density,kingma2018glow}. In this work, we first prove that it is also tractable to conduct such model-based reparameterization \citep{marzouk2016introduction} for implicit models like GANs. 

Nevertheless, it remains unclear whether we can perform the MH test to decide the acceptance of the proposal $\bx'$. Note that a general dependent proposal distribution does not meet a tractable MH acceptance criterion (Eqn. \eqref{eq:dependent-proposal-acceptance}). Perhaps surprisingly, 
% another favorable property of our REP proposal is that its 
it can be shown that with our structured REP proposal, 
the MH acceptance criterion is tractable for general latent proposals $q(\bz'|\bz_k)$. 

\begin{theorem}
\label{thm:emh}
Consider a Markov chain of GAN samples $\bx_{1:K}$ with initial distribution $p_g(\bx)$. For step $k+1$, we accept our REP proposal $\bx'\sim q_{\rm REP}(\bx'|\bx_k)$ with probability
\begin{equation}
\alpha_{\rm REP}\left(\mathbf{x}^{\prime}, \mathbf{x}_{k}\right)
% =\min \left(1, \frac{p_d\left(\mathbf{x}^{\prime}\right)  q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right)  q\left(\mathbf{x}^{\prime}|\bx_k\right)}\right)
% =\min \left(1, \frac{p_0(\bz')q(\bz_k|\bz')(D(\bx_k)^{-1}-1)}{p_0(\bz_k)q(\bz'|\bz_k)(D(\bx')^{-1}-1)}\right),
=\min \left(1,~\frac{p_0(\bz')q(\bz_k|\bz')}{p_0(\bz_k)q(\bz'|\bz_k)}\cdot
\frac{D(\bx_k)^{-1}-1}{D(\bx')^{-1}-1}\right),
\label{eq:cmc-mh-acceptance}
\end{equation}
i.e. let $\bx_{k+1}=\bx'$ if $\bx'$ is accepted and $\bx_{k+1}=\bx_k$ otherwise. Further assume the chain is irreducible, aperiodic and not transient. Then, according to the Metropolis-Hastings algorithm, the stationary distribution of this Markov chain is the data distribution $p_d(\bx)$ \citep{gelman2013bayesian}.
\label{thm:main}
\end{theorem}

\begin{proof}
Note that similar to Eqn \eqref{eq:cmc-proposal-change-of-variable},  we also have the change of variables between $p_g(\bx)$ and $p_0(\bz)$,
\begin{equation}
\log p_g(\bx)\rvert_{\bx=G(\bz)}=\log p_0(\bz) - \frac{1}{2} \log \det J_{\bz}^\top J_{\bz}.
\label{eq:generator-change-of-variable}
\end{equation}
According to \cite{gelman2013bayesian}, the assumptions that the chain is irreducible, aperiodic, and not transient make sure that the chain has a unique stationary distribution, and the MH algorithm ensures that this stationary distribution equals to the target distribution $p_d(\bx)$.
Thus we only need to show that the MH criterion in Eqn. \eqref{eq:cmc-mh-acceptance} holds. Together with Eqn. \eqref{eq:optimal-D}, \eqref{eq:dependent-proposal-acceptance} and \eqref{eq:cmc-proposal-change-of-variable}, we have
\begin{equation}
\begin{aligned}
\alpha_{\rm REP}(\bx',\bx_k)
&=\frac{p_d\left(\mathbf{x}^{\prime}\right)  q\left(\mathbf{x}_{k}|\bx'\right)}{p_d\left(\mathbf{x}_{k}\right)  q\left(\mathbf{x}^{\prime}|\bx_k\right)}
=\frac{{p_d\left(\mathbf{x}^{\prime}\right)}  q(\bz_k|\bz')
% \left|\frac{\partial\,\bx_k}{\partial\, \bz_k}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
{p_g(\bx_k)}p_g(\bx')}{{p_d\left(\mathbf{x}_{k}\right)}  q(\bz'|\bz_k)
\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}
% \left|\frac{\partial\,\bx'}{\partial\, \bz'}\right|^{-1}p_g(\bx_k)
{p_g(\bx')}p_g(\bx_k)}\\
&=\frac{q(\bz_k|\bz'){\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}}p_0(\bz'){\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}}(D(\bx_k)^{-1}-1)}
{q(\bz'|\bz_k){\left(\det J_{\bz'}^\top J_{\bz'}\right)^{-\frac{1}{2}}}p_0(\bz_k){\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}}(D(\bx')^{-1}-1)}\\
&=\frac{p_0(\bz')q(\bz_k|\bz')(D(\bx_k)^{-1}-1)}{p_0(\bz_k)q(\bz'|\bz_k)(D(\bx')^{-1}-1)}.
\end{aligned}
\label{eq:dcgan-cmc-alpha}
\end{equation}
Hence the proof is completed.
\end{proof}

The theorem above demonstrates the following favorable properties of our method:
\begin{itemize}
\item The discriminator score ratio is the same as $\alpha_{\rm MH}(\bx',\bx_k)$, but MH-GAN is restricted to a specific independent proposal. Our method instead works for any latent proposal $q(\bz'|\bz_k)$. When we take $q(\bz'|\bz_k)=p_0(\bz')$, our method reduces to MH-GAN. 
\item Compared to $\alpha_{\rm DEP}(\bx',\bx_k)$ of a general dependent proposal (Eqn. \eqref{eq:dependent-proposal-acceptance}), the unknown data distributions terms are successfully cancelled in the reparameterized acceptance criterion.
%  During the reparameterization, the unknown Jacobian terms are also canceled. 
\item The reparameterized MH acceptance criterion becomes tractable as it only involves the latent priors, the latent proposal distributions, and the discriminator scores.
\end{itemize}

Combining the REP proposal $q_{\rm REP}(\bx'|\bx_k)$ and its tractable MH criterion $\alpha_{\rm REP}(\bx',\bx_k)$, we have developed a novel sampling method for GANs, coined as REP-GAN. See Appendix \ref{algo:REP-GAN} for a detailed description.
% Recently, researchers have developed an interest to bridge MCMC algorithms and neural networks for better sample efficiency. 
Moreover, our method can serve as a general approximate inference technique for Bayesian models by bridging MCMC and GANs. Previous works \citep{marzouk2016introduction,titsias2017learning,hoffman2019neutra} also propose to avoid the bad geometry of a complex probability measure by reparameterizing the Markov transitions into a simpler measure. However, these methods are limited to explicit invertible mappings without dimensionality reduction.
In our work, we first show that it is also tractable to conduct such model-based reparameterization with implicit models like GANs. 

% A more distantly related approach is to directly learn a neural proposal \citep{song2017nice,levy2017generalizing,wang2018meta}.


% \subsection{Transition Reparameterization} 
% According to the law of the unconscious statistician, 
% our Markov chain in the sample space $\cX$ can be reparameterized as a latent Markov Chain followed by push-forward at each step. 
% Specifically, given a pairing pair $(\bz_k,\bx_k)$ in the two chains, the transition $\bx_k\to\bx'$ following $q_{\rm REP}(\bx'|\bx_k)$ is equivalent to that we firstly transit from $\bz_k$ to $\bz'$ in the latent chain following $q(\bz'|\bz_k)$, and then push it forward and get $\bx'=G(\bz')$ through the generator $G$. 

% In this way, through a learned generator, we transport the transition in the high dimensional space $\cX$ into the low dimensional space $\cZ$, which enjoys a much better landscape and makes it easier to design proposals in MCMC algorithms. Previous works \citep{titsias2017learning,hoffman2019neutra} also develop similar sampling methods with invertible models that have explicit density, like flows \citep{dinh2016density,kingma2018glow}. In this work, we first prove that it is also tractable to conduct such model-based reparameterization \citep{marzouk2016introduction} for implicit models like GANs. 



\subsection{A Practical Implementation}
\label{sec:proposal}
REP-GAN enables us to utilize the vast literature of existing MCMC algorithms \citep{neal2010MCMC} to design dependent proposals for GANs. We take Langevin Monte Carlo (LMC) as an example.
As an Euler-Maruyama discretization of the Langevin dynamics, LMC updates the Markov chain with
\begin{equation}
    \bx_{k+1}=\bx_k+\frac{\tau}{2}\nabla_{\bx}\log p_t(\bx_k)+\sqrt{\tau}\cdot\bvarepsilon,~~\bvarepsilon\sim\cN(\bzero,\bI),
\end{equation}
for a target distribution $p_t(\bx)$. Compared to MH-GAN, LMC utilizes the gradient information to explore the energy landscape more efficiently. However, if we directly take the (unknown) data distribution $p_d(\bx)$ as the target distribution $p_t(\bx)$, LMC does not meet a tractable update rule. 

As discussed above, the reparameterization of REP-GAN makes it easier to design transitions in the low-dimensional latent space. Hence, we instead propose to use LMC for the latent Markov chain.
We assume that the data distribution also lies in the low-dimensional manifold induced by the generator, i.e., 
$\operatorname{Supp}\left(p_d\right)\subset\operatorname{Im}(G)$. This implies that the data distribution $p_d(\bx)$ also has a pairing distribution in the latent space, denoted as $p_t(\bz)$. They are tied with the change of variables
\begin{equation}
\log p_d(\bx)\rvert_{\bx=G(\bz)}=\log p_t(\bz) - \frac{1}{2} \log\det\, J_{\bz}^\top J_{\bz},
\end{equation}
Taking $p_t(\bz)$ as the (unknown) target distribution of the latent Markov chain, we have the following Latent LMC (L2MC) proposal 
\begin{equation}
\begin{aligned}
\bz'&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log p_t(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz_k+\frac{\tau}{2}\nabla_{\bz}\log\frac{p_t(\bz_k)
% \left|\frac{\partial\,\bx}{\partial\,\bz}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
}{p_0(\bz_k)
% \left|\frac{\partial\,\bx}{\partial\,\bz}\right|^{-1}
\left(\det J_{\bz_k}^\top J_{\bz_k}\right)^{-\frac{1}{2}}
}+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz+\frac{\tau}{2}\nabla_{\bz}\log\frac{p_d(\bx_k)}{p_g(\bx_k)}+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon\\
&=\bz-\frac{\tau}{2}\nabla_{\bz}\log(D^{-1}(\bx_k)-1)+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon, \quad\bvarepsilon\sim\cN(\bzero,\bI),
\end{aligned}
\label{eq:langevin}
\end{equation}
where $\bx_k=G(\bz_k)$. As we can see, L2MC is made tractable by our structured dependent proposal with pairing Markov chains. DDLS \citep{che2020your} proposes a similar Langevin proposal by formalizing GANs as an implicit energy-based model, while here we provide a straightforward derivation through reparameterization. Our major difference to DDLS is that REP-GAN also includes a tractable MH correction step (Eqn. \eqref{eq:cmc-mh-acceptance}), which accounts for the numerical errors introduced by the  discretization and ensures that detailed balance holds. 
% Experiments show that it is crucial for the final performance.
%  Note that our method is general, and any off-the-shelf MCMC algorithms can be adopted as the latent proposal methods.

% \subsection{Limitations}
% With the additional MH correction step, our REP-GAN has some extra computational overhead. In practice, it takes about 1.2x time than DDLS \cite{che2020your} with only Langevin Monte Carlo. 
% Another limitation of our method is that its superior performance does not generalize to some variants of GAN objectives, like hinge loss \citep{miyato2018spectral}. In practice, we find the additional MH step causes performance drop with these models. We suspect this is because their discriminators do not explicitly estimate the density ratio like GAN. And this bias may get amplified in the MH correction step, which relies crucially on the estimation of density ratio. A more concrete analysis of this phenomenon remains an open problem for future work.
\subsection{Extension to WGAN}
Our method can also be extended to other kinds of GAN, like Wasserstein GAN (WGAN) \citep{arjovsky2017wasserstein}.
The WGAN objective is
\begin{align}
\min_G\max_D\,\mathbb{E}_{\bx\sim p_{d}(\bx)}[D(\bx)]-\mathbb{E}_{\bx\sim p_{g}(\bx)}[D(\bx)],
\end{align}
where $D:\cX\to\bbR$ is restricted to be a Lipschitz function. Under certain conditions, WGAN also implies an approximate estimation of the density ratio \citep{che2020your},
\begin{equation}
D(\bx)\approx\log\frac{p_d(\bx)}{p_g(\bx)}+\text{const}\quad\Rightarrow\quad\frac{p_d(\bx)}{p_g(\bx)}\approx\exp(D(\bx))\cdot \text{const}.
\end{equation}
Following the same derivations as in Eqn. \eqref{eq:dcgan-cmc-alpha} and \eqref{eq:langevin}, we will have the WGAN version of REP-GAN. Specifically, with $\bx_k=G(\bz_k)$, the L2MC proposal follows
\begin{equation}
\bz'=\bz_k+\frac{\tau}{2}\nabla_{\bz}D(\bx_k)+\frac{\tau}{2}\nabla_{\bz}\log p_0(\bz_k)+\sqrt{\tau}\cdot\bvarepsilon, \quad\bvarepsilon\sim\cN(\bzero,\bI),
\label{eq:wgan-proposal}
\end{equation}
and the MH acceptance criterion is 
\begin{equation}
\alpha_{REP-W}\left(\mathbf{x}^{\prime},\mathbf{x}_{k}\right)=\min \left(1,~ \frac{q(\bz_k|\bz')p_0(\bz')}{q(\bz'|\bz_k)p_0(\bz_k)}\cdot\frac{\exp\left(D(\bx')\right)}{\exp\left(D(\bx_k)\right)}\right).
\end{equation}
% For this we name it as REP-WGAN.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{swiss-roll.pdf}
    \caption{Visualization of samples with different sampling methods on the Swiss Roll dataset. Here tau denotes the Langevin step size in Eqn. \eqref{eq:wgan-proposal}.}
    \label{fig:swissroll}
\end{figure}

% \begin{table}[t]\centering
% \caption{Inception Scores and acceptance ratios on CIFAR-10 for DCGAN and WGAN. Computed with 50,000 samples per checkpoint, and averaged over five adjacent checkpoints. }
% \label{tab:cifar-10-incep-results}
% \begin{tabular}{lcccc}
% \toprule
% \multirow{2}{*}{Method} &  \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN} \\
% &  Acceptance & Inception Score &  Acceptance & Inception Score  \\
% % \multirow{1}{*}{Method} & Accept & Inception Score \\
% \midrule
% GAN & - & 3.769 & - & 3.557    \\
% % DRS \citep{turner2019metropolis} & 1.2 & 0.02 \\
% MH-GAN \citep{turner2019metropolis} & 0.033 & 3.735 & 0.084 & 3.509 \\
% DDLS \citep{che2020your}  & -  & 3.312 & -  & 3.500  \\
% REP-GAN (ours)  & \textbf{0.363} & \textbf{3.820}  & \textbf{0.447} & \textbf{3.598} \\
% \bottomrule
% \end{tabular}
% \end{table}


\section{Experiments}
\label{sec:experiments}
We show our empirical results both on synthetic and real image datasets.

\subsection{Synthetic Data}
Following DOT \citep{tanaka2019discriminator} and DDLS \citep{che2020your}, we apply REP-GAN to the synthetic Swiss Roll dataset, where data samples lie on a Swiss roll manifold in the two-dimensional space. We construct the dataset by scikit-learn with 100,000 samples, and train a WGAN as in \cite{tanaka2019discriminator}, where both the generator and discriminator are fully connected neural networks with leaky ReLU nonlinearities. We optimize the model using the Adam optimizer, with $\alpha = 0.0001, \beta_1 = 0.5, \beta_2 = 0.9$.
After training, we draw 1,000 samples with different sampling methods. 

As shown in Figure \ref{fig:swissroll}, with appropriate step size ($\tau=0.01$), the gradient-based methods (DDLS and REP-GAN) outperform independent proposals (DRS and MH-GAN) by a large margin, while DDLS is more discontinuous on shape compared to REP-GAN. In DDLS, when the step size becomes too large ($\tau=0.1,1$), the numerical error of the Langevin dynamics becomes so large that the chain either collapses or diverges. In contrast, those bad proposals are rejected by the MH correction steps of REP-GAN, which prevents the misbehavior of the Markov chain.

\subsection{Real Image Data}

Following MH-GAN \citep{turner2019metropolis}, we conduct experiments on two real-world image datasets, CIFAR-10 and CelebA, for DCGAN \citep{radford2015unsupervised} and WGAN \citep{arjovsky2017wasserstein}. 
% We utilize the official implementation of MH-GAN
% \footnote{\url{https://github.com/uber-research/metropolis-hastings-GAN}}
% and train models with the same hyperparameters. 
% Our baselines include the vanilla GAN, DRS \citep{azadi2019discriminator}, MH-GAN \citep{turner2019metropolis}, DDLS \citep{che2020your}, and our REP-GAN. 
% For a fair comparison of MCMC methods, 
Following the conventional evaluation protocol, we initialize each Markov chain with a GAN sample, run it for 640 steps, and take the last sample for evaluation. We collect 50,000 samples to evaluate the Inception Score\footnote{For fair comparison, our training and evaluation follows the the official code of MH-GAN \citep{turner2019metropolis}: \url{https://github.com/uber-research/metropolis-hastings-gans}} \citep{salimans2016improved}. 
% each Markov chain starts from a GAN sample and runs 640 steps, and we only take the last sample for evaluation. 
The step size $\tau$ of our L2MC proposal is $0.01$ on CIFAR-10 and $0.1$ on CelebA. We calibrate the discriminator with Logistic Regression as in \cite{turner2019metropolis}. 
% and draw 50,000 samples to compute the Inception Score \citep{salimans2016improved}. 


% As Figure \ref{fig:is-epochs} shows, the training of GAN is very unstable, and the performance varies a lot from epoch to epoch. For a fair evaluation of the sampling methods, we average the reported metrics over five adjacent checkpoints with the best vanilla Inception Scores (IS) on average. 
% We believe in such cases, the discriminator is more likely to be nearly optimal, which is the common assumption of the sampling methods. 


\begin{table}[t]\centering
    \caption{Inception Scores (IS) of different sampling methods on CIFAR-10 and CelebA. $^*$ For GAN, DRS and MH-GAN, we report the results in \cite{turner2019metropolis}. $^\sharp$ The results are given by our implementation.}
    \label{tab:cifar-10-incep-results}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} &  \multicolumn{2}{c}{CIFAR-10} &  \multicolumn{2}{c}{CelebA} \\
    & DCGAN & WGAN & DCGAN & WGAN \\
    \midrule
    GAN$^*$   & 2.879 & 3.073  & 2.332 & 2.788 \\
    DRS \citep{azadi2019discriminator}$^*$ & 3.073 & 3.137 & 2.869 & 2.861 \\
    MH-GAN \citep{turner2019metropolis}$^*$  & 3.379 & 3.305 & \textbf{3.106} & 2.889 \\
    % \midrule
    % GAN (our baseline)$^\sharp$   & 3.781 & 3.740  & 2.532 & 2.746 \\
    % DDLS \citep{che2020your}$^\sharp$ & 3.518 & 3.547 & 2.427 & 2.862 \\
    DDLS \citep{che2020your}$^\sharp$ & 3.518 & 3.547 & 2.534 & 2.862 \\    
    % REP-GAN (ours)  & \textbf{3.851} & \textbf{4.035} & 2.534 & \textbf{2.943} \\
    REP-GAN (ours)$^\sharp$  & \textbf{3.851} & \textbf{4.035} & 2.686 & \textbf{2.943} \\
    \bottomrule
    \end{tabular}
\end{table}


\begin{figure}[t]\centering
    \includegraphics[width=\linewidth]{is_epoch.pdf}
    \caption{Average Inception Score (left) and acceptance ratio (right) vs. training epoch on CIFAR-10 based on our re-implementation. The standard deviation is shown with shaded error bar (left).}
    \label{fig:is-epochs}
\end{figure}


\begin{table}[h]\centering
    \caption{Ablation study of our REP-GAN with 
    Inception Scores (IS) and acceptance ratios (Accept) averaged over five adjacent checkpoints. IND refers to the independent proposal of MH-GAN. REP refers to our REP proposal. MH denotes the MH rejection step of the corresponding sampler. 
    % Our REP-GAN is a combination of the REP proposal and the MH rejection step.
    }
    \label{tab:ablation-study}
    % \begin{tabular}{lllcccccccc}
    % \toprule
    % \multirow{3}{*}{Method} & \multirow{3}{*}{Sampler} & \multirow{3}{*}{MH} &  \multicolumn{4}{c}{CIFAR10} &  \multicolumn{4}{c}{CelebA} \\
    % & & &  \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN} &  \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN} \\
    % & & &  Accept & IS &  Accept & IS &  Accept & IS &  Accept & IS  \\
    % % \multirow{1}{*}{Method} & Accept & Inception Score \\
    % \midrule
    % GAN & ID & \xmark & - & 3.769 & - & 3.557 & - & 2.431 & - & 2.799    \\
    % % DRS \citep{turner2019metropolis} & 1.2 & 0.02 \\
    % REP & REP  & \xmark & -  & 3.312 & -  & 3.500 & - & 2.316 & - &2.868  \\
    % MH & ID & \checkmark & 0.033 & 3.735 & 0.084 & 3.509 & \textbf{0.371}  & \textbf{2.576} & 0.747 & 2.879 \\
    % REP-GAN & REP & \checkmark & \textbf{0.363} & \textbf{3.820}  & \textbf{0.447} & \textbf{3.598} & 0.363 & 2.461 &  \textbf{0.955} & \textbf{2.894} \\
    % \bottomrule
    % \end{tabular}

    \begin{tabular}{c|c|cccc|cccc}
        \toprule
        \multirow{3}{*}{Proposal} & \multirow{3}{*}{MH} &  \multicolumn{4}{c|}{CIFAR10} &  \multicolumn{4}{c}{CelebA} \\
         & &  \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c|}{WGAN} &  \multicolumn{2}{c}{DCGAN} &  \multicolumn{2}{c}{WGAN} \\
         & &  Accept & IS &  Accept & IS &  Accept & IS &  Accept & IS  \\
        % \multirow{1}{*}{Method} & Accept & Inception Score \\
        \midrule
        IND & \xmark & - & 3.769 & - & 3.557 & - & 2.431 & - & 2.799    \\
        % DRS \citep{turner2019metropolis} & 1.2 & 0.02 \\
        REP  & \xmark & -  & 3.312 & -  & 3.500 & - & 2.465 & - &2.868  \\
        IND & \checkmark & 0.033 & 3.735 & 0.084 & 3.509 & \textbf{0.678}  & \textbf{2.500} & 0.747 & 2.879 \\
        REP & \checkmark & \textbf{0.363} & \textbf{3.820}  & \textbf{0.447} & \textbf{3.598} & 0.628 & 2.476 &  \textbf{0.955} & \textbf{2.894} \\
        \bottomrule
    \end{tabular}    

\end{table}

% As the training process of GANs is known to be very unstable, we also demonstrate 

From Table \ref{tab:cifar-10-incep-results}, we can see our method outperforms state-of-the-art methods on both datasets.
We also plot the Inception Score and acceptance ratio per epoch in Figure \ref{fig:is-epochs} based on our re-implementation. Although the training process of GANs is known to be very unstable, our REP-GAN can still outperform previous sampling methods both consistently (superior in most epochs) and significantly (the improvement is larger than the error bar) as shown in the left panel of Figure \ref{fig:is-epochs}. In the right panel, we find that the average acceptance ratio of MH-GAN is lower than 0.2 in most cases, while REP-GAN has an acceptance ratio of 0.4-0.8, which is known to be a good tradeoff for MCMC algorithms.
% REP-GAN and MH-GAN outperform DDLS by a large margin, showing that the MH correction steps also matter on real-world datasets. In most cases, REP-GAN has higher Inception Scores and much higher acceptance ratios than MH-GAN, indicating the superiority of our structured dependent proposal. 
We also notice that the acceptance ratio goes down as the training continues. We suspect this is because the distribution landscape becomes complex and a constant sampling step size will produce more distinct samples that are more likely to get rejected.


\textbf{Ablation study.} From Table \ref{tab:ablation-study}, we can see that without the MH correction step, the Langevin steps often result in worse sample quality. Meanwhile, the acceptance is very small on CIFAR-10 without the dependent REP proposal. As a result, our REP-GAN (REP+MH) is the only setup that consistently improves over the baseline and obtains the best Inception Score on each dataset. The only exception is DCGAN on CelebA, where the independent proposal outperforms our REP proposal with a higher acceptance ratio. We believe that this is because the human face samples of CelebA are very alike among each other, such that independent samples from the generator can also be easily accepted. Nevertheless, the acceptance ratio of the independent proposal can be much lower on datasets with diverse sources of images, like CIFAR-10.


\begin{figure}[h]\centering
\includegraphics[width=\linewidth]{cmcgan/chains-red.pdf}
\caption{The first 20 steps of two Markov chains with the same initial samples. The chains are generated by 
% MH-GAN, DDLS, REP-GAN (top to bottom).
% with the same initial samples of DCGAN on CIFAR-10.
MH-GAN (top), DDLS (middle), and REP-GAN (bottom).}
\label{fig:sampled-chain}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{cmcgan/proposals_lr1.png}
    \caption{Visualization of 5 Markov chains of our REP proposals (i.e., REP-GAN without the MH rejection steps) with a large step size ($\tau=1$).}
    \label{fig:proposals}
\end{figure}

% It can be seen that in most cases, our MCMC is superior with much better sample efficiency. 
\textbf{Markov chain visualization.} In Figure \ref{fig:sampled-chain}, we demonstrate two Markov chains sampled with different methods.  We can see that 
% It can be seen that on real-world datasets, REP-GAN also explores the energy landscape more effectively with gradient-based proposals, and rejects bad artifacts with the MH correction steps.
MH-GAN is often trapped in the same place because of the independent proposals. DDLS and REP-GAN instead gradually refine the samples with gradient steps. In addition, compared the gradient-based methods, we can see that the MH rejection steps of REP-GAN help avoid some bad artifacts in the images. For example, in the camel-like images marked in red, the body of the camel is separated in the sample of DDLS (middle) while it is not in the sample of REP-GAN (bottom). 
Note that, the evaluation protocol only needs the last step of the chain, thus we prefer a small step size that finetunes the initial samples for better sample quality. As shown in Figure \ref{fig:proposals}, our REP proposal can also produce very diverse images with a large step size. 

% As shown in Figure \ref{fig:proposals}, our gradient-based proposal can also produce very diverse examples with large step size (e.g., $\tau=1$ for CIFAR-10), just like the independent proposal of MH-GAN, and the chain won’t appear to be stuck. As is mentioned in Sec. 5.2, we evaluate our methods following the conventional setting of the sampling methods of GANs (as in MH-GAN, DDLS) by running a chain for 640 steps, and taking only the sample of the last step for evaluation. In this test scenario, because we only need the last example, we do not need the chain to have diversity among its different steps. Instead, we mainly use the chain to finetune the initial sample with a relatively small step size (e.g., $\tau=0.01$ for CIFAR-10) for better sample quality as in Figure \ref{fig:sampled-chain} and \ref{fig:celeba-chains}. 


\section{Conclusion}
In this paper, we have proposed a novel method, REP-GAN, to improve the sampling of GAN. We devise a structured dependent proposal that reparameterizes the sample-level transition of GAN into the latent-level transition. More importantly, we first prove that this general proposal admits a tractable MH criterion. 
Experiments show our method does not only improve sample efficiency but also demonstrate state-of-the-art sample quality on benchmark datasets over existing sampling methods.

\bibliography{cmcgan}
\bibliographystyle{iclr2021_conference}

\newpage
\appendix
\section{Appendix}
% You may include other additional sections here.

\subsection{Assumptions and Implications}

Note that our method needs a few assumptions on the models for our analysis to hold. Here we state them explicitly and discuss their applicability and potential impacts.

\begin{assumption}
The generator mapping $G:\bbR^n\to\bbR^m (n<m)$ is injective, and its Jacobian matrix $\left[\frac{\partial\,G(\bz)}{\partial\,\bz}\right]$ of size $m\times n$, has full column rank for all $\bz\in\bbR^n$.
\end{assumption}
For the change of variables in Eqn. \eqref{eq:cmc-proposal-change-of-variable} and \eqref{eq:generator-change-of-variable} to hold, according to \cite{ben1999change}, we need the mapping to be injective and its Jaobian should have full column rank. A mild sufficient condition for injectivity is that the generator only contains (non-degenerate) affine layers and injective non-linearities, like LeakyReLU. It is not hard to show that such a condition also implies the full rankness of the Jacobian. In fact, this architecture has already been found to benefit GANs and achieved state-of-the-art results \citep{tang2020lessons}. The affine layers here are also likely to be non-degenerate because their weights are randomly initialized and typically will not degenerate in practice during the training of GANs.

% And this architecture has also been found to benefit GANs and achieved state-of-the-art results \citep{tang2020lessons}. The full rankness is also likely to hold because the generator is a highly non-linear push-forward with randomly initialized weights.

\begin{assumption}
The discriminator $D$ offers a perfect estimate the density ratio between the generative distribution $p_g(\bx)$ and the data distribution $p_d(\bx)$ as in Eqn. \eqref{eq:optimal-D}.
\end{assumption}

This is a common, critical, but less practical assumption among the existing sampling methods of GANs. It is unlikely to hold exactly in practice, because during the alternative training of GANs, the generator is also changing all the time, and the a few updates of the discriminator cannot fully learn the corresponding density ratio. Nevertheless, we think it can capture a certain extent information of density ratio which explains why the sampling methods can consistently improve over the baseline at each epoch. 

From our understanding, the estimated density ratio is enough to push the generator better but not able to bring it up to the data distribution. This could be the reason why the Inception scores obtained by the sampling methods, can improve over the baselines but cannot reach up to that of real data and fully close the gap, even with very long run of the Markov chains.

Hence, there is still much room for improvement. To list a few, one can develop mechanisms that bring more accurate density ratio estimate, or relax the assumptions for the method to hold, or establishing estimation error bounds. Overall, we believe GANs offer an interesting alternative scenario for the development of sampling methods.

% \begin{assumption}
% We assume the Markov chain generated with our REP proposal to be irreducible, aperiodic and not transient in Theorem \ref{thm:emh}.
% \end{assumption}
% As our REP-GAN is designed to be a general framework compatible with many MCMC proposals, we cannot give convergence results without knowing the properties of the specific proposal distribution. Hence, according to \cite{gelman2013bayesian}, we have to assume the chain having these regularity conditions, that the chain is irreducible, aperiodic and not transient, to obtain convergence in general. As for whether these conditions hold for a specific proposal distribution, like the latent Langevin sampler, more involved discussions, e.g., \cite{roberts1996exponential},  are needed to obtain theoretical guarantees on convergence. 

\subsection{Algorithm Procedure}
We give a detailed description of the algorithm procedure of our REP-GAN in Algorithm \ref{algo:REP-GAN}.

\begin{algorithm}[h]
\textbf{Input:} trained GAN with (calibrated) discriminator $D$ and generator $G$, Markov chain length $K$, latent prior distribution $p_0(\bz)$, latent proposal distribution $q(\bz'|\bz_k)$;\\
\textbf{Output:} an improved GAN sample $\bx_K$;
\begin{algorithmic}
% \STATE (Optional) Calibrate the discriminator;
\STATE Draw an initial sample $\bx_1$: 1) draw initial latent $\bz_1\sim p_0(\bz)$ and 2) push forward $\bx_1=G(\bz_1)$;
\FOR{each step $k\in[1,K-1]$}
\STATE Draw a REP proposal $\bx'\sim q_{\rm REP}(\bx'|\bx_k)$: 1) draw a latent proposal $\bz'\sim q(\bz'|\bz_k)$, and 2) push forward $\bx'=G(\bz')$;
\STATE Calculate the MH acceptance criterion $\alpha_{\rm REP}(\bx_k,\bx')$ following Eqn. \eqref{eq:cmc-mh-acceptance};
\STATE Decide the acceptance of $\bx'$ with probability $\alpha_{\rm REP}(\bx_k,\bx')$;
\IF{$\bx'$ is accepted}
\STATE{Let $\bx_{k+1}=\bx', \bz_{k+1}=\bz'$}
\ELSE
\STATE{Let $\bx_{k+1}=\bx_k,\bz_{k+1}=\bz_k$}
\ENDIF
\ENDFOR
\end{algorithmic}
\caption{GAN sampling with Reparameterized Markov chains (REP-GAN)}
\label{algo:REP-GAN}
\end{algorithm}




\subsection{Additional Empirical Results}
Here we list some additional empirical results of our methods.

\textbf{Fréchet Inception Distance (FID).} We additionally report the comparison of Fréchet Inception Distance (FID) in Table \ref{tab:cifar-10-fid-results}. Because previous works do not report FID on these benchmarks, we report our re-implementation results instead. We can see the ranks are consistent with the Inception scores in Table \ref{tab:cifar-10-incep-results} and our method is superior in most cases.

\textbf{Computation overhead.} In Table \ref{tab:compare-computation-time}, we compare different gradient-based sampling methods of GANs. Comparing DDLS and our REP-GAN, they take 88.94s and 88.85s, respectively, hence the difference is negligible. Without the MH-step, our method takes 87.62s, meaning the additional MH-step only costs 1.4\% computation overhead, which is also negligible, but it brings a significant improvement of sample quality as shown in Table \ref{tab:ablation-study}. 

\textbf{Markov chain visualization on CelebA.} We demonstrate two Markov chains on CelebA with different MCMC sampling methods of WGAN in Figure \ref{fig:celeba-chains}. We can see that on CelebA, the acceptance ratio of MH-GAN becomes much higher than that on CIFAR-10. Nevertheless, the sample quality is still relatively low. In comparison, the gradient-based method can gradually refine the samples with Langevin steps, and our REP-GAN can alleviate image artifacts with MH correction steps.

\begin{table}[t]\centering
    \caption{Fréchet Inception Distance (FID) of different MCMC sampling methods on CIFAR-10 and CelebA based on our re-implementation.}
    \label{tab:cifar-10-fid-results}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{Method} &  \multicolumn{2}{c}{CIFAR-10} &  \multicolumn{2}{c}{CelebA} \\
    & DCGAN & WGAN & DCGAN & WGAN \\
    \midrule
GAN    & 100.363 & 153.683 & 227.892 & 207.545 \\
MH-GAN \citep{turner2019metropolis} & 100.167 & 143.426 & {\bf 227.233} & 207.143 \\
DDLS \citep{che2020your}   & 145.981 & 193.558 & 269.840 & 232.522 \\
REP-GAN (ours)   & {\bf 99.798}  & {\bf 143.322} & 230.748 & {\bf 207.053} \\    
%     GAN   & 2.879 & 3.073  & 2.332 & 2.788 \\
%     MH-GAN \citep{turner2019metropolis}  & 3.379 & 3.305 & \textbf{3.106} & 2.889 \\
%     DDLS \citep{che2020your} & 3.518 & 3.547 & 2.534 & 2.862 \\    
% REP-GAN (ours)  & \textbf{3.851} & \textbf{4.035} & 2.686 & \textbf{2.943} \\
    \bottomrule
    \end{tabular}
\end{table}



\begin{table}[t]
    \caption{Comparison of computation cost (measured in seconds) of gradient-based MCMC sampling methods of GANs. We report the total time to sample a batch of 500 samples with DCGAN on a NVIDIA 1080 Ti GPU. We initialize the chain with GAN samples and run each chain for 640 steps.}
    \label{tab:compare-computation-time}
    \begin{center}
    % \vspace{-0.1 in}
    \begin{tabular}{cccc}
    \toprule
    {\bf DDLS} & {\bf REP-GAN (ours)} & {\bf REP-GAN w/o MH correction} \\
    \midrule
    88.94s & 88.85s & 87.62s \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{celeba-chains.pdf}
    \caption{Visualization of the Markov chains of MH-GAN (top), DDLS (middle), and REP-GAN (bottom) on CelebA with WGAN backbone.}
    \label{fig:celeba-chains}
\end{figure}
% \subsection{Experimental Details}

% \subsubsection{Swiss Roll Dataset}

% \subsection{CIFAR-10 and CelebA}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{25Gaussians-mark.pdf}
    \caption{Visualization of samples with different sampling methods on the 25-Gaussians dataset. Here $\tau$ denotes the Langevin step size in Eqn. \eqref{eq:wgan-proposal}.}
    \label{fig:25gaussians}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{more-modes.pdf}
    \caption{Visualization of the multi-modal experiments with more modes. Specifically two cases are considered, 9x9, and 13x13 mixture of Gaussian. The true data points are shown in grey in background, and the generated points with different sampling methods are shown in blue. }
    \label{fig:more-modes}
\end{figure}



\subsection{Multi-modal Experiments}

Aside from the manifold learning example shown in Figure \ref{fig:swissroll}, we additionally conduct experiments to illustrate the performance of our sampling methods for multi-modal distributions.

\textbf{25-Gaussians.} To begin with, we consider the 25-Gaussians dataset widely discussed in previous work \citep{azadi2019discriminator,turner2019metropolis,che2020your}. The 25-Gaussians dataset is generated by a mixture of twenty-five two-dimensional isotropic Gaussian distributions with variance 0.01, and means separated by 1, arranged in a grid. We train a small Wasserstein GAN model with the standard WGAN-GP objective following the setup in  \cite{tanaka2019discriminator}. After training, we draw 1,000 samples with different sampling methods. Similarly, we starts a Markov chain with a GAN sample, run it for 100 steps, and collect the last example for evaluation.

As shown in Figure \ref{fig:25gaussians}, compared to MH-GAN, the gradient-based methods (DDLS and ours) produce much better samples close to the data distribution with proper step size. Comparing DDLS and ours, DDLS tends to concentrate so much on the mode centers that its standard deviation can be even smaller than that of data distribution. Instead, our method preserves more sample diversity while concentrating on the mode centers. 
When the step size becomes larger, the difference becomes more obvious. When $\tau=0.1$, as marked with blue circles, the samples of DDLS become so concentrated that some modes are missed. When $\tau=1$, samples of DDLS diverge far beyond the 5x5 grid. In comparison, our REP-GAN does not suffer from these issues with the MH correction steps accounting for the bias introduced by numerical errors.


\textbf{Scale to more modes.} In the above, we have experimented w.r.t. a relatively easy scenario where the multi-modal distribution only has 5x5 modes ($n=5$ modes along each axis). In fact, the distinctions between the sampling methods become even more obvious when we scale to more modes. Specifically, as shown in Figure \ref{fig:more-modes}, we also compare them w.r.t. mixture of Gaussians with 9x9 and 13x13 modes, respectively. The rest of the setup is similar to 25-Gaussians. Note that throughout the experiments in this part, we adopt proper step size, $\tau=0.01$, for the gradient-based methods (DDLS and REP-GAN) by default.

Under the more challenging scenarios, we can see that the gradient-based methods still consistently outperforms MH-GAN. Moreover, our REP-GAN has a more clear advantage over DDLS. Specifically, for 9x9 modes, our REP-GAN produces samples that are less noisy (i.e., less examples distinct from the modes), while preserving all the modes. For 9x9 modes, DDLS makes a critical mistake that it drops one of the modes (left down corner, marked with red circle) during the Markov chain update. 
% To see that, note  this mode also appears with the vanilla GAN sampling.
As discussed above, we believe this is because DDLS has a bias towards regions with high probability, while ignoring the diversity of the distribution. In comparison, REP-GAN effectively prevents such bias by the MH correction steps.


% \subsection{Re}
% \textbf{REP proposal visualization.} 
% \subsection{Remark on the Sample Diversity of Markov Chains.}
% Following conventions in previous work, we evaluate our methods with short-run MCMCs. 
% As is shown in Figure \ref{fig:proposals}, REP proposals with larger steps also produce diverse images. Nevertheless, the large step size often results in low-quality proposals that


% \textbf{Markov chain visualization on CelebA.} We demonstrate two Markov chains on CelebA with different MCMC sampling methods of WGAN in Figure \ref{fig:celeba-chains}. We can see that on CelebA, the acceptance ratio of MH-GAN becomes much higher than that on CIFAR-10. Nevertheless, the sample quality is still relatively low. In comparison, the gradient-based method can gradually refine the samples with Langevin steps, and our REP-GAN can alleviate image artifacts with MH correction steps.

\end{document}
