\begin{definition}\label{marginal}\textbf{Marginal Node and Interior Node.}
Marginal Nodes $\mathcal{M}$ are nodes linked to nodes with different labels while Interior Nodes $\mathcal{I}$ not. Formally, $\mathcal{M} = \{v_i | v_i \in V \land (\exists v_j \in V, (v_i, v_j) \in E \land y_i \neq y_j)\}$, $\mathcal{I} = V \setminus \mathcal{M}$.
\end{definition}

\begin{assumption} \label{convergence}\textbf{Convergence conditions.} 
When $G$ converges, we expect it to generate fake samples linked to nearby marginal nodes. More specifically, let $V_g$ and $E_g$ be the set of generated fake samples and generated links from generated  nodes to nearby original nodes. we have  $\forall v_g \in V_g, (\exists v_i \in \mathcal{M}, (v_g, v_i) \in E_g) \land (\forall (v_g, v_i) \in E_g, v_i \in \mathcal{M}) $.  
\end{assumption}
\vspace{0.06in}

The loss function of graph Laplacian regularization framework is as follows:

%\begin{small}
\begin{equation}\label{basic}
	\mathcal{L}(y') = \sum\limits_{v_i\in V^L} loss(y_i, y_i') + \lambda\sum\limits_{v_i,v_j \in V}^{i\neq j} \alpha_{ij}\cdot neq(y_i', y_j')
\end{equation}
%\end{small}

\noindent where $y_i'$ denotes predicted label of node $v_i$. The $loss(\cdot, \cdot)$ function measures the supervised loss between real and predicted labels. $neq(\cdot, \cdot)$ is a  0-or-1 function representing {\it not equal}.

%\begin{small}
\begin{equation}\label{alpha}
\alpha_{ij} = \tilde{A_{ij}} = \frac{A_{ij}}{\sqrt{deg(i)deg(j)}}, (i \neq j)
\end{equation} 
%\end{small}

\noindent where $A$ and $\tilde{A}$ are the adjacent matrix and negative normalized graph Laplacian matrix, and $deg(i)$ means the degree of $v_i$. It should be noted that our equation is slightly different from \cite{zhou2004learning}'s because we only consider explicit predicted label rather than label distribution.

Normalization is the core of reducing the marginal nodes' influence. Our approach is  simple: generating fake nodes, linking them to nearest real nodes and solving graph Laplacian regularization. \emph{Fake} label is not allowed to be assigned to unlabeled nodes and loss computation only considers edges between real nodes. The only difference between before and after generation is that marginal nodes' degree changes. And then the regularization parameter $\alpha_{ij}$ changes. 

\subsection{Proof}
We analyze 
how generated fake samples help acquire correct classification.

\begin{corollary}\label{decrease} Under Assumption \ref{convergence}, let $\mathcal{L}(\mathcal{C}_{gt})$ and $\mathcal{L}(\mathcal{C}_{gt})'$ be losses of ground truth on graph $(V + V_g, E + E_g)$ and $(V + V_g', E + E_g')$. We have $\forall V_g \supsetneqq V_g'$, $\mathcal{L}(\mathcal{C}_{gt}) < \mathcal{L}(\mathcal{C}_{gt})'$, where $V_g$ and $E_g$ are set of generated nodes and edges.
\end{corollary}

Corollary \ref{decrease} can be easily deduced because of $\alpha_{ij}$ decreasing. Loss of ground truth continues to decrease along with new fake samples being generated. That indicates ground truth is more likely to be acquired. However, there might exist other classification solutions whose loss decreases more. Thus, we will further prove that we can make a perfect classification under reasonable assumptions with adequate generated samples.
 
\begin{definition} \textbf{Partial Graph.} We define the subgraph induced by all nodes labeled $c$ (aka. $V_c$) and their other neighbors $Ne_c$ as partial graph $G_c$.
\end{definition}
\begin{assumption}\label{connectivity} \textbf{Connectivity.}
The subgraph induced by all interior nodes in each class is connected. Besides, every marginal node connects to at least one interior node in the same class.
\end{assumption}

\hide{
\begin{assumption}\label{necessary} \textbf{Necessary Labels.} 
There is at least one labeled node in every class.
\end{assumption}
}

Most real-world networks are dense and big enough to satisfy Assumption \ref{connectivity}. 
There actually implies another weak assumption that at least one labeled node exists for each class. This is the usually guaranteed by the setting of semi-supervised learning.
Let $m_c$ be the number of edges between marginal nodes in $G_c$. Besides, we define $deg_c$ as the maximum of degrees of nodes in $G_c$ and $loss_i$ as the supervised loss for misclassified labeled node $v_i$.
	
\begin{theorem} \textbf{Perfect Classification.} 
If enough fake samples are generated such that $\forall v \in \mathcal{M}, deg(v) > d_0$, all nodes will be correctly classified. $d_0$ is the maximum of $\max\limits_cm_c^2deg_c$  and $\max\limits_{c,v_i\in V_c}\frac{\lambda m_c}{loss_i}$.
\end{theorem}
\begin{proof}
We firstly consider a simplified problem in partial graphs $G_c$, where nodes from $Ne_c$ have already been assigned fixed label $c'$. We will prove that the new optimal classification $\mathcal{C}_{min}$ are the classification $\mathcal{C}'$, which correctly assigns $V_c$ label $c$. Since $\mathcal{L}(\mathcal{C}_{min})<\mathcal{L}(\mathcal{C}') < \lambda m_c\cdot \frac{1}{\sqrt{d_0\cdot d_0}} < \lambda m_c / \max\limits_{c,v_i\in V_c}\frac{\lambda m_c}{loss_i} \leq \min\limits_{v_i\in V_c}loss_i$, optimal solution $\mathcal{C}_{min}$ should classify all labeled nodes correctly. 

Suppose that $\mathcal{C}_{min}$ assigns $v_i,v_j\in \mathcal{I}$ with different labels. The inequality $\mathcal{L}(\mathcal{C}_{min})\geq \lambda\alpha_{ij} = \frac{\lambda}{\sqrt{deg(v_i)deg(v_j)}}\geq \frac{\lambda}{deg_c} \geq \frac{\lambda}{m_cdeg_c} \geq \frac{\lambda m_c}{d_0} > loss_{\mathcal{C}'}$ would result in contradiction.
%
According to analysis above and Assumption \ref{connectivity}, all interior nodes in $G_c$ are assigned label a $c$ in $\mathcal{C}_{min}$.

Suppose that $\mathcal{C}_{min}$ assigns $v_i \in \mathcal{M}\cap V_c, v_j\in \mathcal{I}$ with different labels and $(v_i, v_j)\in E$. Let $v_i$  be assigned with $c'$. If we change $v_i$'s label to $c$, then $\alpha_{ij}$ between $v_i$ and its interior neighbors will be excluded from the loss function. But some other edges weights between $v_i$ and its marginal neighbors might be added to the loss function. Let $\lambda\Delta$ denotes the variation of loss. 
The following equation will show that the decrease of the loss would lead to a contradiction.

\begin{equation*}
\footnotesize
\begin{split}
\Delta \leq & \sum\limits_{\substack{v_k\in \mathcal{M}\\(v_i, v_k) \in E_c}}\frac{1}{\sqrt{deg(v_i)deg(v_k)}} -  \sum\limits_{\substack{v_j\in \mathcal{I}\\(v_i, v_j) \in E_c}}\frac{1}{\sqrt{deg(v_i)deg(v_j)}} \\ 
\leq & \frac{1}{\sqrt{deg(v_i)}} (\frac{m_c}{\sqrt{\max\limits_cm_c^2deg_c}} - \sum\limits_{\substack{v_j\in \mathcal{I}\\(v_i, v_j) \in E_c}}\frac{1}{\sqrt{deg(v_j)}}) < 0
\end{split}
\normalsize
\end{equation*}

Suppose that $\mathcal{C}_{min}$ avoids all situations discussed above while $v_i \in \mathcal{M}\cap V_c$ is still assigned with $c'$. Under Assumption \ref{connectivity}, there exists an interior node $v_j$ connecting with $v_i$. As we discussed, $v_j$ must be assigned $c$ in $\mathcal{C}_{min}$, leading to contradiction. 
%
Therefore, $\mathcal{C}'$ is the only choice for optimal binary classification in $G_c$. That means all nodes in class $c$ are classified correctly. But what if in $G_c$ not all nodes in $Ne_c$ are labeled $c'$? 
%
Actually no matter which labels they are assigned, all nodes in $V_c$ are classified correctly. If nodes in $Ne_c$ are assigned labels except $c$ and $c'$, the proof is almost identical and $\mathcal{C}'$ is still optimal. If any nodes in $Ne_c$ are mistakenly assigned with label $c$, the only result is to encourage nodes to be classified as $c$ correctly. 

Finally, the analysis is correct for all classes thus all nodes will be correctly classified.
\end{proof}