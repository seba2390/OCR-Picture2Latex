\section{Experiments}\label{sec:experiments}

We conduct experiments on two citations networks and one entity extraction dataset.
Table \ref{table:dataset} summaries statistics of the three datasets.


To avoid over-tuning the network architectures and hyperparameters, in all our experiments we use a default settings for training and test.
Specifically, the classifier has 5 hidden layers with $(500,500,250,250,250)$ units. Stochastic layers are zero-centered Gaussian noise, with 0.05 standard deviation for input and 0.5 for outputs of  hidden layers. Generator has two 500-units hidden layers, each followed by a {\it batch normalization} layer. {\it Exponential Linear Unit} (ELU)~\cite{clevert2015fast} is used for improving the learning accuracy 
except the output layer of $G$, which instead uses $tanh$ to generate samples ranging from $-1$ to $1$. The trade-off factors in Algorithm~\ref{algo} are $\lambda_0=2,\lambda_1=1,\lambda_2=0.3$. Models are optimized by Adam~\cite{kingma2014adam}, where $\beta_1=0.5,\beta_2=0.999$. All parameters are initialized with Xavier~\cite{glorot2010understanding} initializer. 

\begin{table}[b]
 \setlength{\tabcolsep}{4pt}
\caption{Dataset statistics}\label{table:dataset}
\centering
\begin{tabular}{cccccc}
\hline
Dataset&nodes&edges&features&classes&labeled data\\
\hline
Cora & 2,708 & 5,429 & 1,433 & 7 & 140 \\
Citeseer& 3,327 & 4,732 & 3,703 & 6 & 120 \\
DIEL & 4,373,008 & 4,464,261 & 1,233,597 & 4 & 3413.8 \\
\hline
\end{tabular}
\end{table}

\subsection{Results on Citation Networks}\label{citation}
The two citation networks contain papers and citation links between papers.
Each paper has features represented as a bag-of-words and belongs to a specific class based on topic, such as ``database'' or ``machine learning''.
The goal is to classify all papers into its correct class.
For fair comparison, we follow exactly the experimental setting in \cite{yang2016revisiting}, where for each class 20 random instances (papers) are selected as labeled data and 1,000 instances as test data. The reported performance is the average of ten random splits.
In both datasets, we compare our proposed methods with three categories of methods:
\begin{itemize}
	\item \textbf{regularization-based methods} \\ including LP~\cite{zhu2002learning}, ICA~\cite{lu2003link}, and ManiReg~\cite{belkin2006manifold};
	\item \textbf{embedding-based methods} \\ including DeepWalk~\cite{Perozzi:14KDD}, SemiEmb~\cite{weston2012deep}, and Planetoid~\cite{yang2016revisiting};
	\item and \textbf{convolution-based methods}\\ including Chebyshev~\cite{defferrard2016convolutional}, GCN~\cite{kipf2016semi} and GAT~\cite{DBLP:journals/corr/abs-1710-10903}.
\end{itemize}
    
We train our models using {\it early stop} with 500 nodes for validation (average 20 epochs on Cora and 35 epochs on Citeseer). Every epoch contains 100 batches with batch size 64.
Table~\ref{tb:citation} shows the results of all comparison methods. Our method significantly outperforms all the regularization- and embedding-based methods, and also performs much better than Chebyshev and graph convolution networks (GCN), meanwhile slightly better than GCN with attentions (GAT). Compared with convolution-based methods, \smodel is more sensitive to labeled data and thus a larger variance is observed. The large variance might originate from the instability of training of GANs. For example, the \textit{mode collapse} phenomenon~\cite{theis2016note} will hamper \smodel from generating fake nodes evenly in density gaps. 
These instabilities are currently main problems in research of GANs. More advanced techniques for stabilizing \smodel are left for future work.

\begin{table}[t]
\caption{\label{tb:citation}Summary of results of classification accuracy (\%).}\label{results}
\centering
\begin{tabular}{c|ccc}
\hline \hline
Category&Method & Cora & Citeseer\\
\hline
\multirow{3}{*}{Regularization} & LP & 68.0 & 45.3\\
& ICA & 75.1 & 69.1\\
& ManiReg & 59.5 & 60.1 \\
\hline
\multirow{3}{*}{Embedding} & DeepWalk & 67.2 & 43.2\\ 
& SemiEmb & 59.0 & 59.6\\
& Planetoid & 75.7 & 64.7\\ 
\hline
\multirow{3}{*}{Convolution} 
& Chebyshev & 81.2 & 69.8\\ 
&GCN & 80.1 $\pm$ 0.5 & 67.9 $\pm$ 0.5\\
& GAT & \textbf{83.0 $\pm$ 0.7} & 72.5 $\pm$ 0.7\\
\hline
Our Method &\model & \textbf{83.0 $\pm$ 1.3} & \textbf{73.1 $\pm$ 1.8}\\
\hline \hline
\end{tabular}
\end{table}

\input{verification.tex}


\subsection{Results on Entity Extraction}
\label{diel}
The DIEL dataset~\cite{bing2015improving} is a dataset for information extraction. It contains pre-extracted features for each entity mentions in text, and a graph connecting entity mentions to corresponding coordinate-item lists. The objective is to extract medical entities from items given feature vectors, the graph topologies and a few known medical entities.

Again, we follow the same setting as in the original paper~\cite{bing2015improving} for the purpose of comparison, including data split and the average of different runs.
Because the features are very high dimensional sparse vectors, we reduce its dimensions to 128 by Truncated SVD algorithm. We use neighbor fusion on {\it item string nodes} with $\alpha = 0$ as only {\it entity mention nodes} have features. 
We treat the top-$k$ ($k$ = 240,000) entities given by a model as positive, and compare 
recall of top-$k$ ranked results by different methods.
Note that as many items in ground truth do not appear in text, the upper bound of recall is 0.617 in this dataset.

We also compare it with the different types of methods. 
Table~\ref{diel_result}
reports the average recall@$k$ of standard data splits for 10 runs by all the comparison methods.
{\it DIEL} 
represents the method in original paper~\cite{bing2015improving}, which uses outputs of multi-class label propagation to train a classifier. 
The result of Planetoid is the inductive result which shows the best performance among three versions. As the DIEL dataset has millions of nodes and edges, which makes full-batch training, for example GCN, infeasible(using sparse storage, memory needs 
$>$ 200GB),  we do not report GCN and GAT here. From Table~\ref{diel_result}, we see that our method \smodel achieves the best performance, significantly outperforming all the comparison methods ($p-$value$\ll$0.01, $t-$test).






\subsection{Space Efficiency}


Since GCN cannot handle large-scale networks, we examine the memory consumption of \smodel in practice. GPU has become the standard platform for deep learning algorithms. The memory on GPU are usually very limited compared with the main memory. We compare the GPU memory consumption of four representative algorithms from different categories in Figure \ref{bar}. Label Propagation does not need GPU, we show its result on CPU for the purpose of comparison. 

For small dataset, \smodel consumes the largest space due to the most complex structure. But for large dataset, \smodel uses the least GPU memories. LP is usually implemented by solving equations, whose space complexity is $O(N^2)$. Here we use the ``propagation implementation'' to save space. GCN needs full-batch training and cannot handle a graph with millions of nodes. Planetoid and \smodel are trained in mini-batch way so that the space consumption is independent of the number of nodes. High dimensional features in DIEL dataset are also challenging. Planetoid uses sparse storage to handle sparse features and \smodel reduces the dimension using Truncated SVD.  



\subsection{Adversarial Label Propagation}
 \label{subsec:morediscussions}

Will adversarial learning help conventional semi-supervised learning algorithms?
Yes, we have proved theoretically that reducing the influence of marginal nodes can help classification in \S~\ref{sec:theory}. So, we further propose an adversarial improvement for graph Laplacian regularization with generated samples to verify our proofs (Cf. \S~\ref{sec:theory}).
We conduct the experiment by incorporating adversarial learning into the
Label Propagation framework~\cite{zhou2004learning} to see whether the performance can be improved or not.
To incorporate nodes' features, we reconstruct graph by linking nodes to their $k$ nearest neighbors in feature space $\mathbf{x}$. We use the Citeseer network in this experiment, whose settings are described in \S~\ref{citation} meanwhile $k=10$.
 Generating enough fake data is time-consumable, therefore we directly determined marginal nodes by $p_f(\mathbf{x}_i) >\tau $ ($\tau$ is a threshold), because $$\mathbb{E}\frac{p_g(\mathbf{x})}{p_g(\mathbf{x}) + p_{data}(\mathbf{x})} =p_f(\mathbf{x})$$ 
\noindent where $p_f(\mathbf{x})$ is the probability of $\mathbf{x}$ to be fake.

Then, we increase marginal nodes' degree up to $r$ to reduce their influence. 

Figure~\ref{adv} shows the results. Smaller $p_f$ means more marginal nodes. The curves indicates that $\tau = 0.1$ is a good threshold to discriminate marginal and interior nodes. 
A bigger $r$ always perform better, which encourages us to generate as much fake nodes in density gaps as possible.
The performance of LP has been improved by increasing marginal nodes' degree, but still underperforms \model. 

In our opinion, the main reason is that \smodel uses neural networks to capture high-order interrelation between features. 
Thus, we also try to reconstruct the graph using the first layer's output of a supervised multi-layer perceptron and further observed improvements in performance, highlighting the power of neural networks in this problem. 

\begin{table}
\centering
\caption{Recall@k on DIEL dataset (\%).}\label{diel_result}
\begin{tabular}{c|cc}
\hline \hline
Category&Method & Recall@K\\
\hline
\multirow{2}{*}{Regularization} & LP & 16.2\\
&ManiReg & 47.7 \\
\hline
\multirow{3}{*}{Embedding}& DeepWalk & 25.8 \\
&SemiEmb & 48.6 \\
&Planetoid & 50.1 \\
\hline
Original&\textit{DIEL} & 40.5\\
\hline
Our Method& \model & \textbf{51.8} \\
\hline
&Upper bound & 61.7\\
\hline \hline
\end{tabular}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width=0.85\linewidth]{./img/bar.png}
	\caption{GPU Memory Consumption of four typical semi-supervised learning algorithms on graphs. GCN cannot handle large-scale networks. Planetoid and \smodel are trained in mini-batch way, which makes them able to scale up. We reduce the dimensions of features in DIEL dataset(Cf. \S~\ref{diel}) so that the GPU consumption of \smodel even decreases.} \label{bar}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{./img/adv.png}
	\caption{Performance of LP with adversarial learning. Larger threshold means less marginal nodes. Two curves below take $x_i$ as inputs and curves above use outputs of the first layer of MLP. 
	} \label{adv}
\end{figure}
