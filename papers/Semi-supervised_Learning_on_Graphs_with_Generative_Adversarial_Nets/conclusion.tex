\section{Related Work}\label{sec:related}
Related works mainly fall into three categories: Algorithms for semi-supervised learning on graphs, GANs for semi-supervised learning and GAN-based applications on graphs. We discuss them and summarize the main differences between our proposed model and these works as follows:
\subsection{Semi-supervised Learning on Graphs}
	As mentioned in \S~\ref{sec:intro}, previous methods for this task can be divided into three categories. 
    
Label Propagation~\cite{zhu2002learning} is the first work under the graph Laplacian framework. Labeled nodes continues to propagate their labels to adjacent nodes until convergence. After revealing the relationship between LP and graph Laplacian regularization~\cite{zhu2003semi}, the method are improved by sophisticated smoothing regularizations~\cite{zhu2003semi,belkin2006manifold} and bootstrap method~\cite{lu2003link}. This kind of methods mainly focus on local smoothness but neglect clustering property of graphs, making situations like Figure~\ref{example} hard cases. 
        
Deepwalk~\cite{Perozzi:14KDD} is the first work for graph embedding. As an unsupervised method to learn latent representations for nodes, DeepWalk can easily be turned to a semi-supervised baseline model if combined with SVM classifier. Since labels help learn embeddings and then help classification, Planetoid~\cite{yang2016revisiting} jointly learns graph embeddings and predicts node labels.
Graph embedding becomes one step in \smodel and we incorporate GANs for better performance.  

GCN~\cite{kipf2016semi} is the first graph convolution model for semi-supervised learning on graphs. Every filter in GCN learns linear transformations on spectral domain for every feature and combines them. More complex graph convolution methods~\cite{defferrard2016convolutional,DBLP:journals/corr/abs-1710-10903} show better performances. An obvious disadvantage of graph convolution is huge consumptions of space, which is overcome by \model. 

\subsection{GANs for Semi-supervised Learning}

Semi-supervised GANs(SGAN) were first put forward in computer vision domain~\cite{odena2016semi}. SGAN just replaces the discriminator in GANs with a classifier and becomes competitive with state-of-art semi-supervised models for image classification. Feature matching loss is first put forward to prevent generator from overtraining~\cite{salimans2016improved}. The technique is found helpful for semi-supervised learning, leaving the working principles unexplored~\cite{salimans2016improved}. Analysis on the trade-off between the classification performance of semi-supervised and the quality of generator was given in~\cite{dai2017good}. Kumar et al.~\shortcite{kumar2017semi} find a smoothing method by estimating the tangent space to the data manifold. In addition, various auxiliary architectures are combined with semi-supervised GANs to classify images more accurately~\cite{maaloe2016auxiliary,dumoulin2016adversarially,chongxuan2017triple}. All these works focus on image data and leverage CNN architectures. \smodel introduces this thought to graph data and first designs a new GAN-like game with clear and convincing working principles. 

\subsection{GAN-based Applications on Graphs}
Although we firstly introduce GANs to graph-based semi-supervised learning problem, GANs have made successes in many other machine learning problems on graphs.

One category is about graph generation. Liu et al.~\shortcite{liu2017learning} present a hierarchical architecture composed by multiple GANs to generate graphs. The model preserves topological features of training graphs. Tavakoli et al.~\shortcite{tavakolilearning} apply GANs for link formation in social networks. Generated network preserves the distribution of links with minimal risk of privacy breaches. 

Another category is about graph embedding. 
In GraphGAN~\cite{wang2017graphgan}, generator learns embeddings for nodes and discriminator solves link prediction task based on embeddings. Classification-oriented embeddings are got at the equilibrium. Dai et al.~\shortcite{dai2017adversarial} leveraged adversarial learning to regularize training of graph representations. Generator transforms embeddings from traditional algorithms into new embeddings, which not only preserve structure information but also mimic a prior distribution. 


\section{Conclusion and Future Work}
\label{sec:discuss}

We propose \model, a novel approach for semi-supervised learning over graphs using GANs. We design a new competitive game between generator and classifier, in which generator generates samples in density gaps at equilibrium. Several sophisticated loss terms together guarantee the expected equilibrium.
Experiments on three benchmark datasets demonstrate the effectiveness of our approach. 

We also provide a thorough analysis of working principles behind the proposed model \model. Generated samples reduce the influence of nearby nodes in density gaps so as to make decision boundaries clear. The principles can be generalized to improve traditional algorithms based on graph Laplacian regularization with theoretical guarantees and experimental validation. 
%Trained in mini-batch way, 
\smodel is scalable. Experiments on DIEL dataset suggest that our model shows good performance on large graphs too.% large to run graph convolution methods on. 

As future work, one potential direction is to
%will focus on 
investigate more ideal equilibrium, stabilizing the training further, accelerating training, strengthening theoretical basis of this method and extending the method to other tasks on graph data such as~\cite{Tang:08KDD}. 

\vpara{Acknowledgements.}The work is supported by the
%sogou 863
(2015AA124102),
%yuzhou li, with wendy
National Natural Science Foundation of China (61631013,61561130160),
and 
the Royal Society-Newton Advanced Fellowship Award.
