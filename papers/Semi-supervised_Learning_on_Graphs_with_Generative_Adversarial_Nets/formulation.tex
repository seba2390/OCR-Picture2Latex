\section{Preliminaries}
\label{sec:def}
\subsection{Problem Definition}
    Let $G=(V,E)$ denote a graph, where $V$ is a set of nodes and $E \subseteq V \times V $ is a set of edges. Assume each node $v_i$ is associated with a $k-$dimensional real-valued feature vector $\mathbf{w}_i \in \mathbb{R}^k$ and a label $y_i \in \{0,...,M-1\}$.
    If the label $y_i$ of node $v_i$ is unknown, we say node $v_i$ is an unlabeled node. We denote the set of labeled nodes as $V^L$ and the set of unlabeled nodes as $V^U=V\backslash V^L$. Usually, we have $|V^L| \ll |V^U|$. We also call the graph $G$ as \textit{partially labeled graph}~\cite{Tang:11PKDD}.
    Given this, we can formally define the semi-supervised learning problem on graph.

%\vspace{0.06in}
\begin{definition}\textbf{Semi-supervised Learning on Graph.}
	Given a partially labeled graph $G=(V^L\cup V^U, E)$, the objective here is to learn a function $f$ using features $\mathbf{w}$ associated with each node and the graphical structure, in order to predict the labels of unlabeled nodes in the graph.
\end{definition}
%\vspace{0.06in}
 
 Please note that in semi-supervised learning, training and prediction are usually performed simultaneously. In this case, the learning considers both labeled nodes and unlabeled nodes, as well as the structure of the whole graph.
    In this paper, we mainly consider transductive learning setting, though the proposed model can be also applied to other machine learning settings. 
    Moreover, we only consider undirected graphs, but the extension to directed graphs is straightforward.

\subsection{Generative Adversarial Nets (GANs)}

GAN~\cite{goodfellow2014generative} is a new framework for estimating generative models via an adversarial process, in which a generative model $G$ is trained to best fit the original training data and a discriminative model $D$ is trained to distinguish real samples from samples generated by model $G$.
The process can be formalized as a min-max game between $G$ and $D$, with the following loss (value) function:

\beq{\label{gan_aim}
\min\limits_G\max\limits_D V(G,D) = \mathbb{E}_{\mathbf{x}\sim p_{d}(\mathbf{x})} \log D(\mathbf{x}) 
+ \mathbb{E}_{\mathbf{z}\sim p_z(\mathbf{z})} \log [1-D(G(\mathbf{z}))]
}

\noindent where $p_{d}$ is the data distribution from the training data, $p_z(\mathbf{z})$ is a prior on input noise variables.




