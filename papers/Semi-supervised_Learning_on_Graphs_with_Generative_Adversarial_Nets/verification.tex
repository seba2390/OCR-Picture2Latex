 
\subsection{Verification}
\label{sec:verify}

\begin{figure*}[htbp]
 \centering
  \includegraphics[width=1.0\textwidth]{./img/composition2.png}
 \caption{(a)Visualization of outputs of feature layer during training. Color of point indicates its class, where black nodes indicate fake samples. 
 	 (b)Typical $||g||$ and $p_f$ curve for marginal nodes. Horizontal axis represents the number of training iterations, vertical axis representing the value. (c) Typical $||g||$ and $p_f$ curve for interior nodes.}\label{process}
 \end{figure*}


We provide more insights into \smodel with experimental verifications. There are two verification experiments: one is about the expected equilibrium, and the other verifies the working principles of \model.

\subsubsection{Verification of equilibrium}
The first experiment 
is about whether \smodel converges at the equilibrium described in \S~\ref{LD}. In Figure \ref{process}(a), we visualize the training process in the Citeseer experiment using t-SNE algorithm (Cf. \S~\ref{sec:experiments} for detailed experimental settings). At the beginning, $G$ generates samples very different from real samples and the boundaries of clusters are ambiguous. During training, classifier $D$ gradually learns a non-linear transformation in $h^{(n)}(\mathbf{x})$ to map real and fake samples into distinct clusters, while $G$ tries to generate samples in the central areas of real samples. Mini-batch training, $loss_{pt}$ in $\mathcal{L}_G$ and Gaussian noises prevent $G$ from overfitting on the only center point. Adversarial training finally reaches the expected equilibrium where fake samples are in the central areas surrounded by real samples clusters after 20 epochs. 

\subsubsection{Verification of working principles}

The second experiment is to verify the proposed working principles. We have proved in \S~\ref{sec:theory} theoretically that reducing the influence of marginal nodes can help classification. But we should further verify whether generated samples reduce the influence of marginal nodes. On one hand, nodes are mapped into distinct and far clusters in $h^{(n)}(\mathbf{x})$. On the other hand, the ``influence'' is related with ``smooth degree''. For example in graph Laplacian regularization framework, difference between labels of adjacent nodes are minimized explicitly to guarantee the smoothness. 
Thus we examine classifier function's smooth degree around density gaps. Smooth degrees at $x_i$ are measured  by the norm of the gradient of maximum in probabilities for each class. 

	\[||g(\mathbf{x}_i)|| = ||\nabla_{\mathbf{x}_i} \max\limits_{y=0}^{M-1} P(y |\mathbf{x}_i, y_i < M)||\]

Let $p_f(\mathbf{x}_i)$ be predicted fake probability of $\mathbf{x}_i$. We draw curves of $||g(\mathbf{x}_i)||$ and $p_f(\mathbf{x}_i)$ during training. Figure~\ref{process}(b)(c) show two representative patterns: (b) is a marginal node, whose $||g(\mathbf{x}_i)||$ and $p_f(\mathbf{x}_i)$ change synchronously and strictly share the same trend, while (c) is an interior node never predicted fake. The classifier function around (c) remains smooth after determining a definite label. Pearson correlation coefficient $$r_p = \frac{cov(||g||, p_f)}{\sigma_{||g||}\sigma_{p_f}}$$ exceeds 0.6, indicating obvious positive correlation. 


