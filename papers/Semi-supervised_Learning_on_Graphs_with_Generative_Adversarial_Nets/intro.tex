\section{Introduction}\label{sec:intro}

\hide{
Many learning-based applications on graphs suffer from statistically insufficient labeled data. 
For instance, in a large citation graph, it would be difficult to have a large number of labeled papers on ``SVM applications''.
Semi-supervised learning thus has been proposed to improve the learning accuracy by leveraging the large unlabeled data for training.
}

Semi-supervised learning on graphs has attracted great attention  both in theory and practice. 
Its basic setting is that we
are given a graph comprised of a small set of labeled nodes and a large set of unlabeled nodes, and the goal is to learn a model that can predict label of the unlabeled nodes.

There is a long line of works about semi-supervised learning over graphs. 
One important category of the research is mainly based on the graph Laplacian regularization framework. 
For example, 
Zhu et al.~\shortcite{zhu2002learning} proposed a method called Label Propagation for learning from labeled and unlabeled data on graphs, and later the method has been improved by Lu and Getoor~\shortcite{lu2003link} under the bootstrap-iteration framework.
Blum and Chawla~\shortcite{blum2001learning} also formulated the graph learning problem  as that of finding min-cut on graphs. 
Zhu et al.~\shortcite{zhu2003semi} proposed an algorithm based on Gaussian random field and formalized graph Laplacian regularization framework. 
Belkin et al.~\shortcite{belkin2006manifold} presented a 
regularization method called ManiReg by exploiting geometry of marginal distribution for semi-supervised learning. 
The second category of the research is to combine semi-supervised learning with graph embedding.
Weston et al.~\shortcite{weston2012deep} first incorporated deep neural networks into the graph Laplacian regularization framework for semi-supervised learning and embedding. Yang et al.~\shortcite{yang2016revisiting} proposed the Planetoid model for jointly learning graph embedding and predicting node labels. Recently, Defferrard et al.~\shortcite{defferrard2016convolutional} utilized localized spectral Chebyshev filters to perform convolution on graphs for machine learning tasks. Graph convolution networks (GCN) \cite{kipf2016semi} and its extension based on attention techniques~\cite{DBLP:journals/corr/abs-1710-10903} demonstrated great power and achieved state-of-art performance on this problem.  

This paper investigates the potential of generative adversarial nets (GANs) for  semi-supervised learning over graphs.
GANs~\cite{goodfellow2014generative} are originally designed for generating images, by training two neural networks which play a min-max game: discriminator $D$ tries to discriminate real from fake samples and generator $G$ tries to generate ``real'' samples to fool the discriminator. 
To the best of our knowledge, there are few works on semi-supervised learning over graphs with GANs.

We present a novel method \smodel for semi-supervised learning on graphs with GANs.
\smodel
maps graph topologies into feature space and jointly trains generator network and classifier network. 
Previous works~\cite{dai2017good,kumar2017semi} tried to explain semi-supervised GANs' working principles, but only found that generating {\it moderate fake samples} in complementary areas benefited classification and analyzed under strong assumptions. 
This paper explains the working principles behind the proposed model from the perspective of game theory.
We have an intriguing observation that 
fake samples in low-density areas between subgraphs
can reduce the influence of samples nearby, thus help improve the classification accuracy. 
A novel GAN-like game is designed under the guidance of this observation. Sophisticated losses guarantee the generator generates samples in these low-density areas at equilibrium. 
In addition, integrating with the observation, the
graph Laplacian regularization framework (Equation (\ref{basic})) can leverage clustering property to make stable progress.
It can be theoretically proved that this adversarial learning technique yields perfect classification for semi-supervised learning on graphs with plentiful but finite generated samples. 

The proposed \smodel is evaluated on several different genres of datasets. Experimental results show that \smodel significantly outperforms several state-of-the-art methods. \smodel can be also trained using mini-batch, thus enjoys the scalability advantage. 

Our contributions are as follows:
\begin{itemize}
\item We introduce GANs as a tool to solve classification tasks on graphs under semi-supervised settings. \smodel generates fake samples in low-density areas in graph and leverages clustering property to help classification. 

\item We formulate a novel competitive game between generator and discriminator for \smodel and thoroughly analyze the dynamics, equilibrium and working principles during training. In addition, we generalize the working principles to improve traditional algorithms. Our theoretical proof and experimental verification both outline the effectiveness of this method.  

\item We evaluate our model on several dataset with different scales. \smodel significantly outperforms previous works and demonstrates outstanding scalability.
\end{itemize}

The rest of the paper is arranged as follows. In Section~\ref{sec:def}, we introduce the necessary definitions and GANs. In Section~\ref{sec:gen_fs}, we present \smodel and discuss why and how the model is designed in detail. A theoretical analysis of the working principles behind \model is given in Section ~\ref{sec:theory}. We outline our experiments in Section~\ref{sec:experiments} and show the superiority of our model. We close with a summary of related work in Section~\ref{sec:related}, and our conclusions.
