\section{Related work} \label{sec:related}
Our research was influenced by and makes contributions to research that can
broadly be classified into two categories: platform moderation strategies and
their consequences; and the influence of externalities on platform moderation.

\para{Platform moderation strategies and their consequences.} 
The dilemma of how to moderate effectively without resorting to extreme
restrictions on discourse is not new to platforms as they increasingly find
themselves grappling with challenges arising from being too strict or too
lenient. Angwin \cite{stealing-myspace} highlighted how restrictions and
moderation on Friendster led to mass user migrations to more lenient platforms
such as MySpace. Conversely, overly lenient moderation also presents problems
for platforms. For example, the failure to address trolls and misogynistic
content led to a loss of users along with the withdrawal of several offers to
purchase and invest in Twitter \cite{disney-salesforce}. Increasingly, however,
we find platforms offering moderation strategies as a commodity: some advertise
increased safety and protection for its users (\eg Reddit and Twitter) while
others advertise no restrictions on discourse (\eg Gab, Parler, and 4chan).
Several studies, detailed below, have shown the former to suffer from
inconsistency in moderation while the complete lack of moderation in the latter
has been found to encourage extremism and toxicity \cite{zannettou2018gab,
hine2017kek}.

% Recent
% studies, however, have highlighted that the fundamental design of platforms
% itself encourages toxicity and extremism \cite{massanari2017gamergate,
% lauer2021facebook}, suggesting a fundamental flaw to be addressed prior to
% moderation strategies. 
% 
\parait{The challenge of consistent and timely moderation.}
Numerous works have tracked discourse on platforms, specifically to measure the
effectiveness of community-level interventions to suppress dangerous discourse.
Early research conducted on Reddit \cite{eshwar2017you} showed the
effectiveness of interventions applied to \subreddit{fatpeoplehate} and
\subreddit{coontown}. The study revealed a significant downturn in the amount
of incivility amongst community members after the intervention was applied.
However, this finding has been contradicted by several recent studies which
have shown users and discourse from banned communities migrating to newer
communities while maintaining or increasing their incivility
\cite{habib2019act, ali2021understanding}. Habib \etal hypothesize that
increasing inconsistency by platform administrators may be the reason for this
contradiction. Our research which suggests a reactionary moderation strategy
supports this hypothesis.
%
Researchers have highlighted that inconsistencies associated with
moderation may be attributed to the high cost and inherently poor scalability
of human moderation and have proposed machine-learning based tools to assist
moderators (Reddit moderators, specifically) identify communities at risk of
violating platform rules \cite{habib2019act, 2019crossmod}. Additionally,
relying on human moderators as a first line of defense has been shown to have
a severe effect of their mental health \cite{We-are-the-nerds,
roberts2014behind, wohn2019volunteer}. 

\parait{The consequences of inconsistent moderation.}
The effects of moderation inconsistencies have been found to be
substantial. In the context of the 2016 US Presidential elections, several
researchers \cite{benkler2018network, allcott2017social} found that discourse
on social media platforms played a significant role in amplifying propaganda
and fake news. These problems continue to arise today as online platforms
provide a home for fringe elements promoting violent or problematic conspiracy
theories. Failure to act effectively against such harmful ideologies by way of
timely moderator interventions has been shown to result in the development of
more extreme ideologies amongst community members. For example, researchers
\cite{mamie2021anti} showed that anti-feminist communities acted as a pathway
to more radical alt-right communities. Further, the recent attack on the US
Capitol and protests in Charlottesville that resulted in multiple deaths are
both known to have been planned in online communities including large platforms
such as Twitter and Parler \cite{prabhu2021capitol}. The importance of timely
interventions on toxic content has been further highlighted by Scrivens \etal
\cite{scrivens2020examining} who showed that there existed a gradual increase
in the approval of toxic content in response to consistent toxic posting by
community members. These results are in line with other studies showing how
communities can become more extreme over time \cite{simi2015american,
wojcieszak2010don, caiani2015transnationalization, wright2020pussy,
ribeiro2020evolution}.

% 
% Even when the rules of engagement are laid out in a way that handles the
% delicate balance between facilitating uncivil discourse and resorting to
% censorship, there remain challenges associated with their enforcement. For
% example,
% Taken together, the above studies highlight the challenges associated with
% effective platform moderation as well as the problems that arise from untimely
% and ineffective moderation. Our work adds to this body of literature by
% demonstrating the costs of the inconsistent interventions that arise from
% media-driven moderation.
% 

% Most platforms now
% offer moderation as a commodity to 
% enable open and civil discourse. Some argue for unrestricted and unmoderated
% online platforms however, due to the design of modern social media platforms
% with algorithmic personalization, anonymity, and filter bubbles, the theory of
% marketplace of ideas is not applicable in online communities and moderation has
% become a necessaity. The design of these platforms encourages toxicity and
% extremism \cite{massanari2017gamergate} with the companies profiting from such
% content \cite{lauer2021facebook}. On the user's end, moderation has shown to be
% effective in drastically improving the interaction of quality and content on
% mental health communities \cite{wadden2020effect} however, it also results in
% conformity and acts as a gatekeeper to ideas. 

\para{External forces influencing platform moderation.}
Platform moderation does not operate without influence from external
(particularly, economic and regulatory) forces. Numerous research efforts have
analyzed the impact of the online advertising ecosystem on platform moderation.
Bozarth \etal \cite{bozarth2021market} show how many fake news websites are
mostly funded by top-tier advertising firms and an effective strategy towards
combating fake news would be to have these advertisers blacklist these sites.
Braun \etal \cite{braun2019activism} showed how the `Sleeping Giants', an
activist group, strategically reported events of misinformation and racism to
brands and advertisers (rather than the platforms themselves) in an effort to
pressure them to withdraw their advertisements. This direct impact on the
revenue streams of online platforms was found to cause changes in the
moderation of misinformation and racist content. Along a similar vein, in 2019,
YouTube experienced
a series of boycotts from advertising agencies and brands in retaliation to the
proliferation of toxic content. This event, now known as the `Adpocalopyse'
resulted in a large number of changes in YouTube's content policies, comment
moderation, as well as video monetization policies \cite{kumar2019algorithmic,
%dunna2021pay, 
caplan2020youtube}. These studies reflect the impact that
pressure from advertisers can have on the moderation policies of online
platform. Our work suggests that Reddit may not be an exception. 
% Relying on tech companies to moderate platforms against their monetary
% advantage, ad tech giants to blacklist dangerous sites or mainstream media to
% pressure platforms to close communities can only be taken as temporary
% solutions. The United States, notably, has no regulations which impact
% moderation on online platforms. In fact, Section 230 of the US Communications
% and Decency Act effectively absolves platforms of all responsibility and
% liability for content hosted and promoted by them. In recent years, however,
% there have been proposals by both sides of the political spectrum, albeit with
% different motivations, to increase the burden on platforms to perform effective
% and explainable moderation. While it remains to be seen if any regulatory
% changes occur, the impact on platform moderation will be severe. Legal scholars
% have proposed numerous changes and studied their hypothetical impacts on
% platform moderation strategies \cite{goodman2019section, cramer2020liability,
% raynor2021internet} with dire warnings of the possible impacts of imprecise
% language in the proposals.
% 

%\para{Media influences on Social media}
%Zannettoue \etal \cite{zannettou2017web} measured influence of fringe communities on mainstream platforms. Their research highlights the importance of moderation for all platforms regardless of its size and popularity. Moreover, previous works suggest significant influence of social media platforms on the mainstream news media. For example, several studies have acknowledged the significant role platforms played in the 2016 U.S. Elections \cite{benkler2018network, allcott2017social}. Similar research efforts have also shown how social media platforms with lenient policing have been used to spread propaganda such as QAnon to mainstream media consequently legitimizing fringe and extremist ideas and bringing them to a widespread audience. On the other hand, Conway \cite{conway2020routing} shows how the normalization of extreme opinions by prominent figures such as Donald Trump and mainstream news networks such as Fox News have caused a shift in the Overton window (the range of acceptable ideas by the public). This has made difficult for social media paltforms to identify and respond to these extreme ideas without infringing on someones freedom of speech. This lack of clarity and action from the social media platforms results in more extremism.
% 
% \subsection{Media}
% To understand the impact of social media platforms and therefore its moderation,
% several studies have been conducted to explore the growing relationship between
% mainstream media and social media platforms. Zannettou \etal
% \cite{zannettou2017web} measure and show the influence of fringe communities on
% mainstream platforms. Their research highlights the importance of moderation for
% all platforms regardless of its size and popularity. Moreover, previous works
% suggest significant influence of social media platforms on the mainstream news
% media. For example, several studies have acknowledged the significant role
% platforms played in the 2016 U.S. Elections \cite{benkler2018network,
% allcott2017social}. Similar research efforts have also shown how social media
% platforms with lenient policing have been used to spread propaganda to
% mainstream media consequently legitimizing fringe and extremist ideas and
% bringing them to a widespread audience. Among these are extremist ideologies and
% misinformation campaigns such as anti-vaccincation, climate deniers and QAnon
% all of which have had their origin from fringe communities and rose to wide
% spread attention eventually being discussed in political debates. It is only the
% introduction of these ideas into the mainstream that have resulted in wider
% attention from media and consequently crackdown from social media platforms.
