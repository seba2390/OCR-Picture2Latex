\section{Experimental Setup, Challenges  Encountered and Solution Approaches}
%System Architecture and Experimental Setup
\label{sec:arch}
The architecture of our deployed smart farm ecosystem is shown in Figure \ref{fig:arch}. We used Grove Sensors\footnote{https://www.seeedstudio.com/category/Sensor-for-Grove-c-24.html}, a Raspberry Pi Zero\footnote{https://www.raspberrypi.org/products/raspberry-pi-zero/}, a Grove Base Hat to connect the sensors to the Pi, an ORIA Temperature Sensor/Hygrometer, and basic peripheral hardware items including a monitor, keyboard, and mouse. The Grove sensors used were to collect data on air quality (Grove - Air Quality Sensor v1.3), light readings (Grove - Light Sensor), and soil moisture values (Grove - Capacitive Soil Moisture Sensor (Corrosion Resistant)). The ORIA sensor was chosen for its Bluetooth connectivity capability and accompanying smartphone application SensorBlue. The device was placed in our greenhouse environment and when the smartphone with the SensorBlue app was close to the device, it would dump all of the accumulated data to the app via Bluetooth which was then later exported and  formatted into a CSV file. This was helpful in allowing us to select the exact date range that we used for the Grove sensors, and for which we want to process the data. % helped us quickly select the range of dates we wished to process data for. 
The Raspberry Pi Zero was chosen because it is wide availability and cost effective with all the connectivity capabilities needed in a smart farming system. It also works well with the Grove Base Hat which is necessary for connecting to sensors. All of the peripheral hardware we used was for starting and stopping data collection, and later for moving the datasets to our personal machines. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=.75\textwidth]{figures/setupcrop-v2.png}
    \caption{Overview of Deployed Architecture}
    \label{fig:arch}
\end{figure*}

Initially, the sensors were deployed in a single house plant to fine-tune parameters for data collection such as sampling interval, boundary conditions, and time and date formatting. After collecting data for 10 days indoors, we made changes to the Python source code that controls the sensors. Next we were able to move our sensors to Shipley greenhouse owned by Tennessee Technological University (TTU). This allowed us to collect data in an environment where the temperature, light conditions, and watering schedule of the plant in question were more variable than they were indoors, providing a real-world environment for data collection. Our collaboration with the agricultural department of TTU was instrumental in helping us collect data for our model. Scientists there also provided us with the watering and light \textbf{schedules} so that when we observed the data we would be able to identify patterns of behavior. These schedules in turn helped us determine the types of values that would merit labeling an event as anomalous or not. A photo of the deployment set-up in a plant is shown in Figure \ref{fig:datacoll}. Further expansion of this work could involve placing sets of sensors in several plants and connecting them all to a central Raspberry Pi. This would allow more precise comparisons of data and more accurate anomaly detection.



\subsection{Hardware and Sensors Deployed with Limitations}
Grove sensors, though capable of sensing within our necessary accuracy, are on the low end of the price range, analogous to any smart farming deployment. The sensors we used all cost less than \$10, which are kind of what will be likely used in a real ecosystem. Through extensive periods of exposure to heat and moisture, the sensors were prone to fail. The environment inside the greenhouse was consistently in the 90\degree range. Several times throughout the data collection period, the sensors disconnected from the Raspberry Pi temporarily, stopping the stream of data. In order to prevent this and collect data continuously, the sensors and the Pi were enclosed in a box and stored away from direct sunlight and near air circulation. Unfortunately, this also caused several issues which are discussed below.

Another issue with IoT devices is their interconnectivity. In our case, all of the Grove sensors are connected to the Raspberry Pi through a Grove Base Hat. There are several vulnerabilities associated with this aspect of the system. Each sensor connects to ports on the Grove Base Hat with 4-pin cables. These cables fit into the ports loosely and are easily disconnected. If one sensor's connection is compromised and stops collecting data, and the rest of the sensors continue to collect, all of the data will be associated with different points in time. A benefit to the connectivity between devices is that if each Grove sensor shows the same anomalous pattern, it could be clearer where the anomaly originated, likely in the Grove Base Hat, the Raspberry Pi, or somewhere further upstream. Another physical vulnerability is that we stored the Raspberry Pi and sensor set-up inside a plastic container, as mentioned above. The cables were fitted through the top and then placed on or around the plant we used for the experiment. The sensor cables were prone to unplugging during set-up and we sometimes did not know until we went back to the farm to look at the data we collected. More snug fitting cables could make connectivity more reliable and so could creating a higher quality storage box for the devices.  

It must be noted that the issues highlights in this discussion are specific to our deployment, and we believe in more realistic settings such concerns will be taken care in advance. Our goal in this paper is not to discuss or highlight architectural limitations, but to propose a novel solution to detect anomalies in smart farming ecosystems.

\begin{figure}[t!]
    \centering
    \includegraphics[width=6cm]{P4091031.pdf}
    \caption{Data Collection Set-Up at Shipley Greenhouse}
    \label{fig:datacoll}
\end{figure}
\subsection{Sensor Source Code Modifications}
\label{Subsec:soft}
We deployed the Grove sensors using the Python source code files available with each Grove product. These can be accessed through  GitHub\footnote{https://github.com/Seeed-Studio/grove.py}. We altered several sections of the source code to better suit our system. One of the changes we made to each file was to save the data we collected locally. We decided to store the data this way because the greenhouse we used for our project did not have reliably functioning WiFi, which could be the case in a real smart farm as well. Storing locally gave us a way to have access to the data and to leave the sensors running without worrying that a lost internet connection could be contributing to any discrepancies in the data. Although, attacks using internet connectivity \cite{sontowski2020cyber} and or signal connectivity issues would cause anomalous spikes in the data in a true smart farm environment, we ignore such attacks here in order to gain an understanding of what the collected data should look like with as few interruptions as possible. The data was stored locally as a CSV file to make processing easier later in the project. Along the same lines, we removed any non-numeric data being stored by the sensors. The numeric readings were often originally saved with a descriptor such as ``High Pollution" for the air quality sensor or ``Dry" in the case of the soil moisture sensor. These descriptors could not be processed in our model in any meaningful way so they were removed. We also changed the interval of sampling for each Grove sensor to every 60 seconds. Initially, the sensors were set to collect every tenth of a second. This produced too many data points within the time we allotted for data collection. It also created an issue in matching all the sensor outputs together. Changing the interval to 60 seconds meant that we could group all reading by the minute they were collected. We found no issue in reducing the number of samples in terms of lost specificity among changes in the data. However, the temperature and humidity sensor stores one data point every 10 minutes. It did not have source code that we could access, so we made an alternative change, as elaborated in the data processing subsection \ref{subsec:dp}. The last general change made was to the time and date formatting. We used the \texttt{time.asctime()} function in Python to ensure we could separate data points in the CSV file for processing \cite{pythontime}. Each Grove sensor's source code had individual changes made to them as outlined below. %newline

\subsubsection{Air Quality Sensor}
For the most part, the sensors' readings are categorized using \texttt{if-else} loops. The air quality sensor has two groups that readings can fall into: ``High Pollution" and ``Air Quality OK". Originally the threshold for a High Pollution reading was a value of 100 or more. The information page\footnote{https://www.seeedstudio.com/Grove-Air-Quality-Sensor-v1-3-Arduino-Compatible.html} for this sensor said it is responsive to a variety of harmful gases. The specific gas that causes the reading cannot be determined, only that the level of the harmful gas is high. We performed several tests to see how easy it would be to reach a 100 reading. We exposed the sensor to a can of wood stain, which contains potent toxic gases. This raised the reading to around 130. This led us to alter the threshold for ``High Pollution" to a lower value, specifically 40, because exposure to harmful gases similar to wood stain would be damaging to the plants. Setting the threshold at 40 allowed us to see when there were small but out-of-the-ordinary changes in the air quality. 
%\newline
 

\subsubsection{Light Sensor}
The light sensor contains a photo-resistor which detects changes in light intensity in the environment. The brighter the light is, the lower the resistance of the sensor is. Lower resistance means that a higher voltage can be achieved, making the sensor value high in bright conditions and low or even zero in the dark. We found that our data reflected this well. The sensors read values between 0 and 10 in the hours of the night and up to 640 in the brightest part of the day. The readings produced clear separations between night and day, so we didn't change anything regarding the thresholds for this sensor. %\newline

\subsubsection{Capacitive Moisture Sensor}
As the name suggests, this sensor uses changes in capacitance to determine the level of moisture in soil. The readings are counter-intuitive because the more moisture is detected in the soil, the lower the output reading. The sensor initially had two groupings: ``Wet" and ``Dry". We performed several tests and determined that the readings would be better separated into three groups. Originally the threshold for a ``Dry" reading was a value between 0 and 300 and ``Wet" reading was anything greater than 300. When we started our initial set-up and testing of the sensors, we found that when the sensor was exposed to soil that had just been fully watered, it produced a ``Dry" reading. Several tests were performed to make sure that this was not a fluke. Ultimately we decided we would need to alter the code to switch the values around to achieve consistency in the readings and environment. After changing the values in this way, we also found that the values of 0 up to 600 did not fully encompass what we wanted them to. First, the sensor was placed in the soil approximately 6 inches away from the plant. This meant that when the irrigation system turned on for the plant, the sensor would be exposed to running water. This produced values over 1900. This led us to changed the threshold for a ``wet" reading to anything greater than 1900, ``Moist" to between 1300 and 1900, and ``Dry" to any reading less than 1300. Although ultimately we removed the language descriptors from the data before processing, adding in the third grouping helped us understand the sensor readings better.
%\newline

\section{Smart Farming Data and Anomalous Behaviour}
\label{sec:data}
 Data produced from Grove Sensors is time-series data. Time series data is a collection of quantities that are assembled over even intervals in time and ordered chronologically \cite{fu2021hawatcher}. Information collected from any ”smart” environment is considered actionable, in that it can be used to make decisions and take actions on critical systems \cite{adwithauto}. This means that the actuation processes are directly affected by all the data in a system. Reducing anomalous patterns and alerting the user when data is out of the ordinary is imperative to keep smart systems running smoothly. The trouble in identifying anomalies in sensor data is that "normal” can look different every day and oftentimes, several times within one day. On a sunny day with high light and temperature readings, you would expect all the data for the day to flow together seamlessly. However, a thunderstorm could suddenly blow through, and cause a decrease in light and temperature readings. This would be considered anomalous, but not in a \textit{harmful} way. On a day with no storms, a quick decrease in the same sensor measurements would be considered anomalous and potentially harmful or incorrect. Detection of anomalies, both harmful and benign, is the goal of this work.

\subsection{Data Collection Phase}
 \begin{figure}[t!]
    \centering
    \includegraphics[width=9cm]{figures/datasnap-v2.png}
    \caption{Data Collected Over 11 Non-Consecutive Days}
    \label{fig:data}
\end{figure}



We began data collection at the greenhouse in the first week of April in 2021. We used our understanding of the weather (based on historical data) in Middle Tennessee region during the spring to help us understand what normal readings should be. In the first phase, the sensors collected data for 7 days and were stopped. 
This was the first time all of the sensors had run simultaneously in an outdoor real environment and with the changes we made to the interval of collection in the source code as discussed in previous section. The data was collected every 60 seconds. Roughly 10,100 data points were collected for the first phase which we started processing for our ML model. In month of may, we began the second phase of collection. After processing the first dataset, we decided it would be beneficial to train the machine learning model we chose with more data. This time the sensors ran for 6 days and produced approximately 7200 data points. The total number of data points collected was 17349. We believe this dataset\footnote{Please send an an email to mgupta@tntech.edu, if you are interested in working with the dataset.} can be used by the community interested in learning more about the behavior of Grove and ORIA sensors, and working on smart farming research. %\hl{CAN YOU EXPLAIN MORE IN WHAT INTERVALS DATA WAS COLLECTED?}

As shown in Figure \ref{fig:data}, the data translated well visually. The light sensor provided extremely consistent readings, showing distinct night and day times. Other sensors provided less distinct readings, but were generally consistent overall. There are several factors worth mentioning here. For the first 7 days, the moisture sensor fluctuated up and down daily. The change over to the second collection set is distinguished by a sharp peak and then a more level set of points for the next 6 days. As shown in Figure \ref{fig:datacoll}, the moisture sensor is the only one installed in the actual plant. The difference in readings in the first and second set could be due to a different placement in the plant with reference to the irrigation system location. It could also be due to whether the sensor was put in the soil with its electrodes facing the water supply or away from it. It is worth mentioning that the orientation of each of the sensors has the potential to drastically affect the sensor readings. We paid close attention to this after we found the jump in the moisture data.

Creating this environment from scratch meant several things about the data we received. First, this data is unlabeled. It was taken directly from the sensors and then imported for processing. As mentioned in subsection \ref{Subsec:soft}, we made small adjustments to the number of data points collected by each sensor so that they would all match up. However, we did not individually label each data point as anomalous or not. With over 17000 data points, this would have been incredibly time consuming and difficult to achieve accuracy in. \textit{Therefore, the model we selected needed to be able to process unlabeled data, and an Autoencoder is capable of this, as discussed in the next section.} Also, the data falls into the time-series category. Time is an attribute of these data points and implies a correlation between neighboring points. This data is known to contain multiple types of irregularities: contextual, global, and collective \cite{anodot}. Our dataset had predominantly contextual and global outliers. Contextual means that the outlier is only out-of-the ordinary for the time slot it was found in or among its neighboring points. Global means the anomalous point would be anomalous no matter where it was found in time, or with reference to other points. The severe drop in Temperature data in Figure \ref{fig:data} would be an example of both. No other data points were this low in temperature, but even at the lowest temperature, there were no other spikes this low. 

\subsection{Data Processing Phase}
\label{subsec:dp}
In order for our model to properly analyze our input data, we had to pre-process the data before passing it into the model. First, as mentioned in the subsection \ref{Subsec:soft}, the interval of collection between the Grove sensors and the ORIA sensor were different. There was no built-in way to change the ORIA collection interval. To keep the data points separated by a minute, each temperature and humidity reading was copied into each one minute slot encompassed by that ten minute window. For example, on April 16 at 10:19 AM the temperature reading was 85.44 F and the humidity was 36.90\%. This reading was used for each minute from 10:19 AM until the next reading was taken at 10:29 AM. It would be unlikely that a temperature or humidity change large enough to trigger an anomalous reading could occur within these ten minute intervals. Therefore, this is an alteration we felt was relatively low risk. Next, we combined all the data from each of the individual sensors into a single data frame. This helped us visualize data, ensure there were no null values from any sensors, and allowed us to plot our readings more easily \cite{df}.

\subsection{Anomalies}
In order to create anomalies, normal patterns in the data have to be identified first. An anomaly is a pattern or instance in the data that does not conform to a well-defined notion of normal behavior \cite{chandola2009anomaly}. Several anomalous instances occurred naturally in the data. These were removed from the training data set, but left in the testing data set. In Table \ref{table:anoms}, these are listed as "Natural Anomalies". An example of a naturally occurring anomaly is that the temperature sensor has a reading that is "too low". This means that based on a comparison to the current high and low temperatures for Middle Tennessee in April, a reading of less than 54° F would be too low to be normal. It's not so important that this value could never be reached, but that it would be out of the ordinary to get a reading that low. Since we set this floor value ourselves, it would need to be changed based on the location of sampling and time of year. The second section of anomalies is called "Potential Anomalies". These are also bounds on values that we set manually. These statements such as "Moisture too Low" or "Humidity too Low" are readings we would consider anomalous. We selected the bounds for each sensor based on what values we saw over the course of our experimentation. We also injected some anomalies after collection to test the performance of our model. We found that the model was able to detect injected anomalies for all of our sensors. This once again proves that even though we do not train the model with anomalies included in the training data, our approach is still able to detect anomalous readings for each of our devices. A full list of these anomalies can be found in Table \ref{table:anoms}. %\hl{NEED TO EXPLAIN}

\begin{table}[!t]
    \caption{Anomalies}
    \centering
    \begin{tabular}{| c | c |}
    \hline
    Anomaly & Measurement\\ [0.5ex] % inserts table %heading
    \hline
    \multicolumn{2}{|c|}{Natural Anomalies} \\
    \hline
    Temperature too Low & $<$ 54° F \\
    Temperature Difference too Large & $|T1-T2|$ $>$ 25° F \\
    Air Measurement too High & $>$ 20 \\
    \hline
    \multicolumn{2}{|c|}{Potential Anomalies} \\
    \hline
    Moisture too Low & $<$ 1300 \\
    Moisture too High & $>$ 1900 \\
    Light too High & $>$ 640 \\
    Light too Low & $<$ 0 \\
    Air too High & $>$ 40 \\
    Air too Low & $<$ 0 \\
    Temperature too High & $>$ 150° F \\
    Humidity too High & $>$ 90 \\
    Humidity too Low & $<$ 0 \\
    \hline
    \end{tabular}
    \label{table:anoms}
\end{table}

\section{Anomaly Detection Model}
\label{sec:ml-model}
\begin{figure}[!t]
    \centering
    \includegraphics[width=9cm]{figures/ae.png}
    \caption{Visual Representation of Autoencoder Model \cite{aediag}}
    \label{fig:modeldiag}
\end{figure}
%\hl{Why Autoencoders?}\\
In this work, our data collection resulted in a massive amount of normal data samples and very few anomalies, therefore it was suitable to train a model with only normal data points and test it with a collection of normal points and the few anomalies that occurred. As such, for anomaly detection, we used Autoencoders \cite{adwithauto} which is an unsupervised neural network technique that learns how to compress and encode data and how to reconstruct the data back from its reduced representation to its original shape. It works by accepting a given input, encoding it into a smaller size using a bottleneck layer, and then decoding it into its original size. Figure \ref{fig:modeldiag} gives a visual representation of an Autoencocer model. It is trained so that the model is able to ignore data that is not crucial to reconstructing the original data as accurately as possible. In addition, the compression feature of Autoencoders helps significantly in dimensionality reduction which makes it capable of processing large number of features, more so than other unsupervised learning methods. %Autoencoders are able to take large amounts of input and sift through the "noise" commonly found within data sets that have a large number of features.
%This attribute is why Autoencoders work well for removing noise from images and it is also why this is a great model for our use case.
\begin{figure*}[!t]
    \centering
    \includegraphics[width=.75\linewidth]{figures/all_plots-v2.png}
    \caption{Reconstructed Testing Data Compared to Normal Testing Data}
    \label{fig:recon}
\end{figure*}
In order for the Autoencoder to function properly, it must be trained on data that is scrubbed from any anomalous points exceeding a defined threshold. This ensures that the model is able to reconstruct the normal data with as little reconstruction loss \cite{guo2018multidimensional} as possible, while maximizing the reconstruction loss of the anomalies contained within the testing dataset. Labeling data points as anomalous or benign was achieved by checking each of the data points' attributes for conditions we considered abnormal. The thresholds for this loop can be found in Table \ref{table:anoms}.
%After training, the model should be able to reconstruct testing data with little error. %The benefit of using an Autoencoder is that it is able to be trained without labeled data, which is known as unsupervised learning since labels are not involved in the training process.\hl{This was a main aspect for why we selected this particular model to implement. Our data collection resulted in a massive amount of normal data samples and very few anomalies, therefore it was optimal to train a model with only normal data points and test it with a collection of normal points and the few anomalies that occurred. Autoencoders are also extremely capable of processing large amounts of features, more so than other unsupervised learning methods. Autoencoders are able to take large amounts of input and sift through the "noise" commonly found within data sets that have a large number of features. This attribute is why Autoencoders work well for removing noise from images and it is also why this is a great model for our use case.}
The testing data was visually chosen based on a time period that appeared to contain a bulk of anomalous data points, which spanned from April 10\textsuperscript{th} to April 13\textsuperscript{th}.



%Once the test section was removed, the "normal" data was reduced to only numeric values by eliminating the date attribute from each data point.
The values of data points collected from different sensors are of different scales. For instance, the moisture sensor gave measurements ranging from ~1100 to ~2000, whereas the the air quality sensor yielded measurements ranging from ~5 to ~30 (see Figure \ref{fig:data}). As such, the data was normalized using Min-Max normalization \cite{patro2015normalization} to create consistency among the data. Performing normalization on input data has been proven to drastically improve the accuracy of machine learning models. Same parameters used for normalizing the training data were also used to normalize the testing data.


%At this point, the testing section, which included anomalous behavior as well as normal behavior, went through labeling. This was achieved by sending each data point through a loop that checked each of the data points' attributes for conditions we considered abnormal. The thresholds for this loop can be found in Table \ref{table:anoms}.

Our model consists of a single input layer, three encoding layers, a single layer for the latent space (aka bottle neck), three layers for decoding, and a single output layer. A diagram showing these basic features is shown in Figure \ref{fig:modeldiag} \cite{aediag}. The input for our model doesn't represent images, but the principle is the same. The model takes an input, encodes it using three layers, then decodes it, and produces an output image. The hope is that by inputting normal data, the model is able to learn what normal behavior is, and recreate it. After the model is trained, we must determine a classification threshold. The hyper parameters such as learning rate, batch size, number of nodes and epochs were determined by conducting a grid search of various values of these parameters until the combination that produced the most optimal results was found The model was trained for 60 epochs, with a batch size of 8, a learning rate of .000001, and a node size of 256.

For model training, the normal data is split into \textit{train} and \textit{validation} data, 75\%/25\% respectively. As mentioned earlier, we have a separate data set specifically for testing. The model was trained using the training data, and the validation data was used to determine the classification threshold which is discussed later on in this section. Our model was trained in 262 seconds.

% \begin{figure*}[!t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/all_plots.png}
%     \caption{Reconstructed Data Compared to Normal Data}
%     \label{fig:recon}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=.995\linewidth]{figures/white_plot-v2.png}
    \caption{Reconstruction Loss on Test Data}
    \label{fig:thresh}
\end{figure*}
Unlike normal machine learning models that generate predictions, passing data through the trained model does not generate an actual prediction, and instead, it simply returns the loss between the original data point, and the reconstructed data point generated by the Autoencoder model. In order to obtain an actual prediction, we take the loss returned from the model and compare it to a predetermined threshold in order to determine if the data point is anomalous or not. We used our validation data, which contains no anomalous data points, to determine the threshold by taking the average of the 5 highest loss values that were generated. The average was taken in order to prevent one abnormal loss value from skewing threshold while also ensuring that the threshold is larger than the loss generated by a normal data point. The testing dataset is used to evaluate the performance of our model, where it is used as an input to obtain its reconstruction loss. The loss values are then checked against the predetermined threshold to be classified as normal or anomalous.


%Our model was able to achieve a detection time of .0585 seconds. Figure \ref{fig:thresh} shows the loss generated by the Autoencoder for each of the data points within the testing data and how they each compare to the predetermined threshold. Figure \ref{fig:thresh} is discussed in more depth in the next section.
