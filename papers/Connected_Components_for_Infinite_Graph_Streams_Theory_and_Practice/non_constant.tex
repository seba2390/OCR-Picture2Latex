As we have shown, connectivity queries propagate through the \XStream ring
processings in $p$ \XStream ticks, and the query answer is sent from the
tail processor to the head, then back to the I/O processor. Another potentially
useful query that finishes in $p$ \XStream ticks is ``How many edges are in the system?''
\Cindy{Add some more here.  It can just be a list. Many that one might think should be constant probably require some data structure support, in whcih case, add them at the end.} \Cindy{Revisit notation.}

\XStream also supports queries with non-constant-sized output. At most
one such query can be active at a time. The answer to the query is
output in constant-sized pieces using the payload slots. The canonical
non-constant query is a request to output all vertices in small
connected components. Specially, the answer is the names of all
components with at most $\lambda$ vertices and the list of vertices
within them.  This query makes practical sense only in graphs that
have a giant connected component, but most real graphs have one. We
describe how \XStream executes this specific query.

For a local component with name $\eta$, let $s_{\eta}$ be the number
of vertices in $\eta$.  A processor can compute the size of a local
component as the sum of the number of vertices in each building block.
This is $1$ for a primitive building block.  For this discussion, we
assume processors keep track of the number of primitive building
blocks for each local component while building these components. This
adds only constant work per union-find operation.  However, it's also
possible to inialize local-component sizes to zero and compute them
on-the-fly for this query.  But then, the processor does at most k-1
work counting primitive building blocks or outputing the messages
below, which will further delay the query response. Processors receive
the size of non-primitive building blocks from upstream processors.

When the head processor receives the query ``Output the vertices in
components that have at most $\lambda$ vertices'' in the primary slot
of a bundle, it passes the query downstream in the primary slot. This
allows all processors to learn the type of query and the parameter
$\lambda$. The head then uses the $k-1$ payload slots to start
answering the query.  The query is answered in two phases.  In the
first phase, processors compute component sizes.  For each local
component with name $\eta$, such that $s_{\eta} \le \lambda$, the head
processor (eventually) sends a message ``($\eta, s_{\eta})$'' in a
payload basket. The head outputs $k-1$ of these messages per bundle if
it already knows its component sizes. After the last message, it
outputs a ``query phase done'' token.

Each downstream processor passes the initial query downstream.  Then
for each message $(\eta, s_{\eta})$, the processor checks to see if
$\eta$ is a building block for one of its local components $\eta'$.
If it is, then it increments the size of $\eta'$.  If $\eta$ is not a
local building block, the processor sends the message downstream.
When the processor receives the ``query phase done'' token, it knows
the size of all its non-primitive building blocks, and hence knows
the size of all of its local components.  It sends its own ``($\eta,
s_{\eta})$'' messages for each local component $\eta$ such that
$s_{\eta} \le \lambda$. When it has sent all its messages, it passes
the ``query phase done'' token downstream. If the current graph has a
connected component $\eta_G$ that has size at most $\lambda$, the
message with its final size is passed through the tail and out to the
analyst.  The tail also passes the ``query phase done'' token to the
head.

Sealed processors (full of tree edges) can set a flag indicating they
have computed their component sizes. If there is another such query
before an aging, then it removes messages associated with its local
building blocks without incrementing any size counters.

In the second phase, the head processor (eventually) sends a message
$(\eta, v_i)$, for each primitive vertex $v_i$ in each local component
$\eta$ reported in the first phase.  For the head, all building blocks
are primitive vertices. It's possible to put more than one vertex in
the latter kind of message (e.g. $(\eta, v_1, v_2, v_3)$, depending
upon the size of a slot.  After the last such message, the head passes
a ``query done'' token downstream.

When a downstream processor receives a message $(\eta, v_i)$ from
upstream in the second phase, it checks to see if $\eta$ is a building
block for one of its local components $\eta'$. If not, then it passes
the message downstream.  If so, then $s_{\eta'} \le \lambda$ (i.e. the processor
reported local component $\eta'$ in the first phase, it relabels the message,
sending $(\eta', v_i)$ downstream.  If $\eta'$ is too large, it just removes
the message from the system.

When a downstream processor receives the ``query done'' token, it outputs
messages $(\eta, v_i)$, where $\eta$ is a local componet with $s_{\eta} \le \lambda$
and $v_i$ is a primitive building block (vertex) in local component $\eta$.

A somewhat easier non-constant query is spanning tree. Starting with the head, each
processor outputs its tree edges.

Some queries can be either constant-size (latency $p$) or non-constant
depending upon what additional data structures the processors
maintain.  One example is ``What is the degree of node $v$?'' Suppose
each processor maintains adjacency lists for the subgraph it
holds. Then the processor can find the number of edges adjacent to a
vertex $v$ in constant time, given a hash table to access the
adjacency list for each vertex.  In this case, the vertex-degree query
has latency $p$.  The query makes one pass around the ring with the answer progressing
one processor per tick.  Otherwise, without this data structure, each processor will need time to
compute the number of edges it holds that are adjacent to vertex $v$.
In this case, it is a non-constant query.  The message still touches
each processor once, but the processor may require multiple ticks to
compute the number to add to the accumulating degree value.

Linear algebraic computations typically involve a matrix-vector product,
which would be unweildy to compute directly in the \XStream model.
However, the emerging field of randomized linear
algebra~\cite{drineas2018lectures}
offers a path forward. If we devote some space in the tail processor to
accommodate a sample of edges (adjusting Lemma~\ref{lemma:aging-lead-time}
accordingly), payload slots can be used to accumulate a random sample
of the graph.  Techniques such as randomized 
PageRank~\cite{gasnikov2015efficient} or others might then be applied in a 
separate
thread in the tail processor, still with minimal interruption to the input 
stream.
