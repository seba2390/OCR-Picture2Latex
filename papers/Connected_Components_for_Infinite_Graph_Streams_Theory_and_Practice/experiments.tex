
\begin{table*}[htb]
\begin{center}
\begin{tabular}{|c|c|c|c|c|} \hline
  & Bundle Size & 64-bit ints/s & \XStream potential ($k=2$) & \XStream potential ($k=5$) \\ \hline
Benchmark 1 & 5 & 1742160.27 & 174216.02 & 69686.41 \\ \hline
Benchmark 1 & 25 & 8680555.55 & 868055.55 & 347222.22 \\ \hline
Benchmark 1 & 250 & 54112554.11 & 5411255.41 & 2164502.16 \\ \hline
Benchmark 2 & 5  & 1344086.02 & 134408.60 & 53763.44 \\ \hline
Benchmark 2 & 25 & 6281407.03 & 628140.70 & 251256.28 \\ \hline
Benchmark 2 & 250 & 35063113.60 & 3506311.36 & {\bf 1402524.54} \\ \hline
\end{tabular}
\end{center}
\caption{\label{tab:benchmark} TBB benchmarks designed to produce bounds on \XStream performance
on Intel Sky Lake. Benchmark 1 propagates bundles downstream without any 
computation.  Benchmark 2 hashes two of every five integers in the bundle 
to simulate
\XSCCns's \Call{ProcessEdge}{} computation. The rightmost two columns show
upper bounds on \XSCC performance for bandwidth expansion factors $k=2$ and
$k=5$. On this architecture, we must send bundles of size 250 to maximize
performance. \XSCC with $k=5$ is bounded by 1.4 million edges per second.}
\end{table*}

The \XStream model and the \XSCC algorithm are based on message passing.
At each \XStream tick, each processor performs only a constant number of
operations.  These are predominantly hashing operations, union-find operations,
and simple array access.  Therefore, performance of \XSCC is strongly tied
to computer architecture. The faster a system can perform hashing and message
passing, the faster \XSCC will run.

With a current
Intel computer architecture (Sky Lake), we will show that our initial XS-CC
implementation can almost match the peak performance of a simple
Intel/Thread Building Blocks (TBB) benchmark that transfers
data between $P$ cores of the processor.
This translates to streaming rates of between half a million and one million 
edges per second,
which is comparable to the low end of performance spectrum for modern
dynamic graph solutions (none of which handle infinite streams).  The high
end of that spectrum is
not comparable to our context since we require no supercomputer and ingest data
from only one processor.  We have ideas to exploit properties of many
graphs (such as the phenomenon of a giant connected component)
for running many instances of \XSCC concurrently to boost our rates by
orders of magnitude.  However, that is beyond the scope of this paper.

\subsection{Computing setup and benchmarking}

All results in this paper were obtained using a computing cluster with
Intel Sky Lake Platinum 8160 processors, each with 2 sockets,
24 cores/socket, 2 HW threads/core (96 total), and 192GB DDR memory. The
memory bandwidth is 128GB/s, distributed over 6 DRAM channels.  The
interconnect is Intel OmniPath Gen-1 (100Gb/s).
The operating system is CentOS 7.9, and our codes are compiled with Intel icpc 
20.2.254 using the flags \verb+-O3 -xCORE-AVX512+.

Our full implementation of \XSCC is single-threaded~\footnote{the normal-mode
computations and data structures are single-threaded.  We use another thread
for cleanup and reallocation at an aging transition.} and
written in PHISH~\cite{phish}, a streaming
framework based on message passing.
However, before presenting XS-CC results,
we explore the expected peak performance of the algorihm on a single node of
the Sky Lake cluster using the vendor's own software library (Thread Building
Blocks 2019 8).

\paragraph{Mini benchmark}

We implemented a simple ring of X-Stream-style processing modules in TBB.
The head module
accepts bundles of synthetic data from an I/O module and sends them down the
ring toward the tail, which feeds back to the head. The latter merges this
bundle with its next input bundle.  We further distinguish two benchmarks:
\begin{itemize}
\item Benchmark 1: Each processor either simply copies input bundles to output.
\item Benchmark 2: Each processor hashes two of every five integer of the input bundle and copies the input to the output.  This approximately reflects the
main computation kernels of \XSCCns: hashing the timestamp of each each,
and doing a union-find operation.
\end{itemize}


Table~\ref{tab:benchmark} shows the performance of our TBB benchmarks as the
number of 64-bit integers in a bundle is varied. For these runs there are
10 processors in the ring.  Recall that \XSCC edges circulate as 5-tuples of
64-bit integers: $(v_1, l_1, v_2, l_2, t)$ where $v_i$ are vertex ids,
$l_i$ are local component labels, and $t$ is a timestamp.  Therefore the
raw rates of the third column must be divided by 5 to count in units of
\XStream primary edges.  Furthermore, to account for the payload slots in
\XSCC bundles active during aging or non-constant query processing,
the primary edge rate must be
divided by the bandwidth
expansion factor $k$. With optimal use of Intel's TBB, we see that we
should pass messages containing roughly 250 64-bit integers, and we
expect \XSCC edge streaming rates to be bounded by 1.4 million edges per
second.

Since Benchmark 2 is equivalent to Benchmark 1 except for a larger
compute load, we see clearly that Benchmark 2 is not bandwith bound 
on this 
architecture.  We believe that these benchmarks are bound by a combination
of compute and memory latency.  We experience a slow-down from 2.1 million 
edges per
second to 1.4 million simply by adding two hashing operations per bundle.
As our experiments with \XSCC will show, the latter is likely even more
compute bound.  This is welcome since it admits the possibility that
multithreading within TBB nodes and \XStream processors could accelerate our
single-threaded edge-processing results.

Furthermore, in a real deployment of \XSCCns, we would assign a single
\XStream PE to a single compute node and communicate over the interconnect.
In fact, that is the basis of the \XSCC results presented in 
Figures~\ref{fig:experiment1}, \ref{fig:experiment2}, and \ref{fig:experiment3}.
In this case, we can compute the approximate theoretical peak for an
X-Stream-like computation as follows.  The interconnect is 100Gb/s, or
12.5GB/s. That translates to 1.5 billion 64-bit integers per second.  Since
\XSCC uses messages with 5 64-bit ints to represent an edge, and a typical
value of the \XStream bandwidth expansion factor $k$ is 5, we are bounded
by $1.5e9 / 5 / 5 \approx 62.5$ million \XSCC primary edges per second.
The rates of our prototype implementation do not approach this number, so
we believe that like the benchmarks, we are bound by a combination of
compute and memory latency.  A multi-threaded production version of
\XSCC would likely be necessary to better exploit a computing environment such
as our Sky Lake cluster.  With
that said: the TBB benchmark itself falls far short of the possible 
performance suggested by Sky Lake's theoretical peak memory bandwidth of 
128GB/s.
Significant algorithm engineering may be necessary to obtain a perfomant,
production version of \XSCCns.

\paragraph{Datasets}

We present prototype \XSCC results on three datasets:
\begin{enumerate}
    \item An anonymized stream of 10 million real gateway network traffic edges 
    from Sandia (the same stream used in ~\cite{AMP:berry2013maintaining}).
    \item A stream of edges from an R-MAT graph with 2097152 vertices,
    edge factor 8, and SSCA-2 parameters (0.45, 0.15, 0.15, 0.25)~\cite{rmat}.
    \item The Reddit reply network~\cite{kumar2018community} from SNAP~\cite{snapnets}, with 646,024,723 edges.
    \item A synthetic dataset with 100 continguous observations of
    each of a stream of edges with new, unique endpoints.
\end{enumerate}

For experiments below validating Theorem~\ref{thm:infinite-runs}, 
we note that Dataset 3 has a uniqueness parameter ($u$) of roughly 0.67.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=3in]{allratesnoage.png}
\end{center}
\caption{Experiment 1: Prototype \XSCC normal-mode
streaming rates on the four datasets\label{fig:experiment1} on Sky Lake.
Table~\ref{tab:benchmark} places the expected peak performance of \XStream
computations on this architecture at between 1.4 million and 5 million edges
per second.  This indicates that our single-threaded \XSCC implementation is
not bandwidth bound and a future version could benefit from multithreading.
The decrease in performance from $\approx 2$M edges/sec to $\approx 1.25$M
edges/sec occurs when the second \XStream processor becomes the builder.}
\Alex{1=10m,2=reddit with reservoirs, 3=rmat, 4=synthetic (barbells) 5= reddit without reservoirs}
\end{figure}

\subsection{\XSCC implementation}
We used PHISH with the MPI back end to implement the \XSCC algorithm.  Stream
processing modules in PHISH are called ``minnows,'' and we instantiated a 
minnow to serve as the \XStream I/O processor and a group of minnows to form
the \XStream ring of procssors (one per compute node in the Sky Lake cluster).
We also ran with a single compute node hosting all \XSCC PE's.  However,
since our prototype is compute bound the rates we achieved were comparable
and are not presented.

\begin{figure}[htb]
\begin{center}
%\includegraphics[width=3.5in]{cduk_1.png}
\includegraphics[width=3.5in]{cduk_2.png}
\end{center}
\caption{Experiment 2: Empirical validation of Theorem~\ref{thm:infinite-runs}
using Dataset 3.  \label{fig:experiment2}  The value $c$ is the fraction of
edges that survive the aging predicate, the value $d$ is the fraction of 
time that queries are enabled, and the value $k$ is the bandwidth expansion
factor (the number of slots in a bundle).}
\end{figure}

We now present results from Sky Lake runs of our PHISH-based, single-threaded
implemenation of \XSCCns.  Before collecting these results, we
validated the correctness of \XSCC by designating every tenth stream
edge to be a connectivity query, statically computing the correct connected
components, and confirming that \XSCCns's query result matched that from
the static computation (with and without aging events).  We ran this
validation on a prefix of approximately 800,000 edges from
Dataset 3.

\subsection{Experiment 1: \XSCC normal-mode streaming rate}

Figure~\ref{fig:experiment1} shows \XSCC streaming rates for normal mode in
the three datasets.  We streamed the full Datasets 1 and 2 and a prefix of
30 million edges of Dataset 3.  Our single-threaded prototype implementation
is compute bound, as verified by computing to the benchmark results of
Table~\ref{tab:benchmark}.  Note that the performance of our prototype is
heavily data-dependent.  On the ``easy'' synthetic dataset (Dataset 5 in
the figure), note that we match rates with Table~\ref{tab:benchmark}.  When
real datasets cause more work, the ingestion rate drops, again showing that
we are compute bound.  Our prototype achieves rates between 500,000 and
1,000,000 edges per second, depending on the dataset. We note that Dataset 1,
which is a real dataset, has many repeat edges and admits an ingestion
rate of one million edges per second.

\begin{figure}[htb]
\begin{center}
\includegraphics[width=3.5in]{300mcounts_k3.png}
\end{center}
\caption{Experiment 3: Automated aging with reservoir sampling on a
prefix of 300 million edges of Dataset 3.  The aging strategy
honors Theorem~\ref{lemma:aging-lead-time}.  This demonstrates
our strategy to run on infinite graph stream through arbitrary numbers
of aging events. \label{fig:experiment3}}

\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=3.5in]{300mrates_k3.png}
\end{center}
\caption{Experiment 3: The stream processing rate of our
prototype over time during
automated aging.\label{fig:experiment3-rates}.  
Although ingestion rate of new edges does slow during aging due
to factors not modeled such as the proportion of tree edges,
we note that aging during periods of lesser stream intensity 
(such as nighttime for a worksite) should allow infinite 
streaming without dropping edges.
    \Alex{it appears to be because we have more tree edges over time, working on verifying that is correct (treetestrates\_10kto10m.png shows how rate slows down as tree edges are a bigger and bigger fraction, from 10k out of 10m in red to 10m out of 10m in gray)}
}
\end{figure}

\subsection{Experiment 2: \XSCC with a single aging event}
Recall that Theorem~\ref{thm:infinite-runs} relates the \XStream bandwidth
expansion parameter $k$ to parameters of the system and dataset.  We
validate that theorem empirically for a 30 million-edge prefix of Dataset 3
by analyzing a single aging event that is triggered at \XStream tick $10^7$.  In Figure~\ref{fig:experiment2}, the capacity of one
\XStream processor ($s$) is fixed for each value of of $c$ such that the system will be completely full at the end of the 30 million-edge stream, after processing one aging event\footnote{In particular, if we age at \XStream tick $10^7$ such that fraction $f$ of the edges stored so far survives, then our total storage needed is $S= 20^7\cdot u+10^7\cdot f\cdot u$ and $c = (f\cdot u\cdot 10^7)/S$ 
}.
The number of \XStream processors ($p$) is fixed at $10$. 
We vary over a range of values of $c$ (the target fraction of edges that 
survive the aging event) and $d$ (the
fraction of \XStream ticks in which queries are enabled), we show using
a 3-D surface the predicted $k$ by \ref{thm:infinite-runs}.  We overlay
empirical results in the form of observed data points $(c,d,k)$ 
from \XSCC runs when the bandwidth expansion factor $k$ is set as predicted
by the theorem.  We claim that the prediction surface and observed data 
points corroborate the theorm in this experiment.

\subsection{Experiment 3: \XSCC runs of arbitrary length}

The most important contribution of our work is our set of ideas regarding
infinite graph streams and running  \XSCC for indefinite periods of time
without filling up or failing.  We corroborate these ideas empirically
in this section for a simple use case: the aging predicate is a simple
timestamp comparison: delete all edges older than \XStream tick $t_a$.
This strategy could be adapted to accommodate other aging predicates.

The primary challenge facing an \XStream system administrator is when to
initiate an aging event and what threshold $t_a$ to use. In this section
we present an automated solution.  The system administrator initializes
the system with a target $c$ value (the fraction of edges that should
survive an aging event).  Then we run \XSCC with the following
aging-invocation protocol.  This could be specified in detailed pseudocode,
but in the interest of space we describe it informally below.

When the tail processor begins to fill, we begin an automated binary
search to find a timestamp $t'$ that will hit our fractional target $c$
of edges that survive the aging event.  We augment the
data structures of each \XStream ring processor to include a small
reservoir of 100 edges, and ensure that this is a representative
sample by using the classical technique
of \emph{reservoir sampling}~\cite{vitter1985random}.  

The binary search proceeds by varying $t'$ between the oldest timestamp
and newest timestamps in the system.  At 
each candidate value of $t'$, each ring processor estimates
the number of edges that will survive an aging event with threshold $t'$.
In one circuit of the ring in a payload bundle, the tail processor will
know whether $t'$ needs to be increased or decreased in order to hit
the target $c$ value.  In a logarithmic number of passes, the tail
processor knows an accurate value of $t'$ and tells the head to initiate
aging with that threshold.  In practice, this should give plenty of
time to complete aging honoring Theorem~\ref{lemma:aging-lead-time}.

Figure~\ref{fig:experiment3} depicts the result of running \XSCC on 
a 300 million-edge prefix of Dataset 3 using this automated aging
strategy with a target $c$ value of 0.5.  We see that the binary 
search succeeds in finding aging thresholds that reliably 
reestablish a storage level of $cS$ over an arbitrary number of
aging events (we depict the first 27).


%     * goal 1: evaluate performance on big data with one aging
%     * goal 2: reproduce CDUK lemma  (one aging)
%  * Firehose active generator for infinite streams
%     * goal: ensure that we really can run "inifinitely"
%     * explanation: 
%        * active generator
%        * how we get edges, enforcing a U value
%        * remind readers that we'll run in auto-aging-mode
%
%\subsection{XStream results}
%  * CORRECTNESS  (testing query answers vs. static solution)
%  * PERFORMANCE IN NORMAL MODE 
%      * use BigMine datasets (Steve, R-MAT); report:
%         * raw rates
%         * % of "peak" mini-xstreadm/TBB performance
%         * % of "peak" mini-xstream/Phish performance
%         * Predicted GraphCore performance based on mini-xstream factor
%  * A SINGLE AGING
%      * Reddit prefix experiment (explain motivation and mechanism)
%         * CDUK LEMMA VERIFICATION
%            * Assess U offline
%            * Plot of predicted surface with certain empirical points
%         * Performance: Reddit prefix experiment (could pick one or 
%                        could generate a surface)
%  * An ``infinite'' run 
%       * Auto-selection of aging via binary search and reservoir sampling
%       * Ran for 20 agings; correct at end; aggregate edge
%         rate.  
%       * Storage use plot

