\section{Introduction}

Online learning to rank (OL2R) empowers modern retrieval systems to optimize their performance directly from users' implicit feedback \cite{wang2019variance,wang2018efficient,yue2009interactively,hofmann2013balancing,schuth2016multileave,zoghi2017online,lattimore2018toprank,oosterhuis2018differentiable,li2018online}. The essence of OL2R solutions is to infer the quality of individual documents under a given query \cite{radlinski2008learning,kveton2015cascading,zoghi2017online,lattimore2018toprank,li2018online}
or a parameterized ranking function \cite{wang2019variance,wang2018efficient,yue2009interactively,oosterhuis2018differentiable}
via sequential interactions with users, i.e., trial and error. It eliminates classical offline learning to rank solutions' strong dependency on explicit relevance annotations and makes supervised learning of ranking models possible when collecting explicit annotations from experts is economically infeasible or even impossible (e.g., private collection search). 

Although influential and theoretically sound, the current OL2R solutions are not compatible with the successful practices in offline learning to rank, which directly optimize rankers by minimizing loss defined by rank-based metrics, such as Average Relevance Position (ARP) \cite{joachims2017ips} or Normalized Discounted Cumulative Gain (NDCG) \cite{burges2010ranknet}. As a result, the performance of existing OL2R solutions is still behind that of offline solutions, which directly restricts OL2R's real-world applicability.

The key barrier separating the practices in online and offline learning to rank is the need of exploration. Since users' feedback is implicit and known to be biased and noisy \cite{joachims2005accurately,agichtein2006improving,joachims2007evaluating}, more clicks on a top-ranked document do not necessarily indicate greater relevance. Effective exploration in the problem space is thus vital for the online model update. 
Current OL2R solutions explore either in the action space (e.g., presenting currently underestimated results at higher positions of a ranked list) \cite{radlinski2008learning,kveton2015cascading,zoghi2017online,lattimore2018toprank}, or in the model space (e.g., presenting ranked results from different rankers)  \cite{yue2009interactively,schuth2014multileaved}. 
However, due to the combinatorial nature of ranking, the action space is too large to be efficiently explored (e.g., all permutations of returned documents). This forces such OL2R solutions to take a \emph{pointwise} approach to estimate the utility of each query-document pair separately, which has proven to be inferior to the pairwise or listwise approaches in offline learning to rank studies \cite{chapelle2011yahoo}. While for model space exploration,
though an interleaved test makes it possible to compare different rankers with respect to a hidden utility function in an unbiased fashion, it is hard to link this comparison to the optimization of any rank-based metrics. Moreover, due to the required uniform sampling in the model space, this type of OL2R solutions suffers from high variance and high regret during online result serving and model update \cite{wang2019variance}. 

In this work, we aim to bridge the gap by directly training a \emph{pairwise} learning to rank model online. We target pairwise ranking models for three major reasons. First, a pairwise ranker reduces the exponentially sized action space to quadratic, by deriving the full ranking order from the pairwise comparisons between documents. Second, existing studies in search log analysis demonstrate relative preferences derived from clicks are more accurate and reliable than absolute judgments \cite{joachims2005accurately,joachims2007evaluating}. Third, pairwise learning to rank models have competitive empirical performance and have been widely used in practical systems \cite{chapelle2011yahoo,joachims2002optimizing,burges2010ranknet}. 
To gain theoretical insight into the proposed solution, we work with a single layer RankNet model with a sigmoid activation function \cite{burges2010ranknet}, which makes analytical convergence analysis possible. In particular, we explore in the pairwise ranking space of all candidate documents via \emph{divide-and-conquer}: 


we partition documents under a given query in each round of result serving, where the document ranking across different parts of the partition is certain (e.g., all documents in one part should be ranked higher than those in another part), but the ranking among documents within each part is still uncertain.
The ranked list is thus generated by a topological sort across parts of a partition and randomly shuffling within each part. We name our solution \model{}.
We rigorously prove that the exploration space shrinks exponentially fast in \model{} as the ranker estimation converges, such that the cumulative regret defined on the number of mis-ordered pairs has a sublinear upper bound. As most existing ranking metrics can be reduced to different kinds of pairwise comparisons among candidate documents \cite{Wang2018Lambdaloss}, e.g., Average Relevance Position is counted over a relevant document against all other documents, \model{} can directly optimize those ranking metrics based on users' implicit feedback on the fly. Our extensive empirical evaluations also demonstrate the strong advantage of \model{} against a rich set of state-of-the-art OL2R solutions over a collection of OL2R benchmark datasets on standard retrieval metrics.  
