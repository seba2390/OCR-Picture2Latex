In this section, we present our solution, which trains a pairwise learning to rank model online. The key idea is to partition the pairwise document ranking space and only explore the pairs where the ranker is currently uncertain, i.e., divide-and-conquer. We name our solution \model{}. We rigorously prove the regret of \model{} defined on the cumulative number of mis-ordered pairs over the course of online model update. 

\subsection{Problem Formulation}
\label{sec:problem}


% \paragraph{Notation} Let $[L] = \{1, 2, ..., L\}$ denote the first $L$ natural numbers. The set of all permutations of the $L$ items are represented as $\Pi([L])$. For any matrix $M \in R^{d \times d}$, we denote respectively by  $\lambda_{max}(M)$ and $\lambda_{min}(M)$ the maximum and minimum eigenvalue of matrix $M$. For vector $x \in R^d$ and positive definite matrix $V \in R^{d\times d}$, $\Vert x\Vert_V = \sqrt{x^\top Vx}$ denotes the weighted $l_2$-norm associated with matrix $V$. $\dot{\sigma}$ denotes the first derivative of $\sigma$.

In OL2R, a ranker interacts with users for $T$ rounds. At each round $t=1,2,...,T$, the ranker receives a query $q_t$ and its associated candidate documents, 
%$D_t = \{d_1^t, d_2^t, ... d_{L_t}^t\}$ with length $L_t$, 
which are represented as a set of $d$-dimensional query-document feature vectors $\mathcal{X}_t = \{\bx_1^t, \bx_2^t, \dots \bx_{L_t}^t\}$ with $\bx^t_i \in \bR^d$ and $\Vert\bx_i^t\Vert \leq u$. The ranker determines the ranking of the candidate documents $\tau_t = \big(\tau_t(1), \tau_t(2), \dots, \tau_t(L_t)\big) \in \Pi([L_t])$, based on its knowledge so far, where $\Pi([L_t])$ represents the set of all permutations of $L_t$ documents and $\tau_t(i)$ is the rank position of document $i$ under query $q_t$. Once the ranked list is returned to the user, the user examines the results and provides his/her click feedback $C_t = \{c_1^t, c_2^t, ..., c_{L_t}^t\}$, where $c_i^t = 1$ if the user clicks on document $i$ at round $t$; otherwise $c_i^t = 0$. Based on this feedback, the ranker updates itself and proceeds to the next query.

$C_t$ is known to be biased and noisy \cite{joachims2005accurately,agichtein2006improving,joachims2007evaluating}. Existing studies find that users tend to click more on higher-ranked documents, as they stop examination early in the list. This is known as position bias. And users can only interact with the documents shown to them, known as the presentation bias. As a result, the ranker cannot simply treat non-clicked documents as irrelevant. 
Such implicit feedback imposes several key challenges for online learning to rank: how to deal with the implicit feedback, and in the meanwhile, how to effectively explore the unknowns for the model update. 
Following the practice in \cite{joachims2005accurately}, we treat clicks as relative preference feedback and assume that clicked documents are preferred over the \emph{examined but unclicked} ones. In addition, we consider every document that precedes a clicked document and the first subsequent unclicked document as examined. This approach has been widely adopted and proven to be effective in learning to rank \cite{wang2019variance,agichtein2006improving,oosterhuis2018differentiable}. Accordingly, we use $o_t$ to represent the index of the last examined position in the ranked list $\tau_t$ at round $t$.

Exploration is the key component that differentiates OL2R to offline L2R, where OL2R needs to serve while learning from its presented rankings. 
% Therefore, random shuffle is less appreciated for OL2R as it will sacrifice the user online experience, even though it may be a benefit for the model training.
% According to the previous discussion, the feedback in OL2R is incomplete, and we need to explore the unknowns for the model update. 
The most straightforward exploration is to provide a random list of candidate documents. However, such random exploration is less appreciated for OL2R as it hurts user experience, even though it may be beneficial for model training.
Therefore, regret becomes an important metric for evaluating OL2R.
Though various types of regret have been defined and analyzed in existing OL2R studies, few of them link to any rank-based metric, which is the key in ranker evaluation. For example, for OL2R solutions that explore in the document space, regret is typically defined on the number of clicks received on the presented ranking versus that known only in hindsight \cite{li2016contextual,kveton2015cascading,zoghi2017online,lattimore2018toprank}. For solutions that explore in the model space, such as DBGD, regret is defined as the number of rounds where the chosen ranker is preferred over the optimal ranker \cite{yue2009interactively,wang2019variance}. It is difficult to reason how such measures indicate an OL2R solution's ranking performance against a desired retrieval metric, such as ARP and NDCG.
To bridge this gap, we define regret by the number of mis-ordered pairs from the presented ranking to the ideal one, i.e., the Kendall tau rank distance,
\begin{equation*}
    R_T = \mathbb{E}\big[\sum\nolimits_{t=1}^T r_t\big] = \mathbb{E} \big[\sum\nolimits_{t=1}^T K(\tau_t, \tau_t^*)\big]
\end{equation*}
where $K(\tau_t, \tau_t^*)=\Big|\big\{(i,j):i<j,\big(\tau_{t}(i)<\tau _{t}(j)\wedge \tau^*_{t}(i)>\tau^*_{t}(j)\big)\vee \big(\tau _{t}(i)>\tau _{t}(j)\wedge \tau^*_{t}(i)<\tau^*_{t}(j)\big)\big\}\Big|$. As shown in \cite{Wang2018Lambdaloss}, most ranking metrics employed in real-world retrieval systems, such as ARP and NDCG, can be decomposed into pairwise comparisons; hence, our defined regret directly connects an OL2R algorithm's online performance with classical ranking evaluations.
%This regret definition is totally different from that defined in the traditional multi-armed bandit, or the combinatorial bandit algorithms, where they define the regret as a pointwise loss between the proposed action and the optimal action. In ranking systems, the correct order between the documents is more important than approximating the absolute rating scores, and the high reward on the score of a document set does not guarantee the correctness of the ranking.


% The goal of the ranker is to adjust its ranking strategy to minimize the cumulative regret over time.


% Therefore, it is essential to balance the exploration-exploitation 

% Different from traditional multi-armed bandit, or the combinatorial bandit algorithms, where the action is a single arm or a set of arms respectively, in OL2R, the action at each step is a ranked list of the candidate documents, where the order matters. Therefore, we define the regret in OL2R as a pairwise regret in $T$ rounds as: 

% The ranker proposes a ranked list of the candidates, , based on its knowledge so far. Once the ranked list is presented to the user, corresponding click feedback $C_t = \{C_1^t, C_2^t, ..., C_{L_t}^t\}$ will be given to the ranker

% An oracle (i.e., user) then examines the results and provides feedback. The feedback is a list of binary random variables $C_t = \{C_1^t, C_2^t, ..., C_{L_t}^t\}$ where , upon which we could infer the pairwise preference between documents. 

% We assume that for any two documents $d_i^t, d_j^t \in D_t$, the probability that $d_i^t$ is preferred than $d_j^t$, denoted by $\mathbb{P}(d_i^t \succ d_j^t)$, is drawn from a Bernoulli distribution, $Ber(\sigma(f(x_i^t, x_j^t)))$, where $f(\cdot)$ is a utility function over the document pair, and $\sigma (\cdot)$ is the a sigmoid function, i.e., $\sigma(s) = \frac{1}{1 + e^{-s}}$ for any $s \in \mathbb{R}$.

% As mentioned before, OL2R can be viewed as a game between the ranker and the oracle, and modelled as a bandit algorithm. 



