
% \section{Notations}
% We provide a list of symbols used and defined in the paper to facilitate our discussion of the results. \\

% \begin{table}
%   \caption{Notations used in this paper.}
%   \label{tab:freq}
%   \begin{tabular}{cc}
%     \toprule
%     Notation & Description\\
%     \midrule
%     $\bx_i^t$, $\bx_j^t$ & \multicolumn{1}{p{6.8cm}}{feature vector of document $i$ and $j$ under query $q_t$ at round $t$.} \\
%     $\xijt$ & \multicolumn{1}{p{6.8cm}}{$\bx_i^t - \bx_j^t$, the difference vector between feature vectors of $\bx_i^t$ and $\bx_j^t$.} \\
%     $\mathcal{G}_{t}$ & \multicolumn{1}{p{6.8cm}}{$\mathcal{G}_{t} = \{(i, j): c_i^{t} \neq c_j^{t}, \forall i < j \leq o_{t}\}$, the set of document pairs that received different click feedback at round $t$.}\\
%     $y_{ij}^t$ & \multicolumn{1}{p{6.8cm}}{$\textit{sign}(c_i^t > c_j^t)$, whether the document $i$ is preferred over document $j$.} \\
%     $\epsilon_{ij}^t$ & \multicolumn{1}{p{6.8cm}}{feedback noise, follows $\frac{1}{2}$-sub-Gaussian.}\\
%     $\Theta$    & \multicolumn{1}{p{6.8cm}}{admissible set of model parameters. In this paper, $\Theta = \{\btheta, \Vert\btheta\Vert_2 \leq Q\}$.} \\
%     $\tilde{\btheta}_t$ & \multicolumn{1}{p{6.8cm}}{maximum quasi-likelihood estimator of the parameter in loss function.} \\
%     $\hat{\btheta}_t$ & \multicolumn{1}{p{6.8cm}}{model estimator projected from $\tilde{\btheta}_t$ onto $\Theta$. }\\
%     $\sigma(x)$ & \multicolumn{1}{p{6.8cm}}{logistic function $\frac{1}{1 + \exp(-x)}$ .}\\
%     $\dot{\sigma}(x)$ & \multicolumn{1}{p{6.8cm}}{the first order derivative of $\sigma(x)$.}\\
%     $\mathcal{E}_{c}^t$ & \multicolumn{1}{p{6.8cm}}{the set of  certain rank orders.}\\
%     $\mathcal{E}_{u}^t$& \multicolumn{1}{p{6.8cm}}{the set of uncertain rank orders, i.e., $\{(i, j) \in [L_t]^2: (i, j) \notin \mathcal{E}_c^t \wedge (j, i) \notin \mathcal{E}_c^t\}$.}\\
%     $\mathbf{M}_t$ & \multicolumn{1}{p{6.8cm}}{$\sum_{s =1}^{t-1}\sum_{(i, j) \in \mathcal{S}_{\tau}}\bx_{ij}^s{\bx_{ij}^s}^\top + \lambda \mathbf{I}$.}\\ 
%     $c_{\mu}$ & \multicolumn{1}{p{6.8cm}}{a constant, defined as $\inf_{\btheta \in \bTheta}\dot{\sigma}(\bx^\top\btheta)$.} \\
%     $\mathcal{P}_d^t$ & \multicolumn{1}{p{6.8cm}}{the $d$-th partition at round $t$.}\\
%     $N_d^t$ & \multicolumn{1}{p{6.8cm}}{number of uncertain rank orders for partition $\mathcal{P}_d^t$ at round $t$.}\\
%     $N_t$  & \multicolumn{1}{p{6.8cm}}{number of uncertain rank orders at round $t$.}\\
%     $o_{\max}$ & \multicolumn{1}{p{6.8cm}}{the maximal number of examined documents inferred from the clicks.}\\
%     $L_{\max}$ & \multicolumn{1}{p{6.8cm}}{the maximal number of candidate documents any query would have during $T$ rounds.} \\ 
%     $p^*$& \multicolumn{1}{p{6.8cm}}{the minimal probability that all documents in a query are examined.}\\
%     $\Delta_{\min}$& \multicolumn{1}{p{6.8cm}}{$\min_{t\in T}\min_{(i, j) \in [L_t]^2}| \sigma({\mathbf{x}_{ij}^t}^\top\btheta^*) - \frac{1}{2}|$,  the smallest gap between any pair of documents associated to the same query over time.}\\
%   \bottomrule
% \end{tabular}
% \end{table}

\section{Preliminaries}
In this section, we present some basic definitions and inequalities for later use.
The first inequality is about the Lipschitz continuity of the logistic function.
\begin{lemma} For the logistic function $\sigma(x) = {1}/{(1 + \exp(-x))}$, we have:
$|\sigma(x) - \sigma(y)| < k_{\mu} |x - y|$
where $k_{\mu} = 1/4$.
\label{lemma:lipt}
\end{lemma}

The second inequality is the self-normalized bound for martingales, adopted from Lemma 9 in \cite{abbasi2011improved}.

\begin{lemma}[Self-normalized bound for martingales, Lemma 9 in \cite{abbasi2011improved}]
\label{lemma:martigale}
Let $t$ be a stopping time with respect to the filtration $\{F_\tau\}^\infty_{\tau=0}$. Because $\epsilon_{ij}^s$ is $\frac{1}{2}$-sub-Gaussian, for $\delta > 0$, with probability $1-\delta$,
\small
\begin{equation*}
    \left\| \sum\nolimits_{s=1}^{t-1}\sum\nolimits_{(i, j)\in \mathcal{G}_s^{ind}} \epsilon_{ij}^s\bx^s_{ij}\right\|_{\sub \bM_t^{-1}}^2 \leq ({1}/{2})\log({{\det(\bM_t)^{1/2}}/{(\delta \det(\lambda\mathbf{I})^{1/2})}})
\end{equation*}
\normalsize
\end{lemma}

\section{Proof of \model{}'s confidence interval: Lemma~\ref{lemma:cb}}

% In this section, we present the detailed proof of Lemma 1. We first provide the analysis of model convergence based on the one layer RankNet model with sigmoid activation in Section~\ref{sec:modellearning} and then provide the proof of Lemma 1 in Section~\ref{sec:cb}.

\subsection{Model learning for PairRank}
\label{sec:modellearning}

As we introduced in Section 3.2, we adopt a single layer RankNet model with sigmoid activation function as our pairwise ranker. The objective function for our ranker defined in Eq~(2) is the cross-entropy loss between the predicted pairwise preference distribution and the inferred distribution from user feedback. Given this objective function is log-convex with respect to $\btheta$, its solution $\tilde{\btheta}_t$ is unique under the following estimating equation at each round $t$,
\small
\begin{align}
\label{eqn:gradient}
    \sum\nolimits_{s=1}^{t-1}\sum\nolimits_{\scriptscriptstyle (m, n)\in\mathcal{G}_{s}^{ind}} \left(\sigma({\xmns}^\top\btheta) - \ymns \right)\xmns+ \lambda\btheta = 0
\end{align}
\normalsize
Let $g_t(\btheta) = \sum_{\scriptscriptstyle s=1}^{\scriptscriptstyle t-1}\sum_{(m, n) \in \mathcal{G}_s^{ind}}\sigma({\xmns}^\top\btheta)\xmns + \lambda\btheta$ be the invertible function such that the estimated parameter $\tilde{\btheta}_t$ satisfies $g_t(\tilde{\btheta}_t) = \sum_{s=1}^{t-1}\sum_{(m, n)\in\mathcal{G}_{s}^{ind}}\ymns\xmns$. Since $\tilde{\btheta}_t$ might be outside of the admissible set of parameter $\Theta$ that $\btheta \in \Theta$ (e.g., $\Vert\btheta\Vert_2 \leq Q$), the estimation can be projected into $\Theta$ to obtain $\hat{\btheta}_t$:
\small
\begin{align}
\label{eq:opt-solution}
    \hat{\btheta}_t 
    &{=} \argmin_{\btheta \in \Theta} \left\| g_t(\btheta) {-} g_t(\tilde{\btheta}_t)\right\|_{\scriptscriptstyle \bM_t^{-1}} \\
    &{=} \argmin_{\btheta \in \Theta} \left\| g_t(\btheta) {-} \sum_{\scriptscriptstyle s=1}^{\scriptscriptstyle t-1}\sum_{\scriptscriptstyle (m, n)\in\mathcal{G}_{s}^{ind}}\ymns\xmns\right\|_{\scriptscriptstyle \bM_{t}^{-1}} \nonumber
\end{align}
\normalsize
where $\bM_t$ is defined as $\bM_t = \sum_{s =1}^{t-1}\sum_{(m, n) \in \mathcal{G}_{s}^{ind}}\xmns{\xmns}^\top + \lambda \mathbf{I}$. To summarize, $\tilde{\btheta}_t$ is the solution of Eq~(4), and $\hat{\btheta}_t$ is the estimated model which is generated by projecting $\tilde{\btheta}_t$ onto $\Theta$.

\subsection{Confidence interval analysis}
\label{sec:cb}
Now we present detailed proof of the confidence interval in Lemma~\ref{lemma:cb}, which is based on Proposition 1 in~\cite{filippi2010parametric}.

% \begin{proof}[Proof of Lemma 1]
At round $t$, for any document pair $\bx^t_i$ and $\bx^t_j$, the estimation error of the pairwise preference in \model{} is defined as $|\sigma({\xijt}^\top\btheta^*) - \sigma({\xijt}^\top\hat{\btheta}_t)|$, which is based on the model $\hat\btheta_t$ learned from the observations until last round. According to Lemma~\ref{lemma:lipt}, we have $|\sigma({\xijt}^\top\btheta^*) - \sigma({\xijt}^\top\hat{\btheta}_t)| \leq k_{\mu}|{\xijt}^\top \btheta^* - {\xijt}^\top\hat{\btheta}_t|$. As logistic function $\sigma(\cdot)$ is continuously differentiable, $\nabla g_t$ is continuous. Hence, according to the Fundamental Theorem of Calculus, we have $g_t(\btheta^*) - g_t(\hat{\btheta}_t) = \bG_t(\btheta^* - \hat{\btheta}_t)$, where $\bG_t = \int _0^1\nabla g_t \left(l\btheta^* + (1 - l)\hat{\btheta}_t\right) dl.$ Therefore, $\nabla g_t(\btheta) = \sum_{s=1}^{t-1}\sum_{(m, n)\in \mathcal{G}_s^{ind}} \dot\sigma({\xmns}^\top\btheta) {\xmns}{\xmns}^\top + \lambda\mathbf{I}$, where $\dot{\sigma}$ is the first order derivative of $\sigma$. As $c_{\mu} = \inf_{\btheta \in \bTheta} \dot{\sigma}(\bx^\top\btheta)$, it is easy to verify that $c_{\mu} \leq 1/4$. Thus, we can conclude that $\bG_t \succeq c_{\mu}\bM_t$. Accordingly, we have the following inequality,
\small
\begin{align*}
    &\left|\sigma({\xijt}^\top\btheta^*) - \sigma({\xijt}^\top\hat{\btheta}_t) \right| \\
    \leq& k_{\mu} \left| {\xijt}^\top \mathbf{G}_t^{-1} \left(g_t(\btheta^*) - g_t(\hat{\btheta}_t)\right) \right| \leq k_{\mu} \Vert\xijt\Vert_{\scriptscriptstyle \mathbf{G}_t^{-1}}\left\| g_t(\btheta^*) - g_t(\hat{\btheta}_t) \right\|_{\scriptscriptstyle \mathbf{G}_t^{-1}} \\
    % \leq& ({k_{\mu}}/{c_{\mu}}) \Vert\xijt\Vert_{\scriptscriptstyle\bM_t^{-1}}\left\| g_t(\btheta^*) - g_t(\hat{\btheta}_t) \right\|_{\scriptscriptstyle\bM_t^{-1}} 
    \leq& ({2k_{\mu}}/{c_{\mu}}) \Vert\xijt\Vert_{\scriptscriptstyle\bM_t^{-1}}\left\| g_t(\btheta^*) - g_t(\tilde{\btheta}_t) \right\|_{\scriptscriptstyle\bM_t^{-1}} 
\end{align*}
\normalsize
where the first and second inequalities stand as $\mathbf{G}_t$ and $\mathbf{G}_t^{-1}$ are positive definite. The third inequality stands as $\mathbf{G}_t \succeq c_{\mu}\bM_t$, which implies that $\mathbf{G}_t^{-1} \preceq c_{\mu}^{-1} \bM_t^{-1}$ and $\Vert\bx\Vert_{\sub \mathbf{G}_t^{-1}} \leq \Vert\bx\Vert_{\sub \bM_t^{-1}}/{\sqrt{c_{\mu}}}$ hold for any $\bx \in \mathbb{R}^d$. The last inequality stands as $\btheta^* \in \Theta$, and $\hat{\btheta}_t$ is the optimum solution for Eq \eqref{eq:opt-solution} at round $t$ within $\Theta$. 

Based on the definition of $\tilde \btheta_t$ and the assumption on the noisy feedback that $y^t = \sigma({\bx_t}^\top\btheta^*) + \epsilon^t$, where $\epsilon^t$ is the noise in user feedback defined in Section 3.2, we have 
\small
\begin{align*}
    &g_t(\tilde{\btheta}_t) - g_t(\btheta^*) \\
    =& \sum\nolimits_{s=1}^{t-1}\sum\nolimits_{(m, n)\in\mathcal{G}_{s}^{ind}}(\ymns -  \sigma({\xmns}^\top\btheta^*)) \xmns - \lambda\btheta^* \\
    =& \sum\nolimits_{s=1}^{t-1}\sum\nolimits_{(m, n)\in \mathcal{G}_s^{ind}} \epsilon_{mn}^s\xmns - \lambda\btheta^* = S_t - \lambda\btheta^*.
\end{align*}
\normalsize
where we define $S_t = \sum_{s=1}^{t-1}\sum_{(m, n)\in\mathcal{G}_{s}^{ind}}\epsilon^s_{mn}\xmns$.

Therefore, the confidence interval of the estimated pairwise preference in \model{} can be derived as:
\small
\begin{align*}
    \left|\sigma(\xijt^\top\btheta^*) - \sigma(\xijt^\top\hat{\btheta}_t) \right| \leq & ({2k_{\mu}}/{c_{\mu}}) \Vert\xijt\Vert_{\bM_t^{-1}}\left\| S_t - \lambda\btheta^*\right\|_{\bM_{t}^{-1}} \\
    % \leq & \frac{2k_{\mu}}{c_{\mu}} \Vert\xijt\Vert_{\bM_t^{-1}}\left(\left\| S_t\right\|_{\bM_t^{-1}} + \lambda \Vert \btheta^*\Vert_{\bM_{t}^{-1}}  \right) \\
    \leq & ({2k_{\mu}}/{c_{\mu}}) \Vert\xijt\Vert_{\bM_t^{-1}}\left(\left\| S_t\right\|_{\bM_t^{-1}} + \sqrt{\lambda}Q\right)
\end{align*}
\normalsize
where the second inequality is based on minimum eigenvalue $\lambda_{\min}(\bM_t) \geq \lambda$ and $\Vert\btheta\Vert_2 \leq Q$.

As $\epsilon_{mn}^t \sim R$-sub-Gaussian, according to Theorem 1 in~\cite{abbasi2011improved},
\small
\begin{equation*}
    \mathbb{P}\left[ \left\| S_t\right\|_{\bM_t^{-1}}^2 > 2R^2\log{\frac{\det(\bM_t)^{1/2}}{\delta \det(\lambda\mathbf{I})^{1/2}}}\right] \leq \delta
\end{equation*}
\normalsize

Therefore, with probability at least $1 - \delta_1$, we have
\begin{equation*}
    \left|\sigma({\xijt}^\top \hat{\btheta}_t) - \sigma({\xijt}^\top \btheta^*) \right| \leq \alpha_t\Vert\xijt\Vert_{\bM_t^{-1}}
\end{equation*}
with $\alpha_t = ({2k_{\mu}}/{c_{\mu}}) \Big(\sqrt{R^2\log{\frac{\det(\bM_t)}{\delta_1^2 \det(\lambda \mathbf{I})}}} + \sqrt{\lambda} Q\Big)$
% \end{proof}

\section{Proof of Theorem~\ref{theorem}}
In this section, we present the detailed proof of Theorem 1. We first prove Lemma~\ref{lemma:uncertain}, which provides an upper bound of the probability that an estimated pairwise preference is identified as uncertain. The blocks with uncertain rank orders will lead to regret in the ranked list due to the random shuffling based exploration strategy.


\subsection{Proof of Lemma~\ref{lemma:uncertain}}

\begin{proof}

In this proof, we will first provide an upper bound of the minimum eigenvalue of $\bM_t$, and then provide the detailed derivation of the probability's upper bound.

\noindent{\bf $\bullet~$Upper bound the minimum eigenvalue of $\bM_t$}.
As discussion in Lemma~\ref{lemma:matrix} , we assume that pairwise feature vectors are random vectors drawn from  distribution $v$. With $\bSigma = \mathbb{E}[\xmns\xmns^\top]$ as the second moment matrix, define $\bZ = \bSigma^\frac{-1}{2}\mathbf{X}$, where $\mathbf{X}$ is a random vector drawn from the same distribution $v$. Then $\bZ$ is isotropic, namely $\mathbb{E}[\bZ\bZ^\top] = \mathbf{I}_d$. 

Define $\bU = \sum_{s=1}^{t-1}\sum_{(m, n) \in \mathcal{G}_s^{ind}} \bZ_{mn}^s{\bZ_{mn}^s}^\top = \Sigma^\frac{-1}{2}\bar{\bM}_t\Sigma^\frac{-1}{2}$, where $\bar{\bM}_t = \sum_{s=1}^{t-1}\sum_{(m, n)  \in \mathcal{G}_s^{ind}}\xmns\xmns^\top = \bM_t - \lambda\mathbf{I}$. From Lemma~\ref{lemma:matrix}, we know that for any $l$, with probability at least $1 - 2\text{exp}(-C_2l^2)$,
$\lambda_{\text{min}}(\bU) \geq n - C_1\sigma^2\sqrt{nd} - \sigma^2l\sqrt{n}$, 
where $\sigma$ is the sub-Gaussian parameter of $\bZ$, which is upper-bounded by  $\Vert\bSigma^{-1/2}\Vert = \lambda_{\text{min}(\bSigma)}$, and $n = \sum_{s=1}^{t-1}|\mathcal{G}_s^{ind}|$, represents the number of observations so far. We thus can rewrite the above inequality which holds with probability $1 - \delta_2$ as
$\lambda_{\text{min}}(\bU) \geq n - \lambda_{\text{min}}^{-1}(\bSigma)(C_1\sqrt{nd} + l\sqrt{n})$.
We now bound the minimum eigenvalue of $\bar{\bM}_t$, as follows:
% \lipsum[2] 
\small
\begin{align*}
    \lambda_{\text{min}}(\bar{\bM}_t) &= \min_{x\in \mathbb{B}^d}x^\top\bar{\bM}_t x = \min_{x\in \mathbb{B}^d}x^\top\bSigma^{1/2}\bU\bSigma^{1/2}x \geq \lambda_{\text{min}}(\bU)\min_{x\in \mathbb{B}^d}x^\top \bSigma x \\
    &= \lambda_{\text{min}}(\bU)\lambda_{\text{min}}(\bSigma) \geq \lambda_{\text{min}}(\bSigma)n - C_1\sqrt{nd} - C_2\sqrt{n\log(1/\delta_2)}
\end{align*}
\normalsize
According to Lemma~\ref{lemma:matrix}, for $t \geq t^\prime$, we have:
\small
\begin{align*}
    &\lambda_{\text{min}}(\bSigma)t - (c_1\sqrt{d} + c_2\sqrt{\log({1}/{\delta_2})} + abd\sqrt{{o_{\text{max}}u^2}/({d\lambda})})\sqrt{t} \\
    & - (ab\log({1}/{\delta_1^2}) + 4a\lambda Q^2 - \lambda) \geq 0
\end{align*}
As $n \geq t$, we have
\begin{equation}\label{eq:lambda_min}
    \lambda_{\text{min}}({\bM}_t)  \geq  \lambda_{\text{min}}(\bar{\bM}_t) + \lambda \geq abd\sqrt{\frac{o_{\text{max}}u^2}{d\lambda}} + ab\log(\frac{1}{\delta_1^2}) + 4a\lambda Q^2
\end{equation}
\normalsize

\noindent{\bf $\bullet~$Upper bound the probability of being uncertain rank order}.
Under event $E_t$, based on the definition of $\mathcal{E}_u^t$ in Section 3.2, we know that for any document $\bx^t_i$ and $\bx^t_j$ at round $t$, $(i, j) \in \mathcal{E}_u^t$ if and only if $\sigma({\xijt}^\top\hat{\btheta}_t) - \alpha_t\Vert\xijt\Vert_{\bM_t^{-1}} \leq \frac{1}{2}$ and $\sigma({\xjit}^\top\hat{\btheta}_t) - \alpha_t\Vert\xjit\Vert_{\bM_t^{-1}} \leq \frac{1}{2}$. 
For a logistic function, we know that $\sigma(s) = 1 - \sigma(-s)$. Therefore, let $CB_{ij}^t$ denote $\alpha_t\Vert\xijt\Vert_{\bM_t^{-1}}$, we can conclude that $(i, j) \in \mathcal{E}_u^t$ if and only if $|\sigma({\xijt}^\top\hat{\btheta}_t )- \frac{1}{2}| \leq CB_{ij}^t$; and accordingly, $(i, j) \in \mathcal{E}_c^t$, when $|\sigma({\xijt}^\top\hat{\btheta}_t) - \frac{1}{2}| > CB_{ij}^t$.  To further simplify our notations, in the following analysis, we use $\hat{\sigma}_t$ and $\sigma^*$ to present $\sigma({\xijt}^\top\hat{\btheta}_t)$ and $\sigma({\xijt}^\top\btheta^*)$ respectively. 
According to the discussion above, the probability that the estimated preference between document $\bx^t_i$ and $\bx^t_j$ to be in an uncertain rank order, e.g., $(i, j) \in \mathcal{E}_u^t$ can be upper bounded by:
\small
\begin{align*}
    & \mathbb{P}\big((i, j) \in \mathcal{E}_u^t\big) = \mathbb{P}\big(|\hat{\sigma}_t - {1}/{2}| \leq CB_{ij}^t\big) \nonumber \\
    \leq& \mathbb{P}\left(\left||\hat{\sigma}_t - \sigma^*| - |\sigma^* - {1}/{2}|\right| \leq CB_{ij}^t\right) 
    \leq \mathbb{P} \left(|\sigma^* - {1}/{2}| - |\hat{\sigma}_t - \sigma^*| \leq CB_{ij}^t\right) \\
    \leq& \mathbb{P} \left(\Delta_{\min} - |\hat{\sigma}_t - \sigma^*| \leq CB_{ij}^t\right). 
\end{align*}
\normalsize
where, the first inequality is based on the reverse triangle inequality. %The second equality is obtained by considering the two sides of an absolute value. Under event $E_t$, we know that $|\sigma({\xijt}^\top\btheta^*) - \sigma({\xijt}^\top\hat{\btheta}_t) | < CB_{ij}^t$, therefore the probability of the first term  equals to zero and the second inequality hosts. 
The last inequality is based on the definition of $\Delta_{\min}$. Based on Lemma~\ref{lemma:cb}, the above probability can be further bounded by
\small
\begin{align*}
    &\mathbb{P} \left(\Delta_{\min} - |\hat{\sigma}_t - \sigma^*| \leq CB_{ij}^t\right) = \mathbb{P}\left(|\hat{\sigma}_t - \sigma^*| \geq \Delta_{\min} - \alpha_t\Vert\xijt\Vert_{\sub \bM_t^{-1}}\right) \\
    \leq& \mathbb{P} \left(({2k_{\mu}}/{c_{\mu}} )||\xijt||_{\sub \bM_t^{-1}}
    \left(\left\| S_t\right\|_{\sub \bM_t^{-1}} + \sqrt{\lambda}Q\right) \geq \Delta_{\min} - \alpha_t\Vert\xijt\Vert_{\sub \bM_t^{-1}}\right) \\
    \leq& \mathbb{P}\left(\left\| S_t\right\|_{\sub \bM_t^{-1}} \geq \frac{c_{\mu} \Delta_{\min}}{2k_{\mu}||\xijt||_{\sub \bM_{t}^{-1}}} - \left(\sqrt{\frac{1}{4}\log{\frac{\det(\bM_t)}{\delta^2 \det(\lambda \mathbf{I})}}} + 2\sqrt{\lambda} Q\right)\right)
\end{align*}
\normalsize
For the RHS of the inequality inside the probability, we know that:
\small
\begin{align*}
&\left(\frac{c_{\mu}\Delta_{\min}}{2k_{\mu}||\xijt||_{\sub \bM_{t}^{-1}}}
\right)^2 - \left(\sqrt{R^2\log{\frac{\det(\bM_t)}{\delta^2 \det(\lambda \mathbf{I})}}} + 2\sqrt{\lambda} Q\right)^2 \\
% \geq& \left(\frac{\beta c_{\mu}\Delta_{\min}}{2k_{\mu}||\xijt||_{\sub \bM_{t}^{-1}}}
% \right)^2 - \left(\sqrt{R^2\log{\frac{\det(\bM_t)}{\delta^2 \det(\lambda \mathbf{I})}}} + 2\sqrt{\lambda} Q\right)^2\\
\geq& {\lambda_{\min}(\bM_t)}/{a}  - R^2\log({\det(\bM_t)}/{(\delta_1^2\det(\lambda\mathbf{I}))})  \\
&- 4\lambda Q^2 - 4\sqrt{\lambda}QR\sqrt{\log({\det(\bM_t)}/{\det{\lambda\mathbf{I}}}) + \log({1}/{\delta_1^2})} \\
\geq& {\lambda_{\min}(\bM_t)}/{a} - b\left(\log({\det(\bM_t)}/{\det(\lambda\mathbf{I})})  + \log({1}/{\delta_1^2})\right)- 4\lambda Q^2\\
% \geq & \frac{1}{a}\cdot \lambda_{\min}(M_t) - b\cdot\left(d\log(1 + \frac{o_{\max}tU^2}{d\lambda}) + \log(\frac{1}{\delta_1^2})\right)- 4\lambda Q^2 \\
\geq & {\lambda_{\min}(\bM_t)}/{a} - bd\log(1 + \frac{o_{\max}tu^2}{d\lambda}) - b\log(\frac{1}{\delta_1^2}) - 4\lambda Q^2 \geq  0
\end{align*}
\normalsize
where the last inequality follows Eq~\eqref{eq:lambda_min}. Therefore, the probability could be upper bounded as:
\small
\begin{align*}
    &\mathbb{P} \left(\Delta_{\min} - |\hat{\sigma}_t - \sigma^*| \leq CB_{ij}^t\right) \\\leq
    &\mathbb{P}\left(\left\| S_t\right\|_{\sub \bM_t^{-1}}^2  {\geq} \left(\frac{c_{\mu} \Delta_{\min}}{2k_{\mu}\Vert\xijt\Vert_{\sub \bM_{t}^{-1}}} - \Big(\sqrt{R^2\log{\frac{\det(\bM_t)}{\delta \det(\lambda \mathbf{I})}}} + 2\sqrt{\lambda} Q\Big)\right)^2\right)\\
    \leq &  \mathbb{P}\left(\left\| S_t\right\|_{\sub \bM_t^{-1}}^2  {\geq}  \frac{(1 - 2\beta)c_{\mu}^2\Delta_{\min}^2}{4k_{\mu}^2\Vert\xijt\Vert_{\scriptscriptstyle\bM_t}^{-1}} + R^2\log(\frac{\det(M_t)}{\delta_1^2\det(\lambda\mathbf{I})})\right)\\
    \leq & \mathbb{P}\left(\left\| S_t\right\|_{\sub \bM_t^{-1}}^2  {\geq}  2R^2\log\left(\exp\left(\frac{(1 - 2\beta)c_{\mu}^2\Delta_{\min}^2}{8R^2k_{\mu}^2\Vert\xijt\Vert_{\sub \bM_t}^{-1}}\right) \cdot \frac{\det(\bM_t)}{\delta_1^2\det(\lambda\mathbf{I})})\right)\right) \\
    \leq & \delta_1 \cdot \exp^{-1}\left(\frac{(1 - 2\beta)c_{\mu}^2\Delta_{\min}^2}{8R^2k_{\mu}^2\Vert\xijt\Vert_{\sub \bM_t^{-1}}^2}\right) \leq \log(\frac{1}{\delta_1}) \cdot \frac{8R^2k_{\mu}^2\Vert\xijt\Vert_{\bM_t^{-1}}^2}{(1 - 2\beta)c_{\mu}^2\Delta_{\min}^2}
\end{align*}
\end{proof}

\normalsize
\subsection{Proof of Theorem~\ref{theorem}}
With $\delta_1$ and $\delta_2$ defined in the previous lemmas, we have the T-step regret upper bounded as:
\small
\begin{align}
    R_T  = R_{t^\prime} + R_{T - \tp} \leq  R^{\prime} + (1 - \delta_1) (1 - \delta_2)\sum\nolimits_{s = \tp}^{T} r_s 
\label{eqn:regret_all}
 \end{align}
\normalsize
where $R^{\prime} = \tp \cdot L_{\max} + (T - \tp)\left[\delta_2 \cdot L_{\max} + (1 - \delta_2)\cdot \delta_1 L_{\max}\right]$
Under event $E_t^1$ and $E_t^2$, the instantaneous regret at round $s$ is bounded by
\small
\begin{align}
    r_s = \mathbb{E} \big[K(\tau_s, \tau_s^*)\big] = \sum\nolimits_{i=1}^{d_s}\mathbb{E}\left[{(N_i^s + 1)N_i^s}/{2}\right] \leq \mathbb{E}\left[{N_s(N_s + 1)}/{2}\right]
\label{eqn:regret_t}
\end{align}
\normalsize
where $N_i^s$ denotes the number of uncertain rank orders in block $\mathcal{B}_i^s$ at round $s$ and $N_s$ denotes the total number of uncertain rank orders at round $s$. It follows that in the worst case scenario, when $N_s$ uncertain rank orders are all placed into the same block, at most $(N_s^2 + N_s)/2$ mis-ordered pairs will be generated by random shuffling in \model{}. This is due to the fact that based on the block created by \model{}, with $N_s$ uncertain rank orders in one block, this block can have at most $N_s + 1$ documents. 

Therefore, the cumulative regret after $\tp$ can be bounded by:
\small
 \begin{align*}
     \sum\nolimits_{s=\tp}^T r_s \leq & \sum\nolimits_{s=\tp}^T\mathbb{E}\left[{N_s(N_s+1)}/{2}\right] \nonumber \leq  \left(\sum\nolimits_{s=\tp}^T\mathbb{E}[N_s^2] + \mathbb{E}[N_s]\right)/2 \nonumber \\
    \leq &  \left(\mathbb{E}[\sum\nolimits_{s=\tp}^TN_s]^2 +\mathbb{E}[\sum\nolimits_{s=\tp}^TN_s]\right)/2
 \end{align*}
 \normalsize

According to our previous analysis, at round $t \geq \tp$, the number of uncertain rank orders can be estimated by the probability of observing an uncertain rank order, i.e., $\mathbb{P}\big((i, j) \in \mathcal{E}_u^t\big) \leq \log({1}/{\delta_1}) \cdot \frac{8R^2k_{\mu}^2\Vert\xijt\Vert_{M_t^{-1}}^2}{(1 - 2\beta)c_{\mu}^2\Delta_{\min}^2}$. Therefore, the cumulative number of mis-ordered pairs can be bounded by the probability of observing uncertain rank orders in each round, which shrinks with more observations become available over time, 
\small
\begin{align*}
\label{eqn:up}
    \mathbb{E}\left[\sum\nolimits_{s=\tp}^T N_s\right] \leq &  \mathbb{E}\left[\frac{1}{2}\sum\nolimits_{s=\tp}^T \sum\nolimits_{(m, n) \in [L_s]^2} \mathbb{P}\big((m, n) \in \mathcal{E}_u^t\big)\right] \\
    \leq & \mathbb{E}\left[a\sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in [L_s]^2} \Vert\xmns\Vert_{\bM_s^{-1}}^2\right]
\end{align*}
\normalsize
where $a = \log({1}/{\delta_1})\cdot{4k_{\mu}^2}/({(1 - 2\beta)c_{\mu}^2\Delta_{\min}^2})$

Because $\bM_t$ only contains information of observed document pairs so far, \model{} guarantees the number of mis-ordered pairs among the observed documents in the above inequality is upper bounded. 
To reason about the number of mis-ordered pairs in those unobserved documents (i.e., from $o_t$ to $L_t$ for each query $q_t$), we leverage the constant $p^*$, which is defined as the minimal probability that all documents in a query are examined over time, 
\small
\begin{align*}
    &\mathbb{E}\left[\sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in [L_s]^2} \Vert\xmns\Vert_{\bM_s^{-1}}^2\right] \\
    =& \mathbb{E}\left[\sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in [L_s]^2} \Vert\xmns\Vert_{\bM_s^{-1}}^2\times \mathbb{E}\left[\frac{1}{p_{s}}\mathbf{1}\{o_s = L_s\}\right]\right] \\
    \leq& 
    {p^*}^{-1}\mathbb{E}\left[\sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in [L_s]^2} \Vert\xmns\Vert_{\bM_s^{-1}}^2 \mathbf{1}\{o_s = L_s\}\right] 
\end{align*}
\normalsize

Besides, in \model{}, we only use the independent pairs, $\mathcal{G}_t^{ind}$ to update the model and the corresponding $M_t$ matrix. Therefore, to bound the regret, we rewrite the above equation as:

\small
\begin{align*}
    &\mathbb{E}\left[\sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in [L_s]^2} \Vert\xmns\Vert_{\bM_s^{-1}}^2\right]  \\
    =& \mathbb{E}\left[\sum_{s=\tp}^T\sum_{(m, n) \in \mathcal{G}_t^{ind}} \left (\Vert\xmns\Vert_{\bM_s^{-1}}^2 + \sum_{k\in [L_t] \setminus \{m, n\} } \Vert\bx_{mk}^s\Vert_{\bM_s^{-1}}^2 + \Vert\bx_{nk}^s\Vert_{\bM_s^{-1}}^2\right)\right] \\
    =& \mathbb{E}\left[\sum_{s=\tp}^T\sum_{(m, n) \in \mathcal{G}_t^{ind}} \left ((L_t - 1)\Vert\xmns\Vert_{\bM_s^{-1}}^2 + \sum_{k\in [L_t] \setminus \{m, n\} } 2{\bx_{mk}^s}^\top\bM_s^{-1}\bx_{nk}^s\right)\right]
\end{align*}
\normalsize


% The regret is thus upper bounded in terms of $\sum_{s=\tp}^T\sum_{(m, n) \in [L_s]^2} \Vert\xmns\Vert_{\bM_s^{-1}}^2$. The upper bound of the 

% \begin{lemma}
% \label{lemma:det}
% Since for any $1 \leq t \leq T$ and $m\in[L_t]$, $\Vert \bx^t_m\Vert_2 \leq U$. And $\bM_t = \lambda \mathbf{I} + \sum_{s=1}^{t-1}\sum_{(m, n)\in\mathcal{G}_s}$ $\xmns{\xmns}^\top$ with $\lambda > 0$. Then,
% \begin{equation*}
%     \det(\bM_t) \leq (\lambda + o_{\max}tU^2/d)^d
% \end{equation*}
% \end{lemma}

% \begin{proof}
% Let $\alpha_1$, $\alpha_2$, ..., $\alpha_d$ be the eigenvalues of $\bM_t$. All the eigenvalues are positive as $M_t$ is positive definite. With $\det(\bM_t) = \prod_{k=1}^d\alpha_k$,  $\textit{trace}(\bM_t) = \sum_{k=1}^d\alpha_k$, and
% \begin{equation*}
%     \sqrt{\alpha_1\alpha_2...\alpha_d} \leq \frac{\alpha_1 + \alpha_2 +... + \alpha_d}{d}
% \end{equation*}
% we have $\det(\bM_t) \leq (\frac{\textit{trace}(\bM_t)}{d})^d$. The trace can be further upper bounded by:
% \begin{align*}
%         \textit{trace}(\bM_t) & = \textit{trace}(\lambda\mathbf{I}) + \sum_{s=1}^{t-1}\sum_{(i, j) \in \mathcal{G}_s}\textit{trace}({\xmns}{\xmns}^\top) \\
%         &= d\lambda + \sum_{s=1}^{t-1}\sum_{(i, j) \in \mathcal{G}_s} \Vert{\xmns}\Vert_2^2 \leq d\lambda + o_{\max}tU^2
% \end{align*}
% and this concludes the proof of this lemma.
% \end{proof}


% Therefore, the sum of vector norms can be bounded as follows.

% \begin{lemma}
% \label{lemma:norm}
% With $(m, n) \in \mathcal{G}_s$ represents the observed document pairs in round $s$, we have
% \begin{equation*} 
%     \sum_{s=1}^{t}\sum_{(m, n) \in \mathcal{G}_s} \Vert\xmns\Vert_{\bM_{s}^{-1}}^2 \leq 2d\log\left(1 + \frac{o_{\max}tU^2}{d\lambda}\right)
% \end{equation*}
% \end{lemma}

% \begin{proof}
% It is easy to derive that,
% \begin{align*}
%     \det({\bM_t}) &= \det\Big({\mathbf{M}_{t-1} + \sum_{(m, n) \in \mathcal{G}_t} \xmnt{\xmnt}^\top}\Big) \\
%      & = \det({\mathbf{M}_{t-1}) \det\Big(\mathbf{I} + \sum_{(m, n) \in \mathcal{G}_t} \bM_{t-1}^{-1/2}\xmnt{\xmnt}^\top\bM_{t-1}^{-1/2}}\Big) \\
%     &= \det({\mathbf{M_{t-1}}) \Big(1 + \sum_{(m, n) \in \mathcal{G}_t} \Vert\xmnt\Vert_{\bM_{t-1}^{-1}}^2}\Big)\\
%     & = \det(\lambda\mathbf{I})\prod_{s=1}^t \Big(1 + \sum_{(m, n) \in \mathcal{G}_s} \Vert\xmns\Vert_{\bM_{s}^{-1}}^2\Big).
% \end{align*}

% With the fact that $2\log(1 + u) > u$ for any $u\in [0, 1]$, we have
% \begin{align*}
%     \sum_{s=1}^t\sum_{(m, n) \in \mathcal{G}_s} \Vert{\xmns}\Vert_{\bM_{s}^{-1}}^2 &\leq 2\sum_{s=1}^t\log \left(1 + \sum_{(m, n) \in \mathcal{G}_s} \Vert\xmns\Vert_{\bM_{s}^{-1}}^2\right) \\
%     &= 2\log\left(\prod_{s=1}^t (1 + \sum_{(m, n) \in \mathcal{G}_s} \Vert\xmns\Vert_{\bM_{s}^{-1}}^2)\right) \\
%     &\leq 2\log\left(\frac{\det(\bM_t)}{\det(\lambda \mathbf{I})}\right) \\
%     &\leq 2d\log\left(1 + \frac{o_{\max}tU^2}{d\lambda}\right)
% \end{align*}
%  where the last equality is from Lemma~\ref{lemma:det}
%  \end{proof}
 
Based on Lemma 10 and Lemma 11 in \cite{abbasi2011improved}, the expected number of the first term can be upper bounded by,
\small
\begin{align*}
    \sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in \mathcal{G}_t^{ind}} (L_t - 1)\Vert\xmns\Vert_{\bM_s^{-1}}^2 \leq 2dL_{max}\log(1 + \frac{o_{\max}Tu^2}{2d\lambda})
\end{align*}
\normalsize
And the second term can be bounded as:
\small
\begin{align*}
    &\sum\nolimits_{s=\tp}^T\sum\nolimits_{(m, n) \in \mathcal{G}_t^{ind}}\sum\nolimits_{k\in [L_t] \setminus \{m, n\} } 2{\bx_{mk}^s}^\top\bM_s^{-1}\bx_{nk}^s \\
    \leq & \sum\nolimits_{s=\tp}^T {(L_{max}^2 - 2L_{max})u^2 }/{\lambda_{\min}(\bM_s)} = w.
\end{align*}
\normalsize
% where $C_3 = abd\sqrt{\frac{o_{\max}U^2}{2d\lambda}} + 4a\lambda Q$. 

Therefore, combining the conclusion of $\mathbb{E}[\sum_{s = \tp}^T N_s]$, and Eq~\eqref{eqn:regret_all}, Eq~\eqref{eqn:regret_t}, the regret could be upper bounded by:
 \small
 \begin{align*}
     R_T 
    %  \leq& R^{\prime} + (1 - \delta_1) (1 - \delta_2)\sum_{s = \tp}^{T} r_s  \\
     \leq& R^{\prime} + (1 - \delta_1)(1 - \delta_2) 
     \frac{1}{p^*{^2}}\left( 2adL_{\max}\log(1 + \frac{o_{\max}Tu^2}{2d\lambda}) + aw\right)^2
 \end{align*}
 \normalsize
 
%  $\sum_{s = \tp}^{T} r_s \leq \left(\frac{2adL_{\max}}{p^*}\log(1 + \frac{o_mTU^2}{2d\lambda}) + \frac{aw}{p^*}\right)^2$.
 
where $R^{\prime} = \tp L_{\max} + (T - t')(\delta_2L_{\max} - (1- \delta_2)\delta_1 L_{\max})$, with $\tp$ defined in Lemma \ref{lemma:uncertain}, and $L_{\max}$ representing the maximum number of documents associated to the same query over time. 
By choosing $\delta_1 = \delta_2 =1/T$, the theorem shows that the expected regret is at most $R_T \leq O(d\log^4(T))$.
% \end{equation}

% \remark






