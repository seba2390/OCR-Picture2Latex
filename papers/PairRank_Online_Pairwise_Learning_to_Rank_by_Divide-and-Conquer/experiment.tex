\section{Experiments}

\begin{figure*}[t]
%   \vspace{-3mm}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/offline_web10k.png}
    \caption{Offline performance (NDCG@10) on the MSLR-WEB10K dataset.}
    \label{fig:offline_WEB10K}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/offline_yahoo.png}
    \caption{Offline performance (NDCG@10) on the Yahoo! dataset.}
    \vspace{-2mm}
    \label{fig:offline_Yahoo}
  \end{subfigure}
  \caption{Offline ranking performance on two different datasets under three different click models.}
  \label{fig:result}
  \vspace{-4mm}
\end{figure*}

In this section, we empirically compare our proposed \model{} with an extensive list of state-of-the-art OL2R algorithms on two public learning to rank benchmark datasets. Both quantitative and qualitative evaluations are performed to examine our proposed solution, especially its advantages over existing OL2R solutions in online learning efficiency. 

\noindent{\bf Reproducibility}
Our entire codebase, baselines, analysis, and experiments can be found on
Github~\footnote{https://github.com/yilingjia/PairRank}.

\subsection{Experiment Setup}

\textbf{Datasets.} We experiment on two publicly available learning to rank datasets: 1) Yahoo! Learning to Rank Challenge dataset \cite{chapelle2011yahoo}, which consists of 292,921 queries and 709,877 documents represented by 700 ranking features; and 2) MSLR-WEB10K \cite{qin2013introducing}, which contains 30,000 queries, each having 125 assessed documents on average, and is represented by 136 ranking features. Both datasets are labeled with a five-grade relevance scale: from not relevant (0) to perfectly relevant (4). We followed the train/test/validation split provided in the datasets.


\begin{table}
  \caption{Configuration of simulated click models.}
  \vspace{-2mm}
  \label{table:click}
  \centering
  \begin{tabular}{cccc|ccc}
    \hline
                & \multicolumn{3}{c}{Click Probability} & \multicolumn{3}{c}{Stop Probability} \\
Relevance Grade & 0           & 1          & 2          & 0          & 1          & 2          \\ \hline
Perfect         & 0.0         & 0.5        & 1.0        & 0.0        & 0.0        & 0.0        \\
Navigational    & 0.05        & 0.5        & 0.95       & 0.2        & 0.5        & 0.9        \\
Informational   & 0.4         & 0.7        & 0.9        & 0.1        & 0.3        & 0.5        \\ \hline
\end{tabular}
\vspace{-5mm}
\end{table}

\noindent\textbf{User Interaction Simulation.} We simulate user behavior via the standard setup for OL2R evaluations \cite{oosterhuis2018differentiable, wang2019variance} to make our reported results directly comparable to those in literature. First, at each time $t$, a query is uniformly sampled from the training set for result serving. Then, the model determines the ranked list and returns it to the users. The interaction between the user and the list is then simulated with a dependent click model~\cite{guo2009efficient}, which assumes that the user will sequentially examine the list and make a click decision on each document. At each position, the user decides whether to click on the document or not, modeled as a probability conditioned on the document's relevance label, e.g, $\mathbb{P}(click = 1 | \text{relevance grade})$. After the click, the user might stop due to his/her satisfaction with the result or continue to examine more results. The stop probability after a click is modeled as $\mathbb{P}(stop = 1 |click = 1, \text{relevance grade})$. If there is no click, the user will continue examining the next position. We employ three different click model configurations to represent three different types of users, for which the details are shown in Table \ref{table:click}. Basically, we have the \textit{perfect} users, who click on all relevant documents and do not stop browsing until the last returned document; the \textit{navigational} users, who are very likely to click on the first highly relevant document and stop there; and the \textit{informational} users, who tend to examine more documents, but sometimes click on irrelevant ones, and thus contribute a significant amount of noise in their click feedback.




\noindent\textbf{Baselines.} As our \model{} learns a parametric ranker, we focus our comparison against existing parametric OL2R solutions, which are known to be more generalizable and powerful than those estimating pointwise query-document relevance \cite{radlinski2008learning,kveton2015cascading,zoghi2017online,lattimore2018toprank}. We list the OL2R solutions used for our empirical comparisons below.

\noindent $\bullet$ \textbf{DBGD} \cite{yue2009interactively}: As detailed in our related work discussion, DBGD uniformly samples an exploratory direction from the entire model space for exploration and model update.

\noindent $\bullet$ \textbf{MGD} \cite{schuth2016multileave}: It improves DBGD by sampling multiple directions and compares them via a multileaving test. If there is a tie, the model updates towards the mean of all winners.

\noindent $\bullet$  \textbf{PDGD} \cite{oosterhuis2018differentiable}: 
PDGD samples the next ranked document from a Plackett-Luce model and estimates the gradient from the inferred pairwise result preferences.

\noindent $\bullet$  \textbf{$\epsilon$-Greedy} \cite{hofmann2013balancing}: It randomly samples an unranked document with probability $\epsilon$ or selects the next best document based on the currently learned RankNet with probability $1 - \epsilon$, at each position.

\noindent $\bullet$ \textbf{SGD RankNet}: It estimates a single layer RankNet with stochastic gradient descent using the same click preference inference method in \model{}. At each round, it presents the best ranking based on the currently learned RankNet model.

\noindent $\bullet$ \textbf{PairRank}: We compare two shuffling strategies within each partition: 1) random shuffling the documents, denoted as \model{}\_R, and 2) shuffling with respect to the uncertain rank orders, while preserving the certain rank orders within each partition, denoted as \model{}\_C (as illustrated in Figure \ref{fig:model}).

\subsection{Experiment results}

\subsubsection{Offline Performance}
In offline evaluation, while the algorithm is learning, we evaluate the learned rankers on a separate testing set using its ground-truth relevance labels. We use Normalized Discounted Cumulative Gain at 10 (NDCG@10) to assess different algorithms' ranking quality. We compare all algorithms over 3 click models and 2 datasets. We set the hyper-parameters, such as learning rate, in DBGD, MGD, and PDGD according to their original papers. For SGD RankNet, $\epsilon$-Greedy, and our proposed \model{}, we set the hyper-parameters via the best performance on the validation data for each dataset. 
We fix the total number of iterations $T$ to 5000. We execute the experiments
20 times with different random seeds and report the averaged result in Figure~\ref{fig:result}. For comparison purposes, we also include the performance of an offline RankNet trained on the ground-truth relevance labels on the training set, whose performance can be considered as an upper bound in our evaluation.

\begin{figure*}[t]
%   \vspace{-3mm}
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/online_MSLR-WEB10K.png}
    \vspace{-4mm}
    \caption{Online performance (cNDCG@10) on the MSLR-WEB10K dataset.}
    \label{fig:online_WEB10K}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/online_Yahoo.png}
    \vspace{-4mm}
    \caption{Online performance (cNDCG@10) on the Yahoo! dataset.}
    \label{fig:online_Yahoo}
  \end{subfigure}
  \vspace{-6mm}
  \caption{Online ranking performance on two different datasets under three different click models.}
  \label{fig:online}
  \vspace{-2mm}
\end{figure*}

We can clearly observe that \model{} achieved significant improvement over all the baselines. Across the two datasets under different click models, DBGD and MGD performed worse than other OL2R solutions. This is consistent with previous studies' findings: DBGD and MGD depend on interleave test to determine the update direction across rankers. But such model-level feedback is hard to directly inform the optimization of any rank-based metric (e.g., NDCG). PDGD consistently outperformed DBGD-type solutions under different click models and datasets. However, its document sampling-based exploration limits its learning efficiency, especially when users only examine a small number of documents (e.g., the navigational users).
$\epsilon$-Greedy seriously suffered from its random exploration, which is independent of how the current ranker performs. Oftentimes as shown in Figure~\ref{fig:offline_WEB10K}, its performance is even worse than MGD, although $\epsilon$-Greedy can directly optimize the pairwise ranking loss.  
One interesting observation is that SGD RankNet performs comparably to PDGD. Exploration in SGD RankNet is implicitly achieved via stochastic gradient descent. However, because this passive exploration is subject to the given queries and noise in user feedback, its performance in online result serving is hard to predict. 
We attribute \model{}'s fast convergence to its uncertainty based exploration: it only explores when its estimation on a pair of documents is uncertain. As proved in our regret analysis, the number of such pairs shrinks at a logarithmic rate, such that more and more documents are presented in their correct ranking positions as \model{} learns from user feedback. This conclusion is further backed by the comparison between \model{}\_R and \model{}\_C: by respecting the certain rank orders within a partition, \model{}\_C further improves result ranking quality. When click noise is small, i.e., from perfect users, \model{} converges to its offline counterpart with only a few thousand impressions. 


\subsubsection{Online Performance}



In OL2R, in addition to the offline evaluation, a model's ranking performance during online optimization should also be evaluated, as it reflects user experience. Sacrificing user experience for model training will compromise the goal of OL2R. We adopt the cumulative Normalized Discounted Cumulative Gain~\cite{oosterhuis2018differentiable} to assess models' online performance. For $T$ rounds, the cumulative NDCG (cNDCG) is calculated as
\small
\begin{equation*}
    \text{cNDCG} = \sum\nolimits_{t=1}^T \text{NDCG}(\tau_t) \cdot \gamma^{(t-1)},
\end{equation*}
\normalsize
which computes the expected NDCG reward a user receives with a probability $\gamma$ that he/she stops searching after each query~\cite{oosterhuis2018differentiable}. Following the previous work~\cite{oosterhuis2018differentiable, wang2019variance, wang2018efficient}, we set $\gamma = 0.9995$.

Figure~\ref{fig:online} shows the online performance of \model{} and all the other baselines. It is clear to observe that directly adding model-independent exploration, e.g., $\epsilon$-greedy, strongly hurts a model's online ranking performance, and therefore hurts user experience. 
Compared to PDGD, SGD RankNet showed consistently better performance, especially under the navigational and informational click models. We attribute this difference to the exploration strategy inside PDGD: though SGD RankNet has limited exploration and focuses more on exploiting the learned model in ranking the documents, PDGD's sampling-based exploration might introduce unwanted distortion in the ranked results, especially at the early stage of online learning. 

We should note in cumulative NDCG, the earlier stages play a much more important role due to the strong shrinking effect of $\gamma$. Our proposed \model{} demonstrated significant improvements over all the baselines.
Such improvement indicates the effectiveness of our uncertainty based exploration, which only explores when the ranker's pairwise estimation is uncertain. 
We can also observe the difference between PairRank\_C and PairRank\_R in this online evaluation. This is because PairRank\_C preserves the certain rank order within blocks, which further eliminates unnecessary exploration (random shuffling) in result serving.

\subsubsection{Closeness to the \emph{Ideal} Ranker}


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=0.95\textwidth]{figures/cosine_similarity.png}
    \end{subfigure}
    \vspace{-4mm}
    \caption{Cosine similarity between offline RankNet model and online models.}
    \label{fig:cosine}
    \vspace{-4mm}
\end{figure}


In this experiment, we view the offline trained RankNet model on the complete annotated relevance as the ideal model, denoted as $w^*$, and compare the cosine similarities between $w^*$ and the online estimated models in Figure~\ref{fig:cosine}. We can observe that all the pairwise models push the learned ranker closer to the ideal one with more iterations, while \model{} converges faster and closer to $w^*$. Apparently, the rankers obtained by DBGD and MGD are quite different from the ideal model. This confirms our earlier analysis that DBGD-type ranker update can hardly link to any rank-based metric optimization, and thus it becomes less related to the offline learned ranker.  
The difference in objective function can also explain why SGD RankNet converges faster than PDGD, as PDGD adopts the pairwise Plackett-Luce model while SGD RankNet has the same objective function as the offline model.

\subsubsection{Zoom into \model{}}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.235\textwidth}
    \includegraphics[width=\textwidth]{figures/block_size_web10k.png}
    \caption{MSLR-WEB10K Dataset}
    \label{fig:block_size_web10k}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/block_size_yahoo.png}
    \caption{Yahoo Dataset}
    \label{fig:block_size_yahoo}
    \end{subfigure}
    \vspace{-4mm}
    \caption{The size of blocks at top ranks.}
    \label{fig:block_size}
    \vspace{-4mm}
\end{figure}


To further verify the effectiveness of the exploration strategy devised in \model{}, we zoom into the trace of its block size across queries during the online model update. As \model{} uses random shuffling within each block for exploration, a smaller block size, especially at the top-ranked positions, is preferred to reduce regret. Figure~\ref{fig:block_size} shows the size of document blocks at rank position 1, 5 and 10. 

First, we can clearly observe that after hundred rounds of interactions, the sizes of blocks quickly converge to 1, especially at the top-ranked positions. This confirms our theoretical analysis about \model{}'s block convergence. And by comparing the results across different click models, we can observe that the block size converges slower under the navigational click model. Similar trends can be observed in Figure~\ref{fig:number_of_block}. In this figure, the number of blocks is calculated by averaging in every 200 iterations to reduce variance. We can observe that at the early stages, \model{} under the navigational click model has fewer blocks (hence more documents in one block), which indicates a higher uncertainty in model estimation. The key reason is that much fewer clicks can be observed in each round under the navigational click model, as the stop probability is much higher, i.e., stronger position bias. As a result, more interactions are needed to improve model estimation. For the same reason, in Figure~\ref{fig:block_size}, the block size at rank position 10 shrinks much slower than that at other positions also suggests position bias is the main factor that slows down the learning of \model{}. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/Block_size.png}
    \caption{MSLR-WEB10K with informational click model.}
    \label{fig:block_size_inf}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
    \includegraphics[width=\textwidth]{figures/Number_of_Block.png}
    \caption{Number of blocks identified on MSLR-WEB10K dataset}
    \label{fig:number_of_block}
    \end{subfigure}
    \caption{Qualitative analysis of blocks in \model{}}
    \label{fig:block_analysis}
\end{figure}

In Figure~\ref{fig:block_size_inf}, we show the block size at each rank position in \model{} under the informational click model at different time points. One interesting finding is the blocks in the middle ranks, i.e., rank 300 - 400, tend to have more documents, while for the ranks at both the top and the bottom, the corresponding blocks tend to have fewer documents. We attribute it to the pairwise learning in \model{}, where the uncertainty is calculated on the pairwise preferences, and thus it is easy to identify the document pairs with greater differences.





