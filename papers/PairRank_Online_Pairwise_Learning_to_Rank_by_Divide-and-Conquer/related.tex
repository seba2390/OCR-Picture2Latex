\section{Related Work}
%OL2R, which directly learn from the user interactions, have attracted great attention in recent years. Unlike methods in the offline setting, OL2R needs to explore some uncertain retrieval results for model update meanwhile guaranteeing to produce high-quality retrieval results by exploiting what has already been learned. Various algorithms have been proposed, and they can be broadly categorized into two main branches, model-based algorithms that learn the best ranked list under some model of user interaction with the list, such as a click model, and model-free algorithms assume that relevant documents are more likely to receive more clicks than non-relevant documents and exploits clicks to identify the gradient's direction.

We broadly categorize existing OL2R solutions into two families.
%, based on how they perform exploration. 
The first type learns the best ranked list for each individual query separately by modeling users' click and examination behaviors with multi-armed bandit algorithms \cite{radlinski2008learning,kveton2015cascading,zoghi2017online,lattimore2018toprank}. 
Typically, solutions in this category depend on specific click models to decompose the estimation on each query-document pair; as a result, exploration is performed on the ranking of individual documents. For example, by assuming users examine documents from top to bottom until reaching the first relevant document,  cascading bandit models perform exploration by ranking the documents based on the upper confidence bound of their estimated relevance \cite{kveton2015cascading, kveton2015combinatorial, li2016contextual}. 
%To enable learning from multiple clicks in a ranked list, the dependent click model~\cite{guo2009efficient} is later adopted \cite{}. 
Other types of click models have also been explored (such as the dependent click model) \cite{katariya2016dcm,zoghi2017online, lattimore2018toprank, li2018online, kveton2018bubblerank}. 
However, as the relevance is estimated for each query-document pair, such algorithms can hardly generalize to unseen queries or documents. Moreover, pointwise relevance estimation is proved to be ineffective for rank estimation in established offline learning to rank studies \cite{chapelle2011yahoo,burges2010ranknet}.

The second type of OL2R solutions leverage ranking features for relevance estimation and explores for the best ranker in the entire model space \cite{yue2009interactively,li2018online,oosterhuis2018differentiable}. The most representative work is Dueling Bandit Gradient Descent (DBGD) \cite{yue2009interactively,schuth2014multileaved}, which proposes an exploratory direction in each round of interaction and uses an interleaved test \cite{chapelle2012large} to validate the exploration for model update. To ensure an unbiased gradient estimate, DBGD uniformly explores the entire model space, which 
%unfortunately 
costs high variance and high regret during online ranking and model update. 
Subsequent methods improved upon DBGD by developing more efficient sampling strategies, such as multiple interleaving and projected gradient, to reduce variance \cite{hofmann2012estimating,zhao2016constructing,oosterhuis2017balancing, wang2018efficient, wang2019variance}. However, as exploration is performed in the model space, click feedback is used to infer which ranker is preferred under a hypothetical utility function. It is difficult to reason how the update in DBGD is related to the optimization of any rank-based metric. Hence, though generalizable, this type of OL2R solutions' empirical performance is still worse than classical offline solutions.

% For example, \cite{} chose to filter the exploration directions by using the historical data, \cite{} proposed exploring gradients in subspace constructed by a set of pre-selected reference documents, \cite{} proposed using historical interactions to avoid repeated exploring less promising directions
% where the algorithm proposes an exploratory direction in each interaction and uses an interleaved test to validate the exploration for model update.

% Online RankSVM
% DBGD MGD Historical data DP DBGD
% PDGD

The clear divide between the practices in online and offline learning to rank is quite remarkable, which has motivated some recent efforts to bridge the gap. \citet{hofmann2013balancing} adopt $\epsilon$-greedy to estimate a stochastic RankSVM~\cite{joachims2002optimizing, herbrich1999support} model on the fly. Though RankSVM is effective for pairwise learning to rank, the totally random exploration by $\epsilon$-greedy is independent from the learned ranker. It keeps distorting the ranked results, even when the ranker has identified some high-quality results. \citet{oosterhuis2018differentiable} perform exploration by sampling the next ranked document from a Plackett-Luce model and estimate gradients of this ranking model from the inferred pairwise result preferences. Although exploration is linked to the ranker's estimation, the convergence of this solution is still unknown. 
%PDGD created a probability distribution over document set and constructed the result list by sampling documents from the distribution with the . They proved that the gradient is unbiased w.r.t user document pair preference, but the convergence of the model is not theoretically guaranteed.

%But the performance was less effective than DBGD. In our solution, given the success of offline learning to rank, we choose to directly optimize the ranker with the inferred pairwise preference between documents. Different from exploring by sampling, we leverage the model confidence about the estimation to reduce the search space and provide a well-calibrated exploration strategy. 
%The fast convergence and the effectiveness of \model{} are further supported by the theoretical analysis and empirical results.