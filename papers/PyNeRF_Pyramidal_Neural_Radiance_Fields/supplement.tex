\clearpage
\appendix

\section*{Supplemental Materials}
%%%%%%%%% BODY TEXT

\section{Single-scale datasets}

Although \method\ is designed for scenarios that capture scene content at different distances, we also evaluate it on the original Synthetic-NeRF~\cite{mildenhall2020nerf} dataset where the camera distance remains constant. In this scenario, \method\ performs similarly to existing SOTA, as shown in \cref{table:single-scale-results}. 

\begin{table}[H]
\caption{{\bf Single-scale results.} We evaluate \method\ on single-scale Blender~\cite{mildenhall2020nerf}. \method\ performs comparably to existing state-of-the-art.}
\centering
\resizebox{0.9\linewidth}{!}{%
% \footnotesize
\begin{tabular}{l@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}}
\toprule 
PSNR & Lego & Mic & Materials & Chair & Hotdog & Ficus & Drums & Ship & Mean \\ \midrule
K-Planes~\cite{kplanes_2023} & 35.38 & 33.27 & 29.57 & 33.88 & 36.19 & 30.81 & \underline{25.62} & 30.16 & 31.86 \\
TensoRF~\cite{Chen2022ECCV} & 35.14 & 25.70 & \textbf{33.69} & \textbf{37.03} & 36.04 & 29.77 & 24.64 & 30.12 & 31.52  \\
iNGP~\cite{mueller2022instant} & \underline{35.67} & \textbf{36.85} & 29.60 & 35.71 & \underline{37.37} & \underline{33.95} & 25.44 & 30.29 & \underline{33.11} \\
Nerfacto~\cite{nerfstudio} & 34.84 & 33.58 & 26.50 & 34.48 & 37.07 & 30.66 & 23.63 & \textbf{30.95} & 31.46 \\
\midrule
\method & \textbf{36.63} & \underline{36.39} & \underline{29.92} & \underline{35.76} & \textbf{37.64} & \textbf{34.29} & \textbf{25.80} & \underline{30.64} & \textbf{33.38} \\
\bottomrule
\end{tabular}%
}
\vspace*{-4mm}
\label{table:single-scale-results}
\end{table}

\section{Additional results}

We list results for each individual downsampling level in \cref{table:synthetic-results-downsample} and \cref{table:real-world-results-downsample} to supplement those shown in \cref{table:synthetic-results} and \cref{table:real-world-results}.

\begin{table}[H]
\caption{{\bf Synthetic results.} We average results across Multiscale Blender~\cite{barron2021mipnerf} and Blender-A and list metrics for each downsampling level.  All \method\ variants outperform their baselines by a wide margin.}
\centering
\resizebox{\linewidth}{!}{%
% \footnotesize
\begin{tabular}{l@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}}
\toprule 
& \multicolumn{4}{c}{$\uparrow$PSNR} & \multicolumn{4}{c}{$\uparrow$SSIM} & \multicolumn{4}{c}{$\downarrow$LPIPS} \\ 
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& Full Res. & $\nicefrac{1}{2}$ Res. & $\nicefrac{1}{4}$ Res. & $\nicefrac{1}{8}$ Res.
& Full Res. & $\nicefrac{1}{2}$ Res. & $\nicefrac{1}{4}$ Res. & $\nicefrac{1}{8}$ Res.
& Full Res. & $\nicefrac{1}{2}$ Res. & $\nicefrac{1}{4}$ Res. & $\nicefrac{1}{8}$ Res.
& $\downarrow$Avg Error \\ \midrule
Plenoxels~\cite{yu_and_fridovichkeil2021plenoxels} & 22.61 & 23.68 & 24.54 & 23.62 & 0.767 & 0.768 & 0.784 & 0.789 & 0.307 & 0.265 & 0.200 & 0.161 & 0.102 \\
K-Planes~\cite{kplanes_2023} &  25.14 & 27.03 & 30.26 & 30.75 & 0.807 & 0.840 & 0.896 & 0.925 & 0.225 & 0.163 & 0.085 & 0.053 & 0.046 \\
TensoRF~\cite{Chen2022ECCV} & 25.93 & 28.12 & 31.46 & 30.97 & 0.865 & 0.893 & 0.921 & 0.930 & 0.169 & 0.112 & 0.064 & 0.056 & 0.042 \\
iNGP~\cite{mueller2022instant} & 26.90 & 29.14 & 30.89 & 28.49 & 0.865 & 0.905 & 0.947 & 0.947 & 0.152 & 0.095 & 0.047 & 0.054 & 0.032 \\
Nerfacto ~\cite{nerfstudio} & 25.35 & 27.26 & 29.78 & 29.09 & 0.809 & 0.840 & 0.893 & 0.917 & 0.214 & 0.158 & 0.094 & 0.068 & 0.049 \\
Mip-NeRF~\cite{barron2021mipnerf} & 32.07 & 33.65 & 34.76 & 35.00 & 0.952 & 0.959 & 0.961 & 0.960 & 0.048 & 0.036 & 0.028 & 0.021 & 0.020 \\
\midrule
\method & \textbf{33.18} & \textbf{35.83} & \textbf{37.59} & \textbf{38.29} & \textbf{0.964} & \textbf{0.977} & \textbf{0.984} & \textbf{0.989} & \underline{0.030} & \textbf{0.013} & \textbf{0.007} & \textbf{0.004} & \textbf{0.008} \\
\method-K-Planes & \underline{33.12} & 35.18 & 36.45 & 36.94 & \underline{0.963} & 0.973 & 0.980 & 0.985 & \textbf{0.028} & \underline{0.014} & 0.009 & \underline{0.005} & \textbf{0.008} \\
\method-TensoRF & 32.94 & \underline{35.34} & \underline{36.92} & \underline{37.46} & 0.959 & \underline{0.974} & \underline{0.982} & \underline{0.987} & 0.033 & \underline{0.014} & \underline{0.008} & \underline{0.005} & \textbf{0.008} \\
\bottomrule
\end{tabular}%
}
\label{table:synthetic-results-downsample}
\end{table}

\begin{table}[H]
\caption{{\bf Real-world results.} We average results across Boat~\cite{adop} and Mip-NeRF 360~\cite{barron2022mipnerf360}. As in \cref{table:synthetic-results-downsample}, all \method\ variants improve significantly upon their baselines.}
\centering
\resizebox{\linewidth}{!}{%
% \footnotesize
\begin{tabular}{l@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}c@{\hspace{1em}}}
\toprule 
& \multicolumn{4}{c}{$\uparrow$PSNR} & \multicolumn{4}{c}{$\uparrow$SSIM} & \multicolumn{4}{c}{$\downarrow$LPIPS} \\ 
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& Full Res. & $\nicefrac{1}{2}$ Res. & $\nicefrac{1}{4}$ Res. & $\nicefrac{1}{8}$ Res.
& Full Res. & $\nicefrac{1}{2}$ Res. & $\nicefrac{1}{4}$ Res. & $\nicefrac{1}{8}$ Res.
& Full Res. & $\nicefrac{1}{2}$ Res. & $\nicefrac{1}{4}$ Res. & $\nicefrac{1}{8}$ Res.
& $\downarrow$Avg Error \\ \midrule
Plenoxels~\cite{yu_and_fridovichkeil2021plenoxels} & 20.69 & 20.70 & 20.98 & 21.93 & 0.627 & 0.543 & 0.547 & 0.640 & 0.661 & 0.607 & 0.525 & 0.364 & 0.128 \\
K-Planes~\cite{kplanes_2023} & 20.53 & 20.55 & 20.84 & 21.85 & 0.618 & 0.525 & 0.512 & 0.602 & 0.655 & 0.587 & 0.488 & 0.328 & 0.128 \\
TensoRF~\cite{Chen2022ECCV} & 17.31 & 17.33 & 17.49 & 17.96 & 0.548 & 0.431 & 0.367 & 0.384 & 0.748 & 0.714 & 0.662 & 0.552 & 0.190 \\
iNGP~\cite{mueller2022instant} & 19.53 & 19.83 & 16.06 & 20.86 & 0.598 & 0.504 & 0.489 & 0.574 & 0.670 & 0.610 & 0.517 & 0.351 & 0.146 \\
Nerfacto~\cite{nerfstudio} & 21.37 & 21.42 & 21.81 & 23.15 & 0.629 & 0.558 & 0.575 & 0.688 & 0.594 & 0.512 & 0.389 & 0.226 & 0.110 \\
Mip-NeRF 360 w/ GLO~\cite{barron2022mipnerf360} & \underline{21.73} & \underline{21.72} & \underline{22.13} & \underline{23.65} & \textbf{0.650} & \textbf{0.597} & \textbf{0.628} & \textbf{0.736} & \textbf{0.518} & \textbf{0.427} & \textbf{0.309} & \textbf{0.165} & \underline{0.100} \\
Mip-NeRF 360 w/o GLO~\cite{barron2022mipnerf360} & 21.01 & 21.00 & 21.39 & 22.88 & 0.634 & 0.580 & 0.610 & 0.718 & 0.529 & 0.441 & 0.323 & 0.179 & 0.111 \\
Exact-NeRF w/ GLO~\cite{isaacmedina2023exactnerf} & 20.72 & 20.73 & 21.04 & 22.34 & 0.637 & 0.571 & 0.583 & 0.674 & 0.559 & 0.478 & 0.378 & 0.237 & 0.121 \\
Exact-NeRF w/o GLO~\cite{isaacmedina2023exactnerf} & 20.98 & 20.97 & 21.34 & 22.80 & 0.635 & 0.578 & 0.604 & 0.710 & 0.548 & 0.451 & 0.339 & 0.192 & 0.113 \\
\midrule
\method & \textbf{22.05} & \textbf{22.16} & \textbf{22.56} & \textbf{23.84} & \underline{0.645} & \underline{0.591} & \underline{0.620} & \underline{0.725} & \underline{0.535} & \underline{0.441} & \underline{0.316} & \underline{0.184} & \textbf{0.098} \\
\method-K-Planes & 21.47 & 21.49 & 21.87 & 23.18 & 0.633 & 0.570 & 0.591 & 0.694 & 0.563 & 0.478 & 0.362 & 0.217 & 0.108 \\
\method-TensoRF & 20.82 & 20.89 & 21.25 & 22.48 & 0.594 & 0.521 & 0.528 & 0.630 & 0.648 & 0.558 & 0.438 & 0.284 & 0.122 \\
\bottomrule
\end{tabular}%
}
\label{table:real-world-results-downsample}
\end{table}

% \section{Videos}

% We include several videos as supplementary materials under the \textbf{vids} folder. We refer reviewers to \textbf{index.html} for descriptions and further details.

% \section{Space Efficiency}

% As mentioned in \cref{sec:limitations}, a downside of our method is an increased serialization footprint due to training a
% hierarchy of spatial grid NeRFs. We explore several mitigations:

% \textbf{Saving memory on coarser levels.} The learned feature grids used in fast NeRF methods usually comprise the bulk of their memory footprint. In the case of iNGP~\cite{mueller2022instant} (which we use as the backbone model in our experiments), the feature grid is stored as a multi-resolution hash table. One possible optimization is to use smaller hash tables for lower-resolution levels. Assume that $S_l$ is the hash table size of the finest-resolution level $l$. By setting $S_{l-1} = S_l/2$, we can reduce the overall storage cost of our pyramid from $lS_l$ (when each level has the same capacity) to less than $2S_l$.

% \textbf{Space-efficient grid implementations.} Storing features in an explicit voxel grid is expensive at high resolutions as the space complexity is $\mathcal{O}(n^3)$ for spatial scenes and $\mathcal{O}(n^4)$ spatio-temporal scenes. iNGP~\cite{mueller2022instant} instead uses hash tables that are smaller that the explicit grids. As an alternative, TensoRF~\cite{Chen2022ECCV} uses CANDECOMP/PARAFAC (CP)~\cite{Carroll1970AnalysisOI} or Vector-Matrix (VM) decomposition to store features in a highly space-efficient manner. We test both decomposition methods as alternatives to our default hash table encoding.

% \textbf{Existing grid hierarchy.} Note that multi-resolution grids such as those used by iNGP~\cite{mueller2022instant} or K-Planes~\cite{kplanes_2023} already define a scale hierarchy that is a natural fit for \method. Rather than learning a separate feature grid for each model in our pyramid, we can reuse the same multi-resolution features across levels (while still training different MLP heads).

% We measure the perceptual quality and memory footprint of these strategies across the datasets defined in \cref{sec:synthetic} and \cref{sec:real-world} and compare to our default \method\ method and the base TensoRF-CP, TensoRF-VM, and iNGP implementations. We summarize our results in \cref{table:space-efficiency}. Using smaller tables for coarser levels reduces the overall model size by over 2× but comes at a 0.5 db cost in PSNR. Sharing the feature grid across levels reduces the overall size even further (by 4×) without a noticeable degradation in quality. Using TensoRF instead of iNGP hash tables also reduces disk space, especially with CP decomposition (300× reduction) although this is offset by a significant decrease in quality. All PyNeRF implementations perform significantly better than their base models.  

% \begin{table}[htbp!]
% \caption{\textbf{Space Efficiency.} Using smaller tables for coarser levels or sharing the feature grid across hierarchy levels both decrease model size, with the latter strategy resulting in no apparent degradation in quality. Using TensoRF as the backbone instead of iNGP also reduces model size but reduces quality significantly. All PyNeRF implementations perform better than the base models. }
% \centering
% \footnotesize
% \begin{tabular}{lccccc}
% \toprule
% Strategy & $\uparrow$PSNR & $\uparrow$SSIM & $\downarrow$LPIPS & $\downarrow$Avg Error & $\downarrow$ Model Size (MB)  \\ \midrule
% \method\ - smaller coarse tables & 29.06 & 0.773 & 0.236 & 0.068 & 410 \\
% \method\ - TensoRF-CP & 22.19 & 0.591 & 0.524 & 0.141 & \underline{3} \\
% \method\ - TensoRF-VM & 23.88 & 0.642 & 0.412 & 0.118	& 531 \\
% \method\ - shared iNGP table & \textbf{29.49} & \underline{0.778} & \textbf{0.230} & \textbf{0.066} & 222 \\
% \midrule
% TensoRF-CP~\cite{Chen2022ECCV} & 21.63 & 0.580 & 0.529 & 0.150 & \textbf{0.4} \\
% TensoRF-VM~\cite{Chen2022ECCV} & 22.81 & 0.588 & 0.461 & 0.132 & 67 \\
% iNGP~\cite{mueller2022instant} & 23.96 & 0.697 & 0.354 & 0.096 & 214 \\
% \method & \textbf{29.49} & \textbf{0.780} & \underline{0.233} & \textbf{0.066} & 912 \\
% \end{tabular}
% \label{table:space-efficiency}
% \end{table}


% Here, we briefly compare our approach to the concurrent related work of ZipNERF \cite{barron2023zipnerf}, which can be seen as a more direct application of mipNERF~\cite{barron2021mipnerf,barron2022mipnerf360} to multiresolution voxel encodings.  We rely on the same insight from~\cite{barron2021mipnerf} that rendered pixels should be computed by integrating over frustrum volumes rather than rays. To accomplish this, we similarly sample local volumes along the frustrum.  To do, such past work computes the integral of the position embedding over a local sampled volume (approximated with a volumetric Gaussian~\cite{barron2021mipnerf,barron2022mipnerf360}) or with additional point samples within that volume~\cite{barron2023zipnerf}. Instead, we leverage the fact that voxel encodings can themselves be seen as ``one-hot" positional embeddings~\cite{yu_and_fridovichkeil2021plenoxels}, implying that {\em multiresolution} voxels can be seen as "many-hot" positional embeddings that can naturally integrate over different spatial regions by selecting the appropriate hierarchy level. Our approach exploits the fact that multiresolution representations are inherently multiscale, inspired by well-known observations for classic multiresolution representations such as Gaussian pyramids~\cite{burt1987laplacian} and mipmap pyramids~\cite{williams1983pyramidal}. Our findings can be seen as somewhat contrary to ZipNERF's claim that "grid-based representations like iNGP do not natively allow for sub-volumes to be queried... which results in learned models that cannot reason about scale or aliasing."  \deva{Likely want to remove, but put in here for fun.}