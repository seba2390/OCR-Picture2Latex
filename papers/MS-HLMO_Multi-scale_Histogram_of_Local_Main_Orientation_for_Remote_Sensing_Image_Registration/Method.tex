\begin{figure*}[!h]
 \centering
 \centerline{\includegraphics[width=17cm]{framework}}
 \caption{The proposed multi-source remote sensing image registration framework of MS-HLMO, including Harris feature point detection, Histogram of Local Main Orientation feature extraction, and multi-scale registration strategy.}
 \label{fig:framework}
\end{figure*}

The framework of the proposed MS-HLMO registration algorithm is shown in Fig.\ref{fig:framework}. The input multi-source image pair to be registered is preprocessed, which includes data normalization and basic denoising. Then, the preprocessed single-band images are used for feature points detection and feature extraction. Harris corner point detection, which contains detail treatments for multi-source images, is adopted to generate feature points between the image pair for matching. The key process of the proposed HLMO feature extraction is carried out in a multi-scale strategy, in which Gaussian pyramids are built to create a scale-space of the images. The HLMO feature descriptors of each Harris corner point are extracted on the PMOM of the images. The feature points between the image pair are then matched according to the descriptors, and Fast Sample Consensus (FSC) is carried out to remove the outliers. The matching results in the scale-space are combined through a multi-scale matching strategy. Finally, the spatial transformation between the original image pair is determined by the coordinate relationship between matched feature points according to a selected transformation model.



\subsection{Harris Feature Point Detection}
\label{ssec:subhead}
Harris corner \cite{harris1988combined} is one of the most stable feature points, which is slightly affected by intensity and scale difference and has high computational efficiency \cite{gao2021multi,2021Multi}. It has the advantage in multi-source remote sensing images with multi-modal properties and large data size. Here, the similar strategy \cite{2021Multi} is used for feature points detection. The Harris corner response of each pixel is calculated by:
\begin{equation}
cornerness = \frac{{{\rm{det}}(\textbf{\emph{M}})}}{{{\rm{tr}}(\textbf{\emph{M}})}}
\end{equation}
\begin{equation}
\textbf{\emph{M}} = \left[ {\begin{array}{*{20}{l}}
{\sum\limits_{{\bf{W}_\sigma}} {{{\bf{G}}_x}^2} }&{\sum\limits_{{\bf{W}_\sigma}} {{{\bf{G}}_x} {{\bf{G}}_y}}}\\
{\sum\limits_{{\bf{W}_\sigma}} {{{\bf{G}}_x} {{\bf{G}}_y}} }&{\sum\limits_{{\bf{W}_\sigma}} {{{\bf{G}}_y}^2}}
\end{array}} \right]
\end{equation}
where $\rm{det}(\textbf{\emph{M}})$ and $\rm{tr}(\textbf{\emph{M}})$ are the determinant and trace of $\textbf{\emph{M}}$, respectively, ${{\bf{G}}_x}$ and ${{\bf{G}}_y}$ are the image's gradient along $x$ and $y$ directions, respectively, and $\bf{W}_\sigma$ is a Gaussian window with variance $\sigma$. Pixels with strong response are considered to be feature points with distinct structure and stability between multi-source images.

An important issue in practical multi-source remote sensing image registration is that the data size and scale relations between images to be registered are diverse. For example, an image with high resolution covers a smaller spatial area. Many of the existing algorithms only deal with the ideal case that the image pair has the same scale and size. This paper focuses on solving several key problems at the same time, that is, two uncertain factors of image scale and size should be considered simultaneously. The proposed MS-HLMO adopts local non-maximum suppression (LNMS) to solve this problem. Since the size and scale difference of the image pairs are uncertain, in Harris corner detection, it is expected that the feature points in the image pair are distributed as uniformly as possible with the use of LNSM. Then, the ratio of the window size in LNMS is set depending on the ratio of the data size of the image pair:
\begin{equation}
ratio = \sqrt {\frac{{M \times N}}{{m \times {\rm{n}}}}}
\end{equation}
where $M,N$ and $m,n$ are the length and width of the two images, respectively.

\begin{figure}[!h]
    \centering
        \subfloat[]{\includegraphics[width=2.5in]{keypoints_1.pdf}
        \label{fig:harris:a}}
    \hfil
        \subfloat[]{\includegraphics[width=2.5in]{keypoints_2.pdf}
        \label{fig:harris:b}}
    \caption{Examples of feature points detection results with LNMS. (a) Detection results of the image pair with different scale. (b) Detection results of the image pair with different scale and size.}
    \label{fig:harris}
\end{figure}

Consequently, the distribution of feature points is consistent with the images' scale proportion when there are scale differences, as shown in Fig.\ref{fig:harris:a}. When there is a size difference, the feature points are far more uniformly distributed, and the repeatability is higher, as shown in Fig.\ref{fig:harris:b}.



\subsection{Histogram of Local Main Orientation}
\label{ssec:subhead}

%\subsubsection{Robust Feature of Gradient Orientation}
%
%Most feature-point-based registration algorithms use a local descriptor to extract the neighborhood information of the keypoints, and generate their feature vectors for similarity matching.  For example, SIFT, SURF, HOG and PIIFD use local gradient information for statistics. However, the performance of these algorithms is greatly reduced when processing multi-source , especially multi-sensor images. Through practice and analysis on , it is believed that multi-modal images will have unpredictable and serious differences in the magnitude of gradient, but the orientation of gradient is a more stable feature.
%
%\begin{figure}[h!]
% \begin{center}
%  \includegraphics[width=3.5in]{Robust_Gradient_Orientation.pdf}
%  \caption{Robust gradient orientation.}
%  \label{fig:res}
% \end{center}
%\end{figure}
%
%To intuitively show the characteristics of this orientation, we briefly summarize the common intensity distortion in multi-modal images, as shown in Fig.0. The four images can be taken as the edge of an object or the interface of two substances in the image. Assume that the center point is the detected feature point, and the image block represents the intensity information of the neighborhood of it. In Fig.0 (a), the intensity amplitude of the left part is lower than that of the right part. Obviously, the gradient orientation is horizontal to the right, which is 0°; Take Fig.0 (a) as the original or reference image, then Fig.0 (b), Fig.0 (c), and Fig.0 (d) is considered as intensity distortions ${{\cal F}_{{\rm{Ra}}}}( \bullet )$ of (a). In (b), the magnitude relationship of the two parts remains unchanged, but the difference between them changes. So the gradient amplitude changes, but the orientation remains 0°; In (c), due to large intensity distortion, the magnitude relationship changes. At this time, the gradient orientation is reversed compared with (a), but notice that it is still on the same line with the orientation in (a). If the gradient orientation is limited to [0°, 180°), the orientation in (c) is still 0°. (d) represents a general situation, which is considered as non-linear distortion of intensity, or some degradation of (a), such as down-sampling or blurring. At this time, the amplitude is hard to determine, but the orientation is still 0° to the right horizontally. A typical multi-source data is shown in Fig.0, which is an optical-infrared image pair. It basically contains the above intensity distortion problems, which is discussed in detail in the next subsection.
%
%In multi-source images, due to the differences in sensors, tempors, environments, etc., various intensity distortions may be caused, resulting in the multi-modal attributes of the image. The morphology of the detailed part of the image is basically in the four cases in Fig.0. In summary, the magnitude of the local gradient varies, but the orientation is basically stable, which defines the feature information that should be focused.


\subsubsection{Partial main orientation map}
Feature-point-based registration algorithms often use a local descriptor to extract the neighborhood information of the keypoints, and generate their feature vectors for similarity matching. For example, SIFT \cite{lowe2004distinctive}, HOG \cite{dalal2005histograms}, SURF \cite{bay2006surf} and PIIFD \cite{chen2010partial} employ gradient information as the basic feature. However, the performance of these algorithms is greatly degraded when processing multi-source, especially multi-sensor images. So, it is critical to extract invariant feature that is robust to ${{\cal F}_{\rm{Int}}}( \bullet )$, ${{\cal F}_{\rm{Rot}}}( \bullet )$, and ${{\cal F}_{\rm{Chg}}}( \bullet )$ for feature points description. A partial main orientation map (PMOM) is designed as the feature map in MS-HLMO, in which the Average Squared Gradient (ASG)\cite{kass1987analyzing} is adopted.

The ASG is a gradient weighting method. The elementary gradient of the image along $x$ and $y$ directions, i.e., ${\bf{G}}_x$ and ${\bf{G}}_y$ are calculated as
\begin{equation} \label{eqPMOM1}
\left[ \begin{array}{l}
{{\bf{G}}_x}(x,y)\\
{{\bf{G}}_y}(x,y)
\end{array} \right] = \left[ \begin{array}{l}
\frac{\partial }{{\partial x}}{\bf{I}}(x,y)\\
\frac{\partial }{{\partial y}}{\bf{I}}(x,y)
\end{array} \right]
\end{equation}
where ${\bf{I}}(x,y)$ represents the single-layer gray-scale image. The magnitude and orientation of its gradient, i.e., ${\bf{G}}_\rho$ and ${\bf{G}}_\varphi$ are
\begin{equation} \label{eqPMOM2}
\left[ \begin{array}{l}
{{\bf{G}}_\rho}\\
{{\bf{G}}_\varphi}
\end{array} \right] = \left[ \begin{array}{l}
\sqrt {{{\bf{G}}_x}^2 + {{\bf{G}}_y}^2} \\
\arctan \frac{{\bf{G}}_y}{{\bf{G}}_x}
\end{array} \right]\end
{equation}

In the ASG, a locally weighted squared gradient \cite{kass1987analyzing} along $x$ and $y$ directions, ${\bf{G}}_{{{\bf{W}}_\sigma},s,x}$ and ${\bf{G}}_{{\bf{W}_\sigma},s,y}$ are obtained as
\begin{equation} \label{eqPMOM3}
\left[ \begin{array}{l}
{{\bf{G}}_{{{\bf{W}}_\sigma},s,x}}\\
{{\bf{G}}_{{{\bf{W}}_\sigma},s,y}}
\end{array} \right] = \left[ \begin{array}{l}
\sum\limits_{{\bf{W}}_\sigma} {{{\bf{G}}_x}^2 - {{\bf{G}}_y}^2} \\
\sum\limits_{{\bf{W}}_\sigma} {2{{\bf{G}}_x}{{\bf{G}}_y}}
\end{array} \right]
\end{equation}
where ${\bf{W}}_\sigma$ is a Gaussian window with variance $\sigma$. Accordingly, the orientation of this gradient is
\begin{equation} \label{eqPMOM4}
{{\bf{G}}_{{{\bf{W}}_\sigma },s,\varphi }} = \angle ({{\bf{G}}_{{{\bf{W}}_\sigma },s,x}},{{\bf{G}}_{{{\bf{W}}_\sigma },s,y}})
\end{equation}
where $\angle (X,Y)$ is defined as
\begin{equation} \label{eqPMOM5}
\angle (X,Y) = \left\{ \begin{array}{l}
\arctan (\frac{Y}{X}), X \ge 0\\
\arctan (\frac{Y}{X}) + \pi , X < 0,Y \ge 0\\
\arctan (\frac{Y}{X}) - \pi , X < 0,Y < 0
\end{array} \right.
\end{equation}
making ${\bf{G}}_{{{\bf{W}}_\sigma },s,\varphi }$ within $(-\pi,\pi)$. According to \cite{kass1987analyzing}, this gradient is obtained by doubling the angle of the original gradient, so the orientation of the ASG is
\begin{equation} \label{eqPMOM6}
{{\bf{G}}_{{{\bf{W}}_\sigma},\varphi }} = \frac{1}{2}{{\bf{G}}_{{{\bf{W}}_\sigma},s,\varphi }}
\end{equation}

Compared with the classical gradient operator, the ASG orientation ${{\bf{G}}_{{{\bf{W}}_\sigma},\varphi}}\in(-\frac{\pi}{2},\frac{\pi}{2})$ reflects the weighted gradient orientation of a local region ${\bf{W}}_\sigma$, which is more robust and stable. In addition, the $x$ direction gradient is constant, and this characteristic meets the requirement that not affected by the reversal of gradient in intensity difference. Note that when $\sigma$ increases, the scale of ASG increases, which makes the local orientation more invariant to intensity difference and noise, but the uniqueness of local features decreases. From this, the following function is defined:
\begin{equation} \label{eqPMOM7}
{{\bf{G}}_{PMOM}} = \frac{1}{2}\angle (\sum\limits_\sigma  {\sum\limits_{{\bf{W}}_\sigma } {{{\bf{G}}_x}^2 - {{\bf{G}}_y}^2}} ,\sum\limits_\sigma  {\sum\limits_{{\bf{W}}_\sigma } {2{{\bf{G}}_x}{{\bf{G}}_y}} } )
\end{equation}
where a series of scale $\sigma$ are preset, the weighted responses ${\bf{G}}_{{{\bf{W}}_\sigma},s,x}$ and ${\bf{G}}_{{{\bf{W}}_\sigma},s,y}$ at each scale are added, and the ASG orientation is obtained. By filtering the image with Eq.(\ref{eqPMOM7}), the PMOM is obtained, where its value reflects the overall orientation of multiple scales in each partial area of the image.

\begin{figure}[h!]
 \begin{center}
  \includegraphics[width=3.5in]{Comparion_of_Feature_Maps.pdf}
  \caption{Comparison of feature maps of a selected scene, including a visible and an infrared image.}
  \label{fig:maps}
 \end{center}
\end{figure}

A visualized comparison of PMOM with other feature maps of typical multi-source data is shown in Fig.\ref{fig:maps}. The original data is a visible-infrared image pair, which contains obvious intensity difference. For comparison purposes, the images have been registered manually, basically eliminating the spatial differences of scale, rotation, and size. The magnitude and orientation of images' gradient are shown in Fig.\ref{fig:maps}, which are obtained using Eqs.(\ref{eqPMOM1})-(\ref{eqPMOM2}). These are the basic feature information in most algorithms \cite{lowe2004distinctive,dalal2005histograms,bay2006surf,chen2010partial,2021Multi}. It is observed that, due to the multi-modal properties of the original image pair, these two feature maps have large differences and instability, which is the main reason for the failure of most algorithms. The MIM \cite{li2019rift} of RIFT shown in Fig.\ref{fig:maps} also focuses on the local orientation of the image, where the maximum index is the main orientation among several ones. Compared with the directional gradient, Gabor transformation has a more stable response, which leads to RIFT robust to intensity difference. However, its value will also mutate due to local changes in images, and the rotation invariance is slightly poor. The image pair's PMOMs are shown at the bottom of Fig.\ref{fig:maps}. Compared with MIM, the proposed PMOM is not only more robust and stable between multi-modal images, but also continuous in value, which is conducive to achieving effective rotation invariance. In HLMO, PMOM is used as the unique feature information to extract local features of keypoints that are invariant to multi-modal properties.


\subsubsection{Descriptor extraction}

After determining the feature points and the specific feature for discrimination, the next step is to make use of the local feature information around each point and generate descriptors. Gradient Location and Orientation Histogram (GLOH) has shown excellent ability through experiments \cite{mikolajczyk2005performance}, and has been applied in multi-source remote sensing image registration \cite{dellinger2014sar,ma2016remote}. The original GLOH descriptor is a circular region divided by three circles, similar to that shown in Fig.\ref{fig:des180}, in which the two outer circular regions are divided into 4 parts, and the radius of the circular region divided are 5, 9, and 11. The partition size and the number are then improved \cite{mikolajczyk2005performance,dellinger2014sar,ma2016remote}. However, different parameters have various effects when treating multifarious types of images. In addition, if the number of outer ring regions is too small, the character of feature points is not significant, which makes it difficult to match accurately. If it is too large, the features are unstable, and the dimension of the descriptor is too high, which increases the burden of redundant calculation. To deal with this, a generalized GLOH-like (GGLOH) descriptor is proposed, and its structure of which is shown in Fig.\ref{fig:des:a}.

\begin{figure}[!h]
    \centering
        \subfloat[]{\includegraphics[width=2.2in]{Descriptor_Structure_a.png}%
        \label{fig:des:a}}
    \hfil
        \subfloat[]{\includegraphics[width=1.1in]{Descriptor_Structure_b.png}%
        \label{fig:des:b}}
    \caption{Descriptor structure of the proposed GGLOH. (a) Subregion partition of the local neighborhood of a feature point. (b) Angle quantification within each subregion.}
    \label{fig:des}
\end{figure}

Let ${{\rm{A}}^0}$ denote the central circular region, and ${\rm{A}}_j^i,(i=1,2, j=1,...,{N_{\rm{A}}})$ represent the sector subregion $j$ in the outer ring region $i$. Let ${N_{\rm{A}}}$ be the number of the subregions in each out ring region, which is even, ${\theta _{\rm{0}}}$ be the main orientation of the feature point, and $R_0$, $R_1$, $R_2$ be the radii of the central and outer regions, respectively. Note that the orientations of pixels’ gradient in each region are counted as feature information, therefore, fair use of information in each region is expected. The number of pixels in each region should be roughly the same, and the weight of the outer regions should not change due to the change of ${N_{\rm{A}}}$. So the area of each region should be the same, that is
\begin{equation} \label{eqggloh}
{N_{\rm{A}}} \cdot \pi {\rm{R}}_{\rm{0}}^2 = \pi ({\rm{R}}_{\rm{1}}^2 - {\rm{R}}_{\rm{0}}^2) = \pi ({\rm{R}}_{\rm{2}}^2 - {\rm{R}}_{\rm{1}}^2)
\end{equation}
which also fixes the relationship between $R_0$, $R_1$, $R_2$ and ${N_{\rm{A}}}$. When ${N_{\rm{A}}}$ is given different values, the stability and importance of each region’s feature remains the same. In HLMO, the GGLOH is used to extract local features on the PMOM, where the orientation values within $(-\frac{\pi}{2},\frac{\pi}{2})$ are uniformly quantified to ${N_{\rm{O}}}$ values, as shown in Fig.\ref{fig:des:b}, where ${\phi_k}(k=1,2,...,{N_{\rm{O}}})$ is the angle after quantization. A histogram with ${N_{\rm{O}}}$ bins is obtained in each region.

It is simple to achieve rotation invariance of HLMO. For each keypoint, the PMOM value at its location is the main orientation, that is, the reference orientation ${\theta _0}$ of the GGLOH. Then, all of the PMOM values within the local area of GGLOH also take ${\theta _0}$ as the reference (0°), that is, all angle values minus ${\theta _0}$, and those beyond $(-\frac{\pi}{2},\frac{\pi}{2})$ are flipped to their opposite angles.

\begin{figure}[h!]
 \begin{center}
  \includegraphics[width=3.3in]{Problem_of_180.pdf}
  \caption{The problem caused by the jump of main orientation near $-\frac{\pi}{2}$ or $\frac{\pi}{2}$.}
  \label{fig:des180}
 \end{center}
\end{figure}

Another key problem is that the rotation and nonlinear intensity difference may cause the jump of the main orientations of some feature points near $-\frac{\pi}{2}$ and $\frac{\pi}{2}$. An example is shown in Fig.\ref{fig:des180}. In PIIFD \cite{chen2010partial}, a similar problem has been discovered and improvement has been made for SIFT. Then a similar strategy is adopted to process GLOH-like descriptors,

\begin{equation}
{{\bf{D}}_1} = \left[ {\begin{array}{*{20}{c}}
{\begin{array}{*{20}{c}}
{{\bf{H}}_1^1}&{{\bf{H}}_2^1}& \cdots &{{\bf{H}}_{{{{N_{\rm{A}}}} \mathord{\left/
 {\vphantom {{{N_{\rm{A}}}} 2}} \right.
 \kern-\nulldelimiterspace} 2}}^1}
\end{array}}\\
{\begin{array}{*{20}{c}}
{{\bf{H}}_1^2}&{{\bf{H}}_2^2}& \cdots &{{\bf{H}}_{{{{N_{\rm{A}}}} \mathord{\left/
 {\vphantom {{{N_{\rm{A}}}} 2}} \right.
 \kern-\nulldelimiterspace} 2}}^2}
\end{array}}
\end{array}} \right]
\end{equation}

\begin{equation}
{{\bf{D}}_2} = \left[ {\begin{array}{*{20}{c}}
{\begin{array}{*{20}{c}}
{{\bf{H}}_{{{{N_{\rm{A}}}} \mathord{\left/
 {\vphantom {{{N_{\rm{A}}}} 2}} \right.
 \kern-\nulldelimiterspace} 2} + 1}^1}&{{\bf{H}}_{{{{N_{\rm{A}}}} \mathord{\left/
 {\vphantom {{{N_{\rm{A}}}} 2}} \right.
 \kern-\nulldelimiterspace} 2} + 2}^1}& \cdots &{{\bf{H}}_{{N_{\rm{A}}}}^1}
\end{array}}\\
{\begin{array}{*{20}{c}}
{{\bf{H}}_{{{{N_{\rm{A}}}} \mathord{\left/
 {\vphantom {{{N_{\rm{A}}}} 2}} \right.
 \kern-\nulldelimiterspace} 2} + 1}^2}&{{\bf{H}}_{{{{N_{\rm{A}}}} \mathord{\left/
 {\vphantom {{{N_{\rm{A}}}} 2}} \right.
 \kern-\nulldelimiterspace} 2} + 2}^2}& \cdots &{{\bf{H}}_{{N_{\rm{A}}}}^2}
\end{array}}
\end{array}} \right]
\end{equation}

\begin{equation} \label{eqdes}
{\bf{D}} = \left[ {\begin{array}{*{20}{c}}
{{{\bf{D}}_1} + {{\bf{D}}_2}}\\
{c\left| {{{\bf{D}}_1} - {{\bf{D}}_2}} \right|}
\end{array}} \right]
\end{equation}
where ${\bf{H}}_j^i$ is the histogram vector of gradient orientation of region ${\rm{A}}_j^i$. In this way, no matter whether the main orientation of feature points is reversed 180° or not, descriptor ${\bf{D}}$ is composed of the addition and subtraction of the upper and lower parts of GGLOH according to the main orientation axis, without changing the regions' order. Finally, a descriptor vector ${\bf{D}}_P$ is generated for the feature point $P$, whose dimension is $(2 \cdot {N_{\rm{A}}}+1) \cdot {N_{\rm{O}}}$.

%\subsection{Advanced Outlier Removal}
%\label{ssec:subhead}


\subsection{Multi-scale Registration Strategy}
\label{ssec:subhead}
Scale difference ${{\cal F}_{{\rm{Sc}}}}( \bullet )$ of multi-source images has a great influence on local features. Some algorithms have quantitative scale judging methods, such as SIFT \cite{lowe2004distinctive} and LHOPC. However, it is found that these methods are invalid in images with large modal differences. The reason is that when images do not belong to the same degradation model, it is not credible to judge the scale quantitatively through local image feature information. Multi-source images often have scale differences, and sometimes the scale proportion is unknown. In order to deal with this key problem and realize scale robustness, a multi-scale feature extraction and matching strategy is designed in MS-HLMO.
%由于传感器成像能力或成像条件的差异，图像显示出了尺度的差异。这种尺度差异往往体现在两个方面，一个是分辨率，一个是模糊程度。其中分辨率是指图像中一个像素对应实际空间的尺寸，另外一个是指

\begin{figure}[h!]
 \begin{center}
  \includegraphics[width=2.5in]{Pyramid.pdf}
  \caption{Structure of the Gaussian pyramid used in MS-HLMO.}
  \label{fig:pyramid}
 \end{center}
\end{figure}

Local information of feature points is extracted in the scale-space of the images. Based on the scale-space theory \cite{lindeberg1994scale}, the method of building image's Gaussian pyramids is adopted. The schematic diagram of establishing Gaussian pyramid of the image in the proposed algorithm is shown in Fig.\ref{fig:pyramid}. The original image is first sampled down step by step to obtain a series of images with different resolutions, that is, the first layer in each octave. Then in each octave, a series of Gaussian blurs are performed:
\begin{equation}\label{eqGauss1}
{\bf{L}} = {\bf{G}} * {\bf{I}}
\end{equation}
\begin{equation}\label{eqGauss2}
{\bf{G}} = \frac{1}{{\sqrt {2\pi {\sigma ^2}} }}{e^{\frac{{-({x^2} + {y^2})}}{{2{\sigma ^2}}}}}
\end{equation}
where ${\bf{I}}$ is the original image, ${\bf{G}}$ is a Gaussian kernel with a standard deviation of $\sigma$, and ${\bf{L}}$ is the Gaussian blur image.
%高斯尺度空间模拟出了图像尺度变化的效果,为获取图像多个尺度层面的特征提供了重要帮助。

\begin{algorithm}[htp]
\caption{\label{multi1} \footnotesize Proposed MS-HLMO Feature Extraction}
\begin{algorithmic}
\footnotesize
\STATE {\bf Input:} single-band image $\mathbf{I}$, feature point set $P_{\mathbf{I}}$, total number of octaves ${N_{\rm{GO}}}$ and layers ${N_{\rm{GL}}}$ in Gaussian pyramid, subregion and angle partition parameters ${N_{\rm{A}}}$, ${N_{\rm{O}}}$ in GGLOH , patch size $S$ of HLMO.
\STATE Through down-sampling and Eq.(\ref{eqGauss1})(\ref{eqGauss2}), the Gaussian pyramid ${\bf{G}}_{\bf{I}}(O,L)$ of image $\mathbf{I}$ is built with ${N_{\rm{GO}}}$ octaves and ${N_{\rm{GL}}}$ layers in each octave.
\STATE In each layer of ${\bf{G}}_{\bf{I}}(O,L)$:
\STATE \hspace*{0.1in}Calculate the PMOM of this layer to get ${\bf{F}}_{\bf{I}}(O,L)$ according to Eq.(\ref{eqPMOM1})(\ref{eqPMOM7})
\STATE \hspace*{0.1in}For each feature point $P$ in $P_{\bf{I}}$:
\STATE \hspace*{0.2in}Calculate the corresponding position
\STATE \hspace*{0.2in}Take the PMOM value at the position as the main orientation ${\theta_0}$
\STATE \hspace*{0.2in}Taking the main orientation as the reference direction ($0^{\circ}$), establish a GGLOH window with size (diameter) of S
\STATE \hspace*{0.2in}Statistics the PMOM value within each region of GGLOH to obtain the basic feature descriptor $D_{1}(P,O,L)$ and $D_{2}(P,O,L)$
\STATE \hspace*{0.2in}Obtain the descriptor $D(P,O,L)$ of $P$ with Eq.\ref{eqdes}.
\STATE {\bf Output:} feature descriptor set $D_{P_{\bf{I}}}(O,L)$
\end{algorithmic}
\end{algorithm}

After the scale-space of the images is established, for each Harris corner point, the HLMO descriptor is calculated by obtaining the local information at the corresponding location of each feature point in the scale-space. The proposed multi-scale HLMO feature extraction method is provided in Algorithm 1, where $O$ is the octave number in the Gaussian pyramid, and $L$ is the layer number. The algorithm outputs the feature point descriptor set $D_{P_{\bf{I}}}(O,L)$, which contains $(2 \cdot {N_{\rm{A}}}+1) \cdot {N_{\rm{O}}}$-dimensional vectors for each feature point at each scale.

The next step is to match the feature point sets of the image pair according to the descriptor sets. The process of the multi-scale feature matching is provided in Algorithm 2. In the process, each scale is matched in turn. Then the matching results are merged step by step while the outlier removal is carried out to realize the optimization of matching points. The final matching results are used to determine the spatial transformation between images. The most critical is to combine all the matching results of feature points and remove outliers, so as to maximize the correct matches of all scales. Fig.\ref{fig:pyramids} shows this process visually. Obviously, this is a general approach to handle all kinds of unknown scale differences. When the scale proportion of images is known or can be estimated, then the above process can be greatly simplified. In this case, the proposed multi-scale strategy still has the advantages of enhancing feature matching and maximizing the number of matching points.

\begin{algorithm}[htp]
\caption{\label{multi2} \footnotesize Proposed MS-HLMO Feature Matching}
\begin{algorithmic}
\footnotesize
\STATE {\bf Input:} feature point set of the image pair $P_{{\bf{I}}1}$, $P_{{\bf{I}}2}$, feature descriptor set of the image pair $D_{P_{{\bf{I}}1}}(O_{1},L_{1})$, $D_{P_{{\bf{I}}2}}(O_{2},L_{2})$
\STATE Take each layer of $D_{P_{{\bf{I}}1}}(O_{1},L_{1})$:
\STATE \hspace*{0.1in}Take each layer of $D_{P_{{\bf{I}}2}}(O_{2},L_{2})$:
\STATE \hspace*{0.2in}Match $P_{{\bf{I}}1}$ and $P_{{\bf{I}}2}$ using Euclidean distance of the descriptorss
\STATE \hspace*{0.2in}Remove outliers, producing the matching result of a single scale $M(O_{1},O_{2},L_{1},L_{2})$
\STATE The matching results of all layers in each octave of the scale-space are union and then optimized with outlier removal, producing the matching result $M_{L}(O_{1},O_{2})$
\STATE The matching results of all octaves in $M_{L}(O_{1},O_{2})$ are union and then optimized with outlier removal, producing the final matching result $M_{OL}(P_{{\bf{I}}1},P_{{\bf{I}}2})$
\STATE {\bf Output:} feature points matching set $M_{OL}(P_{{\bf{I}}1},P_{{\bf{I}}2})$
\end{algorithmic}
\end{algorithm}

\begin{figure}[h!]
 \begin{center}
  \includegraphics[width=3.5in]{MS_Matching.pdf}
  \caption{Multi-scale keypoints matching strategy in MS-HLMO.}
  \label{fig:pyramids}
 \end{center}
\end{figure}