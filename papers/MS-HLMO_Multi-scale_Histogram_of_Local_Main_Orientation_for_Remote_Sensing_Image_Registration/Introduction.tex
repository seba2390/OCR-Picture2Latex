Image registration is a key preprocessing step in remote sensing applications. With earth observation systems being developed rapidly in
recent years, to achieve in-depth ground analysis, the use of multi-source remote sensing images becomes popular in achieving in-depth ground analysis \cite{zhang2018feature,zhao2020joint,liu2020joint,zhang2020transfer,gao2021hyperspectral}. The accurate alignment of the multi-source images is a prerequisite of subsequent applications such as data fusion, change detection, joint analysis, and other techniques.

The purpose of image registration is to spatially align multiple images for the same scene. In general, remote sensing images are spatially corrected by reference coordinates (such as fictitious graticule) or image control points. However, there are inconsistent or missing spatial references. In particular, in multi-source remote sensing, the inconsistency of spatial correction between different sources is more obvious. This phenomenon can be seen from the examples shown in Fig.\ref{fig:fail}, which is about the correction of the multi-sensor image solely through the spatial reference information provided by the data source. It is obvious that the ground covers correspond inconsistently. In addition, for some UAV or ground-based images, there is no spatial reference information. It is very important to correct multi-source remote sensing images automatically by registration algorithms in order to provide accurately registered data for the same scene space.

\begin{figure}[h!]
 \begin{center}
  \includegraphics[width=3.0in]{Failed_Example.pdf}
  \caption{Examples of alignment failure of multi-source remote sensing images with only calibration information.}
  \label{fig:fail}
 \end{center}
\end{figure}

Existing automatic image registration algorithms are generally divided into two categories, i.e., the traditional methods and the deep learning-based methods. Among them, the deep learning-based methods \cite{litjens2017survey} are relatively advanced, but a large amount of annotated samples are needed for model training, and the model is often highly targeted. The traditional ones are systematically classified into area-based and feature-based methods \cite{zitova2003image}. The area-based, also called intensity-based methods
register images by establishing the similarity measure of the intensity values or in a transform domain \cite{pratt1974correlation,de1987registration,viola1997alignment,le2002automated,pluim2003mutual,mahmood2011correlation,oliveira2014medical,tong2015novel}, and optimal geometric transformation parameters between the image pairs are found through optimization \cite{zhang2021multimodal}. Apart from efficiency and other factors, for multi-source images, this type algorithm is prone to fail when multi-modal differences are present.

Registration based on feature matching is a relatively mature technique, in which the transformation parameters are estimated through the coordinate correspondence of matched features. Scale-Invariant Feature Transform (SIFT) \cite{lowe1999object,lowe2001local,lowe2004distinctive} is one of the most classic, effective, and commonly used feature extraction and matching methods, and became the basis of many improved algorithms \cite{ke2004pca,mikolajczyk2005performance,bay2006surf,morel2009asift,sedaghat2011uniform,dellinger2014sar,sedaghat2015remote}. SIFT uses Gaussian scale-space, supported by scale-space theory \cite{lindeberg1994scale}, for feature points extraction, which also provides scale invariance. The feature points are then described by statistical local gradient information for points matching. This also lays the framework for the subsequent algorithms. In addition, many feature extraction algorithms have been proposed and applied to image registration \cite{moravec1980obstacle,harris1988combined,dalal2005histograms,rosten2008faster,calonder2011brief,rublee2011orb,leutenegger2011brisk,alahi2012freak,alcantarilla2012kaze,ye2019fast}.

However, most algorithms are only applicable to single-modal image registration. Algorithms for multi-modal image registration are more critical. Focusing on this, multi-modal image registration algorithms have been proposed. Chen et al. \cite{chen2010partial} proposed Partial Intensity Invariant Feature Descriptor (PIIFD) based on SIFT algorithm for multi-source retinal image registration, which overcame the problem of intensity difference problems. PSO-SIFT \cite{ma2016remote} presented a new gradient definition and an enhanced feature matching method for the registration of remote sensing images with intensity differences. Ye et al. developed Histogram of Orientated Phase Congruency (HOPC) \cite{ye2017robust} and Local HOPC (LHOPC) \cite{ye2018local} for multi-modal remote sensing registration, in which Minimum Moment of Phase Congruency (MMPC)-Lap is used for keypoints detection and an extended phase congruency model is used for feature description. Radiation-variation insensitive feature transform (RIFT) \cite{li2019rift} utilized a maximum index map (MIM) as the feature map, which is invariant to intensity diffeerence. The MIM is obtained by assigning the strongest response of the log-Gabor filtering at several predetermined orientations as the maximum index at each pixel. Multi-scale PIIFD (MS-PIIFD) \cite{2021Multi} improved PIIFD by adopting Gaussian scale-space, in which feature extraction and matching are completed in a multi-scale strategy. It achieves scale robustness and performs well on multi-modal images with scale differences.

The multi-modal differences of multi-source images result in multiple inconsistency of local features in the same area of image pairs, which makes it difficult to match correctly unless all inconsistencies are conquered. It is needed to handle the differences and enhance the similarity of the features. Therefore, through deep-going analysis of practical work and real data, we refine and model the differences of multi-source remote sensing images as
\begin{equation}
{{\bf{I}}_2} = {{\cal F}_{{\rm{Cut}}}}({{\cal F}_{{\rm{Tran}}}}({{\cal F}_{{\rm{Sc}}}}({{\cal F}_{{\rm{Rot}}}}({{\cal F}_{{\rm{Int}}}}({{\cal F}_{{\rm{Chg}}}}({{\bf{I}}_1}))))))
\end{equation}
where ${{\bf{I}}_1}$ and ${{\bf{I}}_2}$ are a multi-source image pair, ${{\cal F}_{{\rm{Chg}}}}( \bullet )$ represents the changes of the real ground object content sensed in the images, which is often caused by multi-temporal sensing.
${{\cal F}_{\rm{Int}}}( \bullet )$ denotes intensity transformation.
${{\cal F}_{{\rm{Rot}}}}( \bullet )$, ${{\cal F}_{{\rm{Sc}}}}( \bullet )$, and ${{\cal F}_{{\rm{Tran}}}}( \bullet )$ are spatial transformation of rotation, scale change, and translation, respectively, which are the basic operation of similarity transformation.
${{\cal F}_{{\rm{Cut}}}}( \bullet )$ represents the spatial cutting operation, which results in images covering different ranges of vision.
There may also be large perspective differences or severe local geometric distortions between images, which leads to more complex image distortion and registration modeling. Our goal is to first resolve the most common and fundamental problem of image differences. Among the above multi-modal differences, ${{\cal F}_{{\rm{Cut}}}}( \bullet )$, ${{\cal F}_{{\rm{Tran}}}}$, and ${{\cal F}_{{\rm{Chg}}}}( \bullet )$ have been solved by feature-based process. Thus, the primary goal is to find a feature robust to ${{\cal F}_{{\rm{Sc}}}}( \bullet )$, ${{\cal F}_{{\rm{Rot}}}}( \bullet )$, and ${{\cal F}_{\rm{Int}}}( \bullet )$ in multi-source images.

Based on the above analysis, a novel feature-based registration framework for multi-source remote sensing images is proposed through designing effective strategies for robust feature extraction that overcomes multiple image differences. It is able to automatically register remote sensing images under various conditions without human intervention. The main contributions are summarized as follows:

\begin{enumerate}
\item{An image registration algorithm named Multi-scale Histogram of Local Main Orientation (MS-HLMO) is designed to cope with various multi-source remote sensing images with multi-modal differences. Through comprehensive experimental verification, MS-HLMO is able to effectively deal with common multi-source remote sensing image registration, including multi-sensor, multi-temporal, and multi-viewpoint image pairs with different resolutions and data size, which reflects strong robustness and generalization.}

\item{A local feature named Histogram of Local Main Orientation (HLMO) is proposed, in which a basic feature map called Partial Main Orientation Map (PMOM) is developed for local feature extraction, which handles multi-modal differences and provides robust orientation information. In addition, HLMO utilizes a generalized GLOH-like (GGLOH) feature descriptor to extract local features. The proposed HLMO is invariant to intensity and rotation, and has outstanding ability in robust feature extraction from multi-source images.}

\item{To overcome the problem of scale differences in multi-source images, a multi-scale feature extraction and matching strategy based on Gaussian scale-space is applied in MS-HLMO, which has a good effect on solving scale differences and optimizing feature matching. MS-HLMO has a complete processing flow without manual intervention and is able to directly applied to the practical multi-source remote sensing joint analysis. To further research and assessment the proposed method, the code will be released on https://github.com/MrPingQi.}
\end{enumerate}

The rest of this paper is arranged as follows. Section II describes the overall framework of the proposed MS-HLMO and each critical process in detail. Section III presents experiments and discussion in terms of discrete and overall testing. Finally, this research is summarized in Section IV. 