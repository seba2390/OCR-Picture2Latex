To validate the registration performance of the proposed framework, a set of typical multi-source remote sensing images are carefully selected, and six effective registration algorithms are used for comparison, including SIFT \cite{lowe2004distinctive}, SAR-SIFT \cite{ma2016remote}, PSO-SIFT \cite{ma2016remote}, PIIFD \cite{chen2010partial}, MS-PIIFD \cite{2021Multi}, and RIFT \cite{li2019rift}, all of which contain processes dealing with multi-modal properties. The experiments are implemented using MATLAB2021a on a platform with AMD-Ryzen-7-5800X 3.80GHz CPU, 32GB RAM.



\subsection{Experimental Data and Preprocessing}
\label{ssec:data}
The data used in experiments are multi-source remote sensing images of 17 scenes, labeled a$\sim$q. These images include hyperspectral image (HSI), multispectral (MSI), synthetic aperture radar (SAR), visible images, infrared images, depth maps such as LiDAR-derived DSM (digital surface model), and even artificial maps. They contain day-night image pair and images from different years. The scenes cover spaceborne, airborne, and ground-based remote sensing data. There are various intensity and geometric distortions with the images. Table \ref{tbl:data} shows the types and sizes of each set of data, as well as whether they have significant intensity difference, rotation, or scale difference. The specific form of the scenes is shown in Fig.\ref{fig:matching} and \ref{fig:checker}. Except for artificial maps, these are all real data without manual manipulation.

\begin{table}[h!]
 \centering
 \setlength{\tabcolsep}{1.2mm}
 \caption{Multi-source remote sensing data of 17 scenes.}
 \label{tbl:data}
 \begin{tabular}{ccccccc}
  \toprule
   \multicolumn{2}{c}{\multirow{2}{*}{Scene}} & \multirow{2}{*}{Type} & \multirow{2}{*}{Size} & Intensity  & \multirow{2}{*}{Rotation} & Scale \\
                      &                       &                       &                       & Difference &                           & Difference \\
  \midrule
   \multirow{6}{*}{a} & \multirow{3}{*}{a1} &      MSI & 3555$\times$4026 & \multirow{3}{*}{$\checkmark$} &  & \multirow{3}{*}{$\checkmark$} \\
                      &                     &      HSI & 1185$\times$1342 &  &  &  \\
                      &                     &      SAR & 3555$\times$4026 &  &  &  \\
          \cline{2-7} & \multirow{3}{*}{a2} &      MSI & 1287$\times$2035 & \multirow{3}{*}{$\checkmark$} &  & \multirow{3}{*}{$\checkmark$} \\
                      &                     &      HSI &  422$\times$678  &  &  &  \\
                      &                     &      SAR & 1366$\times$887  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{b}}   &      MSI & 3529$\times$1756 & \multirow{2}{*}{$\checkmark$} &  & \multirow{2}{*}{$\checkmark$} \\
                      &                     &      HSI & 1175$\times$585  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{c}}   &      MSI & 2461$\times$4139 & \multirow{2}{*}{$\checkmark$} &  & \multirow{2}{*}{$\checkmark$} \\
                      &                     &      MSI & 2271$\times$3152 &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{d}}   &      PAN & 1000$\times$1000 & \multirow{2}{*}{$\checkmark$} &  & \multirow{2}{*}{$\checkmark$} \\
                      &                     &      HSI &  550$\times$450  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{e}}   &  visible &  614$\times$614  & \multirow{2}{*}{$\checkmark$} & \multirow{2}{*}{$\checkmark$} &  \\
                      &                     & infrared &  611$\times$611  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{f}}   &  visible &  617$\times$593  & \multirow{2}{*}{$\checkmark$} & \multirow{2}{*}{$\checkmark$} &  \\
                      &                     & infrared &  617$\times$593  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{g}}   &  visible & 1920$\times$1080 & \multirow{2}{*}{$\checkmark$} &  & \multirow{2}{*}{$\checkmark$} \\
                      &                     & infrared &  667$\times$504  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{h}}   &  visible & 4056$\times$3040 & \multirow{2}{*}{$\checkmark$} &  & \multirow{2}{*}{$\checkmark$} \\
                      &                     & infrared &  640$\times$512  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{i}}   &  visible &  633$\times$460  & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     & infrared &  670$\times$508  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{j}}   &      HSI &  349$\times$1905 & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     &    LiDAR &  349$\times$1905 &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{k}}   &      HSI &  166$\times$600  & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     &    LiDAR &  166$\times$600  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{l}}   &      HSI &  325$\times$220  & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     &    LiDAR &  325$\times$220  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{m}}   &  visible &  500$\times$500  & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     &    depth &  500$\times$500  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{n}}   &  visible &  500$\times$500  & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     &      map &  500$\times$500  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{o}}   &  visible &  500$\times$500  & \multirow{2}{*}{$\checkmark$} &  &  \\
                      &                     &  visible &  500$\times$500  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{p}}   &      SAR &  600$\times$500  & \multirow{2}{*}{$\checkmark$} & \multirow{2}{*}{$\checkmark$} &  \\
                      &                     &      SAR &  600$\times$500  &  &  &  \\ \cline{1-7}
   \multicolumn{2}{c}{\multirow{2}{*}{q}}   &  visible &  300$\times$300  &  & \multirow{2}{*}{$\checkmark$} &  \\
                      &                     &  visible &  300$\times$300  &  &  &  \\
  \bottomrule
 \end{tabular}
\end{table}

As MS-HLMO and other methods only focus on spatial rather than waveband features, while some types of images have multiple wavebands, such as MSI, HSI, or LiDAR, single-band gray-scale images need to be obtained from the original multi-band ones. It is acceptable to use data dimension reduction, such as Principal Component Analysis (PCA), to obtain single-band images. However, in multi-source registration, the images may have multi-modal properties. Due to its enhancing pixel-level features, data dimension reduction could break the stable and unified spatial structures of each type of ground covers in the image, and increase the structural differences between images. Then, registration would be more difficult as ${{\cal F}_{{\rm{Chg}}}}(\bullet)$ is exacerbated, which is undesired. Note that HLMO is not subject to the changes in image's grayscale, so it is optional to select any one of the bands as the input, or just simply add up all the bands, which has little effect on the registration result.

\subsection{Experiment Settings}
\label{ssec:para}
The proposed MS-HLMO contains seven main parameters that greatly affect the registration, which are the number of feature points, the scale of PMOM, the $R_2$, ${N_{\rm{A}}}$, and ${N_{\rm{O}}}$ of GGLOH, and ${N_{\rm{GO}}}$ and ${N_{\rm{GL}}}$ of the Gaussian pyramid. In general, the lower the number of subregions ${N_{\rm{A}}}$ and angles ${N_{\rm{O}}}$ are, the higher the robustness of features is, but the lower the separability of the feature points is. Conversely, the more distinct the features are, but the lower the stability is, which also increases the computational burden. In the experiment, both ${N_{\rm{A}}}$ and ${N_{\rm{O}}}$ are set to 12, and $R_2$ is set to 48, which has the best performance and stability in the majority of multi-source data. Then, $R_1$ and $R_0$ in GGLOH are determined by Eq.\ref{eqggloh}, and the patchsize of HLMO is $2\times R_2 = 96$. Through a simple set of tests, the threshold of the Harris corner points' number is set at 2000, and ${N_{\rm{GO}}}$ and ${N_{\rm{GL}}}$ are set to 3 and 4, which handles the registration effectively enough with high efficiency.

In the calculation of PMOM, 10 scales are set for fusion, of which the radiuses are evenly spaced between $R_0$ and $R_2$, and the $\sigma$ is 1/3 of each radius. In this way, when GGLOH is applied to a feature point, the main orientation of the patch, that is the PMOM value of the center point, is only determined by the pixels' values within the patch, excluding information beyond the GGLOH region. 10 scales are sufficient to fully obtain the local multi-scale weighted main orientation information in various multi-source images, which balances the robustness and uniqueness with a low computational burden.
% PMOM的计算设置为每一级尺度的ASG加权半径是从$R_0$到$R_2$之间等间隔的10个量级，高斯加权标准差σ为半径的三分之一。这样在使用GGLOH作用于某一特征点时，该patch的主方向，即中心点的PMOM值，均是由该patch内的像素值决定的，不包含超过GGLOH区域范围的信息。 10个尺度足够充分获取各种多源图像中每一个位置的局部多尺度加权主方向信息，平衡出较好的鲁棒性和独特性，同时计算负担不高。

The number of correct matches (NCM) is employed as the main evaluation metric of feature matching. The correct matches refer to the matched points that are basically at the same position in the real space, and the NCM is positively correlated with the accuracy of the transformation model. Note that if the NCM of an image pair is less than 3, the parameters of the spatial transformation model cannot be solved, then the registration process is considered to fail.

To ensure the fairness of the comparison, except for the feature point detection and descriptor extraction methods, all the processes are the same, including preprocessing, feature point matching, outlier removal, transformation, etc. The feature points used in PIIFD, MS-PIIFD, and RIFT are the same as the proposed algorithm with the strategy in \cite{2021Multi}. Different algorithms use various feature matching and outlier removal methods in their original process, which interferes with the comparison of registration effects. In this experiment, Euclidean distance measurement is used for all feature matching, and FSC is used for outlier removal for a fair comparison. As FSC has random property, the result fluctuates in the experiment, which is not very large but does have an impact. So each test is repeated 50 times and the result with the most NCM is taken.



\begin{figure*}[!h]
    \centering
        \subfloat[]{\label{fig:rot:a}
        \includegraphics[height=0.8in]{rotation_a1.jpg}}
        \subfloat[]{\label{fig:rot:b}
        \includegraphics[height=0.8in]{rotation_a2.jpg}}
        \subfloat[]{\label{fig:rot:c}
        \includegraphics[height=0.8in]{rotation_a4.jpg}}
        \subfloat[]{\label{fig:rot:d}
        \includegraphics[height=0.8in]{rotation_d.jpg}}
    \hfil
        \subfloat[]{\label{fig:rot:e}
        \includegraphics[height=0.8in]{rotation_g.jpg}}
        \subfloat[]{\label{fig:rot:f}
        \includegraphics[height=0.8in]{rotation_j.jpg}}
        \subfloat[]{\label{fig:rot:g}
        \includegraphics[height=0.8in]{rotation_m.jpg}}
        \subfloat[]{\label{fig:rot:h}
        \includegraphics[height=0.8in]{rotation_p.jpg}}
    \caption{NCMs of MS-HLMO on different types of multi-source remote sensing scenes as the rotation angles from 0° to 359°. (a) MSI-HSI. (b) MSI-SAR. (c) MSI-map. (d) PAN-HSI. (e) visible-infrared. (f) HSI-LiDAR. (g) visible-depth. (h) SAR-SAR.}
    \label{fig:rot}
\end{figure*}

\subsection{Invariance Tests}

The crux of multi-source image registration based on feature matching lies in various multimodal differences, and the robustness of local features to these differences is crucial. According to the analysis, intensity difference ${{\cal F}_{\rm{Int}}}( \bullet )$, rotation ${{\cal F}_{{\rm{Rot}}}}( \bullet )$, and scale difference ${{\cal F}_{{\rm{Sc}}}}( \bullet )$ are the most common problems. So we design experiments to singly test the invariance of MS-HLMO from these three aspects, so as to test whether it deals with each multimodal problem between multi-source images.


\subsubsection{Intensity invariance}
In order to independently test the intensity invariance of MS-HLMO, the influence of other factors is excluded. Several image pairs are selected and manually registered in advance, providing image pairs of the same size that basically eliminated the differences in scale and rotation as the input image of registration. The NCMs of the registration results are listed in Table \ref{tbl:intensity}.
%, and the visual feature matching results of example image pairs are shown in Fig.0
The MS-HLMO$^+$ represents the proposed algorithm without the operation of rotation invariance, that is, the main orientation is not assigned to each feature point, and the reference orientation ${\theta _{\rm{0}}}$ in feature description is set to 0. In this way, the robustness to intensity difference is evaluated more accurately, and the influence of descriptor rotation and PMOM value changes is excluded. As RIFT is an effective feature focusing on intensity difference, it is chosen for comparison.

\begin{table}[h!]
 \centering
 \setlength{\tabcolsep}{1.5mm}
 \caption{NCMs of 8 scenes with only intensity difference by MS-HLMO and RIFT.}
 \label{tbl:intensity}
 \begin{tabular}{ccccccccc}
  \toprule
  \multirow{2}{*}{Method} & \multicolumn{8}{c}{Scene}\\
                            \cline{2-9} & a1 & a2 & d & j & g & m & a4 & p \\
  \midrule
   RIFT$^+$    & 1032 &  385 &  393 & 176 & 104 & 189 &  33 &  92 \\
   MS-HLMO     & 1417 &  620 & 1321 & 149 & 280 & 374 & 220 & 575 \\
   MS-HLMO$^+$ & 1434 & 1017 & 1454 & 771 & 351 & 846 & 386 & 868 \\
  \bottomrule
 \end{tabular}
\end{table}

The 8 scenes basically cover common remote sensing image types, including various optical images, SAR, depth map, and artificial map. From the numerical results, each group has obtained sufficient correct matches, especially in scene (a1) and (d), about 1400 of 2000 detected Harris corner points have been successfully matched. In most scenes, the NCMs of the MS-HLMO are far more than RIFT, reflecting stronger robustness to intensity difference. In scene (j), the MS-HLMO with rotation invariance obtained fewer matches than RIFT. Through analysis, this is because scene (j) is an HSI-LiDAR image pair with large size and complex content, where the modal difference between them is large. Thus, the stability of the feature points' main orientation is not high. The inconsistent reference direction leads to large differences in some feature points' descriptors, while RIFT does not have influence caused by the feature point's orientation. However, without rotation interference, the MS-HLMO$^+$ obtains a significant number of correct matches, more than four times to RIFT. The experiment shows that MS-HLMO with PMOM as its basic feature has strong intensity invariance and is very effective for multi-modal image registration.


\subsubsection{Rotation invariance}

To test rotation invariance, for each one of the above image pairs without scale and rotation differences, one of them is fixed and the other is rotated. The rotation angles are from 0° to 359° with an interval of 1°, from which a total of 360 image pairs of each scene are obtained. Then, the registration algorithms are performed on each image pair, and the corresponding NCMs are plotted in Fig.\ref{fig:rot}. Samples of registration results under different rotation angles are shown in Fig.\ref{fig:rot_r}.

\begin{figure}[!h]
 \begin{center}
  \includegraphics[width=3.5in]{result_o.pdf}
  \caption{Examples of feature matching results with image rotation.}
  \label{fig:rot_r}
 \end{center}
\end{figure}

It is observed that the NCMs of the 8 scenes vary as the rotation angles change, but are all greater than 100. None of the image pairs fails to be registered at any angles, indicating that the proposed MS-HLMO has rotation invariance over the entire angle interval of 0° to 360°. In most scenes, as the images rotate, the NCMs fluctuate with a cycle of 90°, the reason of this phenomenon is that the image rotation involves data interpolation as a digital image is stored a matrix. In other words, the change of the local gradient orientation is not linear, which is inevitable in digital image processing. As a result, the main orientation of each feature point is not completely stable with a certain offset from the image rotation angle. Relatively, the rotation of 0°, 90°, 180°, and 270° do not change the data value, and the main orientation of each feature point will not shift, resulting in the four peaks in the curve. Theoretically, the four intervals evenly divided by 90° from 0° to 360° are completely consistent, that is, the descriptors of a feature point are completely the same when rotated by 90°, 180°, and 270° which is confirmed by the experimental results. MS-HLMO has a strong rotation invariance and fully copes with image rotation from various angles. In addition, the rotation invariance is not restricted by the types of imaging sensors.


\subsubsection{Scale invariance}
To evaluate the scale invariance of MS-HLMO, the same strategy is used by fixing one image and scaling the other. The scaling ratios are from 1 to 2 with an interval of 0.1, providing a total of 11 image pairs in each scene. An example of the feature points matching results of MS-HLMO and MS-HLMO$^+$ are shown in Fig.\ref{fig:scale}-\ref{fig:scale_r}, respectively.

\begin{figure}[h!]
 \begin{center}
  \includegraphics[width=2.2in]{scale_m.png}
  \caption{NCMs of MS-HLMO and MS-HLMO$^+$ on scene (m) as the scaling ratios from 1 to 2.}
  \label{fig:scale}
 \end{center}
\end{figure}

\begin{figure}[h!]
    \centering
        \subfloat[]{\label{fig:scale_r:a}
        \includegraphics[width=3.5in]{result_s1.pdf}}
    \hfil
        \subfloat[]{\label{fig:scale_r:b}
        \includegraphics[width=3.5in]{result_s2.pdf}}
    \caption{Examples of feature matching results of scene (m) with scale differences. (a) Matching results by MS-HLMO with scale ratio of 1.5 and 2. (b) Matching results by MS-HLMO$^+$ with scale ratio of 1.5 and 2.}
    \label{fig:scale_r}
\end{figure}

Even if there are scale differences between images with integer or non-integer proportions, the algorithm remains effective and obtains accurate matching. By observing the NCM curve, with the gradual change of the scale ratio from 1 to 2, the number of matching points gradually decreases, and reaches the lowest when the scale ratio was 1.5, and then gradually increases. This is because, in the experiment, the down-sampling ratio in building gaussian scale-space is set as 2. The interference caused by scale difference is well resisted when the sampling scale is integer multiple of 2. In other scale proportions, a significant difference of the receptive field in ground covers of the feature descriptor is caused between the image pair. So the local windows used for feature extraction between the two images are different in the receptive field. In practice, the sampling step in Gaussian pyramid is further refined to achieve better scale invariance.

This experiment demonstrates that MS-HLMO features are also robust to a certain extent when images are at scale proportion between two octaves in scale space and The proposed MS-HLMO has good robustness to scale difference of multi-source images, which plays an extremely positive role in the actual multi-source remote sensing image registration.


\begin{table*}[h!]
 \centering
 \setlength{\tabcolsep}{1.2mm}
 \caption{NCMs of the 17 remote sensing scenes comparison by eight registration methods.}
 \label{tbl:result}
 \begin{tabular}{lcccccccccccccccccccc}
  \toprule
  \multirow{2}{*}{Method} & \multicolumn{20}{c}{Scene}\\
    \cline{2-21} &   a1 &  a2 &  a3 &  a4 &   b &   c &    d &   e &   f &   g &   h &   i &   j &   k &   l &   m &   n &   o &   p &   q \\
  \midrule
   SIFT          &  794 &  30 &  25 &  —— &  —— & 145 &   86 &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  15 &  14 & 256 \\
   SAR-SIFT      &   16 &   6 &  —— &  —— &  —— &  66 &    4 &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &   9 &  36 & 262 \\
   PSO-SIFT      &    3 &  —— &  —— &  —— &  —— &  —— &   —— & 171 &  13 &  —— &  —— &  —— &   9 &   6 &  10 &  —— &  —— &  19 &  24 & 259 \\
   PIIFD         &   —— &  —— &  —— &  —— &  —— &  —— &   —— &  53 &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— &  —— & 493 \\
   MS-PIIFD      & 1105 &  —— & 481 &  —— & 444 & 145 &  999 & 460 &  78 &  —— &  —— &  25 &   5 & 339 &  30 &  74 & 165 &  —— & 322 & 860 \\
   RIFT$^+$    &   —— &  21 &  —— &  34 &  —— &  —— &   —— &  —— &  —— &  —— &  —— &  39 & 176 & 109 & 188 & 189 &  96 & 179 &  —— &  —— \\
   MS-HLMO       & 1126 & 112 & 865 & 119 & 603 & 394 & 1030 & 689 & 161 & 130 & 231 &  79 & 149 & 511 & 420 & 374 & 161 &  97 & 432 & 883 \\
   MS-HLMO$^+$ & 1194 & 321 & 996 & 149 & 984 & 564 & 1163 &  —— & 693 & 162 & 337 & 145 & 761 & 789 & 861 & 846 & 273 & 451 &  —— &  —— \\
  \bottomrule
 \end{tabular}
\end{table*}

\begin{figure*}[h!]
    \begin{center}
        \subfloat[]{\label{fig:matching:a}
        \includegraphics[height=1.0in]{a.pdf}}
%        \subfloat[SAR-MSI-HSI]{\label{fig:matching:a1}
%        \includegraphics[height=1.0in]{a1.pdf}}
%        \subfloat[map-MSI-HSI]{\label{fig:matching:a2}
%        \includegraphics[height=1.0in]{a2.pdf}}
        \subfloat[]{\label{fig:matching:b}
        \includegraphics[height=1.0in]{b.pdf}}

        \subfloat[]{\label{fig:matching:c}
        \includegraphics[height=0.8in]{c.pdf}}
        \subfloat[]{\label{fig:matching:d}
        \includegraphics[height=0.8in]{d.pdf}}
        \subfloat[]{\label{fig:matching:e}
        \includegraphics[height=0.8in]{e.pdf}}
        \subfloat[]{\label{fig:matching:f}
        \includegraphics[height=0.8in]{f.pdf}}

        \subfloat[]{\label{fig:matching:g}
        \includegraphics[height=1.0in]{g.pdf}}
        \subfloat[]{\label{fig:matching:h}
        \includegraphics[height=1.0in]{h.pdf}}
        \subfloat[]{\label{fig:matching:i}
        \includegraphics[height=1.0in]{i.pdf}}

        \subfloat[]{\label{fig:matching:j}
        \includegraphics[height=0.8in]{j.pdf}}
        \subfloat[]{\label{fig:matching:k}
        \includegraphics[height=0.8in]{k.pdf}}
        \subfloat[]{\label{fig:matching:l}
        \includegraphics[height=0.8in]{l.pdf}}
        \subfloat[]{\label{fig:matching:m}
        \includegraphics[height=0.8in]{m.pdf}}

        \subfloat[]{\label{fig:matching:n}
        \includegraphics[height=0.8in]{n.pdf}}
        \subfloat[]{\label{fig:matching:o}
        \includegraphics[height=0.8in]{o.pdf}}
        \subfloat[]{\label{fig:matching:p}
        \includegraphics[height=0.8in]{p.pdf}}
        \subfloat[]{\label{fig:matching:q}
        \includegraphics[height=0.8in]{q.pdf}}
    \end{center}
    \caption{Feature points matching results of the 17 remote sensing scenes by the proposed MS-HLMO. (a) a2:SAR-MSI, a1:MSI-HSI, a4:map-MSI, and a3:MSI-HSI (multi-temporal). (b) MSI-HSI. (c) MSI-MSI. (d) PAN-HSI. (e) visible-infrared. (f) visible-infrared. (g) visible-infrared. (h) visible-infrared. (i) visible-infrared. (j) HSI-LiDAR. (k) HSI-LiDAR. (l) HSI-LiDAR. (m) visible-depth. (n) visible-map. (o) day-night (visible). (p) multi-source SAR. (q) single-source visible.}
    \label{fig:matching}
\end{figure*}

\subsection{Registration Performance}
\label{ssec:final}

\begin{figure*}[!h]
    \begin{center}
        \subfloat[]{\label{fig:checker:a}
        \includegraphics[height=0.85in]{a_show.pdf}}
        \subfloat[]{\label{fig:checker:b}
        \includegraphics[height=1.1in]{b_show.pdf}}

        \subfloat[]{\label{fig:checker:c}
        \includegraphics[height=0.8in]{c_show.pdf}}
        \subfloat[]{\label{fig:checker:d}
        \includegraphics[height=0.8in]{d_show.pdf}}
        \subfloat[]{\label{fig:checker:e}
        \includegraphics[height=0.8in]{e_show.pdf}}
        \subfloat[]{\label{fig:checker:f}
        \includegraphics[height=0.8in]{f_show.pdf}}

        \subfloat[]{\label{fig:checker:g}
        \includegraphics[height=0.8in]{g_show.pdf}}
        \subfloat[]{\label{fig:checker:h}
        \includegraphics[height=0.8in]{h_show.pdf}}
        \subfloat[]{\label{fig:checker:i}
        \includegraphics[height=0.8in]{i_show.pdf}}

        \subfloat[]{\label{fig:checker:j}
        \includegraphics[height=0.8in]{j_show.pdf}}
        \subfloat[]{\label{fig:checker:k}
        \includegraphics[height=0.8in]{k_show.pdf}}
        \subfloat[]{\label{fig:checker:l}
        \includegraphics[height=0.8in]{l_show.pdf}}

        \subfloat[]{\label{fig:checker:m}
        \includegraphics[height=0.85in]{m_show.pdf}}
        \subfloat[]{\label{fig:checker:n}
        \includegraphics[height=0.9in]{n_show.pdf}}
        \subfloat[]{\label{fig:checker:o}
        \includegraphics[height=0.85in]{o_show.pdf}}
        \subfloat[]{\label{fig:checker:p}
        \includegraphics[height=0.85in]{p_show.pdf}}
        \subfloat[]{\label{fig:checker:q}
        \includegraphics[height=0.85in]{q_show.pdf}}
    \end{center}
    \caption{Registration results of the 17 remote sensing scenes by the proposed MS-HLMO, shown in fusion form and alternation form. Feature points matching results of the 17 remote sensing scenes by the proposed MS-HLMO. (a) a2:SAR-MSI, a1:MSI-HSI, a3:MSI-HSI, and a4:map-MSI. (b) MSI-HSI. (c) MSI-MSI. (d) PAN-HSI. (e) visible-infrared. (f) visible-infrared. (g) visible-infrared. (h) visible-infrared. (i) visible-infrared. (j) HSI-LiDAR. (k) HSI-LiDAR. (l) HSI-LiDAR. (m) visible-depth. (n) visible-map. (o) day-night (visible). (p) multi-source SAR. (q) single-source visible.}
    \label{fig:checker}
\end{figure*}

In this section, the registration is performed directly on each original data of the 17 scenes, to evaluate the overall registration performance. The NCMs of the eight registration methods with all 17 scenes are listed in Table \ref{tbl:result}. Fig.\ref{fig:matching} shows the corresponding visualization of feature point matching results of each scene. Due to space limitation, the one with the better result between MS-HLMO and MS-HLMO$^+$ is selected to display in each group.

In most scenes, MS-HLMO obtains the most NCM, and basically far more than other algorithms except for scene (j) and (o), in which RIFT$^+$ has more NCM. The cases in the two scenes have already been discussed in the former section. However, the proposed MS-HLMO still obtains a considerable number of correct matches that are sufficient for accurate registration, and without the interference of orientation, MS-HLMO$^+$ obtains more matches.

SIFT, SAR-SIFT, PSO-SIFT, and PIIFD fail in registration in most multi-source scenes, which cannot well cope with multi-source remote sensing data images in various situations.
Note that some of the test data selected in this paper are also used in MS-PIIFD, and the registration is successful when the number of Harris corners is set to 1000. However, when the number of Harris corners is increased to 2000 in this experiment, the effect of MS-PIIFD is greatly reduced, and it even fails. This is because the proportion of correct matching of feature points obtained by MS-PIIFD before going through FSC is low, and more outliers may be introduced after increasing the number of feature points, which makes it difficult to calculate a consistent transformation model and leads to performance degradation. It indicates that although MS-PIIFD is able to handle multi-scale and multi-mode image registration, its feature robustness and stability are limited. Under the same conditions, MS-HLMO obtains a large number of correct matches, which indicates that its feature robustness and stability are relatively high. The RIFT$^+$ works well in scenes with only intensity difference, but the NCMs are relatively low. RIFT$^+$ cannot cope with the images containing rotation and scale differences.

The last scene is a single-source image with only rotation. This scene proves that MS-HLMO can not only deal with multi-source images effectively but also has a good effect on single-source images, which still performs better than other algorithms. Among all the competitive methods, MS-HLMO is the only algorithm that does not fail in all scenes and obtains sufficient feature matches. In the scenes without obvious image rotation difference, MS-HLMO$^+$ achieves the most feature point matching, far outperforming other algorithms. Through the above experiments, the ability of MS-HLMO is verified from a detailed and comprehensive perspective, and it can effectively handle the registration task of multi-source remote sensing images. Compared with the results of the existing effective algorithms, the proposed MS-HLMO has much more significant effect than other algorithms in most cases. MS-HLMO$^+$ better deals with the situation without obvious rotation difference, which is also the majority of practical multi-source image registration tasks. MS-HLMO and its simplified version MS-HLMO$^+$ have obvious advantages in robust feature extraction and matching.



%其中SIFT、SAR-SIFT、PSO-SIFT和PIIFD在大多数多源场景下都配准失效，难以应对多种情况的多源遥感数据图像。
%评价一下MS-PIIFD其实也很拉,注意本文选用测试的数据有部分在MS-PIIFD中也被使用，并且在Harris角点个数设为1000时成功配准。而在本次实验中将Harris角点个数增至2000时，MS-PIIFD的效果却大大下降，甚至失效。这是由于MS-PIIFD获取的特征点匹配在经过FSC之前正确匹配的比例较低，而增加特征点数量后又会引入更多的outliers，从而难以计算一致的变换模型导致效果下降。说明虽然MS-PIIFD能够应对多尺度、多模态的图像配准，但是其特征的鲁棒性和稳定性偏低。而在同条件下，MS-HLMO能够获得大量的正确匹配，证明了其特征的鲁棒性和稳定性相对较好，
%评价一下RIFT就一个多一点，which has been discussed in the former section.

%最后一个场景是一个同源图像，仅仅存在旋转，这个场景证明了MS-HLMO不仅能够有效应对多源图像，对于同源图像仍然具有很好的效果，优于其他算法。MS-HLMO是所有对比算法中唯一一个在所有场景下都没有失效的算法，并且都能获得充足的特征匹配。在没有图像旋转差异的场景下，简化版的MS-HLMO+都获得了最多的特征点匹配，远超过其他算法。通过上述实验，从细致具体和综合整体的角度验证了MS-HLMO的能力，可以有效处理多源遥感影像的配准任务。通过与多种现有有效算法结果对比，the proposed MS-HLMO效果显著，在大多数情况下远高于其他算法。MS-HLMO$^+$ 能够更佳地应对没有明显角度差异的情况，这在实际的多源遥感配准任务中也是大多数情况。在鲁棒特征的提取和匹配上，MS-HLMO以及它的简化版MS-HLMO$^+$优势是明显的。

% Remote sensing image registration is so far difficult to be numerically measured, especially multi-source ones with inconsistent spatial calibration information.
Fig.\ref{fig:checker} illustrates the registration results of the 17 scenes, in which the transformation parameters is obtained based on the feature matching shown in Fig.\ref{fig:matching}. For each image pair, one with the higher resolution is taken as the reference, and the other is aligned with it through spatial affine transformation and data interpolation. Image pairs with large spatial offset are shown in a fusion form, and the areas with prominent structures in the overlapping parts are selected and shown in an alternation form. All the operations are performed autonomously without human intervention.

It is observed that most of the image pairs are well registered, the outlines and textures of the ground covers stay continuous and consistent. The deviations are mostly within one pixel. The only obvious deviations are found in some areas of scene (g) and (i) shown in Fig.\ref{fig:checker}. This phenomenon does not result from inaccurate feature extraction or matching but is caused by the characteristics of the data. Scene (g) and (i) are ground-based images with relatively close imaging distance, in which there are obvious viewpoint differences. In addition, nonrigid geometric distortions exist in the infrared images, which may be caused by the imaging capability of the sensors. Due to the above reasons, more complex spatial differences exist between the image pairs, which is difficult to be solved with only an affine model globally transforming the images. In practice, this problem can be solved by using more sophisticate nonlinear transformation models, or by dividing the image into blocks for adaptive local registration. Apart from that, most areas is well registered, where the objects are accurately aligned.
