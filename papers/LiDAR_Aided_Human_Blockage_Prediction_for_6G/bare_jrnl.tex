%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

\makeatletter
\def\endthebibliography{%
  \def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
  \endlist
}
\makeatother

\usepackage{array}
\newlength\mylength
\setlength\mylength{\dimexpr.11\columnwidth-2\tabcolsep-0.5\arrayrulewidth\relax}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\setlength{\belowcaptionskip}{-10pt}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{bm}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{soul,color}
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\usepackage{comment}
\usepackage{algorithm}  %algorithm
\usepackage{algpseudocode}  %algorithm


\makeatletter
\newcommand\fs@betterruled{%
  \def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{\vspace*{2pt}\hrule height.8pt depth0pt \kern2pt}%
  \def\@fs@post{\kern2pt\hrule\relax}%
  \def\@fs@mid{\kern2pt\hrule\kern2pt}%
  \let\@fs@iftopcapt\iftrue}
\floatstyle{betterruled}
\restylefloat{algorithm}
\makeatother

\usepackage[backend=biber,
sorting=none,
doi=false,
isbn=false,
url=false,
maxnames=3, minnames=2,
style=ieee, ]{biblatex}
\addbibresource{bibi.bib}
\renewcommand*{\bibfont}{\small}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{LiDAR Aided Human Blockage Prediction for 6G}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
\author{
\IEEEauthorblockN{ Dileepa Marasinghe, Nandana Rajatheva and Matti Latva-aho }\\
\IEEEauthorblockA{\textit{Centre for Wireless Communications,} \\
\textit{Univeristy of Oulu,}\\
Oulu, Finland \\
E-mail: dileepa.marasinghe@oulu.fi,
nandana.rajatheva@oulu.fi, matti.latva-aho@oulu.fi}

}

        % <-this % stops a space
%\thanks{M. Shell was with the Department
%of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
%GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


\pagenumbering{gobble}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
Leveraging higher frequencies up to THz band paves the way towards a faster network in the next generation of wireless communications. However, such shorter wavelengths are susceptible to higher scattering and path loss forcing the link to depend predominantly on the line-of-sight (LOS) path. Dynamic movement of humans has been identified as a major source of blockages to such LOS links. In this work, we aim to overcome this challenge by predicting human blockages to the LOS link enabling the transmitter to anticipate the blockage and act intelligently. We propose an end-to-end system of infrastructure-mounted LiDAR sensors to capture the dynamics of the communication environment visually, process the data with deep learning and ray casting techniques to predict future blockages. Experiments indicate that the system achieves an accuracy of 87\% predicting the upcoming blockages while maintaining a precision of 78\% and a recall of 79\% for a window of 300 ms.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
LiDAR, blockage prediction, 6G, vision aided communications, mmWave, THz.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

Communication networks in the 6G era are envisioned to utilize the enormous bandwidth available in the higher frequencies ranging from mmWave to THz. However, communicating in these bands is extremely challenging as they suffer from high scattering due to their smaller wavelengths compared to conventionally used sub-six GHz frequencies. To combat this, ultra-narrow beams are used and as the beam-widths get smaller the random blockages to the LOS path can degrade the link quality abruptly  \cite{tan2020thz}. Specifically,  blockage occurring with dynamic movements of the humans in the environment has been reported as a significant degrading factor of the link quality \cite{human_blockage}. One solution to minimize this adverse effect of link quality degradation is to predict such blockages allowing the transmitter to act proactively by doing a handover to a transmitter with LOS availability or avoid transmitting in the blocking time duration. Predicting such events require a holistic view of the communication environment. Sensing the environment through heterogeneous modalities emerge as a promising technology to capture the dynamics of the communication environment implying information on the scattering, thus aiding in the communication procedures such as positioning, beam management and blockage prediction. Cameras, LiDARs and radars are potential sensors that can be used for such environment sensing. Particularly, LiDAR sensors are an interesting choice to capture the 3D structure of the communication environment in a detail-rich manner which also eliminates the privacy concerns in contrast to cameras. The use of LiDARs for such monitoring purposes was restrained by the enormous price of the sensors. However, recent developments in the LiDAR technology have reduced the price to couple of hundred dollars which enables a multitude of applications to harness the potential of LiDARs. Moreover, the range of these sensors become adequate to capture the communication environment since the 6G networks converge to tiny cells in terms of coverage distance because of the high path loss in the higher frequencies. Furthermore, algorithms for processing such hetero-modal data to generate predictive insights, specially for vision data has evolved significantly in the recent past with the continuous growth of deep learning techniques.

Therefore, in this work we try to look at the problem of dynamic blockage prediction from the emerging field of vision aided communication point of view. We propose a system to utilize infrastructure-mounted LiDAR sensors \cite{NalinEliD, padmal2021elevated} to predict human blockage in indoor scenarios. We solely rely on the LiDAR data eliminating the burden to the wireless algorithms and provide positioning data and future blockage status of the users in the communication environment. 


The rest of the paper is organized as follows. In section II, we summarize the recent efforts to utilize vision data for communications and work on blockage prediction. We formulate the problem in section III and describe our proposed system for the human blockage prediction problem in section IV. Section V contains the details on the experimental setup we used. We present the results and a discussion on the simulations conducted in section VI and conclude the paper in section VII with our intentions of future work.


% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)


%\subsection{Subsection Heading Here}
%Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Previous work}
%\hl{UPDATE THIS,, keep works on LiDAR data, . blockage detection, blockage prediction etc - check viwi latest paper related work section}
%\hl{put Blockage analysis, 3D THZ, human blockage etc}

Use of visual data from sensors such as LiDAR and camera  for aiding wireless communications is relatively a new area that has gained attention recently. In \cite{Aldebaro}, a deep learning based mmWave beam-selection problem in a vehicular  scenario has been explored for a downlink system with analog beamforming. The authors focus on solving LOS versus non-LOS (NLOS) classification of the current channel and selection of top-M beam pairs for the beamforming from a predefined codebook and shows the potential of using LiDAR data for aiding mmWave communication procedures. The authors in \cite{Dias} consider a similar problem to \cite{Aldebaro} using LiDAR and position data for a mmWave downlink vehicular scenario and conclude that a distributed architecture  where LiDAR processing is done at vehicles performs better.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{PaperSystem.pdf}
    \caption{System overview}
    \label{fig:system}
\end{figure*}


Apart from using LiDAR data, use of images from fixed cameras has been explored for mmWave beam selection in \cite{Xu} and \cite{charan2020visionaided}. In \cite{Xu} a panoramic point cloud generation using camera images is carried out in contrast to obtaining point cloud data from LiDAR. The results conclude that the panoramic point clouds constructed by the proposed method are more appropriate for accurate beam selection when compared with the local point cloud scanned by user as in previous works. This emphasizes the fact that having a wider view of the environment improves the beam selection accuracy. 

Base station selection based on human blockage prediction for a static transmitter receiver pair has been studied utilizing RGBD cameras in \cite{proactivebasestation}. The dynamic blockage prediction problem for mmWave networks in a vehicular scenario has been explored in \cite{charan2020visionaided} and extended to proactive handover problem in \cite{charan2021visionaided}. The authors use camera images from a camera fixed on the base station to predict whether the user will be blocked. A recurrent neural network with gated recurrent units (GRU) is trained with camera images and beam indices. For object detection a pre-trained deep learning network is used. The proposed approach outperforms the method using beam sequences only demonstrating successful use of visual information to aid in wireless algorithms. 
% However, privacy becomes a key problem when using cameras for such monitoring specially in inddor scenarios with humans involved. , YOLO (You Only Look Once) \cite{YOLO}

\section{Problem formulation }

The main objective of this work is to utilize the LiDAR data to predict the dynamic blockage in a future window. We entirely depend on the data from the LiDARs to determine the blockage events, thus radio blockage is directly evaluated as the visual blockage. We consider an indoor area where humans are walking randomly and all the humans are utilizing a user equipment (UE) to connect to an access point (AP) inside the same area operating in mmWave/THz frequencies. The radio links between the transmitter and the UEs are assumed to be established already. Multiple LiDAR sensors are mounted in elevated positions to monitor the indoor area which capture the 3D information and sent to a server which processes the data to generate a combined 3D map of the indoor area at a rate of $f_s$ synchronized with the sensor rates. We denote the link status of the radio link between the AP and the UE carried by a human $p$ at a time instance $t$ as $l_p^{t}$. If there exist a LOS path for the link, $l_p^{t} = 0$ and if blocked which means in NLOS condition, $l_p^{t} = 1$. Note that the time is considered in discrete form synchronized with the publishing of the 3D maps. We define the link status of the UE of human $p$, for a prediction window of $w$ time instances as,
\begin{gather*}
l_p^{t_w} =
\begin{cases}
   0 &  l_p^{t} = 0 \quad  \forall t \in [{t_{e} + 1}, t_{p}]   \\    
   1 & otherwise. 
\end{cases}
\end{gather*}
The objective of the method is to  process the combined 3D maps in an observation window of $t \in [{t_{s}}, t_{e}]$ time instances, and evaluate the link statuses $l_p^{t_w}$ of the UEs in the prediction window $t \in [{t_{e} + 1}, t_{p}]$.  

\section{LiDAR aided human blockage prediction}

We propose an end-to-end system for the blockage prediction problem utilizing point cloud processing and deep learning. The solution consists of three stages. Fig. 1 shows an overview of the system. First stage processes the received 3D maps for the detection of humans in the environment and tracking their movements. The second stage predicts the trajectories of the tracked humans based on the past accumulated position data. The third stage synthesizes the future instance with the bounding boxes and predicted positions of the humans and detect potential blockages using a minimal raytracing algorithm.  Next we describe the three stages in the following subsections.

\subsection{Point cloud based human detection and tracking}
Human detection and tracking stage is based on the proposed method in \cite{koide} for static LiDAR sensors. First, offline environmental mapping is done using the infrastructure-mounted LiDAR system for identifying the static environment and stored as a point cloud which we term as the global map. During the operation, point clouds from the sensors are registered to generate the 3D map using the method described in \cite{padmal2021elevated}. Then the background points are removed by subtracting the global map from the current 3D map to identify the dynamic points. The dynamic points are subjected to Euclidean clustering to detect potential human clusters. This clustering process might result in inaccuracies if the humans are close to the each other which is mitigated using the Haselich’s split-merge clustering algorithm \cite{Haselich}. The detected human clusters are assumed to be walking on the ground plane, thus the tracking is done using $x,y$ coordinates discarding the $z$ coordinate. A Kalman filter with a constant velocity model  and global nearest neighbor data association are used for tracking the detected human clusters between the frames.

Let $N$ be the number of people tracked at the time instance $t=t$. The tracking algorithm outputs the  position and bounding boxes of a tracked human $p$ as a tuple $\bm{D}^t_p=[ \bm{x}^t_p, \bm{b}^t_p] $ where $\bm{x}^t_p = (x_p,y_p,z_p)$ is the position in Cartesian coordinates and $\bm{b}^t_p$ is the bounding box. We consider two methods with axis aligned bounding boxes (AABB) and oriented bounding boxes (OBB). An AABB is defined as $\bm{b}^t_p = ( x_{min},y_{min}, z_{min}, x_{max},y_{max},z_{max})$ and an OBB as $\bm{b}^t_p = ( \hat{x}_{min},\hat{y}_{min},\hat{z}_{min}, \hat{x}_{max},\hat{y}_{max},\hat{z}_{max}, \bm{M})$ where $\hat{x}$ denotes coordinates in object space and $\bm{M}$ is the rotation matrix with respect to the global coordinate system. The calculation of OBB for a human cluster is carried out by computing eigen vectors of the points in the cluster to determine the orientation and calculation of AABB is straightforward. 

\subsection{Trajectory prediction}
Human motions are complex which adds an extra difficulty in predicting their trajectories compared to vehicles, robots etc. Predicting a motion path of a human can be viewed as a sequence generation  problem when the past trajectory of the human is known. Recent works report considerable success in applying recurrent neural networks (RNN), particularly long short term memory (LSTM) models for human trajectory forecasting \cite{sociallstm,trajnet}. LSTM models have shown the ability to encompass the sequential nature while adapting to the non-linearities in human motion. Following the state of the art, we use an LSTM model for predicting the human trajectory based on the tracking data provided by the previous stage. Each human trajectory is modelled by one LSTM which will encompass the motion of the human in the observation time window and the future trajectory is predicted as a sequence using the LSTM. As the humans are assumed to be walking in the ground plane, similar to first stage, we consider only the $x,y$ coordinates of $\bm{x}^t_p$ for a trajectory as $\bm{\Tilde{x}}^t_p = ( x_p , y_p )$. Therefore, the observed trajectory for a human $p$ is $\bm{R}_p = \{\bm{x}^t_p\}^{t_{e}}_{t = t_{s}} $ in the observation window $t= [{t_{s}}, t_{e}]$. We  estimate the future trajectory of the human $p$, $\bm{\Hat{R}}_p = \{\bm{\Tilde{x}}^t_p\}^{t_{p}}_{t = t_{e} + 1}$ for the prediction window  $t= [{t_{e} + 1}, t_{p}]$ using the LSTM model as follows. 

Each position of a trajectory in the observation window are converted to velocities in each dimension by subtracting the previous position from the current position and sent as input to the LSTM model which gives the following recurrence:

\begin{subequations}
\begin{align}
e^t_p &= \phi ( \Delta \bm{\Tilde{x}}^t_p; W_{emb}), \\
h^t_p &= LSTM ( h^{t-1}_p, e^t_p ; W_{enc}),
\end{align}
\end{subequations}where $\phi$ is and embedding function with ReLU non-linearity and $W_{emb}, W_{enc} $ are the embedding and encoder weights of the model. Then the velocity distribution of the time step, $t+1$ is estimated using the hidden state of the time step, $t$ for each of the time steps in the prediction window. Similar to \cite{sociallstm},\cite{trajnet} a bi-variate Gaussian distribution is considered as the velocity distribution with mean $\mu_p^{t+1} = ( \mu_x , \mu_y )$, variance $\sigma_p^{t+1} = (\sigma_x , \sigma_y)$ and correlation coefficient $\rho_p^{t+1}$. This is learnt through:

\begin{equation}
    [\mu_p^{t+1}, \sigma_p^{t+1}, \rho_p^{t+1}] = \phi_{dec}( h^{t}_p; W_{dec}),
\end{equation}
with $\phi_{dec}$ is modelled with a fully connected layer having $W_{dec}$ as the weight matrix. Training of the LSTM model is done with human trajectories using negative log-likelihood loss. The future trajectory of the human $\bm{\Hat{R}}_p$ is generated by sampling the resulting bi-variate Gaussian for each of the time steps in the prediction window as:

\begin{equation}
    \bm{\Tilde{x}}^{t+1}_p =  \bm{\Tilde{x}}^{t}_p + \bm{v},  
\end{equation}
where $\bm{v} \sim \mathcal{N} ( \mu_p^{t+1} , (\sigma_p^{t+1}, \rho_p^{t+1}))$.

\subsection{Ray casting based blockage detection}

This stage utilizes the predicted trajectories and current tracking information and bounding boxes to evaluate the blockage in the predicted positions using ray casting. Ray casting is a technique used in computer graphics rendering which utilizes geometric relations to evaluate the interactions of a ray originating from an emitter with the objects in the considered scene. In this work we utilize the ray-box intersection algorithm for the AABBs known as the slab method \cite{raybox}. The key idea in the slab method is to consider an AABB as three pairs of parallel planes. The intersection of the ray with the bounding box is evaluated by clipping the ray with each pair of the parallel planes and determine whether any portion of the ray is remaining which means there is an intersection of the ray with the AABB which is elaborated in Fig. \ref{fig:slabmethod}. For OBBs the slab method is applied in object coordinate space instead of the global coordinate space  as the bounding box is an AABB in the object space although the rotation is with respect to global space. The considered ray is transformed from the global space to object space using the position and the rotation matrix of the OBB. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{SlabMethod1.pdf}
    \caption{Slab method for ray-box intersection in 2D. Left figure shows the two rays clipped by horizontal planes with intersection points $t_{min}$ and $t_{max}$. The middle figure shows the next step of clipping by vertical planes and taking the maximum between current $t_{min}$ and new intersection points and minimum between current $t_{max}$ and new intersection points. Finally a check is done to determine the intersection whether $t_{max} > t_{min}$. Right figure shows that only the intersecting ray has a remaining portion inside the box.}
    \label{fig:slabmethod}
\end{figure}

The predicted trajectories of the humans, $\bm{\Hat{R}_p}$ from the second stage and the associated tracking information  $\bm{D^t_p}$ from the first stage are taken as input to this stage.  For the blockage evaluation, we transform the bounding boxes to the predicted positions in each time step in the prediction window. Then the ray originating from the transmitter point to the center point of a transformed bounding box is considered. We evaluate whether each of these rays intersect with the bounding boxes of the humans tracked  using the method outlined in Algorithm 1 based on the slab method. If there are intersections detected for a particular ray, we check whether the closest intersection to the transmitter is for the considered bounding box or any other. If the closest intersection is not with the bounding box of the human in consideration, then we declare that the human is blocked which means in NLOS condition.


\begin{algorithm}[!t]
%\SetAlgoLined
1: \textbf{Input} :  \{($\bm{D^}{t_e}_p$, $\bm{\Hat{R}}_p)\}_{p=1}^N$ \\
2: \textbf{Output} : $\{l_p^{t_w}\}_{p=1}^N$ \\%^{t_{p}}_{t = t_{e} + 1}$ \\
3: \textbf{Initialize} : $ l_p^{t_w} \leftarrow 0\  \forall p$  \\
4: \textbf{for} $t \leftarrow t_{e} + 1 , t_p$ \textbf{do} \\
5: \quad $d_{min}^p \leftarrow Inf \ \forall p$ \\
6: \quad $i_{min}^p \leftarrow p \ \forall p$ \\
7: \quad \textbf{for} $p \leftarrow  1, N$ \textbf{do} \\
8: \quad \quad $\bm{\overline{b}}^t_p \leftarrow BoundingBoxTransform (\bm{b}^{t_e}_p,\bm{\Tilde{x}}^t_p  ) $ \\
9: \quad \quad  \textbf{for} $q= 1 $ to $N$ \textbf{do} \\
10: \quad \quad \quad $\textbf{if}: l_q^{t_w} = 0$ and $\bm{\Tilde{x}}_q^t \boldsymbol{\cdot} \bm{\Tilde{x}}_p^t > 0 $\\
11: \quad \quad \quad \quad \quad \quad \quad \quad \quad and $\| \bm{\Tilde{x}}_q^t - \bm{\Tilde{x}}_{tx}\| < \| \bm{\Tilde{x}}_p^t - \bm{\Tilde{x}}_{tx}\| $\\
12: \quad \quad \quad \quad \quad $d_{intersect} \leftarrow CheckIntersect(\bm{x}_{tx}, \bm{\Tilde{x}}_q^t, \bm{\overline{b}}^t_p)$\\
13: \quad \quad \quad \quad\quad  \textbf{if}: $d_{intersect} < d_{min}^q$ \textbf{then}\\
14: \quad \quad \quad \quad \quad  \quad $d_{min}^q \leftarrow d_{intersect}$\\
15: \quad \quad \quad \quad \quad  \quad $i_{min}^q \leftarrow p$\\
16: \quad \quad \quad \quad\quad  \textbf{end if} \\
17: \quad \quad \quad $\textbf{end if}$  \\
18: \quad \quad \textbf{end for}\\
19: \quad \textbf{end for}\\
20: \quad\textbf{for} $p \leftarrow  1, N$ \textbf{do} \\
21: \quad \quad \textbf{if}: $i_{min}^p \neq p$ \textbf{then} \\
22: \quad \quad  \quad $l_p^{t_w} \leftarrow 1$\\
23: \quad \quad  \textbf{end if} \\
24: \quad \textbf{end for}\\
25: \textbf{end for}\\
\caption{Ray casting based blockage detection}
\label{Algorithm1}
\end{algorithm}



In Algorithm 1, the function $BoundingBoxTransform$ computes the translation and rotation of a human from current time step to a predicted time step based on the predicted positions and computes the bounding box at the predicted time step. The $CheckIntersect$ function performs ray-box intersection test and returns the distance form the transmitter to the intersection point using the slab method when AABBs are considered. In case of OBBs, $CheckIntersect$ first transforms the coordinates of the transmitter and destination point of the ray to its object space based on the transformed bounding box and perform  ray-box intersection test in object coordinate space. Since the identification of blockages in all the time steps within the window is an exhaustive search, we apply few enhancements to reduce the number of searches per each human in the prediction window in Algorithm 1. Although the outermost loop starting in line 4 evaluates the scenario for each time step, the first condition at line 10 checks whether a previous time step has detected blockage. If a blockage has been detected, then the algorithm skips checking any other time steps for the ray directed towards that human, as we are interested in determining any blockage during a prediction window. Furthermore the other two conditions in lines 10 and 11 ensure that the ray-box intersection check is done only for humans who are closer to the transmitter than the considered human and both have similar directional vectors with respect to the transmitter. 
%\hl{why only centroid is considered}

% This stage receives the predicted positions of the humans until $T_{pred}$ from the second stage and the associated clusters from the first stage. We follow an analysis by synthesis approach for identification of the blockage in the future instances. The human clusters from first stage for the current time instance i.e.  $T_{obs:end}$ are transformed to the predicted positions from $T_{obs:end} + 1$ to $T_{pred}$ and combined with the global map to synthesize the future point cloud instances for the prediction window. The main aim of synthesizing the future point clouds is to utilize the shape information of the human reflected from the point cloud data. One alternative way to evaluate the blockage is to consider the bounding boxes of the clusters with a geometric approach. This eliminates the shape information of the human which will essentially affect in blockage evaluation since our main aim is to detect the RF(radio frequency) blockage. Moreover combining cluster clouds and environment map in the synthesized point clouds provides the possibility of detecting the blockage from the environment as well.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\linewidth]{octreecomparison.png}
%     \caption{Octree comparison with different resolutions}
%     \label{fig:octreecomprison}
% \end{figure}

% The blockage evaluation for the human clusters is carried out by raycasting thorough the point cloud. In raycasting, a ray is cast from the 3D location of the transmitter and the path of the ray is traversed through the point cloud to identify any obstruction to the path for the ray propagation. This is a popular technique used in computer graphics for rendering images. However, point clouds contain sparse unorganized data, thus raycasting is generally done by associating the point cloud with an octree graph. An octree is a tree-based data structure used for partitioning 3D space where each internal node has eight children. The root node describes a cubic bounding box which encapsulates all points. At every tree level, this space becomes subdivided by a factor of 2 which results in an increased voxel resolution. Figure \ref{fig:octreecomprison} shows a comparison at different resolutions for the Stanford Bunny point cloud. 

% Therefore the synthesized point clouds are associated with octrees and raycasting is done by traversing through the point cloud with the fast voxel traversal algorithm described in \cite{raycasting}. The rays are cast towards the centroid of each human cluster at each time instance in the prediction window and obtain the first intersection voxel of the ray with the point cloud. Then we determine that the user is blocked if the first intersection voxel does not fall within the bounding box of the intended human cluster. Figure \ref{fig:raybloc} shows an example scenario where a human is blocking the line of sight path to two other users detected by raycasting. In the figure, the rays cast towards the centroid of the humans marked in green and cyan colors hit the voxels of the human marked in blue first, which reflects the blockage to the line of sight paths.

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.8\linewidth]{rayblockage.png}
%     \caption{An example blockage detection scenario with raycasting}
%     \label{fig:raybloc}
% \end{figure}

% \hl{For the implementation of the code, we used a readily available library for point cloud processing known as Point Cloud Library --cite-- which has implementations on these fast raycasting methods based on octrees.}
% %\hl{Algorithm??}


\section{Experimental setup}

%In this section we present the details on the data generation methods and the implementation of the proposed system.

\subsection{Data generation method}
We use simulations for evaluating the proposed system in this work. LiDAR point clouds are simulated using Blensor \cite{blensor} which is an open source software tool for simulating LiDAR and depth sensors with 3D animations based on Blender \cite{blender} which is a computer graphics animation tool. Blensor provides the facility of adding LiDAR/depth sensors on a Blender based animation and generating the point clouds as the 3D animation progresses. We generate 2D human trajectories using ORCA simulator \cite{orca} as utilized in \cite{trajnet}. Trajectories are generated such that the humans start randomly from a point in the circumference of a 12.5 m radius circle and walk towards the antipodal position on the circle. We generate 500 scenes with 10 humans starting at random positions and a human model is animated to follow these trajectories in the 3D environment in Blensor while point clouds are generated from the deployed sensors. We used two Velodyne HDL-32E LiDAR sensors with 50 m range operating at 10 Hz. Fig. \ref{fig:blensor} shows an example 3D scene modeled in Blensor where the sensors are placed on the elevated positions, tilted 35 degrees downwards to cover the indoor area and the resulting LiDAR point clouds. We extract the 3D transformation matrices to register the point clouds from the two sensors and generate the input point cloud to the system. For the LSTM model, we use the generated trajectory data for training and validation with a 60-40 split using the implementation provided for \cite{trajnet}. Blockage status ground truth data for each human in a scene are generated for each frame using the raytracing functionality in the 3D model in Blender. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{Blensor.png}
    \caption{An example instance in Blensor LiDAR data generation. Left figure is the 3D modelled environment in Blender and the right figure shows generated LiDAR point clouds based on the 3D model.}
    \label{fig:blensor}
\end{figure}



\subsection{Implementation}

The implementation of the system was carried out on the Robot Operating System (ROS) which is a software tool platform for building robots. %ROS supports point cloud processing with PCL as LiDARs, depth cameras are heavily used when robots are built. Another key advantage in using ROS is, it provides a server-client platform for passing different data between entities called nodes which carry out different tasks. It also provides the flexibility of using multiple languages in our implementation as PCL library is written in C++ while PyTorch uses python which we used in the LSTM network.%
The three stages  outlined in the system as in Fig. \ref{fig:system} are launched separately while the data transfer is done thought ROS messages. The human  detection and tracking stage implementation is an adapted version of the open source code available for \cite{koide}. We generate the globalmap cloud from Blensor without any humans and load it to the system prior to the start. The input point clouds are published to the system by emulating the sensors from the stored point clouds in the data generation stage. %This method helps us to evaluate real sensor data which will be done as our future work. 
The detection and tracking nodes publish the positions and bounding boxes of the humans in the scene which is subscribed by the trajectory predictor node running the second stage. For the LSTM model, we used an embedding layer dimension of 64 and LSTM  hidden state dimension of 512 which was trained using the 2D data generated from ORCA simulator for 30 epochs. % with \hl{L2} loss and the model parameters are stored%.For the inference with the system, we load the pretrained model in ROS and provide past trajectory data which comes out of the first stage. 
The inference is done utilizing a 10-frame sliding window for past trajectory data provided from the detection and tracking data. The predicted trajectories are sent to the ray casting node which evaluates blockage status of each tracked human in the prediction window.

%At the start the trajectory predictor will wait to accumulate 10 frames and start predicting the next 5 frames. When a new frame tracking data becomes available from the first stage, trajectory predictor will remove the earliest data from the window and add the latest data to generate a new prediction. The predicted positions are published by the trajectory predictor node which is subscribed by the raycaster node. The raycaster node also subscribes to the human cluster data associated with the predicted positions and the globalmap. Based on the subscribed data, the synthesized clouds are generated which are then associated with octrees with a resolution \hl{10cm}. Then raycasting is done for the each human for each future frame for 5 frames to identify whether the centroid of each human is blocked or not. 

\section{Results and Discussion}
In this section we present the results of the proposed method based on the simulation data generated. We evaluated the proposed system with 500 scenes from the test dataset generated. Visual inspection of the  trajectory prediction results as in Fig. \ref{fig:trajectory} shows satisfactory agreement with the ground truth trajectories. It can be seen that the LSTM model successfully adapts to the non-linear trajectories of the humans. Average displacement error (ADE) in the test dataset with the future time step is given in Table \ref{table1}  for the trained LSTM model. The error increases as the prediction time step increases.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\linewidth]{trajs.png}
    \caption{Top view visualization of the trajectory prediction. Magenta lines show the ground truth path while yellow lines are the predicted paths based on LSTM method.}
    \label{fig:trajectory}
\end{figure}


\begin{table}
\caption{Average displacement error with future time step (metres) }\label{table1}
\begin{tabular}{|P{\mylength}|P{\mylength}|P{\mylength}|P{\mylength}|P{\mylength}|P{\mylength}|P{\mylength}|P{\mylength}|P{\mylength}|}
 \hline
  1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
  \hline
 0.22 & 0.27 & 0.34 & 0.42 & 0.51 & 0.61 & 0.72 & 0.83 & 0.96 \\ 
 \hline
\end{tabular}
\end{table}

The blockage prediction problem is a binary classification problem in the future window. Therefore, we evaluated our proposed methods with evaluation metrics for binary classification. The accuracy is defined as the total number of correct results from all the samples given which means correctly identified LOS windows and NLOS windows out of all the instances considered. As shown in Fig. \ref{results}, both AABB and OBB methods achieve an accuracy around 88\% and for shorter prediction windows and decrease monotonically as we try to predict further into the future since the ADE increases with the time steps. This problem is inherently an unbalanced problem since there will be more LOS instances than NLOS instances which means accuracy itself is not sufficient to evaluate the method. Therefore, we look at the metrics precision, recall and $F_1$ score for more insights. $F_1$ score gives the harmonic mean between precision and recall and a high $F_1$ score means a better predictor.
% \begin{subequations}
% \begin{align}
%     Precision &= \frac{TP}{TP+FP},\quad Recall = \frac{TP}{TP+FN} \\
%     F_{\beta}\ score &= 2*\frac{ Precision * Recall}{Precision + Recall}
% \end{align}
% \end{subequations}
For this blockage prediction scenario, precision indicates the fraction of blockage windows correctly predicted from the all the predicted blockage windows while recall indicates the fraction of blockage windows correctly predicted from the true blocked windows. 

% The $F_{1}$ score calculates the harmonic mean between precision and recall.

% Now consider the two errors that can occur with the method. A false positive (overestimation) means the transmitter algorithm might decide to avoid  transmitting to a LOS UE as the there is a false alarm in the prediction window. But a false negative (underestimation) instance means the transmitter will decide to transmit to a NLOS UE as the UE is detected as LOS which eventually will waste the radio resources as the link is interrupted in the considered window. Therefore the cost of having a false negative is higher, which means more importance should be given to the recall metric. Therefore we evaluate the performance of the method with  

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{final.eps}
    \caption{Evaluation metric scores vs. window size}
    \label{results}
\end{figure}

Clearly, the recall in both methods increases as the window size increases in Fig. \ref{results}. The reason is because the prediction window is considered blocked if there is at least one blockage event during the window. As the window size increases, the probability of having blockage instances in the window increases which results in fewer false negatives. Concurrently, the precision drops as the number of false positives increase for the same reason. The $F_1$ score settles down around 0.815 when the window size is increased to 8 for both methods.
When comparing the two methods, OBB method outperforms the AABB method in recall although precision is higher in AABB method and both methods have very similar accuracy values. However, $F_1$ score is higher with OBB method which means it performs better. Nevertheless, OBB method involves higher complexity when transforming all the rays in to the object space due to the matrix inversions used in the transformations. Therefore, with a trade-off between complexity and performance, AABB method can be considered as the better option. The preferable window size for the prediction can be chosen as 3 which corresponds to 300 ms when the system is running at 10 Hz as both methods have the intersection of precision and recall around window size  of 3. 

We observed tracking failures are a significant source of error in the proposed system. We used a Kalman filter and point cloud processing to detect and track the humans which can further be improved with deep learning-based detection and tracking of humans. Moreover, we used the Vanilla LSTM model to predict a human trajectory. As humans move with respect to their dynamics of the surroundings incorporating interactions between the humans can improve the performance of the trajectory prediction \cite{trajnet} which should improve the blockage prediction accuracy. Furthermore, we completely relied on the visual data in this work, and incorporating information from the wireless algorithms such as received power has the ability to increase the prediction accuracy further which will be considered in our future work. 

\section{Conclusions}
In this work, we proposed a system for dynamic blockage prediction in an indoor scenario utilizing visual information from infrastructure-mounted LiDAR sensors. Our method consisted of 3 stages with point cloud processing, trajectory prediction, and raycasting. We demonstrated that the LiDAR data can be used to predict incoming blockages with a satisfactory performance which will aid the higher frequency systems which will be used in 6G. The proposed system achieved an accuracy 87\% with a precision of 78\% and a recall of 79\% for a prediction window of 300 ms. In future work, evaluating the proposed system with real LiDAR data and the use of data from the wireless system to improve the solution will be explored. 

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%

\begin{comment}

\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\section*{Acknowledgment}


The authors would like to thank...
\end{comment}

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{bibi}
%
\printbibliography
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{comment}

\begin{IEEEbiography}{Michael Shell}
Biography text here.
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{John Doe}
Biography text here.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

\begin{IEEEbiographynophoto}{Jane Doe}
Biography text here.
\end{IEEEbiographynophoto}

\end{comment}
% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


