% !TEX root = suppmain.tex

\section{Dataset}
Meta-Dataset~\cite{triantafillou2019meta} is a few-shot classification benchmark that initially consists of ten datasets: ILSVRC\_2012~\cite{russakovsky2015imagenet} (ImageNet),  Omniglot~\cite{Lake1332}, FGVC-Aircraft~\cite{maji2013fine} (Aircraft), CUB-200-2011~\cite{wah2011caltech} (Birds), Describable Textures~\cite{cimpoi2014describing} (DTD), QuickDraw~\cite{jongejan2016quick}, FGVCx Fungi~\cite{brigit2018fungi} (Fungi), VGG Flower~\cite{nilsback2008automated} (Flower), Traffic Signs~\cite{houben2013detection} and MSCOCO~\cite{lin2014microsoft} then further expands with MNIST~\cite{lecun1998gradient}, CIFAR-10~\cite{krizhevsky2009learning} and CIFAR-100~\cite{krizhevsky2009learning}. We follow the standard procedure in~\cite{triantafillou2019meta} and consider both the `Training on all datasets' (multi-domain learning) and `Training on ImageNet only' (single-domain learning) settings.
In `Training on all datasets' setting, we follow the standard procedure and use the first eight datasets for meta-training, in which each dataset is further divided into train, validation and test set with disjoint classes. 
While the evaluation within these datasets is used to measure the generalization ability in the seen domains, the remaining five datasets are reserved as unseen domains in meta-test for measuring the cross-domain generalization ability.
In `Training on ImageNet only' setting, we follow the standard procedure and only use train split of ImageNet for meta-training. The evaluation of models is in the test split of ImageNet and the rest 12 datasets which are reserved as unseen domains in meta-test.
As in~\cite{triantafillou2019meta}, we evaluate our method on 600 randomly sampled tasks for each dataset with varying number of ways and shots, and report average accuracy and 95\% confidence score in all experiments.

\section{Implementation details}


In this section, we explain the details of task-agnostic (feature extractor) learning and then task-specific (adapter) learning.


\subsection{Task-agnostic learning}

Here we consider learning the parameters of the feature extractor from either multiple or single domains.





\paragraph{Multi-domain learning.}
When we learn the feature extractor from multiple domains, we consider two cases.
In the first case, which we call vanilla multiple domain learning (or MDL), we design a deep network where we share all the layers across all domains and have domain-specific classifiers.
This setting corresponds to Eq~(1) in the main text.
Second we consider a variant of MDL, URL~\cite{li2021universal} which also involves learning a single network with shared and domain-specific layers as such, however, it is learned by distilling information from multiple domain-specific networks as described \cite{li2021universal}.
In these two settings, as in~\cite{dvornik2020selecting,bateni2020improved,li2021universal}, we build  MDL and URL on the ResNet-18~\cite{he2016deep} backbone and use $84\times 84$ image size.



For optimization of both MDL and URL, we follow the same protocol in~\cite{li2021universal}, use SGD optimizer and cosine annealing with a weight decay of $7\times 10^{-4}$  for learning 240,000 iterations. The learning rate is 0.03 and the annealing frequency is 48,000. As in~\cite{li2021universal}, the batch size for ImageNet is $64\times 7$ and is $64$ for the other 7 datasets. We refer readers to \cite{li2021universal} for more details.





\paragraph{Single domain learning (SDL).}

We also evaluate our method on a feature extractor that is learned on single domain which we call SDL.
Here we evaluate our method on two backbones, ResNet-18 (SDL-ResNet-18) and ResNet34 (SDL-ResNet-34).


\begin{table}[h!]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{ccccc}
		    \toprule
		    Backbone & learning rate & batch size & annealing freq. & max. iter. \\
		    \midrule
		    SDL-ResNet-18 & $3\times 10^{-2}$ & 64 & 48,000 & 480,000\\
		    SDL-ResNet-34 & $3\times 10^{-2}$ & 128 & 48,000 & 480,000\\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Training hyper-parameters of single domain learning.}
		\label{supptab:hyperparams}
\end{table}%



% using imagenet pretrain model, mdl model, url
\begin{table*}[ht]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    MDL & $53.4 \pm 1.1$ & $93.8 \pm 0.4$ & $86.6 \pm 0.5$ & $78.6 \pm 0.8$ & $71.4 \pm 0.7$ & $81.5 \pm 0.6$ & $61.9 \pm 1.0$ & $88.7 \pm 0.6$ & $51.0 \pm 1.0$ & $49.7 \pm 1.1$ & $94.4 \pm 0.3$ & $66.7 \pm 0.8$ & $53.6 \pm 1.0$ \\
		    Ours (MDL) & ${\bf 55.6 \pm 1.0}$ & ${\bf 94.3 \pm 0.4}$ & ${\bf 86.7 \pm 0.5}$ & ${\bf 79.4 \pm 0.8}$ & ${\bf 73.2 \pm 0.8}$ & ${\bf 81.7 \pm 0.6}$ & ${\bf 64.0 \pm 0.9}$ & ${\bf 90.9 \pm 0.5}$ & ${\bf 81.1 \pm 0.9}$ & ${\bf 51.4 \pm 1.1}$ & ${\bf 96.9 \pm 0.3}$ & ${\bf 78.5 \pm 0.8}$ & ${\bf 64.3 \pm 1.1}$ \\
		    \midrule
		    URL~\cite{li2021universal} & $58.8 \pm 1.1$ & $94.5 \pm 0.4$ & $89.4 \pm 0.4$ & $80.7 \pm 0.8$ & $77.2 \pm 0.7$ & ${\bf 82.5 \pm 0.6}$ & ${\bf 68.1 \pm 0.9}$ & $92.0 \pm 0.5$ & $63.3 \pm 1.2$ & $57.3 \pm 1.0$ & $94.7 \pm 0.4$ & $74.2 \pm 0.8$ & $63.6 \pm 1.0$ \\
		    Ours (URL) & ${\bf 59.5 \pm 1.0}$ & ${\bf94.9 \pm 0.4}$ & ${\bf89.9 \pm 0.4}$ & ${\bf 81.1 \pm 0.8}$ & ${\bf77.5 \pm 0.7}$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & ${\bf 92.2 \pm 0.5}$ & ${\bf 82.8 \pm 1.0}$ & ${\bf 57.6 \pm 1.0}$ & ${\bf 96.7 \pm 0.4}$ & ${\bf 82.9 \pm 0.7}$ & ${\bf 70.4 \pm 1.0}$ \\
		    \midrule
		    SDL-ResNet-18 & $55.8 \pm 1.0$ & $67.4 \pm 1.2$ & $49.5 \pm 0.9$ & $71.2 \pm 0.9$ & $73.0 \pm 0.6$ & $53.9 \pm 1.0$ & $41.6 \pm 1.0$ & $87.0 \pm 0.6$ & $47.4 \pm 1.1$ & $53.5 \pm 1.0$ & $78.1 \pm 0.7$ & $67.3 \pm 0.8$ & $56.6 \pm 0.9$ \\
		    Ours (SDL-ResNet-18) & ${\bf 59.5 \pm 1.1}$ & ${\bf 78.2 \pm 1.2}$ & ${\bf 72.2 \pm 1.0}$ & ${\bf 74.9 \pm 0.9}$ & ${\bf 77.3 \pm 0.7}$ & ${\bf 67.6 \pm 0.9}$ & ${\bf 44.7 \pm 1.0}$ & ${\bf 90.9 \pm 0.6}$ & ${\bf 82.5 \pm 0.8}$ & ${\bf 59.0 \pm 1.0}$ & ${\bf 93.9 \pm 0.6}$ & ${\bf 82.1 \pm 0.7}$ & ${\bf 70.7 \pm 0.9}$ \\
		    \midrule
		    SDL-ResNet-34 & $62.2 \pm 1.1$ & $72.8 \pm 1.1$ & $62.9 \pm 0.9$ & $79.6 \pm 0.8$ & $75.6 \pm 0.6$ & $64.5 \pm 0.8$ & $47.4 \pm 1.1$ & $90.4 \pm 0.6$ & $54.8 \pm 1.0$ & $56.1 \pm 1.0$ & $79.3 \pm 0.6$ & $83.0 \pm 0.6$ & $74.8 \pm 0.8$  \\
		    Ours (SDL-ResNet-34) & ${\bf 63.7 \pm 1.0}$ & ${\bf 82.6 \pm 1.1}$ & ${\bf 80.1 \pm 1.0}$ & ${\bf 83.4 \pm 0.8}$ & ${\bf 79.6 \pm 0.7}$ & ${\bf 71.0 \pm 0.8}$ & ${\bf 51.4 \pm 1.2}$ & ${\bf 94.0 \pm 0.5}$ & ${\bf 81.7 \pm 0.9}$ & ${\bf 61.7 \pm 0.9}$ & ${\bf 94.6 \pm 0.5}$ & ${\bf 86.0 \pm 0.6}$ & ${\bf 78.3 \pm 0.8}$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.35cm}
		\caption{Results of attaching residual adapters to different baselines. `SDL-ResNet-18' is the single domain model with ResNet-18 backbone pretrained on ImageNet. `SDL-ResNet-34' is the single domain model with ResNet-34 backbone pretrained on ImageNet. `MDL' is a vanilla Multi-Domain Learning (MDL) model trained on eight seen datasets jointly.}
		\label{supptab:baselines}
\end{table*}%

\begin{table*}[t]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccccccccc|ccccc}

		    \toprule
		    \multirow{2}{*}{Test Dataset} & \multirow{2}{*}{classifier} & Aux-Net & serial or & M or & \multirow{2}{*}{$\beta$} & \multirow{2}{*}{\#params} & \multirow{2}{*}{ImageNet} & \multirow{2}{*}{Omniglot} & \multirow{2}{*}{Aircraft} & \multirow{2}{*}{Birds} & \multirow{2}{*}{Textures} & \multirow{2}{*}{Quick Draw} & \multirow{2}{*}{Fungi} & \multirow{2}{*}{VGG Flower} & \multirow{2}{*}{Traffic Sign} & \multirow{2}{*}{MSCOCO} & \multirow{2}{*}{MNIST} & \multirow{2}{*}{CIFAR-10} & \multirow{2}{*}{CIFAR-100}\\
		    & & or Ad & parallel & CW & & & & & & & & & & & & & & & \\
		    \midrule
		    NCC & NCC & - & - & - & \XSolidBrush &  - & $57.0\pm1.1$ & $94.4\pm0.4$ & $88.0\pm0.5$ & $80.3\pm0.7$ & $74.6\pm0.7$ & $81.8\pm0.6$ & $66.2\pm0.9$ & $91.5\pm0.5$ & $49.8\pm1.1$ & $54.1\pm1.0$ & $91.1\pm0.4$ & $70.6\pm0.7$& $59.1\pm1.0$\\ 
		    MD & MD & - & - & - & \XSolidBrush & - & $53.9\pm1.0$ & $93.8\pm0.5$ & $87.6\pm0.5$ & $78.3\pm0.7$ & $73.7\pm0.7$ & $80.9\pm0.7$ & $57.7\pm0.9$ & $89.7\pm0.6$ & $62.2\pm1.1$ & $48.5\pm1.0$ & $95.1\pm0.4$ & $68.9\pm0.8$ & $60.0\pm0.9$ \\ 
		    % \midrule
		    LR & LR & - & - & - & \XSolidBrush & - & $56.0\pm1.1$ & $93.7\pm0.5$ & $88.3\pm0.6$  & $79.7\pm0.8$ & $74.7\pm0.7$ & $80.0\pm0.7$ & $62.1\pm0.8$ & $91.1\pm0.5$ & $59.7\pm1.1$ & $51.2\pm1.1$ & $93.5\pm0.5$  & $73.1\pm0.8$ & $60.1\pm1.1$ \\
		    SVM & SVM & - & - & - & \XSolidBrush & - & $54.5\pm1.1$ & $94.3\pm0.5$ & $87.7\pm0.5$ & $78.1\pm0.8$ & $73.8\pm0.8$ & $80.0\pm0.6$ & $58.5\pm0.9$ & $91.4\pm0.6$ & $65.7\pm1.2$ & $50.5\pm1.0$ & $95.4\pm0.4$ & $72.0\pm0.8$ & $60.5\pm1.1$ \\
		    Softmax & Softmax & - & - & - & \XSolidBrush & - & $42.2 \pm 1.0$ & $85.3 \pm 0.7$ & $71.9 \pm 0.8$ & $59.6 \pm 1.0$ & $62.0 \pm 0.8$ & $61.2 \pm 1.0$ & $37.3 \pm 0.9$ & $66.7 \pm 1.0$ & $51.4 \pm 1.1$ & $48.2 \pm 1.1$ & $93.5 \pm 0.5$ & $70.4 \pm 0.8$ & $59.3 \pm 1.0$ \\
		    KNN & KNN & - & - & - & \XSolidBrush & - & $48.1 \pm 1.1$ & $94.1 \pm 0.4$ & $84.5 \pm 0.6$ & $70.7 \pm 0.8$ & $65.9 \pm 0.8$ & $74.8 \pm 0.7$ & $53.5 \pm 0.9$ & $86.0 \pm 0.6$ & $56.9 \pm 1.2$ & $44.7 \pm 1.1$ & $91.4 \pm 0.5$ & $60.3 \pm 0.8$ & $49.4 \pm 1.0$ \\
		    \midrule
		    PA & NCC & - & - & - & \Checkmark & - & $58.8 \pm 1.1$ & $94.5 \pm 0.4$ & $89.4 \pm 0.4$ & $80.7 \pm 0.8$ & $77.2 \pm 0.7$ & ${\bf 82.5 \pm 0.6}$ & ${\bf 68.1 \pm 0.9}$ & $92.0 \pm 0.5$ & $63.3 \pm 1.1$ & $57.3 \pm 1.0$ & $94.7 \pm 0.4$ & $74.2 \pm 0.8$ & $63.5 \pm 1.0$ \\
		    PA & Softmax & - & - & - & \Checkmark & - & $53.4 \pm 1.2$ & $92.7 \pm 0.5$ & $85.7 \pm 0.6$ & $76.1 \pm 0.9$ & $73.9 \pm 0.8$ & $76.5 \pm 0.8$ & $51.1 \pm 0.9$ & $86.9 \pm 0.7$ & $52.5 \pm 1.1$ & $48.2 \pm 1.1$ & $94.3 \pm 0.4$ & $69.7 \pm 0.8$ & $60.4 \pm 1.0$ \\
		    \midrule
		    Finetune & NCC & - & - & - & \XSolidBrush & - & $55.9 \pm 1.2$ & $94.0 \pm 0.5$ & $87.3 \pm 0.6$ & $77.8 \pm 0.9$ & $76.8 \pm 0.8$ & $75.3 \pm 0.9$ & $57.6 \pm 1.1$ & $91.5 \pm 0.6$ & ${\bf 86.1 \pm 0.9}$ & $53.1 \pm 1.2$ & ${\bf 96.8 \pm 0.4}$ & $80.9 \pm 0.8$ & $65.9 \pm 1.1$ \\
		    Finetune & Softmax & - & - & - & \XSolidBrush & - & $48.4 \pm 1.2$ & $92.2 \pm 0.6$ & $81.6 \pm 0.9$ & $70.3 \pm 1.3$ & $72.0 \pm 0.9$ & $73.5 \pm 1.0$ & $44.2 \pm 1.1$ & $90.3 \pm 0.7$ & $65.5 \pm 1.4$ & $41.0 \pm 1.3$ & $96.3 \pm 0.4$ & $71.6 \pm 1.0$ & $53.8 \pm 1.4$ \\
		    \midrule
		    Aux-S-CW & NCC & Aux-Net & serial & CW & \XSolidBrush & - & $54.6\pm1.1$ & $93.5\pm0.5$ &  $86.6\pm0.5$ & $78.6\pm0.8$ &  $71.5\pm0.7$ &   $79.3\pm0.6$ &  $66.0\pm0.9$ &   $87.6\pm0.6$ &  $43.3\pm0.9$ & $49.1\pm1.0$ & $87.9\pm0.5$ &  $62.8\pm0.8$ &  $51.5\pm1.0$  \\
		    Aux-R-CW & NCC & Aux-Net & residual & CW & \XSolidBrush & - & $56.1\pm1.1$ &  $94.2\pm0.4$ & $88.4\pm0.5$ & $80.6\pm0.7$ & $74.9\pm0.6$ &  $82.0\pm0.6$ & $66.4\pm0.9$ & $91.6\pm0.5$ & $48.5\pm1.0$ & $53.5\pm1.0$ & $90.8\pm0.5$ &  $70.2\pm0.8$ & $59.7\pm1.0$ \\
		    Aux-S-CW & MD & Aux-Net & serial & CW & \XSolidBrush & - & $55.1\pm1.1$ &$93.8\pm0.5$ &$86.8\pm0.5$ & $77.4\pm0.8$ & $73.2\pm0.8$ &$79.9\pm0.7$ &$57.4\pm0.9$ & $88.1\pm0.7$ & $58.4\pm1.1$ & $50.1\pm1.1$ & $92.7\pm0.5$ & $66.5\pm0.8$ & $55.7\pm1.1$ \\
		    Aux-R-CW & MD & Aux-Net & residual & CW & \XSolidBrush & - & $54.8\pm1.1$ & $93.8\pm0.5$ & $87.4\pm0.5$ & $78.2\pm0.7$ & $73.4\pm0.7$ & $81.1\pm0.7$ & $58.8\pm0.9$ & $90.1\pm0.5$ & $63.6\pm1.2$ & $48.5\pm1.1$ & $94.8\pm0.4$ & $69.6\pm0.8$ & $60.6\pm0.9$ \\
		    \midrule
		    Ad-S-CW & NCC & Ad & serial & CW & \XSolidBrush & 0.06\% & $56.8 \pm 1.1$ & $94.8 \pm 0.4$ & $89.3 \pm 0.5$ & $80.7 \pm 0.7$ & $74.5 \pm 0.7$ & $81.6 \pm 0.6$ & $65.8 \pm 0.9$ & $91.3 \pm 0.5$ & $73.9 \pm 1.1$ & $53.6 \pm 1.1$ & $95.7 \pm 0.4$ & $78.4 \pm 0.7$ & $64.3 \pm 1.0$ \\
		    Ad-R-CW & NCC & Ad & residual & CW & \XSolidBrush & 1.57\%   & $57.6 \pm 1.1$ & $94.7 \pm 0.4$ & $89.0 \pm 0.4$ & $81.2 \pm 0.8$ & $75.2 \pm 0.7$ & $81.5 \pm 0.6$ & $65.4 \pm 0.8$ & $91.8 \pm 0.5$ & $79.2 \pm 1.1$ & $54.7 \pm 1.1$ & $96.4 \pm 0.4$ & $79.5 \pm 0.8$ & $67.4 \pm 1.0$ \\
		    Ad-S-M & NCC & Ad & serial & M & \XSolidBrush & 12.50\% & $56.2 \pm 1.1$ & $94.4 \pm 0.4$ & $89.1 \pm 0.5$ & $80.6 \pm 0.7$ & $75.8 \pm 0.7$ & $81.6 \pm 0.6$ & $67.1 \pm 0.9$ & $92.1 \pm 0.4$ & $67.6 \pm 1.2$ & $54.8 \pm 1.1$ & $95.9 \pm 0.4$ & $78.9 \pm 0.7$ & $66.6 \pm 1.1$ \\
		    Ad-R-M & NCC & Ad & residual & M & \XSolidBrush & 10.93\%  & $57.3 \pm 1.1$ & $94.9 \pm 0.4$ & $88.9 \pm 0.5$ & $81.0 \pm 0.7$ & $76.7 \pm 0.7$ & $80.6 \pm 0.6$ & $65.4 \pm 0.9$ & $91.4 \pm 0.5$ & $82.6 \pm 1.0$ & $55.0 \pm 1.1$ & $96.6 \pm 0.4$ & $82.1 \pm 0.7$ & $66.4 \pm 1.1$ \\
		    \midrule
		    Ad-R-CW-PA & NCC & Ad & residual & CW & \Checkmark & 3.91\% & $58.6 \pm 1.1$ & $94.5 \pm 0.4$ & ${\bf 90.0 \pm 0.4}$ & $80.5 \pm 0.8$ & ${\bf 77.6 \pm 0.7}$ & $81.9 \pm 0.6$ & $67.0 \pm 0.9$ & $92.2 \pm 0.5$ & $80.2 \pm 0.9$ & $57.2 \pm 1.0$ & $96.1 \pm 0.4$ & $81.5 \pm 0.8$ & ${\bf 71.4 \pm 0.9}$ \\
		    Ad-R-M-PA & NCC & Ad & residual & M & \Checkmark & 13.27\% & ${\bf 59.5 \pm 1.0}$ & ${\bf94.9 \pm 0.4}$ & $89.9 \pm 0.4$ & ${\bf 81.1 \pm 0.8}$ & $77.5 \pm 0.7$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & ${\bf 92.2 \pm 0.5}$ & $82.8 \pm 1.0$ & ${\bf 57.6 \pm 1.0}$ & $96.7 \pm 0.4$ & ${\bf 82.9 \pm 0.7}$ & $70.4 \pm 1.0$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Comparisons to methods that learn classifiers and model adaptation methods during meta-test stage based on URL model. NCC, MD, LR, SVM, Softmax, KNN denote nearest centroid classifier, Mahalanobis distance, logistic regression, support vector machines, softmax classifier and k-nearest neighbors classifier respectively. PA indicates pre-classifier alignment. `Aux-Net or Ad' indicates using Auxiliary Network to predict $\alpha$ or attaching adapter $\alpha$ directly. `M or CW' means using matrix multiplication or channel-wise scaling adapters. 'S' and 'R' denote serial adapter and residual adapter, respectively. `$\beta$' indicates using the pre-classifier adaptation. Mean accuracy, 95\% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen and used for test only.}
		\label{supptab:testad}
\end{table*}%






\begin{table*}[t]
% \vspace{-0.25cm}
	\centering
	    \resizebox{0.95\textwidth}{!}
    {
		\begin{tabular}{cccccc|ccccc}
			& \multicolumn{4}{c}{Varying-Way Five-Shot} & \multicolumn{4}{c}{Five-Way One-Shot} \\
		    \toprule
		    \multirow{2}{*}{Test Dataset} & Simple & SUR & URT & URL & \multirow{2}{*}{Ours}& Simple & SUR & URT & URL & \multirow{2}{*}{Ours}\\
		    &  CNAPS~\cite{bateni2020improved}  & \cite{dvornik2020selecting} & \cite{liu2020universal} & \cite{li2021universal} & & CNAPS~\cite{bateni2020improved} & \cite{dvornik2020selecting}& \cite{liu2020universal} & \cite{li2021universal} & \\
		    \midrule
		    ImageNet & $47.2\pm1.0$& $46.7\pm1.0$& $48.6\pm1.0$& ${\bf 49.4 \pm 1.0}$ & $48.3 \pm 1.0$ & $42.6\pm0.9$& $40.7\pm1.0$& $47.4\pm1.0$& ${\bf 49.6 \pm 1.1}$ & $48.0 \pm 1.0$ \\
		    Omniglot & $95.1\pm0.3$& $95.8\pm0.3$& $96.0\pm0.3$& $96.0 \pm 0.3$ & ${\bf 96.8 \pm 0.3}$ & $93.1\pm0.5$& $93.0\pm0.7$& $95.6\pm0.5$& $95.8 \pm 0.5$ & ${\bf 96.3 \pm 0.4}$ \\
		    Aircraft & $74.6\pm0.6$& $82.1\pm0.6$& $81.2\pm0.6$& $84.8 \pm 0.5$ & ${\bf 85.5 \pm 0.5}$  & $65.8\pm0.9$&  $67.1\pm1.4$& $77.9\pm0.9$& ${\bf 79.6 \pm 0.9}$ & ${\bf 79.6 \pm 0.9}$ \\
		    Birds & $69.6\pm0.7$& $62.8\pm0.9$& $71.2\pm0.7$& $76.0 \pm 0.6$ & ${\bf 76.6 \pm 0.6}$  &  $67.9\pm0.9$& $59.2\pm1.0$& $70.9\pm0.9$& ${\bf 74.9 \pm 0.9}$ & $74.5 \pm 0.9$ \\
		    Textures & $57.5\pm0.7$& $60.2\pm0.7$& $65.2\pm0.7$& ${\bf 69.1 \pm 0.6}$ & $68.3 \pm 0.7$  &$42.2\pm0.8$& $42.5\pm0.8$& $49.4\pm0.9$& $53.6 \pm 0.9$ & ${\bf 54.5 \pm 0.9}$ \\
		    Quick Draw & $70.9\pm0.6$& $79.0\pm0.5$& ${\bf 79.2\pm0.5}$& $78.2 \pm 0.5$ & $77.9 \pm 0.6$  &$70.5\pm0.9$& ${\bf 79.8\pm0.9}$& $79.6\pm0.9$& $79.0 \pm 0.8$ & $79.3 \pm 0.9$ \\
		    Fungi & $50.3\pm1.0$& $66.5\pm0.8$& $66.9\pm0.9$& $70.0 \pm 0.8$ & ${\bf 70.4 \pm 0.8}$  & $58.3\pm1.1$& $64.8\pm1.1$& $71.0\pm1.0$& $75.2 \pm 1.0$ & ${\bf 75.3 \pm 1.0}$ \\
		    VGG Flower & $86.5\pm0.4$& $76.9\pm0.6$& $82.4\pm0.5$& $89.3 \pm 0.4$ & ${\bf 89.5 \pm 0.4}$  & $79.9\pm0.7$& $65.0\pm1.0$& $72.7\pm0.0$& $79.9 \pm 0.8$ & ${\bf 80.3 \pm 0.8}$ \\
		    \midrule
		    Traffic Sign & $55.2\pm0.8$& $44.9\pm0.9$& $45.1\pm0.9$& $57.5 \pm 0.8$ & ${\bf 72.3 \pm 0.6}$ & $55.3\pm0.9$& $44.6\pm0.9$& $52.7\pm0.9$& ${\bf 57.9 \pm 0.9}$ & $57.2 \pm 1.0$ \\
		    MSCOCO & $49.2\pm0.8$& $48.1\pm0.9$& $52.3\pm0.9$& ${\bf 56.1 \pm 0.8}$ & $56.0 \pm 0.8$ & $48.8\pm0.9$& $47.8\pm1.1$& $56.9\pm1.1$& $59.2 \pm 1.0$ & ${\bf 59.9 \pm 1.0}$ \\
		    MNIST & $88.9\pm0.4$& $90.1\pm0.4$& $86.5\pm0.5$& $89.7 \pm 0.4$ & ${\bf 92.5 \pm 0.4}$  & ${\bf 80.1\pm0.9}$& $77.1\pm0.9$& $75.6\pm0.9$& $78.7 \pm 0.9$ & ${\bf 80.1 \pm 0.9}$ \\
		    CIFAR-10 & $66.1\pm0.7$& $50.3\pm1.0$& $61.4\pm0.7$& $66.0 \pm 0.7$ & ${\bf 72.0 \pm 0.7}$  & $50.3\pm0.9$& $35.8\pm0.8$& $47.3\pm0.9$& $54.7 \pm 0.9$ & ${\bf 55.8 \pm 0.9}$ \\
		    CIFAR-100 & $53.8\pm0.9$& $46.4\pm0.9$& $52.5\pm0.9$& $57.0 \pm 0.9$ & ${\bf 64.1 \pm 0.8}$  & $53.8\pm0.9$& $42.9\pm1.0$& $54.9\pm1.1$& $61.8 \pm 1.0$ & ${\bf 63.7 \pm 1.0}$ \\
		    \midrule
		    Average Seen & $69.0$ & $71.2$ & $73.8$ & $76.6$ & ${\bf 76.7}$ & $65.0$ & $64.0$ & $70.6$ & $73.4$ & ${\bf 73.5}$ \\
		    Average Unseen & $62.6$ & $56.0$ & $59.6$ & $65.2$ & ${\bf 71.4}$ & $57.7$ & $49.6$ & $57.5$ & $62.4$ & ${\bf 63.4}$ \\
		    Average All & $66.5$ & $65.4$ & $68.3$ & $72.2$ & ${\bf 74.6}$ & $62.2$ & $58.5$ & $65.5$ & $69.2$ & ${\bf 69.6}$ \\
		    \midrule
		    Average Rank & $4.1$ & $3.9$ & $3.4$ & $2.1$ & ${\bf 1.5}$ & $3.8$ & $4.5$ & $3.3$ & ${\bf 1.7}$ & ${\bf 1.7}$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Results of Varying-Way Five-Shot and Five-Way One-Shot scenarios. Mean accuracy, 95\% confidence interval are reported.}
		\label{supptab:fixedshot}
\end{table*}%

% Comparison with state-of-the-art methods
\begin{table*}[h!]
	\centering
    \resizebox{0.95\textwidth}{!}
    {
		\begin{tabular}{ccccccccccccc}

		    \toprule
		    Test Dataset & CNAPS~\cite{requeima2019fast} & Simple CNAPS~\cite{bateni2020improved} & TransductiveCNAPS~\cite{bateni2020enhancing} & SUR~\cite{dvornik2020selecting} & URT~\cite{liu2020universal} & FLUTE~\cite{triantafillou2021flute} & tri-M~\cite{liu2021multi} & URL~\cite{li2021universal} & Ours\\
		    \midrule
			ImageNet & $50.8 \pm 1.1$ & $56.5 \pm 1.1$ & $57.9 \pm 1.1$ & $54.5 \pm 1.1$ & $55.0 \pm 1.1$ & $51.8 \pm 1.1$ & ${\bf 58.6 \pm 1.0}$ & $57.5 \pm 1.1$ & $57.4 \pm 1.1$ \\
			Omniglot & $91.7 \pm 0.5$ & $91.9 \pm 0.6$ & $94.3 \pm 0.4$ & $93.0 \pm 0.5$ & $93.3 \pm 0.5$ & $93.2 \pm 0.5$ & $92.0 \pm 0.6$ & $94.5 \pm 0.4$ & ${\bf 95.0 \pm 0.4}$ \\
			Aircraft & $83.7 \pm 0.6$ & $83.8 \pm 0.6$ & $84.7 \pm 0.5$ & $84.3 \pm 0.5$ & $84.5 \pm 0.6$ & $87.2 \pm 0.5$ & $82.8 \pm 0.7$ & $88.6 \pm 0.5$ & ${\bf 89.3 \pm 0.4}$ \\
			Birds & $73.6 \pm 0.9$ & $76.1 \pm 0.9$ & $78.8 \pm 0.7$ & $70.4 \pm 1.1$ & $75.8 \pm 0.8$ & $79.2 \pm 0.8$ & $75.3 \pm 0.8$ & $80.5 \pm 0.7$ & ${\bf 81.4 \pm 0.7}$ \\
			Textures & $59.5 \pm 0.7$ & $70.0 \pm 0.8$ & $66.2 \pm 0.8$ & $70.5 \pm 0.7$ & $70.6 \pm 0.7$ & $68.8 \pm 0.8$ & $71.2 \pm 0.8$ & $76.2 \pm 0.7$ & ${\bf 76.7 \pm 0.7}$ \\
			Quick Draw & $74.7 \pm 0.8$ & $78.3 \pm 0.7$ & $77.9 \pm 0.6$ & $81.6 \pm 0.6$ & ${\bf 82.1 \pm 0.6}$ & $79.5 \pm 0.7$ & $77.3 \pm 0.7$ & $81.9 \pm 0.6$ & $82.0 \pm 0.6$ \\
			Fungi & $50.2 \pm 1.1$ & $49.1 \pm 1.2$ & $48.9 \pm 1.2$ & $65.0 \pm 1.0$ & $63.7 \pm 1.0$ & $58.1 \pm 1.1$ & $48.5 \pm 1.0$ & ${\bf 68.8 \pm 0.9}$ & $67.4 \pm 1.0$ \\
			VGG Flower & $88.9 \pm 0.5$ & $91.3 \pm 0.6$ & ${\bf 92.3 \pm 0.4}$ & $82.2 \pm 0.8$ & $88.3 \pm 0.6$ & $91.6 \pm 0.6$ & $90.5 \pm 0.5$ & $92.1 \pm 0.5$ & $92.2 \pm 0.5$ \\
			\midrule
			Traffic Sign & $56.5 \pm 1.1$ & $59.2 \pm 1.0$ & $59.7 \pm 1.1$ & $49.8 \pm 1.1$ & $50.1 \pm 1.1$ & $58.4 \pm 1.1$ & $63.0 \pm 1.0$ & $63.3 \pm 1.2$ & ${\bf 83.5 \pm 0.9}$ \\
			MSCOCO & $39.4 \pm 1.0$ & $42.4 \pm 1.1$ & $42.5 \pm 1.1$ & $49.4 \pm 1.1$ & $48.9 \pm 1.1$ & $50.0 \pm 1.0$ & $52.8 \pm 1.1$ & $54.0 \pm 1.0$ & ${\bf 55.8 \pm 1.1}$ \\
			MNIST & - & $94.3 \pm 0.4$ & $94.7 \pm 0.3$ & $94.9 \pm 0.4$ & $90.5 \pm 0.4$ & $95.6 \pm 0.5$ & $96.2 \pm 0.3$ & $94.5 \pm 0.5$ & ${\bf 96.7 \pm 0.4}$ \\
			CIFAR-10 & - & $72.0 \pm 0.8$ & $73.6 \pm 0.7$ & $64.2 \pm 0.9$ & $65.1 \pm 0.8$ & $78.6 \pm 0.7$ & $75.4 \pm 0.8$ & $71.9 \pm 0.7$ & ${\bf 80.6 \pm 0.8}$ \\
			CIFAR-100 & - & $60.9 \pm 1.1$ & $61.8 \pm 1.0$ & $57.1 \pm 1.1$ & $57.2 \pm 1.0$ & $67.1 \pm 1.0$ & $62.0 \pm 1.0$ & $62.6 \pm 1.0$ & ${\bf 69.6 \pm 1.0}$ \\
			\midrule
			Average Seen & $71.6$ & $74.6$ & $75.1$ & $75.2$ & $76.7$ & $76.2$ & $74.5$ & $80.0$ & $80.2$ \\
			Average Unseen & - & $65.8$ & $66.5$ & $63.1$ & $62.4$ & $69.9$ & $69.9$ & $69.3$ & $77.2$ \\
			Average All & - & $71.2$ & $71.8$ & $70.5$ & $71.2$ & $73.8$ & $72.7$ & $75.9$ & $79.0$ \\
			\midrule
			Average Rank & - & $6.3$ & $4.9$ & $5.8$ & $5.7$ & $4.3$ & $4.8$ & $2.7$ & $1.5$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.35cm}
		\caption{Comparison state-of-the-art methods on Meta-Dataset (using a multi-domain feature extractor of \cite{li2021universal}). Mean accuracy, 95\% confidence interval are reported. The first eight datasets are seen during training and the last five datasets are unseen and used for test only.}
		\label{supptab:currmethod}
\end{table*}%

\begin{table*}[t]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    10 iterations & $55.5 \pm 1.1$ & $93.9 \pm 0.5$ & $86.4 \pm 0.5$ & $78.6 \pm 0.7$ & $73.3 \pm 0.7$ & $81.9 \pm 0.6$ & $63.1 \pm 0.9$ & $90.3 \pm 0.5$ & $77.6 \pm 1.0$ & $50.6 \pm 1.1$ & $96.9 \pm 0.3$ & $77.0 \pm 0.8$ & $62.6 \pm 1.1$ \\
		    20 iterations & $56.2 \pm 1.1$ & $94.7 \pm 0.4$ & $86.3 \pm 0.5$ & $78.3 \pm 0.8$ & $73.9 \pm 0.7$ & $81.6 \pm 0.6$ & $63.4 \pm 0.9$ & $90.1 \pm 0.6$ & $79.4 \pm 1.0$ & $52.8 \pm 1.1$ & $97.2 \pm 0.3$ & $78.6 \pm 0.8$ & $65.9 \pm 1.1$ \\
		    40 iterations & $55.6 \pm 1.0$ & $94.3 \pm 0.4$ & $86.7 \pm 0.5$ & $79.4 \pm 0.8$ & $73.2 \pm 0.8$ & $81.7 \pm 0.6$ & $64.0 \pm 0.9$ & $90.9 \pm 0.5$ & $81.1 \pm 0.9$ & $51.4 \pm 1.1$ & $96.9 \pm 0.3$ & $78.5 \pm 0.8$ & $64.3 \pm 1.1$ \\
		    60 iterations & $55.9 \pm 1.1$ & $95.1 \pm 0.4$ & $85.9 \pm 0.6$ & $77.5 \pm 0.8$ & $74.7 \pm 0.7$ & $80.9 \pm 0.6$ & $62.1 \pm 0.9$ & $90.7 \pm 0.6$ & $82.2 \pm 0.9$ & $52.2 \pm 1.1$ & $97.0 \pm 0.4$ & $78.4 \pm 0.8$ & $64.4 \pm 1.1$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Sensitivity of performance to number of iterations based on MDL model.}
		\label{supptab:mtliterations}
\end{table*}%


\paragraph{SDL-ResNet-18.}
Following~\cite{triantafillou2019meta,dvornik2020selecting,li2021universal}, we train a ResNet-18 on the train split of ImageNet and use $84\times 84$ image size, which is denoted as SDL-ResNet-18.
For optimization, we follow the training protocol in \cite{dvornik2020selecting,li2021universal}. Specifically, we use SGD optimizer and cosine annealing for all experiments with a momentum of 0.9 and a weight decay of $7\times 10^{-4}$. 
Some other hyperparameters are shown in \cref{supptab:hyperparams} as in~\cite{dvornik2020selecting,li2021universal}. To regularize training, we also use the exact same data augmentations as in \cite{dvornik2020selecting,li2021universal}, \eg random crops and random color augmentations. 

\paragraph{SDL-ResNet-34.}
We also apply our method to the single domain learning model with ResNet-34 backbone learned on ImageNet only as in~\cite{doersch2020crosstransformers}. We follow \cite{doersch2020crosstransformers} and use higher-resolution ($224\times 224$) images for meta-training and meta-testing. For optimization, we follow the training protocol as in~\cite{dvornik2020selecting,li2021universal}. Specifically, we use SGD optimizer and cosine annealing with a momentum of 0.9, a weight decay of $1\times 10^{-4}$ with a batch size of 128. Other hyperparameters are the same as in SDL-ResNet-18 and are shown in \cref{supptab:hyperparams}.
To regularize training, we also use the exact same data augmentations as in \cite{dvornik2020selecting,li2021universal}, \eg random crops and random color augmentations with an additional stage that randomly downsamples and upsamples images as in~\cite{doersch2020crosstransformers}. 


\subsection{Task-specific learning}

\paragraph{Attaching and learning adapters.}
For the optimization of the adaptation parameters $\alpha$ which is attached directly and learned on support set and the pre-classifier adaptation $\beta$, we follow the optimization strategy in~\cite{li2021universal}, initialize $\beta$ as an identity matrix and optimize both $\alpha$ and $\beta$ for 40 iterations using Adadelta~\cite{zeiler2012adadelta} as optimizer. The learning rate of $\beta$ is 0.1 for first eight datasets and 1 for the last five datasets as in~\cite{li2021universal} and we set the learning rate of $\alpha$ as half of the learning rate of $\beta$, \ie 0.05 for the first eight datasets and 0.5 for the last five datasets. Note that, we learn $\alpha$ and $\beta$ on a per-task basis using the task's support set during meta-test. That is, $\alpha$ and $\beta$ are not re-used across the test tasks drawn from $\mathcal{D}_t$.

\paragraph{Predicting $r_{\alpha}$.}
In case of modulating $\alpha$ with the auxiliary network, we follow the auxiliary training protocols in~\cite{bateni2020improved}. We train for 10K episodes to optimize the task encoder using Adam with a learning rate of $1\times 10^{-5}$ on eight training domains in meta-train. We validate every 5K iterations to save the best model for test. 



\begin{table*}[h!]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    10 iterations & $58.4 \pm 1.1$ & $94.8 \pm 0.4$ & $89.9 \pm 0.4$ & $81.3 \pm 0.7$ & $76.6 \pm 0.7$ & $81.8 \pm 0.6$ & $68.4 \pm 0.9$ & $92.5 \pm 0.5$ & $76.5 \pm 1.1$ & $55.6 \pm 1.1$ & $96.4 \pm 0.4$ & $79.0 \pm 0.7$ & $66.9 \pm 1.0$ \\
		    20 iterations & $58.2 \pm 1.1$ & $94.8 \pm 0.4$ & $89.9 \pm 0.4$ & $81.1 \pm 0.7$ & $77.5 \pm 0.8$ & $81.9 \pm 0.6$ & $68.0 \pm 0.9$ & $92.4 \pm 0.5$ & $81.8 \pm 1.0$ & $57.8 \pm 1.1$ & $96.7 \pm 0.4$ & $81.7 \pm 0.8$ & $69.1 \pm 0.9$ \\
		    40 iterations & $59.5 \pm 1.0$ & $94.9 \pm 0.4$ & $89.9 \pm 0.4$ & $81.1 \pm 0.8$ & $77.5 \pm 0.7$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & $92.2 \pm 0.5$ & $82.8 \pm 1.0$ & $57.6 \pm 1.0$ & $96.7 \pm 0.4$ & $82.9 \pm 0.7$ & $70.4 \pm 1.0$ \\
		    60 iterations & $58.7 \pm 1.1$ & $94.9 \pm 0.4$ & $89.5 \pm 0.5$ & $80.8 \pm 0.7$ & $77.4 \pm 0.8$ & $81.8 \pm 0.6$ & $66.2 \pm 0.9$ & $92.5 \pm 0.5$ & $83.7 \pm 0.9$ & $56.9 \pm 1.0$ & $96.6 \pm 0.3$ & $82.0 \pm 0.8$ & $72.0 \pm 0.9$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Sensitivity of performance to number of iterations based on URL model.}
		\label{supptab:urliterations}
\end{table*}%

\section{More results}

\subsection{Our method with different feature extractors} 
\cref{supptab:baselines} shows the results of our method (the proposed residual adapters in matrix form) when incorporated to different feature extractors, single domain model with ResNet-18 backbone (SDL-ResNet-18) pre-trained on ImageNet, single domain model with ResNet-34 (SDL-ResNet-34) pre-trained on ImageNet, vanilla multi-domain learning (MDL) and URL~\cite{li2021universal}. We see that attaching and learning residual adapters can significantly improve the performance on all domains over SDL-ResNet-18, SDL-ResNet-34 and MDL and obtain better performance on most domains over URL (11 out of 13 domains). This strongly indicates that our method can efficiently adapt the model for unseen categories and domains with few support samples while being agnostic to the feature extractor with different backbone and resolution of images.

\subsection{Task-specific parameterizations}

In \cref{supptab:testad}, we report additional 95\% confidence interval of each dataset to the main paper for the comparison of different $r_{\alpha}$ choices based on the URL model. The first eight datasets are seen during training and the last five datasets are unseen and used for test only. We can see that the confidence intervals for different methods have marginal differences.

\subsection{Varying-way 5-shot and 5-way-1-shot}

In the main paper, we only report the average accuracy of Varying-Way Five-Shot and Five-Way One-Shot scenarios due to limited space, and detailed results are depicted in \cref{supptab:fixedshot}. In the table, we report the Mean accuracy, 95\% confidence interval of each dataset. The first eight datasets are seen during training and the last five datasets are unseen and used for test only. URT and URL are two strong baselines surpassing both Simple CNAPS and SUR, while Ours outperforms them on most datasets, especially on unseen domains.

\subsection{Results evaluated with updated evaluation protocol.}
As the code from Meta-dataset has been updated, we evaluate all methods with the updated evaluation protocol from the Meta-dataset~\footnote{As mentioned in \url{https://github.com/google-research/meta-dataset/issues/54}, we also set the shuffle\_buffer\_size as 1000 to evaluate all methods and report the results in \cref{supptab:currmethod}. This change does not affect much on the results as the datasets we used were shuffled using the latest data convert code from \href{https://github.com/google-research/meta-dataset}{Meta-Dataset}.} and report the results~\footnote{The results of Simple CNAPS~\cite{bateni2020improved} and Transductive CNAPS~\cite{bateni2020enhancing} are reproduced by the authors and reported at \url{https://github.com/peymanbateni/simple-cnaps}. Results of FLUTE~\cite{triantafillou2021flute} and tri-M~\cite{liu2021multi} are from their papers. We reproduce the results of SUR~\cite{dvornik2020selecting} and URT~\cite{liu2020universal} with the updated evaluation protocol for fair comparison.} in \cref{supptab:currmethod}. As shown in \cref{supptab:currmethod}, the update does not affect much on the results and our method rank 1.5 in average and the state-of-the-art method URL rank 2.7. Our method outperforms other methods on most domains (9 out of 13), especially obtaining significant improvement on 5 unseen datasets than the second best method, \ie Average Unseen (+7.9). More specifically, our method obtains significant better results than the second best approach (URL) on Traffic Sign (+20.2), CIFAR-10 (+8.7), and CIFAR-100 (+7.0).


\subsection{Ablation study}
% stable
Here, we conduct ablation study of our method with the URL model, unless stated otherwise.
\paragraph{Sensitivity analysis for number of iterations.}
In our method, we optimize the attached parameters ($\alpha,\beta$) with 40 iterations. \Cref{suppfig:mdlstab} and \Cref{suppfig:urlstab} report
the results with 10, 20, 40, 60 iterations and indicates that our method (solid green) converges to a stable solution after 20 iterations and achieves better average performance on all domains than the baseline URL (dash green). The mean accuracy with 95\% confidence interval are reported in \cref{supptab:mtliterations,supptab:urliterations}



\paragraph{Influence of $\alpha$ and $\beta$.} We evaluate different components of our method and report the results in \cref{supptab:com}. The results show that both residual adapters $\alpha$ and the linear transformation $\beta$ help adapt features to unseen classes while residual adapters significantly improve the performance on unseen domains. The best results are achieved by using both $\alpha$ and $\beta$.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\linewidth]{./figures/mtl-iteration.pdf}
\end{center}
\vspace{-0.3in}
\caption{Sensitivity of performance to number of iterations based on MDL model.}
\label{suppfig:mdlstab}
\end{figure}

\begin{figure}[h]
\begin{center}

\includegraphics[width=0.9\linewidth]{./figures/oldurl-iteration.pdf}
\end{center}
\vspace{-0.3in}
\caption{Sensitivity of performance to number of iterations based on URL model.}
\label{suppfig:urlstab}
\end{figure}


\begin{table*}[t]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    Ours w/o $\alpha$ \& $\beta$ & $57.0 \pm 1.1$ & $94.4 \pm 0.4$ & $88.0 \pm 0.5$ & $80.3 \pm 0.7$ & $74.6 \pm 0.7$ & $81.8 \pm 0.6$ & $66.2 \pm 0.9$ & $91.5 \pm 0.5$ & $49.8 \pm 1.1$ & $54.1 \pm 1.0$ & $91.1 \pm 0.4$ & $70.6 \pm 0.7$ & $59.1 \pm 1.0$ \\
		    Ours w/o $\beta$ & $57.3 \pm 1.1$ & ${\bf 94.9 \pm 0.4}$ & $88.9 \pm 0.5$ & $81.0 \pm 0.7$ & $76.7 \pm 0.7$ & $80.6 \pm 0.6$ & $65.4 \pm 0.9$ & $91.4 \pm 0.5$ & $82.6 \pm 1.0$ & $55.0 \pm 1.1$ & $96.6 \pm 0.4$ & $82.1 \pm 0.7$ & $66.4 \pm 1.1$ \\
		    Ours w/o $\alpha$ & $58.8 \pm 1.1$ & $94.5 \pm 0.4$ & $89.4 \pm 0.4$ & $80.7 \pm 0.8$ & $77.2 \pm 0.7$ & ${\bf 82.5 \pm 0.6}$ & ${\bf 68.1 \pm 0.9}$ & $92.0 \pm 0.5$ & $63.3 \pm 1.2$ & $57.3 \pm 1.0$ & $94.7 \pm 0.4$ & $74.2 \pm 0.8$ & $63.6 \pm 1.0$ \\
		    Ours & ${\bf 59.5 \pm 1.0}$ & ${\bf 94.9 \pm 0.4}$ & ${\bf 89.9 \pm 0.4}$ & ${\bf 81.1 \pm 0.8}$ & ${\bf 77.5 \pm 0.7}$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & ${\bf 92.2 \pm 0.5}$ & ${\bf 82.8 \pm 1.0}$ & ${\bf 57.6 \pm 1.0}$ & ${\bf 96.7 \pm 0.4}$ & ${\bf 82.9 \pm 0.7}$ & ${\bf 70.4 \pm 1.0}$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.35cm}
		\caption{Effect of each component. We build our method on the URL model and `Ours w/o $\alpha$ \& $\beta$' means we remove both residual adapters $\alpha$ and the pre-classifier adaptation layer $\beta$ in our method.}
		\label{supptab:com}
\end{table*}%

\begin{table*}[h!]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    Ours(SDL-ResNet-18)-I & $59.5 \pm 1.1$& $78.2 \pm 1.2$& $72.2 \pm 1.0$& $74.9 \pm 0.9$& $77.3 \pm 0.7$& $67.6 \pm 0.9$& $44.7 \pm 1.0$& $90.9 \pm 0.6$& $82.5 \pm 0.8$& $59.0 \pm 1.0$& $93.9 \pm 0.6$& $82.1 \pm 0.7$ & $70.7 \pm 0.9$ \\
		    Ours(SDL-ResNet-18)-R & $58.2 \pm 1.0$& $78.4 \pm 1.2$& $71.1 \pm 1.1$& $74.4 \pm 1.0$& $77.1 \pm 0.7$& $67.2 \pm 1.0$& $45.9 \pm 1.0$& $90.7 \pm 0.6$& $81.9 \pm 1.0$& $57.7 \pm 1.1$& $94.1 \pm 0.5$& $81.9 \pm 0.7$& $70.5 \pm 0.9$ \\
		    \midrule
		    Ours(MDL)-I & $55.6 \pm 1.0$ & $94.3 \pm 0.4$ & $86.7 \pm 0.5$ & $79.4 \pm 0.8$ & $73.2 \pm 0.8$ & $81.7 \pm 0.6$ & $64.0 \pm 0.9$ & $90.9 \pm 0.5$ & $81.1 \pm 0.9$ & $51.4 \pm 1.1$ & $96.9 \pm 0.3$ & $78.5 \pm 0.8$ & $64.3 \pm 1.1$ \\
		    Ours(MDL)-R & $56.0 \pm 1.1$ & $94.1 \pm 0.4$ & $87.1 \pm 0.5$ & $79.7 \pm 0.8$ & $74.0 \pm 0.7$ & $82.0 \pm 0.6$ & $62.6 \pm 0.9$ & $90.6 \pm 0.6$ & $80.9 \pm 0.9$ & $51.7 \pm 1.1$ & $96.9 \pm 0.4$ & $77.7 \pm 0.9$ & $65.8 \pm 1.1$ \\
		    \midrule
		    Ours(URL)-I & $59.5 \pm 1.0$ & $94.9 \pm 0.4$ & $89.9 \pm 0.4$ & $81.1 \pm 0.8$ & $77.5 \pm 0.7$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & $92.2 \pm 0.5$ & $82.8 \pm 1.0$ & $57.6 \pm 1.0$ & $96.7 \pm 0.4$ & $82.9 \pm 0.7$ & $70.4 \pm 1.0$ \\
		    Ours(URL)-R & $58.8 \pm 1.1$ & $94.9 \pm 0.4$ & $90.5 \pm 0.4$ & $81.8 \pm 0.6$ & $77.7 \pm 0.7$ & $82.3 \pm 0.6$ & $66.8 \pm 0.9$ & $92.6 \pm 0.5$ & $83.7 \pm 0.8$ & $57.7 \pm 1.1$ & $96.9 \pm 0.4$ & $82.5 \pm 0.7$ & $72.0 \pm 0.9$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Initialization analysis of adapters. `Ours(URL)-I' indicates our method using URL as the pretrained model and initializing residual adapters as identity matrix (scaled by $\delta = 0.0001$) while `Ours(URL)-R' means our method initialize residual adapters randomly.}
		\label{supptab:initialization}
\end{table*}%

\begin{table*}[h!]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    Ours (block4) & $59.0 \pm 1.1$ & $95.0 \pm 0.4$ & $90.0 \pm 0.4$ & $80.6 \pm 0.8$ & $77.8 \pm 0.7$ & $82.3 \pm 0.6$ & $68.2 \pm 0.9$ & $91.8 \pm 0.6$ & $70.6 \pm 1.1$ & $57.1 \pm 1.1$ & $95.9 \pm 0.4$ & $77.2 \pm 0.8$ & $65.9 \pm 1.0$ \\
		    Ours (block3,4) & $60.4 \pm 1.1$ & $94.7 \pm 0.4$ & $90.0 \pm 0.5$ & $80.4 \pm 0.7$ & $77.8 \pm 0.7$ & $82.2 \pm 0.6$ & $67.2 \pm 0.8$ & $92.5 \pm 0.5$ & $77.2 \pm 1.0$ & $57.9 \pm 1.0$ & $96.7 \pm 0.3$ & $78.8 \pm 0.9$ & $68.6 \pm 0.9$ \\
		    Ours (block2,3,4) & $59.6 \pm 1.1$ & $94.9 \pm 0.4$ & $89.9 \pm 0.5$ & $81.0 \pm 0.8$ & $78.2 \pm 0.7$ & $82.4 \pm 0.6$ & $67.6 \pm 0.9$ & $92.3 \pm 0.5$ & $81.5 \pm 1.0$ & $57.9 \pm 1.0$ & $96.6 \pm 0.4$ & $81.5 \pm 0.8$ & $70.6 \pm 1.0$ \\
		    Ours (block-all) & $59.5 \pm 1.0$ & $94.9 \pm 0.4$ & $89.9 \pm 0.4$ & $81.1 \pm 0.8$ & $77.5 \pm 0.7$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & $92.2 \pm 0.5$ & $82.8 \pm 1.0$ & $57.6 \pm 1.0$ & $96.7 \pm 0.4$ & $82.9 \pm 0.7$ & $70.4 \pm 1.0$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Block (layer) analysis for adapters based on URL model.}
		\label{supptab:layer}
\end{table*}%

\paragraph{Initialization analysis for adapters.}
Here, we investigate using different initialization strategies for adapters: i) Identity initialization: in this work we initialize each residual adapter as an identity matrix scaled by a scalar $\delta$ and we set $\delta=1e-4$; ii) randomly initialization: alternatively, we can randomly initialize each residual adapter. The results of different initialization are summarized in \cref{suppfig:init}. We can see that our methods with different initialization strategies obtain similar results, which indicates that our method works also with randomly initialization and again verifies the stability of our method. Detailed results of each datasets are shown in \cref{supptab:initialization}.

\begin{figure}[h!]

\begin{center}

\includegraphics[width=0.9\linewidth]{./figures/allRA_initialization.pdf}
\end{center}
\vspace{-0.3in}
\caption{Initialization analysis for adapters. '-I' indicates identity initialization and `-R' is randomly initialization.}
\label{suppfig:init}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\linewidth]{./figures/allRA_layers.pdf}
\end{center}
\vspace{-0.3in}
\caption{Block (layer) analysis for adapters.}
\label{suppfig:urllayers}

\end{figure}





\begin{table*}[t]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    Ours & $59.5 \pm 1.0$ & $94.9 \pm 0.4$ & $89.9 \pm 0.4$ & $81.1 \pm 0.8$ & $77.5 \pm 0.7$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & $92.2 \pm 0.5$ & $82.8 \pm 1.0$ & $57.6 \pm 1.0$ & $96.7 \pm 0.4$ & $82.9 \pm 0.7$ & $70.4 \pm 1.0$ \\
		    Ours(N=2) & $58.9 \pm 1.1$ & $95.2 \pm 0.4$ & $89.7 \pm 0.5$ & $80.9 \pm 0.7$ & $76.7 \pm 0.7$ & $81.4 \pm 0.6$ & $67.7 \pm 0.9$ & $92.2 \pm 0.5$ & $82.4 \pm 1.0$ & $57.1 \pm 1.0$ & $96.5 \pm 0.4$ & $82.4 \pm 0.7$ & $70.3 \pm 1.0$ \\
		    Ours(N=4) & $58.7 \pm 1.1$ & $94.9 \pm 0.4$ & $89.7 \pm 0.5$ & $80.3 \pm 0.7$ & $77.0 \pm 0.7$ & $82.5 \pm 0.6$ & $67.2 \pm 0.9$ & $92.5 \pm 0.5$ & $82.6 \pm 1.0$ & $57.5 \pm 1.1$ & $96.5 \pm 0.4$ & $82.5 \pm 0.7$ & $70.8 \pm 0.9$ \\
		    Ours(N=8) & $59.1 \pm 1.1$ & $95.0 \pm 0.4$ & $89.8 \pm 0.5$ & $80.2 \pm 0.8$ & $77.2 \pm 0.7$ & $82.1 \pm 0.6$ & $67.0 \pm 0.9$ & $92.2 \pm 0.5$ & $82.5 \pm 1.0$ & $57.2 \pm 1.1$ & $96.8 \pm 0.4$ & $82.6 \pm 0.7$ & $71.8 \pm 0.9$ \\
		    Ours(N=16) & $58.2 \pm 1.1$ & $94.7 \pm 0.4$ & $90.1 \pm 0.4$ & $80.3 \pm 0.8$ & $76.9 \pm 0.7$ & $81.7 \pm 0.6$ & $67.6 \pm 0.9$ & $92.0 \pm 0.5$ & $81.8 \pm 1.0$ & $58.1 \pm 1.1$ & $96.4 \pm 0.4$ & $81.8 \pm 0.7$ & $71.1 \pm 0.9$ \\
		    Ours(N=32) & $59.2 \pm 1.1$ & $94.8 \pm 0.4$ & $89.6 \pm 0.5$ & $80.0 \pm 0.8$ & $77.3 \pm 0.6$ & $82.4 \pm 0.6$ & $67.2 \pm 0.9$ & $92.1 \pm 0.5$ & $82.1 \pm 1.0$ & $57.1 \pm 1.0$ & $96.7 \pm 0.3$ & $81.6 \pm 0.8$ & $71.1 \pm 0.9$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.15cm}
		\caption{Results of using decomposed RA on layer3,4.}
		\label{supptab:decoml34}
\end{table*}%

\begin{table*}[h!]
	\centering
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lcccccccc|ccccc}

		    \toprule
		    Test Dataset & ImageNet & Omniglot & Aircraft & Birds & Textures & Quick Draw & Fungi & VGG Flower & Traffic Sign & MSCOCO & MNIST & CIFAR-10 & CIFAR-100\\
		    \midrule
		    Ours & $59.5 \pm 1.0$ & $94.9 \pm 0.4$ & $89.9 \pm 0.4$ & $81.1 \pm 0.8$ & $77.5 \pm 0.7$ & $81.7 \pm 0.6$ & $66.3 \pm 0.9$ & $92.2 \pm 0.5$ & $82.8 \pm 1.0$ & $57.6 \pm 1.0$ & $96.7 \pm 0.4$ & $82.9 \pm 0.7$ & $70.4 \pm 1.0$ \\
		    Ours(N=2) & $58.1 \pm 1.1$ & $94.8 \pm 0.4$ & $89.7 \pm 0.5$ & $80.2 \pm 0.8$ & $76.9 \pm 0.7$ & $82.1 \pm 0.6$ & $67.8 \pm 0.9$ & $92.0 \pm 0.6$ & $82.5 \pm 0.9$ & $56.9 \pm 1.1$ & $96.7 \pm 0.3$ & $82.0 \pm 0.8$ & $70.3 \pm 1.0$ \\
		    Ours(N=4) & $59.6 \pm 1.1$ & $94.8 \pm 0.4$ & $89.9 \pm 0.5$ & $80.3 \pm 0.8$ & $77.4 \pm 0.7$ & $82.6 \pm 0.6$ & $66.6 \pm 0.9$ & $92.9 \pm 0.5$ & $79.7 \pm 1.1$ & $57.6 \pm 1.1$ & $96.5 \pm 0.4$ & $80.9 \pm 0.8$ & $70.6 \pm 1.0$ \\
		    Ours(N=8) & $58.2 \pm 1.1$ & $94.6 \pm 0.4$ & $89.6 \pm 0.5$ & $81.2 \pm 0.8$ & $76.6 \pm 0.7$ & $82.7 \pm 0.6$ & $66.5 \pm 0.9$ & $92.3 \pm 0.5$ & $78.1 \pm 1.1$ & $57.3 \pm 1.0$ & $96.3 \pm 0.3$ & $81.0 \pm 0.8$ & $70.9 \pm 0.9$ \\
		    Ours(N=16) & $58.9 \pm 1.1$ & $94.6 \pm 0.4$ & $89.7 \pm 0.5$ & $80.1 \pm 0.7$ & $77.0 \pm 0.7$ & $82.1 \pm 0.6$ & $68.4 \pm 0.9$ & $91.9 \pm 0.5$ & $78.3 \pm 1.0$ & $57.8 \pm 1.1$ & $96.0 \pm 0.4$ & $82.0 \pm 0.7$ & $70.3 \pm 1.0$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.15cm}
		\caption{Results of using decomposed RA on all layers.}
		\label{supptab:decom}
\end{table*}%





\paragraph{Layer analysis for adapters.}
Here we investigate whether it is sufficient to attach the adapters only to the later layers.
We evaluate this on ResNet18 which is composed of four blocks and attach the adapters to only later blocks (block4, block3,4, block2,3,4 and block-all. 
\Cref{suppfig:urllayers} shows that applying our adapters to only the last block (block4) obtains around 78\% average accuracy on all domains which outperforms the URL. With attaching residual adapters to more layers, the performance on unseen domains is improved significantly while the one on seen domains remains stable.
The mean accuracy with 95\% confidence interval for layer analysis are shown in \cref{supptab:layer}.



\paragraph{Decomposing residual adapters.}
Here we investigate whether one can reduce the number of parameters in the adapters while retaining its performance by using matrix decomposition.
As in deep neural network, the adapters in earlier layers are relatively small, we then decompose the adapters in the last two blocks only where the adapter dimensionality goes up to $512\times 512$. \Cref{suppfig:urldecoml34} shows that our method can achieve good performance with less parameters by decomposing large residual adapters, (\eg when $N=32$ where the number of additional parameters equal to around 4\% vs 13\%, the performance is still comparable to the original form of residual adapters, \ie N=0). Results of each datasets in \cref{supptab:decoml34}, also show that, by decomposing large residual adapters, the performance of our method is still comparable to the original form of residual adapters (\ie Ours) with less parameters.

\begin{figure}[h]

\begin{center}

\includegraphics[width=0.9\linewidth]{./figures/DecomRAdinl34.pdf}
\end{center}
\vspace{-0.3in}
\caption{Decomposed residual adapters on block-3,4.}
\label{suppfig:urldecoml34}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.9\linewidth]{./figures/DecomRAdin.pdf}
\end{center}
\vspace{-0.3in}
\caption{Decomposed residual adapters on all layers.}
\label{suppfig:urldecom}
\end{figure}




The similar conclusion can be drawn from results (shown in \cref{suppfig:urldecom}) of our method using decomposed residual adapters in all layers. When N increases, \ie, smaller residual adapters, the average accuracy on all domains is still comparable to the original form of residual adapters (\ie N=0) with less parameters though the average accuracy on unseen domains drops slightly. From the results depicted in \cref{supptab:decom}, we can see that when $N$ increases, the performance of most domains are still comparable to the original form of residual adapters (\ie Ours) while the performance on Traffic Sign drops slightly as the adapters in earlier layers are small and when N is larger the decomposed residual adapters might be too small to tranform the features. In overall, our method can achieve good performance with less parameters by decomposing large residual adapters.

\paragraph{Training time.}
The training time (meta-train) of our method is equal to the one of URL (hence no additional cost), \ie 48 hours in multi-domain setting, 6 hours for Resnet-18 and 33 hours for Resnet-34 in single-domain learning in one Nvidia V100 GPU. 
Whereas CTX meta-training requires 8 Nvidia V100 GPUs for 7 days and approximately 40 times more expensive than ours. 
During the meta-test stage, the model parameters are further trained using support set of each episode. 
Meta-test training cost is depicted in \cref{supptab:testtime} for Meta-Dataset tasks. 
URL baseline only finetunes parameters of PA $\beta$.
Finetune+NCC updates the entire backbone parameters.
Ours learn RA and PA parameters. 
While URL is the fastest baseline, as it does not require backpropagating the error to early layers, ours is more efficient than finetuning all the backbone parameters.

\setcounter{table}{11}
\begin{table}[ht!]
	\centering
	
    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{lccccccccccccccc}

		    \toprule
		    \multirow{2}{*}{Test Dataset} & Image & Omni & Air- & \multirow{2}{*}{Birds} & Tex- & Quick & \multirow{2}{*}{Fungi} & VGG & Traffic & MS- & \multirow{2}{*}{MNIST} & CIFAR & CIFAR\\
		    & -Net & -glot & craft & & tures & Draw & & Flower & Sign & COCO & & -10 & -100 \\
		    \midrule
		    URL & $0.7$ & $0.7$ & $0.4$ & $0.7$ & $0.4$ & $1.0$ & $1.0$ & $0.5$ & $0.9$ & $0.9$ & $0.4$ & $0.4$ & $1.0$ \\
		    Finetune+NCC & $7.7$ & $2.5$ & $7.4$ & $7.0$ & $5.8$ & $9.3$ & $8.7$ & $6.6$ & $9.1$ & $9.0$ & $6.5$ & $6.7$ & $9.3$ \\
		    Ours (URL+RA+PA) & $7.2$ & $2.4$ & $6.1$ & $6.8$ & $4.8$ & $8.9$ & $7.4$ & $5.2$ & $8.8$ & $8.3$ & $6.0$ & $6.2$ & $8.6$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.35cm}
		\caption{\footnotesize Computation cost (\# second per task) during meta-test.}
		\label{supptab:testtime}
\end{table}%



\subsection{Qualitative results}
We qualitatively analyze our method and compare it to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in \cref{suppfig:imagenet,suppfig:omniglot,suppfig:aircraft,suppfig:birds,suppfig:texture,suppfig:quickdraw,suppfig:fungi,suppfig:flower,suppfig:traffic,suppfig:mscoco,suppfig:mnist,suppfig:cifar10,suppfig:cifar100} by illustrating the nearest neighbors in all test datasets given a query image as in~\cite{li2021universal}.
It is clear that our method produces more correct neighbors than other methods. 
While other methods retrieve images with more similar colors, shapes and backgrounds, \eg in \cref{suppfig:traffic,suppfig:mscoco,suppfig:cifar10,suppfig:cifar100}, our method is able to retrieve semantically similar images.
More specifically, as shown in \cref{suppfig:birds}, our method correctly produces neighbors of the bird in the query image while other methods pick images with similar appearances or similar background, \eg images with twigs.
In \cref{suppfig:traffic}, other methods mainly retrieve the triangle sign while our method is able to retrieve the correct sign with illumination distortion. In \cref{suppfig:cifar100}, other methods including SUR, URT are distracted by the blue background but our method select the correct shark images. It again suggests that our method is able to quickly adapt the features for unseen few-shot tasks.


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/imagenet.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in ImageNet. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:imagenet}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/omniglot.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Omniglot. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:omniglot}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/aircraft.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Aircraft. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:aircraft}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/birds.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Birds. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:birds}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/texture.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Textures. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:texture}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/quickdraw.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Quick Draw. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:quickdraw}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/fungi.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Fungi. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:fungi}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/flower.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in VGG Flower. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:flower}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/traffic.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in Traffic Sign. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:traffic}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/mscoco.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in MSCOCO. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:mscoco}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/mnist.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in MNIST. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:mnist}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/cifar10.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in CIFAR-10. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:cifar10}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\linewidth]{figures/cifar100.png}
\end{center}
\vspace{-0.3in}
\caption{Qualitative comparison to Simple CNAPS~\cite{bateni2020improved}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, and URL~\cite{li2021universal} in CIFAR-100. Green and red colors indicate correct and false predictions respectively.}
\label{suppfig:cifar100}
\end{figure}

