% !TEX root = main.tex



Here we start with experimental setup, and
then we compare our method to the state-of-the-art methods and rigorously evaluate various design decisions.
We finally provide further analysis.

\subsection{Experimental setup}
\paragraph{Dataset.} We use the Meta-Dataset~\cite{triantafillou2019meta} which is the standard benchmark for few-shot classification.
It contains images from 13 diverse datasets and we follow the standard protocol in \cite{triantafillou2019meta} (more details in the supplementary).

\paragraph{Implementation details.}
As in~\cite{dvornik2020selecting,bateni2020improved,li2021universal}, we build our method on ResNet-18~\cite{he2016deep} backbone, which is trained over eight training subdatasets by following~\cite{li2021universal}  with the same hyperparameters in our experiments, unless stated otherwise.
Once learned, we freeze its parameters and use them as the task-agnostic weights.
For learning task-specific weights ($\vartheta$), including the pre-classifier transformation $\beta$ and the adapter parameters, we directly attach them to the task-agnostic weights and learn them on the support samples in meta-test by using Adadelta optimizer~\cite{zeiler2012adadelta}. 

In the study of various task adaptation strategies in Section~\ref{sec:analysis}, we consider to only estimate the adapter parameters and learn the auxiliary network parameters by using Adam optimizer as in~\cite{requeima2019fast,bateni2020improved} in meta-train.
Note that estimation of pre-classifier and classifier weights via the auxiliary network leads to noisy and poor results and we do not report them.
Similarly, we found that the auxiliary network fails to estimate very high-dimensional weights.
Hence we only use it to estimate adapter weights that are parameterized with a vector for channelwise multiplication but not with a matrix.

% Comparison with state-of-the-art methods on `Train on ImageNet only' setting
\begin{table*}[ht]
	\centering
    \resizebox{0.9\textwidth}{!}
    {
		\begin{tabular}{cccccccc|ccc}

		    % \toprule
		   	& \multicolumn{7}{c}{ResNet-18} & \multicolumn{3}{c}{ResNet-34} \\
		   	% \midrule
		   	\toprule
		    \multirow{2}{*}{Test Dataset} & Finetune & ProtoNet & fo-Proto- & ALFA+fo-Proto & BOHB & FLUTE & \multirow{2}{*}{Ours} & ProtoNet & CTX & \multirow{2}{*}{Ours}\\
		    & \cite{triantafillou2019meta} & \cite{triantafillou2019meta} & MAML~\cite{triantafillou2019meta} & -MAML~\cite{triantafillou2019meta} & \cite{saikia2020optimized} & \cite{triantafillou2021flute} &  & \cite{doersch2020crosstransformers} & \cite{doersch2020crosstransformers} & \\
		    \midrule
			ImageNet & $45.8 \pm 1.1$ & $50.5 \pm 1.1$ & $49.5 \pm 1.1$ & $52.8 \pm 1.1$ & $51.9 \pm 1.1$ & $46.9 \pm 1.1$ & ${\bf 59.5 \pm 1.1}$ & $53.7 \pm 1.1$ & $62.8 \pm 1.0$ & ${\bf 63.7 \pm 1.0}$ \\
			\midrule
			Omniglot & $60.9 \pm 1.6$ & $60.0 \pm 1.4$ & $63.4 \pm 1.3$ & $61.9 \pm 1.5$ & $67.6 \pm 1.2$ & $61.6 \pm 1.4$ & ${\bf 78.2 \pm 1.2}$ & $68.5 \pm 1.3$ & $82.2 \pm 1.0$ & ${\bf 82.6 \pm 1.1}$ \\
			Aircraft & $68.7 \pm 1.3$ & $53.1 \pm 1.0$ & $56.0 \pm 1.0$ & $63.4 \pm 1.1$ & $54.1 \pm 0.9$ & $48.5 \pm 1.0$ & ${\bf 72.2 \pm 1.0}$ & $58.0 \pm 1.0$ & $79.5 \pm 0.9$ & ${\bf 80.1 \pm 1.0}$ \\
			Birds & $57.3 \pm 1.3$ & $68.8 \pm 1.0$ & $68.7 \pm 1.0$ & $69.8 \pm 1.1$ & $70.7 \pm 0.9$ & $47.9 \pm 1.0$ & ${\bf 74.9 \pm 0.9}$ & $74.1 \pm 0.9$ & $80.6 \pm 0.9$ & ${\bf 83.4 \pm 0.8}$ \\
			Textures & $69.0 \pm 0.9$ & $66.6 \pm 0.8$ & $66.5 \pm 0.8$ & $70.8 \pm 0.9$ & $68.3 \pm 0.8$ & $63.8 \pm 0.8$ & ${\bf 77.3 \pm 0.7}$ & $68.8 \pm 0.8$ & $75.6 \pm 0.6$ & ${\bf 79.6 \pm 0.7}$ \\
			Quick Draw & $42.6 \pm 1.2$ & $49.0 \pm 1.1$ & $51.5 \pm 1.0$ & $59.2 \pm 1.2$ & $50.3 \pm 1.0$ & $57.5 \pm 1.0$ & ${\bf 67.6 \pm 0.9}$ & $53.3 \pm 1.1$ & ${\bf 72.7 \pm 0.8}$ & $71.0 \pm 0.8$ \\
			Fungi & $38.2 \pm 1.0$ & $39.7 \pm 1.1$ & $40.0 \pm 1.1$ & $41.5 \pm 1.2$ & $41.4 \pm 1.1$ & $31.8 \pm 1.0$ & ${\bf 44.7 \pm 1.0}$ & $40.7 \pm 1.1$ & ${\bf 51.6 \pm 1.1}$ & $51.4 \pm 1.2$ \\
			VGG Flower & $85.5 \pm 0.7$ & $85.3 \pm 0.8$ & $87.2 \pm 0.7$ & $86.0 \pm 0.8$ & $87.3 \pm 0.6$ & $80.1 \pm 0.9$ & ${\bf 90.9 \pm 0.6}$ & $87.0 \pm 0.7$ & ${\bf 95.3 \pm 0.4}$ & $94.0 \pm 0.5$ \\
			Traffic Sign & $66.8 \pm 1.3$ & $47.1 \pm 1.1$ & $48.8 \pm 1.1$ & $60.8 \pm 1.3$ & $51.8 \pm 1.0$ & $46.5 \pm 1.1$ & ${\bf 82.5 \pm 0.8}$ & $58.1 \pm 1.1$ & ${\bf 82.7 \pm 0.8}$ & $81.7 \pm 0.9$ \\
			MSCOCO & $34.9 \pm 1.0$ & $41.0 \pm 1.1$ & $43.7 \pm 1.1$ & $48.1 \pm 1.1$ & $48.0 \pm 1.0$ & $41.4 \pm 1.0$ & ${\bf 59.0 \pm 1.0}$ & $41.7 \pm 1.1$ & $59.9 \pm 1.0$ & ${\bf 61.7 \pm 0.9}$ \\
			MNIST & - & - & - & - & - & $80.8 \pm 0.8$ & ${\bf 93.9 \pm 0.6}$ & - & - & ${\bf 94.6 \pm 0.5}$ \\
			CIFAR-10 & - & - & - & - & - & $65.4 \pm 0.8$ & ${\bf 82.1 \pm 0.7}$ & - & - & ${\bf 86.0 \pm 0.6}$ \\
			CIFAR-100 & - & - & - & - & - & $52.7 \pm 1.1$ & ${\bf 70.7 \pm 0.9}$ & - & - & ${\bf 78.3 \pm 0.8}$ \\
			\midrule
			Average Seen & $45.8$ & $50.5$ & $49.5$ & $52.8$ & $51.9$ & $46.9$ & ${\bf 59.5}$ & $53.7$ & $62.8$ & ${\bf 63.7}$ \\
			Average Unseen & $58.2$ & $56.7$ & $58.4$ & $62.4$ & $60.0$ & $53.2$ & ${\bf 71.9}$ & $61.1$ & $75.6$ & ${\bf 76.2}$ \\
			Average All & $57.0$ & $56.1$ & $57.5$ & $61.4$ & $59.2$ & $52.6$ & ${\bf 70.7}$ & $60.4$ & $74.3$ & ${\bf 74.9}$ \\
			\midrule
			Average Rank & $7.9$ & $8.3$ & $7.0$ & $5.3$ & $6.0$ & $8.9$ & ${\bf 2.8}$ & $5.5$ & $1.8$ & ${\bf 1.5}$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.35cm}
		\caption{Comparison to state-of-the-art methods on Meta-Dataset (using a single-domain feature extractor which is trained only on ImageNet). Mean accuracy, 95\% confidence interval are reported. Only ImageNet is seen during training and the rest datasets are unseen for test only.}
		\label{tab:currmethodimagenet}
\end{table*}%

\subsection{Comparison to state-of-the-art methods}
We evaluate our method in two settings, with multi-domain or single-domain feature extractor and compare our method to existing state-of-the-art methods. We also evaluate our method incorporated with different feature extractors, \ie SDL, MDL, and URL in the supplementary.

\paragraph{Multi-domain feature extractor.}
Here we incorporate the proposed residual adapters in matrix form to the multi-domain feature extractor of \cite{li2021universal} and compare its performance with the the state-of-the-art methods  (CNAPS~\cite{requeima2019fast}, SUR~\cite{dvornik2020selecting}, URT~\cite{liu2020universal}, Simple CNAPS~\cite{bateni2020improved}, Transductive CNAPS~\cite{bateni2020enhancing}, FLUTE~\cite{triantafillou2021flute}, tri-M~\cite{liu2021multi}, and URL~\cite{li2021universal}) in \cref{tab:currmethod}. 
To better analyze the results, we divide the table into two blocks that show the few-shot classification accuracy in previously seen domains and unseen domains along with their average accuracy.
We also report average accuracy over all domains and the average rank as in~\cite{triantafillou2021flute,li2021universal}.\footnote{As mentioned in \url{https://github.com/google-research/meta-dataset/issues/54}, we further update the evaluation protocol and report the updated results of all methods in the supplementary.}
Simple CNAPS improves over CNAPS by adopting a simple Mahalanobis distance in stead of learning adapted linear classifier. Transductive CNAPS further improves by using unlabelled test images. SUR and URT fuse multi-domain features to get better performance. FLUTE improves URT by fusing FiLM parameters as initialization which is further finetuned on the support set in meta-test.
tri-M adopts the same strategy of learning modulation parameters as CNAPS, where the parameters are further divided into the domain-specific set and the domain-cooperative set to explore the intra-domain information and inter-domain correlations, respectively.
URL surpasses previous methods by learning a universal representation with distillation from multiple domains.

From the results, our method outperforms other methods on most domains (10 out of 13), especially obtaining significant improvement on 5 unseen datasets than the second best method, \ie Average Unseen (+7.5). More specifically, our method obtains significant better results than the second best approach on Traffic Sign (+19.5), CIFAR-10 (+8.7), and CIFAR-100 (+6.8). 
Achieving improvement on unseen domains is more challenging due to the large gap between seen and unseen domain and the scarcity of labeled samples for the unseen task. 
We address this problem by attaching light-weight adapters to the feature extractor residually and learn the attached adapters on support set from scratch. 
This allows the model to learn more accurate and effective task-specific parameters (adapters) from the support set to efficiently steer the task-agnostic features for the unseen task, compared with predicting task-specific parameters by an auxiliary network learned in meta-train, \eg Simple CNAPS, tri-M, or fusing representations from multiple feature extractors \eg SUR, URT.
Though FLUTE uses a hybrid approach which uses auxiliary networks learned from meta-train to initialize the FiLM parameters for further fine-tuning, their results are not better than URL, 
which achieves very competitive results as it learns a good universal representation that generalizes well to seen domains and can be further improved with the adaptation strategy proposed in this work, especially significant improvements on unseen domains.



\begin{table*}[ht!]
	\centering
    \resizebox{0.95\textwidth}{!}
    {
		\begin{tabular}{lcccccccccccccc|ccccc}

		    \toprule
		    \multirow{2}{*}{Test Dataset} & \multirow{2}{*}{classifier} & Aux-Net & serial or & M or & \multirow{2}{*}{$\beta$} & \multirow{2}{*}{\#params} & Image & Omni & Air- & \multirow{2}{*}{Birds} & Tex- & Quick & \multirow{2}{*}{Fungi} & VGG & Traffic & MS- & \multirow{2}{*}{MNIST} & CIFAR & CIFAR\\
		    & & or Ad & residual & CW & & & -Net & -glot & craft & & tures & Draw & & Flower & Sign & COCO & & -10 & -100 \\
		    \midrule
		    NCC & NCC & - & - & - & \XSolidBrush &  - & $57.0$ & $94.4$ & $88.0$ & $80.3$ & $74.6$ & $81.8$ & $66.2$ & $91.5$ & $49.8$ & $54.1$ & $91.1$ & $70.6$& $59.1$\\ 
		    MD & MD & - & - & - & \XSolidBrush & - & $53.9$ & $93.8$ & $87.6$ & $78.3$ & $73.7$ & $80.9$ & $57.7$ & $89.7$ & $62.2$ & $48.5$ & $95.1$ & $68.9$ & $60.0$ \\ 
		    % \midrule
		    LR & LR & - & - & - & \XSolidBrush & - & $56.0$ & $93.7$ & $88.3$  & $79.7$ & $74.7$ & $80.0$ & $62.1$ & $91.1$ & $59.7$ & $51.2$ & $93.5$  & $73.1$ & $60.1$ \\
		    SVM & SVM & - & - & - & \XSolidBrush & - & $54.5$ & $94.3$ & $87.7$ & $78.1$ & $73.8$ & $80.0$ & $58.5$ & $91.4$ & $65.7$ & $50.5$ & $95.4$ & $72.0$ & $60.5$ \\
		    \midrule
		    Finetune & NCC & - & - & - & \XSolidBrush & - & $55.9$ & $94.0$ & $87.3$ & $77.8$ & $76.8$ & $75.3$ & $57.6$ & $91.5$ & ${\bf 86.1}$ & $53.1$ & ${\bf 96.8}$ & $80.9$ & $65.9$ \\
		    \midrule
		    Aux-S-CW & NCC & Aux-Net & serial & CW & \XSolidBrush & 76.98\% & $54.6$ & $93.5$ &  $86.6$ & $78.6$ &  $71.5$ &   $79.3$ &  $66.0$ &   $87.6$ &  $43.3$ & $49.1$ & $87.9$ &  $62.8$ &  $51.5$  \\
		    Aux-R-CW & NCC & Aux-Net & residual & CW & \XSolidBrush & 76.98\% & $56.1$ &  $94.2$ & $88.4$ & $80.6$ & $74.9$ &  $82.0$ & $66.4$ & $91.6$ & $48.5$ & $53.5$ & $90.8$ &  $70.2$ & $59.7$ \\
		    Aux-S-CW & MD & Aux-Net & serial & CW & \XSolidBrush & 76.98\% & $55.1$ &$93.8$ &$86.8$ & $77.4$ & $73.2$ &$79.9$ &$57.4$ & $88.1$ & $58.4$ & $50.1$ & $92.7$ & $66.5$ & $55.7$ \\
		    Aux-R-CW & MD & Aux-Net & residual & CW & \XSolidBrush & 76.98\% & $54.8$ & $93.8$ & $87.4$ & $78.2$ & $73.4$ & $81.1$ & $58.8$ & $90.1$ & $63.6$ & $48.5$ & $94.8$ & $69.6$ & $60.6$ \\
		    \midrule
		    Ad-S-CW & NCC & Ad & serial & CW & \XSolidBrush & 0.06\% & $56.8$ & $94.8$ & $89.3$ & $80.7$ & $74.5$ & $81.6$ & $65.8$ & $91.3$ & $73.9$ & $53.6$ & $95.7$ & $78.4$ & $64.3$ \\
		    Ad-R-CW & NCC & Ad & residual & CW & \XSolidBrush & 1.57\%   & $57.6$ & $94.7$ & $89.0$ & $81.2$ & $75.2$ & $81.5$ & $65.4$ & $91.8$ & $79.2$ & $54.7$ & $96.4$ & $79.5$ & $67.4$ \\
		    Ad-S-M  & NCC & Ad & serial & M & \XSolidBrush & 12.50\% & $56.2$ & $94.4$ & $89.1$ & $80.6$ & $75.8$ & $81.6$ & ${\bf 67.1}$ & $92.1$ & $67.6$ & $54.8$ & $95.9$ & $78.9$ & $66.6$ \\

		    Ad-R-M & NCC & Ad & residual & M & \XSolidBrush & 10.93\%  & $57.3$ & $94.9$ & $88.9$ & $81.0$ & $76.7$ & $80.6$ & $65.4$ & $91.4$ & $82.6$ & $55.0$ & $96.6$ & $82.1$ & $66.4$ \\
			\midrule
		    Ad-R-CW-PA & NCC & Ad & residual & CW & \Checkmark & 3.91\% & $58.6$ & $94.5$ & ${\bf 90.0}$ & $80.5$ & ${\bf 77.6}$ & ${\bf 81.9}$ & $67.0$ & ${\bf 92.2}$ & $80.2$ & $57.2$ & $96.1$ & $81.5$ & ${\bf 71.4}$ \\
		    Ad-R-M-PA & NCC & Ad & residual & M & \Checkmark & 13.27\% & ${\bf 59.5}$ & ${\bf94.9}$ & $89.9$ & ${\bf 81.1}$ & $77.5$ & $81.7$ & $66.3$ & ${\bf 92.2}$ & $82.8$ & ${\bf 57.6}$ & $96.7$ & ${\bf 82.9}$ & $70.4$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.35cm}
		\caption{Comparisons to methods that learn classifiers and model adaptation methods during meta-test stage based on URL model. NCC, MD, LR, SVM denote nearest centroid classifier, Mahalanobis distance, logistic regression, support vector machines respectively. `Aux-Net or Ad' indicates using Auxiliary Network to predict $\alpha$ or attaching adapter $\alpha$ directly. `M or CW' means using matrix multiplication or channel-wise scaling adapters. `S' and `R' denote serial adapter and residual adapter, respectively. `$\beta$' indicates using the pre-classifier adaptation. The standard deviation results can be found in the supplementary. The first eight datasets are seen during training and the last five datasets are unseen and used for test only. 
		}
		\label{tab:testad}
\end{table*}%

\paragraph{Single-domain feature extractor.}
We also evaluate our method with a single-domain feature extractor trained on ImageNet only on ResNet-18 as in~\cite{triantafillou2019meta} or ResNet-34 as in~\cite{doersch2020crosstransformers}.
This setting is more challenging than the multi-domain one, as the model is trained only on one domain and tested on both test split of ImageNet but also of other domains.
We report the results of our method and state-of-the-art methods (BOHB~\cite{saikia2020optimized}, FLUTE~\cite{triantafillou2021flute}, Finetune~\cite{triantafillou2019meta}, ProtoNet~\cite{triantafillou2019meta}, fo-Proto-MAML~\cite{triantafillou2019meta}, and ALFA+fo-Proto-MAML~\cite{triantafillou2019meta}, CTX~\cite{doersch2020crosstransformers}) in \cref{tab:currmethodimagenet}.
ALFA+fo-Proto-MAML achieves the prior best performance by combining the complementary strengths of Prototypical Networks and MAML (fo-Proto-MAML), with extra meta-learning of per-step hyperparameters: learning rate and
weight decay coefficients. FLUTE fails to surpass it with one training source domain, probably due to the lack of FiLM parameters from multiple domains.
Our method, when using ResNet18 backbone, outperforms other methods on all domains, especially obtaining significant improvement, \ie Average Unseen (+9.5), on 12 unseen datasets than the second best method.
We compare our method to CTX and ProtoNet, which use ResNet-34 backbone.
\footnote{Note that CTX also uses augmentation strategies such as AutoAugment~\cite{cubuk2019autoaugment} and other ones from SimClr~\cite{chen2020simple}. We expect applying the same augmentation strategies to our method would yield further improvements, but we leave this for future work.}
CTX is very competitive by learning coarse spatial correspondence between the query and the support images with an attention mechanism. Ours is orthogonal to CTX and both CTX and our method can potentially be complementary, but we leave this as future work due to high computational cost of CTX.
Specifically,
we see that our method obtains the best average rank and outperforms CTX on most domains (6 out of 10) while our method being more efficient (We train our model on one single Nvidia GPU for around 33 hours while CTX requires 8 Nvidia V100 GPUs and 7 days for training. Please refer to the supplementary for more details). 





\begin{figure*}[ht]
\RawFloats
\noindent\begin{minipage}[t]{0.32\textwidth}
\begin{center}
\includegraphics[width=0.9\linewidth]{./figures/oldurl-iteration.pdf}
\end{center}
\vspace{-0.3in}
\caption{Sensitivity of performance to number of iterations.}
\label{fig:urlstab}
\end{minipage} \hfill
\begin{minipage}[t]{0.32\textwidth}
\begin{center}
\includegraphics[width=0.96\linewidth]{./figures/allRA_layers.pdf}
\end{center}
\vspace{-0.3in}
\caption{Block (layer) analysis for adapters.}
\label{fig:urllayers}
\end{minipage} \hfill
\begin{minipage}[t]{0.32\textwidth}
\begin{center}
\includegraphics[width=0.9\linewidth]{./figures/DecomRAdinl34.pdf}
\end{center}
\vspace{-0.3in}
\caption{Decomposed residual adapters on block-3,4.}
\label{fig:urldecoml34}
\end{minipage} \hfill
\vspace{-0.35cm}
\end{figure*}

\subsection{Analysis of task-specific parameterizations}
\label{sec:analysis}

\paragraph{Classifier learning.}
First we study the adaptation strategies for learning only a task-specific classifier on the pre-trained feature extractor of \cite{li2021universal}.
We evaluate non-parametric classifiers including nearest cetroid classifier (\emph{NCC}) and NCC Mahalanobis Distance (\emph{MD}) and parametric classifiers including logistic regression (\emph{LR}), support vector machine \emph{SVM} whose parameters are learned on support samples.
We also include another baseline with NCC that finetunes all the feature extractor parameters, and report the results in \cref{tab:testad}.
We observe that NCC obtains the best results for the seen domains and its performance is further improved by MD, while SVM achieves the best for the unseen domains among other classifiers. 
Finetuning baseline provides competitive results especially for the unseen domains. However, it performs poor in most seen domains.

\paragraph{Feature extractor adaptation.}
Next we analyze various design decisions for the feature extractor adaptation including connection types (serial, residual), \ie \cref{fig:framework}(b), (c), its parameterization including channelwise modulation (\emph{CW}) when they are estimated by an auxiliary network (\emph{Aux-Net}), which has around 77\% capacity of the feature extractor.
We use with each combination with two nonparameteric classifier, either NCC or MD.
\emph{While the adaptation strategies using residual connections performs better than the serial one in almost all cases, the gains are more substantial when generalizing to unseen domains.}
Learning adapter weights from few samples only can be very noisy. With residual addition, it is not necessary to change all connections for passing the information forward, which can improve the robustness of useful features and reduce learning burdens for new task, hence increase the generalization ability. While the serial connections may damage the previous learned structures.
We also observe that NCC and MD obtain comparable performances.
Note that Aux-S-CW with MD corresponds to our implementation of Simple CNAPS~\cite{bateni2020improved} with the more powerful feature extractor.
We show that replacing its serial connection with a residual one leads to a strong performance boost.

Next we look at the adaptation strategy that learns the task-specific weights directly on the support set as in~\cref{eq:learn-tsw}.
We evaluate serial and residual connection types with channelwise and matrix parameterizations by using NCC. 
We denote this setting as Ad in \cref{tab:testad}.
Note that we omit MD here, as it produces similar results to NCC.
First we observe that learning the weights on the support set outperforms the strategy of estimating them through an auxiliary network almost in all cases.
In addition, the learnable weights requires less number of parameters per task, while the capacity of auxiliary network is fixed.
We again observe that the residual connections are more effective, especially when used with the matrix parameterization (Ad-R-M).
However,the channelwise ones provide a good performance/computation tradeoff.
Finally using the pre-classifier alignment (Ad-R-CW-PA and Ad-R-M-PA) further boosts the performance of the best models and we use our best model Ad-R-M-PA to compare against the state-of-the-art.






\subsection{Further results}
\vspace{-0.15cm}
% similar to url
\paragraph{Varying-way Five-shot.}
After evaluating our method over a broad range of varying shots (\eg up to 100 shots), we follow \cite{doersch2020crosstransformers,li2021universal} to further analyze our method in 5-shot setting of varying number of categories.
In this setting, we sample a varying number of ways with a fixed number of shots to form balanced support and query sets. As shown in Table~\ref{tab:fixedshot}, overall performance for all methods decreases in most datasets compared to results in Table~\ref{tab:currmethod} indicating that this is a more challenging setting. It is due to that five-shot setting samples much less support images per class than the standard setting. 
The top-2 methods remain the same and ours still outperforms the state-of-the-art URL when the number of support images per class is fewer, especially on unseen domains (Average Unseen +6.2).

\paragraph{Five-way One-shot.}
The similar conclusion can be drawn from this challenging case. Note that there are extremely few samples available for training in this case. As we can see, Ours achieves similar results with URL on seen domains but much better performance on unseen domains due to the learning of attached residual adapters is less over-fitting.

\begin{table}[h]
% \vspace{-0.25cm}
	\centering
	    \resizebox{1.0\textwidth}{!}
    {
		\begin{tabular}{cccccc|ccccc}
			& \multicolumn{5}{c}{Varying-Way Five-Shot} & \multicolumn{5}{c}{Five-Way One-Shot} \\
		    \toprule
		    \multirow{2}{*}{Test Dataset} & Simple & SUR & URT & URL & \multirow{2}{*}{Ours}& Simple & SUR & URT & URL & \multirow{2}{*}{Ours}\\
		    &  CNAPS~\cite{bateni2020improved}  & \cite{dvornik2020selecting} & \cite{liu2020universal} & \cite{li2021universal} & & CNAPS~\cite{bateni2020improved} & \cite{dvornik2020selecting}& \cite{liu2020universal} & \cite{li2021universal} & \\
		    \midrule
		    Average Seen & $69.0$ & $71.2$ & $73.8$ & $76.6$ & ${\bf 76.7}$ & $65.0$ & $64.0$ & $70.6$ & $73.4$ & ${\bf 73.5}$ \\
		    Average Unseen & $62.6$ & $56.0$ & $59.6$ & $65.2$ & ${\bf 71.4}$ & $57.7$ & $49.6$ & $57.5$ & $62.4$ & ${\bf 63.4}$ \\
		    Average All & $66.5$ & $65.4$ & $68.3$ & $72.2$ & ${\bf 74.6}$ & $62.2$ & $58.5$ & $65.5$ & $69.2$ & ${\bf 69.6}$ \\
		    \midrule
		    Average Rank & $4.1$ & $3.9$ & $3.4$ & $2.1$ & ${\bf 1.5}$ & $3.8$ & $4.5$ & $3.3$ & ${\bf 1.7}$ & ${\bf 1.7}$ \\
			\bottomrule
		\end{tabular}%
			}
		\vspace{-0.25cm}
		\caption{Results of  Varying-Way Five-Shot and Five-Way One-Shot scenarios. Mean accuracies are reported and more detailed results can be found in the supplementary.}
		\label{tab:fixedshot}
\end{table}%
\vspace{-0.2cm}

\subsection{Further ablation study}
Here, we conduct ablation study for the sensitivity analysis for number of iterations, layer analysis for adapters, and decomposed residual adapters. We summarize results in figures and refer to supplementary for more detailed results.

% stable
\paragraph{Sensitivity analysis for number of iterations.}
In our method, we optimize the attached parameters ($\alpha,\beta$) with 40 iterations. \Cref{fig:urlstab} reports
the results with 10, 20, 40, 60 iterations and indicates that our method (solid green) converges to a stable solution after 20 iterations and achieves better average performance on all domains than the baseline URL (dash green). 


% layer 4, layer 3-4, all layers
\paragraph{Layer analysis for adapters.}
Here we investigate whether it is sufficient to attach the adapters only to the later layers.
We evaluate this on ResNet18 which is composed of four blocks and attach the adapters to only later blocks (block4, block3,4, block2,3,4 and block-all, see \cref{fig:framework}). 
\Cref{fig:urllayers} shows that applying our adapters to only the last block (block4) obtains around 78\% average accuracy on all domains which outperforms the URL. With attaching residual adapters to more layers, the performance on unseen domains is improved significantly while the one on seen domains remains stable.



% reduce parameters without too much drop on performance
\paragraph{Decomposing residual adapters.}
Here we investigate whether one can reduce the number of parameters in the adapters while retaining its performance by using matrix decomposition (see \cref{sec:method}).
As in deep neural network, the adapters in earlier layers are relatively small, we then decompose the adapters in the last two blocks only where the adapter dimensionality goes up to $512\times 512$. \Cref{fig:urldecoml34} shows that our method can achieve good performance with less parameters by decomposing large residual adapters, (\eg when $N=32$ where the number of additional parameters equal to around 4\% vs 13\%, the performance is still comparable to the original form of residual adapters, \ie N=0). We refer to supplementary for more details.



