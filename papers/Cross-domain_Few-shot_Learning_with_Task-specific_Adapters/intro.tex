% !TEX root = main.tex

% Few-shot learning problem
Deep learning methods have seen remarkable progress in various fields where large quantities of data and compute power are available.
However, the ability of deep networks to learn new concepts from small data remains limited.
Few-shot classification~\cite{lake2011one,miller2000learning} is inspired from this limitation and aims at learning a model that can be efficiently adapted to recognize unseen classes from few samples.
In particular, the standard setting for learning few-shot classifiers involves two stages: (i) learning a model, typically from a large training set, (ii) adapting this model to learn new classes from a given small support set. 
These two stages are called meta-training and meta-testing respectively.
The adapted model is finally evaluated on a query set where the task is to assign each query sample to one of the classes in the support set.

\begin{figure}
\begin{center}
\includegraphics[width=0.95\linewidth]{figures/cdfsl.pdf}
\end{center}
\vspace{-0.45cm}
\caption{\textbf{Cross-domain Few-shot Learning} considers to learn a model from one or multiple domains to generalize to unseen domains with few samples. Prior works often learn a task-agnostic model with an auxiliary network during meta-training (a) and a set of adapters are generated by the auxiliary network to adapt to the given support set (b). While in this work, we propose to attach adapters directly to a pretrained task-agnostic model (c), which can be estimated from scratch during meta-testing (d). We also propose different architecture topologies of adapters and their efficient approximations. 
}
\label{fig:cdfsl}
\end{figure}


% Earlier work
Early methods~\cite{vinyals2016matching,ravi2016optimization,finn2017model,oreshkin2018tadam,rusu2018meta,snell2017prototypical} pose the few-shot classification problem in a learning-to-learn formulation by training a deep network over a distribution of related tasks, which are sampled from the training set, and transfer this experience to improve its performance for learning new classes.
Concretely, Vinyals~\etal~\cite{vinyals2016matching} learn a feature encoder that is conditioned on the support set in meta-training and does not require any further training in meta-test thanks to its non-parametric classifier.
Ravi and Larochelle~\cite{ravi2016optimization} take the idea of learning a feature encoder in meta-train further by also learning an update rule through an LSTM that produces the updates for a classifier in meta-test.
Finn~\etal~\cite{finn2017model} pose the task as a meta-learning problem and learn the parameters of a deep network in meta-training such that a network initialized with the learned parameters can be efficiently finetuned on a new task.
We refer to \cite{wang2020generalizing, hospedales2020meta} for comprehensive review of early works.


% Cross-domain few-shot learning
Despite the significant progress, the scope of the early methods has been limited to a restrictive setting where training and test samples come from a single domain (or data distribution) such as Omniglot~\cite{Lake1332}, miniImageNet~\cite{vinyals2016matching} and tieredImageNet~\cite{ren2018meta}.
They perform poorly in the more challenging cross-domain few-shot tasks, where test data is sampled from an unknown or previously unseen domain~\cite{triantafillou2019meta}.
This setting poses an additional learning challenge, not only requires leveraging the limited information from the small support set for learning the target task but also \emph{selectively transferring relevant knowledge} from previously seen domains to the target task.

Broadly, recent approaches address this challenge by parameterizing deep networks with a large set of task-agnostic and a small set of task-specific weights that encode generic representations valid for multiple tasks and private representations are specific to the target task respectively.
While the task-agnostic weights are learned over multiple tasks, typically, from a large dataset in meta-training, the task-specific weights are estimated from a given small support set (\eg 5 images per category)~\cite{requeima2019fast,bateni2020improved,lee2019meta,dvornik2020selecting,liu2020universal,li2021universal,triantafillou2021flute}.
In the literature, the task-agnostic weights are used to parameterize a single network that is trained on large data from one domain~\cite{requeima2019fast,bateni2020improved,doersch2020crosstransformers} or on multiple domains~\cite{li2021universal}, or to be distributed over multiple networks, each trained on a different domain~\cite{dvornik2020selecting,liu2020universal,triantafillou2021flute} \footnote{Note that the task-agnostic weights can also be finetuned on the target task (\eg \cite{chen2020new,dhillon2019baseline}).}.
The task-specific weights are utilized to parameterize a linear classifier~\cite{lee2019meta}, a pre-classifier feature mapping~\cite{li2021universal} and an ensemble of classifiers at each layer of a deep neural network~\cite{adler2020cross}.

Recently, inspired from \cite{perez2018film}, \emph{task-specific adapters}~\cite{requeima2019fast,bateni2020improved}, small capacity transformations that are applied to multiple layers of a deep network, have been successfully used to steer the few-shot classifiers to new tasks and domains.
Their weights are often estimated dynamically through an auxiliary network conditioned on the support set~\cite{requeima2019fast,bateni2020improved,liu2020universal,triantafillou2021flute} (see \cref{fig:cdfsl}.(a,b)), in a similar spirit to~\cite{bertinetto2016learning,jia2016dynamic}.
As the auxiliary network is trained on multiple tasks in meta-training, the premise of estimating the task-specific adapter weights with it is based on the principle of transfer learning such that it can transfer the knowledge from the previous tasks to better estimate them for unseen tasks.
However, learning an accurate auxiliary network is a challenging task due to two reasons.
First, it has to generalize to previously unseen tasks and especially to significantly different unseen domains.
Second, learning to predict high-dimensional weights where each corresponds to a dimension of a highly nonlinear feature space is a difficult learning problem too.



Motivated by this shortcoming, as shown in \cref{fig:cdfsl}, we propose to employ a set of light-weight task-specific adapters along with the task-agnostic weights for adapting the few-shot classifier to the tasks from unseen domains.
Unlike the prior work, we learn the weights of these adapters from scratch by directly optimizing them on a small support set (see \cref{fig:cdfsl}.(c,d)).
Moreover, we systematically study various combinations of several design choices for task-specific adaptation, which have not been explored before, including adapter connection types (serial or residual), parameterizations (matrix and its decomposed variations, channelwise operations) and estimation of task-specific parameters.
Extensive experiments demonstrate that attaching parameteric adapters in matrix form to convolutional layers with residual connections significantly boosts the state-of-the-art performance in most domains, especially resulting in superior performance in unseen domains on Meta-Dataset with negligible increase in computations.


\paragraph{More related work.} 
Here we provide more detailed discussion of the most related work.
Both CNAPS~\cite{requeima2019fast} and Simple CNAPS~\cite{bateni2020improved} employ task-specific adapters via FiLM layers (which uses a channelwise affine transformation and connected to the backbone in a serial way)~\cite{perez2018film} to adapt their feature extractors to the target task and estimate them via an auxiliary network. 
Compared to them, we propose learning residual adapters in matrix form directly on the support set.
SUR~\cite{dvornik2020selecting} and URT~\cite{liu2020universal} learn an attention mechanism to select/fuse features from multiple domain-specific models in meta-train respectively. 
As we build on a single multi-domain feature extractor, our method does not require such attention but we attach task-specific adapters to the feature extractor to adapt the features to unseen tasks.
URL~\cite{li2021universal} learns a pre-classifier feature mapping to adapt the feature from a single task-agnostic model learned from multiple domains for unseen tasks. 
While we build on their feature extractor and pre-classifier alignment, the pre-classifier alignment provides very limited capacity for task adaptation, which we address by adapting the feature extractor with adapters at multiple layers.
FLUTE~\cite{triantafillou2021flute} follows a hybrid three step approach that first learns the parameters of domain-specific FiLM layers so called templates, employs an auxiliary network to initialize the parameters of a new FiLM layer for unseen task by combining the templates and finetunes them on the small support set.
Different from FLUTE, our method learns such adaptation in a single step by learning residual adapters in meta-test.

There are also methods (\eg \cite{saikia2020optimized,doersch2020crosstransformers}) that do not fit into task-agnostic and task-specific parameterization grouping.
BOHB~\cite{saikia2020optimized} proposes to use multi-domain data as validation objective for hyper-parameter optimization such that the feature learned on ImageNet with the optimized hyper-parameter generalizes well to multi-domain. 
CTX~\cite{doersch2020crosstransformers} proposes to learn spatial correspondences from ImageNet and evaluates on the remaining (unseen) domains.
We also compare our method to them in the setting where we use a standard single domain learning network learned from ImageNet and adapt its representations through residual adapters. 

