\documentclass{ws-rmta}




%\documentclass[probth]{svjour}
%\documentclass[reqno]{amsart}
%\documentclass[onecolumn,draft]{IEEEtran}
%\documentclass[a4paper,onecolumn,draft]{IEEEtran}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage{eucal}
\usepackage{graphicx}
%\usepackage{verbatim}
%\usepackage{pdfsync}
%\usepackage{stmaryrd}
\usepackage{bbm}
%\usepackage{mathabx}
%\usepackage{arydshln}
%\usepackage{flushend}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[T2A,T1]{fontenc}
\newcommand\script[1]{{\fontfamily{pzc}\fontshape{it}\selectfont#1}}
%\usepackage{algorithmic}


%\usetikzlibrary{arrows,petri,topaths}
%\usepackage{tkz-berge}


%\newcommand{\sq}[1]{\ensuremath{\mathsf{#1}}}
%\newcommand{\pw}[1]{\psframebox[linewidth=0.4pt,framearc=.3]{#1}}

% Shorthand notations for sets N, Z, R and C
\newcommand{\conjug}[1]{{\fontdimen8\textfont3=0.25pt\mkern2mu\overline{\mkern-1mu #1\mkern-2mu}}\mkern2mu}
\newcommand{\cor}{\text{\reflectbox{R}}}
\newcommand{\un}{\mathbbm{1}}
\newcommand{\RR}{{\mathbb{R}}}
\newcommand{\NN}{{\mathbb{N}}}
\newcommand{\ZZ}{{\mathbb{Z}}}
\newcommand{\CC}{{\mathbb{C}}}
\newcommand{\herm}{{\sf H}}
\newcommand{\trans}{{\sf T}}
\newcommand{\N}{{\bf N}}
\newcommand{\V}{{\bf V}}
\newcommand{\J}{{\bf J}}
\newcommand{\D}{{\bf D}}
\newcommand{\R}{{\bf R}}
\newcommand{\X}{{\bf X}}
\newcommand{\Z}{{\bf Z}}
\newcommand{\G}{{\bf G}}
\newcommand{\Q}{{\bf Q}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\W}{{\bf W}}
\newcommand{\A}{{\bf A}}
\newcommand{\B}{{\bf B}}
% \newcommand{\C}{{\bf C}}
\newcommand{\T}{{\bf T}}
% \newcommand{\U}{{\bf U}}
\newcommand{\K}{{\bf K}}
\newcommand{\I}{{\bf I}}
% \newcommand{\x}{{\bf x}}
\newcommand{\h}{{\bf h}}
\newcommand{\s}{{\bf s}}
\newcommand{\z}{{\bf z}}
\newcommand{\p}{{\bf p}}
\newcommand{\bnu}{{\bm \nu}}
\newcommand{\y}{{\bf y}}
\newcommand{\uu}{{\boldsymbol u}}
\newcommand{\vv}{{\boldsymbol v}}
\newcommand{\e}{{\bf e}}
\newcommand{\w}{{\bf w}}
% \newcommand{\f}{{\bf f}}
\newcommand{\n}{{\bf n}}
\newcommand{\oh}{{\frac{1}{2}}}

\newcommand{\asto}{\overset{\rm a.s.}{\longrightarrow}}
\newcommand{\EE}{{\rm E}}
\newcommand{\lip}{\mathcal L \textit{ip}}

% \usepackage[T2A,T1]{fontenc}
% \newcommand{\Zhe}{\mbox{\usefont{T2A}{\rmdefault}{m}{n}\CYRZH}}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\diag}{Diag}
\DeclareMathOperator{\Sp}{Sp}
\DeclareMathOperator{\trd}{TrDg}
\DeclareMathOperator{\res}{R}
\DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\vect}{Vect}
\def\restriction#1#2{\mathchoice
              {\setbox1\hbox{${\displaystyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\textstyle #1}_{\scriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}
              {\setbox1\hbox{${\scriptscriptstyle #1}_{\scriptscriptstyle #2}$}
              \restrictionaux{#1}{#2}}}
\def\restrictionaux#1#2{{#1\,\smash{\vrule height .8\ht1 depth .85\dp1}}_{\,#2}} 

% \DeclareSymbolFont{cyrillic}{T2A}{cmr}{m}{n}
% \DeclareMathSymbol{\Sha}{\mathalpha}{cyrillic}{216}
% \DeclareMathSymbol{\cor}{\mathalpha}{cyrillic}{223}
%\DeclareMathOperator{\liminf}{liminf}
%\DeclareMathOperator{\limsup}{limsup}

\newcounter{ctheorem}
\newtheorem{theoreme}[ctheorem]{Theorem}
\newcounter{coffshot}
\newtheorem{declinaison}[coffshot]{Offshot}
% \newcounter{cassumption}
% \newtheorem{assumption}{Assumption}
\newcounter{csetting}
\newtheorem{setting}[csetting]{Setting}

%\spnewtheorem{assumption}{Assumption}{\bf}{\it}

% \newproof{proof}{Proof}

\newcounter{cexample}
\newtheorem{exemple}[cexample]{Example}
\newcounter{cproposition}
% \newtheorem{proposition}[cproposition]{Proposition}
\newtheorem{property}[cproposition]{Property}
\newcounter{ccorollary}
\newtheorem{corollaire}[ccorollary]{Corollary}
\newcounter{clemma}
\newtheorem{lemme}[clemma]{Lemma}
\newcounter{cdefinition}
% \newtheorem{definition}[cdefinition]{Definition}
\newtheorem{defpro}[cdefinition]{Definition/Proposition}
\newcounter{cremark}
\newtheorem{remarque}[cremark]{Remark}
\newcounter{cconjecture}
\newtheorem{conjecture}[cconjecture]{Conjecture}
 \newenvironment{assbis2}
  {\addtocounter{assumption}{-2}%
   \renewcommand{\theassumption}{\arabic{assumption} bis}%
   \begin{assumption}}
  {\end{assumption}
  \addtocounter{assumption}{2}}
  \newenvironment{asster3}
  {\addtocounter{assumption}{-4}%
   \renewcommand{\theassumption}{\arabic{assumption} ter}%
   \begin{assumption}}
  {\end{assumption}
  \addtocounter{assumption}{1}}
  \newenvironment{asster3_2}
  {\addtocounter{assumption}{-2}%
   \renewcommand{\theassumption}{\arabic{assumption} ter}%
   \begin{assumption}}
  {\end{assumption}
  \addtocounter{assumption}{2}}
  \newenvironment{assbis7}
  {\addtocounter{assumption}{-7}%
   \renewcommand{\theassumption}{\arabic{assumption} bis}%
   \begin{assumption}}
  {\end{assumption}
  \addtocounter{assumption}{7}} 

\def\bibsection{\section*{References}}

%\synctex=1


\pgfplotsset{compat=1.11}

\begin{document}



%%%%%%%%%%%%%%%%%%%%% Publisher's Area please ignore %%%%%%%%%%%%%%%
%
\catchline{}{}{}{}{}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Sharp Bounds for the Concentration of the Resolvent \\ in Convex Concentration Settings}

\author{Cosme Louart\footnote{
GIPSA-lab.}}

\address{GIPSA-lab, 11 rue des Mathématiques,
38401 St Martin d'Hères\\ 
\email{cosmelouart@gmail.com} }


\maketitle

\begin{history}
\received{(26/12/2021)}
%\revised{(Day Month Year)}
\end{history}

\begin{abstract}
Considering random matrix $X \in \mathcal M_{p,n}$ with independent columns satisfying the convex concentration properties issued from a famous theorem of Talagrand, we express the linear concentration of the resolvent $Q = (I_p - \frac{1}{n}XX^T) ^{-1}$ around a classical deterministic equivalent with a good observable diameter for the nuclear norm. The general proof relies on a decomposition of the resolvent as a series of powers of $X$.
%We propose here to study the concentration of random objects that are implicitly formulated as fixed points to equations $Y = f(X)$ where $f$ is a random mapping. Starting from an hypothesis taken from the concentration of the measure theory, we are able to express precisely the concentration of such solutions, under some contractivity hypothesis on $f$. This statement has important implication to random matrix theory, and is at the basis of the study of some optimization procedures like the logistic regression for instance. In those last cases, we give precise estimations to the first statistics of the solution $Y$ which allows us predict the performances of the algorithm.
\end{abstract}

\keywords{random matrix theory ; concentration of measure ; convariance matrices ; convex concentration ; concentration of products and infinite series ; Hanson-Wright Theorem.}

\ccode{Mathematics Subject Classification 2000: 15A52, 60B12, 62J10}


% \section{Convex concentration Hypotheses}
\section*{Introduction}
Initiated by Milman in the 70s \cite{MIL71}, the theory of concentration of the measure provided a wide range of concentration inequalities, mainly concerning continuous distribution (i.e. with no atoms), in particular thanks to the beautiful interpretation with the bound on the Ricci curvature \cite{GRO79}. To give a simple fundamental example (more examples can be found in \cite{LED05}), a random vector $Z \in \mathbb R^n$ having independent and Gaussian entries with unit variance satisfies for any $1$-Lipschitz mapping $f: \mathbb R^n \to \mathbb R$:
\begin{align}\label{eq:concentration_lipschitz}
  \forall t >0: \ \ \mathbb P \left( \left\vert f(Z) - \mathbb E[f(Z)] \right\vert \geq t \right) \leq 2 e^{-t^2/2}.
\end{align}
This inequality is powerful for two reasons: first, it is independent on the dimension $n$, second it concerns any $1$-Lipschitz mapping $f$. It is then interesting to formalize this behavior to introduce a class of ``Lipschitz concentrated random vectors'' satisfying the same concentration inequality as the Gaussian distribution (in particular, all the Lipschitz transformation of a Gaussian vector). This was done in several books and papers where this approach proved its efficiency in the study of random matrices \cite{TAO12,ELK09,LOU21RHL}...

% The independence towards the dimension of this inequality and the 
We want here to extend those results to a new class of concentrated vectors discovered by  Talagrand in \cite{TAL95}. Although the concentration result looks similar, its nature is quite different as it concerns bounded distributions for which classical tools of differential geometry do not operate. In a sense, it could be seen as a combinatorial result of concentration.
Given a random vector $Z \in [0,1]^n$ with independent entries, this result sets that for any $1$-Lipschitz \emph{and convex} mapping $f: [0,1] \to \mathbb R$:
\begin{align}\label{eq:concentration_convexe}
  \forall t >0: \ \ \mathbb P \left( \left\vert f(Z) - \mathbb E[f(Z)] \right\vert \geq t \right) \leq 2 e^{-t^2/4}.
\end{align}
We can mention here the recent results of \cite{Hua21} that extend this kind of inequalities for random vectors with independent and subgaussian entries.
Adopting the terminology of \cite{VU2014,MEC11,ADA11}, we call those vectors convexly concentrated random vector (see Definition~\ref{def:cnocentration_convexe} below).
The convexity required for the observations to concentrate makes the discussion on convexly concentrated random vector far more delicate. There is no more stability towards Lipschitz transformations and given a convexly concentrated random vector $Z$, just its affine transformations are sure to be concentrated. 
This issue raises naturally for one of the major objects of random matrix theory, namely the resolvent $Q^z = (zI_n - X)^{-1}$ that can provide important eigen properties on $X$. %whose concentration raises several issues in the case of convex concentration. 
In the case of convex concentration, the concentration of the resolvent $Q^z = (zI_n - X)^{-1}$ is no more a mere consequence of a bound on its differential on $X\in \mathcal{M}_{p}$. Still, as first shown by \cite{Gui00}, it is possible to obtain concentration properties on the sum of Lipschitz functionals of the eigen values. Here we pursue the study, looking at \textit{linear} concentration properties of $Q^z$ for which similar inequalities to \eqref{eq:concentration_lipschitz} or~\eqref{eq:concentration_convexe} are only satisfied by $1$-Lipschitz \emph{and linear} functionals $f$. 
% This is an extension of the result of \cite{Gui00} since t
The well known identity
\begin{align}\label{eq:importance_stieltjes_transform}
  \frac{1}{p}\sum_{\lambda \in \text{Sp}(X)} f(\lambda) = - \frac{1}{2i\pi} \oint_\gamma f(z) \tr(Q^z)dz,
\end{align}
is true for any analytical mapping $f$ defined on the interior of a path $\gamma \in \mathbb C$ containing the spectrum of $X$ (or any limit of such mappings), therefore, our results on the concentration of $Q^z$ concern in particular the quantities studied in \cite{Gui00}.

% Note that $\tr(Q^z)$ is only a particular linear functional of $Q^z$, we prove in the paper the concentration of any $\tr(AQ^z)$, for $A$ deterministic and provide a deterministic equivalent.

Although it is weaker\footnote{Lipschitz concentrated random vectors are convexly and linearly concentrated, convexly concentrated random vectors are linearly concentrated.}, the class of linearly concentrated vectors behaves very well towards the dependence and the sum and allows us to obtain the concentration of the resolvent expressing it as a sum $Q^z = \frac{1}{z}\sum_{i=1}^\infty (X/z)^i$. The linear concentration of the powers of $X$ was justified in\footnote{We provide an alternative proof in the appendix.} \cite{MEC11} in the case of convexly concentrated random matrix $X$. We call this weakening of the concentration property ``the degeneracy of the convex concentration through multiplication''. The linear concentration of the resolvent is though sufficient for most practical applications that rely on an estimation of the Stieltjes transform $m(z) = \frac{1}{p} \tr(Q^z)$.% and estimation of the projection on a signal $u$: $u^T Q^zu$).
%This method is quite elegant but it does not allow us to express the concentration of $Q^z = (zI_n - X)^{-1}$ when $z$ is inside a disc containing $\mathbb E[\|X\|]$ and centered on $0$. 
%For those values, we employ a laborious technique that decomposes each linear functional of $Q^z$ into a sum of Lipschitz and convex mappings.

We present below our main contribution without the useful but non-standard formalism introduced in the rest of the article. It concerns the concentration and the estimation of\footnote{The concentration of $ (zI_p - \frac{1}{\sqrt n}X)^{-1}$ for a positive symmetric matrix $X \in \mathcal{M}_{p}$ is immediate from our approach. The estimation of its expectation is more laborious and goes beyond the scope of the paper although it can be obtained with the same tools.}
\begin{align*}
   Q^z \equiv \left( zI_p - \frac{1}{n}XX^T \right)^{-1}
 \end{align*} for a random matrix $X \in \mathcal{M}_{p,n}$.
% \begin{proposition}[Concentration of infinite sum]
%   Let us consider a sequences of random variables $(Z_n^{(m)})_{n,m \in \mathbb N} \in \mathbb R^{\mathbb N^2}$ and a sequence of parameters $(\sigma_n^{(m)})_{n,m \in \mathbb N} \in \mathbb R_+^{\mathbb N^2}$ such that there exists two constants $C,c>0$ satisfying for any $n,m\in \mathbb N$:
%    % and any deterministic vector $u \in \mathbb R^{p_n}$:
%   \begin{align*}
%     \mathbb P \left( \left\vert Z_n^{(m)} - \mathbb E[Z_n^{(m)}] \right\vert \geq t \right) \leq Ce ^{-c(t/\sigma^{(m)}_n)^2},
%   \end{align*}
%   There exist two constants $C',c'>0$ such that for any sequence $(a_m)_{m\in \mathbb N} \in \mathbb R^{\mathbb N}$ and for any $n \in \mathbb N$:
%   \begin{align*}
%     \mathbb P \left( \left\vert \sum_{m=1}^\infty a_m \left( Z_n^{(m)} - \mathbb E \left[ Z_n^{(m)} \right] \right) \right\vert \geq t \right) \leq C'e ^{-c'(t/\sigma^a_n)^2},
%   \end{align*}
%   where, for all $n\in \mathbb N$, $\sigma^a_n \equiv \sum_{m=1}^\infty |a_m | \sigma_n^{(m)}$
% \end{proposition}
% \begin{theorem}[Concentration of entry-wise product]
%   Considering a sequence of integers $m_n \in \mathbb N^{\mathbb N}$ two sequence of scalars $\sigma_n,\kappa_n>0$ and two constants $C,c>0$, we suppose we are given for all $n\in \mathbb N$, $m_n$ random vectors $X^{(1)}_n, \ldots, X_n^{(m_n)} \in \mathbb R^n$ such that $\sup_{1\leq j \leq m_n} \|X_n^{(j)}\|_\infty \leq \kappa_n$ and for any quasi-convex mapping $g: (\mathbb R^n)^{m_n} \to \mathbb R$, $1$-Lipschitz for the euclidean norm
%   \begin{align*}
%     \mathbb P \left( \left\vert g(X^{(1)}_n, \ldots, X^{(m_n)}_n) - \mathbb E \left[ g(X^{(1)}_n, \ldots, X^{(m_n)}_n) \right] \right\vert \geq t\right) \leq C e^{-c(t/\sigma_n)^2}.
%   \end{align*}
%   Then there exit two constants $c',C'>0$ such that for any $n \in \mathbb N$, for any deterministic vector $u \in \mathbb R^n$:
%   \begin{align*}
%     \mathbb P \left( \left\vert u^{T} \left( X^{(1)}_n \odot \cdots \odot X^{(m_n)}_n - \mathbb E[X^{(1)}_n \odot \cdots \odot X^{(m_n)}_n] \right) \right\vert \geq t\right) \leq C' \exp \left( -c' \left( \frac{t}{\sigma_n (2e\kappa_n)^{m_n - 1}} \right)^2 \right)
%   \end{align*}
%   where $\odot$ designates the entry-wise product. The term ``$2e$'' disappears when $X_1 = \cdots = X_n$.
% \end{theorem}
% \begin{theorem}[Concentration of matricial product]
%   Considering a sequence of integers $m_n \in \mathbb N^{\mathbb N}$, two sequence of scalars $\sigma_n,\kappa_n>0$ and two constants $C,c>0$, we suppose we are given for all $n\in \mathbb N$, $m_n$ random vectors: $X^{(1)}_n, \ldots, X_n^{(m_n)} \in \mathcal M_n$ such that $\sup_{1\leq j \leq m_n} \|X_n^{(j)}\| \leq \kappa_n$ and for any quasi-convex mapping $g: \mathcal M_n^{m_n} \to \mathbb R$, $1$-Lipschitz for the euclidean norm:
%   \begin{align*}
%     \mathbb P \left( \left\vert g \left( X^{(1)}_n, \ldots, X^{(m_n)}_n \right) - \mathbb E \left[ g \left( X^{(1)}_n, \ldots, X^{(m_n)}_n \right) \right] \right\vert \geq t\right) \leq C e^{-c(t/\sigma_n)^2}.
%   \end{align*}
%   Then there exist two constants $c',C'>0$ such that for any $n \in \mathbb N$, for any deterministic matrix $A \in \mathbb R^n$ such that $\|A\| \leq 1$:
%   \begin{align*}
%     \mathbb P \left( \left\vert  \tr \left( \left( X^{(1)}_n \cdots  X^{(m_n)}_n - \mathbb E \left[ X^{(1)}_n  \cdots X^{(m_n)}_n \right] \right) A \right) \right\vert \geq t\right) \leq C' \exp \left( -c' \left( \frac{t}{\sqrt n \sigma_n \kappa_n^{m_n - 1}} \right)^2 \right).
%   \end{align*}
%   % where $\odot$ designates the entry-wise product. The term ``$2e$'' disappears when $X_1 = \cdots = X_n$.
% \end{theorem}
% To set this result we need to introduce a well known object of random matrix theory, namely the deterministic equivalent of $Q^z$ that is a computable approximation of $\mathbb E[Q^z]$. 
Following the formalism of the random matrix theory, the computable estimation of $\mathbb E[Q^z]$ will be called a ``deterministic equivalent''.
Its definition relies on a well known result that states that given a family of symmetric matrices $\Sigma = (\Sigma_1,\ldots, \Sigma_n) \in \mathcal{M}_{p}^n$, there exists a unique vector $\tilde \Lambda_\Sigma^z \in \mathbb C^n$ satisfying:
  \begin{align*}
    \forall i\in [n]:&
    &[\tilde \Lambda_\Sigma^z]_i = \frac{1}{n} \tr \left( \Sigma_i \tilde Q_\Sigma^{\tilde \Lambda_\Sigma^z} \right)&
    &\text{with } \ \tilde Q_{\Sigma}^{\tilde \Lambda_\Sigma^z} = \left( zI_p - \frac{1}{n}  \sum_{j=1}^n \frac{\Sigma_j}{1 - [\tilde \Lambda_\Sigma^z]_j} \right)^{-1}.
  \end{align*}

With those notations at hand, let us state:
\begin{theorem}[Concentration of the resolvent]\label{the:concentration_resolvent_main}
  Considering two sequences $(p_n)_{n\in \mathbb N} \in \mathbb N^{\mathbb N}$, $(\sigma_n) \in \mathbb R_+^{\mathbb N}$ and four constants $c,C,K, \gamma >0$, we suppose that we are given for any $n\in \mathbb N$ a random matrix: $X_n = (x_1^{(n)}, \ldots, x_n^{(n)})\in \mathcal M_{p_n,n}$ such that 
  % $\sup_{i,j\in [n]} \|\mathbb E[x_i^{(j)}] \| \leq K$ and
  \begin{itemize}
    \item $p_n \leq \gamma n$
    \item for all $n \in \mathbb N$, $x_n^{(1)}, \ldots, x_n^{(n)}$ are independent,
    \item $\sup_{n \in \mathbb N,j\in [n]} \left\Vert \mathbb E \left[ x_n^{(j)}  \right] \right\Vert \leq K \sqrt n$
    \item for any $n \in \mathbb N$ quasi-convex mapping $g: \mathcal M_{n,p_n}^{m_n} \to \mathbb R$, $1$-Lipschitz for the euclidean norm:
  \begin{align*}
    \mathbb P \left( \left\vert g(X_n) - \mathbb E \left[ g(X_n) \right] \right\vert \geq t\right) \leq C e^{-c(t/\sigma_n)^2}.
  \end{align*}
   \end{itemize} 
  % $\sup_{1\leq j \leq m_n} \|X_n^{(j)}\| \leq \kappa_n$ and 
  Then for any constant $\varepsilon>0$, there exist two constants $c',C'>0$ such that for all $n \in \mathbb N$, for any deterministic matrix $A \in \mathbb R^n$ such that
  % \footnote{If one only assume that $\|A\|\leq 1$, one still has the concentration $\mathbb P \left( \left\vert  \tr \left( \left(Q^z - \mathbb E \left[ Q^z \right] \right) A \right) \right\vert \geq t\right) \leq C' \exp \left( -c' \left( \frac{t}{ \sigma } \right)^2 \right) + C' e^{-c'n},$ for some constants $C,c>0$} 
  $\|A\| \leq 1$ and for any $z \in \mathbb C$, such that $d(z, \text{Sp}(\frac{1}{n}XX^T)) \geq \varepsilon$:
  \begin{align*}
    \mathbb P \left( \left\vert  \tr \left( \left(Q^z - \tilde Q_\Sigma^{\tilde \Lambda_\Sigma^z} \right) A \right) \right\vert \geq t\right) \leq C' e^{ -c' (t/\sigma_n)^2 }  + C' e^{-c'n},
  \end{align*}
    % where $Q^z \equiv (zI_p - \frac{1}{n}XX^T)^{-1}$ 
    where $\Sigma_i = \mathbb E[x_ix_i^T]$. Besides there exists a constant $K>0$ such that for all $n \in \mathbb N$:
    \begin{align*}
      \left\Vert \mathbb E [Q^z ] - \tilde Q_\Sigma^{\tilde \Lambda_\Sigma^z} \right\Vert_F \leq \frac{K}{\sqrt n}.
    \end{align*}
  % where $\odot$ designates the entry-wise product. The term ``$2e$'' disappears when $X_1 = \cdots = X_n$.
\end{theorem}

This theorem allows us to get good inferences on the eigen values distribution through the identity \eqref{eq:importance_stieltjes_transform} and the estimation of the Stieltjes transform $g(z) \equiv -\frac{1}{p} \tr (Q^z) $ satisfying the concentration inequality:
\begin{align*}
  \mathbb P \left( \left\vert g(z) + \frac{1}{n} \tr \left( \tilde Q^{\tilde \Lambda^z} \right) \right\vert \geq t \right)\leq Ce^{-c p^2 t^2},
\end{align*}
for two constants $C,c>0$ (and for $d(z, \text{Sp}(\frac{1}{n}XX^T)) \geq O(1)$).

When the distribution of the spectrum of $\frac{1}{n}XX^T$ presents different bulks, this theorem also allows us to understand the eigen-spaces associated to those different bulks. Indeed, considering a path $\gamma \in \mathbb C$ containing a bulk of eigen-values $B \subset \text{Sp}(\frac{1}{n}XX^T)$, if we note $E_B$ the associated random eigen-space and $\Pi_B$ the orthogonal projector on $E_B$, then for any deterministic matrix $A \in \mathcal M_{p}$:
 \begin{align}\label{eq:estimation_eigenspaces}
   \tr(\Pi_BA) = -\frac{1}{2i\pi}\int_\gamma \tr(AQ^z) dz&
   % &\text{ with } R(z) \equiv \left(\frac{1}{n}XX^T - z I_p\right)^{-1}.
 \end{align}
  we can estimate this projection $\Pi_B$ defining $E_B$ thanks to the concentration inequality\footnote{for the concentration to be valid on all the values of the path $\gamma$, one must be careful to consider a path staying at a distance $O(1)$ from the bulk, that is why we only consider here multiple bulk distributions}:
\begin{align*}
  \forall t >0 : \ \ \mathbb P \left( \left\vert \frac{1}{\text{Rg}(\Pi)} \tr(\Pi Q^z) - \frac{1}{\text{Rg}(\Pi)} \tr \left( \Pi\tilde Q^{\tilde \Lambda^z} \right) \right\vert \geq t \right) \leq Ce^{-c\text{Rg}(\Pi)^2t^2},
  % \forall t >0 : \ \ \mathbb P \left( \left\vert \frac{1}{\text{dim}(E_k)} \tr(\Pi_{E_k}Q^z) - \frac{1}{\text{dim}(E_k)} \tr(\Pi_{E_k}\tilde Q^{\tilde \Lambda^z}) \right\vert \geq t \right) \leq Ce^{-c\text{dim}(E_k)^2t^2},
  % \frac{1}{\text{dim}(E_k)} \tr(\Pi_{E_k}Q^z) \in \frac{1}{\text{dim}(E_k)} \tr(\Pi_{E_k}\tilde Q^{\tilde \Lambda^z}) \pm \mathcal E_2 \left( \frac{1}{\sqrt {\text{dim}(E_k)}} \right) 
\end{align*}
for some constants $C,c>0$ and for any projector $\Pi$ defined on $\mathbb R^p$.

The approach we present here does not only allows us to set the concentration of $Q^z$, but also the concentration of any polynomial of finite degree taking as variable combination of $Q^z$, $X$ and $X^T$. The general idea is to develop the polynomial as an infinite series of powers of $X$ in a way that the observable diameters of the different terms of the series sum to the smallest value possible. As it is described in the proof of Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre}, the summation becomes slightly elaborate when $z$ gets close to the spectrum.

After presenting the definition and the basic properties of the convex and linear concentration (\textbf{Section 1}), we express the concentration of the sum of linearly concentrated random vectors (\textbf{Section 2}). Then we express the concentration of the entry wise product and the matricial product of convexly concentrated random vectors and matrices (\textbf{Section 3}). Finally we deduce the concentration of the resolvent and provide a computable deterministic equivalent (\textbf{Section 4}).



% \section{Concentration of Measure general inferences}
\section{Definition and first properties}

% Between Lipschitz concentration and linear concentration lies the convex concentration which arises from a theorem of Talagrand \citep{TAL95}, extending the list of examples of concentrated vectors. A random vector is said to be convexly concentrated if its Lipschitz and \textit{quasi-convex} observations are concentrated.
The concentration inequality \eqref{eq:concentration_convexe} is actually also valid for quasi-convex functionals defined folowingly.
\begin{definition}\label{def:quasi_convexe}
  Given a normed vector space $(E,\|\cdot \|)$, an application $f : E \mapsto \mathbb R$ is said to be quasi-convex iif for any $t \in \mathbb R$, the set $\{f \leq t\} \equiv \{x \in E \ | \ f(x) \leq t\}$ is convex.
\end{definition}
The theory of concentration of measure becomes relevant only when dimensions get big. In the cases under study in this paper, the dimension is either given by the number of entries, either by the number of columns $n$ of random matrices - the number of rows $p$ is then understood to depend on $n$, we will sometimes note $p = p_n $. 
We follow then the approach with Levy families \cite{Le51} whose aim is to track the concentration speed through dimensionality. Therefore, we do not talk about a static concentration of a vector but about the concentration of \emph{a sequence of random vectors} as seen in the definition below. In this paper, $E_n$ will either be $\mathbb R^{n}$, $\mathbb R^{p_n}$ $\mathcal{M}_{n}$, $\mathcal{M}_{p_n}$ or $\mathcal{M}_{p_n,n}$. 

There will generally be three possibilities for the norms defining the Lipschitz character of the concentrated observations. Talagrand Theorem gives the concentration for the euclidean norm - i.e. the Frobenius norm for matrices - but we will see that some concentrations are expressed with the nuclear norm (the dual norm of the spectral norm). Given two integers $l,m \in \mathbb N$, the euclidean norm on $\mathbb R^l$ is noted $\|\cdot\|$, the spectral, Frobenius and nuclear norm are respectively defined for any $M \in\mathcal M_{l,m}$ with the expressions:
\begin{align*}
  \|M\| = \sup_{x \in \mathbb R^m} \|Mx\|;&
  &\|M\|_F = \sqrt{\tr(MM^T)};&
  &\|M\|_* = \tr \left( \sqrt{MM^T}  \right).
\end{align*}
\begin{definition}\label{def:cnocentration_convexe}
  \sloppypar{Given a sequence of normed vector spaces $(E_n, \Vert \cdot \Vert_n)_{n\geq 0}$, a sequence of random vectors $(Z_n)_{n\geq 0} \in \prod_{n\geq 0} E_n$, a sequence of positive reals $(\sigma_n)_{n\geq 0} \in \mathbb R_+ ^{\mathbb N}$, we say that $Z =(Z_n)_{n\geq 1}$ is \emph{convexly  concentrated} with an \emph{observable diameter} of order $O(\sigma_n)$ iff there exist two positive constants $C,c>0$ such that $\forall n \in \mathbb N$ and for any $1$-Lipschitz and quasi-convex function $f : E_n \rightarrow \mathbb{R}$ (for the norms $\Vert \cdot \Vert_n$)\footnote{In this inequality, one could have replaced the term ``$\mathbb E[f(Z_n)]$'' by ``$f(Z_n')$'' (with $Z_n'$, an independent copy of $Z_n$) or by ``$m_{f}$'' (with $m_{f}$ a median of $f(Z_n)$). All those three definitions are equivalent.}, }
     \begin{align*}
     \forall t>0:&
     &\mathbb P \left(\left\vert f(Z_n) - \mathbb E[f(Z_n)]\right\vert\geq t\right) \leq C e^{-c(t/\sigma_n)^2},
     \end{align*}
  % \begin{itemize}
  %    \item $\exists c_n \leq O(\sigma_p), \exists C \leq O(1) \ | \ \forall p \in \mathbb N, \forall t>0:$ 
  %    \begin{center}
  %     $\mathbb P \left(\left\vert f(Z_p) - f(Z_p')\right\vert\geq t\right) \leq C e^{(t/c_p)^2}$
  %    \end{center}
  %    \item $\exists c_p \leq O(\sigma_p), \exists C \leq O(1) \ | \ \forall p \in \mathbb N, \forall t>0:$ 
  %    \begin{center}
  %     $\mathbb P \left(\left\vert f(Z_p) - m_{f}\right\vert\geq t\right) \leq C e^{(t/c_p)^2}$
  %    \end{center}
  %    \item $\exists c_p \leq O(\sigma_p), \exists C \leq O(1) \ | \ \forall p \in \mathbb N, \forall t>0:$ 
  %    \begin{center}
  %     $\mathbb P \left(\left\vert f(Z_p) - \mathbb E[f(Z_p)]\right\vert\geq t\right) \leq C e^{(t/c_p)^2}$ %\footnote{The existence of the expectation of $f(Z_p)$ is guaranteed assuming any of the two other assertions \cite[Proposition 1.7]{LED05}}
  %    \end{center}
  %  \end{itemize} 
   % where $Z_p'$ is an independent copy of $Z_p$ and $m_{f}$ is a median of $f(Z_p)$ (it satisfies $\mathbb P \left(f(Z_p) \geq m_{f}\right), \mathbb P \left(f(Z_p) \leq m_{f}\right) \geq \frac{1}{2}$). 
   We write in that case\footnote{The index $2$ in ``$\mathcal E_2$'' is here a reference to the power of $t$ in the concentration bound $C e^{-c(t/\sigma_n)^2}$, we will see some example where this exponent is $1$, in particular in the Hanson-Wright Theorem~\ref{the:hanson_wright} where we will let appear a notation ``$\mathcal E_1$''.} $Z_n \propto_c \mathcal E_{2}(\sigma_n)$ (or more simply $Z \propto_c \mathcal E_{2}(\sigma)$).
\end{definition}
The Theorem of Talagrand then writes:
% The relevance of this definition is given by the next theorem. It is a combinatorial result which provides concentration inequalities for ``discontinuous'' distributions.% (we can have atoms for instance: none of the previous theorems in Subsection~\ref{sse:lipschitz_concentration} allowed us to handle this scenario).
\begin{theorem}[\cite{TAL95}]\label{the:talagrand}
  A (sequence of) random vector $Z \in [0,1]^n$ with independent entries satisfies $Z \propto_c \mathcal E_2$.
\end{theorem}
% The class of convexly concentrated random vectors is far less stable than the class of Lipschitz concentrated vectors: t
Convex concentration is preserved through affine transformations (as for the class of linearly concentrated vectors). Given two vector spaces, $E$ and $F$, we note $\mathcal A(E,F)$ the set of affine transformation from $E$ to $F$, and given $\phi\in \mathcal A(E,F)$, we decompose $\phi = \mathcal L(\phi) + \phi(0)$, where $\mathcal L(\phi)$ is the linear part of $\phi$ and $\phi(0)$ is the translation part. When $E=F$, $\mathcal A(E,F)$ is simply noted $\mathcal A(E)$.
\begin{proposition}\label{pro:stabilite_concentration_convexe_affine}
  Given two normed vector spaces $(E,\|\cdot\|)$ and $(F,\|\cdot\|')$, a random vector $Z \in E$ and an affine mapping $\phi \in \mathcal A(E,F)$ such that $\|\mathcal L(\phi)\|\leq \lambda$:
  \begin{align*}
    Z \propto_c \mathcal E_2(\sigma)&
    &\Longrightarrow&
    &\phi(Z) \propto_c \mathcal E_2(\lambda\sigma).
  \end{align*}
\end{proposition}
% {\color{blue}
% As for Lipschitz concentration in Remark~\ref{rem:lipscitz_sous_ensemble} (and for linear concentration -- but this results was not displayed as it is not used in our study), a 

% Let us end with the definition of the linear concentration that is weaker than the convex concentration and still allows to control the norm.
We pursue our presentation with the introduction of the linear concentration. It is the ``minimal'' hypothesis necessary on a random vector $X$ to be able to bound quantities of the form $\mathbb E[\|X - \mathbb E[X]\|]$, as it has been explained in \cite{LOU19}. Here we will need its stability towards the sum when we will express $Q^z$ as an infinite series.
% We will see in next subsection that it is also very convenient to control sums  because it allows us to follow the concentration rate through concatenation of possibly dependent random vectors.
\begin{definition}[Linearly concentrated vectors]\label{def:linear_concentration}
  Given a sequence of normed vector spaces $(E_n, \Vert \cdot \Vert_n)_{n\geq 0}$, a sequence of random vectors $(Z_n)_{n\geq 0} \in \prod_{n\geq 0} E_n$, a sequence of deterministic vectors $(\tilde Z_n)_{n\geq 0} \in \prod_{n\geq 0} E_n$, a sequence of positive reals $(\sigma_n)_{n\geq 0} \in \mathbb R_+ ^{\mathbb N}$, $Z_n$ is said to be \emph{linearly concentrated} around the \emph{deterministic equivalent} 
  % {\color{red} ** peut-être mettre un peu plus d'emphase sur cette notion d'équivalent déterministe, non? **}{\color{orange} Comme ça c'est bon ou tu voulais aussi du texte en plus ?}
  $\tilde Z_n$ with an \emph{observable diameter} of order $O(\sigma_n)$ iff there exist two constants $c,C >0$ such that $\forall n \in \mathbb N$ and for any unit-normed linear form $f \in E_n'$ ($\forall n \in \mathbb N$, $\forall x \in E$: $|f(x)| \leq \|x\|_n$):
  \begin{align*}
    \forall t>0:& 
      &\mathbb P \left(\left\vert f(Z_n ) - f(\tilde Z_n)\right\vert\geq t\right) \leq C e^{-c(t/\sigma_n)^2}.
  \end{align*}
  When the property holds, we write $Z \in \tilde Z \pm \mathcal E_2(\sigma)$. 
  If it is unnecessary to mention the deterministic equivalent, we will simply write $Z \in \mathcal E_2(\sigma)$; and if we just need to control the order of the norm of the deterministic equivalent, we can write $Z \in O(\theta)\pm \mathcal E_2(\sigma)$ when $\|\tilde Z_n \|_n \leq O(\theta_n)$.
\end{definition}
In the literature \cite{bou13}, those vectors are commonly called sub-Gaussian random vectors. 
% We need this definition with generic $q$ to prove Proposition ~\ref{pro:concentration_lineaire_YAX} which involves a weaker than $\mathcal E_2$ tail decay.

The notions of linear concentration, convex concentration (and Lipschitz concentration) are equivalent for random variables and we have this important characterization with the moments:
\begin{proposition}[\cite{LED05}, Proposition 1.8., \cite{LOU19}, Lemma 1.22.]\label{pro:concentration_variable_caracterisation_moment}
  Given a sequence of random variables $Z_n \in \mathbb R$ and a sequence of positive parameters $\sigma_n>0$, we have the equivalence:
  \begin{align*}
    Z_n \propto_c \mathcal E_2(\sigma_n)
    &\Longleftrightarrow
    Z_n \in \mathbb E[Z_n] \pm \mathcal E_2(\sigma_n)\\
    &\Longleftrightarrow \exists C >0 \ | \ \forall n,m \in \mathbb N:
    \mathbb E \left[ \left\vert Z_n - \mathbb E[Z_n] \right\vert^n \right] \leq C m^{\frac{m}{2}}\sigma_n^m \\
    &\Longleftrightarrow \exists C >0 \ | \ \forall n \in \mathbb N, \forall r>0: \mathbb E \left[ \left\vert Z_n - \mathbb E[Z_n] \right\vert^r \right] \leq Cr^{\frac{r}{2}}\sigma_n^r .
  \end{align*}
\end{proposition}

We end with a simple lemma that allows us to state that "every deterministic vector at a distance smaller than the observable diameter to a deterministic equivalent is also a deterministic equivalent".
\begin{lemma}[\cite{LOU19}, Lemma 2.6.]\label{lem:trou_noir_diametre_observable}
  Given a sequence of random vectors $Z_n \in E_n$ and two sequence of deterministic random vector $\tilde Z_n, \tilde Z'_n \in E_n$, if $\|\tilde Z_n - \tilde Z'_n \| \leq O(\sigma_n)$, then:
  \begin{align*}
    Z \in \tilde Z \pm \mathcal E_2(\sigma)&
    &\Longleftrightarrow&
    &Z \in \tilde Z' \pm \mathcal E_2(\sigma).
  \end{align*}
\end{lemma}

\section{Linear concentration through sums and integrals}
% We saw in Subsection~\ref{sssec:concentration_norm} that 
% Linear concentration was the minimal necessary concentration property to be able to control the norm, it is also very convenient to control sums because it allows us to follow the concentration rate through concatenation of possibly dependent random vectors. 
Independence is known to be a key elements to most of concentration inequalities.
However, linear concentration behaves particularly well for the concatenation of random vectors whose dependence can not be disentangled.
% To show this results we are going to employ the characterization with moments given in Proposition~\ref{pro:concentration_variable_caracterisation_moment} and the notion of statistical domination.

% \begin{definition}\label{def:variable_de_controle}
%     Given two random variables $Z \in \mathbb R$ and $Y \in \mathbb R_+$, one says that $Y$ statistically dominates $Z$ iif $\forall t\geq 0$~:
%     \begin{align*}
%       \mathbb{P}\left(\vert Z \vert \geq t \right) \leq \mathbb{P}\left(Y \geq t\right).
%     \end{align*}
%   \end{definition} 
% Given a set of random variables $Z_1, \ldots, Z_m \in \mathbb R_+$, any random variable $Y$ admitting the cumulative distribution function~:
% \begin{align*}
%   F_Y(t) = \mathbb{P}\left(Y\leq t\right) = 1- \sup_{1\leq i \leq m} \mathbb{P} \left(\vert Z_i \vert > t\right)
% \end{align*}
% clearly controls $Z_1,\ldots, Z_m$.
% If we consider a random variable $Z$ depending on $Z_1,\ldots,Z_m$ and want to express the concentration of $Z$ thanks to Proposition~\ref{pro:concentration_variable_caracterisation_moment}, we are led to bound the quantities $\mathbb E [\vert Z-Z'\vert^r]$ or $\mathbb E [\vert Z-a\vert^r]$.  The next lemma gives the central idea allowing to control those quantities with expectations taken on $Y$ only, when $r\in \mathbb N$ and $Z=P(Z_1,\ldots,Z_m)$ is a polynomial functional with positive coefficients. It simply relies on H\"older's inequality and the fact that $\mathbb E[Z_i^l]\leq \mathbb E[Y^l]$ for all $l\in \mathbb N$, $1\leq i \leq m$.
% \begin{lemma}\label{lem:borne_moment_polynome_de_variables_dependantes}
%   Given two integers $d,m\in \mathbb N$, a polynomial $$P(X_1,\ldots,X_m) = \sum_{b_1+\cdots + b_m\leq d} c_b X_1^{b_1} \cdots X_m^{b_m}$$ with positive coefficients $c_b=c_{b_1,\ldots,b_m}\geq 0$, $b_1+\cdots+b_m \leq d$ and $p$ positive random variables $Z_1,\ldots,Z_m \in \mathbb R_+$ (possibly dependent), for any nonnegative random variable $Y\geq 0$ dominating $Z_1,\ldots,Z_m$, we have the inequality~:
%   \begin{align*}
%     \mathbb E \left[P(Z_1,\ldots,Z_m)\right] \leq \mathbb E \left[P(Y,\ldots,Y)\right].
%   \end{align*}
%   % % for any  verifying $\forall t\geq 0$
%   % \begin{align*}
%   %   \mathbb{P}\left(Y\geq t\right) \geq \sup_{1\leq i \leq p} \mathbb{P}\left(Z_i\geq t\right)  
%   % \end{align*}  
% \end{lemma}
% \begin{proof}
%   It is a simple application of H\"older's inequality~:
%   \begin{align*}
%     \mathbb E \left[P(Z_1,\ldots,Z_m)\right]
%     & = \sum_{b_1+\cdots + b_m\leq d} c_b \mathbb{E} \left[Z_1^{b_1} \ldots Z_m^{b_m}\right] \\
%     &\leq \sum_{b_1+\cdots + b_m\leq d} c_b \mathbb{E} \left[Z_1^{\vert b\vert}\right]^{\frac{b_1}{\vert b \vert}} \ldots \mathbb E\left[ Z_m^{\vert b\vert}\right]^{\frac{b_m}{\vert a \vert}} \\
%     &= \sum_{b_1+\cdots + b_m\leq d} c_b\mathbb{E} \left[ Y^{\vert b\vert}\right] 
%     \ \ = \ \mathbb E \left[P(Y,\ldots,Y)\right]
%   \end{align*}
%   where $\vert b\vert = \sum_{i=1}^p b_i \leq d$.
% \end{proof}

The next proposition sets that the observable diameter for the $\ell^\infty$ norm remains unchanged through concatenation. Given a product $E \equiv \prod_{1\leq i\leq m} E_i$, where $(E_1,\|\cdot\|_\infty), \ldots, (E_m,\|\cdot\|_\infty)$ are $m$ normed vector spaces we define the $\ell^\infty$ norm on $E$ with the following identity:
\begin{align}\label{eq:definition_l_infty}
  (z_1,\ldots ,z_m) \in E: \ \Vert (z_1,\ldots , z_m) \Vert_{\ell^\infty}  = \sup_{1\leq i \leq m} \| z_i\|_i.
\end{align}
\begin{proposition}\label{pro:concentration_concatenation_vecteurs_lineairement_concentres}
    Given two sequences $m \in \mathbb N^{\mathbb N}$ and $\sigma \in \mathbb R_+^{\mathbb N}$, a constant $q$, $m$ sequences of normed vector spaces $(E_i, \|\cdot\|_i)_{1\leq i \leq m}$, $m$ sequences of deterministic vectors $\tilde Z_1\in E_1,\ldots, \tilde Z_m\in E_m$, and $m$ sequences of random vectors $Z_1 \in E_1,\ldots,Z_m \in E_m$ (possibly dependent) satisfying, for any $i\in\{1,\ldots ,m\}$, $Z_i \in \tilde Z_i \pm \mathcal E_2(\sigma)$, we have the concentration~:% the sum $Z_1\cdots Z_p$ is concentrated, it verifies~: 
  \begin{align*}
    (Z_1,\ldots,Z_m) \in (\tilde Z_1,\ldots, \tilde Z_m) \pm \mathcal E_2(\sigma),
     \ \ \text{in } (E, \Vert \cdot \Vert_{\ell^\infty}).
  \end{align*}
\end{proposition}
In other word, the linear observable diameter of $(Z_1,\ldots, Z_m)$ can not be bigger than the observable diameter of $(Z, \ldots,Z)$, where $Z$ is chosen as the worse possible random vector satisfying the hypotheses of $Z_1,\ldots, Z_m$.
\begin{remark}\label{rem:concatenation_impossible_concentration_convexe_ou_lipschitz}
  Example 2.27. in \cite{LOU19} shows that this stability towards concatenation is not true for Lipschitz and convex concentration.
\end{remark}
% \sloppypar{The basic idea behind the proof is that the linear observable diameter of $(Z_1,\ldots, Z_m)$ can not be bigger than the observable diameter of $(Z, \ldots,Z)$, where $Z$ is chosen as the worse possible random vector satisfying the hypotheses of $Z_1,\ldots, Z_m$. This heuristic is formalized below with statistical domination.}
  \begin{proof}
    Let us consider a linear function $u: E \rightarrow \mathbb{R}$, such that $$\left\Vert u\right\Vert_{\infty} \equiv \sup_{\left\Vert z\right\Vert_{\infty}\leq 1} \vert u(z)\vert \leq 1.$$
    %For $z\in E$, let us note $z^{(i)}=(0,\ldots,0,z_i,0,\ldots,0)\in E$, the vector full of $0$ with $z_i$ at the $i^{\text{th}}$ entry
    Given $i\in [m]$, let us note $u_i : E_i \rightarrow \mathbb R$ the function defined as $u_i(z) = u((0,\ldots,0,z,0,\ldots,0))$ (where $z$ is in the $i^{\text{th}}$ entry). For any $z\in E$, one can write~:
    \begin{align*}
      u(z) = \sum _{i=1}^m n_i u'_i(z_i), 
    \end{align*}
    where $n_i \equiv \|u_i\| = \sup_{\|z \|_i\leq 1} u_i(z)$ and $u'_i = u_i/n_i$ ($\| u'_i\| =1$). We have the inequality~:
    $$\sum _{i=1}^m n_i = \sum_{i=1}^m n_i\sup_{\|z_i\|_i \leq 1} u_i'(z_i) = \sup_{\Vert z \Vert_\infty \leq 1} u(z) \leq 1.$$
    With this bound at hand, we plan to employ the characterization with the centered moments. Let us conclude thanks to Proposition~\ref{pro:concentration_variable_caracterisation_moment} and the convexity of $t \mapsto t^l$, for any $l \geq 1$:
    % Considering $l\in \mathbb N_*$, we introduce the polynomial $P(X_1,\ldots, X_m) = (n_1X_1 + \cdots + n_mX_m)^l$: % = \sum_{k_1+\cdots+k_m=m} \binom{m}{k_1,\ldots,k_m} X_1^{k_1}\cdots X_m^{k_m}$
    % and $P_n(X_1,\ldots, X_m) = \sum_{i=1}^p n_i X_i$ ; they both 
    % it has positive coefficients which allows us to employ Lemma~\ref{lem:borne_moment_polynome_de_variables_dependantes} in order to bound~:%Let us note $Z=(Z_1,\ldots, Z_p)$ and $\tilde Z=(\tilde Z_1,\ldots, \tilde Z_p)$. Given $i\in\{0, \ldots, m\}$, we note $Z^{(i)} = (Z_1,\ldots Z_i, \tilde Z_{i+1},\ldots, \tilde Z_m)$ (with this notation : $Z^{(0)}=Z$ and $Z^{(m)}=\tilde Z$). For any $r\geq \max(q,1)$, let us exploit the convexity of $t \mapsto t^r$ to bound~:
  \begin{align*}
    \mathbb{E}\left[\left\vert u(Z)-u(\tilde Z)\right\vert^l\right]
    &\leq \mathbb{E}\left[ \left( \sum_{i=1}^{m} n_i \left\vert u_i'\left(Z_i\right)-u'_i\left(\tilde Z_i\right)\right\vert \right)^l \right] \\
    &\leq \left( \sum_{i=1}^{m} n_i \right)^l \mathbb{E}\left[ \sum_{i=1}^{m} \frac{n_i}{ \sum_{i=1}^{m} n_i} \left\vert u_i'\left(Z_i\right)-u'_i\left(\tilde Z_i\right)\right\vert^l \right] \\
    % &= \mathbb{E} \left[P \left(\left\vert u_1'\left(Z_1\right)-u'_1\left(\tilde Z_1\right)\right\vert,\ldots,\left\vert u_m'\left(Z_m\right)-u'_m\left(\tilde Z_m\right)\right\vert\right) \right]\\
    % &\leq \mathbb{E} \left[P \left(Y,\ldots,Y\right)  \right] 
    % = \mathbb E \left[\left(\sum_{i=1}^m n_i Y\right)^l\right] \\
    % &\leq  \mathbb E \left[Y^l\right] 
    &\leq \sup_{l\in[m]} \mathbb{E}\left[ \left\vert u_i'\left(Z_i\right)-u'_i\left(\tilde Z_i\right)\right\vert^l \right]\ \ \leq \ C l^{\frac{l}{2}} \sigma^l.
  \end{align*}
  % where $Y\geq 0$ is a nonnegative random variable having a cumulative distribution function equal to $1-\min(1,Ce^{-(\cdot/\sigma)^2})$ (such a variable dominates $\vert u_i'(Z_i)-u'_i(\tilde Z_i)\vert$, for all $i \in [m]$).
  % We can then conclude thanks to Proposition~\ref{pro:concentration_variable_caracterisation_moment}.
\end{proof}

If we want to consider the concatenation of vectors with different observable diameter, it is more convenient to look at the concentration in a space $(\prod_{i=1}^m E_i, \ell^r)$, for any given $r>0$, where, for any $(z_1,\ldots, z_m) \in \prod_{i=1}^m E_i$:
\begin{align*}
  \left\Vert (z_1,\ldots, z_m)\right\Vert_{\ell^r} = \left(\sum_{i=1}^m \|z_i\|_i^r\right)^{1/r}.
\end{align*}
\begin{corollaire}\label{cor:concentration_concatenation_vecteur_lineaireent_concentre}
   Given two constants $q,r>0$, $m \in \mathbb N^{\mathbb N}$, $\sigma_1,\ldots,\sigma_m \in (\mathbb R_+^{\mathbb N}) ^m$, $m$ sequences of $(E_i, \|\cdot\|_i)_{1\leq i \leq m}$, $m$ sequences of deterministic vectors $\tilde Z_1\in E_1,\ldots, \tilde Z_m\in E_m$, and $m$ sequences of random vectors $Z_1 \in E_1,\ldots,Z_m \in E_m$ (possibly dependent) satisfying, for any $i\in\{1,\ldots ,m\}$, $Z_i \in \tilde Z_i \pm \mathcal E_2(\sigma_i)$, we have the concentration~:% the sum $Z_1\cdots Z_m$ is concentrated, it verifies~: 
  \begin{align*}
    (Z_1,\ldots,Z_m) \in (\tilde Z_1,\ldots, \tilde Z_m) \pm \mathcal E_2(\|\sigma\|_r),
     \ \ \text{in } (E, \Vert \cdot \Vert_{\ell^r}),
  \end{align*}
\end{corollaire}
\begin{remark}\label{rem:concentration_concatenation_variable_lineairement_concentre}
  % For instance, when $r= 2$
  % Of course the classical use is to take $r=2$ and 
  When $E_1=\cdots=E_m = E$ in the setting of Corollary~\ref{cor:concentration_concatenation_vecteur_lineaireent_concentre}, then for any vector $a=(a_1,\ldots,a_m)\in \mathbb R_+^m$, we know  that:
  \begin{align*}
    \sum_{i=1}^m a_i Z_i \in \sum_{i=1}^m a_i \tilde Z_i \pm \mathcal E_2(|a|^T\sigma),
    % \sum_{i=1}^m a_i Z_i \in \sum_{i=1}^m a_i \tilde Z_i \pm \mathcal E_2(\|a\|_1\|\sigma\|_\infty),
  \end{align*}
  where $|a|=(|a_1|,\ldots,|a_m|)\in \mathbb R_+^m$
  % now taking $r=2$, we have:
  % \begin{align*}
  %   \sum_{i=1}^m a_i Z_i \in \sum_{i=1}^m a_i \tilde Z_i \pm \mathcal E_2(\|a\|\|\sigma\|),
  % \end{align*}
  % and taking $r=1$:
  % \begin{align*}
  %   \sum_{i=1}^m a_i Z_i \in \sum_{i=1}^m a_i \tilde Z_i \pm \mathcal E_2(\|a\|_\infty\|\sigma\|_1).
  % \end{align*}
\end{remark}
\begin{proof}
  We already know from Proposition~\ref{pro:concentration_concatenation_vecteurs_lineairement_concentres} that:
  \begin{align*}
    \left(\frac{Z_1}{\sigma_1},\ldots,\frac{Z_m}{\sigma_m} \right) \in \left(\frac{\tilde Z_1}{\sigma_1},\ldots, \frac{\tilde Z_m}{\sigma_m}  \right) \pm \mathcal E_2,
     \ \ \text{in } (E, \Vert \cdot \Vert_{\ell^\infty}).
  \end{align*}
  Let us then consider the linear mapping:
  \begin{align*}
    \phi :
    \begin{aligned}[t]
      (E,\|\cdot \|_{\ell^\infty}) \ &
      &\longrightarrow&
      &(E,\|\cdot \|_{\ell^r})\hspace{0.6cm}\\
      (z_1,\ldots, z_m)&
      &\longmapsto&
      &(\sigma_1z_1,\ldots, \sigma_mz_m),
    \end{aligned}
  \end{align*}
  the Lipschitz character of $\phi$ is clearly $\|\sigma\|_r  = (\sum_{i=1}^m \sigma_i^r)^{1/r}$, and we can deduce the concentration of $Z = \phi(\sigma_1Z_1,\ldots, \sigma_m Z_m)$.  
\end{proof}
Corollary~\ref{cor:concentration_concatenation_vecteur_lineaireent_concentre} is very useful to set the concentration of infinite series of concentrated random variables. This is settled thanks to an elementary result of \cite{LOU19} that sets that the observable diameter of a limit of random vectors is equal to the limit of the observable vectors. Be careful that rigorously, there are two indexes, $n$ coming from Definition~\ref{def:linear_concentration} that only describes the concentration of sequences of random vectors, and $m$ particular to this lemma that will tend to infinity. For clarity, we do not mention the index $n$. %For simplicity, we provide this result for random variables
\begin{lemma}[\cite{LOU19}, Proposition 1.12.]\label{lem:passage_a_la_limite_dans_la_concetration}
  % Given a sequence of random vectors $(Z_{p,n})_{p,n \in \mathbb N} \in E_p^{\mathbb N^2}$ and a sequence of positive reals $(\sigma_{p,n})_{p,n \in \mathbb N} \in \mathbb R^{\mathbb N^2}_+$ such that:% for all $n,p \in \mathbb N$:
  % \begin{align*}
  %   Z_{p,n} \propto \mathcal E_2(\sigma_{p,n}),
  % \end{align*}
  % if we assume that $(Z_{p,n})_{p,n \in \mathbb N}$ converges in law
  % %There exists a sequence of random vectors $(Z_{p,\infty})_{p \in \mathbb N}$, such that f
  % \footnote{For any bounded sequence of continuous mapping $(f_p)_{p \geq 0} : \prod_{n\geq 0} E_p \to \mathbb R^{\mathbb N}$:
  % \begin{align*}
  %    \sup_{p \in \mathbb N} \left\vert \mathbb E[f_p(Z_{p,n}) - \mathbb E[f_p(Z_{p,\infty})]\right\vert \underset{n\to \infty}{\longrightarrow} 0
  %  \end{align*} },
  % %  \footnote{For any mapping $(f_p)_{p \geq 0} : \prod_{n\geq 0} E_p \to \mathbb R^{\mathbb N}$ supported on compact subset of $\prod_{n\geq 0} E_p$:
  % % \begin{align*}
  % %    \sup_{p \in \mathbb N} \left\vert \mathbb E[f_p(Z_{p,n}) - \mathbb E[f_p(Z_{p,\infty})]\right\vert \underset{n\to \infty}{\longrightarrow} 0
  % %  \end{align*} },
  %   when $n$ tends to infinity to a sequence of random vectors $(Z_{p,\infty})_{p\in \mathbb N} \in \prod_{p \geq 0} E_p$  and that $\sigma_{p,n} \underset{n\to \infty}{\longrightarrow} \sigma_{p,\infty} $ then:
  %  \begin{align*}
  %    Z_{p,\infty} \propto \mathcal E_2(\sigma_{p,\infty}).
  %  \end{align*}
  Given a sequence of random vectors $(Z_{m})_{m \in \mathbb N} \in E^{\mathbb N}$, a sequence of positive reals $(\sigma_{m})_{m \in \mathbb N} \in \mathbb R^{\mathbb N}_+$ and a sequence of deterministic vectors $(\tilde Z_{m})_{m \in \mathbb N} \in E^{\mathbb N}$ such that:% for all m$n,p \in \mathbb N$:
  \begin{align*}
    Z_{m} \in \tilde Z_m \pm  \mathcal E_2(\sigma_{m}),
  \end{align*}
  if we assume that $(Z_{m})_{m \in \mathbb N}$ converges in 
  %There exists a sequence of random vectors $(Z_{p,\infty})_{p \in \mathbb N}$, such that f
  law\footnote{For any $n \in \mathbb N$, for any bounded continuous mapping $ f : \prod_{m\geq 0} E_p \to \mathbb R^{\mathbb N}$:
  \begin{align*}
     \sup_{n \in \mathbb N} \left\vert \mathbb E[f(Z_{n, m}) - \mathbb E[f(Z_{n, \infty})]\right\vert \underset{m\to \infty}{\longrightarrow} 0
   \end{align*} }
  %  \footnote{For any mapping $(f_p)_{p \geq 0} : \prod_{n\geq 0} E_p \to \mathbb R^{\mathbb N}$ supported on compact subset of $\prod_{n\geq 0} E_p$:
  % \begin{align*}
  %    \sup_{p \in \mathbb N} \left\vert \mathbb E[f_p(Z_{p,n}) - \mathbb E[f_p(Z_{p,\infty})]\right\vert \underset{n\to \infty}{\longrightarrow} 0
  %  \end{align*} },
  when $m$ tends to infinity to a random vector $(Z_{\infty}) \in E$, that $\sigma_{m} \underset{n\to \infty}{\longrightarrow} \sigma_{\infty} $ and that $\tilde Z_{m} \underset{n\to \infty}{\longrightarrow} \tilde Z_{\infty}$, then:
  \begin{align*}
    Z_{\infty} \in \tilde Z_{\infty} \pm \mathcal E_2(\sigma_{\infty}).
  \end{align*}
  (The result also holds for Lipschitz and convex concentration)
  % if we only assume linear concentration for $Z_m$ then we obtain the linear concentration of $Z_\infty$.
\end{lemma}
\begin{corollaire}\label{cor:concentration_serie_vecteur_lineairement_concentres}
  \sloppypar{Given two constants $q,r>0$, $\sigma_1,\ldots,\sigma_m \ldots \in (\mathbb R_+^{\mathbb N}) ^\mathbb N$, a (sequences of) normed vector spaces $(E, \|\cdot\|)$, $\tilde Z_1 \ldots, \tilde Z_m,\ldots \in E^{\mathbb N}$ deterministic, and $ Z_1 \ldots,  Z_m,\ldots \in E^{\mathbb N}$ random (possibly dependent) satisfying, for any $n\in \mathbb N$, $Z_m \in \tilde Z_m \pm \mathcal E_2(\sigma_m)$. If we assume that $Z \equiv\sum_{n \in \mathbb N} Z_m$ is pointwise convergent\footnote{For any $w \in \Omega$, $\sum_{m \in \mathbb N} \|Z_m(w)\| \leq \infty$ and we define $Z(w) \equiv \sum_{m \in \mathbb N}Z_m(w)$}, that $\sum_{m \in \mathbb N} \tilde Z_m$ is well defined and that $\sum_{n\in \mathbb N} \sigma_i \leq \infty$, then we have the concentration~:% the sum $Z_1\cdots Z_p$ is concentrated, it verifies~: 
  \begin{align*}
    \sum_{m \in \mathbb N} Z_m \in \sum_{m \in \mathbb N} \tilde Z_m \pm \mathcal E_2 \left(\sum_{m \in \mathbb N} \sigma_m\right),
     \ \ \text{in } (E, \Vert \cdot \Vert),
  \end{align*}}
\end{corollaire}
\begin{proof}
  We already know from Corollary~\ref{cor:concentration_concatenation_vecteur_lineaireent_concentre} that for all $m \in \mathbb N$:
  \begin{align*}
    \sum_{m = 1}^M Z_m \in \sum_{m = 1}^M \tilde Z_m \pm \mathcal E_2 \left(\sum_{m \in \mathbb N} \sigma_m\right),
     \ \ \text{in } (E, \Vert \cdot \Vert).
  \end{align*}
  Thus in order to employ Lemma~\ref{lem:passage_a_la_limite_dans_la_concetration} let us note that for any bounded continuous mapping $f:E \to \mathbb R$, the dominated convergence theorem allows us to set that:
  \begin{align*}
    \mathbb E \left[ f \left(\sum_{m = 1}^M Z_m \right)\right] \underset{M \to \infty} \longrightarrow \mathbb E \left[ f \left(\sum_{m = 1}^\infty Z_m \right)\right],
  \end{align*}
  thus $(\sum_{m = 1}^M Z_m)_{N \in \mathbb N}$ converges in law to $\sum_{m = 1}^\infty Z_m$, which allows us to set the result of the corollary.

\end{proof}
% As will be seen in next corollary, t
The concentration of infinite series directly implies the concentration of resolvents and other related operators (like $(I_n - X/\sqrt p)^{-1} X^k$ for instance). 
\begin{corollaire}\label{cor:Concentration_linearire_solution_implicite_hypo_concentration_phi^k_pour tout_k}
  Given a (sequence of) vector space $(E, \| \cdot \|)$, let $\phi\in \mathcal A(E)$ be a (sequence of) random affine mapping such that there exists a constant $\varepsilon>0$  satisfying $\left\Vert\mathcal L(\phi)\right\Vert \leq 1-\varepsilon$ and a (sequences of) integers $\sigma >0$ satisfying for all (sequence of) integer $k$:
  \begin{align*}%\label{eq:concentration_phi^k_avec_norme_forte}
    & \mathcal L(\phi)^k(\phi(0)) \in \mathcal E_2 \left(\sigma(1-\varepsilon)^k\right)  \ \ \text{in} \  \ (E, \| \cdot \|)
  \end{align*}
  % for some constants $C,c>0$.
  % such that $\mathcal A_\phi \subset $ (recall that $\mathcal L(\phi) = \phi - \phi(0) \in \mathcal L(E)$) and that $\mathbb E_{\mathcal A_\phi}[\|\phi(0)\|] \leq O(1)$. 
  Then the random equation
  \begin{align*}
    Y = \phi(Y)
  \end{align*}
  admits a unique solution $Y = ( Id_E - \mathcal L(\phi))^{-1} \phi(0)$ satisfying the linear concentration:
  \begin{align*}
    Y\in \mathcal E_2(\sigma)  .
  \end{align*}
\end{corollaire}
In practical examples, $\|\mathcal L(\phi)\|$ is rarely bounded by $1-\varepsilon$ for all drawings of $\phi$ and to obtain the concentration of $\mathcal L(\phi)^k$ with an observable diameter of order $\sigma(1- \varepsilon)^k$, one needs to place oneself on an event $\mathcal A_\phi$ satisfying $\mathcal A_\phi \subset \{\left\Vert\mathcal L(\phi)\right\Vert \leq 1-\varepsilon\}$. Then, thanks to a simple adaptation of Lemma~\ref{lem:concentration_conditionnee_convexe} below to the case of linear concentration, we have the concentration $(Y \ | \ \mathcal A_\phi)\in \mathcal E_2(\sigma)$. When $\mathbb E[\|\mathcal L(\phi)\|] \leq 1-2 \varepsilon$ for $\varepsilon\geq O(1)$ and $\phi$ is sufficiently concentrated, it is generally possible to chose an event $\mathcal A_\phi$ of overwhelming probability. %(for instance $\mathbb P(\mathcal A_\phi) \leq Ce^{-cn}$ for some constants $C,c>0$, when $\phi = \frac{1}{n}XX^T$ and $$) 

% Returning to Subsection~\ref{sse:concentration_lipschitz_resolvent}, note that this corollary could not have been used to set the concentration of $Q^z$ for all $z\in \mathbb C \setminus [0,1-\varepsilon]$, but just for $z \in \{|z| \geq 1-\varepsilon\}$, so that $\mathcal L(\phi)^k(\phi(0)) = (\frac{1}{n}XX^T)^k\overset{\mathcal A_\phi}\in \mathcal E_2 \left((1-\varepsilon)^k/\sqrt n\right) \ | \ e^{- n}$ (thanks to Proposition~\ref{pro:concentration_intelligente_produit}). Anyway, even for $z \in \{|z| \geq 1-\varepsilon\}$, one would just have obtained a linear concentration of $Q^z$. 
As it will be seen in Subsection~\ref{sse:convex_concentration_resolvent}, this corollary finds its relevancy under convex concentration hypotheses, where the linear concentration seems to be the best concentration property to obtain on the resolvent $Q^z = (z I_p - \frac{1}{n}XX^T)^{-1}$.
% Although this theorem is far easier to use than Theorems~\ref{the:COncentration_solution_conentrated_equation_phi_affine_k_leq_log_eta} or~\ref{the:COncentration_lineaire_solution_conentrated_equation_phi_affine_k_leq_log_eta_bis}, we did not give directly because the complex setting of Theorem~\ref{the:COncentration_solution_conentrated_equation_phi_affine_k_leq_log_eta} can be adapted more easily to study afterwards Lipschitz concentration of solutions to non affine equation.
\begin{proof}
  By contractivity of $\phi$, $Y$ is well defined and expresses:
  \begin{align*}
    Y = ( Id_E - \mathcal L(\phi))^{-1} \phi(0) = \sum_{k=0}^\infty \mathcal L(\phi)^k \phi(0).
  \end{align*}
  % For all $m \in \mathbb N$, we know from Corollary~\ref{cor:concentration_concatenation_vecteur_lineaireent_concentre} that $\sum_{k=0}^m \mathcal L(\phi)^k \phi(0) \in \mathcal E_2(\sigma\sum_{k=0}^m (1-\varepsilon)^k)$, thus letting $m$ tend to infinity (this asymptotic inference is a simple consequence of the definition of the concentration), 
  One can then conclude with Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres} that $Y \in \mathcal E_2(\sigma/\varepsilon) = \mathcal E_2(\sigma)$.
\end{proof}

In order to satisfy the hypothesis of Corollary~\ref{cor:Concentration_linearire_solution_implicite_hypo_concentration_phi^k_pour tout_k}, but also for independent interest, we are now going to express the concentration of the product of convexly concentrated random matrices.
% \section{Concentration under highly probable event}
% \label{sse:conc_under_highly_probable_event}
% % {\color{blue}





\section{Degeneracy of convex concentration through product}\label{sse:convex_concentration}
% There exists an interesting concentration property weaker than the Lipschitz concentration and stronger than the 


Given two convexly concentrated random vectors $X,Y \in E$ satisfying $X,Y \propto_c \mathcal E_2(\sigma)$, the convex concentration of the couple $(X,Y) \propto_c \mathcal E_2(\sigma)$ is ensured if:
\begin{enumerate}
  \item $X$ and $Y$ are independent
  \item $(X,Y) = u(Z)$ with $u$ affine and $Z\propto_c \mathcal E_2(\sigma)$.
\end{enumerate}
We can then in particular state the concentration of $X+Y$ as it is a linear transformation of $(X,Y)$. For the product it is not as simple as for the Lipschitz concentration, let us first consider the particular case of the entry-wise product in $E = \mathbb R^p$.
% : we will consider the special cases $E = \mathbb R^p$, $E = \mathcal M_{p,n}$ to retrieve interesting properties. 
Since this result is not important for the rest of the paper, we left its proof in \ref{app:concentration_produit_vecteurs}.
% All the coming study is based on a preliminary elementary result that does not need any proof.

\begin{theorem}\label{the:concentration_convexe_produit_odot_Rp}
  Given a (sequences of) integer $m \in \mathbb N^{\mathbb N}$ and a (sequence of) positive number $\sigma >0$ such that $m \leq O(p)$, a (sequence of) $m$ random vectors $X_1,\ldots,X_m \in \mathbb R^p$, if we suppose that 
  % Given three sequences $m \in \mathbb N^{\mathbb N}$ and $\sigma,\eta \in \mathbb R_ + ^{\mathbb N}$ such that $\eta \geq  O(\log(p))$ and $\log m \leq O(\eta)$, $m$ random vectors $X_1,\ldots,X_m \in \mathbb R^p$ such that $\sup_{1\leq i \leq m}\|\mathbb E[X_i] \|_\infty = O((\log p)^{1/q})$, if we suppose that%\footnote{The tail controlling term $e^{-\eta}$ with $\eta \leq  O(\log(p))$ might seem unsatisfactory, the concentration of the product can still be improved to become for any $\eta$}
  \begin{align*}
    X \equiv(X_1,\ldots,X_m) \propto_c \mathcal E_2(\sigma)
    &\text{ in } \left((\mathbb R^p)^m, \| \cdot \|_{\ell^\infty}\right),
   \end{align*}
   (with the notation $\|\cdot \|_{\ell^\infty}$ defined in \eqref{eq:definition_l_infty}) % for any $z= (z_1,\ldots,z_m) \in (\mathbb R^p)^m$, $\|z\|_{\ell^\infty} = \sup_{1\leq i\leq m} \|z_i\|$, 
   % For those who need a better rate than $C e^{-c\log p}$ for the probability to be outside the concentration zone, we present a result similar to Theorem~\ref{the:Concentration_produit_de_vecteurs_d_algebre_optimise} in the case where $X \propto_c \mathcal E_2(\sigma)$:
   % To simplify the preceding inequality, we add that 
   and that there exists a (sequence of) positive numbers $\kappa >0$ such that $\forall i\in [m]: \|X_i \|_\infty \leq \kappa$, then:
   % if there exists a (sequence of) positive numbers $\kappa >0$ and two constants $C,c >0$ such that, noting $\mathcal A_\kappa \equiv \{\forall i\in [m]: \|X_i \|_\infty \leq \kappa\}$, $\mathbb P(\mathcal A_\kappa^c) \leq C e^{-cp}$, then:
   \begin{align*}
    X_1\odot \cdots \odot X_m \in \mathcal E_2 \left( (2e\kappa)^{m-1} \sigma\right)  \ \ \ \text{in } \ (\mathbb R^p, \|\cdot\|).
   \end{align*}
   And if $X_1=\cdots=X_m =X$, the constant $2e$ is no more needed and we get the concentration $X^{\odot m} \in \mathcal E_2 \left( \kappa^{m-1} \sigma\right)$.
\end{theorem}
\begin{remark}\label{rem:controle_de_la_queue_de_distribution}
  If we replace the strong assumption $\forall i\in [m]: \|X_i \|_\infty \leq \kappa$, with the bound $\sup_{1\leq i \leq m}\|\mathbb E[X_i] \|_\infty \leq O((\log p)^{1/q})$ we can still deduce a similar result to \cite[Example 4.]{LOU21HV}, stating the existence of a constant $\kappa \leq O(1)$ such that:
   \begin{align*}
    X_1\odot \cdots \odot X_m \in \mathcal E_2 \left( \left(\kappa \sigma\right)^{m} (\log(p))^{(m-1)/q}\right) + \mathcal E_{q/m} \left( \left(\kappa \sigma\right)^{m}\right)  \ \ \ \text{in } \ (\mathbb R^p, \|\cdot\|).
   \end{align*}
\end{remark}
% \begin{remark}\label{rem:produit_odot_vecteurs_egaux}
%   \sloppypar{Note from the preceding proof that in the setting of Theorem~\ref{the:concentration_convexe_produit_odot_Rp} if $X_1=\cdots= X_m$ we have the sharper concentration inequality $X^{\odot m} \in \mathcal E_2 \left( \kappa^{m-1} \sigma\right) \ | \ e^{-p}$, which means that it is independent with $m$ when $\kappa \leq 1$.}
% \end{remark}
% A similar result holds for matrix products; for simplicity, we only consider the power of matrices.
% \begin{proposition}\label{pro:concentration_produit_matrice_convexement_concentres}
%   Given two sequences $m \in \mathbb N^{\mathbb N}$ and $\sigma \in \mathbb R_ + ^{\mathbb N}$ such that $\log m \leq O(\eta)$, a random nonnegative definite symmetric random matrix $X \in \mathcal S_p^+$ such that $\|\mathbb E[X] \| = O(p^{1/q})$, if we suppose that%\footnote{The tail controlling term $e^{-\eta}$ with $\eta \leq  O(\log(p))$ might seem unsatisfactory, the concentration of the product can still be improved to become for any $\eta$}
%   \begin{align*}
%     X \propto_c \mathcal E_2(\sigma) \ | \ e^{-p}
%     &\text{ in } \left(\mathcal M_p, \| \cdot \|_F\right),
%    \end{align*}
%    % with, for any $M= (M_1,\ldots,M_m) \in (\mathbb S_p^+)^m$, $\|M\|_{\ell^\infty} = \sup_{1\leq i\leq m} \|M_i\|$, 
%    then there exists a constant $\kappa \leq O(1)$ such that
%    \begin{align*}
%     X^m \in \mathcal E_2 \left( \left(\kappa \sigma\right)^{m} p^{(m-1)/q}\right) \ | \ e^{-p}&
%     &\text{ in } \left(\mathcal M_p, \| \cdot \|\right),
%    \end{align*}
%    Now with the same hypotheses, if we consider this time that the random matrix $X$ belongs to $\mathcal M_{p,n}$ and satisfies $\|\mathbb E[X] \| = O((p+n)^{1/q})$, we have the concentration
%    \begin{align*}
%     (XX^T)^m \in \mathcal E_2 \left( \left(\kappa \sigma\right)^{2m} (p+n)^{(2m-1)/q}\right) \ | \ e^{-(p+n)}&
%     &\text{ in } \left(\mathcal M_p, \| \cdot \|\right),
%    \end{align*}
% \end{proposition}
% \begin{remark}\label{rem:concentration_convexe_produit}
  
% \end{remark}
% \begin{proof}
%   Considering $A \in \mathcal M_{p}$, $M \in \mathcal S_p^+$ and $H \in \mathcal S_p$ and assuming $m\geq 2$, we introduce the mapping:
%   \begin{align*}
%     \phi^A : \begin{aligned}[t]
%       \mathbb R&& \longrightarrow&&\mathbb R \hspace{1.3cm} \\
%       t& &\longmapsto& &\tr(AX^m).\\
%     \end{aligned}
%   \end{align*}
%   We can differentiate it twice to obtain
%   \begin{align*}
%     \restriction{d^2\phi^A}{X}\cdot H = \sum_{k_1+k_2+k_3 = m} \tr(AX^{k_1}HX^{k_2}H X^{k_3}).
%   \end{align*}
%   Now note that, for $M$ nonnegative symmetric, $\phi^{I_p}(M) = \|M\|_m^m$ where $\|\cdot \|_m$ is the norm defined as $\|A\|_m = (\tr((MM^T)^{m/2}))$. Since the norm $\|\cdot\|_m$ is a convex mapping having values on $\mathbb R^+$ and $t \mapsto t^m$ is convex on $\mathbb R^+$, we know that $\phi^{I_p}$ is also convex. Therefore, the inequality $\restriction{d^2\phi^A}{X}\cdot H \leq \left\Vert A\right\Vert \restriction{d^2\phi^{I_p}}{X}\cdot H$ implies that $\restriction{d^2\phi^{\|A\|I_p - A}}{X}\cdot H\geq 0$ and therefore $\phi^A = \phi^{\|A\|I_p} -\phi^{\|A\|I_p - A}$ is the difference of two convex mappings. 
%   Now, to find Lipschitz properties, we have to restrict ourselves to the ball $\mathcal B_\kappa \equiv \{M \in \mathcal S_p^+ \ | \ \|M \| \leq \kappa \sigma p^{1/q}\}$ where the constant $\kappa \leq O(1)$ is chosen such that $\mathbb P ( X \notin \mathcal B_\kappa) \leq C e^{-p/c}$ for some constants $C,c \leq O(1)$. Thus, for any $B \in \mathcal M_{p}$ such that $\phi^B$ is convex, the mapping $\phi^B$ is $\|B\|_F(\kappa \sigma)^{m-1}p^{(m-1)/q}$-Lipschitz. 
%   % We can then extend by continuity $\restriction{\phi^B}{\mathcal B_\kappa}$ to a $\|B\|_F(\kappa \sigma)^{m-1}p^{(m-1)/q}$-Lipschitz and convex mapping defined on all the vector space $\mathcal S_{p}$ {\color{red} à vérifier}.
%   Once again, playing on the condition $\{X \in \mathcal B_\kappa \}$ with Lemma~\ref{lem:concentration_conditionnee_convexe}, one can show that for any $A\in \mathcal M_{p,n}$ such that $\|A\|_F \leq 1$,
%   \begin{align*}
%     \tr(A X^m) \in \mathcal E_2 \left( \left(\kappa \sigma\right)^{m} p^{(m-1)/q}\right) \ | \ e^{-p}.
%   \end{align*}
%   The concentration inequality for $(XX^T)^m$ when $X \in \mathcal M_{p,n}$ is obtained similarly.
% \end{proof}
% {\color{blue}
 % However, we have a weaker estimation result which is very useful for the design of a deterministic equivalent of the resolvent.% 
% to cases where $D$ is constant (and possibly non diagonal) and $X, Y$ are convexly concentrated. That gives us
% However, under convex concentration hypothesis, we still have a similar result (this time $D$ is deterministic and do not have to be diagonal) very close












% \color{red}
% Under convex concentration hypothesis we also have a Hanson-Wright-like concentration inequality as in Corollary~\ref{cor:hanson_wright_frobenius}.
% \begin{corollaire}\label{cor:hanson_wright_convex_frobenius}
%   \sloppypar{Given a deterministic matrix $A \in \mathcal M_{p}$ and two random vectors $Z,W \in \mathbb R^{p}$ satisfying $Z,W \propto_c \mathcal E_2$ and such that $\|\mathbb E[Z]\|, \| \mathbb E[W] \| \leq O(\sqrt{\log(p)} )$, we have the concentration:
%   % \sloppypar{Given a deterministic matrix $A \in \mathcal M_{p}$ and two random matrices $Z,W \in \mathcal M_{p,n}$ satisfying $Z,W \propto_c \mathcal E_2$ and such that $\|\mathbb E[Z]\|, \| \mathbb E[W] \| \leq O(\sqrt{\log(pn)} )$, we have the concentration:
%   \begin{align*}
%     Z^TAW \in \mathcal E_2 \left(  \|A\|_F \sqrt{\log p}\right)  + \mathcal E_{1} (\|A\|_F) \ \ \ \text{in } \ (\mathcal M_{n}, \| \cdot \|_F).
%   \end{align*}}
% \end{corollaire}
% \begin{proof}
%   % The proof closely related to the proof of Corollary~\ref{cor:Concentration_XDYu}; 
%   % Given a matrix $B \in \mathcal M_{n}$ such that $\|B\|_F \leq 1$, we 
%   We are going to employ the singular decomposition:% $A$ and $B$ with two orthogonal matrices :
%   \begin{align*}
%     A = P_A^T \Lambda Q_A&
%     % &\text{and}&
%     % &B = P_B^T\Gamma Q_B,
%   \end{align*}
%   with $\Lambda = \diag(\lambda) \in \mathcal D_p$, $P_A,Q_A \in \mathcal O_{p}$. Noting $\check Z \equiv P_AX$ and $\check W \equiv Q_AY$, since $M \to P_AM$ and $M \to Q_AM$ are both $1$-Lipschitz linear transformations on $\mathbb R^{p}$, we see from Proposition~\ref{pro:stabilite_concentration_convexe_affine} that $\check Z, \check W \propto_c \mathcal E_2$. 
%   % Then we know from Example~\ref{exe:concentration_covariance_empirique} and the hypotheses of the Corollary that 
%   Besides we know from Example~\ref{exe:borne_esp_norm_vecteur_lin_conc} that:% treating the case of the infinity norm on sets of the form $\mathbb R^d$ that:% ($d=pn$ for $\mathcal M_{p,n}$) that: 
%   % with $\Lambda = \diag(\lambda) \in \mathcal D_p$ and $\Gamma = \diag(\gamma) \in \mathcal D_n$, $P_A,Q_A \in \mathcal O_{p}$ and $P_B,Q_B \in \mathcal O_{n}$. Then, noting $\check Z \equiv P_AXQ_B^T$ and $\check W \equiv Q_AYP_B^T$, since $M \to P_AMQ_B^T$ and $M \to Q_AMP_B^T$ are both $1$-Lipschitz linear transformations on $\mathcal M_{p,n}$, we see from Proposition~\ref{pro:stabilite_concentration_convexe_affine} that $\check Z, \check W \propto_c \mathcal E_2$. Besides we know from Example~\ref{exe:borne_esp_norm_vecteur_lin_conc} treating the case of the infinity norm on sets of the form $\mathbb R^d$ ($d=pn$ for $\mathcal M_{p,n}$) that: 
%   \begin{align*}
%    \mathbb E[\|\check Z\|_\infty]  
%    &\leq \|\mathbb E[\check Z] \|_\infty + O(\sqrt{\log p} ) 
%    \leq \|\mathbb E[ Z] \| + O(\sqrt{\log p} )  \leq  O(\sqrt{\log p} ), 
%   \end{align*}
%   and the same way $\mathbb E[\|\check W \|_\infty]\leq O(\sqrt{\log p} )$. 
%   % Thus since the norm index of $(\mathcal M_{p,n},\|\cdot \|_\infty)$ is $\log(p+n)$, $\mathbb E[\|\check Z \|_\infty],\mathbb E[\|\check W \|_\infty]\leq O(\sqrt{\log(p+n)} )$ and we have thanks to 
%   Now, since for all vector $z,w \in \mathbb R^p$, $\|z\odot w\| = \|w\odot z\| \leq \|z\| \|w\|_\infty$, Theorem~\ref{the:concentration_convexe_produit_odot_Rp} implies the concentration $\check Z \odot \check W \in \mathcal E_2(\sqrt{\log(pn)}) + \mathcal E_1$ which allows us to conclude that:
%   \begin{align*}
%     Z^TA W = (\check Z \odot \check W)^T \lambda \in \mathcal E_2 \left(\|\lambda \|\sqrt{\log p}\right) + \mathcal E_1(\|\lambda \|),
%     % \tr(BX^TA W)  = \gamma^T \check Z \odot \check W \lambda \in \mathcal E_2(\|\lambda \|\|\gamma \| \sqrt{\log(pn)}) + \mathcal E_1(\|\lambda \|\|\gamma \|),
%   \end{align*}
%   that gives us the result of the corollary since $\|\lambda \| = \| A\|_F$.% and $\| \gamma \| = \| B \|_F =1 $.
% \end{proof}
% \color{black}

% Corollary~\ref{cor:concentration_lineaire_XDY} that gives the linear concentration of random matrices $\frac{1}{n}XDY$ for $X,Y,D$ Lipschitz concentrated and $D$ diagonal can not be set with good convergence speed when $X,D,Y$ are simply convexly concentrated.
% We can still obtain a precise estimation of $\frac{1}{n}\mathbb E[XDY]$ as it is done in Proposition~\ref{pro:estimation_XDY}, this time thanks to Corollary~\ref{cor:hanson_wright_convex_frobenius}.%that will be utterly important for the design of a computable deterministic equivalent of the resolvent $Q = (I_n - \frac{1}{n}XX^T)^{-1}$.  %It demonstrates the relevency of the notion of linear concentration, since we see here that the Lipschitz concentration can degenerate 
% \begin{corollaire}\label{cor:estimation_XDY}
%   Given three random matrices $X,Y \in \mathcal M_{p,n}$ and $D \in \mathcal D_n$ and a deterministic matrix $\tilde D \in \mathcal D_n$, such that $X,Y\propto_c \mathcal E_2 $ in $(\mathcal M_{p,n}, \|\cdot\|)$, $\|\mathbb E[X]\|, \|\mathbb E[Y]\| \leq O(1)$ $D \in \mathcal E_2$, in $(\mathcal D_n, \|\cdot\|)$ and $\|\mathbb E[D] -\tilde D\|_F \leq O(1)$ we have the estimation:
%   \begin{align*}
%     \left\Vert \mathbb E[XDY^T] -\mathbb E[X\tilde DY^T]\right\Vert_F \leq O \left(n\sqrt{\log p}\right)
%   \end{align*}
% \end{corollaire}
% The result is given here in a convex concentration setting, but it is of course also true for Lipschitz concentrated matrices, in that case (when $X,Y\propto \mathcal E_2$ and $D \propto \mathcal E_2(1/\sqrt n)$) we have the Lipschitz concentration $\frac{1}{n}XD Y^T \propto \mathcal E_2(1/\sqrt n) \ |\ e^{-n}$ thanks to Theorem~\ref{the:Concentration_produit_de_vecteurs_d_algebre_optimise} (see remark~\ref{rem:theoreme_concentration_produit_plus_puissant_proposition_conc_intelligente}). 
% \begin{remark}\label{rem:concentration_XDY_avec_proposition_concentration_convexe_produit_matrices}
%   Proposition~\ref{pro:concentration_produit_matrice_convexement_concentres} below (and more precisely Remark~\ref{rem:concentration_produit_differentes_matrice_convexes}) and the hypotheses of the corollary would just allow us to set:
%   \begin{align*}
%     \frac{1}{n}XD Y^T \in \mathcal E_2 \ | \ e^{-p}&
%     &\text{in} \ \ \left(\mathcal M_{p}, \|\cdot \|_*\right),
%   \end{align*}
%   we need Corollary~\ref{cor:hanson_wright_convex_frobenius} to get a stronger concentration.
% \end{remark}
% \begin{proof}%[Proof of Corollary~\ref{lem:estimation_XDY}]
% % We know from Proposition~\ref{pro:concentration_produit_matrice_convexement_concentres} and the hypotheses of the corollary that:
% % We can express:
% % \begin{align*}
% %   \frac{1}{n}\tr \left(AXD Y^T \right)
% %     % &\left\vert \mathbb E \left[\tr \left(AXD Y^T - AX \mathbb E[D]Y^T\right)\right]\right\vert\\
% %     &= \frac{1}{n}\sum_{i=1}^nD_i y_i^TAx_i
% % \end{align*}
% % With the decomposition already employed in the proof of Corollary~\ref{cor:hanson_wright_convex_frobenius}, we write $A = P^T \Lambda Q$, $\check x_i \equiv Qx_i$ and $\check y_i \equiv Py_i$, then the identity:
% % \begin{align*}
% %    D_i y_i^TAx_i =   (D_i \un \odot \check y_i \odot \check x_i)^T \lambda 
% %   % \left\vert D_i y_i^TAx_i\right\vert = \left\vert D_i (\check y_i \odot \check x_i)^T \lambda\right\vert \leq
% %   % \left\{\begin{aligned}
% %   %   &|D_i| \|x_i\|\|y_i\|_\infty\\
% %   %   &|D_i| \|x_i\|_\infty\|y_i\|,
% %   % \end{aligned} \right.
% % \end{align*}
% % allows us to set thanks to Theorem~\ref{the:concentration_convexe_produit_odot_Rp} the concentration $D_i y_i^TAx_i \in \mathcal E_2(\log p/\sqrt n) \| \ e^{-n}$ since $D_i \un \propto_c \mathcal E_2(1/\sqrt n)$
% % \begin{align*}
  
% % \end{align*}
%   As in the proof of Corollary~\ref{cor:Concentration_XDYu} note $x_1,\ldots, x_n$ and $y_1,\ldots, y_n$, the columns of, respectively, $X$ and $Y$, considering a matrix $A \in \mathcal M_{p}$ such that $\|A\|_F\leq 1$, we can bound:
%   \begin{align*}
%     % &\frac{1}{n}\tr \left(AXD Y^T - \mathbb E[AX \mathbb E[D]Y^T]\right)\\
%     \left\vert \mathbb E \left[\frac{1}{n}\tr \left(AXD Y^T - AX \mathbb E[D] Y^T\right)\right]\right\vert
%     % &\hspace{1cm}= \frac{1}{n}\sum_{i=1}^nD_i y_i^TAx_i-\mathbb E[D_i]\mathbb E[y_i^TA_i]\\
%     % &\hspace{1cm}= \left\vert \mathbb E \left[\tr \left((Y^TAX-\mathbb E[Y^TAX])(D-\mathbb E[D])\right)\right]\right\vert\\
%     % &\hspace{1cm}=\frac{1}{n}\sum_{i=1}^n \left( y_i^TAx_i - \mathbb E[y_i^TAx_i ]\right) \mathbb E[D_i] + \mathbb E[y_i^TAx_i ] \left( D_i -\mathbb E[D_i]\right) \\
%     &\leq\frac{1}{n} \sum_{i=1}^n \mathbb E\left[ \left\vert y_i^TAx_i - \mathbb E[y_i^TAx_i ]\right\vert \left\vert  D_i -\mathbb E[D_i]\right\vert\right]
%     % &\hspace{1cm} +\left\vert \frac{1}{n} \tr \left(\mathbb E \left[Y^T AX\right] \left(\mathbb E \left[D\right] - \tilde D \right)\right)\right\vert
%     % &\hspace{1cm}
%     % \ \ \leq O(n \sqrt{\log p})
%     % &\hspace{1cm}\leq \sum_{i=1}^n \mathbb E \left[\left\vert y_i^TAx_i - \mathbb E[y_i^TAx_i ]\right\vert \left\vert \Gamma_i -\mathbb E[\Gamma_i]\right\vert\right] \ \ \leq O(n \sqrt{\log p})
%   \end{align*}
%   Corollary~\ref{cor:hanson_wright_convex_frobenius} implies that for all $i \in [n]$ $y_i^TAx_i \in \mathbb E[y_i^TAx_i] \pm \mathcal E_2(\sqrt{\log p}) + \mathcal E_1$, and we can bound thanks to H\"older inequality:
%   \begin{align*}
%     \mathbb E\left[ \left\vert y_i^TAx_i - \mathbb E[y_i^TAx_i ]\right\vert \left\vert  D_i -\mathbb E[D_i]\right\vert\right] \leq O \left(\sqrt{\frac{\log p}{n}}\right).
%   \end{align*}
%   which allows us to set the first result of the corollary.

%   For the second result, let us bound:
%   \begin{align*}
%     &\left\vert \mathbb E \left[\frac{1}{n}\tr \left(AXD Y^T - AX \tilde D Y^T\right)\right]\right\vert \\
%     &\hspace{0.5cm}\leq  \left\vert \mathbb E \left[\frac{1}{n}\tr \left(AXD Y^T - AX \mathbb E[D] Y^T\right)\right]\right\vert 
%     + \left\vert \frac{1}{n} \tr \left(\mathbb E \left[Y^T AX\right] \left(\mathbb E \left[D\right] - \tilde D \right)\right)\right\vert,
%   \end{align*}
%   we can then conclude since $\|\frac{1}{n}\mathbb E \left[Y^T AX\right]\|_F \leq O(1)$ and $\|\mathbb E \left[D\right] - \tilde D\|_F \leq O(1/\sqrt n)$.
%   % which directly implies the result of the corollary (the constants are the same for all $i \in [n]$).
%   % We can then conclude the result thanks to Corollary~\ref{cor:concentration_concatenation_vecteur_lineaireent_concentre} 
%   % {\color{orange} Les combinaisons de concentrations exonentielles d'ordre différents ne sont pas prises en compte par ce corollaire... mais ça se vérifie avec de bonne découpes des différents régimes.}
%   % \begin{align*}
%   %   \left( y_i^TAx_i - \mathbb E[y_i^TAx_i ]\right) \mathbb E[D_i] \in 0 \pm \mathcal E_2(\sqrt{\log p})\\
%   % \end{align*}
% \end{proof}

% \begin{proposition}\label{pro:concentration_produit_matrice_convexement_concentres}
%   Given two sequences $m \in \mathbb N^{\mathbb N}$ and $\sigma \in \mathbb R_ + ^{\mathbb N}$ such that $\log m \leq O(\eta)$, $m$ random nonnegative definite symmetric random matrix $X_1,\ldots,X_m \in \mathcal S_p^+$ such that $\forall i\in[m]$, $\|\mathbb E[X_i] \| = O(p^{1/q})$, if we suppose that%\footnote{The tail controlling term $e^{-\eta}$ with $\eta \leq  O(\log(p))$ might seem unsatisfactory, the concentration of the product can still be improved to become for any $\eta$}
%   \begin{align*}
%     (X_1,\ldots, X_m) \propto_c \mathcal E_2(\sigma) \ | \ e^{-p}
%     &\text{ in } \left(\mathcal M_p, \| \cdot \|_F\right),
%    \end{align*}
%    with, for any $M= (M_1,\ldots,M_m) \in (\mathbb S_p^+)^m$, $\|M\|_{\ell^\infty} = \sup_{1\leq i\leq m} \|M_i\|$, 
%    then there exists a constant $\kappa \leq O(1)$ such that
%    \begin{align*}
%     X_1\cdots X_m\propto_{\mathcal O_{p}}^{T} \mathcal E_2 \left( \left(\kappa \sigma\right)^{m} p^{(m-1)/q}\right) \ | \ e^{-p}&
%     &\text{ in } \left(\mathcal M_p, \| \cdot \|\right),
%    \end{align*}
%    Now with the same hypotheses, if we consider this time that the random matrices $X_1,\ldots, X_m$ belong to $\mathcal M_{p,n}$ and satisfy $\forall i\in[m]:\|\mathbb E[X_i] \| = O((p+n)^{1/q})$, we have the concentration
%    \begin{align*}
%     X_1X_1^T \cdots X_mX^T_m \propto_{\mathcal O_{p,n}}^{T} \mathcal E_2 \left( \left(\kappa \sigma\right)^{2m} (p+n)^{(2m-1)/q}\right) \ | \ e^{-(p+n)}&
%     &\text{ in } \left(\mathcal M_{p,n}, \| \cdot \|\right),
%    \end{align*}
% \end{proposition}
% We can now set t
The result of concentration of a product of matrices convexly concentrated was already proven in \cite{MEC11} but since their formulation is slightly different, we reprove in \ref{app:concentration_produit_vecteurs} the following result with the formulation required for the study of the resolvent.

\begin{theorem}[\cite{MEC11}, Theorem 1]\label{the:concentration_lineaire_produit_matrice_convexement_concentres}
  \sloppypar{Let us consider three sequences $m \in \mathbb N^{\mathbb N}$ and $\sigma,\kappa \in \mathbb R_ + ^{\mathbb N}$, and a sequence of $m$ random random matrices $X_1 \in \mathcal{M}_{n_0,n_1},\ldots, X_m \in \mathcal{M}_{n_{m-1},n_m}$, satisfying\footnote{The norm $\|\cdot \|_{F}$ is defined on $\mathcal{M}_{n_{0},n_1} \times \cdots\times \mathcal{M}_{n_{m-1},n_m}$ by the identity: $$\|(M_1,\ldots,M_p)\|_F = \sqrt{\|M_1\|_F^2 + \cdots + \|M_m\|_F^2}$$}:%\footnote{If we take a general assumption $X \propto \mathcal E_2$ for $q>0$, one has to assume $\|X \| \leq \kappa n^{1/q}$, then we obtain the concentration $(X/ n^{1/q})^m \in \mathcal E_2 \left(\sqrt n \left(2e\kappa \right)^{m-1} \sigma\right)$}:%\footnote{The tail controlling term $e^{-\eta}$ with $\eta \leq  O(\log(p))$ might seem unsatisfactory, the concentration of the product can still be improved to become for any $\eta$}
  \begin{center}
    $(X_1,\ldots,X_m) \propto_c \mathcal E_2(\sigma)$ in $\left(\mathcal{M}_{n_{0},n_1} \times \cdots\times \mathcal{M}_{n_{m-1},n_m}, \| \cdot \|_F\right),$
  % $X \propto_c \mathcal E_2$
  %   in $\left(\mathcal M_n, \| \cdot \|_F\right),$\footnote{Nothing is changed if we rather assume $X \propto_c \mathcal E_2.}
    \end{center}
    In the particular case where $X_1=\cdots = X_n \equiv X$, it is sufficient\footnote{Be careful that $X \propto_c \mathcal E_2(\sigma)$ does not imply that $(X,\ldots, X) \propto_c \mathcal E_2(\sigma)$, it is only true when $(\mathcal{M}_{n})^m $ is endowed with the norm $\|\cdot \|_{F,\ell^\infty}$, satisfying for any $M= (M_1,\ldots,M_m) \in (\mathcal M_{n})^m$, $\|M\|_{F,\ell^\infty} = \sup_{1\leq i\leq m} \|M_i\|_F$} to assume that $X \propto_c \mathcal E_2(\sigma)$ in $(\mathcal{M}_{n}, \|\cdot \|_F)$.
   % with, for any $M= (M_1,\ldots,M_m) \in (\mathbb S_p^+)^m$, $\|M\|_{\ell^\infty} = \sup_{1\leq i\leq m} \|M_i\|$, 
   % with, for any $M= (M_1,\ldots,M_m) \in (\mathcal M_n)^m$, $\|M\|_{F,\ell^\infty} = \sup_{1\leq i\leq m} \|M_i\|_F$.
   If there exist a sequence of positive values $\kappa>0$ such that $\forall i\in[m], \|X_i \| \leq \kappa $, then the product is concentrated for the nuclear norm:
   % and two constants $C,c >0$ such that $\mathbb P(\|X \| \geq \kappa) \leq C e^{-cp}$,
   % then:
   \begin{align*}
    X_1\cdots X_m \in \mathcal E_2 \left(\kappa^{m-1} \sigma \sqrt{n_0+\cdots + n_m}\right) &
    % (X_1/\sqrt n)\cdots (X_m/\sqrt n) \in \mathcal E_2 \left(\sqrt n \left(2e\kappa \right)^{m-1} \sigma\right) &
    &\text{ in } \left(\mathcal M_{n_0,n_m}, \| \cdot \|_*\right),
   \end{align*}
   where, for any $M \in \mathcal M_{n_0,n_m}$, $\|M\|_* = \tr(\sqrt{MM^T})$ (it is the dual norm of the spectral norm). 
   % When $X_1 = \cdots X_m \equiv X \in \mathcal{M}_{p,n}$, $X^m \in \mathcal E_2 \left(\kappa^{m-1} \sigma \sqrt{n +p}\right)$
   % With the same hypotheses, if we assume this time that $X_1,\ldots, X_m \in \mathcal M_{p,n}$, we have the concentration:
   % If we consider $X \in \mathcal M_{p,n}$ satisfying $X \proptp \mathcal E_2(\sigma)$
   % $\|X \| \leq \kappa$, we have the concentrations
   % $\|\mathbb E[X] \| = O((p+n)^{1/q})$, we have the concentrations
   % \begin{align*}
   %  X_1X_1^T \cdots X_mX_m^T \in \mathcal E_2 \left(\sqrt{p+n} \left(2e\kappa \right)^{2m-1} \sigma\right) &
   %  % (XX^T/(p+n))^m \in \mathcal E_2 \left(\sqrt{p+n} \left(2e\kappa \right)^{2m-1} \sigma\right) \ | \ e^{-(p+n)}&
   %  &\text{ in } \left(\mathcal M_{p}, \| \cdot \|_*\right),\\
   %  % (XX^T)^mX \in \mathcal E_2 \left(\sqrt{p+n} \left(2e\kappa \right)^{2m} \sigma\right) \ | \ e^{-(p+n)}&
   %  % &\text{ in } \left(\mathcal M_{p,n}, \| \cdot \|_*\right)
   % \end{align*}
   }

\end{theorem}
\begin{remark}\label{rem:quand_X_non_borne}
  The hypothesis $\|X\|\leq \kappa$ might look quite strong, however in classical settings where $X \propto \mathcal E_2$ and $\|\mathbb E[X]\|\leq O(\sqrt n)$ it has been shown that there exist three constants $C,c, K >0$ such that $\mathbb P(\|X \| \geq K\sqrt n) \leq C e^{-cn}$. Placing ourselves on the event $\mathcal A = \{\|X \| \leq K\sqrt n\}$, we can then show from Lemma~\ref{lem:concentration_conditionnee_convexe} below that:
  \begin{align*}
    \left( (X/\sqrt n)^m \ | \ \mathcal A \right) \in \mathcal E_2 \left(K^{m-1}/\sqrt n\right)&
    &\text{and}&
    &\mathbb P(A^c) \leq C e^{-cn},
  \end{align*}
  (here $\sigma = 1/\sqrt n$ and $\kappa = K$). The same inferences hold for the concentration of $(XX^T/(n+p))^m$.
\end{remark}

We end this section on the concentration of the product of convexly concentrated random vectors with the Hanson-Wright Theorem that will find some use of the estimation of $\mathbb E[Q^z]$. This result was first proven in \cite{ADA14}, an alternative proof with our notations is provided in \cite[Proposition 8]{LOU21HV}\footnote{This paper only studies the Lipschitz concentration case, however, since quadratic forms are convex, the arguments stays the same with convex concentration hypotheses.}.
\begin{theorem}[\cite{ADA14}]\label{the:hanson_wright}
  Given two random matrices $X,Y \in \mathcal{M}_{p,n}$ such that $(X,Y) \propto_c \mathcal E_2$ and $\|\mathbb E[X]\|_F,\|\mathbb E[Y]\|_F\leq O(1)$, for any $ A \in \mathcal{M}_{p}$:
  \begin{align*}
    Y^TAX \in \mathcal E_2(\|A\|_F) + \mathcal E_1(\|A\|).
  \end{align*}
\end{theorem}
% \begin{remark}\label{rem:concentration_produit_differentes_matrice_convexes}
%   We could have given a result of concentration concerning the product of different matrices $X_1,\ldots, X_m$ but the expression would have been complicated since our proof relies on Lemma~\ref{lem:decomposition_poly_symetrique}, and without a strong hypothesis of commutativity on the matrices $X_1,\ldots, X_n$, one could not have gone further than a concentration on the whole term $\sum_{\sigma \in \mathfrak S_p}\tr(X_{\sigma(1)} \cdots X_{\sigma(m)})$. However, if $m$ is small, say, if $m \leq O(1) $, and the 
% % \begin{align*}
% %     X_1 \cdots X_m ,
% %    \end{align*}
%    % Besides, if the 
%    matrices $X_1,\ldots, X_m$ have different sizes, we can still introduce the random matrix
%     \begin{align*}
%     Y = \left( \begin{array}{cccc}
%     0&X_{m-1} &&  \\
%     & \ddots&\ddots& \\
%     & &\ddots&X_1 \\
%     X_m& &&0  \end{array} \right)&
%     &\text{then}&
%     &Y^m = \left( \begin{array}{cccc}
%     0&X^{m}_1 &&  \\
%     & \ddots&\ddots& \\
%     & &\ddots&X_{3}^{2}\\
%     X_{2}^{1}& &&0  \end{array} \right),
%     \end{align*}
%     where for $i,j \in \{2, \ldots, m-1\}$, $X^j_i \equiv X_i X_{i+1} \cdots X_m X_{1}\cdots X_j$ and $X_1^m \equiv X_1 \cdots X_m$, then the concentration $(Y/\sqrt n)^m \in \mathcal E_2 \left(\sqrt n \kappa^{m-1}\sigma\right) \ | \ e^{-p}$ in $\left(\mathcal M_n, \| \cdot \|_*\right)$ directly implies the concentration of $X_1^m$.
%    \end{remark}
% \begin{remark}\label{rem:choix_des_normes_resultat_concentration_produit_matrice_convexe}
  % In Theorem~\ref{the:concentration_lineaire_produit_matrice_convexement_concentres} we take as hypothesis a convex concentration of $X$ with the spectral norm $\|\cdot \|$ since we do not need any stronger result. This is however useless since the Theorem of Talagrand (Theorem~\ref{the:talagrand}) only gives the concentration in euclidean spaces, like $(\mathcal M_p,\|\cdot \|_F)$ or $(\mathcal M_{p,n},\|\cdot \|_F)$.

  % Although a concentration with the nuclear norm is a stronger result that the concentration with the Frobenius norm as the one we got for a product of Lipschitz concentrated matrices (see Proposition~\ref{pro:concentration_intelligente_produit}), note however that here, there is a supplementary constant $\sqrt p$ (or $\sqrt{p+n}$) that increases greatly the observable diameter of $X^m$ when $X$ is only convexly concentrated.
% \end{remark}
% Before show Theorem~\ref{the:concentration_lineaire_produit_matrice_convexement_concentres}, we need a central result on the concentration of the singular decomposition of a matrix.
% \begin{lemma}\label{lem:concentration_convexe_décomposition_singuliere}
%   Given a random matrix $X \in \mathcal M_{p,n}$ such that $X \propto \mathcal E_2$, if we note $P\sigma(X)Q$, its singular decomposition with $P, Q \in \mathcal O_{p,n}$ and $\sigma(X) \in \mathcal D_{p,n}$, then:
%   \begin{align*}
%     (\sigma(X),P,Q) \propto_c&
%     &\text{and}&
%     &
%   \end{align*}
% \end{lemma}

% \begin{remark}\label{rem:concentration_XX^T^mX}
%   One could have also shown that the matrix $(\frac{XX^T}{p+n})^mX/\sqrt{p+n}$ follows the same concentration as $\frac{XX^T}{p+n}$ expressing in the preceding proof the concentration of the matrix:
%   \begin{align*}
%     Y^{2m +1} =\left( \begin{array}{cc}
%     0 & (X^TX)^mX^T \\ 
%     (XX^T)^mX & 0 \end{array} \right)\in \mathcal E_2 \left(\left(2e\kappa \sqrt {p+n}\right)^{2m} \sigma\right) \ | \ e^{-(p+n)}.
%   \end{align*}
% \end{remark}

\section{Concentration of the resolvent of the sample covariance matrix of convexly concentrated data}\label{sse:convex_concentration_resolvent}
% lemma allows us to treat the concentration of observations $f(Z)$ when $f$ is Lipschitz and quasi-convex only on a convex subset $A$ and $\{Z\in A\}$ has positive probability non decreasing with the dimension.
\subsection{Assumptions on $X$ and ``concentration zone'' of the resolvent}
%This subsection is the only one that employs results of Section~\ref{sec:concentration_of_the_solutions_to_conc_eq}, to show that the resolvent is concentrated ( recall that in the case of a Lipschitz concentration hypothesis on $X$, it is more simple to show that the resolvent is a $O(1/\sqrt n)$-Lipschitz transformation of $X$). 
Given $n$ data $x_1,\ldots, x_n \in \mathbb R^p$, to study the eigen behavior of the sample (non centered) covariance matrix $\frac{1}{n}XX^T$, where $X = (x_1,\ldots, x_n) \in \mathcal{M}_{p,n}$, one classically studies the resolvent $Q^z = (zI_p - \frac{1}{n} XX^T)^{-1} $ for the values of $z$ where it is defined. Let us note the $p$ eigen values of $\frac{1}{n}XX^T$: $\lambda_i = \sigma_i(\frac{1}{n}XX^T)$, for $i \in [p]$ ( then $\lambda_1\geq \cdots \geq \lambda_n$), then the spectral distribution of $\frac{1}{n}XX^T$:
\begin{align*}
  \mu = \frac{1}{p} \sum_{i=1}^p \delta_i
\end{align*}
has for Stieltjes transform $g : z \mapsto \frac{1}{p} \tr (Q^z)$. 
% Understanding the concentration of $Q^z$ and approximate its expectation gives us direct inferences on $\mu$. 

The present study was already lead in previous papers in the case of Lipschitz concentration of $X$ \cite{LOU21RHL}, or in the case of convex concentration of $X$ but with negative $z$ \cite{LOU19}. The goal of this section, is manly to present the consequences of Theorem~\ref{the:concentration_lineaire_produit_matrice_convexement_concentres} and adapt the recent results of \cite{LOU21RHL} on the case of convex concentration.
We adopt here classical hypotheses and assume a convex concentration for $X=(x_1,\ldots, x_n)$.
\begin{assumption}[Convergence scheme]\label{ass:p_plus_petit_que_n}
  $p = O(n)$.
\end{assumption}
\begin{assumption}[Independence]\label{ass:independence_x_i}
  $x_1,\ldots, x_n$ are independent.
\end{assumption}
\begin{assumption}[Concentration]\label{ass:concentration_covexe_X}
  $X \propto_c \mathcal E_2$.
\end{assumption}
\begin{assumption}[Bounding condition\footnote{As already done in \cite{LOU19} (but with real negative $z$), one can obtain the same conclusion assuming that there are a finite number of classes for the distribution of the columns $x_1,\ldots, x_n$ and that $\sup_{i\in [n]}\|\mathbb E[x_i]\| \leq O(\sqrt n)$}]\label{ass:borne_x_i}  $\sup_{i\in [n]}\|\mathbb E[x_i]\| \leq O(1)$.
\end{assumption}
When $n$ gets big, $\mu$ distributes along a finite number of bulks. To describe them, let us consider a positive parameter, $\varepsilon>0$, that could be chosen arbitrarily small (it will though be chosen independent with $n$ in most practical cases) and introduce as in \cite{LOU21RHL} the sets:
\begin{align*}
  \mathcal S = \left\{\lambda_i\right\}_{i \in [p]} &
  &\bar{\mathcal S} = \left\{ \mathbb E[\lambda_i]\right\}_{i \in [p]} &
  &\bar{\mathcal S}^\varepsilon = \left\{ x \in \mathbb R, \exists i \in [n], |x - \lambda_i | \leq \varepsilon \right\}
\end{align*}
% Let us note $\nu = \mathbb E[\|X\|]/ \sqrt n \leq O(\sqrt n)$ (see \cite{}) and 
One can show that $\nu \equiv \sup \bar{\mathcal S} = \mathbb E[\lambda_1] \leq O(1)$ and introducing the event:
\begin{align*}
  \mathcal A_\varepsilon \equiv \left\{ \forall i \in[p], \sigma_i \left( \frac{1}{n}XX^T \right) \in \bar{\mathcal S}^{\varepsilon/2} \right\},
\end{align*}
the concentration of $\sigma(X) / \sqrt n \in \mathbb E[\sigma(X)] \pm \mathcal E_2(1/\sqrt n)$, allows us to set:\footnote{In \cite{LOU21RHL}, the proof is conducted for Lipschitz concentration hypotheses on $X$. However, since only the linear concentration of $\sigma(X)$ is needed, the justification are the same in a context of convex concentration thanks to Theorem~\ref{the:concentrtaion_transversale_matrice_vecteurs}.}
\begin{lemma}[\cite{LOU21RHL}, Lemma 3.]\label{lem:P_A_overwhelming}
  There exist two constants $C,c>0$ such that $\mathbb P \left(\mathcal A^c\right) \leq C e^{-cn\varepsilon^2}$.
\end{lemma}
% we know that there exist two constants $C,c>0$ such that:
% \begin{align*}
%   \mathbb P \left( \|X\| \geq \left( \nu +\frac{\varepsilon}{2} \right) \sqrt n \right) \leq C e^{-cn}.
% \end{align*}
% Let us then note $\bar \nu = \nu \varepsilon$ and $\mathcal A = \{\|X\| \geq \bar \nu  \sqrt n\}$. 
The following lemma allows us to conduct the concentration study on the highly probable event $\mathcal A_\varepsilon$ (when $\varepsilon \geq O(1)$).
 % $\mathcal A \equiv \{\|X\| \geq \bar \nu  \sqrt n\}$, where $\bar \nu = \nu \varepsilon$.

\begin{lemma}\label{lem:concentration_conditionnee_convexe}
  Given a (sequence of) positive numbers $\sigma>0$, a (sequence of) random vector $Z\in E$ satisfying $Z \propto \mathcal E_2(\sigma)$, and a (sequence of) convex subsets $ A \subset E$, if there exists a constant $K>0$ such that $\mathbb P(Z \in  A)\geq K$ then:\footnote{There exist two constants $C,s>0$ such that for any (sequence of) $1$-Lipschitz and quasi-convex mappings $f:A\to \mathbb R$:
  \begin{align*}
    \forall t>0: \ \mathbb P \left(\left\vert f(Z) - \mathbb E[f(Z) \ | \ Z \in A]\right\vert \geq t \ | \ Z \in A \right) \leq C e^{-(t/c\sigma)^2},
    % X \propto_c \mathcal E_2(\sigma)&
    % &\Longrightarrow&
    % &(X \ | \ X\in A) \propto_c \mathcal E_2(\sigma)&
  \end{align*}
  and similar concentration occur around any median of $f(Z)$ or any independent copy of $Z$ (under $\{Z \in A\}$).}
  \begin{align*}
     (Z | Z \in A) \propto_c \mathcal E_2(\sigma).
   \end{align*}
\end{lemma}
\begin{proof}
  The proof is the same as the one provided in \cite[Lemma 2.]{LOU21RHL} except that this time, one needs the additional argument that since $S = \{f \leq m_f\}$ (for $m_f$, a median of $f$) is convex, the mappings $z \mapsto d(z, S)$ and $z \mapsto -d(z, S)$ are both quasi-convex thanks to the triangular inequality.
\end{proof}


We can deduce from Lemma~\ref{lem:concentration_conditionnee_convexe} that for all $\varepsilon \geq O(1)$, $(X \ |\ \mathcal A_\varepsilon) \propto_c \mathcal E_2$, and the random matrix $(X \ |\ \mathcal A_\varepsilon)$ is far easier to control because $\|(X \ |\ \mathcal A_\varepsilon)\| \leq \nu + \frac{\varepsilon}{2}$ (we recall that $\nu \equiv \mathbb E[\lambda_1]$).

\subsection{Concentration of the resolvent}
Placing ourselves under the event $\mathcal A_\varepsilon$, let us first show that the resolvent $Q^z \equiv (zI_p - \frac{1}{n}XX^T)^{-1}$ is concentrated if $z$ has a big enough modulus. Be careful that the following concentration is expressed for the nuclear norm (for any deterministic matrix $A \in \mathcal{M}_{p}$ such that $\|A\|\leq O(1)$, $\tr(AQ^z) \in \mathcal E_2$). All the following results are provided under Assumptions~\ref{ass:p_plus_petit_que_n}-\ref{ass:borne_x_i}.
The next proposition is just provided as a first direct application of Theorem~\ref{the:concentration_lineaire_produit_matrice_convexement_concentres} and Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres}, a stronger result is provided in Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre}.
%\footnote{We showed in \cite{LOU19} that we have a linear concentration of $Q^z$ for $z \in \mathbb R^-$ with slightly different techniques.}. % we need to show the concentration of $D z$.
\begin{proposition}\label{pro:concentration_resolvent_concentration_convexe_z_hors_de_boule}
  Given two parameters $\varepsilon>0$ and $z \in \mathbb C$ such that $|z| \geq \nu + \varepsilon$:
  % $|z| \geq 4e^2 (\nu + \varepsilon)$:
  % \footnote{Recall that $\mathcal A_Q = \{\|XX^T\| \leq \bar \nu\}$ and that $\mathbb P(\mathcal A_Q^c) \leq C e^{-cn}$ for two constants $C,c>0$.}:
  \begin{align*}
    (Q^z \ | \ \mathcal A_\varepsilon) \in \mathcal E_2 \left( \frac{4}{\varepsilon} (\nu + \varepsilon )\right)&
    &\text{in} \ \ (\mathcal M_{p}, \| \cdot \|_*).
    % &\text{and}&
    % &\mathbb P(\mathcal A^c) \leq C e^{-cn},
  \end{align*}
  % for two constants $C,c>0$.
\end{proposition}
\begin{proof}
  % We already know from Corollary~\ref{cor:concentration_resolvante_concentration_convexe} that 
  We know from Lemma~\ref{lem:concentration_conditionnee_convexe} that $(X \ | \ \mathcal A_\varepsilon) \propto_c \mathcal E_2$ and from Theorem~\ref{the:concentration_lineaire_produit_matrice_convexement_concentres} that (here $\kappa = \nu + \frac{\varepsilon}{2} \leq O(1)$, $\sigma = 1/\sqrt n$ and $p= O(n)$):
  \begin{align*}
     \text{Under $\mathcal A_\varepsilon$:}&
     &\left(\frac{1}{n}XX^T\right)^m \in \mathcal E_2 \left( \left( \nu + \frac{\varepsilon}{2} \right)^{m} \sqrt m \right)&
    &\text{ in } \left(\mathcal M_{p}, \| \cdot \|_*\right).
   \end{align*}
   % Let us note $\rho \equiv 8e^2 \bar \nu$. Given a complex number $z \in \mathbb C$ such that $|z| \geq \rho$, $\frac{4e^2\bar\nu}{|z|} \leq \frac{1}{2}$ and we can already deduce from Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres}:
   % Let us then consider a constant $\epsilon \in (0,1)$ (such that $\epsilon \geq O(1)$) and a complex number $z \in \mathbb C$ such that $|z| \geq \frac{ 4e^2 \bar \nu }{1-\epsilon}$, w
   Let us then note that $\left(\nu + \frac{\varepsilon}{2} \right)^{m} \sqrt m  = O\left(\left(\nu + \frac{3\varepsilon}{4} \right)^{m}\right)$ and for $z \in \mathbb C$ satisfying our hypotheses: $(\nu + \frac{3\varepsilon}{4})/|z| \leq 1 - \frac{\varepsilon}{4(\nu + \varepsilon)}$.
   % Taking , $\frac{\nu}{|z|} \leq 1 -\epsilon$ for a constant $\epsilon \geq O(1)$, w
   We can then deduce from Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres} that under $\mathcal A_\varepsilon$:
   \begin{align*}
     Q^z = \frac{1}{z} \left(I_p - \frac{1}{zn}XX^T\right)^{-1} = \frac{1}{z}\sum_{i=1}^\infty \left(\frac{1}{zn}XX^T\right)^i \in \mathcal E_2 \left( \frac{4}{\varepsilon} (\nu + \varepsilon )\right).
     % &\text{ in } \left(\mathcal M_{p}, \| \cdot \|_*\right)
   \end{align*}
   % in $\left(\mathcal M_{p}, \| \cdot \|_F\right)$.
    % and $\frac{X}{\sqrt{z}}$ satisfies the hypothesis of Corollary~\ref{cor:Concentration_linearire_solution_implicite_hypo_concentration_phi^k_pour tout_k} ($\sqrt z$ is a given square root of $z$, $\|X/\sqrt z\| \leq \sqrt p (1-\varepsilon)/2e$ with high probability). Therefore, the concentration of $Q^z$ is already valid in that case.


  % where $Z \equiv \left(\sum_{i=1}^k \binom{k}{i} (-1)^{i} \left(\frac{1}{nz_0}XX^T\right)^k\right) \in \mathcal E_2 \left( \left(\frac{2\bar\nu}{|z_0|}\right)^k\right)$
  % Thanks to Lemma~\ref{lem:Borne_resolvante_2}:
  % $$\|(z_0-z) Q^{z_0}\| \leq \frac{|z_0-z|}{d(z_0,[0,1-\varepsilon])} \leq 1 - \frac{d(z,[0,\bar \nu])}{d(z_0,[0,1-\varepsilon])},$$
  % with $\frac{d(z,[0,\bar \nu])}{d(z_0,[0,1-\varepsilon])} \geq O(1)$. 
\end{proof}
% For complex number $z \in \mathbb C$ such that $|z|$ is smaller than $\rho \equiv 4e^2\bar \nu$ (and $d(z,[0,\bar \nu])\geq O(1)$), we can still show that $\tr(AQ^z)$ is concentrated when $A = I_p$.
Let us now try to study the concentration of $Q^z$ when $z$ gets close to the spectrum, for that we now require $\varepsilon>0$ to be a constant ($\varepsilon \geq O(1)$).
\begin{proposition}\label{pro:Concentration_lineaire_Q_proche_spectre}
  Given $\varepsilon \geq O(1)$, for all $z \in \mathbb C \setminus \bar{\mathcal S}^ \varepsilon$:
  \begin{align*}
    (Q^z \ | \ \mathcal A_\varepsilon) \in \mathcal E_2&
    &\text{in } \ \ (\mathcal{M}_{p}, \|\cdot \|_*),
  \end{align*}
  and we recall that there exist two constants $C,c>0$ such that $ \mathbb P \left( \mathcal A_\varepsilon^c \right)  \leq C e^{-cn}$.
\end{proposition}
% \begin{remark}\label{rem:conc_norm_nucl_implique_conc_norme_frob}
%   Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} implies in particular that for any deterministic matrix $A\in \mathcal{M}_{p}$ such that $\|A\|\leq 1$ , $\frac{1}{n}\tr(AQ^z) \in \mathcal E_2$ in $(\mathcal M_{p}, \| \cdot \|_F)$ for all $z \in \mathbb C$ such that $d(z, [0,\bar\nu]) \geq O(1)$.  
%   % Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} implies in particular that $Q^z \in \mathcal E_2$ in $(\mathcal M_{p}, \| \cdot \|_F)$ for all $z \in \mathbb C$ such that $d(z, [0,\bar\nu]) \geq O(1)$.  
%   % (when $z \in \mathbb C \setminus \mathcal B(0, 4e^2(\nu +\varepsilon))$, Proposition~\ref{pro:concentration_resolvent_concentration_convexe_z_hors_de_boule} is stronger).
% \end{remark}
\begin{proof}
  Proposition~\ref{pro:concentration_resolvent_concentration_convexe_z_hors_de_boule} already set the result for $|z| \geq \nu + \varepsilon\equiv \rho $, therefore, let us now suppose that $|z| \leq \rho$. 

  % We can distinguish three cases:% (that altogether contain $\{|z| \geq 8e^2 \bar \nu\}$):
  %   \begin{enumerate}
  %      \item $z \in \mathbb R$ and $z \leq - \epsilon$ with $\epsilon\geq O(1)$
  %      % \item $z \in \mathbb R$ and $z \geq \bar \nu + \epsilon$ with $\epsilon\geq O(1)$
  %      \item $z \in \mathbb C$ and $|\Im z | \geq \epsilon$ with $\epsilon\geq O(1)$
  %    \end{enumerate}
  % If $z \leq -\epsilon$, we can rewrite
  % \begin{align*}
  %   Q^z = \frac{1}{\rho} \left( I_p - \left( I_p - \frac{|z|}{\rho} - \frac{1}{n\rho}XX^T\right) \right)^{-1} = \frac{1}{\rho} \sum_{m=0}^\infty \left( I_p - \frac{|z|}{\rho} - \frac{1}{n\rho}XX^T\right)^m
  % \end{align*}
  % \begin{align*}
  %     \tr(Q^z) 
  %     &= \sum_{i=1}^p \frac{1}{z - \frac{1}{n}\sigma(X)^2_i}
  %     = -\frac{1}{\rho}\sum_{i=1}^p \frac{1}{\frac{|z|}{\rho} + \frac{\sigma(X)^2_i}{\rho n}} \\
  %     &= -\frac{1}{\rho}\sum_{i=1}^p \frac{1}{1 - \left(1 - \frac{|z|}{\rho} - \frac{\sigma(X)^2_i}{\rho n}\right)} 
  %     = -\frac{1}{\rho}\sum_{k=1}^\infty \sum_{i=1}^p  \left(1 - \frac{|z|}{\rho} - \frac{\sigma(X)^2_i}{\rho n}\right)^k
  %     % = \sum_{i=1}^p \frac{\Re(z) - \sigma(X)_i^2}{\Im(z)^2 + (\Re(z) - \sigma(X)_i^2)^2} - \frac{\Im(z)}{\Im(z)^2 + (\Re(z) - \sigma(X)_i^2)^2}\\
  %     % & = \frac{1}{\Im(z)^2} \sum_{i=1}^p \sum_{k=1}^\infty \frac{2\cdots (k+2)}{(k+1)!} \left(\frac{\Re(z) - \sigma(X)_i^2}{\Im(z)}\right)^{k+1} \left(\Re(z) - \sigma(X)_i^2 -\Im(z)\right) 
  %   \end{align*}  
    % This Taylor expansion is allowed since, with the order relation on the set of symmetric matrices:
    % \begin{align*}
    %   -1<-\frac{\nu + \frac{\varepsilon}{2}}{\rho} \leq 1 - \frac{|z|}{\rho} - \frac{1}{\rho n} XX^T < \frac{\varepsilon}{2\rho}  <1
    % \end{align*}
    % Summing the observable diameters with Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres}, we obtain $Q^z \in \mathcal E_2(1/\sqrt n)$.
    % Besides, as in the proof of Theorem~\ref{the:concentration_convexe_produit_odot_Rp}, $\forall k \in \mathbb N$, $y \mapsto \sum_{i=1}^p\left(1 - \frac{|z|}{\rho} + \frac{y^2_i}{\rho n}\right)^k$ can be written as the difference of two $C(1 -\epsilon')^{k-1}$-Lipschitz convex mappings on $\sigma(X)(\mathcal A_Q)$, for well chosen constants $C\leq O(1)$ and $\epsilon\geq O(1)$. We can then conclude from the concentration of $\sigma(X)$ given by Theorem~\ref{the:concentrtaion_transversale_matrice_vecteurs} with Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres}.

    % If $z \in [\bar \nu , \rho]$, $\forall i\in [n]$, $0\leq \frac{\sigma(X)^2_i}{ n} \leq z$ and we rather decompose:
    % \begin{align*}
    %   \tr(Q^z) 
    %   &= \sum_{i=1}^p \frac{1}{1 - \left(1 - z + \frac{\sigma(X)^2_i}{ n}\right)} 
    %   = \sum_{k=1}^\infty \sum_{i=1}^p  \left(1 - z + \frac{\sigma(X)^2_i}{ n}\right)^k
    % \end{align*}
    % we can then conclude as previously.

    % If $\Im(z) \geq \epsilon$ (and $|z|\leq \rho$) we 
    With the notation $|Q^z|^2 \equiv \left( \Im(z)^2 + \left( \Re(z) - \frac{1}{n}XX^T \right)^2 \right)^{-1}$, let us decompose:
    \begin{align}\label{eq:decomposition_Q_Im_Re}
      Q^z = \left( \Re(z) - \frac{1}{n}XX^T \right) |Q^z|^2 - \Im(z) |Q^z|^2.
    \end{align}
    We can then deduce the linear concentration of $|Q^z|^2$ with the same justifications as previously thanks to the Taylor decomposition:
    \begin{align*}
      |Q^z|^2 
      % &= \left( \Im(z)^2 + \left( \Re(z) - \frac{1}{n}XX^T \right)^2 \right)^{-1} \\
      = \frac{1}{\rho^2} \sum_{m=0}^\infty \left( 1 - \frac{\Im(z)^2}{\rho^2} - \frac{\left( \Re(z) - \frac{1}{n}XX^T \right)^2}{\rho^2} \right)^m.
    \end{align*}
    % \begin{align*}
    %   \tr(Q^z) 
    %   &= \sum_{i=1}^p \frac{\Re(z) - \sigma(X)_i^2}{\Im(z)^2 + (\Re(z) - \sigma(X)_i^2)^2} - \frac{\Im(z)}{\Im(z)^2 + (\Re(z) - \sigma(X)_i^2)^2}\\
    %   & = \frac{1}{2\rho^2} \sum_{i=1}^p \sum_{k=1}^\infty  \left(1 - \frac{\Im(z)^2}{2\rho^2}  - \frac{1}{2\rho^2}\left(\Re(z) - \sigma(X)_i^2\right) ^2\right)^{k} \left(\Re(z) - \sigma(X)_i^2 -\Im(z)\right) 
    %   % & = \frac{1}{\rho} \sum_{i=1}^p \sum_{k=1}^\infty \frac{2\cdots (k+2)}{(k+1)!} \left(\frac{\Re(z) - \sigma(X)_i^2}{\Im(z)}\right)^{k+1} \left(\Re(z) - \sigma(X)_i^2 -\Im(z)\right) 
    % \end{align*}
    % We conclude on 
    Indeed, $\|\Re(z)I_p - \frac{1}{n}XX^T\| \leq d(\Re(z), S)$ and $d(z, S)^2 = \Im(z)^2 + d(\Re(z), S)^2 \leq \rho$ thus:
    \begin{align*}
      \left\Vert 1 - \frac{\Im(z)^2}{\rho^2}  - \frac{1}{\rho^2}\left(\Re(z)I_p - \frac{1}{n}XX^T\right) ^2\right\Vert 
      & \leq 1 -  \frac{d(z, S)^2}{\rho^2}   
      % & \leq 1 -  \frac{\Im(z)^2 + d(\Re(z), S)^2}{\rho^2}   \\ 
      % & \leq 1 -  \frac{\Im(z)^2}{2\rho^2}  - \frac{1}{2\rho^2}d\left(\Re(z), \mathcal S\right) ^2  \\ 
      \leq 1 - \frac{\varepsilon^2}{\rho^2} <1 .
    \end{align*}
  We therefore deduce from \eqref{eq:decomposition_Q_Im_Re} that:
  \begin{align*}
    (Q^z \ | \ \mathcal A_\varepsilon) 
    \in \mathcal E_2 \left(\frac{ 2}{\varepsilon^2} \left( |\Im(z)| + |\Re(z)| + \nu + \frac{\varepsilon}{2} \right)  \right) 
    = \mathcal E_2.
  \end{align*}
    % We can then develop $\frac{1}{{\Im(z)^2 + (\Re(z) - \sigma(X)_i^2)^2}} = \frac{1}{{1 -\Im(z)^2 + (\Re(z) - \sigma(X)_i^2)^2}}$
\end{proof}
For the sake of completeness, we left in the appendix an alternative laborious proof (but somehow more direct) already presented in \cite{LOU19}. 
% we can still show the linear concentration of the resolvent $Q^z$ for values of $z$ close to the spectrum of $\frac{1}{n}XX^T$ with the nuclear norm $\|\cdot \|_*$ but with a bigger observable diameter.
 % For any $M \in \mathcal M_{p,n}$, the nuclear norm is defined with the identity $\|M\|_* = \tr(\sqrt{MM^T})$ (it is the dual norm of the spectral norm).
% \begin{proposition}\label{pro:concentration_lineaire_Q_hypo_convexe_proche_spectre}
%   Given $z \in \mathbb C$ such that $d(z, [0,\bar\nu]) \geq O(1)$:
%   \begin{align*}
%     Q^z \in \mathcal E_2 &
%     &\text{in} \ \ (\mathcal M_{p}, \| \cdot \|_*)
%   \end{align*}
% \end{proposition}


\subsection{Computable deterministic equivalent}\label{sse:equivalent_deterministe_hypothese_convexe}

We are going to look for a deterministic equivalent of $Q$. We mainly follow the lines of \cite{LOU21RHL}, we thus allow ourselves to present the justifications rather succinctly. Although Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} gives us a concentration of $Q^z$ in nuclear norm, we will provide a deterministic equivalent for the Frobenius norm with a better observable diameter. 
% The next proposition gives us a first deterministic equivalent, as an approximation to $\mathbb E[Q^z]$. 
For any $z \in \mathbb C \setminus S^\varepsilon$, let us introduce $\bar \Lambda^z = (\tr(\Sigma_i\mathbb E[Q^z]))_{i \in [n]}$ and recall that for any $\delta \in \mathbb C^n$, we note $\tilde Q_\delta^z = (zI_p - \frac{1}{n} \sum_{i=1}^n \frac{\Sigma_i}{1-\delta_i})^z $. We have the following first approximation to $\mathbb E[Q^z]$:
\begin{proposition}\label{pro:first_deterministic_equivalent}
  For any $z \in \mathbb C \setminus \bar{\mathcal S}^\varepsilon$:
  \begin{align*}
    \left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert \leq O(1)&
    &\text{and}&
    &\left\Vert \mathbb E[Q^z] - \tilde Q^z_{\bar \Lambda^z} \right\Vert_F \leq O \left( \frac{1}{\sqrt n} \right).
  \end{align*}
\end{proposition}
To prove this proposition, we will play on the dependence of $Q^z$ towards $x_i$ with the notation $X_{-i} \equiv (x_1, \ldots, x_{i-1}, 0, x_{i+1}, \ldots, x_n) \in \mathcal{M}_{p,n}$ and:
\begin{align*}
  Q_{-i}^z \equiv \left( zI_p - \frac{1}{n}X_{-i}X_{-i}^T \right)^{-1}.
\end{align*}
To link $Q^z$ to $Q^z_{-i}$ we will extensively use a direct application of the Schur identity:
\begin{align}\label{eq:Schur_identity}
   Q^zx_i = \frac{Q^z_{-i}x_i}{1 - \frac{1}{n}x_i^T Q^z_{-i}x_i}.
 \end{align} 
\begin{proof}
  All the estimations hold under $\mathcal A_\varepsilon$, therefore the expectation should also be taken under $\mathcal A_\varepsilon$ to be fully rigorous. Note that if $Q_{-i}$ and $x_i$ are independent on the whole universe, they are no more independent under $\mathcal A_\varepsilon$. However, since the probability of $\mathcal A_\varepsilon$ is overwhelming, the correction terms are negligible, we thus allow ourselves to abusively expel from this proof the independence and approximation issues related to $\mathcal A_\varepsilon$, a rigorous justification is provided in \cite{LOU21RHL}.

  Let us bound for any deterministic matrix $A \in \mathcal{M}_{p}$ such that $\|A\|_F\leq 1$:
  % Let us bound for any deterministic vector $u \in \mathbb R^p$ such that $\|u\|\leq 1$:
  \begin{align*}
   \left\vert  \tr \left( A  \left( \mathbb E[Q^z] - \tilde Q^z_{\bar \Lambda^z} \right) \right) \right\vert
    &\leq \frac{1}{n} \sum_{i=1}^n \left\vert \mathbb E \left[ \tr \left( A \left( Q^z \left( \frac{\Sigma_i}{1+\bar \Lambda^z_i} - x_ix_i^T \right)\tilde Q^z_{\bar \Lambda^z} \right) \right) \right] \right\vert.
    % &\leq \frac{1}{n} \sum_{i=1}^n \left\vert \mathbb E \left[ u^T Q^z \left( \frac{\Sigma_i}{1+\bar \Lambda^z_i} - x_ix_i^T \right)\tilde Q^z_{\bar \Lambda^z} u \right] \right\vert.
  \end{align*}
  We can then develop with \eqref{eq:Schur_identity}:
  \begin{align*}
    &\left\vert  \tr \left( A \left( \mathbb E[Q^z] - \tilde Q^z_{\bar \Lambda^z} \right) \right) \right\vert\\
    % \left\vert  u^T \left( \mathbb E[Q^z] - \tilde Q^z_{\bar \Lambda^z} \right)u \right\vert
    &\hspace{1cm}\leq \frac{1}{n} \sum_{i=1}^n \left\vert   \frac{ \tr \left( A \left( \mathbb E \left[ Q^z - Q_{-i}^z  \right]\Sigma_i\tilde Q^z_{\bar \Lambda^z} \right) \right)}{1 - \bar \Lambda^z_i}  \right\vert\\
    &\hspace{1cm}\hspace{1cm} + \frac{1}{n} \sum_{i=1}^n \left\vert \mathbb E \left[ \tr \left( A \left( Q_{-i}^z \left( \frac{\Sigma_i}{1 - \bar \Lambda^z_i} - \frac{x_ix_i^T}{1 + \frac{1}{n} x_i^T Q_{-i}^z x_i} \right)\tilde Q^z_{\bar \Lambda^z} \right) \right) \right] \right\vert\\
    &\hspace{1cm}\leq \frac{1}{n} \sum_{i=1}^n \left\vert \mathbb E \left[ \frac{ x_i^T\tilde Q^z_{\bar \Lambda^z}A Q^zx_i}{1 - \bar \Lambda^z_i } \left( \frac{1}{n }x_i^t Q_{-i}^z x_i - \bar \Lambda^z_i \right)\right] \right\vert + O \left(  \frac{\left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert}{\sqrt n}\right),
  \end{align*}
  thanks to Lemma~\ref{lem:Q_m_Q_m_i} and the independence between $Q_{-i}^z$ and $x_i$.
  We can then bound thanks to H\"older inequality and Lemma~\ref{lem:concentration xQx} below:
  \begin{align*}
    &\left\vert \mathbb E \left[ x_i^T\tilde Q^z_{\bar \Lambda^z}A Q^zx_i \left( \frac{1}{n }x_i^t Q_{-i}^z x_i - \bar \Lambda^z_i \right) \right] \right\vert\\
    &\hspace{0.5cm}=\left\vert \mathbb E \left[ \left( x_i^T\tilde Q^z_{\bar \Lambda^z}A Q^zx_i - \mathbb E \left[ x_i^T\tilde Q^z_{\bar \Lambda^z}A Q^zx_i \right] \right) \left( \frac{1}{n }x_i^t Q_{-i}^z x_i - \mathbb E \left[ \frac{1}{n }x_i^t Q_{-i}^z x_i \right] \right) \right] \right\vert\\
  &\hspace{0.5cm}\leq \sqrt{\mathbb E \left[ \left(\frac{x_i^T AQ^z_{-i}x_i}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} - \mathbb E \left[ \frac{x_i^T AQ^z_{-i}x_i}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} \right] \right)^2 \right]}O \left( \frac{1}{\sqrt n}  \right)\\
  &\hspace{0.5cm}\leq O \left( \frac{1}{\sqrt n}  \right)\left(\mathbb E \left[ \left(\frac{x_i^T \tilde Q^z_{\bar \Lambda^z}AQ^z_{-i}x_i - \mathbb E[x_i^T \tilde Q^z_{\bar \Lambda^z}AQ^z_{-i}x_i]}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} \right)^2 \right] \right.\\
  &\hspace{1.5cm}\left.+ \mathbb E \left[ \left(\tr\left(\Sigma_i \tilde Q^z_{\bar \Lambda^z}AQ^z_{-i}\right) \left( \frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} - \mathbb E \left[ \frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} \right] \right) \right)^2 \right]\right)^{1/2}\\
  % &\hspace{0.5cm}= \sqrt{\mathbb E \left[ \left( x_i^T\tilde Q^z_{\bar \Lambda^z}A Q^zx_i - \mathbb E \left[ x_i^T\tilde Q^z_{\bar \Lambda^z}A Q^zx_i \right] \right)^2 \right]\mathbb E \left[\left( \frac{1}{n }x_i^t Q_{-i}^z x_i - \mathbb E \left[ \frac{1}{n }x_i^t Q_{-i}^z x_i \right] \right)^2 \right]}\\
  &\hspace{0.5cm}\leq O \left( \frac{\left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert}{\sqrt n}  \right).
  \end{align*}
  indeed since we know that $|\frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} | \leq O(1)$ from Lemma~\ref{lem:Q_borne}, $\frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i}$ is a $O(1)$-Lipschitz transformation of $\frac{1}{n} x_i^TQ_{-i}x_i$, therefore, it follows the same concentration inequality (with a variance of order $O(1/ n)$).
% Now, let us note
%   \begin{align*}
%   x_i^T AQ^zx_i = \frac{\frac{1}{n}x_i^T AQ^z_{-i}x_i}{1 - \frac{1}{n} x_i^TQ_{-i}x_i},
% \end{align*}
% now, since:
% \begin{itemize}
%   \item $x_i^T AQ^z_{-i}x_i \in O(\sqrt n) \pm \mathcal E_2(\|A\|_F) + \mathcal E_1(\|A\|)$ 
%   \item $\frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} \in \in O(1) \pm \mathcal E_2 \left( \frac{1}{\sqrt n} \right)+ \mathcal E_1 \left( \frac{1}{n} \right)$
% \end{itemize}
  % We can then combine H\"older inequality with the characterization of the concentration with the centered moments of Proposition~\ref{pro:concentration_variable_caracterisation_moment} applied to Lemmas~\ref{lem:concentration xQx} and~\ref{lem:uQx} below to obtain:
  % \begin{align*}
  %   &\mathbb E \left[ \frac{u^T Q^zx_i x_i^T\tilde Q^z_{\bar \Lambda^z} u}{1 - \bar \Lambda^z_i } \left( \frac{1}{n }x_i^t Q_{-i}^z x_i - \bar \Lambda^z_i \right)\right]\\
  %   &\hspace{1cm}\leq O \left( \left(  \mathbb E \left[ (u^T Q^zx_i)^3   \right] \mathbb E \left[ (x_i^T\tilde Q^z_{\bar \Lambda^z} u)^3    \right] \mathbb E \left[ \left( \frac{1}{n }x_i^t Q_{-i}^z x_i - \bar \Lambda^z_i \right)^3 \right]\right)^{\frac{1}{3}} \right)\\
  %   &\hspace{1cm}\leq O \left( \frac{\left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert}{\sqrt n}  \right).
  % \end{align*}
  Since this inequality is true for any $A \in \mathcal M_p$, we can bound:
  \begin{align*}
    \left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert \leq \left\Vert \tilde Q^z_{\bar \Lambda^z} - \mathbb E[Q^z] \right\Vert_F + \left\Vert \mathbb E[Q^z] \right\Vert \leq O \left( \frac{\left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert}{\sqrt n} \right) + O(1),
  \end{align*}
  which directly implies that $\left\Vert \tilde Q^z_{\bar \Lambda^z} \right\Vert \leq O(1)$ and $\left\Vert \mathbb E[Q^z] -  \tilde Q^z_{\bar \Lambda^z} \right\Vert_F \leq O(1/\sqrt n)$.
\end{proof}
\begin{lemma}[\cite{LOU21RHL}, Lemmas 4., 8. ]\label{lem:Q_borne}
  $\forall z \in \bar{\mathcal S}^\varepsilon$, under $\mathcal A^\varepsilon$: 
  \begin{align*}
    \|Q^z \| \leq \frac{2}{\varepsilon}&
    &\text{and}&
    &\sup_{i\in [n]}|\frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} | \leq O(1)
    % $\|Q^z \| \leq \frac{2}{\varepsilon}$ and $\sup_{i\in [n]}|\frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} | \leq O(1)$
  \end{align*}
\end{lemma}
\begin{lemma}\label{lem:uQx}
  For any $z \in \mathbb C \setminus \bar{\mathcal S}^\varepsilon$, any $i \in [n]$ and any $u \in \mathbb R^p$ such that $\|u\|\leq 1$:
  \begin{align*}
    (u^T Q_{-i}^zx_i \ | \ \mathcal A_\varepsilon),
    (u^T Q^zx_i \ | \ \mathcal A_\varepsilon)
     \in O(1) \pm \mathcal E_2.
  \end{align*}
\end{lemma}
\begin{proof}
    We do not care about the independence issues brought by $\mathcal A_\varepsilon$. Let us simply bound for any $t>0$ and under $\mathcal A_\varepsilon$:
    \begin{align*}
      &\mathbb P \left( \left\vert u^T Q_{-i}^zx_i - \mathbb E \left[  u^T Q_{-i}^zx_i\right] \right\vert \geq t \right)\\
      &\hspace{1cm}\leq \mathbb P \left( \left\vert u^T Q_{-i}^z(x_i -\mu_i) \right\vert \geq \frac{t}{2} \right)+
      \mathbb P \left( \left\vert u^T  \left( Q_{-i}^z-\mathbb E \left[ Q_{-i}^z\right] \right)\mu_i \right\vert \geq \frac{t}{2} \right)\\
      &\hspace{1cm}\leq \mathbb E \left[ C e^{-cnt^2/\|Q_{-i}\|^2} \right] + C e^{-cnt^2} \ \
      \leq \ 2C e^{-c'nt^2},
    \end{align*}
    for some constants $C,c,c'>0$. Besides, we can bound:
    \begin{align*}
      \left\vert \mathbb E \left[  u^T Q_{-i}^zx_i\right] \right\vert = \left\vert u^T \mathbb E[Q_{-i}^z]\mu_i \right\vert \leq O(1),
    \end{align*}
    thanks to Lemma~\ref{lem:Q_borne} and Assumption~\ref{ass:borne_x_i}.

    The concentration of $u^T Q^zx_i$ is a consequence of the concentration $QX \in \mathcal E_2$ that can be shown thanks to Corollary~\ref{cor:concentration_serie_vecteur_lineairement_concentres} as in the proof of Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre}.
    We are then left to bounding $\mathbb E[u^T Q^zx_i]$. For this purpose, let us write:
    \begin{align*}
      \left\vert \mathbb E[u^T Q^zx_i]  \right\vert
      &= \left\vert \mathbb E[u^T Q_{-i}^zx_i] - \mathbb E \left[   (u^T Q_{-i}^zx_i) \left( \frac{1}{n} x_i^TQ^zx_i \right) \right] \right\vert \\
      &\leq O(1) + O \left( \sqrt{\mathbb E \left[   (u^T Q_{-i}^zx_i)^2 \right]\mathbb E \left[ \left( \frac{1}{n} x_i^TQ^zx_i \right)^2 \right]} \right) \leq O(1),
    \end{align*}
    thanks to Cauchy-Schwarz inequality Lemma~\ref{lem:Q_borne}, and the bound on $x_i$, valid under $\mathcal A_\varepsilon$.
    % Noting $\check Q^z \equiv (zI_n - \frac{1}{n}X^TX)^{-1}$, we know that $\frac{1}{n}X^TQX = \frac{1}{n}X^TXQ = z \check Q^z - I_n$
\end{proof}
\begin{lemma}\label{lem:Q_m_Q_m_i}
  Under $\mathcal A_\varepsilon$, for any $z \in \mathbb C \setminus \bar{\mathcal S}^\varepsilon$ and any $i \in [n]$:
  \begin{align*}
    \|\mathbb E[Q^z - Q^z_{-i}]\| \leq O \left( \frac{1}{n} \right).
  \end{align*}
\end{lemma}
\begin{proof}
  For any $u \in \mathbb R^p$, we can bound thanks to Lemma~\ref{lem:uQx}:
  \begin{align*}
    \left\vert u^T \mathbb E[Q^z - Q^z_{-i}]u \right\vert
    &\leq  \frac{1}{n}\left\vert  \mathbb E \left[ u^TQ^zx_ix_i^TQ^z_{-i}u \right] \right\vert \\
    &\leq  \frac{1}{n}\sqrt{\mathbb E \left[ (u^TQ^zx_i)^2\right]\mathbb E \left[ (x_i^TQ^z_{-i}u)^2 \right] } \ \ \leq O \left( \frac{1}{n} \right).
  \end{align*}
\end{proof}
% We start with a core lemma allowing us to control the size of $Q^z$.

% We will need to control two kind of random variables $\frac{1}{n}x_i^T Q_{-i}x_i$ and $u^T Q x_i$, for $u \in \mathbb R^p$, deterministic. Let us 
\begin{lemma}\label{lem:concentration xQx}
  For any $z \in  \mathbb C \setminus \bar{\mathcal S}^\varepsilon $deterministic matrix $A \in \mathcal{M}_{p}$:
  \begin{align*}
     (x_i^T AQ^z_{-i}x_i \ | \ \mathcal A_\varepsilon) \in \tr(\Sigma_i A\mathbb E[Q^z]) \pm\mathcal E_2 \left( \|A\|_F \right) + \mathcal E_1(\|A\|).
  \end{align*} 
  % and $(x_i^T AQ^zx_i \ | \ \mathcal A_\varepsilon) \in \mathcal E_2 \left( \|A\|_F \right) + \mathcal E_2(\|A\|)$
  % , where for any random vector $Z$, $\mathbb E_{\mathcal A_\varepsilon}[Z] = \mathbb E[Z \ | \ \mathcal A_\varepsilon]$
\end{lemma}

\begin{proof}
Once again, without referring to $\mathcal A_\varepsilon$, we assume that $\|X\|\leq O(1)$ and $\|Q^z\| \leq O(1)$. Given $i \in [n]$, since we know from Lemma~\ref{lem:Q_m_Q_m_i} that $\|\mathbb E[Q^z - Q_{-i}^z\| \leq O(1/\sqrt n)$, we want to bound:
\begin{align*}
  \left\vert x_i^T AQ^z_{-i}x_i - \tr \left( \Sigma_i A \mathbb E \left[ Q_{-i}\right] \right)\right\vert
  &\leq \left\vert x_i^T AQ^z_{-i}x_i -  \tr (\Sigma_iA Q^z_{-i})\right\vert + \left\vert  \tr \left(  \Sigma_iA (Q^z_{-i} - \mathbb E[Q^z_{-i}]) \right)\right\vert.
\end{align*}
Now we know that, for $X_{-i}$ fixed, we can bound thanks to Theorem~\ref{the:hanson_wright}:
 % (and Lemma~\ref{lem:Q_borne} $\|Q_{-i}^z\| \leq O(1)$):
% \begin{align*}
%   \left( \frac{1}{n}x_i^TQ_{-i}^zx_i, X_{-i} \right) 
%   % \left( \frac{1}{n}x_i^TQ_{-i}^zx_i \ | \ \mathcal A_\varepsilon, X_{-i} \right) 
%   \in \frac{1}{n}\tr \left( \Sigma_i Q_{-i}^z\right) \pm \mathcal E_2 \left( \frac{1}{\sqrt n} \right),
%   % \in \frac{1}{n}\tr \left( \mathbb E[x_ix_i^T\ | \ \mathcal A_\varepsilon] Q_{-i}^z\right) \pm \mathcal E_2 \left( \frac{1}{\sqrt n} \right),
% \end{align*}
% and since $\mathbb P(\mathcal A_\varepsilon^c) \leq C e^{-cn}$ for some constants $C,c>0$, we show easily that $\left\Vert \mathbb E[x_ix_i^T\ | \ \mathcal A_\varepsilon] - \mathbb E[x_ix_i^T] \right\Vert \leq O(1/\sqrt n) $. 
% We can then deduce from Lemma~\ref{lem:trou_noir_diametre_observable} that:
\begin{align*}
  \mathbb P \left( \left\vert x_i^T AQ^z_{-i}x_i -  \tr (\Sigma_i^T AQ^z_{-i})\right\vert \geq t  \right)
  &\leq \mathbb E \left[ Ce^{-c(t/\|Q_{-i}^z\| \|A\|_F)^2} + Ce^{-ct/\|Q_{-i}^z\| \|A\|} \right] \\
  &\leq  Ce^{-c't^2/\|A\|_F^2} + Ce^{-c't/\|A\|},
\end{align*}
for some constants $C,c,c'>0$, thanks to Lemma~\ref{lem:Q_borne}.

Besides, we know from Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} and Lemma~\ref{lem:trou_noir_diametre_observable} that $Q_{-i}^z \in \mathbb E[Q^z] \pm \mathcal E_2(1/\sqrt n)$ in $(\mathcal{M}_{p}, \|\cdot\|_*)$, which allows us to bound:
\begin{align*}
  \mathbb P \left( \left\vert \tr (\Sigma_i A Q^z_{-i}) - \tr (\Sigma_i A \mathbb E[Q^z])\right\vert \geq t  \right)
  % &\leq \mathbb E \left[ Ce^{-cnt^2/\|Q_{-i}^z\|} \right] 
  \leq  Ce^{-ct^2/\|A\|^2},
\end{align*}
for some constants $C,c>0$, since $\|\Sigma_i\|\leq O(1)$. Putting the two concentration inequalities together, we obtain the result of the lemma.

% For the concentration of $x_i^T AQ^zx_i$, let us note:
% \begin{align*}
%   x_i^T AQ^zx_i = \frac{\frac{1}{n}x_i^T AQ^z_{-i}x_i}{1 - \frac{1}{n} x_i^TQ_{-i}x_i},
% \end{align*}
% now, since:
% \begin{itemize}
%   \item $x_i^T AQ^z_{-i}x_i \in O(\sqrt n) \pm \mathcal E_2(\|A\|_F) + \mathcal E_1(\|A\|)$ 
%   \item $\frac{1}{1 - \frac{1}{n} x_i^TQ_{-i}x_i} \in \in O(1) \pm \mathcal E_2 \left( \frac{1}{\sqrt n} \right)+ \mathcal E_1 \left( \frac{1}{n} \right)$
% \end{itemize}
% then, we can bound:
% \begin{align*}
%   \left\vert x_i^T AQ^zx_i - \mathbb E[x_i^T AQ^zx_i] \right\vert
%   \leq \left\vert \frac{1}{n}x_i^TQx_i \left( x_i^T AQ^z_{-i}x_i - \mathbb E[x_i^T AQ^z_{-i}x_i] \right) \right\vert + \left\vert \frac{1}{n}x_i^TQx_i \left( x_i^T AQ^z_{-i}x_i - \mathbb E[x_i^T AQ^z_{-i}x_i] \right) \right\vert
% \end{align*}
%  Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} ($Q_{-i}^z$ globally behaves like $Q^z$).
%   % Noting $\check Q^z \equiv (zI_n - \frac{1}{n}X^TX)^{-1}$, we know that $\frac{1}{n}X^TQX = \frac{1}{n}X^TXQ = z \check Q^z - I_n$
\end{proof}

% The deterministic equivalent provided by Proposition~\ref{pro:first_deterministic_equivalent} relies on the quantity $\hat \Lambda^z$ that still needs to be estimated. 
 Theorem~\ref{the:concentration_resolvent_main} is then a consequence of the following proposition proven in \cite{LOU21RHL} (once Proposition~\ref{pro:first_deterministic_equivalent} is proven, the convex concentration particularities do not intervene anymore). Recall that $\tilde \Lambda^z \in \mathbb C^n$ is defined as the unique solution to the equation:
 \begin{align*}
   \forall i\in [n]: \tilde \Lambda^z_i = \frac{1}{n} \tr \left(\Sigma_i \tilde Q^z_{\tilde \Lambda^z} \right),
 \end{align*}
 where $\tilde Q^z_{\tilde \Lambda^z} \equiv \left( zI_p - \frac{1}{n}\sum_{i=1}^n \frac{\Sigma_i}{1 - \tilde \Lambda_i^z} \right)$.

\begin{proposition}\label{pro:deuxième équivalent déterministe}
  % The equation:
  % \begin{align*}
  %   \forall i\in [n]: \delta_i = \frac{1}{n} \tr \left(\Sigma_i \tilde Q^z_{\delta} \right) 
  % \end{align*}
  % admits a unique solution in $\mathbb R^n$ that we note $\tilde \Lambda^z$. 
  For all $z \in \mathbb C \setminus \bar{\mathcal S}_\varepsilon$:
  \begin{align*}
    \left\Vert \mathbb E[Q] - \tilde Q^z_{\tilde \Lambda^z} \right\Vert_F \leq O \left( \frac{1}{\sqrt n} \right).
  \end{align*}
\end{proposition}
% \begin{remark}\label{rem:concentration_finale_resolvante}
%   This last proposition combined with Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} and Lemma~\ref{lem:trou_noir_diametre_observable} just allows us to provide the concentration (for any $z \in \mathbb C \setminus \bar{\mathcal S}_\varepsilon$):
%   \begin{align*}
%     (Q^z \ | \ \mathcal A_\varepsilon) \in \tilde Q^z_{\tilde \Lambda^z} \pm \mathcal E_2(\sqrt n)&
%     &\text{in } \ (\mathcal{M}_{p}, \|\cdot \|_*).
%   \end{align*}
%   That still allows us to set the concentration of the Stieltjes transform $g(z) \equiv \frac{1}{p} \tr \left( Q^z \right)$:
%   \begin{align*}
%     (g(z)\ | \ \mathcal A_\varepsilon)  \in \frac{1}{p} \tr \left( \tilde Q^z_{\tilde \Lambda^z} \right) \pm \mathcal E_2 \left( \frac{1}{\sqrt n} \right).
%   \end{align*}
%   % and any projection of $Q^z$ on a deterministic vector $u \in \mathbb R^p$ satisfying $\|u\| \leq O(1)$:
%   % \begin{align*}
%   %   (u^TQ^zu\ | \ \mathcal A_\varepsilon)  \in u^T \tilde Q^z_{\tilde \Lambda^z} \right) \pm \mathcal E_2 \left( \frac{1}{\sqrt n} \right).
%   % \end{align*}
% \end{remark}
% \begin{lemma}\label{lem:concentration_lin_D_z_hypo_concentration_convexe}
%   Given $z \in \mathbb C$ such that $|z| \geq \rho$:
%   % Given $z \in \{|z| \geq \rho\}$:
%   \begin{align*}
%     D^z \in \Delta^z \pm \mathcal E_2 \left(\frac{1}{\sqrt n}\right) \ | \ e^{-n}&
%     &\text{in} \ \ (\mathcal M_{p}, \| \cdot \|),
%   \end{align*}
%    % $D^z \in \Delta^z \pm \mathcal E_2 \ | \ e^{-n}$ in $(\mathcal D_n(\mathbb C), \|\cdot \|_F)$.
% \end{lemma}
% \begin{proof}
%   Given $i \in[n]$, we can bound:
%   \begin{align*}
%     \left\vert D_i^z - \Delta_i^z\right\vert
%     \leq \left\vert \frac{1}{n} x_i^T Q^z_{-i}x_i - \frac{1}{n} \tr(\Sigma_i Q^z_{-i})\right\vert + \left\vert \frac{1}{n} \tr(\Sigma_i (Q^z_{-i}- \mathbb E[Q_{-i}^z])\right\vert
%   \end{align*}
%   and we know respectively from Corollary~\ref{cor:hanson_wright_convex_frobenius} ($\|Q_{-i}^z\|\leq O(1)$) and Proposition~\ref{pro:concentration_resolvent_concentration_convexe_z_hors_de_boule} ($\|\frac{1}{n}\Sigma_i \|\leq O(1/n)$) that:%{\color{orange} définir concentration autour variable aléatoire}
%   \begin{align*}
%     \frac{1}{n} x_i^T Q^z_{-i}x_i &\in \frac{1}{n} \tr(\Sigma_i Q^z_{-i}) \pm \mathcal E_2 \left(\frac{1}{\sqrt n}\right) \ | \ e^{-n};\\
%     \frac{1}{n} \tr(\Sigma_i Q^z_{-i}) &\in \frac{1}{n} \tr(\Sigma_i \mathbb E_{\mathcal A_Q}[Q^z_{-i}]) \pm \mathcal E_2 \left(\frac{1}{n}\right) \ | \ e^{-n},
%   \end{align*}
%   thus $D_i^z \in \Delta_i^z\pm \mathcal E_2 \left(\frac{1}{\sqrt n}\right) \ | \ e^{-n}$. We then deduce from Proposition~\ref{pro:concentration_concatenation_vecteurs_lineairement_concentres} the result of the lemma.
%   % \begin{align*}
%   %   D^z \in \Delta^z \pm \mathcal E_2 \left(\frac{1}{\sqrt n}\right) \ | \ e^{-n}&
%   %   &\text{in} \ \ (\mathcal M_{p}, \| \cdot \|),
%   % \end{align*}
%   % which implies the result of the Lemma since $\| A\|_F \leq \sqrt p \| A\|$ for all $A \in \mathcal M_{p}$.
%   % which means that $D^z \in \Delta^z \pm \mathcal E_2 \ | \ e^{-n}$ in $(\mathcal M_{p}, \| \cdot \|_F)$.
% \end{proof}
% \begin{proposition}\label{pro:equivalent_deterministe_Q_X_convexement_concentre}
%   Given $z \in \mathbb C$ satisfying $\rho \leq |z| \leq O(1)$:%\footnote{We just need $|z| -1 +\varepsilon \geq O(1)$, where $\varepsilon$ is introduced in Assumption~\ref{ass:XX_pas_trop_grand}}: 
%   % There exists a constant $\kappa \leq O(1)$ such that for any $z \in \mathbb C$ satisfying $d(z,[0,\bar \nu])\geq O(1)$ and $|z| \leq O(1)$:%\footnote{We just need $|z| -1 +\varepsilon \geq O(1)$, where $\varepsilon$ is introduced in Assumption~\ref{ass:XX_pas_trop_grand}}: 
%   \begin{align*}
%     Q^z \in \tilde Q^z(\chi(\Lambda^z)) \pm \mathcal E_2 \left(\sqrt{\frac{\log n}{n}}\right) \ | \ e^{- n}&
%     &\text{in}&
%     &(\mathcal M_{p}, \| \cdot \|),
%   \end{align*}
%   where $\Lambda^z$ is defined in Proposition~\ref{pro:definition_Lambda_recursive}.
% \end{proposition}
% \begin{proof}
%   We already know from Proposition~\ref{pro:concentration_resolvent_concentration_convexe_z_hors_de_boule} that $Q^z \in \mathcal E_2 \ | \ e^{- n}$ in $(\mathcal M_{p}, \| \cdot \|_*)$.%, as a solution to the (highly-probably) contractive equation $q = \frac{1}{z}I_p + \frac{1}{nz} XX^Tq$, $q\in \mathcal M_{p}(\mathbb C)$. 
%   With notations already introduced ($\Delta^z \equiv \diag_{1\leq i \leq n}(\frac{1}{n}\tr(\Sigma_i \mathbb E_{\mathcal A_Q}[Q^z])) $), we already know from Theorem~\ref{the:computable_deterministic_equivalent_Q} that:
%   $$\|\tilde Q^z(\chi(\Delta^z)) - \tilde Q^z(\chi(\Lambda^z))\| \leq \|\tilde Q^z(\chi(\Delta^z)) - \tilde Q^z(\chi(\Lambda^z))\|_F \leq O \left(\sqrt {\frac{\log n}{n}}\right)$$
%   % $$\|\tilde Q^z(\chi(\Delta^z)) - \tilde Q^z(\chi(\Lambda^z))\|_* \leq \sqrt{p}\|\tilde Q^z(\chi(\Delta^z)) - \tilde Q^z(\chi(\Lambda^z))\|_F \leq O(\sqrt {\log n})$$
%   (the deterministic matrices $\tilde Q^z(\chi(\Delta^z))$ and $\tilde Q^z(\chi(\Lambda^z))$ have not changed).
%   It is therefore sufficient to show that $\mathbb E_{\mathcal A_Q}[Q^z]$ is close to $\tilde Q^z(\chi(\Delta^z))$.
%   % The deterministic matrix $\tilde Q^z(\chi(\Delta^z))$ is the same as in the first study of the resolvent, we can therefore employ again the bound given by Proposition~\ref{pro:borne_EQ_m_tQ} : $\|\tilde Q^z(\chi(\Delta^z))\| \leq O(1)$.
%   We follow the steps of the proof of Proposition~\ref{pro:borne_EQ_m_tQ}, starting from $\|\mathbb E_{\mathcal A_Q}[Q^z]-\tilde Q^z(\chi(\Delta^z))\| \leq \sup_{\|u\|,\|v\|\leq 1} u^T(\varepsilon_1+\varepsilon_2)v$ (where $\varepsilon_1,\varepsilon_2$ are defined in \eqref{eq:definition_epsilon_1_2}). First, given two deterministic vectors $u,v \in \mathbb R^p$ such that $\|u\|=\|v\| =1$, we bound thanks to Lemma~\ref{lem:concentration_lin_D_z_hypo_concentration_convexe} and the bound $\|\tilde Q^z(\chi(\Delta^z))\|\leq O(1)$ given by Proposition~\ref{pro:borne_EQ_m_tQ}:
%   % We follow the steps of the proof of Proposition~\ref{pro:borne_EQ_m_tQ}, starting from $\|\mathbb E_{\mathcal A_Q}[Q^z]-\tilde Q^z(\chi(\Delta^z))\| \leq \sup_{\|u\|,\|v\|\leq 1} u^T(\varepsilon_1+\varepsilon_2)v$. First, given two deterministic vectors $u,v \in \mathbb R^p$ such that $\|u\|=\|v\| =1$, we bound thanks to Proposition~\ref{pro:equivalent_deterministe_XDX}\footnote{We do not exactly employ Proposition~\ref{pro:equivalent_deterministe_XDX} since we can just deduce from Corollary~\ref{cor:concentration_resolvante_concentration_convexe} that $Q^zX \in \mathcal E_2 \left(\sqrt n \right) \ | \ e^{- n}$ in $(\mathcal M_{p}, \| \cdot \|_*)$, and we would need an observable diameter of order $O(1)$. However, we can adapt the proof thanks to the bound $\|Q^zX\| \leq O(\|X\|)$  }, Lemma~\ref{lem:concentration_lin_D_z_hypo_concentration_convexe} and the bound $\|\tilde Q^z(\chi(\Delta^z))\|\leq O(1)$:
%   \begin{align*}
%     u^T\varepsilon_1v 
%     % &=\frac{1}{n} \sum_{i=1}^n \left\vert \mathbb E_{\mathcal A_Q} \left[ \chi(\Delta^z_i)\left(D_i^z - \Delta_i^z\right)x_i^T\tilde{Q}^z(\Delta^z)A Q^zx_i  \right]\right\vert\\
%     &=\frac{1}{n} \left\vert \mathbb E_{\mathcal A_Q} \left[u^TQ^zX \chi(\Delta^z)\left(D^z - \Delta^z\right)X^T \tilde{Q}^z(\Delta^z)v \right]\right\vert\\
%     &\leq  \frac{1}{n}  \mathbb E_{\mathcal A_Q} \left[\|\tilde{Q}^z(\Delta^z)\|\|Q^z\|\|X\|^2 \|\chi(\Delta^z)\| \|D^z - \Delta^z\| \right] \leq O \left(\sqrt{\frac{\log n}{n}}\right)
%     % &\leq \frac{1}{n} \mathbb E_{\mathcal A_Q} \left[\|\tilde Q^z(\chi(\Delta^z)) A\|_F \left\Vert \chi(\Delta^z)\right\Vert \|Y \| \| Z\|_\infty \|D^z- \Delta^z\|_F\right]\\
%    % \leq O \left(\sqrt{\frac{\log n}{ n}}\right)
%    % &\leq O \left(\|A\|_F\sqrt{\frac{\log n}{ n}}\right)
%    % \leq O \left(\sqrt{\log n}\right)
%   \end{align*}
%   Second, we can bound again thanks to Cauchy-Schwartz inequality:
%   \begin{align*}
%      \left\vert \mathbb E_{\mathcal A_Q}[u^T\varepsilon_2v ]\right\vert
%      &\leq \frac{1}{n} \sqrt{\mathbb{E}_{\mathcal A_Q}\left[\frac{1}{n}u^TQ^zX\chi(\Delta^z)X^TQ^zu\right]}&\\
%      &\hspace{0.5cm} \cdot  \sqrt{\frac{1}{n}\sum_{i=1}^n \mathbb{E}_{\mathcal A_Q}\left[v^T\tilde{Q}^z(\chi(\Delta^z))Q_{-i}^zQ_{-i}^z\Sigma_i\tilde{Q}^z(\chi(\Delta^z))v\right]}& \hspace{0.1cm}\leq O \left(\frac{1}{n}\right)
%     % &=\sqrt{\frac{1}{n^2} \mathbb{E}_{\mathcal A_Q}\left[\tr(AQ^zX\chi(\Delta^z)^2 X^TQ^zA^T \right]\frac{1}{n^2}\sum_{i=1}^n \mathbb{E}_{\mathcal A_Q}\left[\tr(\tilde{Q}^z(\Delta^z)\Sigma_ iQ_{-i}^z\Sigma_iQ_{-i}^z\Sigma_i\tilde{Q}^z(\chi(\Delta^z))\right]} \\
%     % &&\leq O \left(\frac{1}{n}\right)
%   \end{align*}
%   Putting our different bounds together we obtain the result of the proposition.
%   % We can thus conclude that $Q^z \in \tilde Q^z(\chi(\Delta^z)) \pm \mathcal E_2 \left(\log(n)\right) \ | \ e^{- n}$ in $(\mathcal M_{p}, \| \cdot \|_*)$ from which we deduce our result.%easily that:
%   % \begin{align*}
%   %   \|\tilde Q^z(\chi(\Delta^z))-\tilde Q^z(\chi(\Lambda^z))\|_*
%   %   &\leq \sup_{\|A\|\leq 1} \tr \left(A \left(\tilde Q^z(\chi(\Delta^z))-\tilde Q^z(\chi(\Lambda^z))\right)\right)\\
%   %   &\leq \sup_{\|A\|\leq 1} \frac{1}{n}\sum_{i = 1}^n\chi(\Delta^z_i)\chi(\Lambda^z_i)(\Delta^z_i -\Lambda^z_i)\tr \left(A \tilde Q^z(\chi(\Delta^z))\Sigma_i\tilde Q^z(\chi(\Lambda^z))\right)\\
%   %   &\leq O(\log n),
%   % \end{align*}
%   % since $\|\chi(\Delta^z)\|,\|\chi(\Lambda^z)\|, \| \tilde Q^z(\chi(\Delta^z))\|\|\tilde Q^z(\chi(\Lambda^z))\|, \sup_{1\leq i\leq n}\|\Sigma_i\| \leq O(1)$ and $\|\Delta^z -\Lambda^z\| \leq O(\sqrt{\log n}/n)$ thanks to Proposition~\ref{pro:Delta_tilde_proche_de_delta}. We can then conclude on the concentration of $Q^z$ around $\tilde Q^z(\chi(\Lambda^z))$.
% \end{proof}
% To end our study we now explain how thinks can be controlled when $|z| \leq \rho$ (and $d(z,[0, \bar \nu] \geq O(1)$). In that case we just know that $\tr(Q^z)$ is concentrated (around its expectation), we are just left to show that $\tr(\mathbb E_{\mathcal A_Q}[Q^z])$ is close to $\tr(\tilde Q^z(\chi(\Delta^z)))$.
% \begin{proposition}\label{pro:equivalent_deterministe_tr_Q}
%   Given $z \in \mathbb C$ such that $d(z, \bar\nu) \geq O(1)$ and $|z| \leq O(1)$, we have the concentration:
%   \begin{align*}
%     \frac{1}{p}\tr(Q^z) \in \frac{1}{p} \tr(\tilde Q^z(\chi(\Lambda^z))) \pm \mathcal E_2 \left(\sqrt{\frac{\log n}{p}}\right) \ | \ e^{- n}
%   \end{align*}
% \end{proposition}
% \begin{proof}
%   We already know from Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre} that:
%   \begin{align*}
%     \frac{1}{p}\tr(Q^z) \in \frac{1}{p}\tr(\mathbb E_{\mathcal A_Q}[Q^z]) \pm \mathcal E_2 \left(\sqrt{\frac{\log n}{p}}\right) \ | \ e^{- n},
%   \end{align*}
%   we are thus left to show that $\|\frac{1}{p}\tr \left(\mathbb E_{\mathcal A_Q}[Q^z] -\tilde Q^z(\chi(\Lambda^z))\right)| \leq O (\sqrt{\log n /p})$. The two mappings $z \mapsto \frac{1}{p}\tr(\mathbb E_{\mathcal A_Q}[Q^z])$ and $z \mapsto \frac{1}{p} \tr(\tilde Q^z(\chi(\Lambda^z))) $ being both analytical we can use the fact that they are close on $\{|z|\geq \rho\}$, partly thanks to Proposition~\ref{pro:equivalent_deterministe_Q_X_convexement_concentre}.
% \end{proof}

\appendix
\section{Proofs of the concentration of products of convexly concentrated random vectors and of convexly concentrated random matrices}\label{app:concentration_produit_vecteurs}
We will use several time the following elementary result:
\begin{lemma}\label{lem:sum_f_convexe_convexe}
  Given a convex mapping $f : \mathbb R \to \mathbb R$, and a vector $a \in \mathbb R_+^p$, the mapping $F : \mathbb R^p \ni (z_1,\ldots,z_p) \mapsto \sum_{i=1}^p a_if(z_i) \in \mathbb R$ is convex (so in particular quasi-convex).
\end{lemma}
% We need a supplementary result t
To efficiently manage the concentration rate when multiplying a large number of random vectors, we will also need:
\begin{lemma}\label{lem:decomposition_poly_symetrique}
  Given $m$ commutative or non commutative variables $a_1,\ldots,a_m$ of a given algebra, we have the identity:
  \begin{align*}
     \sum_{\sigma \in \mathfrak S_m} a_{\sigma(1)}\cdots a_{\sigma(m)} = (-1)^m\sum_{I\subset [m]} (-1)^{|I|} \left(\sum_{i\in I} a_i\right)^m,
  \end{align*}
  where $|I|$ is the cardinality of $I$.
\end{lemma}
\begin{proof}
  The idea is to inverse the identity:
  \begin{align*}
    (a_1+\cdots+a_m)^m = \sum_{J\subset I} \sum_{ \{i_1,\ldots,i_m\} =  J}a_{i_1} \cdots a_{i_m},
  \end{align*}
  thanks to the Rota formula (see \cite{Rol06}) that sets for any mappings $f, g$ defined on the set subsets of $\mathbb N$ and having values in a commutative group (for the sum):
  \begin{align*}
    \forall I \subset \mathbb N, f(I) = \sum_{J\subset I} g(J)&
    &\Longleftrightarrow&
    & \forall I \subset \mathbb N, g(I) = \sum_{J\subset I}\mu_{\mathcal P(\mathbb N)}(J,I) f(J),
  \end{align*}
  where $\mu_{\mathcal P(\mathbb N)}(J,I)  =(-1)^{|I\setminus J|}$ is an analog of the Mo\"ebus function for the order relation induced by the inclusions in $\mathcal P(\mathbb N)$.
  In our case, for any $J \subset [m]$, if we set:
  \begin{align*}
    f(J) = \left(\sum_{i\in J} a_i \right)^m&
    &\text{and}&
    &g(J) = \sum_{ \{i_1,\ldots,i_m\} =  J}a_{i_1} \cdots a_{i_m},
  \end{align*}
  we see that for any $I \subset [m]$, $f(I) = \sum_{J\subset I} g(J)$, therefore taking the Rota formula in the case $I = [m]$, we obtain the result of the Lemma (in that case, $\mu_{\mathcal P(\mathbb N)}(J,I)= (-1)^{m-|J|}$ and $\sum_{ \{i_1,\ldots,i_m\} =  I}a_{i_1} \cdots a_{i_m} = \sum_{\sigma \in \mathfrak S_m} a_{\sigma(1)}\cdots a_{\sigma(m)}$).
  % The idea is to develop the null expression $e_1 = \left(t_1-t_1 + \cdots +t_m - t_m\right)^m$ and regroup in the obtained sum $m-1$ terms $p_0,\ldots, p_m$, each $p_k$ being issued from a product of $k$ $-t_i$ for $i\in [m]$ that gives us:
  % \begin{align*}
  %   p_0 &= t_1^m + \cdot +t_m^m\\
  %   p_1 &= - 
  %   t_1^m + \cdot +t_m^m\\
  %   &= \left(t_1-t_1 + \cdots +t_m - t_m\right)^m\\
  % \end{align*}
\end{proof}


\begin{proof}[Proof of Theorem~\ref{the:concentration_convexe_produit_odot_Rp}]
  Let us first assume that all the $X_i$ are equal to a vector $Z \in \mathbb R^p$. 
  Considering $a = (a_1,\ldots, a_p) \in \mathbb R^p$, we want to show the concentration of $a^T Z^{\odot m} = \sum_{i=1}^p a_i z_i^m$ where $z_1,\ldots,z_p$ are the entries of $Z$. 
  
  The mapping $p_m : x \mapsto x^m $ is not quasi-convex when $m$ is odd, therefore, in that case we decompose it into the difference of two convex mappings $p_m(z) = p_m^+(z) - p_m^-(z)$ where:
  \begin{align}\label{eq:decomposition_convexe_puissance}
    p_m^+: z \mapsto \max(z^m,0)&
    &\text{and}&
    &p^-_m: z \mapsto -\min(z^m,0),
   \end{align} 
  (say that, if $m$ is even, then we set $p_m^+ = p_m$ and $p_m^- : z \mapsto 0$).
  For the same reasons, we decompose $\phi^+_a : z \mapsto a^T p_m^+(z)$ and $\phi^-_a : z \mapsto a^T p_m^-(z)$ into:  
  \begin{align*}
     \phi^+_a = \phi^+_{| a|} - \phi^+_{|a|- a}&
     &\text{and}&
     &\phi^-_a = \phi^-_{| a|} - \phi^-_{|a|- a}
   \end{align*}
   (for $|a| = (|a_i|)_{1\leq i \leq p}$), so that:
  \begin{align*}
    a^T Z^{\odot m} = \phi^+_{| a|}(Z) - \phi^+_{|a|- a}(Z) - \phi^-_{| a|}(Z) + \phi^-_{|a|- a}(Z)
  \end{align*}
  becomes a combination of quasi-convex functionals of $Z$. We now need to measure their Lipschitz parameter. Let us bound for any $z \in \mathbb R^p$:
  \begin{align*}
    \left\vert \phi^+_{| a|}(z)\right\vert = \sum_{i=1}^n |a_i| |z_i|^m \leq \|a\|\|z\| \|z\|_\infty^{m-1},
  \end{align*}
  and the same holds for $\phi^+_{|a|- a}$, $\phi^-_{| a|}$ and $\phi^-_{|a|- a}$. 
  % Therefore, we can conclude a similar result to Theorem~\ref{the:Concentration_produit_de_vecteurs_d_algebre_optimise} with $\mu = \mathbb E[\|Z\|_\infty] \leq O(\sigma(\log p)^{1/q})$.
  % To show the second concentration result (still when $X_1=\cdots = X_m$), we just have to n
  Note then that $\phi^+_{|a|}$, $\phi^+_{|a|- a}$, $\phi^-_{| a|}$ and $\phi^-_{|a|- a}$ are all $\|a\| \kappa^{m-1}$-Lipschitz to conclude on the concentration of $X^{\odot m}$.
 %  They can all be seen as 
  % There exists a constant $\kappa \leq O(1)$ such that $\phi_a : z \mapsto a^T z^{\odot m}$ is $\|a\|(\kappa \sigma (\log p)^{1/q})^{m-1}$-Lipschitz on the ball $\mathcal B_\kappa \equiv \{v \in \mathbb R^p \ | \ \|v \|_\infty \leq \kappa \sigma (\log p)^{1/q}\}$ and such that $\mathbb P ( Z \notin \mathcal B_\kappa) \leq C e^{-\log p/c}$ for some constants $C,c \leq O(1)$. 
  % Unfortunately $\phi_a$ is not quasi-convex and we cannot directly employ the convex concentration hypothesis on $Z$; instead, we just decompose $\phi_a = \phi_{| a|} - \phi_{|a|- a}$ (for $|a| = (a_i|)_{1\leq i \leq p}$) and the mapping $p_m : x \mapsto x^m $ into the difference of two convex mappings:
  % \begin{align}\label{eq:decomposition_convexe_puissance}
  %   p_m(z) = \max(z^m,0) - (-\min(z^m,0)).
  %  \end{align}
  % Then Lemma~\ref{lem:sum_f_convexe_convexe} ensures that $\phi_a$ is the difference of convex mappings and, with the same argument (provided in the case of convex concentration by Lemma~\ref{lem:concentration_conditionnee_convexe}) found in the proof of Proposition~\ref{pro:concentration_intelligente_produit}, one can state that
  % $$a^T Z^{\odot m} = \phi_{|a|}(Z) - \phi_{|a| - a}(Z) \overset{\mathcal A_Z, \mathcal B_\kappa}\in \mathcal E_2 \left(\|a\|(\kappa\sigma)^{m} (\log p)^{(m-1)/q} \right) \ | \ e^{-\log p},$$
  % where $\mathcal A_Z$ is the concentration zone of $Z$.

  % Now staying in the second setting ($\mathbb P(\mathcal A_\kappa^c) \leq Ce^{-cp}$), 
  Now, if we assume that the $X_1,\ldots, X_m$ are different, we employ Lemma~\ref{lem:decomposition_poly_symetrique} in this commutative case to write ($|\mathfrak S_m | = m!$):
  \begin{align}\label{eq:decomposition_x_1_odot_x_m}
     (X_1\odot \cdots \odot X_m) = \frac{(-1)^m}{m!} \sum_{I\subset [m]} (-1)^{|I|} \left(\sum_{i\in I} X_i\right)^{\odot m}.
   \end{align} 
  % Let us introduce the event:
  % \begin{align*}
  %   \mathcal A_\kappa = \left\{\forall i \in [m] \ | \ X_i \in  \mathcal B_\kappa\right\} \cap  \mathcal A_{X_1} \cap \cdots \cap \mathcal A_{X_m}.
  % \end{align*}
  % Since $\log m \leq O(\log p)$, we can bound:
  % \begin{align*}
  %   % \mathbb P \left(\exists i \in [m] \ | \ X_i \notin  \mathcal B_\kappa \ \text{or} \ \mathcal A_{X_1}^c \ \text{or} \ldots \text{or} \ \mathcal A_{X_m}\right)
  %   \mathbb P(\mathcal A_\kappa^c)
  %   &\leq me^{-\log p/c} + \sum_{i=1}^m\mathbb P \left( X_i \in  \cap \mathcal B_\kappa\right) 
  %   \leq e^{(\log m - \log p)/c}
  %   \leq e^{\log p/c'}
  % \end{align*}
  % for some constants $c,c' \leq O(1)$. 
  Therefore, the sum $(\mathbb R^p)^I \ni z_1,\ldots, z_{i_{|I|}}  \mapsto \sum_{i\in I} z_i \in \mathbb R^p$ being $m$-Lipschitz for the norm $\|\cdot \|_\infty$, we know that $\forall I \subset [m]$, $\sum_{i\in I} X_i \propto_c \mathcal E_2(m\sigma) $, and $\|\sum_{i\in I} X_i\|_\infty \leq \kappa m$, therefore, $(\sum_{i\in I} X_i)^{\odot m} \in \mathcal E_2(m^m\kappa^{m-1} \sigma )$.
  %(the term $e^{\log(m)-\log p}$ come from the vact that we have to bound the probability of the event $\{\exists i \in I \ | \ X_i \in \mathcal A_{X_i} \cap \mathcal B_\kappa\} $).
  We can then exploit Proposition~\ref{pro:concentration_concatenation_vecteurs_lineairement_concentres} to obtain
  \begin{align*}
    \left(\left(\sum_{i\in I} X_i\right)^{\odot m}\right)_{I \subset [m]} \in \mathcal E_2(m^m\kappa^{m-1} \sigma ) &
    % \overset{\mathcal A_X, \mathcal A_\kappa}
    % \left(\left(\sum_{i\in I} X_i\right)^{\odot m}\right)_{I \subset [m]} \overset{\mathcal A_X, \mathcal A_\kappa}\in \mathcal E_2 \left((\kappa m\sigma)^{m} (\log p)^{(m-1)/q} \right) \ | \ e^{\log p}&
    &\text{in } \ \left((\mathbb R^p)^{2^m}, \| \cdot \|_{\ell^\infty}\right),
  \end{align*}
  (note that $\#\{I \subset [m]\} = 2^m$)
  % where we recall that $2^m$ is the number of subsets of $[m]$. 
  % (the concentration zone of $((\sum_{i\in I} X_i)^{\odot m})_{I \subset [m]}$ is $\mathcal A_\kappa$).
  Thus summing the $2^m$ concentration inequalities, we can conclude from Equation~\eqref{eq:decomposition_x_1_odot_x_m}, and the Stirling formula $\frac{m^m}{m!} = \frac{e^m}{\sqrt{2\pi m}} +O(1)$ that:
  \begin{align*}
     (X_1\odot \cdots \odot X_m) \in \mathcal E_2 \left((2e\kappa)^{m-1} \sigma\right).
  \end{align*}



% A similar result to Theorem~\ref{the:concentration_convexe_produit_odot_Rp} holds for matrix product but to be set, one needs first to 
For the concentration of the matrix product, we introduce a new notion of concentration, namely the \textit{transversal convex concentration}. Let us give some definitions.
\begin{definition}\label{def:concentrtaion_transversale}
  
  Given a sequence of normed vector spaces $(E_n, \Vert \cdot \Vert_n)_{n\geq 0}$, a sequence of groups $(G_n)_{n\geq 0}$, each $G_n$ (for $n \in \mathbb N$) acting on $E_n$, a sequence of random vectors $(Z_n)_{n\geq 0} \in \prod_{n\geq 0} E_n$, a sequence of positive reals $(\sigma_n)_{n\geq 0} \in \mathbb R_+ ^{\mathbb N}$, we say that $Z =(Z_n)_{n\geq 0}$ 
  is convexly concentrated transversally to the action of $G$ with an observable diameter of order $\sigma$ and we note $Z \propto^T_G \mathcal E_2(\sigma)$ iff there exist two constants $C,c \leq O(1)$ such that $\forall n \in \mathbb N$ and for any $1$-Lipschitz, quasi-convex and $G$-invariant\footnote{For any $g \in G$ and $x \in E$, $f(x) = f(g \cdot x)$} function $f : E_n \to \mathbb R$,
  $\forall t>0:$\footnote{Once again, we point out that one could have replaced here $\mathbb E[f(Z_n)]$ by $f(Z_n')$ or $m_f$.} 
     \begin{center}
      $\mathbb P \left(\left\vert f(Z_n) - \mathbb E[f(Z_n)]\right\vert\geq t\right) \leq C e^{-c(t/\sigma_n)^2}$.
     \end{center}
\end{definition}
\begin{remark}\label{rem:concentration_convexe_implique_concentration_transversale}
  Given a normed vector space $(E,\|\cdot \|)$, a group $G$ acting on $E$ and a random vector $Z \in E$, we have the implication chain:
  \begin{align*}
    Z \propto \mathcal E_2(\sigma)&
    &\Longrightarrow&
    &Z \propto_c \mathcal E_2(\sigma)&
    &\Longrightarrow&
    &Z \propto_G^T \mathcal E_2(\sigma).
  \end{align*}
\end{remark}
 
Considering the actions:
\begin{itemize}
  \item $\mathfrak S_n$ on $\mathbb R^p$ where for $\sigma \in \mathfrak S_n$ and $x \in \mathbb R^p$, $\sigma \cdot x = (x_{\sigma(i)})_{1\leq i \leq p}$,
  \item $\mathcal O_{p,n} \equiv \mathcal O_{p} \times \mathcal O_{n}$ on $\mathcal M_{p,n}$ where for $(P,Q) \in \mathcal O_{p,n}$ and $M \in \mathcal M_{p,n}$, $(P,Q) \cdot M = PMQ$,
\end{itemize}
the convex concentration in $\mathcal M_{p,n}$ transversally to $\mathcal O_{p,n}$ can be expressed as a concentration on $\mathbb R^p$ transversally to $\mathfrak S_n$ thanks to the introduction the mapping $\sigma$ providing to any matrix the ordered sequence of its singular values~:
\begin{align*}
  \sigma \ : \
  \begin{aligned}[t]
    &\mathcal{M}_{p,n} &&\rightarrow && \hspace{1.4cm}\mathbb{R}^d_+ \\
    &M&&\mapsto&&(\sigma_1(M),\ldots, \sigma_d(M)).
  \end{aligned}&
  &\text{with } \ d = \min(p,n)
\end{align*}
(there exists $(P,Q) \in \mathcal O_{p,n}$ such that $M = P \Sigma(M) Q$, where $\Sigma \in \mathcal M_{p,n}$ has $\sigma_1(M)\geq \cdots \geq\sigma_d(M)$ on the diagonal).
\begin{theorem}[\cite{LED05}, Corollary 8.23. \cite{LOU19}, Theorem 2.44]\label{the:concentrtaion_transversale_matrice_vecteurs}
  Given a random matrix $Z \in \mathcal M_{p,n}$:
  \begin{align*}
    Z \propto^T_{\mathcal O_{p,n}} \mathcal E_2(\sigma)&
    &\Longleftrightarrow&
    &\sigma(Z) \propto^T_{\mathfrak S_d} \mathcal E_2(\sigma)
  \end{align*}
  (where the concentrations inequalities are implicitly expressed for euclidean norms: $\|\cdot\|_F$ on $\mathcal M_{p,n}$ and $\|\cdot\|$ on $\mathbb R^d$).
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{the:concentration_lineaire_produit_matrice_convexement_concentres}]
  % As for the proof of Theorem~\ref{the:concentration_convexe_produit_odot_Rp}, we consider in a first time that all the 
  % Let us first consider the case where $X$ is a square matrix of $\mathcal M_{p}$.
  % To obtain the linear concentration of $X^m$, what we would like to show is the concentration of any $\tr(AX^m)$, for a given matrix $A \in \mathcal M_{p}$ such that $\|A\|_F\leq 1$
  % Let us note $P\sigma(X)Q$, its singular decomposition with $P, Q \in \mathcal O_{p}$ and $\sigma(X) \in \mathcal D_{p}$.
  Let us start to study the case where $ X_1 = \cdots = X_m \equiv X \in \mathcal{M}_{n}$ and $X \propto \mathcal E_2$ in $(\mathcal{M}_{n}, \|\cdot \|_F)$. 
  % We know that $(X,\ldots, X) \propto \mathcal E_2(\sigma)$ in $((\mathcal{M}_{n})^m, \|\cdot \|_{F, \ell^\infty})$ with, for any $M= (M_1,\ldots,M_m) \in (\mathbb S_p^+)^m$, $\|M\|_{\ell^\infty} = \sup_{1\leq i\leq m} \|M_i\|$, as it is a $1$-Lipschitz transformation\footnote{It is not the case if we endow $(\mathcal{M}_{n})^m$ with the norm $\|\cdot \|_{F} : M_1,\ldots,M_p \mapsto \sqrt{\|M_1\|_F^2 + \cdots + \|M_m\|_F^2}$} of $X$.
  % Theorem~\ref{the:concentrtaion_transversale_matrice_vecteurs} gives us the concentration:
  We know from Theorem~\ref{the:concentrtaion_transversale_matrice_vecteurs} that:
  \begin{align*}
    \sigma(X) \propto_{\mathfrak S_p}^T \mathcal E_2,
  \end{align*}
  and therefore, as a $\sqrt{n}$-Lipschitz linear observation of $\sigma(X)^{\odot m} \in \mathcal E_2 \left( \kappa^{m-1} \sigma\right)$ (see Theorem~\ref{the:concentration_convexe_produit_odot_Rp}), $\tr(X^m)$ follows the concentration:
  % and therefore since $\tr(X^m) = \sum_{i=1}^p \sigma_i(X)^m$ which can be written thanks to \eqref{eq:decomposition_convexe_puissance} as a combination of four quasi-convex, $\mathfrak S_p$-invariant observation of $\sigma(X)$. Those four observations are further more $\sqrt{p}\kappa^{m-1}$-Lipschitz on $\mathcal A_\kappa$, thus:
\begin{align*}
    \tr(X^m) = \sum_{i=1}^p \sigma_i(X)^m \in \mathcal E_2 \left( \sqrt n \kappa^{m-1} \sigma\right) .
\end{align*}
 %  Besides, considering a matrix $A \in \mathcal M_{p} $, such that $\|A\| \leq 1$:
 %  \begin{align*}
 %    \left\vert \tr(A X^m) - \tr(X^m)\right\vert \leq \sqrt p \|A- I_n\| \|X\|^m \leq \kappa^m
 %  \end{align*}
 %  any ball $\mathcal B_\kappa \equiv \{\|X\|\leq \kappa\sigma p^{1/q}\}$ for $\kappa>0$ a constant (since $\|\sigma(X)\|_{\infty} = \|X\|$. Choosing $\kappa$ such that there exist two constants $C,c >0$ satisfying $\mathbb P(\mathcal B_\kappa^c) \leq C e^{-c p}$, we see thanks to Lemma~\ref{lem:concentration_conditionnee_convexe} that:
 %  Adapting the result of Lemma~\ref{lem:concentration_(X,Y)_variation_X_borne} to the convex concentration case and to the random vector $a_{P,Q} = ([QAP]_{i,i})_{1\leq i\leq p} \in \mathbb R^p$, since $\|a_{P,Q}\| \leq 1$, we can show that:
 %  \begin{align*}
 %    (a_{P,Q}, \sigma(X)) \propto_c  \mathcal E_2\ | \ e^{-p}&
 %    &\text{in} \ \ ((\mathbb R^p)^2, \|\cdot\|_{\ell^\infty}),
 %  \end{align*}
 %  where we identify $\sigma(X) \in \mathcal D_n$ with its diagonal projection on $\mathbb R^p$. Actually such a concentration of $(a_{P,Q}, \sigma(X))$ is not need, it is indeed sufficient to follow the steps of the proofs of Lemma~\ref{lem:concentration_(X,Y)_variation_X_borne} and Theorem~\ref{the:concentration_convexe_produit_odot_Rp}, and decompose:
 %  \begin{align*}
 %    \left\vert \tr(AX^m) - \mathbb E \left[ \un^T \sigma(X)^{\odot m}\right]\right\vert\\
 %     = \un^T (a_{P,Q} \odot \sigma(X)^{\odot m}) = \frac{(-1)^m}{m!} \sum_{k=0}^{m-1} \binom{m-1}{k}(-1)^{k}\left(k^m \tr(X^m)- \tr\left(\left(A + kX\right)^m\right) \right)
 %  \end{align*}
  % \begin{align*}
 %    \tr(X^m) = \sum_{i=1}^p \sigma_i(X)^m \overset{\mathcal B_\kappa \cap \mathcal A_X}\in \mathcal E_2 \left( \sqrt p \kappa^{m} \right) \ | \ e^{-p}.
  %   % \tr(X^m) = \sum_{i=1}^p \sigma_i(X)^m \overset{\mathcal B_\kappa \cap \mathcal A_X}\in \mathcal E_2 \left( \sqrt p\left(\kappa \sigma\right)^{m} p^{(m-1)/q}\right) \ | \ e^{-p}.
  % \end{align*}
  % However, to obtain the linear concentration of $X^m$, what we would like to show is the concentration of any $\tr(AX^m)$, for a deterministic matrix $A \in \mathcal M_{p}$ such that $\|A\|\leq 1$.
  %(given $M \in \mathcal M_p$, we can decompose $A=A_s + A_a$ with $A_a \in \mathcal S_p$ and $A_a$ antysymmetric, then $\tr(AX^m) = \tr(A_sX^m)$). 
  % We know from Lemma~\ref{lem:decomposition_poly_symetrique} applied on the matrices $(A,X,\ldots,X) \in (\mathcal M_p )^m$ that:
  % \begin{align*}
  %   \tr( A X^m) = \frac{(-1)^{m+1}}{\kappa (m+1)!} \sum_{k=0}^{m} \binom{m}{k}(-1)^{k}\left(\tr((kX)^{m+1})- \tr\left(\left(\kappa A + kX\right)^{m+1}\right) \right).
  % \end{align*}
  % We can control for all $k \in [m]$:
  % \begin{itemize}
  %   \item $kX\propto \mathcal E_2(m\sigma)$ and $\| kX\| \leq m\kappa$
  %   \item $\kappa A + kX \propto \mathcal E_2(m\sigma)$ and $\|\kappa A + kX\| \leq (m+1)\kappa$
  % \end{itemize}
  % Summing the concentrations obtain from the first part of the proof, we get:
  % \begin{align*}
  %   \tr((kX)^{m+1})- \tr\left(\left(\kappa A + kX\right)^{m+1}\right)
  %   \in \mathcal E_2 \left( \sqrt p\left(\kappa (m+1)\right)^{m} m\sigma\right) ,
  % \end{align*}
  % we obtain the first result of the theorem (the Stirling formula allows us to bound $\left(\kappa (m+1)\right)^{m}m/(m+1)! \leq \kappa^m e^{m+1}$). 

  Now, we consider the general setting where we are given $m$ matrices $X_1,\ldots, X_m$, a deterministic matrix $A \in \mathcal{M}_{n_n,n_0}$ satisfying $\|A\|\leq 1$, and we want to show the concentration of $tr(AX_1,\cdots, X_m)$. First note that we stay in the hypotheses of the theorem if we replace $X_1$ with $AX_1$, we are thus left to show the concentration of $\tr(X_1 \cdots X_m)$.
  We can not employ again Lemma~\ref{lem:decomposition_poly_symetrique} without a strong hypothesis of commutativity on the matrices $X_1,\ldots, X_n$. Indeed, one could not have gone further than a concentration on the whole term $\sum_{\sigma \in \mathfrak S_p}\tr(X_{\sigma(1)} \cdots X_{\sigma(m)})$.
%    However, if $m$ is small, say, if $m \leq O(1) $, and the 
% % \begin{align*}
% %     X_1 \cdots X_m ,
% %    \end{align*}
%    % Besides, if the 
%    matrices $X_1,\ldots, X_m$ have different sizes, w
However, we can still introduce the random matrix
    \begin{align*}
    Y = \left( \begin{array}{cccc}
    0&X_{m-1} &&  \\
    & \ddots&\ddots& \\
    & &\ddots&X_1 \\
    X_m& &&0  \end{array} \right)&
    &\text{then}&
    &Y^m = \left( \begin{array}{cccc}
    0&X^{m}_1 &&  \\
    & \ddots&\ddots& \\
    & &\ddots&X_{3}^{2}\\
    X_{2}^{1}& &&0  \end{array} \right),
    \end{align*}
    where for $i,j \in \{2, \ldots, m-1\}$, $X^j_i \equiv X_i X_{i+1} \cdots X_m X_{1}\cdots X_j$. Since $Y \in \mathcal{M}_{n_0+\cdots+n_m}$ satisfies $Y \propto \mathcal E_2(\sigma)$ and $\|Y\|\leq \kappa$, the first part of the proof provides the concentration $Y^m \in \mathcal E_2 \left(\kappa^{m-1}\sigma\sqrt{n_0+\cdots+n_m}\right)$ in $\left(\mathcal M_n, \| \cdot \|_*\right)$ which directly implies the concentration of $X_1^m= X_1\cdots X_m$.

  % The second result is obtained the same way looking at projections of powers of the matrix
  % \begin{align*}
  %    Y \equiv 
  %     \left( \begin{array}{cc}
  %     0 & X^T \\ 
  %     X & 0 \end{array} \right) \propto_c \mathcal E_2(\sigma)&
  %     &\text{in} \ \ \left(\mathcal M_{p+n,p+n}, \|\cdot \|_F\right)
  %  \end{align*} 
  %  that satisfies $\|Y\| = \|X\|$. Given $m\in \mathbb N$, the concentration of $(XX^T)^m$ is then a consequence of the identities:
  % \begin{align*}
  %   % &\text{indeed for all $k\in \mathbb N$}&
  %   Y^{2m} =\left( \begin{array}{cc}
  %   0 & (X^TX)^m \\ 
  %   (XX^T)^m & 0 \end{array} \right).
  %   % &\text{and}&
  %   % &Y^{2m +1} =\left( \begin{array}{cc}
  %   % 0 & (X^TX)^mX^T \\ 
  %   % (XX^T)^mX & 0 \end{array} \right).
  % \end{align*}

  % $M \in \mathcal S_p^+$ and $H \in \mathcal S_p$ and assuming $m\geq 2$, we introduce the mapping:
  % \begin{align*}
  %   \phi^A : \begin{aligned}[t]
  %     \mathbb R&& \longrightarrow&&\mathbb R \hspace{1.3cm} \\
  %     t& &\longmapsto& &\tr(AX^m).\\
  %   \end{aligned}
  % \end{align*}
  % We can differentiate it twice to obtain
  % \begin{align*}
  %   \restriction{d^2\phi^A}{X}\cdot H = \sum_{k_1+k_2+k_3 = m} \tr(AX^{k_1}HX^{k_2}H X^{k_3}).
  % \end{align*}
  % Now note that, for $M$ nonnegative symmetric, $\phi^{I_p}(M) = \|M\|_m^m$ where $\|\cdot \|_m$ is the norm defined as $\|A\|_m = (\tr((MM^T)^{m/2}))$. Since the norm $\|\cdot\|_m$ is a convex mapping having values on $\mathbb R^+$ and $t \mapsto t^m$ is convex on $\mathbb R^+$, we know that $\phi^{I_p}$ is also convex. Therefore, the inequality $\restriction{d^2\phi^A}{X}\cdot H \leq \left\Vert A\right\Vert \restriction{d^2\phi^{I_p}}{X}\cdot H$ implies that $\restriction{d^2\phi^{\|A\|I_p - A}}{X}\cdot H\geq 0$ and therefore $\phi^A = \phi^{\|A\|I_p} -\phi^{\|A\|I_p - A}$ is the difference of two convex mappings. 
  % Now, to find Lipschitz properties, we have to restrict ourselves to the ball $\mathcal B_\kappa \equiv \{M \in \mathcal S_p^+ \ | \ \|M \| \leq \kappa \sigma p^{1/q}\}$ where the constant $\kappa \leq O(1)$ is chosen such that $\mathbb P ( X \notin \mathcal B_\kappa) \leq C e^{-p/c}$ for some constants $C,c \leq O(1)$. Thus, for any $B \in \mathcal M_{p}$ such that $\phi^B$ is convex, the mapping $\phi^B$ is $\|B\|_F(\kappa \sigma)^{m-1}p^{(m-1)/q}$-Lipschitz. 
  % We can then extend by continuity $\restriction{\phi^B}{\mathcal B_\kappa}$ to a $\|B\|_F(\kappa \sigma)^{m-1}p^{(m-1)/q}$-Lipschitz and convex mapping defined on all the vector space $\mathcal S_{p}$ {\color{red} à vérifier}.
  % Once again, playing on the condition $\{X \in \mathcal B_\kappa \}$ with Lemma~\ref{lem:concentration_conditionnee_convexe}, one can show that for any $A\in \mathcal M_{p,n}$ such that $\|A\|_F \leq 1$,
  % \begin{align*}
  %   \tr(A X^m) \in \mathcal E_2 \left( \left(\kappa \sigma\right)^{m} p^{(m-1)/q}\right) \ | \ e^{-p}.
  % \end{align*}
  % The concentration inequality for $(XX^T)^m$ when $X \in \mathcal M_{p,n}$ is obtained similarly.
\end{proof}

  % The demonstration follows exactly the same steps when we do not place ourselves on $\mathcal A_\kappa$ (it is just a bit longer to write).

  % We do not present the proof of the second result since it is quite technical and relies on exactly the same tricks as those that can be found in the proof of \cite[Theorem 2.5 \ref{}]{LOU19}.
  % Interestingly enough, one can notice that, with the use of Lemma~\ref{lem:decomposition_poly_symetrique}, one obtains a better concentration for $X^{\odot m}$ than with the straightforward approach. 
  % where we introduced $Z = \frac{1}{4}(X+Y)$ and $W = \frac{1}{4}(X-Y)$. We know that $Z,W \propto_c \mathcal E_2(\sigma) \ | \ e^{-\eta}$ and $\|\mathbb E[Z] \|_\infty, \|\mathbb E[W] \|_\infty= O((\log p)^{1/q})$. Thus there exists a constant $K \leq O(1)$ such that if we introduce the ball $\mathcal B_K = \{v \in \mathbb R^p \ | \ \|v \|_\infty \leq K \sigma \eta^{1/q}\}$, the mapping $v \mapsto a^Tv^{\odot 2}$ is $K \sigma \eta^{1/q}$-Lipschitz, and we can continue it to a $K \sigma \eta^{1/q}$-Lipschitz mapping $\phi$ defined on the whole space $\mathbb R^p$. It is however not quasi-convex as required to employ the property of convex concentration of $Z$ and $W$. Let us then write for $v = (v_1, \ldots, v_p)$:
  % \begin{align*}
  %   \phi(v) = \|a\| \un^T v^{\odot 2} - ( (\|a\| \un - a)^T v^{\odot 2})
  %  \end{align*} 
  %  Indeed, we know from Lemma~\ref{lem:sum_f_convexe_convexe} that $t \mapsto t^2$ being convex, $v \mapsto \|a\| \un^T v^{\odot 2}$ and $v \mapsto (\|a\| \un - a)^T v^{\odot 2}$ are both quasi convex. Thus introducing the event $\mathcal A = \{\|Z \|_\infty, \|W \|_\infty \leq K \sigma \eta^{1/q} \} \cap \mathcal A_X \cap \mathcal A_\phi$ (where $\mathcal A_X$ and $\mathcal A_\phi$ are the concentration zones of $X$ and $Y$): $\mathbb P(\mathcal A^c) \leq C e^{- \eta/c}$ and:
  % \begin{align*}
  %   % \mathbb P(\mathcal A^c) \leq C e^{- \eta/c}&
  %   % &\text{and}&
  %   \mathbb P \left(\left\vert a^T(Z^{\odot 2} - W^{\odot 2}) - \mathbb E[a^T(Z^{\odot 2} -W^{\odot 2})] \right\vert \geq t, \mathcal A\right) \leq C e^{-(t/c\sigma)^2/\eta,}
  %  \end{align*}
  %  for some constants $C,c \leq O(1)$. The proof is about the same as the proof of Proposition~\ref{pro:concentration_intelligente_produit}.
\end{proof}


\section{Alternative proof of Proposition~\ref{pro:Concentration_lineaire_Q_proche_spectre}}

% \begin{proof}
  We are going to show the concentration of the real part and the imaginary part of $Q^z$, where:
  \begin{align*}
    &\Re(Q^z) = Q^z \left(\Re(z) I_p - \frac{1}{n}XX^T\right)\bar{Q}^z =(\Re(z)-z) |Q^z|^2 + \bar{Q}^z \\
    &\Im(Q^z) = \Im(z) |Q^z|^2
  \end{align*}
  Since it is harder, we will only prove the linear concentration of $|Q^z|^2 = (\Im(z)^2 + (\Re(z) - \frac{1}{n}XX^T)^2)^{-1}$. For that we are going to decompose, for any matrix $A \in \mathcal M_{p}$ with unit spectral norm, the random variable $\tr(A|Q^z|^2)$ as the sum of convex and $O(1/\sqrt n)$-Lipschitz mappings of $X$. Let us introduce the two mappings, $\psi : \mathcal M_{p} \to \mathcal M_{p}$ and $\phi : \mathcal M_{p,n} \to \mathcal M_{p}$ defined for any $M \in \mathcal M_{p}$ and $B \in \mathcal M_{p, n}$ with:
  \begin{align*}
    \psi(M) = (\Im(z)^2 + M)^{-1}&
    &\phi(B) = \Re(z)^2 - \frac{2\Re(z)}{n}BB^T + \frac{1}{n^2}BB^TBB^T.
  \end{align*}
  We then have the identity $\tr(A Q^z) = \tr \left(A\psi\circ\phi \left(X\right)\right)$. 
  % We already know from Proposition~\ref{pro:concentration_resolvente_1} that $\tr \left(A\psi\circ\phi \right)$ is $O(\|A\|_F/\sqrt n) = O(1)$-Lipschitz (for the Frobenius norm). 

  We then then look at the second derivative of $\psi\circ\phi$ to prove convex properties on $\tr(A\psi\circ\phi)$.
  % \begin{align*}
  %   \tr(A Q^z) = \tr \left(A\psi\circ\phi \left(\frac{X}{\sqrt n}\right)\right)
  % \end{align*}
  Given $H \in \mathcal M_{p}$, let us compute:
  \begin{align*}
    \restriction{d\psi}{M} \cdot H = -\phi(M)H\phi(M)&
    &\restriction{d^2\psi}{M} \cdot (H,H) = 2\phi(M)H\phi(M)H\phi(M),
  \end{align*}
  and given $K \in \mathcal M_{p,n}$:
  \begin{align*}
    &\restriction{d\phi}{B} \cdot K = -\frac{2\Re(z)}{n} L(B,K) + \frac{1}{n^2}P(K,B)\\
    &\restriction{d^2\phi}{B} \cdot (K,K) = -\frac{2\Re(z)}{n} KK^T + \frac{2}{n^2}P_2(K,B),
  \end{align*}
  where:
  \begin{itemize}
    \item $L(B,K) = BK^T +KB^T$
    % \item $L_2(B,K) = KK^T$
    \item $P(B,K) = KB^TBB^T+ BK^TBB^T + BB^TKB^T + BB^TBK^T$
    \item $P_2(B,K) = KK^TBB^T+ KB^TKB^T + KB^TBK^T+ BK^TKB^T + BK^TBK^T + BB^TKK^T$.
  \end{itemize}
  First we deduce from the expression of the first derivative and thanks to Lemma~\ref{lem:Q_borne} that, on $X(\mathcal A)$, $\tr \left(A\psi\circ\phi \right)$ is a $O(\|A\|_F/\sqrt n) = O(1)$-Lipschitz transformation of $X$ (for the Frobenius norm).

  Second,choosing $M = \phi(B)$:
  \begin{align*}
    \restriction{d^2\psi\circ\phi}{B} \cdot (K,K) 
    &= \restriction{d^2\psi}{m
    M} \cdot \left(\restriction{d\phi}{B} \cdot K,\restriction{d\phi}{B} \cdot K\right) + \restriction{d^\psi}{m
    M} \cdot \left(\restriction{d^2\phi}{B} \cdot (K,K)  \right)\\
    &= 2\phi(M) \left(\restriction{d\phi}{B} \cdot K\right)\phi(M) \left(\restriction{d\phi}{B} \cdot K\right)\phi(M) \\
    &\hspace{0.5cm} + \frac{2\Re(z)}{n}\phi(M)  KK^T\phi(M) - \frac{2}{n^2}\phi(M) P_2(K,B) \phi(M).
  \end{align*}
  In this identity the only term raising an issue is $\frac{2}{n^2}\phi(M) P_2(K,B) \phi(M)$ because $P_2(K,B)$ is not nonnegative symmetric. 
  % Still, one can bound for any $x \in \mathbb R^p$:
  % \begin{align*}
  %   2x^TKK^TBB^Tx \leq x^TKK^Tx + x^TBB^TKK^TBB^Tx \leq 2 x^T 
  % \end{align*}
  We can however still bound:
  \begin{align*}
    \frac{12}{n^2} \tr \left(A \phi(M) P_2(K,B) \phi(M)\right) \leq \frac{12}{n^2}\|A\|\|\phi(M)\|^2 \|B\|^2 \|K\|_F^2 \leq O \left(\frac{1}{n}\tr(KK^T)\right),
  \end{align*}
  for $B \in X(\mathcal A_Q)$ (in particular $\|B\| \leq O(\sqrt n)$ and $\|\phi(M)\| \leq O(1)$). Now, if we note $h : \mathcal M_{p,n} \to \mathbb R$ defined for any $B \in \mathcal M_{p,n}$ as $h(B) = \frac{1}{n} \tr(BB^T)$, we see that $\frac{1}{n}\tr(KK^T) = d^2h(B)\cdot (K,K)$ is a quadratic functional on $K$, $h$ is thus convex. It is beside $O(1)$-Lipschitz on $X(\mathcal A_Q)$ (for the Frobenius norm). Assuming in a first time that $A$ is nonnegative symmetric and choosing a constant $C \leq O(1)$ sufficiently big, we show that $B \mapsto \tr(A \psi\circ \phi(B)) +C h(B)$ is convex and $O(1)$-Lipschitz on $X(\mathcal A_Q)$ like $C h$. We have thus the concentration:
  \begin{align*}
     \left( \tr(A |Q^z|^2) \ | \ \mathcal A  \right) \in \mathcal E_2 .
   \end{align*} 
   Now, given a general matrix $A \in \mathcal M_{p}$, we decompose $A = A_+ - A_- + A_0$ where $A_+$ and $A_-$ are nonnegative symmetric and $A_0$ is anti-symmetric, in that case $\tr(A |Q^z|^2) = \tr(A_+ |Q^z|^2) - \tr(A_- |Q^z|^2)$, and we can conclude the same way. That eventually gives us the concentration of the proposition.
% \end{proof}


\bibliographystyle{alpha}

\bibliography{biblio}

\end{document}

\textbf{}