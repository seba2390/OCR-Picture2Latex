%To accomplish the previously defined design goals, we have implemented \textsc{VisRuler}. Our VA tool has been developed using Python, the Scikit-Learn~\cite{Pedregosa2011Scikit} ML library for training the ML models, and the Flask~\cite{Flask} web framework to build the backend. As for the frontend, we utilize JavaScript, Vue.js~\cite{vuejs}, and a mixture of the D3.js~\cite{D3} and Plotly.js~\cite{plotly} visualization libraries.
Motivated by the above design goals and tasks, we have developed \textsc{VisRuler}. Our VA tool is written in Python and JavaScript. More technical details available on GitHub.~\cite{VisRulerCode}
%The backend of our VA tool was built using Python, Scikit-learn~\cite{Pedregosa2011Scikit}, and Flask~\cite{Flask}. As for the frontend, we utilize JavaScript, Vue~\cite{vuejs}, D3~\cite{D3}, and Plotly.js~\cite{plotly}.

\begin{figure}[tb]
\centering
\includegraphics[width=\columnwidth]{workflow-diagram-crop.pdf}
\caption{The \textsc{VisRuler} workflow allows ML experts to select performant and diverse models, choose important features, and retrain models with new hyperparameters. Domain experts can explore robust decisions, compare them to global standards, identify local decisions for a specific test instance, and extract them.}
\label{fig:workflow-diagram}
\end{figure}

The tool consists of five main interactive visualization panels (Figure~\ref{fig:teaser}): (a) \emph{models overview} (\textbf{T1}), (b) \emph{global feature ranking} (\textbf{T2}), (c) \emph{decisions space} (\textbf{T3}), (d) \emph{manual decisions} (\textbf{T4}), and (e) \emph{decisions evaluation} (\textbf{T5}).
%
Our proposed \textbf{workflow} is a two-party system with the ML expert on the one side and the domain expert on the other (see Figure~\ref{fig:workflow-diagram}). The above-mentioned panels of our tool support the experts' collaborative effort, specifically: (i) the ML expert should select powerful and diverse models from the two separate algorithms based on their performance assessed by validation metrics (Figure~\ref{fig:teaser}(a));
(ii) during this phase, the ML expert should choose which features are important for the active models compared to all models (see Figure~\ref{fig:teaser}(b));
(iii) in the next exploration phase, both experts should examine which decisions explain the data set globally and decide upon impactful decisions for a specific test instance (cf. Figure~\ref{fig:teaser}(c)); 
(iv) in this same phase, the domain expert should interpret the manual decisions selected in order to gain insights about the models' decisions---either globally or locally---for a particular test instance (Figure~\ref{fig:teaser}(d)); and 
(v) in the final phase, the domain expert can evaluate the agreement and extract suitable manual decisions while the ML expert should search for new models if the search did not reach a satisfactory level according to the domain expert (Figure~\ref{fig:teaser}(e)). Overall, this is an iterative process with a final goal to receive insightful decisions that should be interpretable for all counterparts. Details about the different views within the panels can be found below.

\textsc{VisRuler} incorporates a single workflow for the synchronous co-located collaboration between the ML expert and the domain expert, as depicted in Figure~\ref{fig:collaboration-diagram}. It is a VA system that comprises multiple coordinate views arranged in a single webpage to manage the entire process without any distractions occuring due to the navigation to different tabs. The three phases and five visualization panels are already described in the previous paragraph. Both experts' usual interactions with the system can be aggregated into seven steps that are followed in the use case explained in this section and the usage scenario in Section~\nameref{sec:case}. Most of the time, the active role (cf. orange color in the diagram) is given to the ML expert who is responsible for selecting models (step 1), selecting features (step 2), exploring decisions (step 4), and finally, in step 7, search for new models (ideally based on the domain expert's feedback). On the other hand, the domain expert often has a more passive role (see teal color in the diagram) since he/she should focus on important features (step 3) and interpret the decision rules (step 5) based on the prior step. But in step 6, the domain expert becomes active because he/she has to decide which manual decisions should be extracted, for example, to be used as input to another tool, or as evidence for his/her diagnosis. The meeting point between both experts is step 4, the exploration of decisions, where the ML expert can control the multiple implemented filtering options (e.g., acceptable impurity level) while the domain expert pinpoints specific decision paths that could be interesting for a detailed manual investigation. \hl{Without their continuous communication while they operate the system and observe each other's actions, gaining deep insights would be very difficult.}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{collaboration-diagram-crop.pdf}
\caption{The \textsc{VisRuler} cooperation diagram illustrates how synchronous co-located collaboration typically happens between the ML expert and the domain expert. Three phases and five panels support their teamwork in a single-page tool, with the ML expert being more active (orange color) than the domain expert who receives and analyzes information (teal color). The seven linear steps taken by each user are also noted at the bottom.}
\label{fig:collaboration-diagram}
\end{figure}

The workflow of \textsc{VisRuler} is model-agnostic, as long as rules can be extracted from the ML algorithms. 
%In this current version, the implementation uses two rather popular EL methods: (1) RF~\cite{Breiman2001Random} and (2) AB~\cite{Freund1999A} (cf. green and blue colors in Figure~\ref{fig:workflow-diagram}, respectively). 
Currently, the implementation uses two popular EL methods: (1) RF and (2) AB (cf. green and blue colors in Figure~\ref{fig:workflow-diagram}, respectively). 
%This choice was made intentionally because bagging ensemble methods work differently than boosting methods, as explained in Section~\nameref{sec:intro}. 
This choice was intentional since bagging methods work differently than boosting, as explained in Section~\nameref{sec:back}. 
Furthermore, each data set is split in a stratified fashion (i.e., keeping the class balance in training/testing split) into 90\% of training samples and 10\% of testing samples. We also use cross-validation with 3-folds on the training set, and we scan the hyperparameter space for 10 iterations using Random search~\cite{Bergstra2012Random} in each algorithm separately. The common hyperparameters for both ML algorithms we experimented with (and their intervals) are: number of trees/estimators (2--20), maximum depth of a tree (10--25), and minimum samples in each leaf of a tree (1--10). 
An extra hyperparameter of RF is the maximum number of features to consider when looking for the best split (($\sqrt{number\_of\_features}$)--($number\_of\_features-1$)). AB has the learning rate (0.1--0.4). It is straightforward to adapt those values through the code.

In the following subsections, we explain \textsc{VisRuler} by describing a use case with the \emph{World Happiness Report 2019}~\cite{Helliwell2019World} data set obtained from the Kaggle repository.~\cite{Kaggle2019} This data set contains 156 countries (i.e., instances) ranked according to an index representing how happy the citizens of each country are. The six other variables that could be considered as features are: (1) \emph{GDP per capita}, (2) \emph{social support}, (3) \emph{healthy life expectancy}, (4) \emph{freedom to make life choices}, (5) \emph{generosity}, and (6) \emph{corruption perception}. Because this data set does not contain any categorical class labels, we follow the same approach as in Neto and Paulovich~\cite{Neto2021Multivariate} to discretize the happiness score in three different bins. Hence, we are converting this regression problem into a multi-class classification problem.~\cite{Salman2012Regression} Also in our case, the original variable Score becomes the target variable that our ML models should predict. In detail, the HS-Level-3 class contains 42 countries with happiness scores (HS) ranging from $6.13$ to $7.76$, the HS-Level-2 groups 79 countries from $4.49$ to $6.13$, and the HS-Level-1 class encloses 35 countries from $2.85$ to $4.49$.

\subsection{Models Overview} \label{sec:models}

%In \textsc{VisRuler}, 
The exploration starts with an overview of how 10 RF and 10 AB models performed based on three validation metrics: accuracy, precision, and recall. The models are initially sorted according to the overall score, which is the average sum of the three metrics. \hl{This choice guides users to focus mostly on the right-hand side of the line chart (as showcased in Section~\nameref{sec:application}).} Green is used for the RF algorithm, while blue is for AB. All visual representations share the same x-axis: the identification (ID) number of each model. \hl{The design decision to align views vertically enables us to avoid repetition and follows the best practices.} The line chart in Figure~\ref{fig:teaser}(a) always presents the worst to best models from left to right. The y-axis denotes the score for each metric as a percentage, with distinct symbols used for the different metrics. 
%The Sankey diagram in Figure~\ref{fig:teaser}(a) visually maps a confusion matrix of only false-positive and false-negative values for each model, divided into two groups reflecting the two algorithms. It presents the confusion of all individual classes for the different instances when comparing two subsequent models, as illustrated in both~Figure~\ref{fig:teaser}(a) and ~Figure~\ref{fig:use_case1_model}(a). The width of the band between two consequetive Sankey nodes indicates the increase or decrease in confusion from one model to the other sequentially, so the smaller the height of a line, the better a model's prediction compared to its predecessor or successor. The same effect applies to each node that absorbs the lines. 
The confusion plot inspired by Sankey diagrams in Figure~\ref{fig:teaser}(a) visually maps a confusion matrix of only false-positive and false-negative values for each model into nodes with different heights depending on the number of confused training instances. Then, they are divided into two groups reflecting the two algorithms. It also presents the confusion of all individual classes for the different instances when comparing two subsequent models, as illustrated in both Figure~\ref{fig:teaser}(a) and Figure~\ref{fig:use_case1_model}(a). The width of the band between two consecutive nodes indicates the increase or decrease in confusion from one model to the other sequentially, so the smaller the height of a line, the better a model's prediction compared to its predecessor or successor. The same effect applies to each node that absorbs the lines. With this plot, users can focus on the misclassified training instances that are more important for a given problem. For example, a medical doctor is typically cautious when dealing with false-negative instances since human lives may be at risk. Users can also check how many misclassified instances exist in each model and propagate from one model to another for each label class. \hl{Inspired by previous works,~\cite{Liu2018Visual,Wang2021Investigating} we utilize a distinct visual metaphor for this plot to convey---as concisely as possible---the per class confusion for the several under examination ML models.}
%For the visualizations above, hover-over interactions are possible. 
The bar charts in Figure~\ref{fig:teaser}(a) showcase the two main architectural components of the bagged and boosted decisions trees, which are the \emph{number of trees/estimators} hyperparameter and the number of decisions generated from these trees for every model mapped in the y-axes, respectively. These visualizations allow users to check the related hyperparameters of the individual models in a juxtaposed manner, since the number of decisions is related to the number of trees and the maximum allowed depth of each tree (i.e., \emph{max\_depth} hyperparameter). Finally, the state shown in Figure~\ref{fig:teaser}(a) designates which models are currently active (green or blue, respectively). In order to enable the comparison between the currently active model against all models, each icon for an active model contains a brown-colored slider thumb (Figure~\ref{fig:teaser}(a), legend on the left).

\subsection{Global Feature Ranking} \label{sec:features}

%To provide a holistic view of the performance of the models, users can explore 
The box plots which aggregate per-algorithm importance (see~Figure~\ref{fig:teaser}(b)) provide a holistic view of the performance of the models. Each pair of boxes is related to a unique feature, summarizing the active models' normalized importance per feature (from 0 to 1, i.e., worst to best). The box plots are sorted according to the average values of all active models, visible as a number in teal. The difference to all models being active is shown with arrows facing up for increase or down for decrease in per-feature importance. For both algorithms, we compute feature importance as the mean and standard deviation of accumulation of the impurity decrease within each tree. This is a measurement that can be calculated directly from RF~\cite{Rogers2006Identifying} and AB~\cite{Wang2012AdaBoost} algorithms, cf. Section~\nameref{sec:back}.

\subsection{Decisions Space} \label{sec:space}

The projection-based view in Figure~\ref{fig:teaser}(c) is produced with the UMAP algorithm,~\cite{McInnes2018UMAP} selected due to its popularity and the results of a recent quantitative survey~\cite{Espadoto2021Toward} that found this algorithm the best overall among many others. In the visual embedding, decision paths are clustered based on their similarity according to their ranges for each feature, as described in Section~\nameref{sec:back} and in the work of Zhao et al.~\cite{Zhao2019iForest} Therefore, it enables an algorithm-agnostic comparison between decisions stemming from both RF and AB models.
% Groups of points nearby have similar (or identical) value ranges for the same features.
The green color in the center of a point indicates that a decision is from RF, while blue is for AB. The outline color reflects the training instances' class based on a decision's prediction. The size maps the number of training instances that are classified by a specific decision, and the opacity encodes the impurity of each decision. Low impurity (with only a few training instances from other classes) makes the points more opaque. The positioning of the points can be used to observe if the RF and AB models produced similar rules, offering a comparison between algorithm decisions. The histogram in Figure~\ref{fig:teaser}(c) shows the number of decisions (y-axis) and the distribution of training instances in these paths (x-axis), and can also be used to filter the number of visible decisions in the projection-based view to avoid overfitting rules containing only a few instances (as shown in Figure~\ref{fig:use_case1_broderline}(a)) or general rules that might not apply in problematic cases.

UMAP is initiated with variable \emph{n\_neighbors} and \emph{min\_dist} fixed to $0.1$. To determine the optimal number of clusters to be visualized, DBSCAN~\cite{Ester1996A} is used to compute an estimated number of core clusters from the derived decisions, which is then used to tune the \emph{n\_neighbors}, with a minimum of 2 and a maximum of 100 neighbors (the aim is to have the same magnitude in both). For the first experiment in Section~\nameref{sec:application}, \emph{n\_neighbors} was automatically set to 20. On the other hand, in the usage scenario of Section~\nameref{sec:case}, DBSCAN estimated 477 clusters, which tuned the hyperparameter to the maximum value.

Multiple interactions are possible in this entire panel. The rounding slider (set to 15) allows users to round all decisions' range values to the desired decimal points. The comparison mode (active in Figure~\ref{fig:teaser}(c)) enables users to anchor groups of points and compare the selection against any other cluster. The two alternative choices are to present either the overlap or difference between the handpicked groups (shown in magenta color); the \emph{Detach} button is for canceling this mode. Density views assist users in observing the distribution of RF against AB decisions in the projection, which is helpful if large amounts of decisions are visualized, as illustrated in Figure~\ref{fig:use_case1_safe}(a), projection. The \emph{Limit Decisions due to Test Instance} checkbox alters the layout and changes global decisions' exploration to local for a particular case. Finally, a limit can be set for the acceptable impurity that is visible. If a decision is more impure than the currently chosen value, then it becomes almost transparent. As this view is tightly connected with the visualization of the following view, we proceed directly to Section~\nameref{sec:manual}.

\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{usecase1_models_S1-crop.pdf}
\caption{Exploration of ML models with \textsc{VisRuler}. View (a) presents the deactivation of all models except for RF8, RF10, and AB10, after consideration of their performance based on multiple metrics displayed in the visualizations. In (b), \emph{Generosity} is the least important feature for the three active ML models and, particularly, its importance decreased while we deactivated most of the available ML models (see brown color). View (c) indicates that, after retraining with 5 of 6 original features, the new AB8 is better than the subsequent models due to the decline in recall; AB8, RF9, and RF10 remain the only active models after this step. In the box plot (d), the feature \emph{H life exp} becomes more important by far than \emph{GDP per cap}. Thus, these features swapped places compared to view (b).}
\label{fig:use_case1_model}
\end{figure*}

\subsection{Manual Decisions} \label{sec:manual}

The vertical PCP-like view in Figure~\ref{fig:teaser}(d) illustrates the range values per feature for each selected decision (comparison mode is active). The vertical polylines represent the training instances and are color-encoded based on the ground truth (GT) class. There are two options: either select to filter instances and show those that belong to the selected rules (see~Figure~\ref{fig:teaser}(d)) or present all training instances at once (see Figure~\ref{fig:use_case1_safe}(c)--(e)). For example, in Figure~\ref{fig:use_case1_safe}(c), we see 12 identical rules that classify the training instances in the HS-Level-3 class (the red horizontal lines). The thick black polyline is the currently explorable test instance; users can compare it to the training instances of the models. All ranges for the features are normalized from 0.0 to 1.0. Scrolling is implemented when many decisions must be shown or the number of features is large. 
%The goal of this visualization is to function as a straightforward approach to compare the decision ranges per feature while highlighting the test instance among the training instances. 
The order of the features is initially the global one, as described in Section~\nameref{sec:features}. When a group of points is selected using the lasso tool in the \emph{decisions space} (DS) view, a contrastive analysis~\cite{Zou2013Contrastive} is used to rank the features and highlight unique features that explain a cluster's separation from the rest. The computation works as follows:
%
(1) break each feature into two disjoint distributions: the values inside the selected group vs. all the rest of the points; 
(2) discretize the two distributions of each feature into bins based on the \emph{Local Feature Ranking - Bins} value set by the user (default is 10); 
(3) compute the cross-entropy~\cite{Mannor2005The} between the two distributions of each feature: higher values of cross-entropy suggest more unique features (i.e. the within-selection distribution is very different than the rest), while lower values suggest more common, shared features; and
(4) rank the features based on step 3, with the more unique features near the top. Either with the overlap or difference setting selected as discussed in Section~\nameref{sec:space}, the decision ranges bounding each feature are visualized in the vertical PCP with a magenta color in this comparison mode.

\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{usecase1_safe_global_S2-crop.pdf}
\caption{Examining several pure global decisions from the active AB model. In (a), we activate the density view in order to distinguish where most decisions are positioned. Note that this screenshot is composed of the \emph{decisions space} (DS) view and the settings for the same view plus the settings for the \emph{manual decisions} (MD) view. In (b), we select step-by-step three clusters of 12 identical decisions each. The decisions for \circled{C1} classify training instances only for HS-Level-3 class (as depicted in (c)). Similarly, \circled{C2} contains decisions for HS-Level-2 (visible in (d)), while \circled{C3} for the remaining class, as shown in (e). The \nth{7} test instance, which is currently under investigation, cannot be classified by those prior decisions. However, it most likely belongs in the medium- or the high-level class.}
\label{fig:use_case1_safe}
\end{figure*}

\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{usecase1_borderline_local_S3-crop.pdf}
\caption{The local exploration of the \nth{6} test instance with specific decisions from all the active models that apply only for this and similar test instances. (a) is a projection-based view that includes \circled{C1} with multiple impure decisions, visible only after zooming in. \circled{C2} is a decision with only 2 influenced training instances falling in this path, hence, we interact with the histogram below it to filter out overfitting cases with only 2 or 3 training instances. The comparison mode is enabled for \circled{C3}, resulting in anchoring two subclusters of different (in terms of class) prediction decisions. In (b), the PCP highlights the differences between these previous subclusters for each feature in magenta color. This view is dynamic since the features are constantly being re-sorted based on contrastive analysis of the selected cluster against all points.}
\label{fig:use_case1_broderline}
\end{figure*}

\subsection{Decisions Evaluation} \label{sec:evaluation}

The panel in Figure~\ref{fig:teaser}(e) contains interactive views that help users find outliers, borderline cases, and misclassified cases in the test set. The first main view supports extracting the \emph{manual decisions} (MD) from the previous phase (see Section~\nameref{sec:manual}). This output is stored in a JSON format where the boundaries of values per-feature are observable for every picked decision rule. This enables domain experts to reuse the hand-picked decision rules for supporting future actions, and also helps in concentrating on cases where the majority of the RF and AB models disagreed when compared to the GT, or for models that did not vote unanimously. It is also possible to go through all test instances one by one. The class agreement between RF and AB models, MD, and the GT is demonstrated via a horizontal stacked bar chart. The colors encode the different classes, and the length of each bar is the number of decisions for (1) MD, (2) RF models, (3) AB models, and (4) the GT (the latter always fills the entire bar). 
%As the number of models or decisions increases, the more the horizontal stacked bars split into individual segments. 
The second main view is used to train new models based on the \emph{Ov. Score (\%)} of each previously-trained model. The two separate standard PCPs present the active RF models in green and the active AB models in blue. The brown color is used for the inactive models. %in both visualizations. 
%A slider visible in Figure~\ref{fig:teaser}(e) is for searching for new models.

\subsection{Use Case} \label{sec:application}

In our use case, we observe that models with ID number 8 and above slightly outperform the rest; notably, recall in AB7 is much lower than AB8 and beyond (cf.~Figure~\ref{fig:use_case1_model}(a), line chart). While RF models perform consistently better than AB models, as shown in both the line chart and the confusion plot of~Figure~\ref{fig:use_case1_model}(a), there is an improvement in the score of AB10. Therefore, we decide to keep only this model. Furthermore, since RF8 is more reliable in training instances for the HS-Level-2 class due to false-positives being lower than the equivalent for RF9 and RF10 (Figure~\ref{fig:use_case1_model}(a), confusion plot), we keep this model and RF10, i.e., the top-performing model of the RF algorithm. In consequence, RF8, RF10, and AB10 are active models after selecting the corresponding states. 

At this point, we want to investigate which features of the training set impacted the predictions more (see Figure~\ref{fig:use_case1_model}). Interestingly, \emph{GDP per cap}, \emph{H life exp}, and \emph{Social sup} are the top three features in the general ranking, as in Neto and Paulovich.~\cite{Neto2021Multivariate} A surprising outcome is that, although two of the features mentioned above are still the most important for the selected RF models (all except Social sup), this is not true for the AB model. As seen in Figure~\ref{fig:use_case1_model}(b), \emph{Social sup}, \emph{Perc of cor}, and \emph{GDP per cap} are vital features for the AB algorithm in general. This pattern supports our hypothesis that different algorithms might take into account alternative features and should be combined to provide a holistic view. On the contrary, \emph{Generosity} is unimportant for all models, specifically for the active models, since there is a $-0.12$ decrease in importance. Thus, we choose to remove this feature and retrain without it (cf. Figure~\ref{fig:use_case1_model}(b)). For the RF algorithm (green), we pick the most performant models based on the overall score (Figure~\ref{fig:use_case1_model}(c)), rightmost models). However, AB8 is better overall than the subsequent AB models due to the stable and high recall value (Figure~\ref{fig:use_case1_model}(c), line chart). In a one-to-one comparison between RF9 and AB9 with the bar charts, we recognize that while they have the same \emph{number of estimators} (i.e., 17 trees), the two models produce 555 and 238 decisions, respectively. In this case, bagged decision trees allow a higher maximum depth than the equivalent boosted decision trees. After the selection of the new models, the most important features collectively are \emph{H life exp} with $0.72$ and \emph{GDP per cap} with $0.49$, as illustrated in Figure~\ref{fig:use_case1_model}(d); the opposite was valid in Figure~\ref{fig:use_case1_model}(b). The new AB model considers the same features more important as the RF models. After this phase is over, AB8, RF9, and RF10 are the remaining three active models.

\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{usecase1_outlier_local_S4-crop.pdf}
\caption{An outlier case exploration, the final prediction, and the training of another bunch of RF and AB models. (a) presents the anchoring of a cluster of 8 HS-Level-2 decisions to compare the overlapping rules against 3 HS-Level-3 decisions. In (b), after checking the common regions of agreement for the two clusters, we conclude that \emph{Perc of cor} and \emph{H life exp} are relatively low for the \nth{15} test instance to belong in HS-Level-3 class. However, the other values for the remaining features are arguably rather high. In (c), we observe that all models voted for the average class while only the 3 selected manual decisions are supporting this case to be categorized as HS-Level-3 country. (d) showcases a potential search for new models by setting constraints in the hyperparameters according to the knowledge acquired from the initial training.}
\label{fig:use_case1_outlier}
\end{figure*}

To investigate the global decisions based on the AB8 model we set the impurity to 0, disable limiting decisions based on the current test instance, hide the RF models, and reveal the density view of the active AB model (cf. Figure~\ref{fig:use_case1_safe}(a)). Most decisions are positioned in the right-hand side of the projection. Thus, we continue with the exploration of identical pure decisions from that region. After hiding back the Density (AB) as shown in Figure~\ref{fig:use_case1_safe}(b), we notice from the size of the decisions that if we analyze three core clusters (\circled{C1}--\circled{C3}) we can get a better understanding of global decisions. In Figure~\ref{fig:use_case1_safe}(c), all 140 training instances ($31+71+38$ spread across the classes) are observable together with the \nth{7} test instance, which is currently under investigation because the majority of the RF and AB models disagree with the GT \hl{(not shown due to space limits, but a similar case is visible in Figure~\ref{fig:teaser}(e))}. From Figure~\ref{fig:use_case1_safe}(c), we see that \emph{Social sup} and \emph{GDP per cap} should be very high for test instances to belong to this class. In contrast, for test instances to be in the HS-Level-2 class, they need to have a low-to-average \emph{Social sup}, and average \emph{GDP per cap} and \emph{H life exp} (Figure~\ref{fig:use_case1_safe}(d)). Low values in the features (1) \emph{Freedom}, (2) \emph{GDP per cap}, and (3) \emph{H life exp} are common for the low score in happiness countries (see Figure~\ref{fig:use_case1_safe}(e)), as also identified by Neto and Paulovich.~\cite{Neto2021Multivariate} Regarding Saudi Arabia (the \nth{7} test instance), it does not appear to belong to any of those decisions, but it is far away from the values reported for the HS-Level-1 class. It has a very high \emph{GDP per cap} to belong in the average class, but the \emph{Social sup} is on the lower side. Despite that, \emph{GDP per cap} is 1 out of the 2 most important features according to the analysis in Section~\nameref{sec:features}. Our conclusion matches the fact that it was ranked in \nth{28} place out of the 156 countries, thus, belonging to the list of 42 countries classified as HS-Level-3.
%We move now to the results concerning an outlier case, as in~\cite{Neto2021Multivariate}.

Using the tool's mechanism to detect problematic test instances, a borderline case that stands out is the \nth{6} test instance. in Figure~\ref{fig:use_case1_broderline}(a), we first lower the impurity threshold to focus only on completely pure decisions. This example is for a specific case, thus, \emph{Limit Decisions due to Test Instance} is checked. \circled{C1} seems to have a zero impurity, but when zooming in, we recognize that it contains several decisions with very high impurity values. Hence, we ignore this cluster, and we move to \circled{C2} with a decision influencing only 2 training instances. Since this number is undoubtedly low and could overfit these 2 instances, we increase the lower limit of visible decisions through the bar chart at the bottom. We exempt 4 decisions with 2 or 3 influenced training instances spread throughout the projection with this action. An interesting insight is observable in \circled{C3}: 8 decisions produced by the RF models are divided, with decisions suggesting that the test instance should be classified as either HS-Level-1 or HS-Level-2. We anchor one of the subclusters and select the other for comparison by viewing the difference in the value ranges of the five features (cf. Figure~\ref{fig:use_case1_broderline}(b)). 12 out of the 18 training instances suggest that Guinea (i.e., the \nth{6} instance) is similar to low-happiness countries. If \emph{Perc of cor}, \emph{H life exp}, and/or \emph{GDP per cap} were slightly higher, then the outcome would have been entirely different. According to the GT ranking Guinea is in \nth{118} place, remarkably close to the \nth{122} test instance which is the first country classified as low happiness.

\begin{figure*}[tb]
  \centering
  \includegraphics[width=\linewidth]{usecase2_safe_global-crop.pdf}
  \caption{The exploration of clusters of decision paths from both ML algorithms. View (a) presents the selection of three clusters of global decisions that classify multiple training instances, thus, avoiding unimportant paths that might overfit. (b) provides an in-depth analysis of the decisions rules affected by \circled{C1}. In (c), \emph{Len\_emp} emerges as a unique feature that characterizes \circled{C2} with values from approximately 0.4 to 1.0. Finally in (d), high values in \emph{P\_st\_cred} and \emph{Ins\_perc} turn over the prediction of the applicant to reject, visible via the exploration of \circled{C3}.}
  \label{fig:use_case2_safe_global}
\end{figure*}

Checking the cases where the majority of the models disagree with the GT, we stop in the \nth{15} test instance. Figure~\ref{fig:use_case1_outlier}(a) shows the decisions applicable for this unusual case. We use the comparison mode to select a pure cluster on the left to juxtapose it with decisions classifying countries as HS-Level-3 on the right. Anchoring these clusters of points shows us the overlap of value ranges for the different features, as depicted in Figure~\ref{fig:use_case1_outlier}(b). 28 out of the 30 training instances are similar to this test instance and belong to the HS-Level-2 class. The ranking of the features indicates that \emph{Perc of cor} and \emph{H life exp} are two unique features for the selected points, with low values for the former and average values for the latter, as in Neto and Paulovich.~\cite{Neto2021Multivariate} Furthermore, for the first four features, the overlap is narrow between the two selected clusters, indicating that this instance could be considered an outlier. Indeed, Figure~\ref{fig:use_case1_outlier}(c) presents that 8 out of the 11 decisions consider this instance as HS-Level-2. All active models are wrongly predicting Trinidad and Tobago (i.e., the \nth{15} test instance) as an average HS country. Interestingly, the 3 MD of the RF models classified this country as HS-Level-3.

From the analyses and the overall score of the RF and AB models, we observe that the most performant models for RF consider only 2 features when splitting the nodes (i.e., \emph{max\_features} hyperparameter). The PCPs in Figure~\ref{fig:use_case1_outlier}(d) enable us to scan the internal regions of the hyperparameters' solution space for RF. As for AB, the \emph{learning\_rate} should be as low as possible for this specific data set, as seen in Figure~\ref{fig:use_case1_outlier}(d). Also, by searching for models with high values for \emph{min\_samples\_leaf}, AB models are created with complex decision trees compared to simple decision stumps, which seems to be an appropriate limitation of the hyperparameter space that could lead to better models. After all these constraints, we move the \emph{Search for New Models} slider from 0 to 10 in Figure~\ref{fig:use_case1_outlier}(d) to request 10 additional models for each algorithm with the hope of discovering more powerful ones. \hl{In summary, \textsc{VisRuler} supported the exploration of diverse decision rules extracted from two different ML algorithms and boosted the trustworthiness of the decision making process (\textbf{RQ1}).}