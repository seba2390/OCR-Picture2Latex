In this section we present a quick overview of the general algorithmic steps of (and differences between) the RF and AB algorithms, in order to familiarize potential readers with the techniques involved and to highlight the importance of our tool. For more details, please refer to the extensive literature published on the two algorithms since their seminal works.~\cite{Freund1999A,Breiman2001Random} 

Suppose there are $N$ instances in our training data set. If we consider a binary classification problem, then we will have: ${x}_i \in \mathbb{R}^n$, ${y}_i \in \{-1,1\}$, where $n$ is the number of features, $x$ is the set of instances with $i = 1, 2, \dots, N$, and $y$ is the target variable which is either $-1$ or $1$, 
designating either class ${C}1$ or ${C}2$.

\textbf{Random Forest.} This algorithm works in two stages. The first stage involves integrating numerous decision trees to construct the RF, and the second stage involves making predictions for each tree created in the first stage, followed by a majority voting strategy.

The algorithm starts by looping for $m = 1$ to $M$, where $M$ is the \emph{number of trees/estimators} hyperparameter. Then, a bootstrap sample $\mathbb{Z}^*$ of size $N$ is drawn from the training data by sampling $N$ times with replacement. Afterwards, a decision tree $T_m$ is grown with the bootstrapped data by recursively repeating the following steps for each terminal node of the tree, until specific conditions have been met (e.g., the \emph{maximum depth} of a tree $d_{\mathrm{max}}$ is achieved). The first step is to form individual decision trees: $n$ features, their number is limited by the \emph{maximum number of features} hyperparameter, and it is selected at random from the $N$ instances (defined in the previous paragraph). Next, the best \emph{split point} $p_v$ among the $n$ is picked. Finally, the node $v$ is split into two child nodes.

The output is an ensemble of trees $\{T_m\}_1^M$ that can be used to make predictions at a new test instance $k$, as follows. Let $\hat{C}_m(k)$ be the class prediction of the $m$th decision tree; the output will be calculated as: $\text{majority\_vote}\{ \hat{C}_m(k)_1^M \}$. For a test instance, the \emph{decision path} that predicts it is followed repeatedly for every decision tree, and the test instance gets assigned to the category that wins the majority votes.

For each $v$ in a decision tree, the $p_v$ refers to a feature and different criteria (e.g., the \emph{minimum samples} in each leaf of a tree $s_{\mathrm{min}}$ is reached) tunable through the hyperparameter settings that are used to split this node to children nodes. The \textbf{Gini impurity}~\cite{Genuer2010Variable} measurement is utilized in our case to pick this $p_v$, formulated for each $v$ as follows: $GI(v) = \sum_{c = 1}^C P_{vc} \cdot (1 - P_{vc})$, where $P_{vc}$ is the probability of a certain classification $c$ (in our example ${C}1$ and ${C}2$). Given an input test instance, each decision tree will fall into a root-to-leaf decision path, which leads to its prediction. Every \textbf{decision path} (or simply \emph{decision}) is constructed by defining a range of minimum and maximum values for each feature extracted from RF and AB grown decision trees. Therefore, for $n$ features, we will have $n \cdot 2$ ranges, i.e., dimensions for projecting the decisions visible in Figure~\ref{fig:teaser}(c).

\textbf{Adaptive Boosting.} This algorithm fits successively many decision stumps or even decision trees to the training data, using various weights. The latter occurs only if the \emph{maximum depth} hyperparameter is set to $>1$% greater than one
. It begins by forecasting the original data set and weighting each observation equally. If the first decision stump's prediction is inaccurate, the observation that was mistakenly anticipated is given more weight. Because it is an iterative process with a specific improvement step controlled by the \emph{learning rate} hyperparameter, it will continue to add decision stumps until the \emph{number of trees/estimators} reaches a limit.

For each instance, AB calculates the weights. Each training example is given a weight to determine its importance in the training data set. When the given weights are substantial, that set of training instances is more likely to influence the training set and vice versa. All training instances will start with the same weight, defined as: $w_i = 1/N$. The weighted samples always add up to one, and each individual weight's value will be between 0 and 1.

The algorithm starts by looping for $m = 1$ to $M$, fitting a model $G_m(x)$ to the training data using weights $w_i$. After that, AB uses the method to calculate the real effect of this classifier in categorizing the training instances: $\alpha_m = \log((1 - \text{err}_m) / \text{err}_m)$, where $\text{err}_m$ is the total number of misclassifications for that training set divided by the training set size. Therefore, $\alpha_m$ represents how influential this stump will be in the final categorization. When a decision stump performs well or has no misclassifications, the error rate is 0 and the $\alpha$ value is relatively significant and positive. The alpha value will be 0 if the stump only classifies half correctly and half erroneously. Finally, the $\alpha$ would have a big negative number if the stump consistently produced misclassified data.

After plugging in the actual values of total error for each stump, the sample weights are updated. The following formula is used to accomplish that: $w_i = w_{i-1} \cdot \euler^{\alpha}$.
%$w_i = w_{i-1} \cdot \euler^{\pm\alpha}$
In detail, the new sample weight will be equal to the old sample weight multiplied by Euler's number, raised to $\alpha$. As explained in the previous paragraph, the two cases for $\alpha$ are: (a) positive when the predicted and the actual output agree or (b) negative when the predicted output does not agree with the actual class (i.e., the sample is misclassified). In the first case, the sample weight is decreased from what it was before, since the algorithm already performs well. In the second case, the sample weight gets increased so that the same misclassification does not repeat in the next stump. This procedure is followed so that the stumps are dependent on their predecessors. Lastly, the output for AB is the sum of all trees: $G(x) = \pm \bigl[ \sum_{m = 1}^M \alpha_m G_m(x) \bigr]$.

\textbf{Differences.} There are two key distinctions between bagging and boosting, which apply for the RF and AB algorithms. First of all, boosting changes the distribution of the training set adaptively based on the performance of previously constructed classifiers, whereas bagging changes the distribution of the training set stochastically. Second, boosting employs a function of a classifier's performance as a weight for voting, whereas bagging uses equal weight voting. Boosting appears to minimize both bias and variance, unlike bagging, which is primarily for variance reduction. Following the training of a decision stump, the weights of misclassified training examples are raised while the weights of correctly classified training examples are dropped in order to train the following decision stump. Thus, efforts to overcome the bias of the most recently generated weak model by concentrating greater attention on the samples that it misclassified gain traction. Because of its capacity to minimize bias, boosting works particularly effectively with high-bias but low-variance weak models. The fundamental issue with boosting appears to be resilience to noise.~\cite{Bauer1999Empirical} This is to be expected, as noisy cases are more likely to be misclassified, and their weight will rise as a result.

To sum up, for data with little noise, boosting is regarded as being stronger than bagging; nevertheless, bagging algorithms are far more resilient than boosting in noisy environments.~\cite{Kotsiantis2007Combining} To mitigate this problem, using both algorithms to derive decisions is a novel idea we adopt. In the future, we may expand our approach to experiment with any decision-based algorithm (\hl{as in prior works}~\cite{Kotsiantis2007CombiningBagging,Kotsiantis2011Combining}), but in this paper, we chose to combine two already widely-used algorithms (i.e., RF and AB).