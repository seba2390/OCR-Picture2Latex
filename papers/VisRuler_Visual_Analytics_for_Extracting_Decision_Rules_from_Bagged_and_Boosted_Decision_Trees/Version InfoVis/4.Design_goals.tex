\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{usecase2_teaser-crop.pdf}
 \caption{Extracting decision rules for manual evaluation with \textsc{VisRuler}: (a) panel with visual metaphors for selecting performant and diverse models; (b) box plot for feature selection according to per algorithmic importance; (c) visual embedding of computed decisions that training instances fall in due to their values; (d) vertical parallel coordinates plot that summarizes the rules with value ranges for each feature and highlights the current test instance; and (e) horizontal stacked bar chart for revealing the class agreement of each model against the manual decisions, together with the parallel coordinates plots for tuning hyperparameters and training new models.}
 \label{fig:teaser}
\end{figure*}

In this section, we specify the main design goals (\textbf{G1--G5}) upon which \textsc{VisRuler} (or any other VA system) should base its development regarding scenarios of extracting decision rules from bagging and boosting ensemble algorithms. Afterwards, we report the corresponding analytical tasks (\textbf{T1--T5}) that a user should be capable of performing while he/she obtains assistance and guidance from our proposed system. One important aspect of the design of \textsc{VisRuler} is the need for collaboration between different experts, which we also motivate here.

%This section starts with a brief motivation of why the collaboration between different experts is necessary, which is also considered by our VA system. 

\subsection{Target Groups}

In the InfoVis/VA communities, most of the research in explainable ML focuses on assisting \emph{ML experts and developers} in understanding, debugging, refining, and comparing ML models.~\cite{Chatzimparmpas2020A,Chatzimparmpas2020The} In this paper, we expand our method to involve another target group: the various \emph{domain experts} affected by the ML progress in fields such as finance, social care, and health care. With the growing adoption of ML in different areas, domain experts with little knowledge of ML algorithms might still want (or be required) to use them to assist in their decision-making. On the one hand, their trust in such decisions could be low due to a lack of in-depth knowledge on how models are learning from the training data. On the other hand, ML experts often have little prior knowledge about the data from particular domains. Thus, the primary goal of \textsc{VisRuler} is to combine the best of both worlds, i.e., to offer a solution that combines the above-mentioned benefits from both expert groups. More details about the collaboration between the ML and domain experts can be found in Section~\nameref{sec:overview}.

\subsection{Design Goals}

Our design goals originate from the analysis of the related work in Section~\nameref{sec:relwo}, especially the three design goals from Zhao et al.~\cite{Zhao2019iForest} (\textbf{G2 \& G3}) and the four questions from Ming et al.~\cite{Ming2019RuleMatrix} (\textbf{G1, G4, \& G5}) targeted to experts in domains such as health care, finance, security, and policymakers. 
%Additionally, our own experience in the development of VA tools~\cite{Chatzimparmpas2021StackGenVis,Chatzimparmpas2021VisEvol} for constructing powerful and diverse ML ensembles played a major role.
Our methodology is similar to Zhao et al.,~\cite{Zhao2019iForest} who reviewed 35 papers from the ML, visualization, and human-computer interaction (HCI) communities to come up with their decision goals.

\textbf{G1: Bring diverging models' performance to the spotlight.}
A VA system must first focus on what each model has learned in general; as such, the assessment of every model's performance (to decide which should remain under use) is a prerequisite. Models that fail to perform according to user-defined standards should not be part of the following procedure.

\textbf{G2: Disclose connections between features and predictions.}
VA systems should expose the features' impacts on predictions and allow humans to delete needless features based on that. During training, ML models learn different mappings between input features and resulting predictions based on the inherent mathematical functions used and the setting of hyperparameters. These mappings describe model behavior and help humans comprehend RF and AB models' properties.

\textbf{G3: Discover the core hidden operating processes.}
The underlying functioning processes of RF and AB models must also be revealed to check whether the models are working correctly and to understand why a given prediction was made. Before making a decision, humans should be able to audit the decision process of a prediction and ensure that they agree. Many RF and AB model interpretation problems can be resolved by analyzing the individual decision paths.

\textbf{G4: Reason about the relationship of certain features and knowledge acquired.}
Unlike \textbf{G2}, this one concentrates on deviations in ranking features for justifying the rule-based generated knowledge. VA systems can support humans with this alignment of features and knowledge extracted through the decision rules. Since domain experts may have knowledge and ideas based on years of research and study that current ML models do not make use of, the facilitation of communication between ML models and humans is a primary design goal.

\textbf{G5: Guide to unconfident and contradictory predictions.} 
This goal emerges when a model fails to perform well on particular test instances. In the production deployment of ML models, a rule that some models are confident about may not be generalizable. Although undesirable, it is relatively common for models to produce contradictory predictions for certain difficult-to-classify test instances. As a result, VA systems must advise users on which occasions each model failed to predict correctly.

\subsection{Analytical Tasks}
To fulfill our design goals, we have determined five analytical tasks that should be supported by our VA system (described in Section~\nameref{sec:overview}).

\textbf{T1: Compare the performance and architecture of models for selecting the most effective ones.}
Users should be able to compare different models with the support from various measurements (\textbf{G1}), as follows: (1) illustrate the performance of each model based on multiple validation metrics; (2) distill the number of false-positive and false-negative instances from the confusion matrix for every model; and (3) derive the number of decision trees and decision paths per model, to facilitate high-level comparison.
%, to compare their structure.

\textbf{T2: Investigate the contribution of global features according to different models and algorithms.}
Following the preceding task, users should be guided through the process of selecting important features  (\textbf{G2}). Thus, it is crucial to enable the comparison between per-algorithm and per-model feature importances.

\textbf{T3: Explore alternative clusters of decisions for global explanation and case-based reasoning.} The summarization of the decisions in a single view that combines the decisions of different algorithms and models should be accomplished to allow users to assess the influence of each decision (\textbf{G3}). For example, some decisions could overfit, and others could contain a mixture of instances falling in different classes. This last phenomenon increases their impurity. Users should be able to interact and explore this \emph{decisions space}.

\textbf{T4: Compare decision rules based on local feature ranking.} The global features described in~\textbf{T2} might not be similarly important for specific decisions, hence, local feature ranking via contrastive analysis~\cite{Zou2013Contrastive} could shed some light upon this task (\textbf{G4}). Moreover, the interpretation of rules extracted from the space of solutions (see~\textbf{T3}) could be achieved if users are capable of investigating the values of both training and testing instances.

\textbf{T5: Identify the different types of failure cases and confrontation via manual decisions.} Failure to converge to a certain result due to the disagreement of the ML models should be highlighted to users (\textbf{G5}). For instance, if there is no uniformity in the final decision or the majority voted for the wrong result, it could be that these instances are outliers, borderline cases, or simply misclassified; being able to explore such cases is essential.