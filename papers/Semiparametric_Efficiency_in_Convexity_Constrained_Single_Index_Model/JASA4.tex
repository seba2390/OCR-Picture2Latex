%% ADD before submitting
% \begin{filecontents*}{Supp_label_file.aux}
% \end{filecontents*}

   % } \affil[1]{Department of Statistics \& Data Science, CMU} \affil[2]{Department of Statistics, University of Pennsylvania}
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\input{header_arxiv}
\usepackage[numbers]{natbib}
\usepackage{blindtext}
\usepackage{nicefrac}

\usepackage{libertine}

\usepackage[font=small,labelfont=bf]{caption}



\usepackage[colorlinks,citecolor=blue,urlcolor=blue,linkcolor=red,naturalnames=true]{hyperref}
\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\usepackage{authblk}

\newcommand{\blind}{1}

\begin{document}
%% ADD before submitting
\numberwithin{equation}{section}
\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}
\spacingset{1.5} % DON'T change the spacing!

\if1\blind
{
\title{\bf Semiparametric Efficiency in Convexity Constrained Single Index Model}
\author[1]{Arun K. Kuchibhotla\thanks{Email: {\tt             arunku@cmu.edu}.}}
 \author[2]{ Rohit K. Patra\thanks{Email: {\tt rohitpatra@ufl.edu}.}} 
 \author[3]{Bodhisattva Sen\thanks{Email:{\tt bodhi@stat.columbia.edu} Supported by NSF Grants DMS-17-12822 and AST-16-14743.}}
\affil[1]{Carnegie Mellon University }
 \affil[2]{University of Florida}
 \affil[3]{Columbia University}
    \date{}
    \maketitle
  } \fi
  
  \if0\blind
  {    \title{Semiparametric Efficiency in Convexity Constrained Single Index Model}
  \date{}
        \maketitle
  } \fi

%  \author{Arun K. Kuchibhotla, Rohit K. Patra, and Bodhisattva Sen}
% \maketitle



\begin{abstract}
We consider estimation and inference in a single index regression model with an unknown convex link function.  We introduce a convex and Lipschitz constrained least squares estimator (CLSE) for both  the parametric and the nonparametric components given independent and identically distributed observations. We prove the consistency and find the rates of convergence of the CLSE when the errors are assumed to have only $q \ge 2$ moments and are allowed to depend on the covariates. When $q\ge 5$, we establish $n^{-1/2}$-rate of convergence and asymptotic normality of  the estimator of the parametric component.  Moreover,  the CLSE is proved to be semiparametrically efficient if the  errors happen to be homoscedastic. \if1\blind{We develop and implement a numerically stable and computationally fast algorithm to compute our proposed estimator in the R package~\texttt{simest}}\fi\if0\blind{We develop and implement a numerically stable and computationally fast algorithm to compute our proposed estimator in an R package}\fi. We illustrate our methodology through extensive simulations and data analysis. Finally,  our proof of efficiency is geometric and provides a general framework that can be used to prove efficiency of estimators in a wide variety of semiparametric models even when they do not satisfy the efficient score equation directly. 
 % Our proposed algorithm works even when $n$ is modest and $d$ is large (e.g., $n = 500$, and $d = 100$). 
 \end{abstract}
\noindent%
{\it Keywords:} {bundled parameter}; {errors with finite moments}; {geometric proof of semiparametric efficiency}; {Lipschitz constrained least squares};  {shape restricted function estimation}
% \end{frontmatter}
% {\bf Keywords:} Approximately least favorable sub-provided models, interpolation inequality,  penalized least squares,   shape restricted function estimation.
% \tableofcontents
\section{Introduction}\label{sec:intro}
Suppose  we have $n$ i.i.d.~observations $\{(X_i,Y_i)\in   \rchi\times \R, 1\le i\le n\}$
from the following single index regression model:
\begin{equation}\label{eq:simsl}
Y = m_0(\theta_0^{\top}X) + \epsilon,
\end{equation}
where $X \in   \rchi \subset \R^d$ ($d \ge 1$) is the predictor,  $Y \in \R$ is the response variable, and $\epsilon$ satisfies $\mathbb{E}(\epsilon|X) = 0$ and $\mathbb{E}(\epsilon^2|X) < \infty$ almost everywhere (a.e.) $P_X$, the distribution of $X$. We assume that the real-valued link function  $m_0$ and $\theta_0 \in \R^d$ are the unknown parameters of interest. 

Single index models are ubiquitous in regression because they provide convenient dimension reduction and interpretability. The single index model circumvents the curse of dimensionality encountered in estimating the fully nonparametric regression function $\E(Y|X = \cdot)$ by assuming that the link function depends on $X$ only through a one dimensional projection, i.e., $\theta_0^\top X$; see e.g.,~\cite{Powelletal89}. Moreover, the coefficient vector $\theta_0$ provides interpretability~\citep{liracine07} and the one-dimensional nonparametric link function $m_0$ offers some flexibility in modeling. The above model has received a lot of attention in statistics in the last few decades; see e.g.,~\cite{Powelletal89,LiDuan89,ICHI93,HardleEtAl93,Hristacheetal01,DelecroixEtal06,cuietal11,Patra16} and the references therein. The above papers propose estimators for the single index model under the assumption that $m_0$ is smooth (i.e., two or three times differentiable). 

 However, quite often in the context of a real application, qualitative assumptions on $m_0$ may be available. 
% {\color{red}The implicit assumption here that the conditional expectation of $Y$ given $X$ is only a function of $\theta_0^{\top}X$, although reduces the flexibility of full non-parametric model, compensates by a lot in quality of estimation and interpretation. Here the quality of estimation is in terms of the rate at which the regression function can be estimated. It is well-known from the results in \cite{Gyorfi02} that without the single index restriction, the best rate of convergence is $n^{-2/(4+d)}$ but with this restriction we are able to obtain $n^{-2/5}$. The single index model assumption also allows for recognizing relative importance of predictor variables in predicting $Y$.}
% In this paper, we assume further that $m_0$ is a convex Lipschitz function. This assumption is motivated by the fact that in a wide range of applications in various fields the regression function is known to be convex or concave.
 For example, in microeconomics, production and utility functions are often assumed to be concave and nondecreasing; concavity indicates decreasing marginal returns/utility~\citep{Varian84,Matzkin91,liracine07}. In finance, the relationship between call option prices and strike price are often known to be convex and decreasing~\citep{AD03}; in stochastic control, value functions are often assumed to be convex~\citep{K11}.  The following two real-data examples further illustrate that convexity/concavity constraints arise naturally in many applications.

\begin{figure}[ht!]
\centering
\includegraphics[width=.9\textwidth]{real_data_plot1stRev_before_WY.pdf}
%% scale=.8
  \caption[]{Scatter plots of $\{(X_i^\top\hat{\theta}, Y_i)\}_{i=1}^n,$ where $\hat{\theta}$ is the estimator of $\theta_0$ proposed in~\cite{MR2529970}. The plot is overlaid with the smoothing and regression splines based  function estimators of $m_0$ proposed in~\cite{Patra16} and~\cite{MR2529970}, respectively. Left panel: Boston housing data (see Section~\ref{sec:boston}); right panel: the car mileage data (see Section~\ref{sec:car}).}
  \label{fig:real_data_plot_prelim}
\end{figure}

\begin{example}[Boston housing data]\label{ex:boston}
\citet{harrison1978hedonic} studied the effect of different covariates on real estate price in the greater Boston area.  The response variable $Y$ was the log-median value of homes in each of the $506$ census tracts in the Boston standard metropolitan area. A single index model is  appropriate for this dataset; see e.g.,~\cite{gu2015oracally,MR2529970,MR2589322,MR2787613}. The above papers considered the following covariates in their analysis: average number of rooms per dwelling, full-value property-tax rate per $10000$ USD, pupil-teacher ratio by town school district, and proportion of population that is of ``lower (economic) status'' in percentage points. In the left panel of Figure~\ref{fig:real_data_plot_prelim}, we provide the scatter plot of  $\{(Y_i, \hat{\theta}^\top X_i)\}_{i=1}^{506}$, where $\hat{\theta}$ is the estimate of $\theta_0$ obtained in~\cite{MR2529970}. We also plot estimates of $m_0$ obtained from~\cite{Patra16} and~\cite{MR2529970}. The plot suggests a convex and nondecreasing relationship between the log-median home prices and the index, but the fitted link functions satisfy these shape constraints only approximately. 

  
\end{example}

\begin{example}[Car mileage data]\label{ex:car}
  \citet{cars_1983} consider a dataset containing mileages of different cars. The data contains mileages of $392$ cars as well as the following covariates:  displacement,   weight,  acceleration,   and horsepower.  \citet{MR2957294} and \cite{Patra16} have fit a partial linear model and a  single index model, respectively. In the right panel of Figure~\ref{fig:real_data_plot_prelim}, we plot the estimators  proposed in~\cite{Patra16} and~\cite{MR2529970}. Both of these works consider estimation in the single index model under only smoothness assumptions. The ``law of diminishing returns'' suggests $m_0$ should be convex and nonincreasing. However, as observed in Figure~\ref{fig:real_data_plot_prelim}, the estimators based only on smoothness assumptions satisfy this shape constraint only approximately. 


\end{example}
% subsection example_2_car_mileage_data (end)
In both of the examples, the smoothing based estimators do not incorporate the known shape of the nonparametric function. Thus the estimators are not guaranteed to be convex (or monotone) in finite samples. Moreover, the choice of the tuning parameter in smoothness based estimators is tricky as different values for the tuning parameter lead to very different shapes. This unpredictable behavior makes the smoothness  based  estimators of $m_0$ less interpretable, and motivates the study of a convexity constrained single index model. {We discuss these two datasets and our analysis in more detail in Sections~\ref{sec:boston} and~\ref{sec:car}.}

In this paper, we propose constrained least squares estimators for $m_0$ and $\theta_0$ that is guaranteed to satisfy the inherent convexity constraint in the link function everywhere. The proposed methodology is appealing for two main reasons: (1) the estimator is interpretable and takes advantage of naturally occurring qualitative constraints; and (2) unlike smoothness based estimators, the proposed estimator is highly robust to the choice of the tuning parameter without sacrificing efficiency. 

In the following, we conduct a systematic study of the computation, consistency, and rates of convergence of the estimators, under mild assumptions on the covariate and error distributions. We further prove that the estimator for the finite-dimensional parameter $\theta_0$ is asymptotically normal. Moreover, this estimator is shown to be semiparametrically efficient if the errors happen to be homoscedastic, i.e., when $\mathbb{E}(\epsilon^2|X) \equiv \sigma^2$ a.e.~for some constant $\sigma^2$. It should be noted that in the examples above the link function is also known to be monotone. To keep things simple, we focus on  only convexity constrained single index model. However, \textit{all} our results continue to hold under the additional monotonicity assumption, i.e., our conclusions hold for convex/concave and nondecreasing/nonincreasing $m_0$. {More generally, our results continue to hold under \textit{any} additional shape constraints; see Remarks~\ref{rem:Monotone},~\ref{rem:Efficency_additional}, and~\ref{rem:Add_mono} and Section~\ref{sec:real_data_analysis} in the paper for more details.}
 % 

% estimation and inference in shape-restricted single index models has not received much attention.




One of the main contributions of this paper is our novel geometric proof of the semiparametric efficiency of the constrained least squares estimator. Note that proving semiparametric efficiency of constrained (and/or penalized) least squares estimators often requires a delicate use of the structure of the estimator of the nonparametric component (say $\hat m$) to construct {\it least favorable paths}; see e.g.~\cite{VANC},~\cite[Chapter 9.3]{VdV02}, and~\cite{MR1394975} (also see Example~\ref{ex:coxmodel}). In contrast, our approach is based on the following simple observation. For a traditional smoothness based estimator $\hat{m}$, the path $t \mapsto \hat{m} + t a$ will belong to the (function) parameter space for {\it any} smooth ``perturbation'' $a$ (for small enough $t \in (-1,1)$). However this is no longer true when the underlying parameter space is constrained. But,  observe that the projection of $\hat{m} + t a$ onto the constrained function space certainly yields a ``valid" path. Our proof technique is based on differentiability properties of the path $t\mapsto \Pi (\hat{m} + ta)$, where $\Pi$ denotes the $L_2$-projection onto the (constrained) function space. This general  principle is applicable to other shape constrained semiparametric models, because differentiability of the projection operator is well-studied  in the context of constrained optimization algorithms; see~Section~\ref{sub:_semiparametric_eff_shape} below for a more detailed discussion. {Also see Example~\ref{ex:coxmodel}, where we discuss the applicability of our technique in (re)proving the semiparametric efficiency of the nonparametric maximum likelihood estimator in the Cox proportional hazard model under current status censoring~\cite{MR1394975}.}
 %Due to the assumed shape constraint on $m_0$, the parametric submodels for the link function are nonlinear and the nuisance tangent space is intractable.}
To be more specific, we study the following Lipschitz constrained convex least squares estimator (CLSE): 
\begin{equation}\label{eq:CLSE}
(\check{m}_{L},\check{\theta}_{L}) \coloneqq \argmin_{(m,\theta) \in \M_{L} \times\Theta} Q_n(m, \theta),
\end{equation}
where
\begin{equation}\label{eq:CLSE-2}
Q_n(m, \theta)\coloneqq \frac{1}{n} \sum_{i=1}^n \{Y_i - m(\theta^\top X_i)\}^2
\end{equation}
and  $\M_L$ denotes the class of all $L$-Lipschitz real-valued convex functions on $\R$ and
\be\label{smooth:eq:theta}
\Theta \coloneqq \{\eta=(\eta_1, \ldots, \eta_d) \in\mathbb{R}^d: |\eta|=1\mbox{ and }\eta_{1}\ge 0 \} \subset S^{d-1}.
\ee 
Here $| \cdot |$ denotes the usual Euclidean norm, and  $S^{d-1}$ is the Euclidean unit sphere in $\mathbb{R}^d$. The norm-1 and the positivity constraints are necessary for identifiability of the model\footnote{\label{foo:ident} Without any sign or scale constraint on $\Theta$ no $(m_0, \theta_0)$ will be identifiable. To see this, fix any $(m_0, \theta_0)$ and define $m_1(t) \coloneqq m_0(-2t)$ and $\theta_1 = -\theta_0/2$, then  $m_0(\theta_0^\top \cdot)\equiv m_1(\theta_1^\top \cdot)$; see~\cite{carroletal97}, \cite{cuietal11}, and~\cite{MR2369025} for identifiability of the model~\eqref{eq:simsl}. Also see Section~\ref{sec:ident} for further discussion.}. 

The Lipschitz constraint in~\eqref{eq:CLSE} is not restrictive as all convex functions are Lipschitz in the interior of their domains. Furthermore in shape-constrained single index models,  the Lipschitz constraint is known to  lead to computational advantages~\citep{kalai2009isotron,kakade2011efficient,lim2014convergence,ganti2015learning,mazumder2019computational}. 
% It should be noted that  the additional Lipschitz assumption is not generally required when studying the nonparametric convex regression model (see e.g.,~\cite{MR2850215}) but this restriction eases the theoretical analysis in this semiparametric model.
{ Additionally on the theoretical side, the Lipschitzness assumption allows us to control the behavior of  the estimator near the boundary of its domain. This control is crucial for establishing semiparametric efficiency. To the best of our knowledge, this is the first work proving semiparametric efficiency for an estimator in a \textit{bundled parameter} problem (where the parametric and nonparametric components are intertwined; see \cite{huang1997interval}) where the nonparametric estimate is shape constrained and non-smooth.} % The asymptotic normality of $\check{\theta}_{L,n}$ can be readily used to  construct confidence intervals for ${\theta}_0$.} 
%As any convex function is Lipschitz in the interior of its domain, Our estimator $(\check{m}_{L},\check{\theta}_{{L}})$ can be thought of as a shape-constrained nonparametric LSE for the parameters $m_0$ and $\theta_0$ in model~\eqref{eq:simsl}. 
Note that the convexity constraint in~\eqref{eq:CLSE} leads to a convex piecewise affine estimator $\check{m}_{L}$ for the link function $m_0$; see Section~\ref{sec:CLSE} for a detailed discussion. 

% ~\citet{groeneboom2016current} propose a $\sqrt{n}$-consistent and asymptotically normal but \textit{inefficient} estimator of the index vector in the current status model based on the (non-smooth) maximum likelihood estimate (MLE) of the nonparametric component under just monotonicity constraint.  They also propose two other estimators of the index vector based on kernel smoothed versions of the MLE for the nonparametric component.  Although these estimators do not achieve the efficiency bound their asymptotic variances can be made arbitrarily close to the efficient variance.



% \subsection{Summary of our contributions} % (fold)
% \label{sub:contributions_of_the_paper}

% In this paper, we study the CLSE~\eqref{eq:CLSE}. 
Our theoretical and methodological study can be split in two broad categories. In Section~\ref{sec:CLSE}, we find the rate of convergence of the CLSE as defined in~\eqref{eq:CLSE}, whereas in Section~\ref{sec:SemiInf} we establish the asymptotic normality and semiparametric efficiency of $\check{\theta}_{L}$. Suppose that $m_0$ is $L_0$-Lipschitz, i.e., $m_0 \in \M_{L_0}$. If the tuning parameter $L$ is chosen such that $L \ge L_0$, then under mild distributional assumptions on $X$ and $\epsilon$, we show that $\check{m}_{L}$ and $\check{m}_{L}(\check{\theta}_{L}^\top \cdot)$ are minimax rate optimal for estimating $m_0$  and $m_0(\theta_0^\top \cdot)$, respectively; see Theorems~\ref{thm:rate_m_theta_CLSE} and~\ref{thm:ratestCLSE}. We also allow for the tuning parameter $L$ to depend on the data and show that the rate of convergence of $\check{m}_{L}(\check{\theta}_{L} \cdot)$ is uniform in $ L\in [L_0, nL_0]$, up to a $\sqrt{\log\log n}$ multiplicative factor; see Theorem~\ref{thm:UniformLRate}. This result justifies the usage of a data-dependent choice of $L$, such as cross-validation. Additionally, in Theorem~\ref{thm:rate_derivCLSE}, we find the rate of convergence of $\check{m}'_{L}.$ In Section~\ref{sec:SemiInf}, we establish that if $L\ge L_0$, then  $\check{\theta}_{L}$  is $\sqrt{n}$-consistent and $n^{1/2}(\check{\theta}_L - \theta_0)$ is asymptotically normal with mean $0$ and finite variance; see Theorem~\ref{thm:Main_rate_CLSE}. The asymptotic normality of $\check{\theta}_{L}$ can be readily used to  construct confidence intervals for ${\theta}_0$. Further, we show that  if the errors happen to be homoscedastic, then $\check{\theta}_{L}$ is semiparametrically efficient. 

Our contributions on the computational side are two fold. In Section~\ref{sec:compute} of the supplementary file, we propose an alternating descent algorithm for estimation in the single index model~\eqref{eq:simsl}. Our descent algorithm works as follows: when $\theta$ is fixed, the $m$ update is obtained by solving a quadratic program with linear constraints, and when $m$ is fixed, we update $\theta$ by taking a small step on the Stiefel manifold $\Theta$ with a guarantee of descent. \if1\blind  {We implement the proposed algorithm in the R package \texttt{simest}}\fi
  \if0\blind  {We implement the proposed algorithm in the R package {\texttt{***}}}\fi. Through extensive simulations (see Section~\ref{sec:Simul_Cvx} and Section~\ref{app:add_simul} of the supplementary file) we show that the finite sample performance of our estimators is robust to the choice of the tuning parameter $L$.  Thus we think the practitioner can choose $L$ to be very large without sacrificing any finite sample performance. Even though the minimization problem is non-convex, we illustrate that the proposed algorithm (when used with multiple random starting points) performs well in a variety of simulation scenarios when compared to existing methods.




\subsection{Semiparametric efficiency and shape constraints} % (fold)
\label{sub:_semiparametric_eff_shape}
Although estimation in single index models under smoothness assumptions is well-studied (see e.g.,~\cite{Powelletal89,LiDuan89,ICHI93,HardleEtAl93,Hristacheetal01,DelecroixEtal06,MR2529970,cuietal11} and the references therein), estimation and efficiency in shape-restricted single index models have not received much attention. The earliest reference on this topic we could find was the work of \citet{VANC}, where the authors considered a penalized likelihood approach in the current status regression model (which is similar to the single index model) with a monotone link function.~\citet{CHSA} consider maximum likelihood estimation in a generalized additive index model (a more general model than \eqref{eq:simsl}) and only prove consistency of the proposed estimators.  In~\citet{2016arXiv161006026B}, the authors study model~\eqref{eq:simsl} under  monotonicity constraint and prove $n^{1/3}$-consistency of the LSE of $\theta_0$; however they do not obtain the limiting distribution of the estimator of $\theta_0.$ \citet{2017arXiv171205593B} propose a tuning parameter-free $\sqrt{n}$-consistent (but not semiparametrically efficient) estimator for the index parameter in the monotone  single index model. 

% subsection _se_ (end)


In this paper, we show that $\check{\theta}_L$ is semiparametrically efficient under homoscedastic errors. Our proof of the semiparametric efficiency is novel and can be applied to other semiparametric models when the estimator does not readily satisfy the efficient score equation. In fact, we provide a new and general technique for establishing semiparametric efficiency of an estimator when the nuisance tangent set is not the space of all square integrable functions. The basic idea is as follows. Suppose $\ell_{\theta_0, m_0}(y, x)$ represents the semiparametrically efficient influence function, meaning that the ``best'' estimator $\tilde{\theta}$ of $\theta_0$ satisfies the following asymptotic linear expansion:
\begin{equation}\label{eq:eff_intro}
\eta^\top (\tilde{\theta} - \theta_0) = \frac{1}{n}\sum_{i=1}^n \eta^\top \ell_{\theta_0, m_0}(Y_i, X_i) + o_p(n^{-1/2}),
\end{equation}
for every $\eta\in \R^d$. A crucial step in establishing that $\check{\theta}_L$ satisfies~\eqref{eq:eff_intro} is to show for any $\eta\in\mathbb{R}^d$,
\[n^{-1}\sum_{i=1}^n  \eta^{\top}\ell_{\check{\theta}_L, \check{m}_L}(Y_i, X_i) = o_p(n^{-1/2}),\]
 i.e., $\check{\theta}_L$ is an \emph{approximate zero} of the efficient score equation~\cite[Theorem 6.20]{VdV02}. Because $(\check{m}_L,\check{\theta}_L)$ minimizes $(m,\theta) \mapsto Q_n(m,\theta)$ over $\mathcal{M}_L\times\Theta$, the traditional way to prove the approximate zero property is to use the fact that $\partial Q_n(\check{m}_L + t a, \check{\theta}_L + t\eta)/\partial t|_{t = 0} =0$ for all perturbation ``directions'' $(a, \eta)$ and find an $a$ such that the derivative of $t\mapsto Q_n(\check{m}_L + ta, \check{\theta}_L + t\eta)$ at $t=0$  is $n^{-1}\sum_{i=1}^n \eta^\top \ell_{\check{\theta}_L, \check{m}_L}(Y_i, X_i)$; see e.g.,~\cite{NeweyStroker93}. {In fact, using this method one can often show that the estimator satisfies the efficient score equation \textit{exactly}.} If $\check{m}_L+ta$ is a valid path (i.e., $\check{m}_L+ t a \in \M_L$ for all $t$ in some neighborhood of zero) for an arbitrary but ``smooth'' $a$ then it is relatively straightforward to establish the approximate zero property~\citep{NeweyStroker93}.\footnote{ As $\theta\in \Theta$ is restricted to have norm $1$, ${\theta} + t \eta$ does not belong to the parametric space for $t\neq 0$ and $\eta^\top {\theta} \ne 0$. However, this can be easily remedied by considering another path that is differentiable and has the same ``direction''; we define such a path in~\eqref{eq:path_para}.} 
However, this approach does not work when the nonparametric function $m_0$ is constrained. This is because under constraints,  $\check{m}_L + t a$ might not be a valid path for arbitrary but smooth $a$.
% The novelty of our proposed approach lies in using the fact that  $\check{m}_L + t a$ might not be a valid path but $\Pi_{\mathcal{M}_L}(\check{m}_L + ta)$ is always a valid path, here $\Pi_{\mathcal{M}_L}(f)$ for any $f\in L_2$ denotes the projection of $f$ into $\mathcal{M}_L$.
 The novelty of our proposed approach lies in observing that in contrast to $t\mapsto\check{m}_L + ta$, $t \mapsto\Pi_{\mathcal{M}_L}(\check{m}_L + ta)$ is always a valid path for every smooth $a$; here $\Pi_{\mathcal{M}_L}(f)$ is the $L_2$-projection of $f$ onto $\mathcal{M}_L$. Thus if $t\mapsto \Pi_{\mathcal{M}_L}(\check{m}_L + ta)$ is differentiable, then $\partial Q_n(\Pi_{\mathcal{M}_L}(\check{m}_L + ta), \check{\theta}_L + t\eta)/\partial t|_{t = 0} = 0$ for any perturbation $(a,\eta)$. Then establishing that $\check{\theta}_L$ is an approximate zero boils down to finding an $a$ such that 
 \[
 \frac{\partial }{\partial t}Q_n(\Pi_{\mathcal{M}_L}(\check{m}_L + ta), \check{\theta}_L + t\eta)\Big|_{t = 0}= n^{-1}\sum_{i=1}^n \eta^\top  \ell_{\check{\theta}_L, \check{m}_L}(Y_i, X_i) +o_p(n^{-1/2}).
 \]
  Differentiability of projection operators is well-studied; e.g., see~\cite{dharanipragada1996quadratically,fitzpatrick1982differentiability,mccormick1972gradient,shapiro1994existence,sokolowski1992shape} for sufficient conditions for a general projection operator to be differentiable. The generality and the usefulness of our technique can be understood from the fact that no specific structure of $\check{m}_L$ or $\mathcal{M}_L$ is used in the previous discussion; we elaborate on this in Section~\ref{sec:SemiCLSE}. On the other hand, existing methods (see e.g.,~\cite{VANC}) require delicate (and not generalizable) use of the structure of the nonparametric estimator to create valid paths around the nonparametric function; see e.g.,~\cite{VANC} for semiparametric efficiency in  current status regression, and~\cite[Chapter 9.3]{VdV02} and~\cite{MR1394975} for efficiency in the Cox proportional hazard model with current status data; see Example~\ref{ex:coxmodel}. 


% Our method does not require the user to find the exact characterization of the nuisance tangent space but only a large enough subset of the nuisance tangent space (so that $\ell_{\check{m},\check{\theta}}$ is close to this subset, as above). This can be very useful because in many  cases when the estimator is non-smooth and/or the estimator satisfies shape constraints. Because in such cases the nuisance tangent space (at the estimator)  can be quite complicated and hard to characterize (see cox prop hazard model; will look after class).


% The proof of Theorem~\ref{thm:Main_rate_CLSE} is complicated as the nuisance tangent space at $\check{m}_{L}$ is not easy to quantify.  This behavior is due to the fact that $\check{m}_{L}$ (a piecewise affine function) lies on the boundary of the set of all convex functions, i.e., the nuisance tangent space at $\check{m}_{L}$ is not $L_2(\Lambda)$ {\clr What is $\Lambda$?}; see Section~\ref{sec:SemiCLSE} for more details. {\clr Anything lost if we remove this$\rightarrow$ Our analysis is further complicated by the fact that $m_0$ and $\theta_0$ are \textit{bundled} (where the parametric and nonparametric components are intertwined; see~\cite{huang1997interval})}.  



% \todo[inline]{In summary...}



% subsection contributions_of_the_paper (end)

\subsection{Organization of the exposition} % (fold)
\label{sub:description_of_exposition}

% subsection description_of_exposition (end)
Our exposition is organized as follows: in Section \ref{sec:Estim}, we introduce some notation and formally define the CLSE. In Section \ref{sec:CLSE}, we state our assumptions, prove consistency, and give rates of convergence for the  CLSE. 
% In Section~\ref{sec:AsymRegFunEstimate} we analyze $\check{m}_{L_n,n}(\check{\theta}_{L_n,n}^\top \cdot)$, the estimator for the regression function $m_0(\theta_0^\top \cdot)$,  and in Section~\ref{sec:SepPara} we analyze the CLSE for $m_0$ and $\theta_0$ separately. 
In Section~\ref{sec:SemiInf}, we detail our new method to prove semiparametric efficiency of the CLSE.  We  use this to prove $\sqrt{n}$-consistency, asymptotic normality,  and efficiency (when the errors happen to be homoscedastic) of the CLSE of $\theta_0$. We discuss an algorithm to compute the proposed estimator in Section~\ref{sec:compute}. In Section~\ref{sec:Simul_Cvx}, we provide an extensive simulation study and compare the finite sample performance of the proposed estimator with existing methods in the literature. In Section~\ref{sec:real_data_analysis}, we analyze the Boston housing data \cite{harrison1978hedonic} and the car mileage data \cite{cars_1983} introduced in Examples~\ref{ex:boston} and~\ref{ex:car} in more details. In both of the cases,  we show that the natural shape constraint leads to stable and interpretable estimates.  Section~\ref{sec:discussion} provides a brief summary of the paper and discusses some open problems. 

{Section numbers in the supplementary file are prefixed with ``S.". Section~\ref{app:sketchCLSE} of the supplementary file provides some insights into the proof of Theorem~\ref{thm:Main_rate_CLSE},  one of our main results. 
Section~\ref{app:add_simul} provides further simulation studies. Section~\ref{sec:proof:Estims}  provides additional discussion on the identifiability of the parameters. Sections~\ref{app:proof:existanceCLSE}--\ref{sec:proof_semi} contain the proofs of our results.  Section~\ref{sec:proof_of_eq:app_score_equation}  completes our novel proof of semiparametric efficiency sketched in Section~\ref{sec:SemiCLSE}.}

\section{Notation and Estimation}\label{sec:Estim}
\subsection{Preliminaries} \label{sec:prelim}
% \subsection{Notation}

In what follows, we assume that we have i.i.d.~data $\{(X_i,Y_i)\}_{i=1}^{n}$ from~\eqref{eq:simsl}. We start with some notation. Let  $\rchi \subset \mathbb{R}^d$ denote the support of $X$ and define
\begin{equation}\label{eq:Defn_Q_D}
D \coloneqq\text{conv} \{\theta^{\top}x :\,  x\in\rchi, \theta\in \Theta\},\quad D_{\theta}\coloneqq \{ \theta^\top x: x \in \rchi\}, \quad\text{and}\quad D_{0} \coloneqq D_{\theta_0},
\end{equation}
 where $\text{conv}(A)$ denotes the convex hull of the set $A$. Let $\M_L$ denote the class of real-valued convex functions on $D$ that are uniformly Lipschitz with Lipschitz bound $L.$ For any $m\in \M_L$, let $m^\prime$ denote the nondecreasing right derivative of the real-valued convex function $m$. Because $m$ is a uniformly Lipschitz function with Lipschitz constant $L$, without loss of generality, we can assume that $|m'(t)|\leq L$, for all $t\in D.$ We use $\p$ to denote the probability of an event and $\E$ for the expectation of a random quantity. For any $\theta\in\Theta$, let $P_{\theta^\top X}$ denote the distribution of $\theta^\top X$.  For $g : \rchi \to \mathbb{R}$, define $  \|g\|^2 \coloneqq \int g^2(x) dP_X(x).$ Let $P_{\epsilon, X}$ denote the joint distribution of $(\epsilon, X)$ and let $P_{\theta,m}$ denote the joint distribution of $(Y,X)$ when $Y = m(\theta^\top X) +\epsilon,$ where $\epsilon$ is defined in~\eqref{eq:simsl}. In particular,  $P_{\theta_0, m_0}$ denotes the joint distribution of $(Y,X)$ when  $X\sim P_X$ and $(Y,X)$ satisfies \eqref{eq:simsl}.  For any set $I \subseteq \mathbb{R}^p$ ($p\ge 1$) and any function $g: I \to \mathbb{R}$, we define $\|g\|_{\infty} \coloneqq \sup_{u \in I} |g(u)|$ and $\|g\|_{I_1} \coloneqq \sup_{u \in I_1} |g(u)|,$ for $I_1 \subseteq I.$  The notation $a\lesssim b$ is used to express that $a\le C b$ for some  constant $C>0$.  For any function $f:\rchi\rightarrow\R^r, r\ge1$, let $\{f_i\}_{1\le i \le r}$ denote each of the components of $f$, i.e.,  $f(x)= (f_1(x), \ldots, f_r(x))$ and  $f_i: \rchi \to \R$. We define $\| f\|_{2, P_{\theta_0, m_0}}\coloneqq \sqrt{ \sum_{i=1}^r \|f_i\|^2}$  and $\| f\|_{2, \infty}\coloneqq \sqrt{ \sum_{i=1}^r \|f_i\|^2_\infty}.$ For any  function $g:D\rightarrow\R$ and $\theta\in\Theta$, we define $(g\circ\theta)(x) \coloneqq g(\theta^{\top}x),$ for all $ x \in \rchi.$ We use the following (standard) empirical process theory notation. For any function $f: \R\times \rchi\rightarrow \R$, $\theta \in \Theta,$ and $m :\R \rightarrow \R$, we define \[P_{\theta, m} f \coloneqq \int f(y,x) dP_{\theta, m}(y,x).\] Note that   $P_{\theta, m} f$ can be a random variable when $\theta$ or $m$ or both are random. Moreover, for any function $f: \R\times \rchi\to \R$, we define $\p_nf \coloneqq n^{-1}\sum_{i=1}^n f(Y_i, X_i)$ and $\g_n f\coloneqq {\sqrt{n}} (\p_n -P_{\theta_0,m_0}) f.
$
\subsection{Identifiability}\label{sec:ident} We now discuss the identifiability of $m_0\circ\theta_0$ and $(m_0, \theta_0)$. Letting $Q(m,\theta) \coloneqq \E[Y -m(\theta^\top X)]^2,$ observe that $(m_0, \theta_0)$ minimizes $Q(\cdot, \cdot).$ In fact we can show in Section~\ref{proof:lem:Ident_cvxSim}, that
\begin{equation}\label{eq:true_mimina}
\inf_{\{(m, \theta):\; m\circ\theta \in L_2(P_X) \text{ and } \|m\circ\theta - m_0\circ\theta_0\| > \delta\}} \big[Q(m,\theta) - Q(m_0,\theta_0)\big] > \delta^2, \quad \text{for any} \quad \delta > 0.
\end{equation}
This implies that $m_0\circ\theta_0$ is always identifiable and further, one can hope to consistently estimate $m_0\circ\theta_0$ by minimizing  the sample version of $Q(m, \theta)$; see~\eqref{eq:CLSE}.

Note that the identification of $m_0\circ\theta_0$ does not guarantee that both $m_0$ and $\theta_0$ are separately identifiable. Hence, in what follows, when dealing with the properties of separated parameters, we will directly assume:
\begin{enumerate}[label=\bfseries (A\arabic*)]
\setcounter{enumi}{-1}
 \item  The parameters $m_0\in \M_{L_0}$ and $\theta_0\in \Theta$ are separately identifiable, i.e., $m\circ \theta = m_0\circ\theta_0$ for some $(m, \theta)\in \M_{L_0}\times \Theta$ implies that $m = m_0$ and $\theta= \theta_0$.\label{a0}
 \end{enumerate} 
 \citet{ICHI93} has found general sufficient conditions on the distribution of $X$ under which~\ref{a0} holds; these sufficient conditions allow for some components of $X$ to be discrete, also see~\citet[Pages 12--17]{Horowitz98} and~\citet[Proposition 8.1]{liracine07}. When $X$ has a density with respect to Lebesgue measure,~\citet[Theorem 1]{LinKul07} find a simple sufficient condition for~\ref{a0}. We  discuss and compare these two sufficient conditions in Section~\ref{sec:IdentDisc} of the supplementary file.

%{\color{red} \cite{ICHI93} gives conditions for identifiability of $\theta$ and $m$.  Should we just use the same conditions?}


%\section{Two estimators} \label{sec:Estim}
%In Lemma~\ref{lem:Ident_cvxSim}, we show that $(m_0, \theta_0)$ minimizes the population version of the square error loss. In this section, we use $Q_n(m,\theta),$ the empirical version of the square error loss, to formally define the CLSE and the PLSE.
% using the notation developed in Section \ref{sec:prelim}.
\section{Convex and Lipschitz constrained LSE} \label{sec:CLSE}
Recall that CLSE is defined as the minimizer of $(m, \theta) \mapsto Q_n(m, \theta)$ over $\M_L\times \Theta$. Because $Q_n(m, \theta)$ depends   only on the values of the function at  $\{{\theta} ^\top X_i\}_{i=1}^n$,  it is immediately clear that  the minimizer $\check{m}_{{L}}$ is  unique only at $\{\check{\theta}_{{L}} ^\top X_i\}_{i=1}^n$. Since $\check{m}_{{L}}$ is restricted to be convex, we interpolate the function linearly between $\check{\theta}_{{L}} ^\top X_i$'s  and extrapolate the function linearly outside the data points.\footnote{\label{foo:extrapolate}Linear interpolation/extrapolation does not violate the convexity or the $L$-Lipschitz property} Thus $\check{m}$ is piecewise affine. In Section~\ref{app:proof:existanceCLSE} of the supplementary file,  we prove the existence of the minimizer in \eqref{eq:CLSE}. The optimization problem~\eqref{eq:CLSE} might not have a unique minimizer and the results that follow hold true for any global minimizer. 
 % In Sections \ref{sec:AsymRegFunEstimate} and  \ref{sec:SemiCLSE} we study the  $(\check{m}_{{L}},\check{\theta}_{{L}})$ is a consistent estimator of $(m_0, \theta_0)$ and study its asymptotic properties, respectively.
\begin{remark}\label{rem:compute}
 For every fixed $\theta$,  $m (\in  \M_L) \mapsto Q_n(m,\theta)$ has a unique minimizer. The minimization over the class of uniformly Lipschitz functions is a quadratic program with linear constraints and can be computed easily; see Section~\ref{sec:CLSE_comp}.
\end{remark}

\subsection{Asymptotic analysis of the regression function estimate}\label{sec:AsymRegFunEstimate}
In this section, we study the asymptotic behavior of $\check{m}_{{L}}\circ\check{\theta}_{{L}}$. We will now list the assumptions under which we study the rates of convergence of the CLSE for the regression function.
\begin{enumerate}[label=\bfseries (A\arabic*)]
\setcounter{enumi}{0}
\item \label{aa1_new} The unknown convex link function $m_0$ is bounded by some constant $M_0$ $ (\ge 1)$ on $D$ and is uniformly Lipschitz with Lipschitz constant $L_0$. 
\item \label{aa1} The support of $X$, $\rchi$, is a  subset of $\mathbb{R}^d$ and  $\sup_{x\in \rchi} |x| \le T,$ for some finite $T\in \R$.
\item \label{aa2}The error $\epsilon$ in model~\eqref{eq:simsl} has finite  $q$th moment, i.e., $K_q\coloneqq \big[\E(|\epsilon|^q)\big]^{1/q}  < \infty$ where $q\ge 2$. Moreover, $\mathbb{E}(\epsilon|X) = 0,$ $P_X$ a.e.~and $\sigma^2(x)\coloneqq \E(\epsilon^2|X=x) \le \sigma^2 < \infty$ for all $x\in \rchi.$ 

\end{enumerate}

The above assumptions deserve comments. \ref{aa1} implies that the support of the covariates is bounded. 
% However, using arguments similar to Remark 4.2 (and Section 9.2) of~\cite{2016arXiv161006026B} one can relax assumption~\ref{aa1} and allow for distribution $X$ to be sub-Gaussian. {\clr Write some arguments?} 
In assumption~\ref{aa2}, we allow $\epsilon$ to be heteroscedastic and $\epsilon$ can depend on $X$. Our assumption on $\epsilon$ is more general than those considered in  the shape constrained literature, most works assume that all moments of $\epsilon$ are finite and ``well-behaved'', see e.g., \cite{2017arXiv171205593B},~\cite{Hristacheetal01}, and~\cite{Xiaetal02}.

Theorem \ref{thm:rate_m_theta_CLSE} (proved in Section~\ref{proof:rate_m_theta_CLSE}) below  provides an upper bound on the rate of convergence of $\check{m}_L\circ\check{\theta}_L$ to $m_0\circ\theta_0$ under the $L_2(P_X)$ norm. The following result is a finite sample result and shows the explicit dependence of the rate of convergence on $L = L_n, d,$ and $q$.

\begin{thm}\label{thm:rate_m_theta_CLSE}
Assume \ref{aa1_new}--\ref{aa2}. Let $\{L_n\}_{n\ge 1}$ be a fixed sequence such that $L_n\ge L_0$ for all $n$ and let 
\begin{equation}\label{eq:r_n_def}
r_n\coloneqq\min \left\{  \frac{n^{2/5}}{d^{2/5}L_n},  \frac{n^{1/2-1/2q}}{L_n^{(3q+ 1)/(4q)}} \right\}.
\end{equation}
 Then for every $n\ge 1$ and $u\ge 1$, there exists a constant $\mathfrak{C}\ge 0$ depending only on $\sigma, M_0, L_0, T,$ and  $K_q$, and constant $C$ depending only on $K_q, \sigma$, and $q,$ such that
  \[ \sup_{\theta_0, m_0,\epsilon, X} \p\left( r_n \|\check{m}_{L_n}\circ\check{\theta}_{L_n} - m_0\circ\theta_0\| \ge u \mathfrak{C}\right) \le \frac{C}{u^{q}}+ \frac{\sigma^2}{n},\]
where the supremum is taken over all $\theta_0\in \Theta$ and all joint distributions of $(\epsilon,X)$ and parameters $m_0$ for which assumptions~\ref{aa1_new}--\ref{aa2} are satisfied with constants $\sigma, M_0, L_0, T,$ and  $K_q$. In particular if $q\ge 5$, $d = O(1),$ and $L_n = O(1)$ as $n\to\infty$, then $\|\check{m}_{L_n}\circ\check{\theta}_{L_n} - m_0\circ\theta_0\| = O_p({n^{-2/5}}).$
\end{thm}
Note that~\eqref{eq:r_n_def} allows for the dimension $d$ to grow with $n$ and $\theta_0$ to change with $n$. For example if $L_n\equiv L$ for some fixed $L\ge L_0$, then we have that $\|\check{m}_{L_n}\circ\check{\theta}_{L_n} - m_0\circ\theta_0\| =o_p(1)$  if $d = o(n^{1 - 1/q})$. In  the rest of the paper, we assume that $d$ is fixed.  In Proposition~\ref{lem:MinimaxLowerbound} in Section~\ref{sec:minimax_lower_bound}, we find the minimax lower bound for the single index model ~\eqref{eq:simsl}, and show that $\check{m}_{L}\circ\check{\theta}_L$ is minimax rate optimal when $q\ge 5$.

  The next result shows that the rates  in Theorem~\ref{thm:rate_m_theta_CLSE} are in fact uniform (up to a $\sqrt{\log\log n}$ factor) in $ L \in [L_0, n L_0]$.
  % \footnote{The dependence on $L$ in the second term of the rate can be improved via a slightly more careful calculation.} 
This uniform-in-$L$ result is important for the study of the estimator with a data-driven choice of $L$ such as cross-validation or Lepski's method~\cite{lepski1997optimal}. Theorem~\ref{thm:rate_m_theta_CLSE} alone cannot provide such a rate guarantee because it requires $L$ to be non-stochastic.  


\begin{thm}\label{thm:UniformLRate}
Under the assumptions of Theorem~\ref{thm:rate_m_theta_CLSE}, the CLSE satisfies 
\begin{equation}\label{eq:UniformLRate}
\sup_{L_0 \le L \le nL_0}\,\min\left\{\frac{n^{2/5}}{L}, \frac{n^{1/2 - 1/(2q)}}{\sqrt{L}}\right\}\|\check{m}_L\circ\check{\theta}_L - m_0\circ\theta_0\| = O_p\left(\sqrt{\log\log n}\right).
\end{equation}
\end{thm}
\begin{remark}[Diverging $L$]\label{rem:DependenceonL}   The dependence on $L$ in Theorems~\ref{thm:rate_m_theta_CLSE} and~\ref{thm:UniformLRate} suggest that the estimator may not be consistent if $L \equiv L_n$ diverges too quickly with the sample size. The simulation in Section~\ref{sub:robustness_of_choice_of_} suggests that the estimation error has negligible dependence on $L$ and that the dependence on $L$ in Theorems~\ref{thm:rate_m_theta_CLSE} and~\ref{thm:UniformLRate} might be sub-optimal. We believe this discrepancy is due to the lack of available technical tools to prove uniform boundedness of the estimator $\check{m}_{n,L}$ in terms of $L$. At present, we are only able to prove that with high probability, $\|\check{m}_{n,L}\|_{\infty} \le LT + M_0 + 1$ for all $L \ge L_0$; see Lemma~\ref{lem:Upsilion_ep}. If one can prove $\|\check{m}_{n,L}\|_{\infty} \le C$ for all $L \ge L_0$, with high probability, for a constant $C $ independent of $L$, then our proofs can be modified to remove the dependence on $L$ in Theorems~\ref{thm:rate_m_theta_CLSE} and~\ref{thm:UniformLRate}. 
\end{remark}


% \begin{remark}\label{rem:Tailbound}
% The results above are possibly the first (in the shape constrained literature) deriving the rates for the (constrained) LSE  when the errors have only finite moments and can depend on the covariates. See \cite{han2017sharp,2018arXiv180502542H} for a discussion for results when errors are independent of the covariates. Also, see Section~\ref{sec:MajorGeneral} for more details. 
% \end{remark}

\subsection[Asymptotic analysis of the separated parameters]{Asymptotic analysis of~$\check{m}$ and $\check{\theta}$}\label{sec:SepPara}
In this section we establish the consistency and find rates of convergence of $\check{m}_{L_n}$ and $\check{\theta}_{L_n}$ separately. In Theorem~\ref{thm:rate_m_theta_CLSE} we proved that $\check{m}_{L_n}\circ\check{\theta}_{L_n}$ converges in the $L_2(P_{\theta_0, m_0})$ norm but that does not guarantee that $\check{m}_{L_n}$ converges to $m_0$ in the $\|\cdot\|_{D_0}$ norm. A typical approach for proving consistency of $\check{m}_{L_n}$ is to prove that $\{\check{m}_{L_n}\}$ is precompact in the $\|\cdot\|_{D_0}$ norm ($D_0$ is defined in~\eqref{eq:Defn_Q_D}); see e.g.,~\cite{2016arXiv161006026B,VANC}. The Arzel\`{a}-Ascoli theorem establishes that the necessary and sufficient condition for compactness (with respect to the uniform norm) of an arbitrary class of continuous functions on a bounded domain is that the function class be uniformly bounded and equicontinuous. However, if $L_n$ is allowed to grow to infinity, then it is not clear whether the sequence of functions $\{\check{m}_{L_n}\}$ is equicontinuous.  Thus to study the asymptotic properties of $\check{m}_{L_n}$ and $\check{\theta}_{L_n},$ we assume that $L_n\equiv L \ge L_0$, is a fixed constant.  For the rest of paper, we will use $\check{m}$ and $\check\theta$ to denote $\check{m}_L$ (or $\check{m}_{{L_n}}$) and $\check\theta_L$ (or $\check{\theta}_{{L_n}}$), respectively. The next theorem (proved in Section~\ref{proof:thm:uucons}) establishes consistency of $\check{m}$ and $\check{\theta}$ separately. Recall that $m_0^\prime$ denotes the nondecreasing right derivative of the  convex function $m_0$. 
 \begin{thm}\label{thm:uucons} Suppose the assumptions of Theorem~\ref{thm:rate_m_theta_CLSE} and~\ref{a0} hold. Then, for any fixed $L\ge L_0$ and any compact subset $C$ in the interior of $D_0$, we have
\[
|\check{\theta} - \theta_0| = o_p(1), \qquad  \|\check{m} - m_0\|_{D_0} = o_p(1), \quad \text{and} \quad \|\check{m}' - m'_0\|_{C} = o_p(1). \]
%\todo[inline]{Proof has gap}
% Under assumptions \ref{c1}--\ref{c4}, the constrained LSE satisfies $\check{\theta}_n\overset{P}{\to}\theta_0$ and $\|\check{m}_n\circ\theta_0 - m_0\circ\theta_0\| = O_p(n^{-2/5}L_n^{4/5}).$
\end{thm}

Fix an orthonormal basis  $\{e_1,\ldots, e_d\}$ of $\R^d$ such that $e_1 =\theta_0.$ Define $H_{\theta_0}\coloneqq [e_2,\ldots,e_d]\in\mathbb{R}^{d\times(d-1)}$. We will use the following two additional assumptions to establish upper bounds on the rate of convergence of $\check{m}$ and $\check{\theta}.$ 


% \todo[inline]{Do we need uniform consistency on $D_0$ or just compact subsets would do? \\Ans: We need uniform consistency. Otherwise we would not be able to establish rates of convergence for $\check{m}'$.}

\begin{enumerate}[label=\bfseries (A\arabic*)]
\setcounter{enumi}{3}
% \item  is a nonsingular matrix. \label{aa4}
\item $ H_{\theta_0}^\top\E\big[\Var(X|\theta_0^\top X) \{m_0'(\theta_0^{\top}X)\}^2 \big]H_{\theta_0}$ is a positive definite matrix.
% The conditional distribution of $X$ given $\theta_0^{\top}X$ is nondegenerate.
% {\clr Where is this used, and how is this connected to the semi parametric part. {\cln Used to Theorem~\ref{thm:ratestCLSE}, see~\eqref{eq:G1SquareBound}, not the semiparametric part}}
 \label{aa5_new}
% \item $\Var(X)$ is a positive definite matrix. \label{aa5}

\item  \label{aa6} The density of $\theta_0^\top X$ with respect to the Lebesgue measure is bounded above by $\overline{C}_d < \infty$. 
% \item There exist positive constants $\overline{C}_d, \underline{C_d},$ and $r$,  such that for every $\theta\in B_{\theta_0}(r),$ the density of $\theta^\top X$ with respect to the Lebesgue measure  on $D_\theta$ is bounded below by $\underline{C_d}$ and bounded above by $\overline{C}_d$.

\end{enumerate}
Assumption~\ref{aa5_new}, is used to find the rate of convergence for $\check\theta$ and $\check{m}$ separately and is widely used in all works studying root-$n$ consistent estimation of $\theta_0$ in the single index model, see e.g.,~\cite{Powelletal89,ICHI93,Patra16,2017arXiv171205593B}; also see~Remark~\ref{rem:SingularInformation}. \ref{aa6} is mild, and is satisfied if $X=(X_1,\ldots, X_d)$ has a continuous covariate $X_k$ such that: (1) $X_k$ has a bounded density; and (2) $\theta_{0,k}>0$. Compare assumption~\ref{aa6} with~\citep{ICHI93,cuietal11,2017arXiv171205593B,MR2529970,wang2015spline} where it is assumed that $\theta^\top X$ has a density bounded away from zero for all $\theta$ in a neighborhood of $\theta_0$.
Assumption~\ref{aa6} is used to find rates of convergence of the derivative of the estimators of $m_0$. In Theorem~\ref{thm:ratestCLSE}, we only use the fact that $\theta_0^\top X$ is absolutely continuous with respect to Lebesgue measure. The following result (proved in Section~\ref{sec:proof_ratestPLSE}) establishes upper bounds on the rate of convergence of $\check{\theta}$ and $\check{m}$ respectively.
 \begin{thm}\label{thm:ratestCLSE}
If assumptions~\ref{a0}--\ref{aa6} hold, $q\ge 5$, and $L\ge L_0$, then we have 
\[
|\check{\theta} - \theta_0| = O_p(n^{-2/5}) \quad \text{and} \quad  {\int (\check{m}(t)- m_0 (t))^2 dP_{\theta_0^{\top}X}(t) dt = O_p(n^{-4/5})}.\]
% \begin{equation}\label{eq:IntegralFtheta_011}
% \int_{D_0} \left(\check{m}_n(t) - m_0(t)\right)^2f_{\theta_0^{\top}(X)}(t) dt = O_p(n^{-4/5}).
% \end{equation}}
\end{thm}
\begin{remark}\label{rem:SingularInformation}
Note that, under homoscedastic errors in~\eqref{eq:simsl}, the efficient information for $\theta_0$ is a scalar multiple of $H_{\theta_0}^\top\E\big[\Var(X|\theta_0^\top X) \{m_0'(\theta_0^{\top}X)\}^2 \big]H_{\theta_0}=: \mathcal{I}_0$; see Section~\ref{sec:eff_score}. If $\mathcal{I}_0$ is  not positive definite, then there is zero information for $\theta_0$ along some directions. In that case, we can show that $| \mathcal{I}_0^{1/2} (\check{\theta} - \theta_0)| = O_p(n^{-2/5})$; see~\eqref{eq:step_th6} in the supplementary file. 
\end{remark}
A simple modification of the proof of Proposition~\ref{lem:MinimaxLowerbound} will prove that $\check{m}$ is also minimax rate optimal. {Under additional smoothness assumptions on $m_0$, in the following theorem (proved in Section~\ref{proof:DerivCLSE}) we show that $\check{m}'$,  the right derivative of $\check{m},$ converges to $m'_0$ in both the $L_2$  and the supremum norms.


\begin{thm}\label{thm:rate_derivCLSE} Suppose assumptions  of Theorem~\ref{thm:ratestCLSE} hold and $m_0'$ is $\nicefrac{1}{2}$-H\"{o}lder continuous on $D_0$, then 
\begin{equation} \label{eq:deriv_theta_0}
{\|\check{m}^\prime \circ \theta_0- m'_0\circ\theta_0\| = O_p\big(n^{-2/15}\big)}\quad \text{and} \quad  \|\check{m}^\prime \circ \check\theta- m'_0\circ\check\theta\|= O_p\big(n^{-2/15}\big).
\end{equation}
Further, if $m_0$ is twice continuously differentiable and assumption~\ref{bb2'} (in Section~\ref{sec:SemiInf}), then for any compact subset $C$ in the interior of $D_0$, we have % {\clr such that $ f_{\theta_0^{\top}(X)}$ is bounded away from $0$ on $C$, } we have that
\begin{equation}\label{eq:sup_rate}
\sup_{t\in C} |\check{m} (t)- m_0 (t)| = O_p(n^{-8/(25 + 5\beta)})%n^{-2/5} n^{(2 + 2\beta)/(25 + 5\beta)}
 \quad \text{and} \quad \sup_{t\in C} |\check{m}^\prime (t)- m'_0 (t)| = O_p(n^{-4/(25 + 5\beta)}).%n^{-1/5} n^{(1+\beta)/(25 + 5\beta)}
\end{equation}
\end{thm}
% The fact that $\check{m}^\prime$ is a step function complicates the proof of the above result (given in Section~\ref{proof:DerivCLSE} of the supplementary file). 
\begin{remark}\label{rem:TwiceCont}
As in~\eqref{eq:deriv_theta_0},~\eqref{eq:sup_rate} can also be proved under $\gamma$-H\"{o}lder continuity of $m_0'$, but in this case the rate of convergence depends on $\gamma$ explicitly. Assumption~\ref{bb2'} allows for the density of $\theta_0^{\top}X$ to be zero at some points in its support; see Section~\ref{sec:SemiInf} for a detailed discussion. Further if the density of $\theta_0^{\top}X$ is bounded away from zero, then $\beta$ can be taken to be $0$. 
\end{remark}
\begin{remark}\label{rem:lowerQ}
The condition $q\ge5$ in Theorems~\ref{thm:ratestCLSE} and~\ref{thm:rate_derivCLSE} can be relaxed at the expense of slower rates of convergence. { In fact, by following the arguments in the proofs, we can show, with $p_n := \max\{n^{-2/5}, n^{-1/2 + 1/(2q)}\}$ for any $q\ge2$, that $|\check{\theta} - \theta_0| = O_p(p_n)$, and 
\[
\|\check{m}\circ \theta_0- m_0\circ \theta_0 \|  = O_p(p_n), \quad \|\check{m}^\prime \circ \theta_0- m'_0 \circ \theta_0\| = O_p(p_n^{1/3})\quad \text{and} \quad  \|\check{m}^\prime \circ \check\theta- m'_0\circ\check\theta\|= O_p(p_n^{1/3}).
\]}
% where
%  \[
%   p_n \coloneqq \min\left\{{n^{2/5}},{n^{1/2 - 1/(2q)}}\right\}.
% \]
\end{remark}
% }% In fact, the obtained rate need not be optimal, but is sufficient for our purposes (in deriving the efficiency of $\check\theta$; see Section~\ref{sec:SemiCLSE}).
\begin{remark}[Additional shape constraints on the link function]\label{rem:Monotone} {It might often be the case that in addition to convexity, the practitioner is interested in {imposing additional shape constraints (such as monotonicity, unimodality, or $k$-monotonicity~\cite{MR3881209}) on $m_0$.} For example, in the datasets considered in Examples~\ref{ex:boston} and~\ref{ex:car}, the link function is {plausibly} both convex \textit{and} monotone; see~\cite{CHSA} for further motivation {on} additional shape constraints. The conclusions (and proofs) of {Theorems~\ref{thm:rate_m_theta_CLSE} and~\ref{thm:UniformLRate}--\ref{thm:rate_derivCLSE}} also hold for the  CLSE under additional constraints {on the link function}. An intuitive explanation is that the parameter space $\M_L$ is only reduced by imposing {additional constraints} on the link function and this can only give better rates (if not the same). In case of an additional monotonicity constraint on $m_0$, one can modify the proof of Proposition~\ref{lem:MinimaxLowerbound} to show that {the} rate obtained in Theorem~\ref{thm:rate_m_theta_CLSE} is in fact minimax optimal for {the} the CLSE  (under further monotonicity constraint). }


\end{remark}



\section{Semiparametric inference for the CLSE} \label{sec:SemiInf}
The main result in this section shows that $\check{\theta}$ is $\sqrt{n}$-consistent and asymptotically normal; see Theorem~\ref{thm:Main_rate_CLSE}. Moreover,~$\check{\theta}$ is shown to be semiparametrically efficient for $\theta_0$ if the errors happen to be homoscedastic. The asymptotic analysis of $\check \theta$  is involved  as $\check{m}$ is a piecewise affine function and hence not differentiable everywhere.
%As stated in Theorem \ref{thm:existanceCLSE}, the estimate $\check{m}$ of the link function $m_0$ is a piecewise affine function and hence not differentiable everywhere. This complicates the asymptotic analysis of $\check{\theta}$, relative to $\hat{\theta}$.

Before deriving the limit law of $\check{\theta}$, we introduce some  notations and assumptions.
 Let $p_{\epsilon,X}$ denote the joint density (with respect to some dominating measure on $\R \times \rchi$) of $(\epsilon, X)$. Let $p_{\epsilon|X} (\cdot,x)$ and $p_X(\cdot)$ denote the corresponding conditional probability density of $\epsilon$ given $X = x$ and the marginal density of $X$, respectively. In the following we list additional assumptions used in Theorem~\ref{thm:Main_rate_CLSE}. Recall $D$ and $D_0$ from~\eqref{eq:Defn_Q_D} and let  $\Lambda$ denote the Lebesgue measure.
% \begin{enumerate}[label=\bfseries (B\arabic*)]
% \setcounter{enumi}{0}
%  \item For every $\theta\in B_{\theta}(r, \theta_0)$, $D_\theta$ is strict subset of $D^{(r)}$. For the rest of the paper we redefine $D\coloneqq D^{(r)}$.\label{bb1}
%  \end{enumerate}
 \begin{enumerate}[label=\bfseries (B\arabic*)]
\setcounter{enumi}{0}
 \item $m_0\in \M_{L_0}$ and  $m_0$ is $(1+ \gamma)$-H\"{o}lder continuous on $D_0$ for some $\gamma>0$. Furthermore, $m_0$ is strongly convex on $D$, i.e., there exists a $\kappa_0>0$ such that  $m_0(t)- \kappa_0 t^2$ is convex.   \label{bb1}

 \item  There exists $\beta\ge 0$ and $\underline{C}_d>0$ such that $\p(\theta_0^\top X \in I) \ge \underline{C}_d\,\Lambda(I)^{1 + \beta},$  for all intervals $I \subset D_0$. \label{bb2'}
  
% \item Assume that there exist  $r>0$ such that for all $\theta \in S^{d-1}\cap B_{\theta_0}(r)$ we have
% $$D_\theta \subsetneq D.$$\label{b1}
\end{enumerate}
For every $\theta\in\Theta$, define  $h_\theta(u) \coloneqq \E[X |\theta^\top X=u]$.
%  \begin{align}
% h_\theta(u) &\coloneqq \E[X |\theta^\top X=u].\label{eq:h_beta}
% \end{align}
 \begin{enumerate}[label=\bfseries (B\arabic*)]
\setcounter{enumi}{2}
{\item 
The function $u\mapsto h_{\theta_0}(u)$ is $1/2$-H\"{o}lder continuous and for a constant $\bar{M}>0$,\label{bb2}
\begin{equation}\label{eq:L_2Lip}
\E\Big(|h_{\theta}(\theta_0^{\top}X) - h_{\theta_0}(\theta_0^{\top}X)|^2\Big) \le \bar{M}|\theta - \theta_0|\quad\mbox{for all}\quad \theta\in\Theta.
\end{equation}}
\item The  density $p_{\epsilon|X} (e,x)$ is differentiable with respect to $e$ for all $x\in \rchi$.
\label{bb3}
\end{enumerate}

Assumptions \ref{bb1}--\ref{bb3} deserve comments. \ref{bb1} is much weaker than the  standard assumptions used in semiparametric inference in  single index models~\cite[Theorem 3.2]{VANC}. Assumption~\ref{bb2'} is an improvement compared to the assumptions in the existing literature. Assumption~\ref{bb2'} pertains to the distribution of $\theta_0^\top X$ and is inspired by~\cite[assumption~(D)]{MR2369025}. In contrast, most existing works require the density of $\theta_0^\top X$ to be bounded away from zero (i.e., $\beta=0$); see e.g.,~\cite[Assumption 5.3(II)]{ICHI93},~\cite[Assumption (d)]{cuietal11},~\cite[Lemma F.3]{2017arXiv171205593B},~\cite[Assumption~A2]{MR2529970},~\cite[Assumption (A2)]{wang2015spline}. Our assumption is significantly weaker because it allows the density of $\theta_0^\top X$ to be zero at some points in its support. For example, when $X\sim\text{Uniform}[0,1]^d$, the density of $\theta_0^\top X$ might not be bounded away from zero~\citep[Figure 1]{MR2369025}, but~\ref{bb2'} holds with $\beta=1.$
  Assumption~\ref{bb2} can be favorably compared to those in \cite[Theorem 3.2]{VANC},~\cite[Assumption (A5)]{groeneboom2016current},~\cite[Assumption (A5)]{2017arXiv171205593B}, and~\cite[Assumption G2 (ii)]{song2014semiparametric}.  We use the smoothness assumption~\ref{bb2} when establishing semiparametric efficiency of $\check\theta$. The Lipschitzness assumption~\eqref{eq:L_2Lip} can be verified by using the techniques of~\cite{alonso1998lp}, when $u\mapsto h_{\theta} (u)$ is $1/2$-H\"{o}lder continuous for all $\theta$ in a neighborhood of $\theta_0$ and the H\"{o}lder constants  are uniformly bounded in $\theta$.
% \subsection{Disc} % (fold)
% \label{sub:disc}

% subsection disc (end)
{In general, establishing semiparametric efficiency of an estimator proceeds in two steps. Let $\hat{\xi}$ and $\hat\gamma$ denote the estimators of a parametric component $\xi_0$ and a nuisance component $\gamma_0$ in a general semiparametric model. In a broad sense, the proof of semiparametric efficiency} of $\hat{\xi}$ involves two main steps: (i) finding the efficient score of the model at the truth (call it ${\ell}_{\xi_0, \gamma_0}$); and (ii) proving that $(\hat{\xi},\hat\gamma)$ satisfies $\p_n {\ell}_{\hat\xi, \hat\gamma} = o_p(n^{-1/2})$; see~\cite[pages 436-437]{VdV02} for a detailed discussion. In the Sections~\ref{sec:eff_score} and~\ref{sec:SemiCLSE}, we discuss steps (i) and (ii) in our context, respectively.
%{\clr \textbf{Things to fix}
% \begin{itemize}
%\item change $D$ to ${D}_r$ and $D_\beta$ to ${D}_\beta$.
%\item Fix the order of $(t,\beta)$ in the subscript.
%\item Check what assumptions you need
%\item change $t$ to $s$ in the earlier part of section 4.
%\item  Horowitz 1992 page 509 after assumption 3. similar compactness for the form of $\beta$  arguments are used
%\item Give justifiction for reparamterization
%\end{itemize}
%}


\subsection{Efficient score}\label{sec:eff_score}
In this subsection we  calculate the efficient score for the model:
\begin{equation}\label{eq:Score_model}
Y=m(\theta^\top X)+\epsilon,
\end{equation}
where $m, X,$ and $\epsilon$ satisfy assumptions~\ref{bb1}--\ref{bb3}.  First observe that the parameter space $\Theta$ is a closed subset of $\R^d$ and the interior of $\Theta$  in $\R^d$ is the empty set. Thus to compute the score for model~\eqref{eq:Score_model}, we construct a path on the sphere. We use $\R^{d-1}$ to parametrize the paths for model~\eqref{eq:Score_model} on $\Theta$ when $\theta_{0,1} >0$. For each $\eta\in\mathbb{R}^{d-1}, $ $s \in \R$, and $|s| \le |\eta|^{-1}$,  define the following path , with ``direction'' $\eta$, through $\theta$ (which lies on the unit sphere)
\be\label{eq:path_para}
\zeta_s(\theta,\eta)\coloneqq\sqrt{1-s^2|\eta|^2}\, \theta + s H_\theta \eta,
\ee
 where for every $\theta\in \Theta$, $H_\theta \in \R^{d\times(d-1)}$ is such that for every $\eta \in \R^{d-1}$,  $| H_\theta \eta| =|\eta|$ and $H_\theta\eta$ is orthogonal to $\theta$. Furthermore, we need $\theta \mapsto H_\theta$ to satisfy some smoothness properties; see Lemma 1 of~\cite{Patra16} for such a construction. {Note that, if $\theta_{0,1}=0$, then for any $s$ in a neighborhood of zero, there exists an $\eta\in \R^{d-1}$ such that  $\zeta_s(\theta_0,\eta) \notin \Theta$. Thus, if $\theta_{0,1}=0$, then $\theta_0$ lies on the ``boundary'' of $\Theta$ and the existing semiparametric theory breaks down. Therefore, for the rest of the paper, we assume that $\theta_{0,1}$ is strictly positive.}

%  \begin{enumerate}[label=(H\arabic*)]
% \item $\xi\mapsto H_\theta\xi$ are bijections from $\mathbb{R}^{d-1}$ to the hyperplanes $\{x\in\mathbb{R}^d: \theta^\top  x=0\}$. \label{H1}
% \item The columns of $H_\theta$ form an orthonormal  basis for $\{x\in\mathbb{R}^d: \theta^\top  x=0\}$.\label{H2}
% \item   $\|H_\theta - H_{\theta_0}\|_2\le|\theta- \theta_0 |.$ Here for any matrix $A\in \R^{d_1\times d_2}$, $\|A\|_2\coloneqq \sup_{\{ b\in \R^{d_2}:\, |b|=1\}} |Ab|$.\label{H3}
% \item For all  distinct $\eta,\beta \in \Theta\setminus\theta_0$, such that $|\eta-\theta_0|\le1/2$ and $|\beta-\theta_0|\le1/2,$\label{H4}
% \begin{equation}\label{eq:H_lip_gen}
% \|H_{\eta}-H_{\beta}\|_2 \le 8(1+8/\sqrt{15})\frac{ |\eta-\beta| }{ |\eta-\theta_0|+|\beta-\theta_0|}.
% \end{equation}
% \end{enumerate}%
% See Lemma 1 of~\cite{Patra16} for a construction of a class of matrices satisfying the above properties.
 The log-likelihood  of model~\eqref{eq:Score_model} is $l_{\theta, m}(y,x)= \log[ p_{\epsilon|X} (y-m(\theta^\top x),x) p_X(x)].$ For  any $\eta \in S^{d-2}$, consider the path defined as $s \mapsto \zeta_s(\theta,\eta)$. Note that by the definition of $H_\theta$, $s \mapsto \zeta_s(\theta,\eta)$ is  a valid path in $\Theta$ through $\theta$; i.e., $\zeta_0(\theta,\eta)=\theta$ and $\zeta_s(\theta,\eta) \in \Theta$ for every $s$ in some neighborhood of $0$. Thus the score for the parametric submodel is
\begin{equation}\label{eq:paraScore}
\left. \frac{\partial l_{\zeta_s(\theta,\eta), m} (y,x) }{\partial s}\right\vert_{s=0}= \eta^\top S_{\theta,m}(y,x),
\end{equation}
where
\begin{equation}\label{eq:S_def_118}
S_{\theta,m}(y,x):=   -\frac{p'_{\epsilon|X} \big(y-m(\theta^\top x),x\big)}{p_{\epsilon|X} \big(y-m(\theta^\top x),x\big)}m^\prime(\theta^\top x) H_\theta ^\top x.
\end{equation}

% \begin{remark}\label{rem:qmd}
% % {\clr Before computing the score for the above submodel, note that $m \in \M_L$ need not be differentiable everywhere.  showing that the underlying class of distributions is differentiable in quadratic mean requires some careful analysis; in Remark~\ref{rem:qmd} (in Section~\ref{sec:qmd} of the supplementary file) we show this for the model with Gaussian errors. With this in mind,  the score for the above submodel is }
% \end{remark}
The next step in computing the efficient score for model~\eqref{eq:Score_model} at $(m,\theta)$ is to compute the nuisance tangent space of the model (here the nuisance parameters are $p_{\epsilon|X}, p_X$, and $m$). To do this define a parametric submodel for  the unknown nonparametric components:
\begin{align} \label{eq:nonp_path}
\begin{split}
m_{s,a}(t)&=m(t) - s a(t), \quad p_{\epsilon|X;s, b}(e, x) = p_{\epsilon|X}(e, x) (1 + s b(e, x)), \quad  p_{X; s,q}(x) =p_X(x)(1+s q(x)),
\end{split}
\end{align} where $s\in \R$,   $b: \R \times \rchi\to\R$ is a bounded function such that $\E(b(\epsilon, X) |X)=0$ and $\E(\epsilon b(\epsilon, X) |X)=0$,  $q: \rchi \to \R$ is a bounded function such that $\E(q(X))=0,$ and $a\in\D_m$, with 
\begin{align}\label{eq:d_def_m}
\begin{split}
\D_m := \big\{f \in L_2(\Lambda): f'(\cdot)\;&\text{exists and }m_{s,f}(\cdot) \in \M_{L}\; \text{ for all } s \in B_0(\delta)\;\text{for some }\delta >0\big\}.
\end{split}
\end{align}
% \todo{Why $\M_{L_0}$}
  Note that when $m$ satisfies~\ref{bb1} then $\D_m$ reduces to $ 
 \D_m=  \big\{f \in L_2(\Lambda): f'(\cdot)\;\text{exists}\}.$
    % Since $m_0$ satisfies assumption~\ref{bb1}, we can find  $\delta$ (small enough) such that for all $s\in B_{0}(\delta)$, $m_{s,a}\in \M_L$ and  $p_{\epsilon|X;s, b}$ and $p_{X; s,q}$ are valid densities.
    Thus
% \begin{equation}\label{eq:NonBdry}
     $\overline{\mathrm{lin}}\,\D_m = L_2(\Lambda).$
% \end{equation}
 Theorem~4.1 of~\cite{NeweyStroker93} (also see~\citet[Proposition 1]{MaZhusemi13})  shows that when  the parametric score is $\eta^\top S_{\theta,m} (\cdot, \cdot)$ and the nuisance tangent space corresponding to $m$ is $L_2(\Lambda)$, then the efficient score for model~\eqref{eq:Score_model} is 
\begin{equation}\label{eq:eff_score_smooth_def}
\frac{1}{\sigma^2(x)}(y-m(\theta^\top x)) m^\prime(\theta^\top x)  H_\theta^\top \left\lbrace x -  \frac{\E(\sigma^{-2}(X)X|\theta^\top X=\theta^\top x)}{\E(\sigma^{-2}(X)|\theta^\top X=\theta^\top x)}\right\rbrace.
\end{equation}
Note that the efficient score depends on $p_{\epsilon|X}$ and $p_X$ only through $\sigma^2(\cdot)$. However if the errors happen to be homoscedastic (i.e., $\sigma^2(\cdot)\equiv\sigma^2$) then the \textit{efficient score} is $\ell_{\theta,m} (x,y)/\sigma^2$, where 
\begin{equation} \label{eq:EffScoreCLSE}
 \ell_{\theta,m} (x,y) := (y-m(\theta^\top x)) m^\prime(\theta^\top x) H_\theta^\top [  x -h_{\theta}(\theta^\top x) ].
 \end{equation}
 As $\sigma^2(\cdot)$ is unknown we restrict ourselves to efficient estimation under homoscedastic error; see Remark~\ref{rem:Efficency_general} for a brief discussion.  
% m_0^\prime(\theta^\top x) m^\prime(\theta^\top x)
 
 % $\psi_{\check\theta,\check m}$ is ``well-behaved''  and satisfies



\subsection{Efficiency of the CLSE} \label{sec:SemiCLSE}
The $\sqrt{n}$-consistency, asymptotic normality, and  efficiency (when the errors are homoscedastic) of $\check{\theta}$ will be established \textit{if} we could show that 
 \begin{equation}\label{eq:surrogate}
\sqrt{n}\,\p_n \ell_{\check\theta,\check m} = o_{p}(1)
\end{equation}
   and the class of functions $\ell_{\theta,m}$ indexed by  $(\theta, m)$ in a ``neighborhood'' of $(\theta_0,m_0)$ satisfies some technical conditions; see e.g., \citet[Chapter 6.5]{VdV02}.  As discussed in Section~\ref{sub:_semiparametric_eff_shape}, because $(\check{m},\check{\theta})$ minimizes $(m,\theta) \mapsto Q_n(m,\theta)$ over $\mathcal{M}_L\times\Theta$, the traditional way to prove~\eqref{eq:surrogate} is to use the fact that $\partial Q_n(\check{m}_{s,a}, \zeta_s(\theta,\eta))/\partial s|_{s = 0} =0$ for any $(a, \eta)$ such that $s\mapsto (\check{m}_{s,a}, \zeta_s(\theta,\eta))$ is a valid path (i.e., $a\in \overline{\mathrm{lin}}\,\D_{\check{m}}$). One then finds $(a, \eta) \in \D_{\check{m}}\times \R^{d-1}$ such that the derivative of $s\mapsto Q_n(\check{m}_{s,a}, \zeta_s(\theta,\eta))$ at $s=0$  is approximately $n^{-1}\sum_{i=1}^n \eta^\top \ell_{\check{\theta}, \check{m}}(Y_i, X_i)$; such an $(a, \eta)$ is called the  (approximate) \textit{least favorable submodel}; see~\citet[Section 9.2]{VdV02}. In Section~\ref{sec:eff_score}, we saw that if $m$ is strongly convex then $\overline{\mathrm{lin}}\,\D_m = L_2(\Lambda)$. However $\check{m}$ is piecewise affine and we can only show that $\overline{\mathrm{lin}}\,\D_{\check{m}} \subset L_2(\Lambda)$. Thus $s \mapsto \check{m}_{s,a}$ is valid path only if $ a\in \D_{\check{m}}$; see~\cite{VANC} for another example where $\overline{\mathrm{lin}}\,\D_{\check{m}} \neq L_2(\Lambda)$. In such cases it is hard to find the least favorable submodel as often the step to compute the least favorable model involves computing projection onto  $\overline{\mathrm{lin}}\,\D_{\check{m}}$; see e.g.,~\cite{Newey90}. Thus when $\overline{\mathrm{lin}}\,\D_{\check{m}}$ is not $L_2(\Lambda)$ (or a very simple subspace of $L_2(\Lambda)$), the standard linear path arguments fail to find the least favorable submodel.  To overcome this,~\cite{VANC} use a very complicated and non-linear path; see Section 6.2 of~\cite{VANC}; also see~\cite{Patra16}. 



 Our proposed technique crucially relies on the observation that $s \mapsto \Pi_{\M_L}(\check{m}_{s,a})$ is a valid path for every $a\in L_2(\Lambda)$.  Thus if $s\mapsto \Pi_{\mathcal{M}_L}(\check{m}_{s,a})$ is differentiable, then establishing that $\check{\theta}$ is an approximate zero boils down to finding an $a\in L_2(\Lambda)$ such that 
 \begin{equation}\label{eq:approx_pi_score}
 \frac{\partial }{\partial s}Q_n(\Pi_{\mathcal{M}_L}(\check{m}_{s,a}), \zeta_s(\theta,\eta))\Big|_{s = 0}= n^{-1}\sum_{i=1}^n \eta^\top \ell_{\check{\theta}, \check{m}}(Y_i, X_i) +o_p(n^{-1/2}).
 \end{equation}
 for every $\eta \in \R^{d-1}. $  In Section~\ref{sec:proof_of_eq:app_score_equation}, we show $s \mapsto \Pi_{\M_L}(\check{m}_{s, a})$ is differentiable if $a \in \mathcal{X}_{\check{m}}$, where 
 \begin{equation}\label{eq:X_m_def}
 \mathcal{X}_{\check{m}} :=\big\{a\in L_2(\Lambda): a \text{ is a piecewise affine continuous function with kinks at }\{\check{t}_i\}_{i=1}^\mathfrak{p}\big\},
 \end{equation}
 and $\{\check{t}_i\}_{i=1}^\mathfrak{p}$ are the set of  kinks of $\check{m}$. For  a piecewise affine function, a kink is a point where the slope changes. Furthermore, in Theorem~\ref{thm:projection}, we find an $a\in \mathcal{X}_{\check{m}}$ that satisfies~\eqref{eq:approx_pi_score}. The advantage of the technique proposed here is that the construction of approximate least favorable submodel is  analytic and does not rely on the ability of the user to ``guess'' the least favorable submodel; see e.g.,~\cite[Section 9.2-9.3]{VdV02} and~\cite{VANC}. The above discussion and~\cite[Theorem 6.20]{VdV02} lead to our main result (Theorem~\ref{thm:Main_rate_CLSE}) of this section.  Recall $S_{\theta_0,m_0}$ and $\ell_{\theta,m}$  defined in~\eqref{eq:paraScore} and~\eqref{eq:EffScoreCLSE}, respectively.
 % A typical approach for proving~\eqref{eq:surrogate} is to construct a path  whose score at $(\check{\theta},\check{m})$ is $\ell_{\check{\theta},\check{m}}.$ However such a path might not exist as $\check{m}$ is a piecewise affine convex function and the nuisance tangent space of $m$ at $\check{m}$  {\clr contains
 % \begin{equation}\label{eq:D_mhat1}
 % \overline{\mathrm{lin}}\, \D_{\check{m}}= \overline{\mathrm{lin}}\,\big\{a\in L_2(\Lambda): a \text{ is a piecewise affine function with kinks at } \{\check{t}_i\}_{i=1}^q\big\} \subsetneqq L_2(\Lambda) \footnote{ If there exist an $a(\cdot)$ such that $\check{m}_{s,a}(\cdot)$ is convex for both $s$ and $-s$ for some non-zero $s$, then $a(\cdot)$ must be affine between the points $\{\check{t}_i\}_{i=1}^q$.}
 % \end{equation}
 % where $\{\check{t}_i\}_{i=1}^q$ are the set of  kinks\footnote{Set of kinks of a piecewise affine function are the points at which the slopes change.} of $\check{m}$.  For any $m\in\M_{L}$, we say that $m$ lies on the \textit{boundary} of $\M_L$ if $\overline{\mathrm{lin}}\, \D_{{m}} \subsetneqq L_2(\Lambda)$, cf.~\eqref{eq:NonBdry}. Thus $\check{m}$ lies on the boundary of $\M_L$ and we could not find a path whose score at $(\check{\theta}, \check{m})$ is  $\ell_{\check\theta,\check m}$, i.e., efficient score function at $(\check{\theta}, \check{m})$ may not be a score function. This phenomenon has also been observed in semiparametric models where the parameters are not necessarily bundled; \citet[Chapter 9.3]{VdV02} introduced the notion of approximately least favorable subprovided model to get around this difficulty. 
 %  }


\begin{thm} \label{thm:Main_rate_CLSE}
 Assume \ref{a0}--\ref{aa6} and \ref{bb1}--\ref{bb3} hold. Let $\theta_{0,1}>0$, $q\ge 5$, and $L\ge L_0$.  
% Define the function
% \be \label{eq:EffScoreCLSE}
% {\ell}_{\theta,m}(y,x) :=\big(y-m(\theta^\top x)\big) m^\prime(\theta^\top x)  H_\theta^\top \left\lbrace x - h_\theta(\theta^\top x)\right\rbrace.
% \ee
If $\gamma > 1/2+\beta/8$ and $V_{\theta_0,m_0}:=P_{\theta_0,m_0}({\ell}_{\theta_0,m_0} S^\top_{\theta_0,m_0})$ is a nonsingular matrix in $\R^{(d-1) \times (d-1)}$, then
\begin{equation}\label{eq:globalEffCLSE}
\sqrt{n} (\check{\theta}- \theta_0)\stackrel{d}{\rightarrow}   N(0,H_{\theta_0} V_{\theta_0,m_0}^{-1}  {I}_{\theta_0,m_0}(H_{\theta_0} V_{\theta_0,m_0}^{-1})^\top), 
\end{equation}
where  ${I}_{\theta_0,m_0} := P_{\theta_0,m_0} ({\ell}_{\theta_0,m_0}{\ell}^\top_{\theta_0,m_0})$. Further, if $\sigma^2(\cdot)\equiv\sigma^2$, then $V_{\theta_0,m_0}= {I}_{\theta_0,m_0}$ and
\begin{equation}\label{eq:localeffestimCLSE}
\sqrt{n} (\check{\theta}-\theta_0) \stackrel{d}{\rightarrow} N(0, \sigma^4 H_{\theta_0}{I}^{-1}_{\theta_0,m_0} H_{\theta_0}^\top).
\end{equation}
\end{thm}
\begin{remark}\label{rem:gammaBeta}
{If $m_0$ is twice continuously differentiable then $\gamma=1$. Hence, $\gamma > 1/2+\beta/8$ is equivalent to assuming $\beta \in [0,4)$. Note that $\beta>0$ allows for covariate distributions for which the density of $\theta_0^\top X$ can go to zero. In Theorem~\ref{thm:Main_rate_CLSE}, to keep notations in the proof simple, we assume that $q\ge 5$. However, by using Remark~\ref{rem:lowerQ}, this condition can be weakened to $q\ge4$. In Section~\ref{sec:Degenracy}, we show that the limiting variances in Theorem~\ref{thm:Main_rate_CLSE} are unique and do not depend on the particular choice of $\theta\mapsto H_\theta$.}
\end{remark}
\paragraph{Sketch of the proof.} The proof follows along the lines of Theorem 6.20 of~\cite{VdV02}. The main novelty in the proof is a new mechanism to verify that the estimator satisfies the score equation~\eqref{eq:surrogate}.  However to simplify the algebra involved,\footnote{All the proofs will go through with $\ell_{\theta,m}$ instead of $\psi_{\theta, m}$. However, usage of $\ell_{\theta, m}$ will require more remainder terms to be controlled and thus will lead to more tedious proofs.} we will work with 
\begin{equation} \label{eq:App_score}
 \psi_{\theta,m} (x,y) := (y-m(\theta^\top x))m^\prime(\theta^\top x) H_\theta^\top [  x -h_{\theta_0}(\theta^\top x)],
 \end{equation}
 a slight modification of $\ell_{\theta, m}$. The only difference between $\ell_{\theta, m}$ and $\psi_{\theta, m}$ is the last term ($h_{\theta}(\theta^\top X))$. 
 In Section~\ref{app:sketchCLSE} of the supplementary file we show that     \begin{equation}\label{eq:App_score_equation}
   \sqrt{n}\, \p_n \psi_{\check{\theta},\check{m}} =o_p(1),
   \end{equation}
implies 
   \begin{equation}\label{eq:RAL_1}
        \sqrt{n}V_{\theta_0,m_0}  H_{\theta_0}^\top(\check{\theta}- \theta_0)  ={}\g_n \psi_{\theta_0,m_0} + o_p(1+\sqrt{n} |\check{\theta}-\theta_0|).
       \end{equation}    The conclusion of the proof follows by  observing that 
$\psi_{\theta_0,m_0} = {\ell}_{\theta_0,m_0}$. 
We will now give a brief sketch of the proof of~\eqref{eq:App_score_equation}. Define for every $(m, \theta)$,  $\eta\in \R^{d-1}$, $a: D\to \R$, and $t\in \R$, 
\[\zeta_t(\theta,\eta):=\sqrt{1-t^2|\eta|^2}\, \theta + t H_\theta \eta \qquad\text{and}\qquad  \xi_t(u;a, {m}) := \Pi_{\M_L}({m} - t a)(u).
\]
Observe that $(\check{m}, \check{\theta})$ is the minimizer of $(m,\theta) \mapsto Q_n(m,\theta)$ and  $t \mapsto (\zeta_t(\check\theta,\eta), \xi_t(u;a, \check{m}))$ is a valid path in $\M_L\times \Theta$ through $( \check \theta, \check{m})$. Thus  $t=0$ is the minimizer of  $t\mapsto Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a, \check{m}))$ for every  $\eta\in \R^{d-1}$ and $a: D\to \R$.
Hence if $t\mapsto Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a, \check{m}))$ is differentiable then 
\begin{equation}\label{eq:derivative_Q_n_equals_zero} \frac{\partial}{\partial t} Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a, \check{m}))\Big|_{t=0}=0.\end{equation}
Furthermore, if functions $a_1, a_2, \ldots, a_K$ (for some $K\ge1$) are such that $t\mapsto Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a_j, \check{m}))$ is differentiable for all $1\le j\le K$, then
\[
\sum_{j=1}^{K} \alpha_j\frac{\partial}{\partial t} Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a_j, \check{m}))\Big|_{t=0}=0,
\]
for any $\alpha_1,\ldots,\alpha_K\in\mathbb{R}$. Note that the proof of~\eqref{eq:App_score_equation} will be complete, if we can show that for every $\eta\in S^{d-2}$, there exist a $K\ge1$ and functions $a_j: D\to \R, 1\le j\le K$ such that $t\mapsto \Pi_{\M_L}(\check{m} - t a_j)(u)$ is differentiable and 
\begin{equation}\label{eq:psi_approx_phi}
\eta^\top   \p_n \psi_{\check{\theta}, \check{m}} = \sum_{j=1}^K \alpha_j\frac{\partial}{\partial t} Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a_j, \check{m}))\Big|_{t=0} + o_p(n^{-1/2}).
\end{equation}
This means that it is enough to consider the approximation of $\eta^{\top}\mathbb{P}_n\psi_{\check{\theta},\check{m}}$ by the linear closure of $\{\partial Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a, \check{m}))/\partial t|_{t=0}: t\mapsto Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a, \check{m}))\mbox{ is differentiable at } t=0\}$. {Instead of fully characterizing the linear closure set, we find a large enough subset that suffices for our purpose using the following steps.}
\begin{enumerate}
\item We find a set of perturbations $a$ such that $t\mapsto \xi_t(\cdot;a, {m})$ is differentiable. Recall $\mathcal{X}_{\check{m}}$ defined in~\eqref{eq:X_m_def}. In Lemma~\ref{lem:projection_is_identity} (stated and proved in the supplementary file), we show that $\mathcal{X}_{\check{m}} \subseteq \{a:D\to\mathbb{R}\,|\,t\mapsto\xi_t(\cdot; a, \check{m}) \text{ is differentiable at } t=0\}.$
\item For every such $a\in\mathcal{X}_{\check{m}}$, in Lemma~\ref{lem:deriv}, we show that
\[
-\frac{1}{2}\frac{\partial}{\partial t}Q_n(\zeta_t(\check\theta,\eta), \xi_t(\cdot;a, \check{m}))\Big|_{t = 0} ~=~ \mathbb{P}_n\left[\big(y-\check{m}(\check\theta^\top x)\big)\Big\{ \eta^\top  \check{m}'(\check\theta ^\top x) H_{\check\theta}^\top x - a(\check\theta^\top x)\Big\}\right].
\]
\end{enumerate}
 % For every $a\in\mathcal{X}_{\check{m}}$, the following provides an expression for the derivative on the left hand side of~\eqref{eq:derivative_Q_n_equals_zero}.  
Thus to prove~\eqref{eq:psi_approx_phi}, it is enough to show that
\[
\inf_{a\in\overline{\mathrm{lin}}(\mathcal{X}_{\check{m}})}\left|\eta^{\top}\mathbb{P}_n\psi_{\check{\theta},\check{m}} - \mathbb{P}_n\left[(y-\check{m}(\check\theta^\top x))\{ \eta^\top  \check{m}'(\check\theta ^\top x) H_{\check\theta}^\top x - a(\check\theta^\top x)\}\right]\right| = o_p(n^{-1/2}),
\] 
where $\psi_{\theta, m}$ is defined in~\eqref{eq:App_score}.
In more general constraint spaces, one might need to use the generality of $\overline{\mathrm{lin}}(\mathcal{X}_{\check{m}})$ but in our case, it suffices to work with $\mathcal{X}_{\check{m}}$; see Theorem~\ref{thm:projection}. \qed

 % A full proof is provided in Section~\ref{sec:proof_of_eq:app_score_equation} of the supplementary file. 

% \end{proof}

\begin{remark}[Efficiency under heteroscedasticity]\label{rem:Efficency_general} It is important to note that~\eqref{eq:eff_score_smooth_def}, the efficient score, depends on $\sigma^2(\cdot)$.  Without additional assumptions, estimators of $\sigma^2(\cdot)$  will have poor finite sample  performance (especially if $d$ is large) which in turn will lead to  poor finite sample performance of the weighted LSE; see \citet[pages 93-95]{Tsiatis06}. 
\end{remark}

\begin{remark}[Efficiency under additional shape constraints]\label{rem:Efficency_additional}
As discussed in Remark~\ref{rem:Monotone}, it might be the case that the practitioner is interested in {imposing} additional shape constraints such as monotonicity, unimodality, or $k$-monotonicity (in addition to convexity). If $m_0$ satisfies these constraints {in a strict sense} (i.e., $m_0$ is strictly monotone {or} $k$-monotone) then the discussion in~Section~\ref{sec:eff_score} implies that {the efficient score (at the truth) is still~\eqref{eq:eff_score_smooth_def} even under the additional shape constraints}. This is true, because $\overline{\mathrm{lin}}\,\D_{m_0} = L_2(\Lambda)$ even under these additional shape constraints on link functions, as $m_0$ does not lie {on} the ``boundary'' of the parameter space. In fact, under these additional constraints, the proof of Theorem~\ref{thm:Main_rate_CLSE} can be used with minor modifications to show that CLSE of $\theta_0$ satisfies~\eqref{eq:globalEffCLSE}.
\end{remark}

To further illustrate the usefulness of our new approach we discuss the proof of semiparametric efficiency in the Cox proportional hazards model under  current status censoring~\cite{MR1394975,VdV02}. 

\begin{example}[Cox proportional hazards model with current status data]\label{ex:coxmodel}
Suppose that we observe a random sample of size $n$ from the distribution of $X = (C, \Delta, Z)$, where $\Delta = 1\{T \le C\}$, such that the survival time $T$ and the  observation time $C$ are independent given $Z \in \R^d$, and that $T$ follows a Cox proportional hazards model with parameter $\theta_0$ and cumulative hazard function $\Lambda_0$; e.g., see~\cite[Section 2]{MR1394975} for a more detailed discussion of this model.~\citet{MR1394975} shows that $\hat\Lambda$, the nonparametric maximum likelihood estimator (NPMLE) of $\Lambda_0$, is a right-continuous step function with possible discontinuities only at $C_1,\ldots, C_n$ (the observed  censoring/inspection times).~\citet{MR1394975} also proves that $\hat{\theta}$ (the NPMLE for $\theta_0$) is an efficient estimator for $\theta_0$. However just as in the single index model, the proof of efficiency is complicated due to the fact that $s\mapsto \hat{\Lambda}+ s h$ will not necessarily be a valid hazard function for every smooth $h(\cdot)$.\footnote{$\hat{\Lambda}+ s h$ is not guaranteed to be monotone as $\hat{\Lambda}$  is a nondecreasing piecewise constant function and not strictly increasing.}  To establish~\eqref{eq:surrogate} for the above model,~\citet[pages 563-564]{MR1394975} ``guesses'' an approximately least favorable path (also see~\cite[pages 439-441]{VdV02}). However, using the arguments above we can easily see that $ s \mapsto \Pi(\hat{\Lambda}+ s h)$ is differentiable if $h$ is a piecewise constant function with possible discontinuities only at the points of discontinuities of $\hat{\Lambda}$. Then using the property that $\|\hat{\Lambda}-\Lambda_0\| =o_p(n^{-1/3}),$ one can establish a result similar to~\eqref{eq:approx_pi_score}. A similar strategy can be used to establish efficiency in the current status regression model in~\citet{VANC}.
 \end{example}



 \subsection{Construction of confidence sets and validating the asymptotics}\label{sec:cfd_set}
% \clr{Theorem~\ref{thm:Main_rate_PLSE} proves asymptotic normality of the estimator $\hat{\theta}$ of $\theta_0$. In order to do inference with our estimator, one needs an estimate of the asymptotic variance. Under homoscedasticity, a simple plug-in approach works.
Theorem~\ref{thm:Main_rate_CLSE} shows that when the errors happen to be homoscedastic the CLSE of $\theta_0$ is $\sqrt{n}$-consistent and asymptotically normal with covariance matrix:
\begin{equation}\label{eq:finite_var}
\Sigma^0:= \sigma^4 H_{\theta_0} P_{\theta_0,m_0}[ {\ell}_{\theta_0,m_0}(Y,X) {\ell}^\top_{\theta_0,m_0}(Y,X)]^{-1} H_{\theta_0}^\top,
\end{equation}
where $\ell_{\theta_0, m_0}$ is defined in~\eqref{eq:EffScoreCLSE}. This result can be used to construct confidence sets for $\theta_0.$ However since $\Sigma^0$ is unknown, we propose using the following plug-in estimator of $\Sigma^0$:
\begin{equation}\label{eq:V_hat}
\check{\Sigma} := \check{\sigma}^4 H_{\check{\theta}} \big[\p_n\big( {\ell}_{\check{\theta},\check{m}}(Y,X) {\ell}^\top_{\check{\theta},\check{m}}(Y,X)\big)\big]^{-1} H_{\check{\theta}}^\top,
\end{equation}
where $\check{\sigma}^2: =  \sum_{i=1}^n [Y_i-\check{m}(\check{\theta}^\top X_i)]^2/n$. Note that Theorems~\ref{thm:ratestCLSE} and~\ref{thm:rate_derivCLSE} imply consistency of $\check\Sigma$.
% To be precise, it can be easily shown (using Therorems~\ref{thm:rate_m_theta_CLSE}--\ref{thm:rate_derivCLSE}) that one can consistently estimate $\Sigma^0$ (the asymptotic variance in \eqref{eq:localeffestimPLSE}) by the following plug-in estimator,
%  is a consistent estimator of $\sigma^2$.

 For example one can construct the following $1-2\alpha$ confidence interval for $\theta_{0,i}$:
\begin{equation}\label{eq:Conf_int}
\bigg[\max \left\{-1, \check{\theta}_i - \frac{z_{\alpha}}{\sqrt{n}} \left(\check{\Sigma}_{i,i}\right)^{1/2}\right\} ,\; \min\left\{1, \check{\theta}_i  + \frac{z_{\alpha}}{\sqrt{n}} \left(\check{\Sigma}_{i,i}\right)^{1/2} \right\}\bigg],
\end{equation}
where $z_{\alpha}$ denotes the upper $\alpha$th-quantile of the standard normal distribution. The truncation guarantees that confidence interval is a subset of the parameter set. 

{We now give an illustrative simulation example.  We generate $n$ i.i.d.~observations from the model: $Y=(\theta_0^\top X)^2 + N(0, .3^2),$ where $ X \sim \text{Uniform} [-1, 1]^3$ and $\theta_0 = (1,1,1)/\sqrt{3},$ for $n$ increasing from $50$ to $1000.$  For the above model, $\Sigma^0_{1,1}$ is $0.22$.\footnote{To compute the limiting variance in~\eqref{eq:finite_var}, we used a Monte Carlo approximation of $P_{\theta_0,m_0}[ {\ell}_{\theta_0,m_0}(Y,X) {\ell}^\top_{\theta_0,m_0}(Y,X)]$ with sample size $2\times 10^5$ and true $(m_0, \theta_0, P_X)$. The limiting covariance matrix $\Sigma^0 = 0.33 {I}_3 - 0.11 J_3$, where $I_3$ is the $3\times 3$ identity matrix and $J_3$ is the $3\times 3$ matrix of all ones.} In the left panel of Figure~\ref{fig:QQplot}, we present the Q-Q plot of $\sqrt{n}[\Sigma^0_{1,1}]^{-1/2}(\check{\theta}_{1}-\theta_{0,1})$  based on 800 replications; on the $x$-axis we have the quantiles of the standard normal distribution. The Q-Q plot validates the asymptotic normality and shows that the sample variance of the CLSE converges to the limiting variance found in Theorem~\ref{thm:Main_rate_CLSE}.  In the right panel of Figure~\ref{fig:QQplot}, we present empirical coverages (from $800$ replications) of $95\%$ confidence intervals based on  the CLSE constructed via~\eqref{eq:Conf_int}.
% We use this to validating the asymptotic properties derived in~Theorem~\ref{eq:globalEffCLSE}. We use~\eqref{eq:Conf_int} to construct a confidence interval and  for a simulated dataset as the sample size increases
}

\begin{figure}[!h]
  \begin{minipage}{0.50\linewidth}
    \centering
    %\includegraphics[width=\linewidth]{example-image}
  \includegraphics[width=\textwidth]{CvxLLSEQQplot_NEW.pdf}%
      \par

  \end{minipage}%
  \begin{minipage}{0.50\linewidth}
    \centering
\begin{tabular}{rcc}
  \toprule
  % \multicolumn{2}{c}{$n$}
  % \cmidrule(r){2-3} \cmidrule(rl){4}\\
   \multirow{2}{*}{$n$} &  \multicolumn{2}{c}{CLSE}\\
   \cmidrule(lr){2-3} 
 & Coverage & Avg Length\\
  \hline
  50  & 0.92 & 0.30 \\
  100  & 0.91 & 0.18 \\
  200  & 0.92 & 0.13 \\
  500  & 0.94 & 0.08 \\
  1000  & 0.93 & 0.06 \\
  % 2000  & 0.92 & 0.04 \\
   \hline
   
\end{tabular}

    \par
\end{minipage}
\caption{Summary of $\check{\theta}$ (over  800 replications) based on $n$ i.i.d.~observations from the model~\ref{sec:cfd_set}. Left panel:  Q-Q plots for $\sqrt{n}\left[\Sigma^0_{1,1}\right]^{-1/2}(\check{\theta}_{1}-\theta_{0,1})$  for  $n\in \{100, 500,1000, 2000\}$. The dotted black line corresponds to the $y=x$ line; right panel: estimated coverage probabilities and average lengths of nominal $95\%$ confidence intervals for the first coordinate of $\theta_0$. }%}
\label{fig:QQplot}
\end{figure}



% \todo[inline]{Remark about small $\hat{\lambda}$ choice doesn't matter and choice of $L$}

\section{Simulation study}\label{sec:Simul_Cvx}
{In Section~\ref{sec:compute} of the supplementary file, we develop an alternating minimization algorithm to compute the CLSE~\eqref{eq:CLSE}. 
  In this section we illustrate the finite sample performance of the CLSE using the implementation in the \texttt{R} package \if1\blind{\texttt{simest}.} \fi  \if0\blind{\texttt{***}}\fi}. We also compare its performance with other existing estimators, namely, the \texttt{EFM} estimator (the estimating function method; see \cite{cuietal11}), the \texttt{EDR} estimator (effective dimension reduction; see~\citet{Hristacheetal01}), and the estimator proposed in~\cite{Patra16} with the tuning parameter chosen by generalized cross-validation (\cite{Patra16}; we denote this estimator by \texttt{Smooth}).
% We use $(\hat{m}_{SS}, \hat{\theta}_{SS})$ to denote the estimator proposed in Section~\ref{smooth:sec:prelim} when the tuning parameter is chosen by generalized cross-validation; \cite{wahba90}. $(\hat{m}, \hat{\theta})$, $(\check{m}, \check{\theta})$, and $(m^\dagger, \theta^\dagger)$ denote the PLSE, the CLSE, and the LSE estimators, respectively.
We use \texttt{CvxLip} to denote the CLSE.


\subsection{Another convex constrained estimator}
Alongside these existing estimators, we also numerically study  another natural estimator under the convexity shape constraint --- the convex LSE --- denoted by \texttt{CvxLSE} below. This estimator is obtained by minimizing the sum of squared errors  subject to only the convexity constraint. Formally, the \texttt{CvxLSE} is
\begin{equation} \label{eq:ultimate_est}
(m^\dagger_n, \theta^\dagger_n):= \argmin_{(m, \theta)\in\, \mathcal{C}\times\Theta} Q_n(m, \theta).
\end{equation}
{The computation of \texttt{CvxLSE} is discussed in Remark~\ref{rem:CvxLSE} and is implemented in the \texttt{R} package \if1\blind{\texttt{simest}.} \fi  \if0\blind{\texttt{***}}\fi. }However, theoretical analysis of this estimator is difficult because of various reasons; see Section~\ref{sec:discussion_on_the_theoretical_analysis_of_the_texttt_CvxLSE} of the supplementary file for a brief discussion. %Although we present some evidence that our estimators $\check{\theta}_n$ and $\hat{\theta}_n$ are robust to the choice of their respective tuning parameters $L$ and $\lambda$, an application of this methods still requires the user to choose them. In this regard, $\theta^{\dagger}_n$ could be more bankable if its theory checks out since there is no tuning parameter to pick.
In our simulation studies we observe that the performance of  \texttt{CvxLSE} is very similar to that of \texttt{CvxLip}.
%, and \texttt{CvxLSE} to denote the convex LSE estimator proposed in~\eqref{eq:ultimate_est}

In what follows, we will use $(\tilde{m},\tilde{\theta})$ to denote a generic estimator that will help us describe the quantities in the plots; e.g., we use $ \|\tilde{m}\circ \tilde{\theta}-m_0\circ\theta_0\|_n =[\frac{1}{n}\sum_{i=1}^n (\tilde{m}( \tilde{\theta}^\top x_i)-m_0(\theta_0^\top x_i))^2]^{1/2}$ to denote the in-sample root mean squared estimation error of $(\tilde{m},\tilde{\theta})$, for all the estimators considered. From the simulation study it is easy to conclude that the proposed estimators have superior finite sample performance in most sampling scenarios considered.

\begin{figure}[!ht]
% \captionsetup[subfigure]{labelformat=empty}
\centering
\includegraphics[width=.75\textwidth]{HeavytailExample3CuiEtAl1stRevison.pdf}\\ [-2ex]%%
\caption{Boxplots of $\sum_{i=1}^d |\tilde{\theta}_i-\theta_{0,i}|/d$ (over 500 replications) based on $100$ observations from the simulation setting in Section~\ref{sub:increasing_dimension} for dimensions $10,$ $25,$ $50,$ and $100,$ shown in the top-left, the top-right, the bottom-left, and the bottom-right panels, respectively. The bottom-right panel doesn't include EDR as the R-package \texttt{EDR} does not allow for $d=100.$}
\label{fig:Ex3Cuietal_homo}
%% label for entire figure
\end{figure}





% \begin{figure}
% \begin{floatrow}
% \ffigbox{%

%   \includegraphics[width=.44\textwidth]{CvxCLSEQQplot.pdf}%
% }{%
%   \caption{QQ-plots for $\sqrt{n}(\tilde{\theta}_{1}-\theta_{0,1})$ (over 800 replications) based on i.i.d.~samples from~\eqref{eq:qqplot} for  $n\in \{100, 500,1000\}$. The solid black line corresponds to the $Y=X$ line. Left panel: \texttt{CvxPen}; right panel: he estimated coverage probabilities and average lengths of nominal $95\%$ confidence intervals for the first coordinate of $\theta_0$. }%
% }
% \capbtabbox{%
% \begin{tabular}{rcc}
%   \toprule
%   % \multicolumn{2}{c}{$n$}
%   % \cmidrule(r){2-3} \cmidrule(rl){4}\\
%    \multirow{2}{*}{$n$} &  \multicolumn{2}{c}{\texttt{CvxLip}}\\
%    \cmidrule(lr){2-3} 
%  & Coverage & Avg Length\\
%   \midrule
%   50  & 0.92 & 0.30 \\
%   100  & 0.91 & 0.18 \\
%   200  & 0.92 & 0.13 \\
%   500  & 0.94 & 0.08 \\
%   1000  & 0.93 & 0.06 \\
%   2000  & 0.92 & 0.04 \\
%    \bottomrule
%    \vspace{.15in}
% \end{tabular}
% }{%
%   \caption{The estimated coverage probabilities and average lengths (obtained from 800 replicates)  of nominal $95\%$ confidence intervals for the first coordinate of $\theta_0$ for the model described in Section~\ref{sub:verifying_the_asymptotics}}%
% }
% \end{floatrow}
% \end{figure}

% \begin{table}[!ht]
% \caption[Coverage of first coordinate]{The estimated coverage probabilities and average lengths (obtained from 800 replicates)  of nominal $95\%$ confidence intervals for the first coordinate of $\theta_0$ for the model described in Section~\ref{sub:verifying_the_asymptotics}.}
% \begin{tabular}{rcc}
%   \toprule
%   % \multicolumn{2}{c}{$n$}
%   % \cmidrule(r){2-3} \cmidrule(rl){4}\\
%    \multirow{2}{*}{$n$} &  \multicolumn{2}{c}{\texttt{CvxLip}}\\
%    \cmidrule(lr){2-3} 
%  & Coverage & Avg Length\\
%   \midrule
%   50  & 0.92 & 0.30 \\
%   100  & 0.91 & 0.18 \\
%   200  & 0.92 & 0.13 \\
%   500  & 0.94 & 0.08 \\
%   1000  & 0.93 & 0.06 \\
%   2000  & 0.92 & 0.04 \\
%    \bottomrule
% \end{tabular}
% \label{tab:coverage_d1}
% % search "tab:coverage_d1" in sublime for code file
% \end{table}
% \vspace{-.4in}
\subsection{Increasing dimension} % (fold)
\label{sub:increasing_dimension}
To illustrate the behavior/performance of the estimators  as $d$ grows, we consider the following single index model $Y=  (\theta_0^\top X)^2 + t_6, \text{ where } \theta_0= (2, 1, \mathbf{0}_{d-2})^\top/\sqrt{5} \text{ and } X\in \R^d \sim\text{Uniform}[-1,5]^d,$
where $t_6$ denotes the Student's $t$-distribution with $6$ degrees of freedom.
In each replication we observe $n= 100$ i.i.d.~observations from the model. It is easy to see that the performance of all the estimators worsen as the dimension increases from $10$ to $100$ and \texttt{EDR} has the worst overall performance; see Figure~\ref{fig:Ex3Cuietal_homo}. However when $d=100$, the convex constrained estimators have significantly better performance. This simulation scenario is similar  to the one considered in Example 3 of Section 3.2 in~\cite{cuietal11}.
% subsection increasing_dimension (end)
% the following works.
% \vspace{-.1in}


% subsection verifying_the_asymptotics_ (end)
% subsection subsection_name (end)

\subsection{Choice of \texorpdfstring{$L$}{Lg}} % (fold)
\label{sub:robustness_of_choice_of_}
In this subsection, we consider a simple simulation experiment to demonstrate that the finite sample performance of the CLSE is robust to  the choice of tuning parameter. We generate an i.i.d.~sample (of size $n= 500$) from the following model:
\begin{equation}\label{eq:robust}
Y=(\theta_0^\top X)^2 + N(0, .1^2), \quad \text{where}\; X \sim \text{Uniform} [-1, 1]^4\; \text{and} \; \theta_0 = (1,1,1,1)^\top/2.
\end{equation}
Observe that, we have   $-2 \le \theta^\top X \le 2$ and  $L_0:= \sup_{t \in [-2,2]} m_0'(t)= 4$ as $m_0(t)= t^2.$ To understand the effect of $L$ on the performance of the CLSE,  we  show the box plot of $\sum_{i=1}^4|\check{\theta}_i-\theta_{0,i}|/4$  as $L$ varies from $3\, (< L_0)$ to $10$ in Figure~\ref{fig:robust}. Figure~\ref{fig:robust} also includes the \texttt{CvxLSE} which corresponds to $L=\infty$.  The plot clearly show that the performance of \texttt{CvxLip}  is not significantly affected by the particular choice of the tuning parameter. The observed robustness in the behavior of the estimators can be attributed to the stability endowed  by the convexity constraint.
\begin{figure}[!ht]
% \captionsetup[subfigure]{labelformat=empty}
\centering
\includegraphics[width=.5\textwidth]{rboust_plot_finalLLSEonly.pdf}\\ %%
% \caption[Boxplots of estimates to study their robustness]{Box plots of $\frac{1}{4}\sum_{i=1}^4|\tilde{\theta}_i-\theta_{0,i}|$ (over $1000$ replications) for the model~\eqref{eq:robust} ($d=4$ and $n=500$) as the tuning parameter varies. Left panel: \texttt{CvxPen} when $\lambda_n= \exp(-T)\times n^{1/5}$ for $T={\{0,0.7, 1, 2, 5,7\}}$; right panel: \texttt{CvxLip} for $L= \{3, 4, 5,7, 10\}$ and \texttt{CvxLSE}. }
\caption[Boxplots of estimates to study their robustness]{Box plots of $\frac{1}{4}\sum_{i=1}^4|\tilde{\theta}_i-\theta_{0,i}|$ (over $1000$ replications) for the model~\eqref{eq:robust} ($d=4$ and $n=500$) \texttt{CvxLip} for $L= \{3, 4, 5,7, 10\}$ and \texttt{CvxLSE} (i.e., $L=\infty$). }
% \clr{Both plots have $1000$ replications}.}
\label{fig:robust}
%% label for entire figure
\end{figure}
\section{Real data analysis} % (fold)
\label{sec:real_data_analysis}


In this following we analyze the two real datasets discussed in Examples~\ref{ex:boston} and~\ref{ex:car}.
% and apply the developed methodology for prediction and estimation.

\subsection{Boston housing data}\label{sec:boston}

% \citet{harrison1978hedonic} studied the effect of different covariates on real estate price in the greater Boston area.  The response variable $Y$ was the log-median value of homes in each of the $506$ census tracts in the Boston standard metropolitan area. A single index model is  appropriate for this dataset; see e.g.,~\cite{gu2015oracally,MR2529970,MR2589322,MR2787613}. The above papers considered the following covariates in their analysis: average number of rooms per dwelling, full-value property-tax rate per $10000$ USD, pupil-teacher ratio by town school district, and proportion of population that is of ``lower (economic) status'' in percentage points. In the left panel of Figure~\ref{fig:real_data_plot_prelim}, we provide a scatter plot of  $\{(Y_i, \hat{\theta}^\top X_i)\}_{i=1}^{506}$, where $\hat{\theta}$ is an estimate of $\theta_0$ {\clr obtained in~\cite{Patra16}}. We also plot estimates of $m_0$ {\clr obtained from~\cite{Patra16} and~\cite{MR2529970}}. The plot suggests a convex and nondecreasing relationship between the log-median home prices and the index, but the fitted link functions satisfy these shape constraints only approximately. 

We briefly recall the discussion in Example~\ref{ex:boston}. The Boston housing dataset was collected by \cite{harrison1978hedonic} to study the effect of different covariates on the real estate price in the greater Boston area. The dependent variable $Y$ is the  log-median value of homes in each of the $506$ census tracts in the Boston standard metropolitan area. \citet{harrison1978hedonic} observed $13$ covariates and fit a linear model after taking $\log$ transformation for $3$ covariates and power transformations for three other covariates; also see \cite{MR2589322} for a discussion of this dataset.


\citet{breiman1985estimating} did further analysis to deal with multi-collinearity of the covariates and selected four variables using a penalized stepwise method. The chosen covariates were:  average number of rooms per dwelling (RM), full-value property-tax rate per $10,000$ USD (TAX), pupil-teacher ratio by town school district (PT), and proportion of population that is of ``lower (economic) status'' in percentage points (LS). Following \cite{MR2529970} and \cite{MR2787613}, we take logarithms of LS and TAX to reduce sparse areas in the dataset.  Furthermore, we have scaled and centered each of the covariates to have mean $0$ and variance $1.$  \citet{MR2529970} fit a nonparametric additive regression model to the selected variables and obtained an $R^2$ (the coefficient of determination) of $0.64$. \citet{MR2589322} fit a single index model to this data using the set of covariates suggested in \cite{MR1624402}. In~\cite{gu2015oracally}, the authors create $95\%$ uniform confidence band for the link function and reject the null hypothesis that the link function is linear. Both in~\cite{gu2015oracally} and~\cite{MR2589322}, the fitted link function is approximately nondecreasing and  convex; see Figure~2 of \cite{MR2589322} and Figure~5 of~\cite{gu2015oracally}.  This motivates us to fit a \textit{nondecreasing} and convex single index model to the Boston housing dataset. 
% Moreover, the shape constraints add interpretability to the estimators of both $\theta_0$ and $m_0$.}
 In particular, we consider the following estimator:
\begin{equation}\label{eq:CLSE_nondec}
(\hat{m}_{L},\hat{\theta}_{L}) \coloneqq \argmin_{\substack{\theta \in  \Theta \\  m \in \M_L \cap \,\mathcal{N} } }\; \sum_{i=1}^n (Y_i - m(\theta^\top X_i))^2,
\end{equation}
where $\mathcal{N}$ is the set of real-valued nondecreasing functions on $D$.
Following the discussions in~Remarks~\ref{rem:Monotone} and~\ref{rem:Efficency_additional}, we observe that the results in this paper also hold for $(\hat{m}_L,\hat{\theta}_L)$. The computation of the CLSE under the additional monotonicity constraint is discussed in~Remark~\ref{rem:Add_mono} and implemented in the accompanying R package. 

We summarize our results in Table~\ref{tab:real_dat}. We call $(\hat{m}_L,\hat{\theta}_L)$, the \texttt{MonotoneCLSE}. In Figure~\ref{fig:real_data_plot}, we plot the scatter plot of $\{(\hat{\theta}^\top_L X_i, Y_i)\}_{i=1}^{506}$  overlaid with the plot of $\hat{m}_L(\cdot)$ and the regression splines based estimator of~\cite{MR2529970}.  For \texttt{MonotoneCLSE} and \texttt{CvxLip}, we chose $L= 30$ (an arbitrary but large number). We also observe that the $R^2$ for the monotonicity and convexity constrained (\texttt{MonotoneCLSE}) and just convexity constrained single index models (\texttt{CvxLip} and \texttt{CvxLSE}), when using all the available covariates, is approximately $0.80$. {To further understand the predictive properties of the estimators under different smoothness and shape constraints, in Table~\ref{tab:real_dat} we report the $5$-fold cross-validation error averaged over 100 random partitions.  The large cross-validation error for the \texttt{CvxLSE} is due to over-fitting of $m_n^\dagger$ at the boundary of its support; see~Figure~\ref{fig:SimplModel} for an illustration of this boundary effect.}

 %Inclusion of all the extra variables leads to only a minor increase in $R^2$ at the cost of interpretability; \cite{MR2589322} also reached to a similar conclusion.}
% \todo[inline]{({\color{red} Cross-validated prediction error??}).}
% \begin{itemize}
%   \item  \cite{breiman1985estimating} did further analysis to reduce the number of ``impottant variable to 4'' see Wang, Yang 09
%   \item Wang and and Yang take log for  \texttt{LS} \texttt{TAX} so  as not to have gaps. They fit a additive regression model and get a correlation of .80. We get a correlation of .87
%   \item Need to understand \cite{MR2589322}
%   \item see \verb|Boston_MaY_final_analysis.R| file in Smooth single index model folder.
% \end{itemize}



\subsection{Car mileage data}\label{sec:car}
First, we briefly recall the discussion in Example~\ref{ex:car}. We consider the car mileage dataset of~\citet{cars_1983} for a second application for the convex single index model. We model the mileage ($Y$) of $392$ cars using the covariates ($X$):  displacement (Ds),   weight (W),  acceleration (A),   and horsepower (H).  \citet{MR2957294} fit a partial linear model to this this dataset, while \cite{Patra16} fit a single index model (without any shape constraint). The ``law of diminishing returns'' suggests $m_0$ should be convex and nonincreasing. However, the estimators based only on smoothness assumptions satisfy these shape constraints only approximately.  In the right panel of Figure~\ref{fig:real_data_plot}, we fit a convex and nonincreasing single index model. 
 % Our analysis reinforces the notion that cars with higher acceleration and/or weight generally have lower mileage.

We have scaled and centered each of covariates to have mean $0$ and variance $1$ for our analysis, just as in Section~\ref{sec:boston}. We performed a test of significance for $\theta_0$ using the plug-in variance estimate in Section~\ref{sec:cfd_set}. The covariates  A, Ds, and  H were found to be significant and each of them had $p$-value less than $10^{-5}$. In the right panel of  Figure~\ref{fig:real_data_plot}, we have the scatter plot of $ \{(\hat{\theta}_L^\top X_i, Y_i)\}_{i=1}^{392}$ overlaid with the plot of $\hat{m}_L(\cdot)$ and regression splines based estimator obtained in~\cite{MR2529970}; here $\hat{\theta}_L$ is defined as in~\eqref{eq:CLSE_nondec} but $\mathcal{N}$ now denotes the class of real-valued \textit{nonincreasing} functions on $D$. Table~\ref{tab:real_dat} lists different estimators for $\theta_0$ and their respective $R^2$ and cross-validation errors.
  \begin{table}[h]
\caption[Estimates of $\theta_0$  and generalized $R^2$ for the datasets in Sections~\ref{sec:boston} and \ref{sec:car}. {\texttt{EFM} and \texttt{EDR} do not provide a function estimator and hence we do not show an $R^2$ value.}]{Estimates of $\theta_0$  and generalized $R^2$ for the datasets in Sections~\ref{sec:boston} and \ref{sec:car}. {\texttt{EFM} and \texttt{EDR} do not provide a function estimator and hence we do not show an $R^2$ value. { CV-error denotes out of  5-fold cross validation averaged over 100 random partitions.} }}\label{tab:real_dat}
\centering
\begin{adjustbox}{max width=\textwidth}

\begin{tabular}{l*{13}{c}}
 \toprule
\multirow{2}{*}{Method} &\multicolumn{6}{c}{Boston Data}& &\multicolumn{6}{c}{Car mileage data}  \bigstrut \\
\cmidrule(rl){2-7} \cmidrule(rl){9-13}
  &RM& $\log(\text{TAX})$ & PT &$\log(\text{LS})$& $R^2$&  CV-error & & Ds & W  &A&H& $R^2$& CV-error \\

 \midrule
\texttt{LM}\footnote{\texttt{LM} denotes the linear regression model.}  &  2.34 & $-0.37$ & $-1.55$ & $-5.11$ &  0.73 & 20.75& & $-0.63$ & $-4.49$ & $-0.06$ & $-1.68$ &  0.71 & 18.61\\
\texttt{Smooth} &  0.44 & $-0.18$ & $-0.27$ & $-0.83$ &  0.77 & 17.80& &0.42 &  0.18 &  0.11 &  0.88 &  0.76 &15.29\\
%\texttt{CvxPen} &  0.48 & $-0.19$ & $-0.25$ & $-0.82$ &  0.77 &  &0.45 &  0.15 &  0.13 &  0.87 &  0.76 \\
\texttt{MonotoneCLSE} &  0.49 & $-0.21$ & $-0.25$ & $-0.81$ &  0.80 & 17.93& &0.44 &  0.17 &  0.13 &  0.87 &  0.76 & 15.34\\
\texttt{CvxLip} &  0.48 & $-0.23$ & $-0.26$ & $-0.80$ &  0.80 & 17.93&  &0.44 &  0.18 &  0.12 &  0.87 &  0.76& 15.22 \\
% [1]  0.4796955 -0.2341851 -0.2631070 -0.8036319
\texttt{CvxLSE} &  0.43 & $-0.20$ & $-0.28$ & $-0.84$ &  0.80 & 21.44&  &0.39 &  0.14 &  0.12 &  0.90 &  0.77 &16.38\\
\texttt{EFM} &  0.48 & $-0.19$ & $-0.21$ & $-0.83$ &  --- & --- & &0.44 &  0.18 &  0.13 &  0.87 &--- &  --- \\
\texttt{EDR} &  0.44 & $-0.14$ & $-0.18$ & $-0.87$ &  --- &--- &  &0.33 &  0.11 &  0.15 &  0.93 & --- & --- \\

\bottomrule



\end{tabular}
\end{adjustbox}

\end{table}

   % we display the estimates of $\theta_0$  based on the methods considered in the paper. The MAVE, the EFM estimator, and the PLSE give similar estimates while the EDR gives a different estimate of the index parameter.
\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{real_data_plot1stRev_WY.pdf}
%% scale=.8
  \caption[]{Scatter plots of $\{(X_i^\top\check{\theta}, Y_i)\}_{i=1}^n$ overlaid with the plots of function estimates proposed in~\cite{MR2529970} (red, dot-dashed) and monotonicity constrained CLSE proposed in this paper (blue, solid) for the two real datasets considered. Left panel: Boston housing data (Section~\ref{sec:boston}), nondecreasing CLSE; right panel: the car mileage data (Section~\ref{sec:car}), nonincreasing CLSE.}
  \label{fig:real_data_plot}
\end{figure}

\section{Discussion} \label{sec:discussion}
  In this paper we have proposed and studied a Lipschitz constrained LSE in the convex single index model. Our estimator  of the regression function is minimax rate optimal (Proposition~\ref{lem:MinimaxLowerbound}) and  the estimator of the index parameter is semiparametrically efficient when the errors happen to be homoscedastic (Theorem~\ref{thm:Main_rate_CLSE}). This work represents the first in the literature of semiparametric efficiency of the LSE when the nonparametric function estimator is non-smooth and parameters are bundled. Our proof of semiparametric efficiency is geometric and provides a general framework that can be used to prove efficiency of estimators in a wide variety of semiparametric models even when the estimators do not satisfy the efficient score equation directly; see sketch of proof of Theorem~\ref{thm:Main_rate_CLSE} and Example~\ref{ex:coxmodel} in Section~\ref{sec:SemiCLSE}. 

  % {\clr 
  % I don't think we should stress too much about why CvxLse is hard. I guess it would be enough to say ``An interesting future direction would be the study of CvxLSE introduced in the simulation section. This requires a control of the function estimator near the boundary of its domain and generalizations of our theorems 3.2, 3.5 and 3.7.'' Something from below is
  % ``if one can prove results similar to Theorems~\ref{thm:rate_m_theta_CLSE}--\ref{thm:rate_derivCLSE} for the convex LSE, then the techniques used in Section~\ref{sec:SemiInf} can be readily applied to prove asymptotic normality of $\theta^\dagger$. These challenges make the study of the convex LSE a very interesting problem for future research''}
  % % {\clg The new result allows the errors to have only finite moments and to depend on the covariates. }


{Theorem~\ref{thm:rate_m_theta_CLSE} proves the worst case rate of convergence for the CLSE.  It is well-known in convex regression that if the true regression function is piecewise linear, then the LSE converges at a much faster (near parametric) rate~\cite{MR3881209}. This behavior is called the \emph{adaptation} property of the LSE.
It is natural to wonder if such a property also holds for $\check{m}\circ\check\theta$. In Section~\ref{sub:investigation_of_the_adaptation_of_the_clse} of the supplementary file, we investigate the behavior of  $\check{m}\circ\check\theta$ and $\check{\theta}$ (as sample size increases) when $m_0$ is piecewise linear. The simulation suggests that $\check{m}\circ\check{\theta}$ converges at a near parametric rate when $m_0$ is piecewise linear. However a formal proof of this is beyond the scope of this paper as it requires different techniques. Furthermore, the asymptotic behavior of $\check \theta$ in this setting is an open problem. 
% the effect of the adaptive behavior of the CLSE on the asymptotic limit of $\check{\theta}$ is an open problem.


   % but it will require substantially different techniques. Furthermore, the effect of the adaptation behavior of the CLSE on the asymptotic limit of $\check{\theta}$ is an open problem, even in  the much simpler partial linear regression model. In Section~\ref{sub:investigation_of_the_adaptation_of_the_clse} of the supplementary file, we investigate the behavior of  $\check{m}\circ\check\theta$ and $\check{\theta}$ as sample size increases. However, given the focus on semiparametric efficiency in this paper, we refrain from further studying the adaptation behavior here. 
   }


% \begin{supplement}
%  \sname{Supplementary material for}
%  \stitle{Efficient Estimation in Convex Single Index Models}
%  \slink[doi]{10.1214/00-AOASXXXXSUPP}
%  \sdatatype{.pdf}
%  \sdescription{Due to space constraints, some finite sample examples and  the proofs of the results in the paper are relegated to the supplementary file.}
% \end{supplement}
% \noeqref{eq:deriv_theta_0}
% % \noeqref{eq:deriv_theta_hat}
% % \noeqref{eq:mprime_sup_theta}\noeqref{eq:BdryCond}
% \noeqref{eq:sup_rate}
% \noeqref{eq:true_mimina}
% \noeqref{eq:True_inf}
% \noeqref{eq:RAL_1}
% \noeqref{eq:rn_Major}
% \noeqref{eq:RAL_1}



\bibliographystyle{chicago}
{\bibliography{SigNoise}}
\input{JASA_supp4}
\end{document}

