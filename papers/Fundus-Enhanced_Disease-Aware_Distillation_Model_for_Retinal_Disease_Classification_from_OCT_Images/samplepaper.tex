% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}

\usepackage{subfigure}
\usepackage{color}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{bbold}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage{adjustbox}
\usepackage{arydshln}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{breakurl}
\usepackage{marvosym}

% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\wdai}[1]{{\color[rgb]{0.9,0.1,0.1}{#1}}}
\newcommand{\wdaicmnt}[1]{{\color[rgb]{0.9,0.1,0.1}{(WD:#1)}}}
\newcommand{\lhwang}[1]{{\color{blue}{[#1]}}}
\newcommand{\xmli}[1]{{\color{red}{[XM: #1]}}}

\begin{document}
%
% \title{Contribution Title\thanks{Supported by organization x.}}
% \title{Class-Aware Distillation Enhancement for OCT Retinal Disease Classification with Fundus Photographs}

% \title{Class-Aware Distillation Enhancement for Retinal Disease Classification from OCT Images}

\title{Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images}


\titlerunning{FDDM}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Lehan Wang\inst{1} \and
Weihang Dai\inst{1}\and
Mei Jin\inst{2} \and
Chubin Ou\inst{3} \and
Xiaomeng Li\inst{1}$^($\textsuperscript{\Letter}$^)$
}
% index{Wang, Lehan}
% index{Dai, Weihang}
% index{Jin, Mei}
% index{Ou, Chubin}
% index{Li, Xiaomeng}

\authorrunning{L. Wang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% \author{Anonymous}

\institute{The Hong Kong University of Science and Technology, Hong Kong, China \\
\email{eexmli@ust.hk}\\
\and
Department of Ophthalmology, Guangdong Provincial Hospital of Integrated Traditional Chinese and Western Medicine, Guangdong, China \\
\and
Guangdong Weiren Meditech Co., Ltd
}

% \institute{Anonymous Organization \\ \email{**@******.***}}

\maketitle              % typeset the header of the contribution

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etal{\emph{et al. }}
\def\vs{\textit{v.s. }}



%To complement OCT images with a global view of the retina, existing methods use multi-modal learning to provide additional context from fundus images. 
\begin{abstract}
Optical Coherence Tomography (OCT) is a novel and effective screening tool for ophthalmic examination. 
Since collecting OCT images is relatively more expensive than fundus photographs, existing methods use multi-modal learning to complement limited OCT data with additional context from fundus images. 
However, the multi-modal framework requires eye-paired datasets of both modalities, which is impractical for clinical use. 
To address this problem, we propose a novel fundus-enhanced disease-aware distillation model (\textbf{FDDM}), for retinal disease classification from OCT images. 
Our framework enhances the OCT model during training by utilizing unpaired fundus images and does not require the use of fundus images during testing, which greatly improves the practicality and efficiency of our method for clinical use.
%elegantly leverages the large-free available fundus photos to enhance the OCT model during training, where no fundus is required during testing.  
%allows the model to be trained with unpaired multi-modal data and inference to be performed using only OCT images instead of both modalities. 
Specifically, we propose a novel class prototype matching to distill disease-related information from the fundus model to the OCT model and a novel class similarity alignment to enforce consistency between disease distribution of both modalities.  Experimental results show that our proposed approach outperforms single-modal, multi-modal, and state-of-the-art distillation methods for retinal disease classification. Code is available at \href{https://github.com/xmed-lab/FDDM}{https://github.com/xmed-lab/FDDM}.

\keywords{Retinal Disease Classification \and Knowledge Distillation \and OCT Images}
\end{abstract}

% \wdai{(Any statistics? or any way to make it sound more serious? Idea is to make it sound like a extremely serious problem)}

\section{Introduction}
Retinal diseases are one of the most common eye disorders, which can lead to vision impairment and blindness if left untreated. 
%It is estimated that at least 2.2 billion people around the world suffer from retinal diseases, half of which could have been prevented with timely diagnosis~\cite{world2019world}. 
Computer-aided diagnosis has been increasingly used as a tool to detect ophthalmic diseases at the earliest possible time and to ensure rapid treatment. 
Optical Coherence Tomography (OCT) \cite{huang1991optical} is an innovative imaging technique with the ability to capture micrometer-resolution images of retina layers, which provides a deeper view compared to alternative methods, such as fundus photographs~\cite{muller2019ophthalmic}, thereby allowing diseases to be detected earlier and more accurately. 
Because of this, OCT imaging has become the primary diagnostic test for many diseases, such as age-related macular degeneration, central serous chorioretinopathy, and retinal vascular occlusion~\cite{ehlers2019retina}.
%Because of this, OCT imaging has been treated as the golden standard input modality for computer-aided retinal disease detection in recent years.
% \xmli{The advantages of OCT should be revised.}

% To detect ophthalmic disease at the earliest possible stage, computer-aided diagnosis has been employed to conduct retinal image classification. 
% As the most widespread imaging technique, fundus photography shows a relatively large scope of an eye, including retina, optic nerve, and blood vessels, which has been proved to be helpful in the automatic diagnosis of various diseases \cite{cheng2013superpixel,chen2015glaucoma,gargeya2017automated,li2018efficacy,zago2020diabetic,gour2021multi}. Nevertheless, it is challenging to discover retinal abnormalities only through fundus images due to the lack of depth appreciation.

% achieving earlier and more confident detection of most diseases \cite{wang2016comparison}. 
% Therefore, OCT-based computer-aided retinal disease detection has become increasingly prevalent in recent years.
% Thus, more attention has been drawn on Optical Coherence Tomography (OCT) \cite{huang1991optical} considering its capability to capture micrometer-resolution three-dimensional images of retina layers with a deeper view \cite{muller2019ophthalmic}. Researches confirm that OCT can reflect significant features that is invisible in commonly used fundus photographs, achieving earlier and more confident detection of most diseases \cite{wang2016comparison}. Since then, OCT-based computer-aided retinal disease detection has become increasingly prevalent in recent years.

Traditional methods manually design OCT features and adopt machine learning classifiers for prediction \cite{liu2011automated,srinivasan2014fully,lemaitre2016classification}. 
In recent years, deep learning methods have achieved outstanding performance on various medical imaging analysis tasks and have also been successfully applied to retinal disease classification with OCT images \cite{lee2017deep,karri2017transfer,kermany2018identifying}.
However, diagnosing disease with a single OCT modality, as shown in Fig.~\ref{fig:setting} (a), is still challenging since OCT scans are inadequate compared with fundus photos due to their more expensive cost in data collection.
Some methods attempt to use extra layer-related knowledge from the segmentation task to improve prediction despite limited OCT data \cite{li2019deep,huang2019automatic,fang2019attention,liu2021one}, but this leads to increased training costs since an additional segmentation model is required. 

% With the development of deep learning, diagnosis algorithms have dramatically improved. 
% \sout{Deep learning based OCT classification methods generally fall into two categories, employing specific networks and utilizing additional information.
% % Basically, OCT classification networks can be divided into scan-level classification and volume-level classification. Scan-level OCT classification networks utilize 2D OCT B-scans as input, and the prediction is supervised by instance-level labels. Other methods directly generate classification results from 3D OCT cubes, which alleviate the burden to collect fine-grained annotations for each scan and enable the model to extract spatial information inside volumes as well. 
% Basically, OCT classification networks are designed as convolutional neural networks (CNN) to extract high-level features related to diseases.}
% Lee \etal \cite{lee2017deep} modified VGG16 to form a deep learning model for age-related macular degeneration detection. Kermany \etal \cite{kermany2018identifying} transferred knowledge from natural image classification to diabetic macular edema, choroidal neovascularization, and drusen diagnosis tasks.
% However, as shown in Fig.~\ref{fig:setting} (a), diagnosis with a single OCT modality is still challenging since the global view of the retina is not captured. \xmli{This is not correct.}
% Moreover, additional modalities are employed to promote retinal disease classification, which is known as multi-modal learning shown in Fig.~\ref{fig:setting} (b).

Recent works have attempted to include additional modalities for classification through multi-modal learning shown in Fig.~\ref{fig:setting} (b),
where fundus and OCT images are jointly used to detect various retinal diseases and achieve promising results~\cite{yoo2019possibility,wang2019two,wang2022learning,ou2021m,he2021multi,li2021multi,li2022multimodal}.
% Multi-modal classification based on joint fundus and OCT images is one of the most popular approaches, and has promising results when detecting age-related macular degeneration (AMD), diabetic retinopathy (DR) and glaucoma \cite{yoo2019possibility,wang2019two,wang2022learning,ou2021m,he2021multi,li2021multi,li2022multimodal}.
% \wdai{(can describe in very short sentences)}
%based on a VGG model and a random forest classifier. 
% Yoo \textit{et al.} \cite{yoo2019possibility} first AMD detection with paired fundus and OCT images to prove the effectiveness of combined modalities. 
Wang \textit{et al.} \cite{wang2019two,wang2022learning} used a two-stream structure to extract fundus and OCT features, which are then concatenated for prediction. He \textit{et al.} \cite{he2021multi} designed modality-specific attention networks to tackle differences in modal characteristics.
% To tackle differences in modal characteristics, He \textit{et al.} \cite{he2021multi} designed modality-specific attention networks to obtain multi-scale information from fundus photos and reduce background noise in OCT images.
\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\textwidth]{imgs/setting-v2.pdf}
	\caption{Comparison between (a) single-modal, (b) multi-modal learning, and (c) our proposed distillation enhancement method. Under our setting, images from an additional modalitiy are only used for model training and are not required for inference.}
        % \wdai{better to describe using one sentence. Move entire figure to 2nd page, not 3rd page}
	\label{fig:setting}
\end{figure}
% \wdai{(I think can put in one paragraph) Nevertheless, there are still major limitations in these existing approaches. }
% However, previous works are confronted with several limitations. 
Nevertheless, there are still limitations in these existing approaches.
Firstly, existing multi-modal learning approaches require strictly paired images from both modalities for training and testing. This necessitates the collection of multi-modal images for the same patients, which can be laborious, costly, and not easily achievable in real-world clinical practice.
Secondly, previous works mostly focused on a limited set of diseases, such as age-related macular degeneration (AMD), diabetic retinopathy, and glaucoma, which cannot reflect the complexity and diversity of real-world clinical settings.  
% Firstly, most public multi-modal datasets containing OCT and fundus images are limited to only a specific set of diseases \xmli{specific.}. This differs from realistic clinical settings, which can include more complicated conditions. 
% % detection of a single disease or classification into limited disease types, still remaining a large gap compared with clinical use. 
% Secondly, strictly paired images from both modalities are required for training and testing, which is expensive and impractical. Thirdly, existing fusion strategies work well when fundus and OCT models achieve similar accuracy but become less effective if performance on fundus modality is inferior to its OCT counterpart; see results in Table~\ref{tab:main-results}.
% \wdai{(need to explain this last part a bit more, or rephrase, I don't know what you mean)}. 

% Although automatic disease detection with OCT images or volumes has shown outstanding results, diagnosis with a single OCT modality is still challenging owing to the shortage of information in global view of retina. Hence, additional modalities are employed to promote retinal disease classification, which is known as multi-modal learning. Multi-modal classification based on joint fundus and OCT images is one of the most popular approaches, which displays promising performance in detecting age-related macular degeneration (AMD), diabetic retinopathy (DR) and glaucoma. However, previous works are confronted with several limitations. Firstly, most public multi-modal datasets containing OCT and fundus images focus on either detection of a single disease or classification into limited disease types, still remaining a large gap compared with clinical use. Secondly, during training and test process, both modalities are required to be available and strictly paired in an eye level, which is expensive and time-consuming to collect. Thirdly, existing works neglects the effectiveness of transferring implicit semantic information from inferior fundus modality in a multi-label scenario. 

% \xmli{Free lunch knowledge. Unpaired Fundus to OCT distillation - }

To this end, we propose \textbf{F}undus-enhanced \textbf{D}isease-aware \textbf{D}istillation \textbf{M}odel (\textbf{FDDM}) for retinal disease classification from OCT images, as shown in Fig.~\ref{fig:setting} (c). 
FDDM is motivated by the observation that fundus images and OCT images provide complementary information for disease classification. For instance, in the case of AMD detection, fundus images can provide information on the number and area of drusen or atrophic lesions of AMD, while OCT can reveal the aggressiveness of subretinal and intraretinal fluid lesions~\cite{yoo2019possibility}. Utilizing this complementary information from both modalities can enhance AMD detection accuracy. 

Our main goal is to extract disease-related information from a fundus teacher model and transfer it to an OCT student model, all without relying on paired training data. To achieve this, we propose a class prototype matching method to align the general disease characteristics between the two modalities while also eliminating the adverse effects of a single unreliable fundus instance.
Moreover, we introduce a novel class similarity alignment method to encourage the student to learn similar inter-class relationships with the teacher, thereby obtaining additional label co-occurrence information. Unlike existing works, our method is capable of extracting valuable knowledge from any accessible fundus dataset without additional costs or requirements. Moreover, our approach only needs one modality during the inference process, which can help \textit{greatly reduce the prerequisites for clinical application}.

% Unlike existing works, our method effectively extracts knowledge from a weaker teacher model and only needs one modality during inference, both of which \textit{significantly reduce the requirements for clinical application}.
% In the process, we also collect a multi-label dataset with fundus and OCT images consisting of 11 diseases for training and validation which we release publicly.

% \wdai{(can generally use a pattern like this, show what is your insight, then how you use this:) \textbf{We observe that }even though fundus images are less detailed then OCT images, they still contain semantically relevant information for disease classification xxxx. \textbf{Based on this insight}, we propose distilling disease-related knowledge from a relatively weak fundus teacher model to OCT student model, without the need of paired training data. \textbf{Specifically}, we propose class prototype matching to push closer general disease features between both modalities, and class similarity alignment to stimulate the student to learn accordant disease relationships with the teacher (I think can expand a bit more, try to explain why use these two methods). Unlike existing works, our method XXXXXX and only requires one modality during inference, which significantly reduces the requirements for clinical application XXX. In the process, we also collect a multi-label dataset with fundus and OCT images consisting 11 diseases for training and validation which we release publicly.  }
% We first collect a multi-label dataset with fundus and OCT images consisting 11 diseases. Then, we build a teacher model to extract features from fundus modality. Unlike existing work, our key idea is to distill disease-related knowledge from a relatively weak fundus teacher model to OCT student model without requiring eye-paired training data. Specifically, we propose class prototype matching to push closer general disease features between both modalities, and class similarity alignment to stimulate the student to learn accordant disease relationships with the teacher. % \wdai{(suggest adding additional sentence to emphasize why our approach is better). Unlike existing work, our approach XXXXXX}

To summarize, our main contributions include 1) We propose a novel fundus-enhanced disease-aware distillation model for retinal disease classification via class prototype matching and class similarity alignment; 2) Our proposed method offers flexible knowledge transfer from any publicly available fundus dataset, which can significantly reduce the cost of collecting expensive multi-modal data. This makes our approach more accessible and cost-effective for retinal disease diagnosis; 3) We validated our proposed method using a clinical dataset and other publicly available datasets. The results demonstrate superior performance when compared to state-of-the-art alternatives, confirming the effectiveness of our approach for retinal disease classification.
% To summarize, our main contributions include 1) We collect TOPCON-MM, the first public multi-modal multi-label dataset with both OCT and fundus images, as a benchmark for retinal disease classification; 2) We first propose a novel distillation enhancement for OCT retinal disease classification based on class prototype matching and class similarity alignment. Our method allows training with unpaired fundus images to facilitate OCT model and inference using only OCT images; 3) We validate our proposed method through experiments on a clinically collected dataset and other publicly available datasets, outperforming state-of-the-art alternatives.
% \begin{itemize}
%     \item We collect TOPCON-MM, the first public multi-modal multi-label dataset with both OCT and fundus images, as a benchmark for retinal disease classification.
%     \item We first propose a novel distillation enhancement for OCT retinal disease classification, which allows training with the guidance of unpaired fundus images to facilitate OCT model, while conducting inference without extra modality. 
%     % We further propose class prototype matching and class similarity alignment to extract implicit information from a less confident fundus teacher model.
%     \item We validate our method though experiments on a clinical collected dataset as well as other publicly available datasets and prove the efficiency of distillation enhancement over both single-modal and multi-modal networks on retinal disease classification task.
% \end{itemize}

% Then, knowledge distillation is introduced to transfer valuable information from fundus teacher model to OCT student model in a class-wise manner, amplifying disease-related features with supplementary from another modality. 
% Common knowledge distillation scheme includes logit-based distillation and feature-based distillation. The purpose of logit-based distillation is to force the student to mimic the softened outputs of the teacher. However, compared with OCT models, fundus models show less confidence in certain samples, thus introducing unexpected noise when directly distilling the soft targets. On the other hand, feature-based distillation focuses on encouraging the feature map between teachers and students to be consistent, but might neglect semantic differences of the same position in a multi-modal setting.

% Our method is composed of two kinds of distillation: class prototype matching and class similarity alignment. In class prototype matching,  we compress images into feature vectors and aggregate them in a class-wise manner to obtain class prototypes, which represents global embedding of each disease and alleviate negative influence of a single low-confidence instance. Then, we adopt KL Divergence to push those disease-related features closer between both modalities in a common feature space. In addition, we further propose class similarity alignment to stimulate teacher and student to generate an accordant class similarity matrix, which takes the general relations among diseases into consideration.
% Our method is composed of two kinds of distillation: class prototype matching and class similarity alignment. In class prototype matching,  we compress images into feature vectors and aggregate them in a class-wise manner to obtain class prototypes, and then push those disease-related features closer between both modalities in a common feature space. In addition, we further propose class similarity alignment to stimulate teacher and student to generate an accordant class similarity matrix, which takes the general relations among diseases into consideration.

\section{Methodology}
\label{sec:methodology}
% An overview of our framework is shown in Fig.~\ref{fig:method}. In this section, we first introduce the models for a single modality to extract specific features. Next, we describe our proposed disease-aware enhancement distillation, which is functioned by jointly employing class prototype matching and class similarity alignment.
% \wdai{(I usually write something like, ) Our method is based on two ideas: XXXXX, XXXXXX. An overview is shown in XXXX. We discuss in detail in the sections below. }
Our approach is based on two ideas: class prototype matching, which distills generalized disease-specific knowledge unaffected by individual sample noise, and class similarity alignment, which transfers additional label co-occurrence information from the teacher to the student. Details of both components are discussed in the sections below. An overview of our framework is shown in Fig.~\ref{fig:method}.

% \wdai{(I suggest organizing this way)
% Our approach is based on two ideas, class prototype matching, which XXXXXXXXXXXX (why use this?) and class similarity alignment, which XXXXXX (why use this?). Details of both components are discussed in the sections below. An overview of our framework is shown in Fig.~\ref{fig:method}. 

% (Define annotations)
We denote the fundus dataset as $D_f = \{x_{f,i}, y_{f,i}\}_{i=1}^N$, and the OCT dataset as  $D_o = \{x_{o,j}, y_{o,j}\}_{j=1}^M$. To utilize knowledge from the fundus modality during training, we build a teacher model, denoted $F_t$, trained on $D_f$. Similarly, an OCT model $F_s$ is built to learn from OCT images $D_o$ using the same backbone architecture as the fundus model. We use binary cross-entropy loss as the classification loss $\mathcal{L}_{CLS}$ for optimization, to allow the same input to be associated with multiple classes. During inference time, only OCT data is fed into the OCT model to compute the probabilities $\boldsymbol{p}=\{p^c\}_{c=1}^C$ for each disease, $c$. 
% }

% \wdai{(}
% An overview of our framework is shown in Fig.~\ref{fig:method}. We consider a set of training data which comprises unpaired fundus and OCT images and test data with only OCT modality available. % \wdai{I data paired or unpaired?} 
% Aiming at utilizing additional knowledge from fundus modality during training process, we first build a teacher model denoted $F_t$ trained by fundus data $\{x_{f,i}, y_{f,i}\}_{i=1}^N$ in the training set, and train the model using binary cross-entropy loss.
% % \begin{equation}
% %     \mathcal{L}_{fundus}=-\sum_{i=1}^N{\sum_{c=1}^C{y_{f,i}^c}\log{p_{f,i}^c}+(1-y_{f,i}^c)\log{(1-p_{f,i}^c)}}
% % \end{equation}
% % where $p_{f,i}^c$ denotes the prediction score that the $i^{th}$ fundus instance belongs to class $c$.

% Similarly, an OCT model $F_s$ is built to learn from OCT images $\{x_{o,j}, y_{o,j}\}_{j=1}^M$ where the same backbone as the fundus model is employed.
% \begin{align}
%     v_{o,i}&={\rm CNN_o}(x_{o,i}) \\
%     {p}_{o,i}&={\rm Sigmoid}({\rm FC_o}(v_{o,i}))
% \end{align}
% The classification loss is formulated as:
% \begin{equation}
% \label{eq:loss-function}
%     \mathcal{L}_{CLS}=-\sum_{j=1}^M{\sum_{c=1}^C{y_{o,j}^c}\log{p_{o,j}^c}+(1-y_{o,j}^c)\log{(1-p_{o,j}^c)}}
% \end{equation}


% \subsection{Basic Single Modality Models \wdai{Do you think its possible to integrate this entire section with above? I think a subsection is not needed for this, this only describes the basic models used)}} 
% \label{sec:model}
% To cope with the multi-label retinal disease classification problem, we consider a set of training data $\{(x_i^o, x_i^f), {y}_i\}_{i=1}^N$ which comprises both fundus and OCT imaging and test data $\{x_i^o\}_{i=1}^M$ with only OCT modality available. \wdai{I data paired or unpaired?}

% Aiming at utilizing additional knowledge from fundus modality during training process, we first build a teacher model denoted $F_t$ trained by fundus data $\{x_i^f, {y}_i\}_{i=1}^N$ in the training set. 

% \wdai{(All this detailed description is not needed.) - (A 2D-CNN is applied to compute the feature map for a fundus image, and then a global average pooling (GAP) layer compresses the map into a feature vector $v_{f,i}$ which represents valuable information of the input. Sequentially, a fully connected (FC) layer acts as a multi-label classifier to output the decision score with sigmoid activation to acquire ${p}_{f,i}=\{p_{f,i}^c\}_{c=1}^C$. 
% % The procedure above can be expressed as:
% % \begin{align}
% %     v_{f,i}&={\rm 2D\text{-}CNN_f}(x_{f,i}) \\
% %     {p}_{f,i}&={\rm Sigmoid}({\rm FC_f}(v_{f,i}))
% % \end{align}
% % where $x_{f,i}$ denotes the $i^{th}$ fundus instance in training data, $v_{f,i}$ represents the feature vector of the input image, and ${p}_{f,i}=\{p_{f,i}^c\}_{c=1}^C$ is the final prediction vector which indicates the probability of each class.
% )}

% \wdai{We train the model using }
% multi-label classification loss is formulated as:
% \begin{equation}
%     \mathcal{L}_{fundus}=-\sum_{i=1}^N{\sum_{c=1}^C{y_{f,i}^c}\log{p_{f,i}^c}+(1-y_{f,i}^c)\log{(1-p_{f,i}^c)}}
% \end{equation}

% Similarly, an OCT model is built to learn from OCT images $\{x_i^o, {y}_i\}_{i=1}^N$ where the same backbone as the fundus model is employed.
% % \begin{align}
% %     v_{o,i}&={\rm CNN_o}(x_{o,i}) \\
% %     {p}_{o,i}&={\rm Sigmoid}({\rm FC_o}(v_{o,i}))
% % \end{align}
% And the corresponding classification loss is designed with binary cross entropy loss:
% \begin{equation}
% \label{eq:loss-function}
%     \mathcal{L}_{CLS}=-\sum_{i=1}^N{\sum_{c=1}^C{y_{o,i}^c}\log{p_{o,i}^c}+(1-y_{o,i}^c)\log{(1-p_{o,i}^c)}}
% \end{equation}

% In addition to exploiting features from OCT modality, we expect the OCT network to attend to knowledge from another modality and use such information to enhance itself. As a consequence, a distillation loss is introduced aiming at distill the learned knowledge from fundus teacher model to OCT student model. We define the total loss for OCT model optimization as: 

% where $\mathcal{L}_{cls}$ is the classification loss computed by binary cross entropy, $\mathcal{L}_{distill}$ indicates the distillation loss which is described in Section \ref{sec:distill}, and $\lambda$ is the trade-off hyper-parameter.

% In the inference time, only OCT data is available to be fed into the OCT model to compute the probabilities ${p}=\{p^c\}_{c=1}^C$ for each disease $c$. 
% \wdai{)}
% \wdai{(I know what you mean, but not needed, will confuse people The threshold is set to 0.5 to determine whether the input image belongs to a certain disease. )}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{imgs/method-finalv2.pdf}
	\caption{Overview of our proposed FDDM. Our method is based on class prototype matching, which distills disease-specific features, and class similarity alignment, which distills inter-class relationships.
        % \wdaicmnt{Provide a very brief summary}
        }
	\label{fig:method}
\end{figure*}


% \wdai{(move to overview section) (
% Unlike previous fusion strategies, we introduce knowledge distillation to extract disease-related features from an inferior fundus images that could still help enhance the prediction of OCT models. Instead of adopting classical feature-based \cite{romero2014fitnets} or logit-based \cite{hinton2015distilling} distillation, we propose disease-aware enhancement distillation to transfer information in a class-wise manner, allowing to train with unpaired data.
% Our approach is based on two ideas, class prototype matching and class similarity alignment. Details are discussed in the sections below.)}

% Given that single OCT models achieve far more accurate diagnosis compared with its fundus counterpart, simple fusion strategies, such as early fusion, intermediate fusion and late fusion, cannot effectively share valuable information between both modalities, especially in complicated multi-label scenarios. Therefore, our purpose is to extract disease-related features from an inferior fundus images that could still help enhance the prediction of OCT models.

% Following the knowledge distillation strategy, we distill information from fundus to OCT network with the purpose of improving OCT classification accuracy. 
% We firstly follow the classical knowledge distillation approaches proposed in \cite{hinton2015distilling} and \cite{romero2014fitnets} to distill logit-based or feature-based knowledge with paired fundus and OCT images from the same eye of a patient. However, the prediction of fundus model is generally less accurate than OCT model, especially on samples with few pathological features in fundus imaging. 
% % For example, in the early stage of dryAMD, the formation of drusens begins from a tiny size which is hardly visible in fundus photographs while Retinal Pigment Epithelium (RPE) deformation or thickening that appears with drusens can be clearly observed in OCT images. 
% In such cases, the soft targets from fundus model may lack reliability to function as supplementary for OCT prediction. 
% Besides, as two distinct modalities for retinal imaging, fundus photos and OCT scans provides different views of a retina, inducing pixel-wise misalignment between feature maps. Therefore, directly distilling feature-based information from fundus feature maps might be ineffective as well.
% Hence, we propose class prototype matching as well as class similarity alignment to distill knowledge in the general class level.

% \wdai{Need to simplify. Too complicated at the moment. Also try to add in sentences like: "Unlike existing works, XXXXXXX" to highlight your difference. }


\subsection{Class Prototype Matching}
\label{sec:distill}
% \wdaicmnt{Will be better to structure it to explain why you do it, not just what you did} 

To distill features from the teacher model into the student model, we aim to ensure that features belonging to the same class are similar. However, we note that individual sample features can be noisy since they contain variations specific to the sample instance instead of the class. In order to reduce noise and ensure disease-specific features are learnt, 
% In order to avoid introducing noise from a single instance, 
we compress features of each class into a class prototype vector to represent the general characteristics of the disease. During the training per batch, the class prototype vector is the average of all the feature vectors belonging to each category, which is formulated as:
\begin{equation}
    \boldsymbol{e}^c_f=\frac{\sum_{i=1}^B{v_{f,i}*y_{f,i}^c}}{\sum_{i=1}^B{y_{f,i}^c}}, \ \boldsymbol{e}^c_o=\frac{\sum_{j=1}^B{{\mathbf{P}}\left(v_{o,j}\right)*y_{o,j}^c}}{\sum_{j=1}^B{y_{o,j}^c}} \:,
\end{equation}
where $\boldsymbol{e}^c_f$ and $\boldsymbol{e}^c_o$ denote the prototype vector for class $c$ of the fundus and OCT modality respectively, $v_{f,i}$ $\left( v_{o,j}\right)$ represents the feature vector of the input image, and $y_{f,i}^c$ $\left(y_{o,j}^c\right)$ is a binary number which indicates whether the instance belongs to class $c$ or not. $\mathbf{P}$ demotes an MLP projector that projects OCT features into the same space as fundus features.

In the class prototype matching stage, 
% we utilize class prototype vectors as the objective for distillation to transfer class-wise knowledge from fundus to OCT model and to reduce the effect from biased fundus instances. 
we apply softmax loss to the prototype vectors of fundus modality to formulate soft targets $\mathcal{E}_f^c=\sigma(\boldsymbol{e}_f^c/\tau)$, where $\tau$ is the temperature scale that controls the strength to soften the distribution. 
Student class prototypes $\mathcal{E}_o^c$ are obtained in the same way. KL divergence is then used to encourage OCT student to learn matched class prototypes with fundus teacher:
\begin{equation}
\mathcal{L}_{CPM}=\sum_{c=1}^C{\mathcal{E}_f^c\log{\left(\frac{\mathcal{E}_f^c}{\mathcal{E}_o^c}\right)} \:,}
\end{equation}
% \wdai{finish to suddenly, add something to summarize. Can repeat why it is novel}
By doing so, the OCT model is able to use the global information from fundus modality for additional supervision. Overall, our approach adopts class prototypes from fundus modality instead of noisy features from individual samples, which provides more specific knowledge for OCT student model.

\subsection{Class Similarity Alignment}
% Class prototype matching encourages the OCT student to mimic the global disease features of fundus teacher in a shared feature space.
We also note that for multi-label classification tasks, relationships among different classes also contain important information, especially since label co-occurrence is common for eye diseases. Based on this observation, we additionally propose a class similarity alignment scheme to distill knowledge concerning inter-class relationships from fundus model to OCT model.

First, we estimate the disease distribution by averaging the obtained logits of fundus and OCT model in a class-wise manner to get $\boldsymbol{q}_f=\{q_f^c\}_{c=1}^C, \boldsymbol{q}_o=\{q_o^c\}_{c=1}^C$.
% \wdaicmnt{the what?? do you mean distribution?} 
Then, to transfer information on inter-class relationships, we enforce cosine similarity matrices of the averaged logits to be consistent between teacher and student model. The similarity matrix for teacher model is calculated as $\mathcal{Q}_f^c=\sigma(sim(q_f^c, \boldsymbol{q}_f)/\tau)$ and is obtained similarly for student model, $\mathcal{Q}_o^c$. KL divergence loss 
is used to encourage alignment between the two similarity matrices:
\begin{equation}   \mathcal{L}_{CSA}=\sum_{c=1}^C{\mathcal{Q}_f^c\log{\left(\frac{\mathcal{Q}_f^c}{\mathcal{Q}_o^c}\right)} \:,}
\end{equation}

% \wdai{finish to suddenly, add something to summarize. Can repeat why it is novel}

In this way, disease distribution knowledge is distilled from fundus teacher model, forcing OCT student model to learn additional knowledge concerning inter-class relationships, which is highly important in multi-label scenarios.

\subsection{Overall framework}
The overall loss is the combination of classification loss and distillation enhancement loss:
\begin{equation}
    \mathcal{L}_{OCT}=\mathcal{L}_{CLS} + \alpha\mathcal{L}_{CPM} + \beta\mathcal{L}_{CSA} \:,
\end{equation}
where $\alpha$ and $\beta$ are loss weights that control the contribution of each distillation loss. 
Admittedly, knowledge distillation strategies in computer vision \cite{romero2014fitnets,hinton2015distilling,park2019relational,shu2021channel,zhao2022decoupled,chen2022knowledge} can be applied to share multi-modal information as well. 
% Unlike previous strategies that adopt classical feature-based \cite{romero2014fitnets} or logit-based \cite{hinton2015distilling} distillation,
Unlike classical distillation methods, our two novel distillation losses allow knowledge about \textit{disease-specific features} and \textit{inter-class relationships} to be transferred, thereby allowing knowledge distillation to be conducted with unpaired data. 

% we introduce knowledge distillation to extract disease-related features from a weaker fundus images that could still help enhance the prediction of OCT models. Instead of adopting classical feature-based \cite{romero2014fitnets} or logit-based \cite{hinton2015distilling} distillation, we propose disease-aware enhancement distillation to transfer information in a class-wise manner, allowing to train with unpaired data. 


\section{Experiments}
\label{sec:experiments}
\subsection{Experimental Setup}
\subsubsection{Dataset.}

% collection
To evaluate the effectiveness of our approach, we collect a new dataset TOPCON-MM with paired fundus and OCT images from 369 eyes of 203 patients in Guangdong Provincial Hospital of Integrated Traditional Chinese and Western Medicine using a Topcon Triton swept-source OCT featuring multimodal fundus imaging. 
For fundus images, they are acquired at a resolution of 2576$\times$1934. For OCT scans, the resolution ranges from 320 $\times$ 992 to 1024 $\times$ 992. 
Specifically, multiple fundus and OCT images are obtained for each eye, and each image may reveal multiple diseases with consistent labels specific to that eye. All cases were examined by two ophthalmologists independently to determine the diagnosis label.
If the diagnosis from two ophthalmologists disagreed with each other, a third senior ophthalmologist with more than 15 years of experience was consulted to determine the final diagnosis.
As shown in Table~\ref{tab:data-statistics}, there are eleven classes, including  normal, dry age-related macular degeneration (dAMD), wet age-related macular degeneration (wAMD), diabetic retinopathy (DR), central serous chorioretinopathy (CSC), pigment epithelial detachment (PED), macular epiretinal membrane (MEM), fluid (FLD), exudation (EXU), choroid neovascularization (CNV) and retinal vascular occlusion (RVO). 


% Several ophthalmologists are hired to select one OCT image for each scan. 
% For abnormal cases, only OCT images with lesions are used. 

% Please note that each eye is associated with several pairs of fundus and OCT images and is assigned multiple labels since it can be infected with multiple diseases. 

% We collect multiple fundus and OCT images for each eye. 

% Please note that each eye is correlated with numerous pairs of fundus and OCT images, and is allocated multiple labels due to the possibility of being infected with multiple diseases.

% they can be divided into two modes, 3D scan and radial scan. In 3D scan mode, 256, 320, or 512 OCT slices can be acquired in a single examination with different scanning protocols, and an OCT slice has a resolution of 320 $\times$ 992. Radial scan mode collects 12 OCT B-scans with the resolution of 1024 $\times$ 992 each time.

% annotation and selection
% \wdaicmnt{use the proper name, optometrist?} 
% \wdaicmnt{images without lesions are not used? What about normal classes?}

% Since an eye can be infected with multiple diseases, each image is annotated with multiple labels. 

\begin{table}[t]
    \caption{Statistics of our collected TOPCON-MM dataset.}
    \label{tab:data-statistics}
    \centering
    \begin{adjustbox}{width=0.76\textwidth}
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{Category} & Normal & dAMD & wAMD & DR & CSC & PED & MEM & FLD & EXU & CNV & RVO & Total \\ 
        \hline
        \textbf{Eyes} & 153 & 52 & 30 & 72 & 15 & 23 & 38 & 93 & 90 & 14 & 10 & 369 \\
        \hline
        \textbf{Fundus Images} & 299 & 178 & 171 & 502 & 95 & 134 & 200 & 638 & 576 & 143 & 34 & 1520 \\
        \hline
        \textbf{OCT Images} & 278 & 160 & 145 & 502 & 95 & 133 & 196 & 613 & 573 & 138 & 34 & 1435 \\
        \hline
        \end{tabular}
    \end{adjustbox}
\end{table}

\subsubsection{Implementation Details.}
% preprocess

% \xmli{Following prior work [xx], we use xxx as data preprocessing and xxx in data augmentation.}
Following prior work~\cite{wang2019two,wang2022learning}, we use contrast-limited adaptive histogram equalization for fundus images and median filter for OCT images as data preprocessing. We adopt data augmentation including random crop, flip, rotation, and changes in contrast, saturation, and brightness. 
% For data preprocessing, we adopt Contrast Limited Adaptive Histogram Equalization (CLAHE) to improve the quality of fundus photos. OCT images are passed through a 3$\times$3 median filter to reduce background noise. 
All the images are resized to 448$\times$448 before feeding into the network.
For a fair comparison, we apply identical data processing steps, data augmentation operations, model backbones and running epochs in all the experiments.
% augmentation
% We apply standard data augmentation methods for each image, such as crop, flip, and rotation.
% implement: device, optimizer, hyper-parameters(lr, weight decay, batch size, epoch)
We use SGD to optimize parameters with a learning rate of 1e-3, a momentum of 0.9, and a weight decay of 1e-4. The batch size is set to 8. For weight parameters, $\tau$ is set to 4, $\alpha$ is set to 2 and $\beta$ is set to 1. All the models are implemented on an NVIDIA RTX 3090 GPU. We split the dataset into training and test subsets according to the patient's identity and maintained a training-to-test set ratio of approximately 8:2. To ensure the robustness of the model, the result was reported by five-fold cross-validation.
% \xmli{We split the dataset into xxx subsets according to the patient's identity. To ensure the robustness of the model, the result was reported by xx -fold cross-validation.}
% For a fair comparison, we apply ResNet50 as the backbone in all the experiments. 
% \xmli{not clear if all the data augmentation, preprocessing and backbone are consistent for all experiments?}

% We split the dataset into training/test sets based on patient identities, so images from the same patient do not appear in both. Five different random splits are used to ensure robustness in our results and average performance across splits is computed. \xmli{delete.}
% The mean value of the last 5 epochs is applied as the final result.
%We evaluate using Sensitivity, Specificity, F1 score, AUC and MAP, the common metrics for multi-label classification. Average scores for all the classes are reported. 

\subsubsection{Evaluation Metrics.} 
% Metrics are reported by averaging the results for each eye. The result for each eye is determined by the condition that: if the 
% at the level of the eye, which means that for each eye, we take the highest score of each class as the final prediction. Therefore, if any one case is abnormal, the eye is predicted to have the disease. 
% \xmli{please revise again.}
We follow previous work \cite{li2021multi} to evaluate image-level performance. As each eye in our dataset was scanned multiple times, we use the ensemble results from all the images of the same eye to determine the final prediction. More specifically, if any image indicates an abnormality, the eye is predicted to have the disease.

%Metrics are computed on eye level, such that for every eye, we use the maximum score of each class as the eventual prediction. Thus, an eye is predicted with the disease as long as one case is abnormal.



%\wdaicmnt{why maximum and not mean?} 

% Just don't mention this additional detail I think, will lead to more confusion. 
% \wdai{We split the dataset into 5 copies (What does this mean?) \lhwang{(Tail class only contains 34 images from 10 eyes, which might be imbalanced to conduct five-fold validation. So I run the model for 5 times on different split of training/test set.)} and apply the averaged score of the last 5 epochs as the final result (why do you do this? may need to explain).\lhwang{(I did not form a validation set for model selection due to the lack of data in tail classes, so I report the average score on test set of last 5 epochs.)}}



% \wdai{We evaluate using  Sensitivity, Specificity, F1 score, AUC and MAP, the common metrics for evaluating multi-label classification. Average score of all classes are reported. Metrics are computed in eye level which means XXXXX. }

% \subsubsection{Evaluation Metrics\wdai{delete section}}
% In the multi-label classification setting, Sensitivity, Specificity, F1 score, AUC and MAP are common metrics used to evaluate model performance. 
% %In our imbalanced multi-label setting, MAP and F1 are comparatively more credible. So, 
% We adopt MAP to assess the overall quality of prediction and others as secondary metrics. 
% % For class $c$, we define $TP_c, FP_c, TN_c, FN_c$ as the number of true positive, false positive, true negative, and false negative samples, so the definitions of our evaluation metrics are as follows:
% % \begin{align}
% % Sensitivity_c&=\frac{TP_c}{TP_c+FN_c}\\
% % Specificity_c&=\frac{TN_c}{TN_c+FP_c}\\
% % F1_c&=\frac{2*Sensitivity_c*Specificity_c}{Sensitivity_c+Specificity_c}
% % \end{align}
% We report the average scores of all the classes to reflect the final results. Note that our evaluation metrics are computed in an eye level, which means that for every eye, we obtain the ensemble result of all the samples and apply it as the eventual prediction.
\subsection{Compare with State-of-the-Arts}
% For single-modal approaches, a ResNet50 is established to extract high-level features for fundus and OCT images respectively. For multi-modal approaches, we implement Late Fusion, Two-Stream CNN \cite{wang2019two} and MSAN \cite{he2021multi}. We also compare our method with knowledge distillation algorithms in computer vision \cite{romero2014fitnets,hinton2015distilling,park2019relational,zhao2022decoupled,chen2022knowledge}.

% we reproduce feature-based \cite{romero2014fitnets}, logit-based \cite{hinton2015distilling} and relation-based \cite{park2019relational} distillation. We also compare our method with state-of-the-art distillation approaches, \eg, DKD \cite{zhao2022decoupled} and SimKD \cite{chen2022knowledge}. % \wdai{new paragraph}

\begin{table*}[t]
    \caption{Results on our collected TOPCON-MM dataset.
    % \wdaicmnt{usually try and include additional descriptions to clarify any part that may be confusing.} 
    ``Training" and ``Inference" indicate which modalities are required for both phases. ``Paired" indicates whether paired fundus-OCT images are required in training. All the experiments use ResNet50 as the backbone. For multi-modal methods, two ResNet50 without shared weights are applied separately for each modality. ``Late Fusion" refers to the direct ensemble of the results from models trained with two single modalities. $\dagger$: we implement multi-modal methods in retinal disease classification. $\star$: we run KD methods in computer vision. %\xmli{please check and confirm this Table with weihang further.}
    }
    \label{tab:main-results}
    \centering
    \begin{adjustbox}{width=0.9\textwidth}
        \begin{tabular}{l|ccc|ccccc}
        \hline
        \textbf{Method} & \textbf{Training}& \textbf{Inference} & \textbf{Paired} & \textbf{MAP} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{F1 Score} & \textbf{AUC} \\
        \hline
        \multicolumn{9}{c}{\textit{Single-Modal Methods}} \\
        \hline
        ResNet50 & Fundus & Fundus & - & 50.56\scriptsize{$\pm$3.05} & 43.68\scriptsize{$\pm$6.58} & 92.24\scriptsize{$\pm$0.77} & 54.95\scriptsize{$\pm$7.09} & 79.97\scriptsize{$\pm$1.63} \\
        ResNet50 & OCT & OCT & - & 66.44\scriptsize{$\pm$3.81} & 53.14\scriptsize{$\pm$6.60} & 95.28\scriptsize{$\pm$0.85} & 64.16\scriptsize{$\pm$6.24} & 87.73\scriptsize{$\pm$1.44} \\
        % simple ensemble \\ 
        \hline 
        \multicolumn{9}{c}{\textit{Multi-Modal Methods}} \\
        \hline
        Late Fusion & Both & Both & \ding{51} & 63.83\scriptsize{$\pm$1.34} & 54.45\scriptsize{$\pm$2.72} & 94.29\scriptsize{$\pm$0.74} & 64.93\scriptsize{$\pm$3.00} & 86.92\scriptsize{$\pm$1.48}  \\
        Two-Stream CNN$\dagger$~\cite{wang2019two} & Both & Both & \ding{51} & 58.75\scriptsize{$\pm$2.71} & 53.47\scriptsize{$\pm$3.82} & 92.97\scriptsize{$\pm$0.91} & 61.82\scriptsize{$\pm$4.02} & 84.79\scriptsize{$\pm$2.77} \\
        MSAN$\dagger$~\cite{he2021multi} & Both & Both & \ding{51} & 59.49\scriptsize{$\pm$3.43} & 56.44\scriptsize{$\pm$3.13} & 93.37\scriptsize{$\pm$0.59} & 63.95\scriptsize{$\pm$3.77} & 84.51\scriptsize{$\pm$1.91} \\ 
        % \hline
        % \multicolumn{9}{c}{\textit{Knowledge Distillation Methods in Computer Vision}} \\
        % \hline
        FitNet$\star$~\cite{romero2014fitnets} & Both & OCT & \ding{51} & 63.41\scriptsize{$\pm$3.45} & 54.44\scriptsize{$\pm$4.04} & 94.87\scriptsize{$\pm$0.58} & 65.00\scriptsize{$\pm$4.32} & 87.17\scriptsize{$\pm$1.85} \\
        KD$\star$~\cite{hinton2015distilling} & Both & OCT & \ding{51} & 63.69\scriptsize{$\pm$2.04} & 51.70\scriptsize{$\pm$3.10} & 95.75\scriptsize{$\pm$0.62} & 63.56\scriptsize{$\pm$2.32} & 87.90\scriptsize{$\pm$1.03} \\
        RKD$\star$~\cite{park2019relational} & Both & OCT & \ding{51} & 63.59\scriptsize{$\pm$3.04} & 53.42\scriptsize{$\pm$1.71} & 94.42\scriptsize{$\pm$1.81} & 63.70\scriptsize{$\pm$0.72} & 87.36\scriptsize{$\pm$2.08} \\
        DKD$\star$~\cite{zhao2022decoupled} & Both & OCT & \ding{51} & 64.40\scriptsize{$\pm$2.09} & 53.83\scriptsize{$\pm$5.23} & 95.24\scriptsize{$\pm$0.11} & 64.00\scriptsize{$\pm$4.44} & 87.52\scriptsize{$\pm$0.58} \\
        SimKD$\star$~\cite{chen2022knowledge} & Both & OCT & \ding{51} & 65.10\scriptsize{$\pm$2.63} & 53.13\scriptsize{$\pm$5.49} & 95.09\scriptsize{$\pm$0.92} &  63.19\scriptsize{$\pm$6.53} & 87.97\scriptsize{$\pm$1.32} \\
        Ours & Both & OCT & \ding{55} & \textbf{69.06}\scriptsize{$\pm$3.39} & \textbf{57.15}\scriptsize{$\pm$5.93} & \textbf{95.93}\scriptsize{$\pm$0.57} & \textbf{69.17}\scriptsize{$\pm$6.07} & \textbf{89.06}\scriptsize{$\pm$0.97} \\
        \hline
        \end{tabular}
    \end{adjustbox}
\end{table*}
To prove the effectiveness of our proposed method, we compare our approach with single-modal, multi-modal, and knowledge distillation methods. 
From Table~\ref{tab:main-results},  it is apparent that the model trained with OCT alone performs better than the fundus models.
It is noteworthy that current multi-modality methods~\cite{wang2019two,he2021multi} and knowledge distillation methods~\cite{romero2014fitnets,hinton2015distilling,park2019relational,zhao2022decoupled,chen2022knowledge} do not yield improved results on our dataset. 
% Additionally, \lhwang{comparing single-modal OCT model and knowledge distillation methods \cite{romero2014fitnets,hinton2015distilling,park2019relational,zhao2022decoupled,chen2022knowledge}, it can be seen that directly distilling eye-related knowledge between modalities may lead to performance decrease}, which is likely due to the transfer of low-confidence information from the fundus modality.
% \xmli{How to see this conclusion from Table? Please revise as follows: ;see comparison of xx and xxx in Table 1.}
Table~\ref{tab:main-results} also demonstrates that compared with the single-modal OCT baseline, our method improves MAP from 66.44\% to 69.06\%, F1 from 64.16\% to 69.17\%.
%\wdaicmnt{better to add (xx \vs xx) to help readers know which numbers} 
%\wdaicmnt{better to add (xx \vs xx)}. 
\textit{This shows that it is still possible to learn valuable information from the fundus modality to assist the OCT model, despite being a weaker modality.} It can be observed that our approach outperforms the state-of-the-art multi-modal retinal image classification method \cite{he2021multi} by 9.57\% in MAP (69.06\% \vs 59.49\%). Notably, our method excels the best-performing knowledge distillation method \cite{chen2022knowledge} by 3.96\% in MAP (69.06\% \vs 65.10\%).
% \xmli{Please revise as follows: Compared to existing multi-modal retinal image classification methods, our method can achieve xx\% improvements. Notably, our method excels the best-performing knowledge-distillation methods by xxx\% in F1. }
% \textit{This shows that it is still possible to learn valuable information from the fundus modality to assist the OCT model, despite being a weaker modality. } 
% \xmli{How to get this conclusion?}
We also note that the alternative methods are limited to training with eye-paired fundus and OCT images only, whilst our approach does not face such restrictions.

% \wdaicmnt{can you try and move the graph above the ablation table? In general should appear in order}
To further demonstrate the efficiency of our proposed distillation enhancement approach, we validate our method on a publicly available multi-modal dataset with fundus and OCT images, MMC-AMD \cite{wang2022learning}. MMC-AMD dataset contains four classes: normal, dry AMD, PCV, and wet AMD. We reproduce single-modal ResNet, Two-Stream CNN \cite{wang2019two}, and KD methods\cite{romero2014fitnets,hinton2015distilling} as baselines and show results in Fig.~\ref{fig:other-dataset} (a). It can be seen that our method improves MAP to 92.29\%, largely surpassing existing methods. 
% \xmli{please reorganize this section according to the template.}

%but multi-modal methods perform worse than single-modal OCT CNN on our dataset.
% The performance difference could be attributed to the dominance of OCT imaging in ophthalmology, which displays earlier and clearer degeneration of most diseases.
% Besides, we find that multi-modal methods perform worse than their OCT counterparts on our dataset. 
% A possible explanation for this might be that simple fusion strategies like concatenation are not effective when there is a big gap between the performances of single-modal models, so one modality is inclined to introduce noise into another.  

%due to our novel method. 
% \wdai{too long, simplify to a few key observations. Most of the time results section doesn't need to be too detailed}

\subsection{Results Trained with Other Fundus Datasets}

Since we implement distillation in a disease-aware manner, multi-modal fundus and OCT training data do not need to be paired. Theoretically, any publicly available fundus dataset could be applied as long as it shares a label space that overlaps with our OCT data. To verify this hypothesis, we separately reproduce our methods with fundus images from two datasets, MMC-AMD \cite{wang2022learning} and RFMiD \cite{pachade2021retinal}. %\wdaicmnt{is this edit correct?} 
To ensure label overlap, we only select fundus and OCT images from common classes for training and validation, namely, 3 classes for MMC-AMD and 6 classes for RFMiD.
% \wdaicmnt{(mention how many maybe)}
The results are reported in Fig.~\ref{fig:other-dataset} (b). Compared with single-modal OCT model, our distillation enhancement can achieve an increase of 4.26\% (84.06\% \vs 79.80\%) and 2.21\% (75.47\% \vs 73.26\%) in MAP. Our results proove that \textit{our method has the flexibility to use any existing fundus dataset to enhance OCT classification}.

\pgfplotstableread[row sep=\\,col sep=&]{
	% Model & Fundus CNN & OCT CNN & Two-Stream CNN & Ours  \\
	% {MAP} & 83.90 & 87.98 & 86.91 & 92.29 \\
 %        {F1} & 81.14 & 86.36 & 90.89 & 92.45 \\
 	Metric & MAP \\
	{Fundus CNN} & 83.90 \\
 {OCT CNN} & 87.98 \\
 {Two-Stream} & 86.91 \\
 {FitNet} & 90.72 \\
 {KD} & 90.29 \\
 {Ours} & 92.29 \\
}\mydata

\pgfplotsset{every axis/.append style={
		label style={font=\normalsize},
		tick label style={font=\normalsize} 
}}

\pgfplotstableread[row sep=\\,col sep=&]{
	Model & OCT CNN & Ours  \\
	{MMC-AMD $\rightarrow$ TOPCON-MM} & 79.80 & 84.06 \\
        {RFMiD $\rightarrow$ TOPCON-MM} & 73.26 & 75.47 \\
}\mydatatransfer

\pgfplotsset{every axis/.append style={
		label style={font=\normalsize},
		tick label style={font=\normalsize} 
}}

\begin{figure}[t]
% \subfigcapskip=-1pt
    \subfigure[MAP of different models on patient-split MMC-AMD dataset.]{
	\centering
	% \begin{tikzpicture}[scale=0.45]
	% 	\tikzstyle{every node}=[font=\normalsize]
	% 	\begin{axis}[
	% 		ybar=8pt,
	% 		bar width=1cm,
	% 		enlarge x limits=0.6,
	% 		width= 1.3 \textwidth,
	% 		height= 0.55 \textwidth,
	% 		legend style={at={(0.5,1)},
	% 			anchor=north,legend columns=-1},
	% 		symbolic x coords=
	% 		{{MAP}, {F1}},
	% 		xtick=data,
	% 		nodes near coords,
	% 		every node near coord/.append style={font=\normalsize},
	% 		nodes near coords align={vertical},
	% 		ymin=65,ymax=100,
	% 		]
	% 		\addplot 
	% 		[draw=brown!60!white,fill=brown!25!white] 
	% 		table[x=Model,y=Fundus CNN]{\mydata};
	% 		\addplot 
	% 		[draw=gray,fill=gray!30!white] 
	% 		table[x=Model,y=OCT CNN]{\mydata};
 %                \addplot 
	% 		[draw=cyan!40!blue!60!white,fill=cyan!60!blue!20!white] 
	% 		table[x=Model,y=Two-Stream CNN]{\mydata};
 %                \addplot 
	% 		[draw=violet!50!white,fill=violet!40!pink!20!white] 
	% 		table[x=Model,y=Ours]{\mydata};
	% 		\legend{{Fundus CNN}, {OCT CNN}, {Two-Stream CNN}, {Ours}}
	% 	\end{axis}
	% \end{tikzpicture}
        \begin{tikzpicture}[scale=0.45]
		\tikzstyle{every node}=[font=\normalsize]
		\begin{axis}[
			ybar=8pt,
			bar width=1cm,
			enlarge x limits=0.3,
			width= 1.1 \textwidth,
			height= 0.55 \textwidth,
			% legend style={at={(0.5,1)},
			% 	anchor=north,legend columns=-1},
			symbolic x coords={{Fundus CNN}, {OCT CNN}, {Two-Stream}, {FitNet}, {KD}, {Ours}},
   x tick label style = {text width = 1.5cm, align=center},
			xtick=data,
                xlabel=Model,
                xlabel style={at={(ticklabel* cs:1)},anchor=north west},
                ylabel=MAP,
                 ylabel style={at={(ticklabel* cs:1)},anchor=south west,rotate=-90},
			nodes near coords,
			every node near coord/.append style={font=\normalsize},
			nodes near coords align={vertical},
			ymin=65,ymax=97.5,
			]
			\addplot 
			[draw=cyan!40!blue!60!white,fill=cyan!60!blue!20!white] 
			table[x=Metric,y=MAP]{\mydata};
		\end{axis}
	\end{tikzpicture}
    }
    \subfigure[Transfer from fundus datasets.]{
	\centering
	\begin{tikzpicture}[scale=0.45]
		\tikzstyle{every node}=[font=\normalsize]
		\begin{axis}[
			ybar=8pt,
			bar width=1.2cm,
			enlarge x limits=0.7,
			width= 1.0 \textwidth,
			height= 0.55 \textwidth,
			legend style={at={(0.5,1)},
				anchor=north,legend columns=-1},
			symbolic x coords=
			{{MMC-AMD $\rightarrow$ TOPCON-MM}, {RFMiD $\rightarrow$ TOPCON-MM}},
      x tick label style = {text width = 4cm, align=center},
			xtick=data,
                xlabel=Transfer,
                xlabel style={at={(ticklabel* cs:1)},anchor=north west},
                ylabel=MAP,
                 ylabel style={at={(ticklabel* cs:1)},anchor=south west,rotate=-90},
			nodes near coords,
			every node near coord/.append style={font=\normalsize},
			nodes near coords align={vertical},
			ymin=50,ymax=95,
			]
			\addplot 
			[draw=gray,fill=gray!30!white] 
			table[x=Model,y=OCT CNN]{\mydatatransfer};
                \addplot 
			[draw=violet!50!white,fill=violet!40!pink!20!white] 
			table[x=Model,y=Ours]{\mydatatransfer};
			\legend{{OCT CNN}, {Ours}}
		\end{axis}
	\end{tikzpicture}
    }
	\caption{Results on other publicly available datasets.}
 \label{fig:other-dataset}
\end{figure}

\subsection{Ablation Studies}

Table~\ref{tab:ablation} shows the ablation study of our method. 
% To evaluate the effectiveness of our proposed methods, namely class prototype matching (CPM) and class similarity alignment (CSA), we conduct ablation studies on TOPCON-MM dataset, as shown in . 
To provide additional insight, we also show the results on majority classes, which contain over 10\% images of the dataset, and minority classes with less than 10\%. 
% we also show the impact on majority and minority classes, where a majority class contains over 10\% images of the dataset and a minority class less than 10\%. 
It can be seen that individually using CPM and CSA can improve the overall result by $1.32\%$ and $1.06\%$ in MAP, respectively. Removing either of the components degrades the performance. 
Results also show that CPM improves classification performance in majority classes by distilling disease-specific knowledge, while CSA benefits minority classes by attending to inter-disease relationships. By simultaneously adopting CPM and CSA, the overall score of all the classes is improved. 

% \wdaicmnt{"To investigate possible reasons". This makes it sound like there is something wrong. There is nothing though, the logic use isn't correct.} 


% \wdaicmnt{by XXXXXX?? you explained reason for minority, so better to explain here as well}

% \wdaicmnt{"boost" in general isn't a very suitable word to describe results}

% \xmli{The key focus of this section is : we show that our method is very flexible and any existing fundus photos can be used can enhance OCT classification. }

% \begin{table}[t]
%     \caption{Results on MMC-AMD dataset}
%     \label{tab:mmc-amd}
%     \centering
%     \begin{adjustbox}{width=0.45\textwidth}
%         \begin{tabular}{l|c|c|c}
%         \hline
%         \textbf{Method} & \textbf{MAP} & \textbf{F1 Score} & \textbf{AUC} \\
%         \hline
%         Fundus CNN & 83.90 & 81.14 & 93.02  \\
%         OCT CNN & 87.98 & 86.36 & 95.22 \\
%         Two-Stream CNN \cite{wang2019two} & 86.91 & 90.89 & 95.56 \\
%         Ours & \textbf{92.29} & \textbf{92.45} & \textbf{97.08} \\
%         \hline
%         \end{tabular}
%     \end{adjustbox}
% \end{table}

\begin{table}[t]
    \caption{Ablation study of our method. ``Majority" and ``Minority" refers to the average score of classes that represent more than 10\% or less than 10\% of the total number of images, respectively. ``Overall" indicates overall performance on all the classes.}
    \label{tab:ablation}
    \centering
    \begin{adjustbox}{width=0.7\textwidth}
        \begin{tabular}{cc|ccc|c|c}
        \hline
        \multicolumn{2}{c|}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{MAP}} & \multirow{2}{*}{\textbf{F1 Score}} & \multirow{2}{*}{\textbf{AUC}} \\
        \cline{3-5}
        \textbf{CPM} & \textbf{CSA} & Overall & Majority & Minority & & \\
        \hline
        \ding{55} & \ding{55} & 66.44\scriptsize{$\pm$3.81} & 71.12\scriptsize{$\pm$4.04} & 58.26\scriptsize{$\pm$7.42} & 64.16\scriptsize{$\pm$6.24} & 87.73\scriptsize{$\pm$1.44} \\
        \ding{55} & \ding{51} & 67.50\scriptsize{$\pm$3.00} & 70.60\scriptsize{$\pm$4.91} & \textbf{62.08}\scriptsize{$\pm$8.72} & 65.40\scriptsize{$\pm$5.21} & 88.09\scriptsize{$\pm$1.28} \\
        \ding{51} & \ding{55} & 67.76\scriptsize{$\pm$2.34} & 72.26\scriptsize{$\pm$3.20} & 59.90\scriptsize{$\pm$8.32} & 65.58\scriptsize{$\pm$2.58} & 88.73\scriptsize{$\pm$1.32} \\\
        \ding{51} & \ding{51} & \textbf{69.06}\scriptsize{$\pm$3.39} & \textbf{73.34}\scriptsize{$\pm$3.48} & 61.47\scriptsize{$\pm$8.17} & \textbf{69.17}\scriptsize{$\pm$6.07} & \textbf{89.06}\scriptsize{$\pm$0.97} \\
        \hline
        \end{tabular}
    \end{adjustbox}
\end{table}

% \subsubsection{Validation on Paired Dataset}
% \xmli{Move this section to 3.2. The name should be validation on xxx  public dataset. No need to highlight paired .}
% To further demonstrate the efficiency of our proposed distillation enhancement approach, we validate our method on a publicly available multi-modal dataset with paired fundus and OCT images, MMC-AMD \cite{wang2022learning}. MMC-AMD dataset contains four classes: normal, dry AMD, PCV, and wet AMD. We reproduce single-modal CNN as well as Two-Stream CNN \cite{wang2019two} as baselines and show results in Fig.~\ref{fig:other-dataset} (a). It can be seen that our method improves MAP by 4.31\% and 5.38\%, F1 by 6.09\% and 1.56\% in comparison with single-modal OCT CNN and multi-modal Two-Stream CNN respectively.

% \subsubsection{Transfer between Unpaired Dataset}

\section{Conclusion}
Our work proposes a novel fundus-enhanced disease-aware distillation module, FDDM, for retinal disease classification. The module incorporates class prototype matching to distill global disease information from the fundus teacher to the OCT student, while also utilizing class similarity alignment to ensure the consistency of disease relationships between both modalities. Our approach deviates from the existing models that rely on paired instances for multi-modal training and inference, making it possible to extract knowledge from any available fundus data and render predictions with only OCT modality. As a result, our approach significantly reduces the prerequisites for clinical applications. Our extensive experiments demonstrate that our method outperforms existing baselines by a considerable margin.

\vspace{6pt}
\noindent
\textbf{Acknowledgement.} This work is supported by grants from Foshan HKUST Projects under Grants FSUST21-HKUST10E and FSUST21-HKUST11E, as well as by the Hong Kong Innovation and Technology Fund under Projects PRP/041/22FX and ITS/030/21.

% \wdaicmnt{cut to 8 pages. If you need to make images smaller / flatter

% Check references section is corretly formated. Your style is different to mine for osme reason.

% Use grammarly or something to double check spelling 

% For formulas, add $\:,$  or $\:,$  at end, should follow punctuation (see equation 1)}

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{refs-sim}
\input{refs.bbl}
\end{document}
