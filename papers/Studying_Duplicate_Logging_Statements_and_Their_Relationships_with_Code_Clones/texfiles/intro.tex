

\IEEEPARstart{S}{oftware} logs are widely used in software systems to record system execution behaviors. Developers use the generated logs to assist in various tasks, such as debugging~\cite{Yuan:2011:ISD:1950365.1950369, Yuan:2010:SED:1736020.1736038, Fu:2014:DLE:2591062.2591175,gino_DS, ARC_log_bug_report, cyfICSE2019,hpj_log_analysis_TDSC18,hpj_ISSRE16}, testing~\cite{Chen:2017:ALT:3103112.3103144, jacktool, jackase2018,JF_ASE19}, program comprehension~\cite{Hassan:2008:ICS:1368088.1379445, Shang:2014:ULL:2705615.2706065,joy_mobile_log}, system verification~\cite{Busany:2016:BLA:2884781.2884805, DBLP:journals/jacic/BarringerGHS10}, and performance analysis~\cite{Chen:2016:CHD:2950290.2950303, kundi_icpe_2018, zs_performance_test,kundi_log_compression}. A logging statement (i.e., code that generates a log) contains a static message, to-be-recorded variables, and log verbosity level. For example, in the logging statement: {\em logger.error(``Interrupted while waiting for fencing command: '' + cmd)}, the static text message is {\em ``Interrupted while waiting for fencing command:''}, and the dynamic message is from the variable {\em cmd}, which records the command that is being executed. The logging statement is at the {\em error} level, which is the level for recording failed operations~\cite{log4j}.


Even though developers have been analyzing logs for decades~\cite{Kabinna:2016:LLM:2901739.2901769}, there exists no industrial standard on how to write logging statements~\cite{Fu:2014:DLE:2591062.2591175, 7202961}. Prior studies often focus on recommending where logging statements should be added into the code (i.e., {\em where-to-log})~\cite{Zhu:2015:LLH:2818754.2818807, wheretologASE, wheretologSRC, Zhao:2017:LFA:3132747.3132778}, and what information should be added in logging statements (i.e., {\em what-to-log})~\cite{Shang:2014:ULL:2705615.2706065, Yuan:2011:ISD:1950365.1950369, aseLog2018,loglevel_ICSE21}. A few recent studies~\cite{log_pattern_ICSE2017, mehran_emse_2018} aim to detect potential problems in logging statements. However, these studies often only consider the appropriateness of one single logging statement; while logs are typically analyzed in sequences or clusters~\cite{Yuan:2011:ISD:1950365.1950369, Chen:2016:CHD:2950290.2950303}. In other words, we consider that the appropriateness of a log is also influenced by other logs that are generated in system execution.

In particular, an intuitive case of such influence is duplicate logs, i.e., multiple logs that have the same text message. Even though each log itself may be impeccable, duplicate logs, in some occasions, may affect developers' understanding of the dynamic view of the system. For example, as shown in Figure~\ref{fig:M_example_intro}, there are two logging statements in two different {\em catch} blocks, which are associated with the same {\em try} block. These two logging statements have the same static text message and do not include any other error-diagnostic information. Thus, developers cannot easily distinguish what is the occurred exception when analyzing the produced logs.
%we find that developers copied and pasted a log statement from {\em removeGroup} to {\em updateGroup} (i.e., causes duplicate logs), but the copied log statement in {\em updateGroup} still contains the static text {\em ``remove group''}, which should be updated in the new context. %Although the two methods have syntactic similarities, the semantic are different (i.e., update v.s. remove).
Since developers rely on logs for debugging and program comprehension~\cite{Shang:2014:ULL:2705615.2706065}, such duplicate logging statements may negatively affect developers' activities in maintenance and quality assurance. %On the other hand, developers may also intentionally inject such duplicate logs to accomplish tasks that are not easily supported by the current logging libraries.

 \begin{figure}
 \centering
 %\begin{lstlisting}
%public boolean updateGroup(final CloudianGroup group) {
 %   ...
  %  } catch (final IOException e) {
   %     LOG.error("Failed to remove group due to:", e);
    %    checkResponseTimeOut(e);
    %}
    %...
%}
 %\end{lstlisting}
%\includegraphics[width=0.9\linewidth]{figures/M_example_intro.pdf}

\begin{lstlisting}
...
} catch (AlreadyClosedException closedException) {
       s_logger.warn("Connection to AMQP service is lost.");
} catch (ConnectException connectException) {
       s_logger.warn("Connection to AMQP service is lost.");
}
...
\end{lstlisting}
\vspace{-0.3cm}
 \caption{An example of duplicate logging code smell that we detected in CloudStack. The duplicate logging statements in the two {\em catch} blocks contain insufficient information (e.g., no exception type or stack trace) to distinguish the occurred exception.}
 \vspace{-0.3cm}
 \label{fig:M_example_intro}
 \end{figure}



 \begin{figure}
 \centering
\includegraphics[width=\columnwidth]{figures/duplog_overall.pdf}
\vspace{-0.6cm}
 \caption{The overall process of our study.}
 \vspace{-0.6cm}
 \label{fig:overall}
 \end{figure}

%In order to understand the phenomenon of log duplication in depth,
%\ian{inconsistent between present and past tense, stick to one.} \zhenhao{converted to past tense}
To help developers improve logging practices, in this paper, we focus on studying duplicate logging statements in the source code. We conducted a manual study on five large-scale open source systems, namely Hadoop, CloudStack, ElasticSearch, Cassandra and Flink. We first used static analysis to identify all duplicate logging statements, which are defined as two or more logging statements that have the same static text message. We then manually study all the (over 4K) identified duplicate logging statements and uncovered five patterns of {\em duplicate logging code smells}. We follow prior code smell studies~\cite{budgen2003software, fowler1999refactoring}, and consider duplicate logging code smell as a ``surface indication that usually corresponds to a deeper problem in the system''. Thus, {\em not} all of the duplicate logging code smell are problematic and require fixes (i.e., {\em problematic duplicate logging code smells}). In particular, context (e.g., surrounding code and usage scenario of logging) may play an important role in identifying fixing opportunities. We further categorized duplicate logging code smells into potentially {\em problematic} or {\em justifiable} cases. In addition to our manual analysis, we sought confirmation from developers on the manual analysis result. For some of the potentially problematic duplicate logging code smells, developers considered them as technical debt and were reluctant to fix. For the rest of the potentially problematic instances, developers agreed that they are problematic and fixed them. For the justifiable ones, we communicated with developers for discussion (e.g., emails or posts on developers' forums).

We implemented a static analysis tool, \tool, to automatically detect {\em problematic} duplicate logging code smells. \toolS leverages the findings from our manual study, including the uncovered patterns of duplicate logging code smells and the categorization on problematic and justifiable cases.
We evaluated \toolS on eight systems: five are from the manual study and three are additional systems. We also applied \toolS on the updated versions of the five manually studied systems. The evaluation shows that the uncovered patterns of the duplicate logging code smells also exist in the additional systems, and duplicate logging code smells may be introduced over time. An automated approach such as \toolS can help developers avoid duplicate logging code smells as systems evolve.
In total, we reported 91 instances of duplicate logging code smell to developers and all the reported instances are fixed. %Our fix suggestions are all accepted by developers.
%\footnote{\label{link}The link to share our archived data is omitted due to the double blind review policy.}
Figure~\ref{fig:overall} shows the overall process of finding and detecting duplicate logging code smells.

%Moreover, during the process of our manual study, we found that duplicate logging statements might be a consequence \zhenhao{or side effect?} of code clones.
%To provide a more comprehensive understanding of duplicate logging code smells, we further investigate the relationship between the problematic instances of duplicate logging code smells and code clones. 
We further investigate the relationship between the problematic instances of duplicate logging code smells and code clones. Intuitively, duplicate logging statements could be related to, or are even a consequence of code clones (e.g., logging statements can be copied along with other code since cloning is often performed hastily without much attention on the context~\cite{5463343}). 
The findings of our study may show other negative effect of code clones on logging statements and inspire future code clone and logging research. 
%The findings of our study may show other negative effect of code clones on non-functional code (i.e., logging statements) and may inspire future code clone and logging research. 
We combine both an automated code clone detection tool (i.e., NiCad~\cite{nicad}) and manual study on the eight studied systems to examine if the duplicate logging code smell instances reside in cloned code snippets. We find that 83\% of the problematic duplicate logging code smell instances reside in cloned code snippets; however, 17\% of the instances reside in very short code blocks that are difficult to detect using automated code detection tools. %We match the locations of the code clones with the problematic instances of duplicate logging code smells. %To mitigate possible false positives in the clone detection tool, we then manually study the 


%To mitigate possible false negatives in the clone detection tool, we then manually study duplicate logging statements, which reside in code snippets that are classified as {\em non-cloned} by NiCad5. We find that almost half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in very short code blocks (i.e., micro-clones) which may not be effectively detected by existing code clone detection tools.


%\zhenhao{Or change the above sentence to ``during the process of our manual study, we found that duplicate logging statements might be a consequence of code clones.''}

%Code clones may negatively affect the maintenance of software systems, and introduce subtle program errors~\cite{refactoring1999, contextCloneBugs, tracyhallcodesmell}. Moreover, cloned logging statements may fail to accurately record the runtime behaviors and increase debugging difficulties. Hence, we investigate the relationship between duplicate logging statements and code clones by both running a clone detection tool and conducting a manual study. We run NiCad5~\cite{nicad} on the eight studied systems to detect cloned code snippets. We match the locations of the clones with duplicate logging statements. To mitigate possible false negatives in the clone detection tool, we then manually study duplicate logging statements, which reside in code snippets that are classified as {\em non-cloned} by NiCad5. We find that almost half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in very short code blocks (i.e., micro-clones) which may not be effectively detected by existing code clone detection tools.


In summary, this paper makes the following contributions: %that we receive into a static code checker that we implement. %Once the anti-patterns are derived, we then manually study the genealogy
%~\jinqiu{The last bullet can be the third one? The findings first, then the tool and result.}
\vspace{-0.1cm}
\begin{itemize} \itemsep 0em
  \item We uncovered five patterns of duplicate logging code smells through an extensive manual study on over 4K duplicate logging statements.%This is the first study on helping developers refactor logging code.



  %\item We find that although 60\% of duplicate logs are related to duplicate code, 40\% of them are not.
  \item We presented a categorization of duplicate logging code smells (i.e., problematic or justifiable), based on both our manual analysis %(i.e., studying the logging statement and its surrounding code) 
  and developers' feedback.

%  \item We manually assessed every logging code smell instance, whether it is problematic or justifiable, based our understanding (i.e., studying the logging statement and its surrounding code) and developers' feedback. %We further categorize each pattern of duplicate logging code smell into sub-categories based on its potential impact.

  \item We proposed \tool, a static analysis tool that integrates our manual study result and developers' feedback to detect problematic duplicate logging code smells. %We evaluated \toolS for both the accuracy and generalization (i.e., on new systems and on the newer versions as systems evolve).


  \item We reported 91 instances of problematic duplicate logging code smells to developers (\toolS is able to detect 81 of them), and all of them are fixed.

  %\item \peter{revisit this point}We found that almost half of the duplicate logging statements reside in cloned code snippets, and many of such clones reside in very short code blocks. %current clone detection tools might not be effective in detecting duplicate logging code smells.


  \item We found that most of the problematic instances of duplicate logging code smells (83.0\%) reside in cloned code snippets, which indicates that code clones may also result in bad logging practices that increase maintenance difficulties.

  \item We found that more than half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in short code blocks (i.e., micro-clones) which are difficult to detect using existing code clone detection tools.

  %\item We find that considering duplicate logging statements help clone detection tools improve detection accuracy by 25\% to 47\%. 

  \item We found that duplicate logging statements have a non-negligible impact on helping the detection of code clones. After removing them, from 25.0\% to 47.1\% of the cloned code snippets with duplicate logging statements can not be detected as cloned code snippets. 

  \item We provided a replication package of our paper for future studies on logging code and code clones\footnote{We share the data of this paper at: \url{https://github.com/SPEAR-SE/Duplicate_Logs_Data}}.


  %\item Based on our manual analysis,\zhenhao{Keep this finding or disgard?} we provided a detailed discussion regarding the possibility of considering duplicate logging statements as an indicator of short cloned code snippets, and the potential of using duplicate logging statements to further improve clone detection tools.


\end{itemize}

Our study provides an initial step on creating a logging guideline for developers to improve the quality of logging code. \toolS is also able to detect duplicate logging code smells with high precision and recall. Future code clone studies should also consider other possible side effects of code clones (e.g., understanding system runtime behaviour), and may consider including information from other software artifacts (e.g., duplicate logging statements) to further improve clone detection results.


This work extends our previous work~\cite{DLFinder}. First, we add one more system to our manual study and extend our evaluation to include an additional system and compare our text-analysis-based algorithm on detecting inconsistently updated log messages with two baselines. We also add discussions on duplicate logging statements that do not belong to one of the uncovered smells.
Second, we study the relationship between duplicate logging statements, including the problematic instances of duplicate logging code smells, and code clones 
Finally, we investigate the potential impact between duplicate logging statements and code clones.

%\jinqiu{Based on RQ4 and RQ5, we can conclude that clone detection tools are not effective in detecting problematic duplicate logging statements. Might worth mentioning somehwere to highlight the necessity of our tool (however the ICSE paper is published already, seems no need to justify this??)}
\phead{Paper organization.} 
Section~\ref{sec:prestudy} describes data preparation %how we prepare the data for manual study (i.e., duplicate logging statements) 
and the studied systems. Section~\ref{sec:manual} discusses the process and the results of our manual study. %, and also developers' feedback on our results. 
Section~\ref{sec:detection} discusses the implementation details of \tool. Section~\ref{sec:results} presents the case study results. 
Section~\ref{sec:clone} investigates the relationship between problematic instances of duplicate logging code smells and code clones. Section~\ref{sec:rq5} investigates the relationship between duplicate logging statements and code clones, as well as the potential impact of duplicate logging statements on detecting code clones. Section~\ref{sec:threats} discusses the threats to validity of our study. Section~\ref{sec:related} surveys related work. Section~\ref{sec:conclusion} concludes the paper. %Appendix~\ref{sec:appendix1} analyzes how many duplicate logging statements (i.e., regardless whether or not they belong to one of the uncovered duplicate logging code smells) reside in cloned code snippets. 
Appendix~\ref{sec:appendix2} discusses the false positive rate of the automated clone detection tool. 
%Appendix~\ref{sec:appendix3} investigates the potential impact of duplicate logging statements on detecting code clones.


