\section{Threats to Validity}
\label{sec:threats}
\vspace{-0.1cm}
%\phead{Construct Validity.} \peter{do we still need this? I think we can remove it}A potentially more intuitive approach for detecting duplicate logging statements may rely on code clone detection tools such as CCFinder~\cite{kamiya2002} and CCLearner~\cite{8094426}. However, it is not clear if duplicate logging statements are indeed associated with code clones. Moreover, the performance of clone detection tools is dependent on the thresholds~\cite{roy09, 7081830, 5070528, Lin:2014:DDA:2568225.2568298}. Choosing the optimal thresholds is a non-trivial task and the value may differ across systems~\cite{nikos_icse_2018}. To understand whether the duplicate logging statements reside in code clones, the first two authors of the paper independently examine the sampled duplicate logs from Section~\ref{sec:manual} and their surrounding code. We consider the surrounding code of two duplicate logs is clone if we observe syntactic similarities. We find that, among all the manually studied duplicate logs, 60\% are related to code clone (vary between 59\% to 73\% for the studied systems), whereas 40\% are not related to code clone. The results from the two authors have a Cohen's kappa above 0.9 for all studied systems (i.e., a high-level of agreement~\cite{kappa}). Our finding shows that, there is still a significant number of duplicate logs that are not related to code clones.


%, but clone detection techniques typically require setting a certain thresholds~\cite{roy09}. Non-optimal thresholds have significant impacts on the detected clones~\cite{roy09, 7081830}. However, choosing the optimal thresholds is a non-trivial task and the value may differ across systems~\cite{nikos_icse_2018}. In order to understand whether the duplicate logs reside in code clones, the first two authors of the paper independently examine the sampled duplicate logs from Section~\ref{sec:manual} and their surrounding code. We consider the surrounding code of two duplicate logs is clone if we observe syntactic similarities. We find that, among all the manually studied duplicate logs, 60\% are related to code clone (vary between 59\% to 73\% for the studied systems), whereas 40\% are not related to code clone. The results from the two authors have a Cohen's kappa is above 0.9 for all studied systems (i.e., a high-level of agreement~\cite{kappa}). Our finding shows that, there is still a significant number of duplicate logs that are not related to code clones.

%A potentially more intuitive approach for detecting duplicate may be based on code clone detection tools such as CCFinder~\cite{kamiya2002}, but clone detection techniques typically require setting a certain thresholds~\cite{roy09}. Non-optimal thresholds have significant impacts on the detected clones~\cite{roy09, 7081830}. However, choosing the optimal thresholds is a non-trivial task and the value may differ across systems~\cite{nikos_icse_2018}. In order to understand whether the duplicate logs reside in code clones, the first two authors of the paper independently examine the sampled duplicate logs from Section~\ref{sec:manual} and their surrounding code. We consider the surrounding code of two duplicate logs is clone if we observe syntactic similarities. We find that, among all the manually studied duplicate logs, 60\% are related to code clone (vary between 59\% to 73\% for the studied systems), whereas 40\% are not related to code clone. The results from the two authors have a Cohen's kappa is above 0.9 for all studied systems (i.e., a high-level of agreement~\cite{kappa}). Our finding shows that, there is still a significant number of duplicate logs that are not related to code clones.


\phead{Construct validity.}
In this paper, we study duplicate logging statements from a static point of view. There may be other types of unclear log messages that are dynamically generated during system runtime. Using such dynamic information can also be helpful in identifying unclear log messages. However, the generated log messages are highly dependent on the executed workloads (i.e., hard to achieve a high recall). \toolS statically identifies and improves duplicate logging statements, is useful as it does not require any run-time information. Future studies may consider studying runtime-generated logs and further improve logging practices. 
We detect duplicate logging code smells by analyzing the surrounding code of logging statements as their context. Apart from that, the sequence of generated logs may also provide context information (e.g., the relationship among preceding logs and subsequent logs). However, for most of the duplicate logging code smells discussed in this paper, they are not directly related to the log sequences (e.g., the patterns of IC and IE are related to the logging statements and their surrounding catch blocks). Even though analyzing the generated log sequences may provide more information, the duplicate logging code smells can still cause challenges and increase maintenance costs, as acknowledged by the developers in the studied systems. Future study may consider the execution path of logging statements as the context information to further improve logging practice.




\phead{Internal validity.}
We conducted manual studies to uncover the patterns of duplicate logging code smells, study their potential impact and examine duplicate logging statements that are not classified by the automated clone detection tool as clones. Involving external logging experts may uncover more patterns of logging statements or have different manual study results. To mitigate the biases, two of the authors examine the data independently. For most of the cases the two authors reach an agreement. Any disagreement is discussed until a consensus is reached. In order to reduce the subjective bias from the authors, we have contacted the developers to confirm the uncovered patterns and their impact.
When detecting LM instances, using different approaches to split the text into words may have different results. We follow common text pre-processing techniques to split the text by space and camel case~\cite{Chen:2016:SUT:2992358.2992444}.
We define duplicate logging statements as two or more logging statements that have the same static text message. We were able to uncover five patterns of duplicate logging code smells and detect many duplicate logging code smell instances. However, logging statements with non-identical but similar static texts may also cause problems to developers (e.g., when analyzing dynamically generated logs). Future studies should consider different types of duplicate logging statements (e.g., logs with similar text messages). 
We remove the top 50 most frequent words when detecting LM, because there is a considerable number of generic words across different log messages. However, this might also introduce false negatives. Future studies may consider applying more advanced techniques to better detect the instances of LM. 
There is a considerable number of code clone detection tools proposed by prior studies~\cite{kamiya2002,cpminer,DCCFINDER,duplix,nicad,gabel}. We use NiCad~\cite{nicad} to detect code clone, as it has high precision (95\%), recall (96\%) and outperforms the state-of-the-art code clone detection tools~\cite{nicad,NicadEvaluation,ROY2009470} when detecting near-miss clones, and is actively maintained (latest release was in July 2020). We also manually examine the precision of Nicad in Appendix~\ref{sec:appendix2}, where we find its precision to be 96.8\% in our manual verification, which is consistent with the results from prior studies~\cite{ROY2009470,NicadEvaluation}. 


%<<<<<<< HEAD
%=======
%\peter{moved to here}We rely on NiCad5 for automated clone detection. To examine the false positives of NiCad5, we manually verify a randomly sampled set of duplicate logging statements (281 sets in total, with 95\% confidence level and 5\% confidence interval) that are classified as clones by NiCad5. For each set of the sampled duplicate logging statements, we manually go through the logging statements and their surrounding code to verify whether they are clones or not. Overall, we find that 272 out of the 281 sampled sets (96.8\%) are clones, which is similar to the performance of NiCad5 that is reported in prior studies. For the 9 false positives, 3 of them are duplicate logging statements located in different branches of a nested method (i.e., developers define a method within a method). In such cases, NiCad5 would analyze the code block twice. For example, in ElasticSearch~\footnote{\url{https://github.com/elastic/elasticsearch/blob/70b8d7bc64f165735502de9d8c5fa673fa21e02b/server/src/main/java/org/elasticsearch/cluster/InternalClusterInfoService.java}}, two duplicate logging statements with the same static text message {\em ``Failed to execute NodeStatsAction for ClusterInfoUpdateJob''} are located in different branches of the same nested method {\em onFailure(Exception e)}. However, since the method {\em onFailure(Exception e)} is defined in the method {\em refresh()}, NiCad5 would analyze the same code block twice and detect them as clones. For the remaining 6 out of 9 false positives, we could not identify the reasons that they are classified as clones, since the code snippets look neither structurally nor semantically similar.
%>>>>>>> a0eae0d901899d8cccb5f2144634e775d17bcf50


 %We narrow down our analysis on five manually-uncovered patterns. To reduce our sampling bias, we randomly sample 289 sets of duplicate logs based on 95\% confidence level and 5\% confidence interval~\cite{boslaugh2008statistics}.

%We uncover The detection instances are classied by manual works, individual differences of the people who conduct the work might affect the result. In our study, all the related instances are independently examined by the first two authors of this paper. For most of the cases the two authors reach an agreement, any disagreement is discussed until a consensus is reached. Future works may involve a larger size of people to show further reliability of the manual work.
%We do not examine all the duplicate log sets for the manual uncovering of duplicate logging code smells. Since it will be an extremely challenging work to check all the logging statements with their surrounding code. Hence, we randomly sample 289 sets of duplicate logs based on 95\% confidence level and 5\% confidence interval \cite{boslaugh2008statistics}, which is feasible and generally reliable.% might apply fuzzy match to possibly uncover more potential problems under logging practices.
%We only focus on studying the semantic logging information because prior studies~\cite{Yuan:2012:CLP:2337223.2337236,Shang:2014:ULL:2705615.2706065} have shown that log message is crucial for log understanding.

%However logs with slight differences may still cause confusion to operators. Future works might apply fuzzy match to possibly uncover more potential problems under logging practices.


%We only do exact match for log message when we are identifying duplicate logging statements, and we are able to get a large set of results. However logs with slight differences may still cause confusion to operators. Future works might apply fuzzy match to possibly uncover more potential problems under logging practices.


\phead{External validity.}
We conducted our study on five large-scale open source systems in different domains. We found that our uncovered patterns and the corresponding problematic and justifiable cases are common among the studied systems. However, our finding may not be generalizable to other systems. Hence, we studied whether the uncovered patterns exist in three other systems. We found that the patterns of duplicate logging code smells also exist in these systems and we did not find any new duplicate logging code smell patterns in our manual verification. Our studied systems are all implemented in Java, so the results may not be generalizable to systems in other programming languages. Future studies should validate the generalizability of our findings in systems in other programming languages.

%We only conducted our study on four studied systems, but our findings are widespread and generalizable in these four systems. We choose the studied systems with different sizes, across various domains in order to improve the generalizability. However, conducting the study on other more projects would further present the generalizability of our approach. We conduct the manual study and implement the duplicate logging code smells detecting tool, namely DLFinder, based on Java projects. Some of our findings may not able to be straightforwardly applied on projects based on other programming languages. However our approach of tool-assisted manual study is generic and generalizable, it can be used to uncover the duplicate logging code smells for other programming languages.

\vspace{-0.1cm}
