%\section{RQ4: What are the Relationships between Duplicate Logging Statements and Code Clone?}

\section{RQ4: What are the Relationships between Problematic Duplicate Logging Code Smells and Code Clones?}

\label{sec:clone}


\phead{Motivation. }
Code clone or duplicate code is considered a bad programming practice and an indication of deeper maintenance problems~\cite{refactoring1999}. Prior studies often focus on studying clones in source code and understanding their potential impact. However, there may also be other negative side effects that are related to code clones. For example, logging statements can also be copied along with other code since cloning is often performed hastily without much attention on the context~\cite{5463343}. In the previous RQs, we focus on studying problematic and technical debt instances of duplicate logging code smells (i.e., IC, IE, LM, and DP). In this section, we further investigate the potential causes of these instances by examining their relationship with code clones (we refer both the problematic and technical debt instances as problematic instances in this section for simplification). Our findings may provide researchers and practitioners with insights of other possible effects of code clones, other ways to further improve logging practices, and inspire future code clone studies.

%During our manual analysis in Section~\ref{sec:manual}, we noticed that some duplicate logging statements or duplicate logging code smells may be related to code clones. Code clone or duplicate code is considered a bad programming practice and an indication of deeper maintenance problems~\cite{refactoring1999}.
%Prior studies focus on the detection of code clones by applying text-based approaches~\cite{cpminer, kamiya2002} or abstract syntax tree analysis~\cite{baxter1998, jian2007} on program source code. However, these studies did not study code clones from the perspective of logging code. Logging statements might also be copied along with other code since cloning is often performed hastily without much attention on the context~\cite{5463343}. Some cloned logging statements (e.g., the LM logging code smells in Section~\ref{sec:manual}) may fail to correctly record the runtime behaviors; and thus, increase maintenance difficulties.

%Hence, given the initial observation from our manual study, in this section, we investigate the relationship between duplicate logging statements and code clones. Our finding may provide researchers and practitioners with insights of other possible effects of code clones (e.g., cloned logging statements), how to further improve logging practices, and may inspire future code clone studies. %We conduct our study by answering two research questions.


%Duplicate logging statements may reside in cloned code snippets because code sixth similar syntactics (i.e., cloned code) can be expressed by identical semantic information in log messages (i.e., duplicate logs). However, it is also possible that cloned code, although with similar syntactical structures, may occur in different contexts, therefore is expressed using different semantic information (i.e., different logging statements).

%As shown in previous sections, we observe that some duplicate logging code smells may be related to code clones. Therefore, in this section, we further study the relationship between duplicate logging statements and code clones. Our goal is to empirically study such relationship, provide insights to researchers and practitioners on other possible effects of code clones, and how to further improve logging practice. We conduct our study by answering two research questions. For each RQ, we describe our motivation, approach, result, and followed by a more detailed discussion.

%\jinqiu{Do we miss the rationale behind studying clones and duplicate logging statement? I think the messages are scattered in diff. places.``Code clones are expected to have structural similarities. Duplicate logging statements indicate similar semantic information about program execution. In this or these RQ/RQs, we investigate the relationship between duplicate logging statements and whether the surrounding code snippets are clones. It is possible that duplicate logging statements reside in cloned code snippets because code with similar syntactics could be represented by the same semantic information (i.e., duplicate logs). Meanwhile, it is also possible that clonded code, despite the similar structure, may appear in different contexts, thus should be expressed by different semantic information. Therefore, the fact that duplicate logging statements appearing in cloned code snippets may indicate maintenance issues. Thus, investigating the relationship between syntatic similarity and one indication of semantic similarity (i.e., through logs) can motivate future research in combining these two sources of information or identifying inconsistencies between these two types of similarity due to maintenance challenges."}



%code clone survey: http://research.cs.queensu.ca/home/cordy/Papers/RC_ICPC08_ScenarioEval.pdf


%explore the potentiality of which the study of code clone could help with the problem of duplicate logging statements and further improve the logging practice. %We first illustrate the approach of how we conduct the study, then present the results with discussion.


%Code clone or duplicate code is a bad smell and thus one of the indicators of poor maintainability~\cite{refactoring1999}. If a piece of code is buggy, then the cloned code could also replicate the bug silently. Logging statements might also be copied together with other code since cloning is often performed hastily without much care about the context~\cite{5463343}. This could result in the logging statements fail to correctly logging the runtime behaviors. In this section, we study the relationship between duplicate logging statements and code clone, in order to explore the potentiality of which the study of code clone could help with the problem of duplicate logging statements and further improve the logging practice. We first illustrate the approach of how we conduct the study, then present the results with discussion.


%\phead{RQ4: What is the relationship between duplicate logging statements and code clones?}

%\noindent{\bf Motivation.} In this RQ, we want to further investigate how many duplicate logging statements are indeed related to code clones. The result of this RQ may provide an initial evidence on the prevalence of log-related clones. Our findings may also highlight the issues in maintaining code clones from the perspective of logging statements, and help researchers and developers avoid such problems in the future.

%\zhenhao{This paragraph looks more like the motivation of RQ5} \peter{need to polish: Our results may provide an initial evidence on the prevalence of log-related clones and help researchers and developers avoid such problems in the future.}\peter{TODO: revisit the motivations after the results are done}

%\jinqiu{The result may highlight the issues in maitaining code clones from the aspect of logging statements. Future research in code clone study may consider logging statements to provide developers with better support.}

%Code clone or duplicate code is a bad smell and thus one of the indicators of poor maintainability~\cite{refactoring1999}. If a piece of code is buggy, then the cloned code could also replicate the bug silently. Logging statements might also be copied together with other code since cloning is often performed hastily without much care about the context~\cite{5463343}. This could result in the logging statements fail to correctly logging the runtime behaviors. In this research question, we would like to investigate the relationship between duplicate logging statements and cloned code snippets. We expect to provide an inspiration of how ... \zhenhao{selling point}
%{\em et al$.$}
\noindent{\bf Our approach of mapping code clones to problematic instances of duplicate logging code smells.}
%We use both an automated and a manual approach to study the relationship between code clones and instances of problematic duplicate logging code smells (i.e., DP and fixed instances of IC, IE, and LM). 
Due to the large number of duplicate logging statements in the studied systems (Appendix~\ref{sec:appendix2} also studies the relationship between general duplicate logging statements and code clone), we first leverage automated clone detection tools to study whether these instances (i.e., DP and fixed instances of IC, IE, and LM) reside in cloned code. In particular, we use NiCad~\cite{nicad} as our clone detection tool. NiCad uses hybrid language-sensitive text comparison to detect clones. We choose NiCad because, as found in prior studies~\cite{nicad,NicadEvaluation}, it has high precision (95\%) and recall (96\%) when detecting near-miss clones (i.e., code clones that are very similar but not exactly the same) and is actively maintained (latest release was in July 2020). Note that, we find NiCad's precision to be 96.8\% in our manual verification, which is consistent with the results from prior studies (more details in Appendix~\ref{sec:appendix2}). Note that, we find NiCad's precision to be 96.8\% in our manual verification, which is consistent with the results from prior studies (more details in Appendix~\ref{sec:appendix2}). 

In NiCad, the source code units of comparison are determined by partitioning the source code into different granularities. %a set of disjoint fragments (e.g., methods or code blocks).
%These source code units are the largest source code fragments (e.g., basic blocks) that are considered in the clone detection~\cite{nicad}. %may be clone targets involved in direct clone relations with each other~\cite{nicad}.
The structural granularity of the source code units could be set as the method-level or block-level (e.g., the blocks of {\em catch}, {\em if}, {\em for}, or {\em method}, etc). In our study, we set the level of granularity to block-level and use the default configuration (i.e., similarity threshold is 70\% and the minimum lines of a comparable code block is 10),
%\jinqiu{Does the paper say the default configurations would yield better results? If so, add that here. Intuitively, 10 sounds too big a number for this configuration though. The example of IL in Table 2 does not have more than 10 lines}
which is suggested by prior studies indicating this configuration could achieve remarkably better results in terms of precision and recall~\cite{EvaluatingModernCloneDetectionTools, ROY2009470, nicad}. Block-level provides finer-grained information, since logging statements are usually contained in code blocks for debugging or error diagnostic purposes~\cite{Fu:2014:DLE:2591062.2591175}. Note that if the block is nested, the inner block is listed twice: once inside its parent block and once on its own. Hence, all blocks with lines of code above the default threshold will be compared for detecting clones. %\peter{I moved the numbers above. We can define what they meant by threshold above as well}For the thresholds and settings of detection, we keep them in default (the minimum lines of a comparable block is 10, the similarity threshold is 70\% \zhenhao{might briefly explain how do they define similarity.})\zhenhao{These are the threshold they used, but didn't describe the reasons in their paper}.
%\peter{avoid passive voice}The study is conducted by answering two research questions. For the identification of cloned code, We use NiCad5 to conduct the detection of code clone. NiCad5 is a flexible hybrid language-sensitive text comparison clone detection tool originally based on the previous work of Roy. {\em et al$.$}~\cite{nicad} with continuously maintainence and improvement afterwards. Up to the submission of this work, the latest version of NiCad5 was released on Oct. 05, 2018. The source unit of comparison are determined by partitioning the source into a set of disjoint fragments. These units are the largest source fragments that may be involved in direct clone relations with each other~\cite{nicad}. The structural granularity of source units could be set as method-level or block-level (e.g., the blocks of {\em catch, if, for, method, etc}). In our study, we set the level of granularity as block-level, a more fine-grained level which might provide us with more accurate results. Note that if the block is nested, the inner block is listed twice: once inside its parent block and once on its own. \peter{Hence, all qualified blocks will be compared by the tool.}%So that all the qualified blocks (with more potential cloned lines than the threshold) could be compared.
We run NiCad on the eight studied open source systems that are mentioned in Section~\ref{sec:results}. We then analyze the clone detection results and match the location of the clones with that of problematic instances. If two or more cloned code snippets contain the same set of instances, we consider the instances are related to the clone. 

To reduce the effect of false negatives, we also manually study the code of {\em all} the remaining instances that are not identified as clones by NiCad. We manually classify the clones into the three following categories: %If the missed clones reside in code blocks that have fewer than 10 lines of code, we manually classify them as micro-clones. 

    %\item Clones (C): if at least two of the logging statements in the set meet the following requirements: 1) The surrounding code block that the logging statement locates in has 5 or more lines of code. \zhenhao{the reason is, it's hard to say if short blocks are clones or just by chance.} 2) At least 5 lines of code surrounding the logging statement are exactly the same, or nearly the same, only with the differences of identifier names.
{\em Clones}: The code around the logging statements is more than 10 lines of code (same as the threshold of the clone detection tool). The code is exactly the same, or only with differences in identifier names (i.e., Type 1 and Type 2 clones~\cite{CompareCloneDetectionTools2007}) but not detected by the clone detection tool.

{\em Micro-clones}: The code around the logging statements is very similar but is less than the minimum size of regular code clones~\cite{microclones}. Prior studies show that micro-clones are also important for consistent updates and they are more difficult to detect due to their small size~\cite{microclone4,microclone5,microclones}. However, the effect of micro-clones on code maintenance and quality is similar to regular code clones~\cite{MicroclonesAndBugsICPC, MicroclonesAndBugsSANER}. Micro-clones should not be ignored when making decisions of clone management.
     %1) The code block where the logging code statements are located is short (i.e., has fewer than ten lines of code\peter{might need to remove the rest?}, below the threshold of the tool). 2) The code in the block is exactly the same, or only with differences in identifier names. For this kind of situations, though we feel they are likely to be clones, their sizes are not long enough for us to make a decision. Therefore, we label them as "Undecidable".
     %Prior studies indicate that micro-clones have similar tendencies of replicating severe bugs as regular clones~\cite{MicroclonesAndBugsICPC, MicroclonesAndBugsSANER}.

{\em Non-clones}: We classify other situations as non-clones.
%\end{itemize}


%\zhenhao{CSD:4, CS:119, ES:5, HD:7, CM:113, FLK:25, KFK:6, WKT:2, Total: 281}
%\zhenhao{272/281, 9 FPs}








%\phead{Discussion: Studying the relationship between problematic instances of duplicate logging code smells and code clones.} 
%\peter{we can merge table 8 and 9 together to avoid confusion. We can say directly that we use tool assisted manual study}We further analyze our clone detection results to investigate how many {\em problematic instances} of duplicate logging code smells are related to code clones. We use the automated clone detection results obtained in this RQ and match them with problematic instances of logging code smells. To reduce the effect of false negatives, we manually study the code of {\em all} the remaining instances that are not identified as clones. 

%\noindent{\bf Motivation.}
%In RQ4 we conduct an investigation of the relationship between duplicate logging statements and code clones. However, As discussed in the previous sections, not all of the duplicate logging code smells are problematic and require fixes. While non-problematic duplicate logging statements may be an indication of maintenance problem, problematic instances of duplicate logging code smells should be fixed in order to correctly record the execution behaviors. Thus, in this RQ, we would like to specifically investigate how many problematic instances of duplicate logging code smells are related to code clones and the potential impact of code clones on logging practice.

%\noindent{\bf Approach.}
%We use the clone detection results obtained in RQ4 and match the results with the instances of problematic duplicate logging code smells that are studied in Section~\ref{sec:manual} and \ref{sec:results}. We then manually study {\em all} the instances of problematic duplicate logging code smells residing in code snippets that are {\em not} classified by NiCad5 as clones.



\begin{table*}
  \caption{The results of code clone analysis on problematic instances and code clones. } 
    \vspace{-0.3cm}
    \centering
    \resizebox{\textwidth}{!} {
    \scalebox{0.8}{
    \tabcolsep=2pt
    \renewcommand\arraystretch{1.1}
    \begin{tabular}{c|rrrr|rrrr|rrrr|rrrr}

        \hline
    & \multicolumn{4}{c|}{\textbf{IC}} & \multicolumn{4}{c|}{\textbf{IE}} & \multicolumn{4}{c|}{\textbf{LM}} & \multicolumn{4}{c}{\textbf{DP}}\\
    & Clone (A) & Clone (M) &Micro.& Clone/Total & Clone (A) & Clone (M)  & Micro.& Clone/Total & Clone (A) &Clone (M) & Micro.& Clone/Total  & Clone (A)  &Clone (M)   &Micro. & Clone/Total\\
\midrule

        \textbf{Cassandra}     & 0& 0 & 0  & 0/1     & 0  &0 & 0 & 0/0       &0 & 0&  0  &0/0     & 2&  0  &0 &2/2\\
        \textbf{CloudStack}     & 5  &0 & 3 & 8/8    &4 & 0& 0 & 4/4       &  20 & 1  &5  & 26/27     & 60&  12  &9&81/107\\
        \textbf{Elasticsearch}  & 0   &0 & 0 & 0/1    &0   &0 & 0  & 0/0      &0  & 0 &1 &1/1    & 0&  1  &0 & 1/3\\
        \textbf{Flink}          & 0  &0 & 0 & 0/0     & 1 & 0  & 1 & 2/2     & 2 & 0 &2 &4/4     & 19&  0 &3 &22/24\\
        \textbf{Hadoop}         & 1  &0 & 2 & 3/5    &0   &0& 0 & 0/0        & 0& 3  &3 & 6/9     & 5&  14  &6 & 25/27\\

       \textbf{Camel}          & 0  &0 & 1 & 1/1    & 0  & 0 & 0 & 0/0       & 6 & 2 &6&14/14     & 22&  2  &4 & 28/29\\
       \textbf{Kafka}          & 0   &0 & 0  & 0/0    & 0  &0 & 0  & 0/0       & 1 & 0  &1 &2/3     & 3&  3  &2 & 8/14\\
       \textbf{Wicket}         & 0  &0 & 0  & 0/1   & 0  &0 & 0  & 0/0       & 0 & 1  &0 &1/1     & 1&  0  &0 & 1/1\\

        \hline

        \textbf{Total}         &6  & 0 & 6  & 12/17   &5 & 0 &1&6/6      &29 & 7 &18 &54/59     &112 & 32 &24 &168/207\\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \hline
    \end{tabular}
    }
    }
     \noindent {\sf Clone (A)}: number of problematic duplicate logging code smell instances that are detected as clones by NiCad, {\sf Clone (M)}: number of problematic duplicate logging code smell instances that are identified as clones by manual study, {\sf Micro.}: number of problematic duplicate logging code smell instances that are identified as Micro clones by manual study.

    \vspace{-0.6cm}

    \label{table:RQ5}
\end{table*}





\noindent{\bf Result of code clone analysis on problematic instances.}
{\em We find that 240 out of 289 (83\%) of the problematic instances of duplicate logging code smells reside in cloned code snippets.}
Table~\ref{table:RQ5} presents the results of our  code clone analysis. {\sf Clone (A)} refers to the number of problematic instances that are detected by NiCad as in code clones. {\sf Clone (M)} refers to the number of problematic instances that are manually found as in code clones. {\sf Micro.} refers to the number of problematic instances that are manually found as in Micro clones (i.e., less than 10 lines of code). In general, our findings show that these problematic instances are potentially caused by code clones. {\bf In other words, in addition to the finding from prior code clone studies, which indicates that code clones may introduce subtle program errors~\cite{contextCloneBugs, tracyhallcodesmell}, we find that code clones may also result in bad logging practices that could increase maintenance difficulties.} Future studies should further investigate the negative effect of code clones on the quality of logging statements and provide a comprehensive logging guideline.%of duplicate logging code smells are closely related to code clones



{\em We find that 64.2\% (88/137) of the problematic instances of duplicate logging code smells that are labeled as Non-clones by the automated code clone detection tool are actually from cloned code snippets. Among them, more than half (55.7\%, 49/88) reside in micro clones, which often do not get enough attention in the process of code clone management.}
As mentioned in the approach section of this RQ, to overcome potential false negatives, we manually study {\em all} the 137 problematic instances that are labeled as Non-clones by NiCad. %Table~\ref{table:RQ5} presents the results of our manual study for each pattern. {\sf Clone (M)} refers to the number of problematic instances that are identified our manual study as in code clones. {\sf Micro. } refers to the number of problematic instances that are identified as in Micro clones by our manual study. 
We classify each instance that we study into three categories: 

{\em \underline{Category 1:} Code clones reside in part of a large code block.} Since the structural granularity level of the source code units is block-level (i.e., the minimal comparable source code unit of the tool is a block), the similarity of the code is computed by comparing blocks. However, developers may copy a small part of the code into a large code block. In such cases, the similarity would be low between two different large code blocks which only have a few lines of cloned code.  %For example, developers may copy the code (along with the logging code statements) that is related to establishing network connect from one file to another. However, the copied code is just part of another complex method; hence, the clone detection tool fails to detect the code snippets as clones since the minimum detectable source code unit is block.

{\em \underline{Category 2:} Code clones reside in code with very similar semantics but have minor differences.} The surrounding code of duplicate logging statements share highly similar semantics (i.e., implement a similar functionality), but have minor differences (e.g., additions, deletions, or partial modification on existing lines). Such scattered modifications might reduce the similarity between the code structures, and thus, result in miss detection~\cite{ROY2009470,nicad}. For example, there is a code block in {\tt\small \textbf{FTP}Consumer} of Camel which does a series of operations based on the file transfer protocol (FTP). %The code in the block may be reuseable for other similar ptopocols. Thus,
Due to the similarity between FTP and secured file transfer protocol (SFTP), Camel developers copied the code block and made modifications (e.g., change class and method names) to the all the places where SFTP is needed (e.g., {\tt\small \textbf{SFTP}Consumer}). Therefore, clone detection tools may fail to detect this kind of cloned code blocks as due to minor yet scattered changes.

{\em \underline{Category 3:} Short methods/blocks.}
The logging statements reside in very short methods or code blocks with only a few lines of code. For example, there is a method in CloudStack named {\em verifyServicesCombination()} containing only six lines of code and duplicately locates in three different classes. The method verifies the connectivity of services, and generates a warning-level log if it fails the verification. Clone detection tool fails to detect this category of cases due to their small size compared to regular methods.

\noindent{\bf {\em IC \& IE: }} {\em 30\% (7/23) of the IC and IE instances in cloned code are related to micro-clones.} Since both of IC and IE reside in {\em catch} blocks, which usually contain only a few lines of code, we discuss these two duplicate logging code smells together. As shown in Table~\ref{table:RQ5}, , 7 (6 IC + 1 IE) out of 23 (17 IC + 6 IE) instances are labeled as {\em Micro-clones}, and 11 instances are identified as clones by the clone detection tool. The remaining five instances are labeled as {\em Non-clones}, since they are single logging statement thrown with multiple types of exceptions (e.g., {\sf catch (Exception1 \(|\)  Exception2 e)}). We find that all of the seven {\em Micro-clones} instances belong to Category 1 (i.e., short code snippets within a large code block). The reason might be that these logging statements all reside in {\em catch} blocks, which are usually very short. Thus, although the code in these short code blocks are identical or highly similar, they are not long enough to be considered as comparable code blocks by the clone detection tool. %Their parent blocks may have sufficient length, but the cloned code only occupies a small portion of their parent blocks. Thus, the tool fails to detect them due to the insignificant similarity of their parent blocks.
%\peter{not sure if we still want to keep this}Compared to clone detection tools, since \toolS analyzes the code in the {\em catch} blocks using static analysis, \toolS is able to detect both IC and IE with very high precision and recall.

%\noindent{\bf {\em IC \& IE: }} Since both of IC and IE reside in {\em catch} blocks, which usually contain only a few lines of code, we discuss these two duplicate logging code smells together. As shown in Table~\ref{table:manualrq5}, none of the IC and IE instances are labeled as Clones, while 7 (6 IC + 1 IE) out of 12 (11 IC + 1 IE) instances are labeled as Micro-clones. The remaining 5 instances are single logging statement that might be thrown with multiple types of exceptions (e.g., {\sf catch (Exception1 \(|\)  Exception2 e)}). We find that all of the seven Micro-clone instances belong to Category 1 (i.e., short code snippets within a large code block, discussed in RQ4). The reason might be that these logging statements all reside in {\em catch} blocks, which are usually very short. Thus, though the code in these short code blocks are identical or highly similar, it is difficult to determine whether they are code clones or due to the fact that code can be repetitive, especially in smaller code blocks~\cite{naturalness}. Compared to clone detection tools, since \toolS analyzes the code in the {\em catch} blocks using static analysis, \toolS is able to detect both IC and IE with very high precision and recall.

% \jinqiu{This sounds an inheritated problem with micro clones? I don't think it is ``hard to identify'', as some tools are aimed at micro clones and claim good results. Can we say micro clone is an on-going research effort and partially remain subjective, and are less likely caused by maintenance debt? We can find some insights from that SANER 2019 paper.}
%\zhenhao{Don't know how to connect with Micro-clones, since they define micro-clones as clones that no more than five lines of code, however our threshold is 10.}\peter{just add the citation somewhere, we don't need to discuss in detail} \zhenhao{Discussed about it in related works.}

\noindent{\bf {\em LM: }}{\em 25/54 (46\%) of the LM instances in cloned code cannot be detected by automated clone detection tools. 92\% (54/59) of LM are related code clones.} As shown in Table~\ref{table:RQ5}, 36 out of 59 instances are labeled as {\em Clones} (29 instances by tool + 7 instances by manual study), 18 out of 59 instances are labeled as {\em Micro-clones}, and the remaining 5 instances are labeled as {\em Non-clones}. For the seven instances that are identified as {\em Clones} by manual study, they all belong to Category 2 (i.e., they share highly similar semantics, but have minor differences). The reason might be that developers copy and paste a piece of code along with the logging statement to another location, and apply some modifications to the code. However, developers forgot to change the log message. Similarly, for the five instances that are labeled as {\em Non-clones}, we find that even though the code is syntactically different, the log messages do not reflect the associated method. For the 18 {\em Micro-clones} instances, 11 out of 18 instances belong to Category 3 (short methods), and the remaining 7 are Category 2 (short code snippets within a larger code block). As confirmed by the developers (in Section~\ref{sec:manual}), these LM instances are related to logging statements being copied from other places in the code without the needed modification (e.g., updating the method name in the log). 

Our manual analysis on LM instances provides insights on possible maintenance problems that are related to the modification and evolution of cloned code. Moreover, 92\% of the LM instances are related to code clones. Future studies may further investigate the inconsistencies in the source code and other software artifacts (e.g., logs or comments) that are caused by code clone evolution.


%In our manual analysis, we find that the log messages indeed do not reflect the associated method (which is also confirmed by developers, as discussed in Section~\ref{sec:manual}). However, these logging statements are inside short methods or blocks, which make them difficult to detect using clone detection techniques.


%As confirmed by the developers, these logging statements are copied from other places in the code without the needed modification (e.g., to update the method name that is recorded in the log). Our finding provides insights on possible maintenance problems that are related to the modification and evolution of cloned code. Future studies may further investigate the inconsistencies in the source code and other software artifacts that are caused by code clone evolution. % and out-of-sync %software artchitecure (e.g., comments or logging statements).


%caused by the evolution and maintenance of cloned code snippets. Future studies may further investigate the inconsistencies that are caused by

%~\jinqiu{``This inspires futhure research might pay attention to the inconsistencies caused by clone evolution and out-of-sync semantic information such as logging statements."}

%developers modified the code significantly~\jinqiu{not sure about this, how do you find that it is due to copy and paste and then the code significantly?}, but they forgot to change the log messages. Therefore, even though the code is not considered clone, we still find inconsistent log messages. ~\jinqiu{``This inspires futhure research might pay attention to the inconsistencies caused by clone evolution and out-of-sync semantic information such as logging statements."}


%\peter{maybe remove this part}Compared to clone detection techniques, \toolS detects LM by analyzing log messages and the corresponding method names. Hence, \toolS may complement clone detection tools on finding LM instances~\jinqiu{how does clone detection tool find LM instances? If by firstly finding clond snippets, isn't that super inaccurate?} that are located in short code blocks.


%Similar to our observation for Clones, the reason might be that developers forget to consistently update the log message  when other code changes. Even though the code in short methods and short blocks might be generic, they are very likely caused by copy-and-paste (since their log messages does not match with the execution behavior), it is difficult to make a decision whether they are clones or intentionally written by developers (they might write the log messages improperly).
\noindent{\bf {\em DP: }}{\em 81\% (168/207) of the DP instances are either Clones or Micro-clones, which shows that developers may often copy code along with the logging statements across sibling classes.} In total, 144 out of 207 DP instances are labeled as {\em Clones} (112 by tool + 32 by manual study), 24 are labeled as {\em Micro-clones}, and the remaining 39 instances are {\em Non-clones}. For the 32 instances that are labeled as {\em Clones} by manual study, 16 instances are Category 1 (part of a large code block), the remaining 16 instances are Category 2 (very similar semantics with minor differences). For the 24 {\em Micro-clones} instances, 11 instances belong to Category 3 (short methods), and the remaining 13 are categorized as Category 1 (short code snippets within a larger code block).
%Combined with the results from the clone detection tool, around 70\%$\sim$81\% (\peter{the sentence in the bracket is not clear}144/207~\jinqiu{Where is this 207 from?} without any instances of Undecidable, 167/207 with all the instances of Undecidable \zhenhao{(112+32)=144 Clones, with 24 Undecidable instances out of 207 total instances. If 0 Undecidable instance is considered, then 70\%. If all, then 81\%. }) of the DP instances are related to code clones.
Combined with the results from the clone detection tool, 81\% (112 detected by the tool + 32 Clones + 24 Micro-clones identified by manual study, out of 207 total instances) of the DP instances are related to code clones.
One possible reason that many DP instances are related to code clone is that DP is related to inheritance. Classes that inherit from the same parent class may share certain implementation details. Nevertheless, due to the similarity of the code, developers should consider updating the log messages to distinguish the executed methods during production to assist debugging runtime errors.

For all of the remaining problematic instances (49/289) that are not classified as clones by the automated tool and manual analysis, they mostly reside in very short code blocks (e.g., only 1$\sim$3 lines of code). Even though these code blocks may be similar or even identical, we cannot tell whether they are clones or not. It is possible that developers implemented such similar code by coincidence, or the code was copied from other places and are then modified (but forgot to modify the log-related code).

%However, as discussed in Section~\ref{sec:manual}, resolving DP often requires systematic refactoring, and the fixes of DP instances are not definitely expected as the technical debts need to be considered~\cite{Kruchten:2012:TDM:2412381.2412847}. \zhenhao{what can we learn from the view of code clones?}

%\jinqiu{I am forming one possible highlight here, not necessarily a correct argument: Our finding indicates that semantic information such as duplicate logging statements may serve as trails to distill copy-and-paste clones from general clones, which are commonly considered as bad programming practices and a source of bugs.}

%In summary, our finding indicates that the semantic information in the code, such as logging statements, may be used as trails to help


%\rqbox{Take-home box. \zhenhao{Todo: Talk about sth like LM instances usually locate in short blocks (shorter than 10 lines), so it might be hard for the existing clone detection approaches to find this kind of problems at the meantime of addressing code clone problems. More specific approaches or tools are needed to tackle it.}
%\zhenhao{Still a large portion of problematic instances reside in short methods / short blocks, current clone detection tools might not be able to detect them effectively.}\peter{@Zhenhao, see if you can come up with the takehome based on the updated discussions}
%}
%\vspace{-0.1cm}

\noindent{\bf Implication and highlights of our code clone analysis.} Our finding shows that most of problematic instances of duplicate logging code smells are indeed related to code clones, and many of which cannot be easily detected by state-of-the-art clone detection tools. Our finding shows additional maintenance challenges that may be introduced by code clones -- maintaining logging statements and understanding the runtime behaviour of system execution. %We find that duplicate logging statements may also be a possible indication of hard-to-detect clones based on our manual study. Moreover, most of the problematic instances of duplicate logging code smell reside in cloned code snippets. 
Hence, future code clone detection studies should consider other possible side effects of code clones in addition to code maintenance and refactoring overheads. Future studies may also consider integrating different information in the software artifacts (e.g., duplicate logging statements or comments) to further improve clone detection results. %when implementing clone detection tools. %that for the code snippets that are not detected as clones



\rqboxc{83\% of the problematic instances of duplicate logging code smells (240 out of 289 instances, combining the results of tool detection and manual study) are related to code clones. Our finding further shows the potential negative effect of code clones on system maintenance. Moreover, 17\% of the instances reside in short code blocks, which might be difficult to detect by using existing code clone detection tools.
}
%\vspace{1mm}
\noindent{\bf Discussion: the potential of using code clone detection tool to assist in finding problematic instances of duplicate logging code smells.} In the previous section, we found that most of the problematic instances of duplicate logging code smells (83\%) are related to code clones. Therefore, we use the results of our code clone analysis to compare with and/or assist our detection approach of LM. We focus on studying LM for two reasons. First, we found that 92\% (54/59) of the LM instances are related to code clones. Second, unlike other patterns that have a detection accuracy of 100\%, our current detection approach for LM analyzes textual similarity of the logging statement and its surrounding code, which has a lower precision and recall. Using clone detection results may further help improve our detection accuracy. 

\vspace{-0.05cm}
We first use the clone detection result as a baseline and compare the results with the detection approach implemented in \tool. If two duplicate logging statements reside in cloned code, we consider them as a possible instance of LM. Overall, the average precision and recall of using clone detection result are 3.7\% and 53.7\%, respectively, in the studied systems in RQ1. The average precision and recall in the additional systems in RQ2 are 1.5\% and 38.9\%, respectively. Compared to using clone detection result as a baseline, our approach has a better precision and recall (around 12\% in precision and 80\% in recall). However, among the 10 LM instances that cannot be detected using our approach, four of them are detected by this baseline approach. After manual investigation on these four instances, we found the log message describes a local code block while the class-method name describes the functionality of the entire method. Hence, in such cases, using clone detection results may be more effective in detecting LM. 


Inspired by the analysis result, we then study if clone detection result can assist \toolS in finding LM. We use the automated clone detection results from NiCad to filter the LM instances that are detected by \tool. Namely, \toolS only reports that a set of duplicate logging statements is a potential LM instance if they reside in cloned code. We find that, after using clone detection results to filter out potential false positives, the average precision and recall for the eight studied systems are 17.7\% and 42.4\%, respectively. Compared to \tool's detection result (Table~\ref{table:detection}), the precision increases by around 5\% but the recall decreases by around 40\%. The reason may be that many problematic LM instances reside in code clones that are difficult to detect by clone detection tool (e.g., micro clones). As shown in Table~\ref{table:RQ5}, NiCad only detects 29/54 of the LM instances that reside in cloned code. As we discussed in Section~\ref{sec:results}, we believe that recall is more important when detecting LM, since we found the manual effort of evaluating LM instances to be small (i.e. within a few minutes). Our findings also shed light on balancing the precision and recall of detecting duplicate logging code smells. Future studies may consider further improving code clone detection techniques to detect code smells that are related to logging statements. %Future code clone studies may consider improving the detection of micro clones %Nevertheless, our findings also show that, with better clone detection results, 


%developers could quickly go through the detection results to identify the true positives, we consider the recall is relatively important for the detection of LM. 


%are 23.2\% and 46.3\%, respectively, for the studied systems in RQ1. The average precision and recall are 10.2\% and 33.3\%, respectively, for the systems in RQ2. 




%\zhenhao{Therefore, we then combine our detection rule and code clone detection tool to detect the instances of LM (i.e., filter the detection results of \toolS that are not identified by the code clone detection tool as {\em Clones}.)} \zhenhao{ As for the results, the average precision and recall of this combined approach in the manually studied systems are 23.2\% and 46.3\%, respestively. The average precision and recall in the additional systems are 10.2\% and 33.3\%, respectively.  Though the precision improved in the five manually studied systems compared to the precision of \toolS(12.1\%), the recall remarkably decreased (85.4\% of \toolS compared to 46.3\% of this combined approach). As we discussed previously, developers could quickly go through the detection results to identify the true positives, we consider the recall is relatively important for the detection of LM. Hence, it still remains a complicated task for us to balance the trade-off of a reduced size of detection results and the mis-detection of many true problematic instances for merging the code clone analysis into our detection approach. In short, although most of the problematic instances of duplicate logging code smells are related to code clones, they can not be effectively detected by clone detection tools, \toolS is still relatively efficient in detecting duplicate logging code smells.  } 



%We then discuss the potential of whether code clone detection tool can substitute or complement \toolS in terms of detecting problematic instances of duplicate logging code smells. We use the results of our code clone analysis as a baseline to compare with the detection results of \tool. Similar to our evaluation in Section~\ref{sec:results}, we only compare the detection results of LM since the other patterns are well-defined and relatively independent of the surrounding code. LM depends on the semantics of the logging statement and its surrounding code, of which the detection might potentially be helped by code clone analysis. 

%We consider the code clones that contain duplicate logging statements as the total amount of detected instances. We then analyze the clone detection results and match the location of the clones with that of the ground truth instances of LM. If two or more cloned code snippets contain the logging statements of an LM instance, we consider the duplicate logging statements of this instance as a true positive. Overall, the average precision and recall of this baseline in the manually studied systems are 3.7\% and 53.7\%, respestively. The average precision and recall in the additional systems are 1.5\% and 38.9\%, respectively. In terms of the performance of detection, our approach is noticeably better than this baseline. Moreover, as we discussed in Section~\ref{sec:results}, 10 out of 59 instances of LM are not detected by \tool. Among them, 4 out of 10 instances are detected by this baseline approach. We investigate those instances and find those log messages and class method names are at different levels of abstraction: The log message describes a local code block while the class-method name describes the functionality of the entire method (similar to what we discussed in Section~\ref{sec:results}). In this kind of scenarios, the code clone analysis outperforms our detection rule, which might be an inspiration of improving our detection rule. 



%it still remains a complicated task for us to balance the trade-off of a few more true positives and much more false positives for merging the code clone analysis into our detection approach. In short, although most of the problematic instances of duplicate logging code smells are related to code clones, they can not be effectively detected by clone detection tools.
%RQ1 systems: GT: 37, TP:20, Detected: 498, for 4+4
%RQ2 systems: GT: 22, TP: 9, Detected: 544, for 4+4
%10 instaces missed by DLFinder
%\vspace{-0.1cm}






\begin{comment}

\begin{table*}
  \caption{The result of manual study in Secion~\ref{sec:clone} {\sf DupSet}: Sampled sets of duplicate logging statements, discussed in RQ4. {\sf IC, IE, LM, DP}: Instances of each pattern, discussed in RQ5.\peter{let's break the Table into 2. Otherwise, in RQ5, we need to keep look at Table 7 which is far away from the discussion}\jinqiu{Missing the category of Non-cloned}}
    \vspace{-0.3cm}
    \centering
    %\resizebox{\textwidth}{!} {
    \tabcolsep=8pt
    \scalebox{0.8}{
    \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc}

        \toprule
     & \multicolumn{3}{c|}{\textbf{DupSet}} & \multicolumn{3}{c|}{\textbf{IC}} & \multicolumn{3}{c|}{\textbf{IE}} & \multicolumn{3}{c|}{\textbf{LM}} & \multicolumn{3}{c}{\textbf{DP}} \\
     &Clones &Undec. & Total &Clones &Undec. & Total &Clones &Undec. & Total &Clones &Undec. & Total &Clones &Undec. & Total \\
\midrule

        \textbf{Cassandra}    & 1 & 3 & 7    & 0 & 0 & 1     & 0  &0  & 0     & 0 & 0 & 0     &  0  &0 &0 \\
        \textbf{CloudStack}    & 22 & 26 & 94    & 0  &3 & 3    & 0 &0 & 0    & 1  &5 & 7     & 12 &9 &47 \\
        \textbf{ElasticSearch} & 1 & 1 & 5    & 0 & 0 & 1      & 0 &0  & 0   &0  &1  & 1       & 1  &0  &3\\
        \textbf{Hadoop}      & 12 & 6 & 43    & 0  &2 & 4     & 0 &0  & 0   &3  &3  & 9       & 14  &6  &22\\

       \textbf{Camel}       & 28 & 30 & 103    & 0 &1 & 1     & 0  &0 & 0    & 2 & 6 & 8      &2 &4  &7\\
       \textbf{Flink}       & 5 & 4 & 25    & 0 &0 & 0     & 0 &1 & 1    & 0 & 2 & 2       &0  & 3 &5\\
       \textbf{Kafka}       & 3 & 7 & 18    & 0  &0 & 0    & 0 &0 &0    & 0  &1  &2     & 3 & 2 &11 \\
       \textbf{Wicket}       & 1 & 1 & 3    & 0 &0 & 1       & 0 &0 & 0     & 1 &0  &1          & 0 &0  &0 \\

        \midrule

        \textbf{Total}       & 73 &78 & 298     & 0 &6 & 11    & 0 &0  &1     & 7 & 18 &30         &32 &24 &95\\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \bottomrule
    \end{tabular}}
    %}
    \vspace{-0.3cm}

    \label{table:manualrq5}
\end{table*}

\end{comment}
