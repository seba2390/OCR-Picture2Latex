%Developers use software logging statements to record system execution.
Developers rely on software logs for a variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. %Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in sequences or clusters. %Similar to other part of the source code, there may be certain code smells that are associated with logging code. Even though there is an established line of research on code smells such as duplicate code, these studies do not help developers refactor possible code smells in logging code.
In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 4K duplicate logging statements and their surrounding code in five large-scale open source systems: Hadoop, CloudStack, Elasticsearch, Cassandra, and Flink. We uncovered five patterns of duplicate logging code smells. For each instance of the duplicate logging code smell, we further manually identify the potentially problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers to verify our manual study result. %Developers confirmed our findings in general while considered that some of the potentially problematic instances are related to technical debt. 
We integrated our manual study result and developers' feedback into our automated static analysis tool, \tool, which automatically detects problematic duplicate logging code smells. We evaluated \toolS on the five manually studied systems and three additional systems: Camel, Kafka and Wicket. In total, combining the results of \toolS and our manual analysis, we reported 91 problematic duplicate logging code smell instances to developers and all of them have been fixed. We further study the relationship between duplicate logging statements, including the problematic instances of duplicate logging code smells, and code clones. We find that 83\% of the duplicate logging code smell instances reside in cloned code, but 17\% of them reside in micro-clones that are difficult to detect using automated clone detection tools. We also find that more than half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in very short code blocks which may not be effectively detected by existing code clone detection tools. Our study shows that, in addition to general source code that implements the business logic, code clones may also result in bad logging practices that could increase maintenance difficulties. %We find that nearly half of the duplicate logging statements reside in cloned code snippets, and a large portion of them reside in very short code blocks which may not be effectively detected by existing code clone detection tools. We also find that most of the problematic instances of duplicate logging code smells reside in cloned code snippets, which indicates that code clones may lead to bad logging practices that could increase maintenance difficulties. 




%Such duplication in logs are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We conduct our study on \peter{update from here to the new flow}four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncover five patterns of duplicate logging code smells by manually studying a statistical sample of duplicate logging statements. %Similar to other code smells such as duplicate code, some duplicate logging code smell instances may not be a problem under certain contexts. Hence,
%We further manually study all the code smell instances and identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases of the uncovered patterns. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developersâ€™ feedback into our automated static analysis tool, \tool, which helps developers identify duplicate logging code smells. In total, we reported \zhenhao{X} problematic duplicate logging code smells instances to developers, and our fix suggestions are all accepted. %We implement a static analysis tool, called \tool, to detect




%we further manually classify all the code smell instances whether they are problematic (i.e., require refactoring) or justifiable (i.e., do not require refactoring).




%Similar to other code smells such as duplicate code, some duplicate logging code smells may not be a problem under certain contexts. Hence, we further conduct a tool-assisted manual study. We first implement a static analysis, called \tool, to detect the five uncovered patterns of duplicate logging code smells. Then, we manually study each detected code smell instance in detail based on the surrounding code. We identify the code contexts that are more likely to be impactful and verify our manual study result by sending inquiries to developers. We received positive feedback from developers regarding our manual study result. We reported 31 code smells instances that are more likely to cause problems in log analysis and 30 of them are now accepted and fixed by developers.

%each type of duplicate logging code smell based on the surrendering code.


%to understand and analyze system execution.
%Developers use logs for a wide variety of tasks, such as debugging, testing, program comprehension, and performance analysis.
