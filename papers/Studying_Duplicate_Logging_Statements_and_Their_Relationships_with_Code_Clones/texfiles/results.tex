\section{Case Study Results}
\label{sec:results}

\begin{table*}
    \caption{The results of \toolS in RQ1 and RQ2. \label{tab:results}}
    \vspace{-0.3cm}
    \centering
    \resizebox{\textwidth}{!} {
    \tabcolsep=8pt
    \begin{tabular}{c|lccc|ccc|ccc|ccc}

        \toprule
    Research    & & \multicolumn{3}{c|}{\textbf{IC}} & \multicolumn{3}{c|}{\textbf{IE}} & \multicolumn{3}{c|}{\textbf{LM}}  & \multicolumn{3}{c}{\textbf{DP}}\\
    questions   & & Pro. & C.Det. & Det. & Pro. & C.Det. & Det.& Pro. & C.Det. & Det. & Tech. & C.Det. & Det. \\
\midrule
\multirow{6}{*}{
\begin{tabular}[c]{@{}p{3.3cm}@{}} RQ1: How well can DLFinder detect duplicate logging code smells in the five manually studied systems?  \end{tabular}
}
        &\textbf{Cassandra}     & 1 & 1 & 1 & 0 & 0 & 0 & 0  & 0& 4  & 2&  2&  2\\
        &\textbf{CloudStack}    & 8 & 8 & 8 & 4 & 4 & 4 & 27& 24 & 186  &  107& 107& 107\\
        &\textbf{Elasticsearch} & 1 & 1 & 1 & 0 & 0 & 0 & 1  & 0 & 15  & 3 & 3& 3\\
        &\textbf{Flink}         & 0 & 0 & 0 & 2 & 2 & 2 & 4  & 4 & 41  & 24 & 24 & 24\\
        &\textbf{Hadoop}        & 5 & 5 & 5 & 0 & 0 & 0 & 9 & 7 & 44  & 27& 27&  27\\

        &\textbf{Total of RQ1}        & 15 & 15 & 15 & 6 & 6 & 6 & 41 & 35 & 290  & 163& 163&  163\\
        & \textbf{Precision / Recall}     & \multicolumn{3}{c|}{100\% / 100\%} & \multicolumn{3}{c|}{100\% / 100\%} & \multicolumn{3}{c|}{12.1\% / 85.4\%}   & \multicolumn{3}{c}{100\% / 100\%} \\

        %\midrule
        %  & \textbf{Total}         & 15 &15 &15 & 4 & 4  & 4 &37 & 31 & 249 & 0 & 0 & 15 &139 & 139 & 139\\
        \midrule
\multirow{6}{*}{
\begin{tabular}[c]{@{}p{3.3cm}@{}} RQ2: How well can \toolS detect duplicate logging code smells in the additional systems?\end{tabular}
%}& \textbf{Camel}         & 1 & 1 & 1 & 0 & 0 & 0 & 14  & 10 & 95 & N/A & N/A & 3 & 29 & 29 & 29\\
}
       & \textbf{Camel}         & 1 & 1 & 1 & 0 & 0 & 0 & 14  & 10 & 95 & 29 & 29 & 29\\
       & \textbf{Kafka}         & 0 & 0 & 0 & 0 & 0 & 0 & 3  & 3 & 15 & 14 & 14 & 14\\
       & \textbf{Wicket}         & 1 & 1 & 1 & 0 & 0 & 0 & 1  & 1 & 4  & 1 & 1 & 1\\
       &\textbf{Total of RQ2}        & 2 & 2 & 2 & 0 & 0 & 0 &18  & 14 & 114  & 44& 44&  44\\
       & \textbf{Precision / Recall}     & \multicolumn{3}{c|}{100\% / 100\%} & \multicolumn{3}{c|}{- / -} & \multicolumn{3}{c|}{12.3\% / 77.8\%}   & \multicolumn{3}{c}{100\% / 100\%} \\

        \midrule
       %& \textbf{Total}         & 2 &2 & 2 & 0 & 0  & 0 &15 & 11 & 99 & 0 & 0 & 3 &30 & 30 & 30\\
        & \textbf{Total}         & 17 &17 & 17 & 6 & 6 & 6 & 59 &49 & 404 &207 & 207 & 207\\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \bottomrule
    \end{tabular}
    }
        \noindent {\sf Pro.}: number of problematic instances as the ground-truth, {\sf Tech.}: number of technical debt instances for DP, {\sf C.Det.}: the combined number of problematic or technical debt instances {\sf correctly} detected by \tool, {\sf Det.}: number of instances detected by \tool.
    \vspace{-0.6cm}

    \label{table:detection}
\end{table*}

\vspace{-0.1cm}

In this section, we conduct a case study to investigate the prevalence of duplicate logging code smells and evaluate \toolS by answering three research questions.

\phead{RQ1: How well can \toolS detect duplicate logging code smells in the five manually studied systems?}

\noindent{\bf Motivation.}
\toolS was implemented based on the duplicate logging code smells uncovered from the manually studied systems (i.e., IC, IE, LM, and DP). Since we obtain the ground truth (i.e., all the duplicate logging code smell instances) in these five systems from our manual study, the goal of this RQ is to evaluate the detection accuracy of \tool.  

\noindent{\bf Approach.}
We applied \toolS on the same versions of the systems that we used in our manual study (Section~\ref{sec:manual}). We calculated the precision and recall of \toolS in detecting problematic instances for IC, IE, and LM, as well as the technical debt instances for DP. 
Precision is the percentage of correctly detected instances among all the detected instances, and recall is the percentage of problematic or technical debt instances that \toolS is able to detect.

\noindent{\bf Results and discussion.}
The first five rows of Table~\ref{tab:results} show the results of RQ1. For the patterns of IC, IE, and DP, \toolS detects all the problematic and technical debt instances of duplicate logging code smells (100\% in recall) with a precision of 100\%. 
%For the IL pattern, since we do not find any problematic instances (as discussed in Section~\ref{sec:manual}), both of the columns of problematic instances in ground truth ({\em Pro.}) and  correctly detected ({\em C.Det.}) in Table~\ref{tab:results} are 0. 
For the LM pattern, \toolS achieves a recall of 85.4\% (i.e., \toolS detects 35/41 problematic LM instances). We manually investigate the six instances of LM that DLFinder cannot detect. We find that the problem is related to the various habits and coding conventions that developers use when writing log messages. For example, developers may write ``mlockall'' instead of ``mLockAll'' (i.e., the camelcase naming convention), which increases the challenge of log message analysis. Hence, the text in the log message cannot be matched with the method name when we split the word using camel cases. The precision of detecting problematic LM instances is modest because, in many false positive cases, the log messages and class-method names are at different levels of abstraction: The log message describes a local code block while the class-method name describes the functionality of the entire method.
For example, {\em encodePublicKey()} and {\em encodePrivateKey()} both contain the duplicate logging statement {\em ``Unable to create KeyFactory''}. The duplicate logging statement describes a local code block that is related to the usage of the {\em KeyFactory} class, which is different from the major functionalities of the two methods (i.e., as expressed by their class-method names).
Nevertheless, \toolS detects the LM instances with a high recall, and developers could quickly go through the results to identify the true positives (it took the first two authors less than 10 minutes on average to go through the LM result of each system to identify true positives).
%Since we focus on the checking of log message and method/class name rather than the full code context. Although we have tried our best to filter the logging statements which have weak descriptive power for the actual system behavior, there are still a large amount of unexpected cases since the coding habit of different developers vary a lot.

To further evaluate our detection approach for LM, we compare our detection results with a baseline. We use random prediction algorithm as our baseline, which is commonly used as the baseline in prior studies~\cite{DBLP:conf/esem/XiaSKLW16, ELBlocker, BlockBugs}. The random prediction algorithm predicts the label of an item (i.e., whether a set of duplicate logging statements belong to LM) based on the distribution of the training data. %\peter{Not sure if we need this. This sounds too easy}For example, if 5\% of the duplicate logging statements contain LM, each set of duplicate logging statement has a 5\% chance of being classified as LM. \peter{the result of each system or the combined distribution? what about the distribution in RQ2?}
%\zhenhao{When doing random prediction, the distribution is based on each system. When presenting the results, we combine the results of systems in RQ1 and RQ2 together, respectively.}
For each system, we use our manually labeled results (which are discussed and verified in the previous sections) as the training data. Note that we only compare the detection results of LM with the baseline. The reason is that pattern IC, IE, and DP are relatively independent and well-defined, unlike LM which depends on the semantics of the logging statement and its surrounding code. We repeat the random prediction 30 times (as suggested by previous studies~\cite{30times, peter_icse}) for each system to reduce the biases. Finally, we report the average precision and recall that are computed based on the 30 times of iterations. Figure~\ref{figure:compare} shows how the precision and recall of our approach compared to that of the baseline. The average precision and recall for the baseline are 3.1\% and 3.0\%, respectively, for the five studied systems. Our detection approach achieves a precision and recall of 12.1\% and 85.4\%, respectively. In short, our approach is better than the baseline and is able to have a very high recall in the five manually studied systems.



\phead{RQ2: How well can \toolS detect duplicate logging code smells in the additional systems?}

%We applied \toolS on two additional systems.
%The goal of this RQ is to study whether the uncovered patterns of duplicate logging code smells are generalizable to other systems. We applied \toolS on four additional systems that are not included in the manual study in Section~\ref{sec:manual}: Kafka 2.1.0 (released on Nov. 20, 2018) Flink 1.7.1 (released on Dec. 21, 2018) Camel 2.21.1 (released on Apr. 28, 2018) and Wicket 8.0.0 (released on May 16, 2018), which are all large-scale open source Java systems. Similar to our manual study, the first two authors of this paper manually collect the problematic duplicate logging code smells in the additional systems, i.e., the ground-truth used for calculating the precision and recall of \tool. Note that the collected ground-truth of the additional systems is only used in this evaluation, but not in designing the patterns in \tool. (There are also no new patterns found in this process.)
\noindent{\bf Motivation.}
The goal of this RQ is to study whether the uncovered patterns of duplicate logging code smells are generalizable to other systems.

\noindent{\bf Approach.}
We applied \toolS to three additional systems that are not included in the manual study in Section~\ref{sec:manual}: Camel, Kafka, and Wicket, which are all large-scale open source Java systems. Details of the systems are presented in Table~\ref{table:systems}. Similar to our manual study, the first two authors of this paper manually collect the problematic and technical debt duplicate logging code smells in the additional systems, i.e., the ground-truth used for calculating the precision and recall of \tool. Note that the collected ground-truth of the additional systems is only used in this evaluation, but not in designing the patterns in \toolS (There are also no new patterns found in this process).

\noindent{\bf Results and discussion.}
The second half of Table~\ref{tab:results} shows the results of the additional systems. In total, we found 20 problematic duplicate logging code code smell instances (\toolS detects 16) in these systems and all of them are reported and fixed. Compared to the five systems in RQ1, \toolS has similar precision and recall values in the additional systems.
%\peter{maybe remove this discussion on IL or move/modify it to threat}\toolS detected three instances of IL in Camel; however, based on the manual investigation and developers' feedback, these IL instances are not problematic. Similar to what we discuss in Section~\ref{sec:manual}, the differences in the log level are related to having different semantics in the code. Different from a prior study~\cite{Yuan:2012:CLP:2337223.2337236}, we found that all IL instances are not problematic in the eight evaluated systems. Future studies are needed to investigate the effect of IL. 
\toolS detects DP instances with 100\% in recall and precision; however, 
developers are reluctant to fix them due to limited support from logging frameworks. 
Similar to our observation in RQ1, we find that \toolS cannot detect some LM instances due to the various habits and coding conventions when developers write log messages. We also compare our LM detection results with the baseline mentioned in RQ1 using the same approach. The average precision and recall for \toolS are 12.3\% and 77.8\%, respectively, which are considerably better than the precision (2.2\%) and recall (2.1\%) of the baseline. In summary, apart from the manually studied systems in RQ1, \toolS also achieves noticeably better precision and recall than the baseline and is able to have a reasonably high recall in the additional systems.

%we repeat the random predicion 30 times for each system of RQ2. Everytime we record the number of predictions (P) it makes and number of correct predictions (CP). For each system, we use the average of P and TP to calculate the precision and recall for Random Prediction. The average standard deviation of P in the four manually studies systems is 1.55, of CP is 0.34. As shown in Figure~\ref{figure: compare}, \toolS achieves definitely high recall (above 80\%) in the systems of RQ1 and RQ2, while the recall of baseline is observably low (below 5\%).



\begin{table}
    \caption{\label{RQ3}The results of \toolS in RQ3. 
    %Instances of duplicate logging code smells newly introduced in the newer releases of systems studied in Section\ref{sec:manual} Gap. shows the duration of time (days) between the original release (Org.) studied in Section\ref{sec:manual} and the newer release (New.)
    }
    \vspace{-0.3cm}
    \centering
    \resizebox{\columnwidth}{!} {
    \tabcolsep=8pt


    \begin{tabular}{lll|c|c|c|c}

        \toprule
        & \multicolumn{2}{c|}{\textbf{Releases}} & \multicolumn{1}{c|}{\textbf{IC}} & \multicolumn{1}{c|}{\textbf{IE}} & \multicolumn{1}{c|}{\textbf{LM}} & \multicolumn{1}{c}{\textbf{DP}}\\
        & Org., New. &Gap.  & & & & \\
        \midrule
        \textbf{Cassandra}     & 3.11.1, 3.11.3 & 294 & 0 & 0 & 0 & 1 \\
        \textbf{CloudStack}     & 4.9.3, 4.11.1 & 297 & 5 & 0 & 2 & 0 \\
        \textbf{Elasticsearch}  & 6.0.0, 6.1.3 & 77 & 0 & 0 & 0 & 0 \\
        \textbf{Flink}         & 1.7.1, 1.9.1 & 301 & 0 & 0 & 0 & 1 \\
        \textbf{Hadoop}         & 3.0.0, 3.0.3 & 208 & 0 & 0 & 2 & 21 \\

        \midrule
        \textbf{Total}         & -  & - & 5 & 0 & 4 & 23 \\
        \bottomrule
    \end{tabular}
    }

    Gap.: duration of time in days between the original (Org.) and the newer release (New.).
    \vspace{-0.4cm}
\end{table}





\phead{RQ3: Are new duplicate logging code smell instances introduced over time?}

\noindent{\bf Motivation.}
In this RQ, we investigate if new instances of duplicate logging code smell are introduced during the evolution of systems. An automated detection tool may then help developers detect such problems overtime.


\noindent{\bf Approach.}
We applied \toolS on the latest versions of the five studied systems, i.e., Hadoop, CloudStack, Elasticsearch, Cassandra and Flink, and compare the results with the ones on previous versions. The gaps of days between the manually studied versions and the new versions vary from 77 days to 301 days.


\noindent{\bf Results and discussion.}
Table~\ref{RQ3} shows that new instances of duplicate logging code smells are introduced during software evolution. 
All the detected problematic instances (i.e., instances of IC, IE, and LM) are reported and fixed.
As mentioned in Section~\ref{sec:manual} and \ref{sec:detection}, our goal of detecting DP is to show developers the logging technical debt in their systems. The number of commits for the studied time periods are: 282 commits for Cassandra, 1,097 commits for Cloud Stack, 1,036 for Elasticsearch, 485 commits for Hadoop, and 3,036 commits for Flink. These 9 instances that we detected and fixed were introduced during the studied period. For the systems that we did not find new instances of IC, IE, and LM, the number of commits is either small (e.g., 282 commits for Cassandra) or have fewer log lines (e.g., Elasticsearch has only 1.7K log lines). However, we still find new instances of DP in Cassandra and Flink. In short, we found that duplicate logging code smells are still introduced over time, and an automated approach such as \toolS can help developers avoid duplicate logging code smells as the system evolves.

\rqboxc{The duplicate logging code smells exist in both manually studied and additional systems. In total, \toolS is able to detect 81 out of 91 problematic duplicate logging code smell instances (combining the results of RQ1, RQ2, and RQ3 for pattern IC, IE, and LM). We also find that new instances of logging code smells are introduced as systems evolve.}
%\vspace{-0.2cm}

%the results related to this RQ.
%Since the purpose of this RQ is to show whether new duplicate logging code smells instances are introduced to systems over time, the instances that have already been studied in the previous sections are excluded.
%Our results show that new duplicate logging code smells are introduced during software evolution. \toolS detects 31 new problematic instances in the latest version of the four systems.


\begin{figure}
    \centering
  \subfloat[Precision\label{comparea}]{%
       \includegraphics[width=0.45\linewidth]{figures/precision1}}
    \hfill
  \subfloat[Recall\label{compareb}]{%
        \includegraphics[width=0.45\linewidth]{figures/recall1}}
    \\

  \caption{The precision (a) and recall (b) of \toolS detecting LM on the systems of RQ1 and RQ2 respectively, compared with the baseline (random prediction).}
  \label{figure:compare}
  \vspace{-0.4cm}
\end{figure}

