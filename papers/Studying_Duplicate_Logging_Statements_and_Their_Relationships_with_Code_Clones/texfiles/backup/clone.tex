\section{RQ4: What are the Relationships between Duplicate Logging Statements and Code Clone?}
\label{sec:clone}


\phead{Motivation. }
\zhenhao{Comment 2.3, motivation } During our manual analysis in Section~\ref{sec:manual}, we noticed that some duplicate logging statements or duplicate logging code smells may be related to code clones. Code clone or duplicate code is considered a bad programming practice and an indication of deeper maintenance problems~\cite{refactoring1999}.
Prior studies focus on the detection of code clones by applying text-based approaches~\cite{cpminer, kamiya2002} or abstract syntax tree analysis~\cite{baxter1998, jian2007} on program source code. However, these studies did not study code clones from the perspective of logging code. Logging statements might also be copied along with other code since cloning is often performed hastily without much attention on the context~\cite{5463343}. Some cloned logging statements (e.g., the LM logging code smells in Section~\ref{sec:manual}) may fail to correctly record the runtime behaviors; and thus, increase maintenance difficulties.
%Duplicate logging statements may happen by chance (e.g., two different methods share the same error handling mechanism), or may happen due to code clones. Therefore, even if we do not find any duplicate logging code smells, such cloned logging statements may still cause maintenance overhead.

Hence, given the initial observation from our manual study, in this section, we investigate the relationship between duplicate logging statements and code clones. Our finding may provide researchers and practitioners with insights of other possible effects of code clones (e.g., cloned logging statements), how to further improve logging practices, and may inspire future code clone studies. %We conduct our study by answering two research questions.


%Duplicate logging statements may reside in cloned code snippets because code sixth similar syntactics (i.e., cloned code) can be expressed by identical semantic information in log messages (i.e., duplicate logs). However, it is also possible that cloned code, although with similar syntactical structures, may occur in different contexts, therefore is expressed using different semantic information (i.e., different logging statements).

%As shown in previous sections, we observe that some duplicate logging code smells may be related to code clones. Therefore, in this section, we further study the relationship between duplicate logging statements and code clones. Our goal is to empirically study such relationship, provide insights to researchers and practitioners on other possible effects of code clones, and how to further improve logging practice. We conduct our study by answering two research questions. For each RQ, we describe our motivation, approach, result, and followed by a more detailed discussion.

%\jinqiu{Do we miss the rationale behind studying clones and duplicate logging statement? I think the messages are scattered in diff. places.``Code clones are expected to have structural similarities. Duplicate logging statements indicate similar semantic information about program execution. In this or these RQ/RQs, we investigate the relationship between duplicate logging statements and whether the surrounding code snippets are clones. It is possible that duplicate logging statements reside in cloned code snippets because code with similar syntactics could be represented by the same semantic information (i.e., duplicate logs). Meanwhile, it is also possible that clonded code, despite the similar structure, may appear in different contexts, thus should be expressed by different semantic information. Therefore, the fact that duplicate logging statements appearing in cloned code snippets may indicate maintenance issues. Thus, investigating the relationship between syntatic similarity and one indication of semantic similarity (i.e., through logs) can motivate future research in combining these two sources of information or identifying inconsistencies between these two types of similarity due to maintenance challenges."}



%code clone survey: http://research.cs.queensu.ca/home/cordy/Papers/RC_ICPC08_ScenarioEval.pdf


%explore the potentiality of which the study of code clone could help with the problem of duplicate logging statements and further improve the logging practice. %We first illustrate the approach of how we conduct the study, then present the results with discussion.


%Code clone or duplicate code is a bad smell and thus one of the indicators of poor maintainability~\cite{refactoring1999}. If a piece of code is buggy, then the cloned code could also replicate the bug silently. Logging statements might also be copied together with other code since cloning is often performed hastily without much care about the context~\cite{5463343}. This could result in the logging statements fail to correctly logging the runtime behaviors. In this section, we study the relationship between duplicate logging statements and code clone, in order to explore the potentiality of which the study of code clone could help with the problem of duplicate logging statements and further improve the logging practice. We first illustrate the approach of how we conduct the study, then present the results with discussion.


%\phead{RQ4: What is the relationship between duplicate logging statements and code clones?}

%\noindent{\bf Motivation.} In this RQ, we want to further investigate how many duplicate logging statements are indeed related to code clones. The result of this RQ may provide an initial evidence on the prevalence of log-related clones. Our findings may also highlight the issues in maintaining code clones from the perspective of logging statements, and help researchers and developers avoid such problems in the future.

%\zhenhao{This paragraph looks more like the motivation of RQ5} \peter{need to polish: Our results may provide an initial evidence on the prevalence of log-related clones and help researchers and developers avoid such problems in the future.}\peter{TODO: revisit the motivations after the results are done}

%\jinqiu{The result may highlight the issues in maitaining code clones from the aspect of logging statements. Future research in code clone study may consider logging statements to provide developers with better support.}

%Code clone or duplicate code is a bad smell and thus one of the indicators of poor maintainability~\cite{refactoring1999}. If a piece of code is buggy, then the cloned code could also replicate the bug silently. Logging statements might also be copied together with other code since cloning is often performed hastily without much care about the context~\cite{5463343}. This could result in the logging statements fail to correctly logging the runtime behaviors. In this research question, we would like to investigate the relationship between duplicate logging statements and cloned code snippets. We expect to provide an inspiration of how ... \zhenhao{selling point}
%{\em et al$.$}
\noindent{\bf Our approach of mapping code clones to duplicate logging statements.}
We use both an automated and a manual approach to study the relationship between code clones and duplicate logging statements. Due to the large number of duplicate logging statements in the studied systems, we first leverage automated clone detection tools to study whether these logging statements reside in cloned code. In particular, we use NiCad5~\cite{nicad} as our clone detection tool. NiCad5 uses hybrid language-sensitive text comparison to detect clones. We choose NiCad5 because, as found in prior studies~\cite{nicad,NicadEvaluation}, it has high precision (95\%) and recall (96\%) when detecting near-miss clones (i.e., code clones that are very similar but not exactly the same) and is actively maintained (latest release was in July 2019). %We choose NiCad5 because it is capable of detecting near-miss clones with high precision and recall which is at least as well as existing techniques, while it is more flexible that it does not require compiled code~\cite{ROY2009470,nicad}. \zhenhao{In that evaluation of clone detection papers, they didn't say Nicad is the best or better than other tools. They just provided an intention that we could use different tools in different situations, though the situation that Nicad is recommented is not too much of our business.}

In NiCad5, the source code units of comparison are determined by partitioning the source code into different granularities. %a set of disjoint fragments (e.g., methods or code blocks).
%These source code units are the largest source code fragments (e.g., basic blocks) that are considered in the clone detection~\cite{nicad}. %may be clone targets involved in direct clone relations with each other~\cite{nicad}.
The structural granularity of the source code units could be set as the method-level or block-level (e.g., the blocks of {\em catch}, {\em if}, {\em for}, or {\em method}, etc). In our study, we set the level of granularity to block-level and use the default configuration (i.e., similarity threshold is 70\% and the minimum lines of a comparable code block is 10),
%\jinqiu{Does the paper say the default configurations would yield better results? If so, add that here. Intuitively, 10 sounds too big a number for this configuration though. The example of IL in Table 2 does not have more than 10 lines}
which is suggested by prior studies indicating this configuration could achieve remarkably better results in terms of precision and recall~\cite{EvaluatingModernCloneDetectionTools, ROY2009470, nicad}. Block-level provides finer-grained information, since logging statements are usually contained in code blocks for debugging or error diagnostic purposes~\cite{Fu:2014:DLE:2591062.2591175}. Note that if the block is nested, the inner block is listed twice: once inside its parent block and once on its own. Hence, all blocks with lines of code above the default threshold will be compared for detecting clones. %\peter{I moved the numbers above. We can define what they meant by threshold above as well}For the thresholds and settings of detection, we keep them in default (the minimum lines of a comparable block is 10, the similarity threshold is 70\% \zhenhao{might briefly explain how do they define similarity.})\zhenhao{These are the threshold they used, but didn't describe the reasons in their paper}.
%\peter{avoid passive voice}The study is conducted by answering two research questions. For the identification of cloned code, We use NiCad5 to conduct the detection of code clone. NiCad5 is a flexible hybrid language-sensitive text comparison clone detection tool originally based on the previous work of Roy. {\em et al$.$}~\cite{nicad} with continuously maintainence and improvement afterwards. Up to the submission of this work, the latest version of NiCad5 was released on Oct. 05, 2018. The source unit of comparison are determined by partitioning the source into a set of disjoint fragments. These units are the largest source fragments that may be involved in direct clone relations with each other~\cite{nicad}. The structural granularity of source units could be set as method-level or block-level (e.g., the blocks of {\em catch, if, for, method, etc}). In our study, we set the level of granularity as block-level, a more fine-grained level which might provide us with more accurate results. Note that if the block is nested, the inner block is listed twice: once inside its parent block and once on its own. \peter{Hence, all qualified blocks will be compared by the tool.}%So that all the qualified blocks (with more potential cloned lines than the threshold) could be compared.
We run NiCad5 on the eight studied open source systems that are mentioned in Section~\ref{sec:results}. We then analyze the clone detection results and match the location of the clones with that of the duplicate logging statements. If two or more cloned code snippets contain the same set of duplicate logging statements (i.e., the static text messages are the same), we consider the logging statements are related to the clone and record the code clone similarity score.

%\zhenhao{CSD:4, CS:119, ES:5, HD:7, CM:113, FLK:25, KFK:6, WKT:2, Total: 281}
%\zhenhao{272/281, 9 FPs}


Although NiCad5 has a great precision in detecting clones (its precision is 96.8\% in our studied dataset, more details in Section~\ref{sec:threats}), there may still exist some false negatives. Therefore, we then manually investigate a sample of duplicate logging statements which reside in code snippets that are {\em not} classified by NiCad5 as clones. Our goal is to study if these duplicate logging statements are indeed not in cloned code, or they are not detected due to other reasons, e.g., limitation of the clone detection tool NiCad5. We follow similar steps as described in Section~\ref{sec:manual} in our manual analysis. We randomly sample sets of duplicate logging statements that are {\em not} classified by NiCad5 as clones, based on a confidence level of 95\% and a confidence interval of 5\%. Then, we study their surrounding code.



\begin{table}
    \caption{Automated code clone analysis results on duplicate logging statements. {\sf DupSet}: Total sets of duplicate logging statements. {\sf CloneSet}: Sets of duplicate Logging statements that are from cloned code snippets.  {\sf Avg. Sim.}: Average similarity of the cloned code snippets.
}


    \vspace{-0.3cm}
    \centering
    %\resizebox{\columnwidth}{!} {
    \tabcolsep=18pt
    \scalebox{0.8}{

    \begin{tabular}{l|c|c|c}

        \toprule
        & \multicolumn{1}{c|}{\textbf{DupSet}} & \multicolumn{1}{c|}{\textbf{CloneSet}}  & \multicolumn{1}{c}{\textbf{Avg. Sim.}} \\

        \midrule
        \textbf{Cassandra}     & 46  & 14 (30.4\%) & 79.7  \\
        \textbf{CloudStack}     & 865  & 442 (51.1\%) & 80.3  \\
        \textbf{ElasticSearch}  & 40  & 17 (42.5\%) & 72.2  \\
        \textbf{Hadoop}         & 217  & 25 (11.5\%) & 76  \\
        \textbf{Camel}         & 886  & 421 (47.5\%) & 80.7  \\
        \textbf{Flink}         & 203  & 92 (45.3\%) & 78.8 \\
        \textbf{Kafka}         & 104  & 23 (22.1\%) & 75.4  \\
        \textbf{Wicket}         & 21  & 8 (38.1\%) & 83.1 \\
\midrule
        \textbf{Overall}         & 2,382   & 1,042 (43.7\%) & 80.0  \\
        \bottomrule
    \end{tabular}
    }%}
    \vspace{-0.6cm}
\label{table:RQ4}
\end{table}


\begin{comment}
Cloudstack: Y40 H7, 94
Hadoop: Y15 H7, 43
Elasticsearch: Y2, 5
Cassandran: Y3 H1, 7
Camel: Y45 H13, 103
Wicket: H1, 3
Kafka: Y8 H2, 18
Flink: Y8 H1, 25
Total: Y121 H28, 298
\end{comment}

\peter{I wonder if we should put the first part of RQ4 (both automated and manual) in appendix. The motivation would be stronger if we start directly with problematic instances.}
\noindent{\bf Results of automated code clone analysis on duplicate logging statements.} {\em We find that a significant number of duplicate logging statements (43.7\% on average) reside in cloned code snippets.}
Table~\ref{table:RQ4} presents the results of our code clone analysis. {\sf DupSet} refers to the total sets of duplicate logging statements (a set contains two or more logging statements with the same text message). {\sf CloneSet} refers to the subset of duplicate logging statement sets ({\sf DupSet}) that are from cloned code snippets. The percentage number is the proportion of {\sf CloneSet} out of {\sf DupSet}. Finally, {\sf Avg. Sim.} refers to the average code clone similarity score among the cloned code snippets. As shown in Table~\ref{table:RQ4}, 11.5\% to 51.1\% sets of duplicate logging statements are from the cloned code snippets in the studied systems. Overall, 1,042 out of 2,382 (43.7\%) sets of duplicate logging statements are related to code clones (with an average 80\% similarity score).

Our finding shows that a significant number of duplicate logging statements are related to code clones, and developers may not change the log messages when they copy a piece of code to another location.
However, due to the importance of logging for understanding system runtime behaviour~\cite{petericseseip2017, Yuan:2012:CLP:2337223.2337236, Yuan:2011:ISD:1950365.1950369}, developers should avoid directly copying logging statements. Developers should consider modifying the log messages (e.g., to include the class name, modify the message to reflect code changes, or record new important dynamic variables) to assist debugging and workload understanding.

%the proportion in the studied systems varies from 11.5\% to 51.1\%. However, overall 1042 out of 2382 (43.7\%) sets of duplicate logging statements are marked as from cloned code with an average similarity of 80 (i.e., 80\% of the compared source code units are identical in the corresponding blocks. \zhenhao{Need to explain similarity in approach} ).


\noindent{\bf Results of manual code clone analysis on duplicate logging statements.} {\em We find that more than 50\% of the sampled duplicate logging statements reside in cloned code snippets that are difficult to detect using automated code clone detection tools. In particular, 24.5\% of the manually studied duplicate logging statements are related to code clones, and 26.2\% are related to micro-clones. }
In total, we randomly sample 298 sets of duplicate logging statements to achieve a confidence of 95\% and a confidence interval of 5\%. For each set of the sampled duplicate logging statements, we manually classify them into three types if they meet the described criteria:

%We follow similar steps as described in Section~\ref{sec:manual} in our manual analysis. We first randomly sample 298 sets of duplicate logging statements from duplicate logging statements reside in code snippets that are {\em not} classified by NiCad5 as clones (with a confidence level of 95\% and a confidence interval of 5\%). Then, we study their surrounding code. For each set of the sampled duplicate logging statements, we manually classify them into three types if they meet the described criteria:

%\zhenhao{What if we say the threshold of manual study is 10 here? And change the label of those fewer than 10 lines from Clones to Undecideable. Then we discuss why those longer than 10 lines were not detected, and say what we find in the Undecidable ones...}\peter{that would work, but would that require a lot of additional work?}
%\zhenhao{Hmm... not a lot of additional works, as long as doing so is reasonable / correct}
%\peter{but that is still bias towards the threshold. }
%For the duplicate logging statements that are not detected as from cloned snippets, we investigate whether they are truely not from cloned snippets, or they are from cloned snippets but failed to be detected due to some reasons (e.g., limitation of the tool, complicated code structure, etc.). We randomly sample 298 sets of duplicate logging statements with their surrounding code blocks and manually study them following similar phases as described in Section~\ref{sec:manual}. The results of this phase have a Cohenâ€™s kappa of X, which is a Y-level of agreement~\cite{kappa}. For each item of the sample, We manually mark it as one of the following three labels:

\begin{itemize}\itemsep 0em
    %\item Clones (C): if at least two of the logging statements in the set meet the following requirements: 1) The surrounding code block that the logging statement locates in has 5 or more lines of code. \zhenhao{the reason is, it's hard to say if short blocks are clones or just by chance.} 2) At least 5 lines of code surrounding the logging statement are exactly the same, or nearly the same, only with the differences of identifier names.
    \item Clones: %1) The code block where the logging code statements are located is not too short (i.e., has ten or more lines of code, consistent with the threshold of the tool). 2)
    The code around the logging statements is more than 10 lines of code (same as the threshold of the clone detection tool). The code is exactly the same, or only with differences in identifier names (i.e., Type 1 and Type 2 clones~\cite{CompareCloneDetectionTools2007}) while not detected by the automated clone detection tool.

    \item Micro-clones: The code around the logging statements is very similar but is less than 10 lines of code~\cite{microclones}. Prior studies show that micro-clones are more difficult to detect due to their size. However, the effect of micro-clones on code maintenance and quality is similar to regular-size clones~\cite{MicroclonesAndBugsICPC, MicroclonesAndBugsSANER}. Micro-clones should not be ignored when making decisions of clone management.
     %1) The code block where the logging code statements are located is short (i.e., has fewer than ten lines of code\peter{might need to remove the rest?}, below the threshold of the tool). 2) The code in the block is exactly the same, or only with differences in identifier names. For this kind of situations, though we feel they are likely to be clones, their sizes are not long enough for us to make a decision. Therefore, we label them as "Undecidable".
     %Prior studies indicate that micro-clones have similar tendencies of replicating severe bugs as regular clones~\cite{MicroclonesAndBugsICPC, MicroclonesAndBugsSANER}.
     \item Non-clones: For other situations, we mark them as non-clones.
\end{itemize}


%The columns under ``DupSet'' in
Table~\ref{table:manualrq4} presents the results of our manual study. Overall, 73 out of the 298 (24.5\%) manually-studied sets of duplicate logging statements are labeled as {\em Clones}. 78 out of 298 (26.2\%) sets are labeled as {\em Micro-clones}. The remaining 147 out of 298 (49.3\%) sets are labeled as {\em Non-clones}.
%The proportion of {\em Clones} that we observe in the manual study (40.1\%) is similar to the clones that are detected by the tool (43.7\%). \zhenhao{Need to apply the new threshold and update the results.}~\jinqiu{I do not think the porportion means much though, we can just say the percentage of agreed results on the `clone' labels between the manual work and the clone tool.}
For the clones and micro-clones that are identified by our manual analysis but not detected by the tool, we summarize the potential reasons into three categories. Note that the tool may fail to detect a clone due to one or more reasons.

{\em \underline{Category 1:} Code clones reside in part of a large code block.} Since the structural granularity level of the source code units is block-level (i.e., the minimal comparable source code unit of the tool is a block), the similarity of the code is computed by comparing blocks. However, for 42 out of the 73 cases of {\em Clones}, and 32 out of 78 cases of {\em Micro-clones}, we find that developers often only copy and paste part of the code into another large code block. For example, developers may copy the code (along with the logging code statements) that is related to establishing network connect from one file to another. However, the copied code is just part of another complex method; hence, the clone detection tool fails to detect the code snippets as clones since the minimum detectable source code unit is block.

%{\em \underline{Category C-1:} Code clones reside in part of a large code block.} Since the structural granularity level of the source code units is block-level (i.e., the minimal comparable source code unit of the tool is a block), the similarity of the code is computed by comparing blocks. However, for 42 out of the 73 sampled cases, we find that developers often only copy and paste part of the code into another large code block. For example, developers may copy the code (along with the logging code statements) that is related to establishing network connect from one file to another. However, the copied code is just part of another complex method; hence, the clone detection tool fails to detect the code snippets as clones.

{\em \underline{Category 2:} Code clones reside in code with very similar semantics but have minor differences.}  %\peter{do you know if they are Type 1, 2, or 3 based on the definition?} \zhenhao{for this category some are type 2 some are type 3. They claim they can detect type 1 - 3 clones in the paper, but idk why some type 2, 3 clones were not detected even though the blocks were longer than 10 lines.}
For 53 out of the 73 cases that are manually identified as {\em Clones}, we find that the surrounding code of duplicate logging statements share highly similar semantics (i.e., implement a similar functionality), but have minor differences (e.g., additions, deletions, or partial modification on existing lines). Such scattered modifications might significantly reduce the similarity between the code structures, and thus, result in miss detection~\cite{ROY2009470,nicad}. %Note that in some cases, they are characterized as belong to both of the two reasons (i.e., they reside in part of a long code block, and with slight modifications).
For example, there is a code block in {\tt\small \textbf{FTP}Consumer} of Camel which does a series of operations based on the file transfer protocol (FTP). %The code in the block may be reuseable for other similar ptopocols. Thus,
Due to the similarity between FTP and secured file transfer protocol (SFTP), Camel developers copied the code block and made modifications (e.g., change class and method names) to the all the places where SFTP is needed (e.g., {\tt\small \textbf{SFTP}Consumer}). Therefore, clone detection tools may fail to detect this kind of cloned code blocks as due to minor yet scattered changes.
%\jinqiu{What about the opposite side? The dupSets are not in code clones. DupSets cannot always be detected by clone tools. Many dup. logging statements are not due to structural similarity but flow similarity??}

{\em \underline{Category 3:} Short methods.}
For more than half of the cases that are labeled as {\em Micro-clones} (46 out of 78), we find that these logging statements reside in very short methods with only a few lines of code. For example, there is a method in CloudStack named {\em verifyServicesCombination()} containing only six lines of code and duplicately locates in three different classes. The method verifies the connectivity of services, and generates a warning-level log if it fails the verification. Clone detection tool fails to detect this category of cases due to their small size compared to regular methods.


%For the cases that are identified as undecidable in our manual study, we categorize them into two categories.

%{\em \underline{Category 3:} Short methods.}
%For more than half of the cases that are labeled as {\em Micro-clones} (46 out of 78 cases), we find that these logging statements reside in very short methods with only a few lines of code. For example, there is a method in CloudStack named {\em verifyServicesCombination()} containing only six lines of code and duplicately locates in different classes. The method verifies the connectivity of services, and generates a warning-level log if it fails the verification. In such situations, although we suspect that two code snippets are clones, it is also possible that the code is the same by coincidence.% since short methods do not execute considerable system behaviours.

%{\em \underline{Category U-2:} Short code blocks within a larger code block.} % reside in long parent blocks.}
%For 32 out of 78 cases, we find that the logging statements are located in short code blocks (e.g., {\em catch} blocks in exception handling), but such code blocks are located within a larger code block.
%For example, there are two different application deployment approaches in Kafka, located in two classes: {\tt\small ConnectStandalone} and {\tt\small ConnectDistributed}. Both of the classes contain {\em catch} blocks that abort the system and record an error message when an exception happen. Even though the code in the catch blocks is exactly the same, the code in the {\em try} blocks is different (i.e., process different deployment types). Hence, it is difficult to say that the code in the {\em catch} blocks is clone. %not identified as code clones by the clone detection tool and is difficult to.
%Moreover, the exception handling code and log messages in such cases may be generic, so it is possible that developers intentionally write the same code for exception handling. %this sort of identical code snippet, rather than copy-and-paste.


%and the rest part of their parent blocks are noticeably different, they are omitted by the clone detection tool. For example, there are two application deployment methods in Kafka, located in Class ConnectStandalone and Class ConnectDistributed, respectively. Both of them contain catch blocks which abort the system and log an error message when an exception happen. Though the code in catch blocks are exactly the same, the code in their parent blocks are different (they process the deployments in different types), they are not detected by the clone detection tool.
%Moreover, the exception handling code and log messages in this kind of situation might be in general. Hence, it is possible that developers intentionally write this sort of identical code snippet, rather than copy-and-paste.

\rqboxc{We find that more than half of the duplicate logging statements reside in cloned code snippets. Our manual study also highlights that many duplicate logging statements reside in cloned code that may be difficult to detect.}



\begin{table}
  \caption{The result of manual study on duplicate logging statements and code clones.}
    \vspace{-0.3cm}
    \centering
    %\resizebox{\textwidth}{!} {
    \tabcolsep=8pt
    \scalebox{0.9}{
    \begin{tabular}{l|cccc}

        \toprule
     &{\textbf{Clones}} &{\textbf{Micro-clones}} &{\textbf{Non-clones}} &{\textbf{Total}}  \\
\midrule

        \textbf{Cassandra}    & 1 & 3 & 3 & 7  \\
        \textbf{CloudStack}    & 22 & 26 &46  & 94 \\
        \textbf{ElasticSearch} & 1 & 1 & 3 & 5 \\
        \textbf{Hadoop}      & 12 & 6 & 25  & 43 \\

       \textbf{Camel}       & 28 & 30 & 45 & 103 \\
       \textbf{Flink}       & 5 & 4 & 16 & 25\\
       \textbf{Kafka}       & 3 & 7 & 8 & 18 \\
       \textbf{Wicket}       & 1 & 1 & 1 & 3 \\

        \midrule

        \textbf{Total}       & 73 &78 & 147 & 298 \\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \bottomrule
    \end{tabular}}
    %}
    \vspace{-0.3cm}

    \label{table:manualrq4}
\end{table}



%\phead{RQ5: What is the relationship between the problematic instances of duplicate logging code smells and code clones?}
\phead{Discussion: Studying the relationship between problematic instances of duplicate logging code smells and code clones.} %As discussed in the previous sections, not all of the duplicate logging code smells are problematic and require fixes. While non-problematic duplicate logging statements may be an indication of maintenance problem, problematic instances of duplicate logging code smells should be fixed in order to correctly record the execution behaviors. Hence, On top of the study on duplicate logging statements and code clones,
\peter{we can merge table 8 and 9 together to avoid confusion. We can say directly that we use tool assisted manual study}We further analyze our clone detection results to investigate how many {\em problematic instances} of duplicate logging code smells are related to code clones. We use the automated clone detection results obtained in this RQ and match them with problematic instances of logging code smells. To reduce the effect of false negatives, we manually study the code of {\em all} the remaining instances that are not identified as clones. % For the instances that are not identified by NiCad5 as code clones (i.e., potential false negatives) \zhenhao{I'm still feeling this should be false negatives. In my understanding, positive = alarm = detected}\peter{you are right...I made a mistake here}, we manually study {\em all} of them to reduce the effect of false positives on the result.%match the results with the instances of problematic duplicate logging code smells that are studied in Section~\ref{sec:manual} and \ref{sec:results}. We then manually study {\em all} the instances of problematic duplicate logging code smells residing in code snippets that are {\em not} classified by NiCad5 as clones (i.e., false positives).

%\noindent{\bf Motivation.}
%In RQ4 we conduct an investigation of the relationship between duplicate logging statements and code clones. However, As discussed in the previous sections, not all of the duplicate logging code smells are problematic and require fixes. While non-problematic duplicate logging statements may be an indication of maintenance problem, problematic instances of duplicate logging code smells should be fixed in order to correctly record the execution behaviors. Thus, in this RQ, we would like to specifically investigate how many problematic instances of duplicate logging code smells are related to code clones and the potential impact of code clones on logging practice.

%\noindent{\bf Approach.}
%We use the clone detection results obtained in RQ4 and match the results with the instances of problematic duplicate logging code smells that are studied in Section~\ref{sec:manual} and \ref{sec:results}. We then manually study {\em all} the instances of problematic duplicate logging code smells residing in code snippets that are {\em not} classified by NiCad5 as clones.



\begin{table*}
  \caption{The result of automated code clone analysis on problematic instances and code clones.  {\sf Num. Ins.} shows the number of problematic code smell instances. {\sf Cloned. Ins.} shows the number of problematic code smell instances that are detected as clones.  {\sf Sim.} shows the average similarity of the cloned code snippets.}
    \vspace{-0.3cm}
    \centering
    %\resizebox{\textwidth}{!} {
    \scalebox{0.78}{
    \tabcolsep=8pt
    \begin{tabular}{c|rrr|rrr|rrr|rrr}

        \toprule
    & \multicolumn{3}{c|}{\textbf{IC}} & \multicolumn{3}{c|}{\textbf{IE}} & \multicolumn{3}{c|}{\textbf{LM}} & \multicolumn{3}{c}{\textbf{DP}}\\
    & Num. Ins. & Cloned. Ins. &Sim. & Num. Ins. & Cloned. Ins. &Sim.& Num. Ins. & Cloned. Ins. &Sim.& Num. Ins. & Cloned. Ins. &Sim. \\
\midrule

        \textbf{Cassandra}     & 1 & 0 (0\%) & --    & 0 & -- (--)  &--     & 0  & -- (--)  &--  & 2&  2 (100\%) &70 \\
        \textbf{CloudStack}    & 8 & 5 (62.5\%) &72.6    & 4 & 4 (100.0\%)  &99     & 27& 20 (74.1\%)  &77.4  &  107 & 60 (56.1\%) &77 \\
        \textbf{ElasticSearch} & 1 & 0 (0\%)  &--    & 0 &-- (--)  &--     & 1  & 0 (0\%)  &--  & 3& 0 (0\%) &-- \\
        \textbf{Hadoop}        & 5 & 1 (20\%)  &70    & 0 &-- (--)  &--     & 9 & 0 (0\%)  &--  & 27& 5 (18.5\%) &70.4 \\

       \textbf{Camel}         & 1 & 0 (0\%) &--     & 0 & -- (--) & --      & 14  & 6 (42.9\%)  &75.2  & 29 & 22 (75.9\%) &72.5 \\
       \textbf{Flink}         & 0 & -- (--)  &--    & 2 & 1 (50.0\%)  & 80     & 4  & 2 (50\%)  &75  & 24 & 19 (79.2\%) &81.7 \\
       \textbf{Kafka}         & 0 & -- (--)  &--    & 0 & -- (--)  &--      & 3  & 1 (33.3\%)  &95.0  & 14 & 3 (21.4\%) &74 \\
       \textbf{Wicket}         & 1 & 0 (0\%) &--     & 0 & -- (--) &--     & 1  & 0 (0\%)  &--  & 1 & 1 (100\%) &100 \\

        \midrule

        \textbf{Total}         & 17 &6 (35.3\%) & 72.2     & 6 & 5 (83.3\%) &95.2     & 59 &29 (49.2\%) &77.4     &207 & 112 (54.1\%) &76.6\\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \bottomrule
    \end{tabular}
    }
    %}
    \vspace{-0.3cm}

    \label{table:RQ5}
\end{table*}

%\peter{You can use multirow to make the second part of the sentence in the next line. Then in the text, we just say, the first column shows...}
%\noindent{\bf Quantitative study.}
\noindent{\bf Result of automated code clone analysis on problematic instances.}
{\em We find that more than half of the problematic instances of duplicate logging code smells (52.6\%) reside in cloned code snippets.}
Table~\ref{table:RQ5} presents the results of our automated code clone analysis. The term {\sf Num. Ins.} refers to the number of instances of problematic duplicate logging code smells. {\sf Cloned. Ins.} refers to the number of problematic instances that are identified by NiCad5 as in code clones. The percentage following each number indicates the proportion of {\sf Cloned. Ins.} out of {\sf Num. Ins}. {\sf Avg. Sim.} refers to the average similarity of {\sf Cloned. Ins}. As shown in Table~\ref{table:RQ5}, 6 out of 17 (35.3\%) instances of IC, 29 out of 59 (49.2\%) instances of LM, 5 out of 6 (83.3\%) instances of IE, and 112 out of 207 (54.1\%) instances of DP are identified as from cloned code snippets.

{\em Overall, we find that many instances (i.e., IC, IE, LM and DP) of problematic duplicate logging code smells (152 out of 289) are related to code clones}. In other words, in addition to the finding from prior code clone studies which indicates that code clones may introduce subtle program errors~\cite{contextCloneBugs, tracyhallcodesmell}, {\em we find that code clones may also result in bad logging practices that could increase maintenance difficulties.} Future studies should further investigate the negative effect of code clones on the quality of logging statements and provide a comprehensive logging guideline.
%\jinqiu{Is IE somehow special and worth a brief discussion?}
%\zhenhao{Hmm the size of IE is too small compared to other patterns.}
%\jinqiu{is it interesting to differenciate type i and type ii clones?}
%\zhenhao{The tool doesn't specify the type in the report.}
%Our results shed light on the potential effects of code clones on other aspects of source code (i.e., logging).~\jinqiu{Is this identical text with that of RQ4? We can be more specific here: due to the problematic nature, more or less related to code clones?} Future studies should further study how code clones may affect debugging production problems and program comprehension.
%program comprehension  %shows that many logging-related issues are related to code clones.

\begin{comment}
\peter{maybe can comment this part out. this is discussed in the discussion}The proportion of clones in IC is lower than that of other patterns. The potential reason might be that the instances of IC are located in different catch blocks of the same try block. Based on our observation during the manual study, the code in catch block is usually short, with only a few lines of exception handling code. Thus, the code is often omitted by clone detection tools which requires a larger code block. %takes a non-negligible number of the minimum lines of comparable source code into account.
Technically it is hard to identify whether the surrounding code of IC instance is from cloned code or not.

%show that most of the problematic logging code smell instances are related to code clones.
\end{comment}



\begin{table*}
  \caption{The result of manual study on problematic instances and code clones. {\sf IC, IE, LM, DP}: Instances of each pattern.}
    \vspace{-0.3cm}
    \centering
    %\resizebox{\textwidth}{!} {
    \tabcolsep=5pt
    \scalebox{0.83}{
    \begin{tabular}{c|ccc|ccc|ccc|ccc}

        \toprule
      & \multicolumn{3}{c|}{\textbf{IC}} & \multicolumn{3}{c|}{\textbf{IE}} & \multicolumn{3}{c|}{\textbf{LM}} & \multicolumn{3}{c}{\textbf{DP}} \\
   &Clones &Micro-clones & Total &Clones &Micro-clones & Total &Clones &Micro-clones & Total &Clones &Micro-clones & Total \\
\midrule

        \textbf{Cassandra}    & 0 & 0 & 1     & 0  &0  & 0     & 0 & 0 & 0     &  0  &0 &0 \\
        \textbf{CloudStack}      & 0  &3 & 3    & 0 &0 & 0    & 1  &5 & 7     & 12 &9 &47 \\
        \textbf{ElasticSearch}    & 0 & 0 & 1      & 0 &0  & 0   &0  &1  & 1       & 1  &0  &3\\
        \textbf{Hadoop}          & 0  &2 & 4     & 0 &0  & 0   &3  &3  & 9       & 14  &6  &22\\

       \textbf{Camel}          & 0 &1 & 1     & 0  &0 & 0    & 2 & 6 & 8      &2 &4  &7\\
       \textbf{Flink}           & 0 &0 & 0     & 0 &1 & 1    & 0 & 2 & 2       &0  & 3 &5\\
       \textbf{Kafka}           & 0  &0 & 0    & 0 &0 &0    & 0  &1  &2     & 3 & 2 &11 \\
       \textbf{Wicket}           & 0 &0 & 1       & 0 &0 & 0     & 1 &0  &1          & 0 &0  &0 \\

        \midrule

        \textbf{Total}           & 0 &6 & 11    & 0 &1  &1     & 7 & 18 &30         &32 &24 &95\\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \bottomrule
    \end{tabular}}
    %}
    \vspace{-0.3cm}

    \label{table:manualrq5}
\end{table*}



%\noindent{\bf Manual study.}
\noindent{\bf Result of manual code clone analysis on problematic instances.}
{\em We find that 64.2\% of the problematic instances of duplicate logging code smells 
%reside in cloned code snippets that are difficult to detect using automated clone detection tools.
that are labeled as Non-clones by the automated code clone detection tool are actually from cloned code snippets.
Among them, more than half (55.7\%) reside in micro-clones which often do not get enough attention in the process of code clone management.}
%Comment 1.6: 64.2\% = numbers in table 9 (all the clones + all the micro-clones)/total
%Comment 1.6: 55.7\% = micro clones / total

%In our clone detection result, we find that only 49.2\% of LM are identified as from cloned code snippets. Based on our manual analysis in previous sections, we find that many LM instances are related to copy-and-paste code, so we expect that more LM instances would be related to code clones. As a result, for the instances that are {\em not} detected as from cloned snippets, we further conduct a manual study on whether they are actually not from cloned code snippets, or the clone detection tool fails to detect them. We follow a similar process that we discuss in the previous part of this section. We manually study {\em all} the problematic instances that are not detected as from cloned code snippets (137 instances in total) and their surrounding code.
%Note that for the instances of IC, since they are in catch blocks of the same try block which are often very short, we mark them as clones if the code in the catch blocks are exactly the same, in spite of how many lines of code they have.
%The columns from ``IC'' to ``DP'' in
We manually study {\em all} the problematic instances (and their surrounding code) that are not detected as from cloned code snippets (137 instances in total). Table~\ref{table:manualrq5} presents the results of our manual study for each pattern. We categorize each instance according to the labels that we discuss earlier in this RQ (i.e., Category 1-3). Below, we describe the results of each pattern followed by a more detailed discussion.

\noindent{\bf {\em IC \& IE: }} {\em 58\% of the IC and IE instances are related to micro-clones.} Since both of IC and IE reside in {\em catch} blocks, which usually contain only a few lines of code, we discuss these two duplicate logging code smells together. As shown in Table~\ref{table:manualrq5}, none of the IC and IE instances are labeled as {\em Clones}, while 7 (6 IC + 1 IE) out of 12 (11 IC + 1 IE) instances are labeled as {\em Micro-clones}. The remaining five instances are single logging statement that might be thrown with multiple types of exceptions (e.g., {\sf catch (Exception1 \(|\)  Exception2 e)}). We find that all of the seven {\em Micro-clones} instances belong to Category 1 (i.e., short code snippets within a large code block). The reason might be that these logging statements all reside in {\em catch} blocks, which are usually very short. Thus, although the code in these short code blocks are identical or highly similar, they are not long enough to be considered as comparable code blocks by the clone detection tool. %Their parent blocks may have sufficient length, but the cloned code only occupies a small portion of their parent blocks. Thus, the tool fails to detect them due to the insignificant similarity of their parent blocks.
%\peter{not sure if we still want to keep this}Compared to clone detection tools, since \toolS analyzes the code in the {\em catch} blocks using static analysis, \toolS is able to detect both IC and IE with very high precision and recall.

%\noindent{\bf {\em IC \& IE: }} Since both of IC and IE reside in {\em catch} blocks, which usually contain only a few lines of code, we discuss these two duplicate logging code smells together. As shown in Table~\ref{table:manualrq5}, none of the IC and IE instances are labeled as Clones, while 7 (6 IC + 1 IE) out of 12 (11 IC + 1 IE) instances are labeled as Micro-clones. The remaining 5 instances are single logging statement that might be thrown with multiple types of exceptions (e.g., {\sf catch (Exception1 \(|\)  Exception2 e)}). We find that all of the seven Micro-clone instances belong to Category 1 (i.e., short code snippets within a large code block, discussed in RQ4). The reason might be that these logging statements all reside in {\em catch} blocks, which are usually very short. Thus, though the code in these short code blocks are identical or highly similar, it is difficult to determine whether they are code clones or due to the fact that code can be repetitive, especially in smaller code blocks~\cite{naturalness}. Compared to clone detection tools, since \toolS analyzes the code in the {\em catch} blocks using static analysis, \toolS is able to detect both IC and IE with very high precision and recall.

% \jinqiu{This sounds an inheritated problem with micro clones? I don't think it is ``hard to identify'', as some tools are aimed at micro clones and claim good results. Can we say micro clone is an on-going research effort and partially remain subjective, and are less likely caused by maintenance debt? We can find some insights from that SANER 2019 paper.}
%\zhenhao{Don't know how to connect with Micro-clones, since they define micro-clones as clones that no more than five lines of code, however our threshold is 10.}\peter{just add the citation somewhere, we don't need to discuss in detail} \zhenhao{Discussed about it in related works.}

\noindent{\bf {\em LM: }}{\em 83\% of the LM instances are labeled as either Clones or Micro-clones.} As shown in Table~\ref{table:manualrq5}, 7 out of 30 instances are labeled as {\em Clones}, 18 out of 30 instances are labeled as {\em Micro-clones}, and the remaining 5 instances are labeled as {\em Non-clones}. For the five instances that are identified as {\em Clones}, they all belong to Category 2 (i.e., they share highly similar semantics, but have minor differences). The reason might be that developers copy and paste a piece of code along with the logging statement to another location, and apply some modifications to the code. However, developers forgot to change the log message. Similarly, for the five instances that are labeled as {\em Non-clones}, we find that even though the code is syntactically different, the log messages do not reflect the associated method. For the 18 {\em Micro-clones} instances, 11 out of 18 instances belong to Category 3 (short methods), and the remaining 7 are Category 2 (short code snippets within a larger code block). As confirmed by the developers (in Section~\ref{sec:manual}), these LM instances are related to logging statements being copied from other places in the code without the needed modification (e.g., updating the method name in the log).

Our manual analysis on LM instances provides insights on possible maintenance problems that are related to the modification and evolution of cloned code. Future studies may further investigate the inconsistencies in the source code and other software artifacts (e.g., logs or comments) that are caused by code clone evolution.


%In our manual analysis, we find that the log messages indeed do not reflect the associated method (which is also confirmed by developers, as discussed in Section~\ref{sec:manual}). However, these logging statements are inside short methods or blocks, which make them difficult to detect using clone detection techniques.


%As confirmed by the developers, these logging statements are copied from other places in the code without the needed modification (e.g., to update the method name that is recorded in the log). Our finding provides insights on possible maintenance problems that are related to the modification and evolution of cloned code. Future studies may further investigate the inconsistencies in the source code and other software artifacts that are caused by code clone evolution. % and out-of-sync %software artchitecure (e.g., comments or logging statements).


%caused by the evolution and maintenance of cloned code snippets. Future studies may further investigate the inconsistencies that are caused by

%~\jinqiu{``This inspires futhure research might pay attention to the inconsistencies caused by clone evolution and out-of-sync semantic information such as logging statements."}

%developers modified the code significantly~\jinqiu{not sure about this, how do you find that it is due to copy and paste and then the code significantly?}, but they forgot to change the log messages. Therefore, even though the code is not considered clone, we still find inconsistent log messages. ~\jinqiu{``This inspires futhure research might pay attention to the inconsistencies caused by clone evolution and out-of-sync semantic information such as logging statements."}


%\peter{maybe remove this part}Compared to clone detection techniques, \toolS detects LM by analyzing log messages and the corresponding method names. Hence, \toolS may complement clone detection tools on finding LM instances~\jinqiu{how does clone detection tool find LM instances? If by firstly finding clond snippets, isn't that super inaccurate?} that are located in short code blocks.


%Similar to our observation for Clones, the reason might be that developers forget to consistently update the log message  when other code changes. Even though the code in short methods and short blocks might be generic, they are very likely caused by copy-and-paste (since their log messages does not match with the execution behavior), it is difficult to make a decision whether they are clones or intentionally written by developers (they might write the log messages improperly).

\noindent{\bf {\em DP: }}{\em 59\% of the DP instances are either Clones or Micro-clones, which may show that developers often copy code along with the logging statements across sibling classes.} In total, 32 out of 95 DP instances are labeled as {\em Clones}, 23 are labeled as {\em Micro-clones}, and the remaining 43 instances are labeled as {\em Non-clones}. For the 32 instances that are labeled as {\em Clones}, 16 instances are Category 1 (part of a large code block), the remaining 16 instances are Category 2 (very similar semantics with minor differences). For the 24 {\em Micro-clones} instances, 11 instances belong to Category 3 (short methods), and the remaining 13 are categorized as Category 1 (short code snippets within a larger code block).
%Combined with the results from the clone detection tool, around 70\%$\sim$81\% (\peter{the sentence in the bracket is not clear}144/207~\jinqiu{Where is this 207 from?} without any instances of Undecidable, 167/207 with all the instances of Undecidable \zhenhao{(112+32)=144 Clones, with 24 Undecidable instances out of 207 total instances. If 0 Undecidable instance is considered, then 70\%. If all, then 81\%. }) of the DP instances are related to code clones.
Combined with the results from the clone detection tool, around 81.2\% (112 detected by the tool + 32 Clones + 24 Micro-clones identified by manual study, out of 207 total instances) of the DP instances are related to code clones.
One possible reason of a higher percentage of DP instances being related to code clone is that DP is related to inheritance. Classes that inherit from the same parent class may share certain implementation details. Nevertheless, due to the similarity of the code, developers should consider updating the log messages to distinguish the executed methods during production to assist debugging runtime errors.

%However, as discussed in Section~\ref{sec:manual}, resolving DP often requires systematic refactoring, and the fixes of DP instances are not definitely expected as the technical debts need to be considered~\cite{Kruchten:2012:TDM:2412381.2412847}. \zhenhao{what can we learn from the view of code clones?}

%\jinqiu{I am forming one possible highlight here, not necessarily a correct argument: Our finding indicates that semantic information such as duplicate logging statements may serve as trails to distill copy-and-paste clones from general clones, which are commonly considered as bad programming practices and a source of bugs.}

%In summary, our finding indicates that the semantic information in the code, such as logging statements, may be used as trails to help


%\rqbox{Take-home box. \zhenhao{Todo: Talk about sth like LM instances usually locate in short blocks (shorter than 10 lines), so it might be hard for the existing clone detection approaches to find this kind of problems at the meantime of addressing code clone problems. More specific approaches or tools are needed to tackle it.}
%\zhenhao{Still a large portion of problematic instances reside in short methods / short blocks, current clone detection tools might not be able to detect them effectively.}\peter{@Zhenhao, see if you can come up with the takehome based on the updated discussions}
%}
\vspace{-0.1cm}
\rqboxc{We find that most of the instances of the problematic duplicate logging code smells (83.0\%, 240 out of 289 instances, combining the results of tool detection and manual study) are related to code clones. Our finding further shows the negative effect of code clones on system maintenance. Moreover, most problematic instances reside in short code blocks which might be difficult to detect by using existing code clone detection tools.
%Our findings may also highlight the potential of using duplicate logging statements to further improve clone detection tools.
}
\vspace{1mm}
\noindent{\bf Discussion: the potential of whether code clone detection tool can substitute or complement \tool.} As we discussed previously, most of the problematic instances of duplicate logging code smells (83.0\%) are related to code clones. We then discuss the potential of whether code clone detection tool can substitute or complement \toolS in terms of detecting problematic instances of duplicate logging code smells. We use the results of our code clone analysis as a baseline to compare with the detection results of \tool. Similar to our evaluation in Section~\ref{sec:results}, we only compare the detection results of LM since the other patterns are well-defined and relatively independent of the surrounding code. LM depends on the semantics of the logging statement and its surrounding code, of which the detection might potentially be helped by code clone analysis. We consider the code clones that contain duplicate logging statements as the total amount of detected instances. We then analyze the clone detection results and match the location of the clones with that of the ground truth instances of LM. If two or more cloned code snippets contain the logging statements of an LM instance, we consider the duplicate logging statements of this instance as a true positive. Overall, the average precision and recall of this baseline in the manually studied systems are 3.7\% and 53.7\%, respestively. The average precision and recall in the additional systems are 1.5\% and 38.9\%, respectively. In terms of the performance of detection, our approach is noticeably better than this baseline. Moreover, as we discussed in Section~\ref{sec:results}, 10 out of 59 instances of LM are not detected by \tool. Among them, 4 out of 10 instances are detected by this baseline approach. We investigate those instances and find those log messages and class method names are at different levels of abstraction: The log message describes a local code block while the class-method name describes the functionality of the entire method (similar to what we discussed in Section~\ref{sec:results}). In this kind of scenarios, the code clone analysis outperforms our detection rule, which might be an inspiration of improving our detection rule. But it still remains a complicated task for us to balance the trade-off of a few more true positives and much more false positives for merging the code clone analysis into our detection approach. In short, although most of the problematic instances of duplicate logging code smells are related to code clones, they can not be effectively detected by clone detection tools.
%RQ1 systems: GT: 37, TP:20, Detected: 498, for 4+4
%RQ2 systems: GT: 22, TP: 9, Detected: 544, for 4+4
%10 instaces missed by DLFinder
\vspace{-0.1cm}

\noindent{\bf Implication and highlights of our code clone analysis.} Our finding shows that most of duplicate logging statements are indeed related to code clones (i.e., regular clones and micro-clones), and many of which cannot be easily detected by state-of-the-art clone detection tools. Our finding shows additional maintenance challenges that may be introduced by code clones -- understanding the runtime behaviour of system execution. We find that duplicate logging statements may also be a possible indication of hard-to-detect clones based on our manual study. Moreover, most of the problematic instances of duplicate logging code smell reside in cloned code snippets. Hence, future code clone detection studies should consider other possible side effects of code clones (e.g., understanding system runtime behaviour) in addition to code maintenance and refactoring overheads. Future studies may also consider integrating different information in the software artifacts (e.g., duplicate logging statements or comments) to further improve clone detection results. %when implementing clone detection tools. %that for the code snippets that are not detected as clones





\begin{comment}

\begin{table*}
  \caption{The result of manual study in Secion~\ref{sec:clone} {\sf DupSet}: Sampled sets of duplicate logging statements, discussed in RQ4. {\sf IC, IE, LM, DP}: Instances of each pattern, discussed in RQ5.\peter{let's break the Table into 2. Otherwise, in RQ5, we need to keep look at Table 7 which is far away from the discussion}\jinqiu{Missing the category of Non-cloned}}
    \vspace{-0.3cm}
    \centering
    %\resizebox{\textwidth}{!} {
    \tabcolsep=8pt
    \scalebox{0.8}{
    \begin{tabular}{c|ccc|ccc|ccc|ccc|ccc}

        \toprule
     & \multicolumn{3}{c|}{\textbf{DupSet}} & \multicolumn{3}{c|}{\textbf{IC}} & \multicolumn{3}{c|}{\textbf{IE}} & \multicolumn{3}{c|}{\textbf{LM}} & \multicolumn{3}{c}{\textbf{DP}} \\
     &Clones &Undec. & Total &Clones &Undec. & Total &Clones &Undec. & Total &Clones &Undec. & Total &Clones &Undec. & Total \\
\midrule

        \textbf{Cassandra}    & 1 & 3 & 7    & 0 & 0 & 1     & 0  &0  & 0     & 0 & 0 & 0     &  0  &0 &0 \\
        \textbf{CloudStack}    & 22 & 26 & 94    & 0  &3 & 3    & 0 &0 & 0    & 1  &5 & 7     & 12 &9 &47 \\
        \textbf{ElasticSearch} & 1 & 1 & 5    & 0 & 0 & 1      & 0 &0  & 0   &0  &1  & 1       & 1  &0  &3\\
        \textbf{Hadoop}      & 12 & 6 & 43    & 0  &2 & 4     & 0 &0  & 0   &3  &3  & 9       & 14  &6  &22\\

       \textbf{Camel}       & 28 & 30 & 103    & 0 &1 & 1     & 0  &0 & 0    & 2 & 6 & 8      &2 &4  &7\\
       \textbf{Flink}       & 5 & 4 & 25    & 0 &0 & 0     & 0 &1 & 1    & 0 & 2 & 2       &0  & 3 &5\\
       \textbf{Kafka}       & 3 & 7 & 18    & 0  &0 & 0    & 0 &0 &0    & 0  &1  &2     & 3 & 2 &11 \\
       \textbf{Wicket}       & 1 & 1 & 3    & 0 &0 & 1       & 0 &0 & 0     & 1 &0  &1          & 0 &0  &0 \\

        \midrule

        \textbf{Total}       & 73 &78 & 298     & 0 &6 & 11    & 0 &0  &1     & 7 & 18 &30         &32 &24 &95\\


        % cass non-dup median 7, 0.146 neg
        % cloud stack non-dup median 8, 0.045 neg
        % es non-dup median 7, 0.27 small
        % hadoop non-dup median 6, 0.08 neg
        \bottomrule
    \end{tabular}}
    %}
    \vspace{-0.3cm}

    \label{table:manualrq5}
\end{table*}

\end{comment}
