%!TEX root = ms.tex
\section{Introduction}
\label{sect:intro}
% Detecting and categorizing semantic relationship in a given context is one of the most important tasks towards text understanding.
One of the most important tasks towards text understanding is to detect and categorize semantic relations between two entities in a given context.
For example, in Fig.~\ref{fig:Framework}, with regard to the sentence of $c_1$, relation between \emph{Jesse James} and \emph{Missouri} should be categorized as \texttt{died\_in}. 
% Accurate relation extraction systems can benefit various downstream applications.
With accurate identification, relation extraction systems can provide essential support for many applications.
% The extraction of entity relations from text is critical to understanding massive text corpora. 
% Identifying whether a relation of interest is expressed between a pair of entities mentioned in text, and label it with an appropriate type (\eg, \texttt{employed\_by}, \texttt{produced\_by}) based on some specific context (\eg, sentences) is a key to structuring content from text corpora for further analysis.
% For a specific question, relationship among entities can help to identify the answer, and deliver valuable information to question answering system~\cite{bao2014knowledge}.
One example is question answering, regarding a specific question, relation among entities can provide valuable information, which helps to seek better answers~\cite{bao2014knowledge}.
% For example, the relationship between a question and a clue often delivers valuable information for question answering~\cite{bao2014knowledge}; 
Similarly, for medical science literature, relations like protein-protein interactions~\cite{fundel2007relex} and gene disease associations~\cite{chun2006extraction} can be extracted and used in knowledge base population.
% Additionally, extracting relationships can provide essential support for many other applications like ontology construction~\cite{schutz2005relext}.
Additionally, relation extractors can be used in ontology construction~\cite{schutz2005relext}.


% Relation extraction can be viewed as a problem of classifying relation mentions into a set of relation types (e.g., Entity \textit{Hussein} and \textit{Amman} in the sentence ``\textit{Hussein} was born in \textit{Amman}" can form  relation mention \emph{$\langle$``\textit{Hussein}'', ``\textit{Amman}'',``\textit{Hussein} was born in \textit{Amman}"$\rangle$}, and should be classified as \texttt{born-in} relation). 

Typically, existing methods follow the supervised learning paradigm, and require extensive annotations from domain experts, which are costly and time-consuming.
To alleviate such drawback, attempts have been made to build relation extractors with a small set of seed instances or human-crafted patterns~\cite{nakashole2011scalable,carlson2010coupled}, 
% while iteratively generating more patterns and instances through bootstrap learning. 
based on which more patterns and instances will be iteratively generated by bootstrap learning.
However, these methods often suffer from semantic drift~\cite{mintz2009distant}.
% Existing methods typically follow the supervised learning paradigm, which requires extensive annotation by domain experts and tends to be domain-dependent as the same relation may have different meanings in different domains.
% To alleviate such domain/expert dependency, attempts have been made to build relation extractor with a small set of seed instances or human-crafted patterns~\cite{nakashole2012patty,agichtein2000snowball,nakashole2011scalable,carlson2010coupled}. 
% Based on these seeds, more patterns and instances are generated iteratively through bootstrap learning. 
% But the resulting patterns often suffer from low precision and semantic drift~\cite{mintz2009distant}.
Besides, knowledge bases like Freebase have been leveraged to automatically generate training data and provide \ds\cite{mintz2009distant}.
% Following this paradigm, partial-label association has been introduced to reduce noise of label~\cite{ren2016cotype}, and Markov logic has been employed to provide additional indirect supervision from a knowledge base~\cite{han2016global}. 
Nevertheless, for many domain-specific applications, distant supervision is either non-existent or insufficient (usually less than $25\%$ of relation mentions are covered~\cite{ren2015clustype,ling2012fine}).

% With the popularity of knowledge bases (e.g. Yago, DBPedia and Freebase), an alternative approach has been proposed~\cite{mintz2009distant}, which leverages knowledge bases to provide \ds.
% Following this paradigm, partial-label association has been introduced to reduce noise of label~\cite{ren2016cotype}, and Markov logic has been employed to provide additional indirect supervision from a knowledge base~\cite{han2016global}. 
% However, for many domain-specific applications, distant supervision is either non-existent or insufficient (less than $25\%$ of relation mentions can be covered~\cite{ren2015clustype,lin2012no}), despite of the availability of other useful information (\eg, seed instances and patterns).

Only recently have preliminary studies been developed to unite different supervisions, including knowledge bases and domain specific patterns, which are referred as \hs.
As shown in Fig. 1, these supervisions often conflict with each other~\cite{ratner2016data}.
To address these conflicts, data programming~\cite{ratner2016data} employs a generative model, which encodes supervisions as \lf, and adopts the \sca: \emph{a source is likely to provide true information with the same probability for all instances}.
This assumption is widely used in true label discovery literature~\cite{Li:2016:STD:2897350.2897352} to model reliabilities of information sources like crowdsourcing and infer the true label from noisy labels. Accordingly, most true label discovery methods would trust a human annotator on all instances to the same level.

However, labeling functions, unlike human annotators, do not make casual mistakes but follow certain ``error routine''. 
Thus, the reliability of a labeling function is not consistent among different pieces of instances.
% However, labeling functions, unlike human, do not make casual mistakes but follow certain ``error routine". 
In particular, a labeling function could be more reliable for a certain subset~\cite{varma2016socratic} (also known as its \emph{proficient subset}) comparing to the rest. 
We identify these proficient subsets based on context information, only trust labeling functions on these subsets and avoid assuming global source consistency.
% Only recently have preliminary studies been developed to utilize \hs, a series of weak supervisions based on different sources of information, such as knowledge base and domain specific patterns~\cite{ratner2016data,varmasocratic,dimitrievlarge}. 
% Due to the noisy nature, weak supervisions often conflict with each other. 
% Table~\ref{tab:lf_stats} summarizes the statistics of conflicts among weak supervisions in three real-world datasets, which shows that the proportion of conflicted mentions tends to increase substantially as the number of relation types increases.
% Thus, the key challenge here is how to resolve these conflicts. 
% Data programming~\cite{ratner2016data} was first proposed to provide heterogeneous supervision for binary classification, which has been applied to extraction of protein-protein interactions~\cite{dimitrievlarge}.
% It introduces \lf as a general expression of weak supervision and describes the noise of labeling functions in a generative way. 
% Data programming is based on a \emph{\sca}: \emph{a source is likely to provide true information with the same probability for all instances}. 
% This assumption is widely held in crowdsourcing~\cite{Li:2016:STD:2897350.2897352}.
% However, labeling functions, unlike human, do not make casual mistakes but follow certain underlying ``error routine". 
% Therefore, a labeling function could be more accurate for a certain subset (referred as its \emph{proficient subset}) comparing to the rest.

% \begin{table}[t]
% \caption{\small Number of relation mentions (Total), relation mentions without \texttt{None} (without \texttt{None})and relation mentions with conflicted labels (with conflicted labels)}
% \label{tab:data_conflicts}
% % \vspace{-1.1ex}
% \centering
% \begin{tabular}{l|c|c}
% \hline
% \textbf{Dataset} & \textbf{Wiki-KBP} & \textbf{NYT} \\
% \hline
% Total & 225977 & 106684\\
% \hline
% without & 7.70\% & \\
% \hline
% \end{tabular}\\[-0.1ex]
% \vspace{-0.7cm}
% \end{table}

Meanwhile, embedding methods have demonstrated great potential in capturing semantic meanings, which also reduce the dimension of overwhelming text features.
% Due to the nice properties of embedding methods in capturing semantic meaning, we adopt a representation learning approach to learn context representations for relation extraction and true label discovery.
 % which also reduces the dimension of overwhelming text features. 
% We design a embedding framework, \our, to project the context into a low-dimensional vector space for relation extraction and true label discovery.
% Also, embedding learning closes the vocabulary gap and reduces the number of text features.
Here, we present \our, a novel framework capturing context's semantic meaning through representation learning, and conduct both relation extraction and true label discovery in a context-aware manner.
% Here, we present \our, an embedding framework conducting both tasks in a context-aware manner, while avoiding the \sca.
Specifically, as depicted in Fig.~\ref{fig:Framework}, we embed relation mentions in a low-dimension vector space, where similar relation mentions tend to have similar relation types and annotations. 
% Accordingly, for relation extraction, we suppose the semantic meaning of embedding vectors corresponds to their relation type; and for true label discovery, we suppose the reliability of labeling function is related to its representation.
% The true labels would be inferred based on reliability  
`True' labels are further inferred based on reliabilities of labeling functions, which are calculated with their proficient subsets' representations.
Then, these inferred true labels would serve as supervision for all components, including context representation, true label discovery and relation extraction.
% During the model learning, the context representation serves as bridge for relation extraction and true label discovery to enhance each other, while both components could enrich the context representation.
Besides, the context representation bridges relation extraction with true label discovery, and allows them to enhance each other.
 % in an iterative fashion.

% Unlike the approaches discussed before, in this paper, we adopt a representation learning approach to conduct relation extraction and truth discovery. 
% Representation learning allows us to represent the proficient subset of a labeling function in the same semantic space as the relation mention, and resolve conflicts based on this representation. 
% By doing so, we can leverage the context information to infer the true label and avoid the assumption of source consistency. 
% Specifically, as depicted in Fig.~\ref{fig:Framework}, for each relation mention, we treat truth discovery as matching a proficient subset to its context, while relation extraction is viewed as matching a relation type to the same context.
% Both matching processes rely on the quality of context representation. 
% We utilize embedding methods to get this representation, which achieves great success in capturing semantic meaning and can scale up to large corpus.

% In recent works, 
% Representation learning and embedding methods have been applied recently to a range of NLP tasks, including Relation Detection and Relation Classification~\cite{zeng2014relation,takase2016composing,nguyen2015combining}. 
% These methods use distributed representation to reduce the dimension of overwhelming text features, significantly relieving the curse of dimensionality, and also demonstrate good properties in capturing semantic meaning.

% In this study, an embedding framework, \our, is proposed, which jointly learns the representations of text features, relation types and supervisions. 
% \our consists of a context-aware truth discovery model and a relation extraction model based on learned representations. 
% The proficient subset of each labeling function is represented  as an embedding vector, based on which the probability of an instance belonging to the subset is calculated. 
% At the same time, relation types are also embedded in the same space, which can be used to calculate the predictive probability of relation types given a relation mention. 
% Furthermore, context representation is learned as another component of these probabilities and further enriched by the co-occurrence information of text features. 
% This leads to high quality representations of context, which serve as the bridge between relation extraction and true label inference, and enrich their representation with feature correlation.

To the best of our knowledge, the framework proposed here is the first method that utilizes representation learning to provide heterogeneous supervision for relation extraction.
% The high-quality context representations serve as the backbone of these two models. 
The high-quality context representations serve as the backbone of true label discovery and relation extraction. 
Extensive experiments on benchmark datasets demonstrate significant improvements over the state-of-the-art.

The remaining of this paper is organized as follows. 
Section~\ref{sect:prelim} gives the definition of relation extraction with heterogeneous supervision. 
We then present the \our model and the learning algorithm in Section~\ref{sect:method}, and report our experimental evaluation in Section~\ref{sect:exp}.
Finally, we briefly survey related work in Section~\ref{sect:related} and conclude this study in Section~\ref{sect:conclusion}. 