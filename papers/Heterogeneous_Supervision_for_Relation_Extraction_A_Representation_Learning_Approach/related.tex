%!TEX root = ms.tex
\section{Related Work}
\label{sect:related}

% There exists four aspects of related works: relation extraction, heterogeneous supervision, truth discovery, and distributed representation learning. 
% In section~\ref{sect:intro}, we have briefly discussed data programming and existing approaches of relation extraction. 
% In this section, we review related principles of truth discovery, distributed representation learning and Socratic Learning, another framework based on heterogeneous supervision.

% \smallskip
% \noindent
% \textsf{\textbf{\small Relation Extraction. }}
\subsection{Relation Extraction}
Relation extraction aims to detect and categorize semantic relations between a pair of entities. To alleviate the dependency of annotations given by human experts, weak supervision~\cite{bunescu2007learning,etzioni2004web} and distant supervision~\cite{ren2016cotype} have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances). 
Universal Schemas~\cite{riedel2013relation,verga2015multilingual,toutanova2015representing} has been proposed to unify patterns and knowledge base, but it's designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus. 
Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type. 
Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor potential to detect more types of relations in a more precise way.

Word embedding has demonstrated great potential in capturing semantic meaning~\cite{mikolov2013distributed}, and achieved great success in a wide range of NLP tasks like relation extraction~\cite{zeng2014relation,takase2016composing,nguyen2015combining}.
In our model, we employed the embedding techniques to represent context information, and reduce the dimension of text features, which allows our model to generalize better.


\subsection{Truth Label Discovery}
% \smallskip
% \noindent
% \textsf{\textbf{\small True Label Discovery. }}
True label discovery methods have been developed to resolve conflicts among multi-source information under the assumption of source consistency \cite{Li:2016:STD:2897350.2897352,zhi2015modeling}. Specifically, in the \emph{spammer-hammer} model \cite{karger2011iterative}, each source could either be a spammer, which annotates instances randomly; or a hammer, which annotates instances precisely. In this paper, we assume each labeling function would be a hammer on its proficient subset, and would be a spammer otherwise, while the proficient subsets are identified in the embedding space. 

Besides data programming, socratic learning \cite{varma2016socratic} has been developed to conduct binary classification under heterogeneous supervision. Its true label discovery module supervises the discriminative module in label level, while the discriminative module influences the true label discovery module by selecting a feature subset. Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier. Thus, its discriminative module might suffer from the overwhelming size of text features. 