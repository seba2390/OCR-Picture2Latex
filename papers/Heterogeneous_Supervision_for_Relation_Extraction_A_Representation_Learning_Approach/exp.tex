%!TEX root = ms.tex

\section{Experiments}
\label{sect:exp}

In this section, we empirically validate our method by comparing to the state-of-the-art relation extraction methods on news and Wikipedia articles.

% \subsection{Data Preparation}

\subsection{Datasets and settings}
In the experiments, we conduct investigations on two benchmark datasets from different domains:\footnote{ Codes and datasets used in this paper can be downloaded at: \url{https://github.com/LiyuanLucasLiu/ReHession}.}

\noindent
\textbf{\small NYT} \cite{riedel2010modeling} is a news corpus sampled from $\sim$ 294k 1989-2007 New York Times news articles. It consists of 1.18M sentences, while 395 of them are annotated by authors of \cite{hoffmann2011knowledge} and used as test data;

\noindent
\textbf{\small Wiki-KBP} utilizes 1.5M sentences sampled from 780k Wikipedia articles \cite{ling2012fine} as training corpus, while test set consists of the 2k sentences manually annotated in 2013 KBP slot filling assessment results \cite{ellis2012linguistic}.

For both datasets, the training and test sets partitions are maintained in our experiments. 
Furthermore, we create validation sets by randomly sampling $10\%$ mentions from each test set and used the remaining part as evaluation sets. 

% \textbf{BioInfer} \cite{pyysalo2007bioinfer} includes 1,530 manually annotated biomedical paper abstracts as test data and 100K sampled PubMed paper abstracts as training corpus.
\smallskip
\noindent
\textsf{\small\textbf{Feature Generation. }} 
As summarized in Table~\ref{table:features}, we use a 6-word window to extract context features for each entity mention, 
% utilized LinePipe~\cite{LingPipe} to detect entities for BioInfer, 
apply the Stanford CoreNLP tool \cite{manning2014stanford} to generate entity mentions and get POS tags for both datasets. Brown clusters\cite{brown1992class} are derived for each corpus using public implementation\footnote{\url{https://github.com/percyliang/brown-cluster}}. All these features are shared with all compared methods in our experiments.

% \begin{table*}[]
% \caption{Number of labeling functions, annotated instances and types w.r.t. two kinds of information}
% \label{tab:lf_stats}
% \centering
% % \begin{small}
% \begin{tabular}{r||c|c|c||c|c|c||c|c|c}
% \hline
% \multirow{2}{*}{Category}& \multicolumn{3}{c||}{Wiki-KBP} & \multicolumn{3}{c||}{NYT} & \multicolumn{3}{c}{BioInfer} \\
% \cline{2-10}
%  & \#Type & \#Ins & \#LF & \#Type & \#Ins & \#LF & \#Type & \#Ins & \#LF\\
% \hline
% Pattern& 12 & 94293 & 146 & 16 & 43820 & 115 & 0 & 0 & 0\\
% \hline
% KB& 7 & 133955 & 7 & 25 & 494794 & 25 & 0 & 0 & 0\\
% \hline
% \end{tabular}
% % \end{small}
% \vspace{-0.5cm}
% \end{table*}


\begin{table}[t]
\centering
\begin{footnotesize}
\begin{tabular}{r||c|c||c|c}
\hline
\multirow{2}{*}{Kind}& \multicolumn{2}{c||}{Wiki-KBP} & \multicolumn{2}{c}{NYT} \\
\cline{2-5}
 & \#Types & \#LF & \#Types & \#LF\\
\hline
Pattern& 13 & 147 & 16 & 115\\
\hline
KB& 7 & 7 & 25 & 26\\
\hline
\end{tabular}
\end{footnotesize}
% \vspace{-0.3cm}
\caption{\small Number of labeling functions and the relation types they can annotated w.r.t. two kinds of information}
\label{tab:lf_stats}
% \vspace{-0.5cm}
\end{table}


% \begin{table}[]
% \centering
% \begin{small}
% \begin{tabular}{r||c|c|c||c|c|c}
% \hline
% \multirow{2}{*}{Kind}& \multicolumn{3}{c||}{Wiki-KBP} & \multicolumn{3}{c}{NYT} \\
% \cline{2-7}
%  & \#TP & \#Ins & \#LF & \#TP & \#Ins & \#LF\\
% \hline
% Pat& 12 & 94293 & 146 & 16 & 43820 & 115\\
% \hline
% KB& 7 & 133955 & 7 & 25 & 494794 & 25\\
% \hline
% \end{tabular}
% \end{small}
% \vspace{-0.3cm}
% \caption{\small Number of labeling functions, annotated instances and types w.r.t. two kinds of information (Pattern and KB)}
% \label{tab:lf_stats}
% \vspace{-0.2cm}
% \end{table}

\smallskip
\noindent
\textsf{\small\textbf{Labeling Functions. }} 
% \subsection{Labeling Functions} \label{subsubsec:lf} 
In our experiments, labeling functions are employed to encode two kinds of supervision information. 
One is knowledge base, the other is handcrafted domain-specific patterns. 
For domain-specific patterns, we manually design a number of labeling functions\footnote{pattern-based labeling functions can be accessed at: \url{https://github.com/LiyuanLucasLiu/ReHession}};
for knowledge base, annotations are generated following the procedure in~\cite{ren2016cotype,riedel2010modeling}.

Regarding two kinds of supervision information, the statistics of the labeling functions are summarized in Table~\ref{tab:lf_stats}.
We can observe that heuristic patterns can identify more relation types for KBP datasets, while for NYT datasets, knowledge base can provide supervision for more relation types.
This observation aligns with our intuition that single kind of information might be insufficient while different kinds of information can complement each other.
% NYT has been heuristically labeled by knowledge base following the procedure in~\cite{riedel2010modeling}, while KBP has been heuristically labeled by knowledge base following the procedure in~\cite{ren2016cotype}. These annotations are transformed to heterogeneous supervision manner, by treating annotations of the same relation type as annotations generated by a labeling function of first kind. Besides, we manually designed a number of labeling functions\footnote{labeling functions can be accessed at: \url{https://github.com/shanzhenren/CoType}}. The statistics of these two kinds of labeling functions are summarized in Table~\ref{tab:lf_stats}. We find that although knowledge base can annotates more relation mentions than domain specific patterns, patterns can annotates more kinds of relation types. By combining knowledge base and domain specific patterns, we can have more annotated relation mentions on more relation types. This observation also verifies our intuition that single kind of information might be insufficient while different kinds of information can enhance each other.

We further summarize the statistics of annotations in Table~\ref{tab:data_conflicts}.
It can be observed that a large portion of instances is only annotated as \texttt{None}, while lots of conflicts exist among other instances. This phenomenon justifies the motivation to employ true label discovery model to resolve the conflicts among supervision. Also, we can observe most conflicts involve \texttt{None} type, accordingly, our proposed method should have more advantages over traditional true label discovery methods on the relation extraction task comparing to the relation classification task that excludes \texttt{None} type.

\input{relationExtraction}

% \begin{small}
\begin{table}[t]
% \vspace{-1.1ex}
\centering
\begin{small}
\begin{tabular}{l|c|c}
\hline
\textbf{Dataset} & \textbf{Wiki-KBP} & \textbf{NYT} \\
\hline
Total Number of RM& 225977 & 530767\\
\hline
RM annotated as \texttt{None} & 100521  & 356497\\
\hline
RM with conflicts & 32008 & 58198\\
\hline
Conflicts involving \texttt{None} & 30559 & 38756 \\
\hline
\end{tabular}
\end{small}
% \vspace{-0.2cm}
\caption{\small Number of relation mentions (RM), relation mentions annotated as \texttt{None}, relation mentions with conflicting annotations and conflicts involving \texttt{None}}
\label{tab:data_conflicts}
% \vspace{-0.5cm}
\end{table}
% \end{small}

\subsection{Compared Methods} 
We compare \our with below methods: 

\noindent
\textbf{\small FIGER}~\cite{ling2012fine} adopts multi-label learning with Perceptron algorithm.
%a classical multi-class classification method.

\noindent
\textbf{\small BFK}~\cite{bunescu2005subsequence} applies bag-of-feature kernel to train a support vector machine;

\noindent
\textbf{\small DSL}~\cite{mintz2009distant} trains a multi-class logistic classifier\footnote{We use liblinear package from \url{https//github.com/cjlin1/liblinear}} on the training data;

\noindent
\textbf{\small MultiR}~\cite{hoffmann2011knowledge} models training label noise by multi-instance multi-label learning;

\noindent
\textbf{\small FCM}~\cite{gormley2015improved} performs compositional embedding by neural language model.

\noindent
\textbf{\small CoType-RM}~\cite{ren2016cotype} adopts partial-label loss to handle label noise and train the extractor.

Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods. The first is to keep all noisy labels, marked as `NL'. 
Alternatively, a true label discovery method, Investment~\cite{pasternack2010knowing}, is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions' reliabilities. 
Then, the second strategy is to only feed the inferred true labels, referred as `TD'.

Universal Schemas~\cite{riedel2013relation} is proposed to unify different information by calculating a low-rank approximation of the annotations $\O$. 
It can serve as an alternative of the Investment method, i.e., selecting the relation type with highest score in the low-rank approximation as the true type. 
But it doesnâ€™t explicitly model noise and not fit our scenario very well. 
Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec.~\ref{subsec:case}. 
Indeed, it performs similarly to the Investment method. 

\smallskip
\noindent
\textsf{\small\textbf{Evaluation Metrics. }} 
For relation classification task, which excludes \texttt{None} type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score~\cite{bunescu2005subsequence,bach2007review} are employed.
% , which are calculated as:
% \begin{align*}
% &\textbf{Prec} = \frac{\sum_{d \in \D_u} \delta(\ob_d^* = \hat{\ob}_d) \cdot \delta(\ob_d^* \neq \texttt{None})}{\sum_{d \in \D_u} \delta(\ob_d^* \neq \texttt{None})}
% \\
% % \end{align*}
% % \begin{align*}
% &\textbf{Rec} = \frac{\sum_{d \in \D_u} \delta(\ob_d^* = \hat{\ob}_d) \cdot \delta(\ob_d^* \neq \texttt{None})}{\sum_{d \in \D_u} \delta(\hat{\ob}_d \neq \texttt{None})} \\
% &\textbf{F1} = \frac{2 \cdot \textbf{Prec} \cdot \textbf{Rec}}{\textbf{Prec}+\textbf{Rec}}
% \end{align*}
% where $\ob_d^*$ is the predicted relation type for $d \in \D_u$ and $\hat{\ob}_d$ is the ground truth relation type. 
Notice that both relation extraction and relation classification are conducted and evaluated in \textit{sentence-level}~\cite{bao2014knowledge}.

\smallskip
\noindent
\textsf{\small\textbf{Parameter Settings. }} 
Based on the semantic meaning of proficient subset, we set $\phi_2$ to {\small $1 / |\R \cup \{\texttt{None}\}|$}, i.e., the probability of generating right label with random guess. 
Then we set $\phi_1$ to $1 - \phi_2$, $\lambda_1 = \lambda_2 = 1$, and the learning rate $\alpha = 0.025$. 
As for other parameters, they are tuned on the validation sets for each dataset. 
Similarly, all parameters of compared methods are tuned on validation set, and the parameters achieving highest F1 score are chosen for relation extraction. 


\begin{table}
% \vspace{-1.1ex}
\centering
\begin{scriptsize}
\begin{tabular}{p{3.3 cm}|c|c}
\hline
\multirow{2}{*}{\textbf{Relation Mention}} & \multirow{2}{*}{\textbf{\our}} & \textbf{Investment \&} \\
& & \textbf{Universal Schemas}\\
\hline
\hline
\textit{Ann Demeulemeester} ( \textbf{born} 1959 , Waregem , \textit{Belgium} ) is ... & \texttt{born-in} & \texttt{None} \\
\hline
\textit{Raila Odinga} was \textbf{born} at ..., in \textit{Maseno}, Kisumu District, ... & \texttt{born-in} & \texttt{None}\\
\hline
\hline
\textit{Ann Demeulemeester} ( \textbf{elected} 1959 , Waregem , \textit{Belgium} ) is ... & \texttt{None} & \texttt{None} \\
\hline
\textit{Raila Odinga} was \textbf{examined} at ..., in \textit{Maseno}, Kisumu District, ... & \texttt{None} & \texttt{None} \\
\hline
\end{tabular}
\end{scriptsize}
% \vspace{-0.4cm}
\caption{\small Example output of true label discovery. The first two relation mentions come from Wiki-KBP, and their annotations are \{\texttt{born-in}, \texttt{None}\}. The last two are created by replacing key words of the first two. Key words are marked as bold and entity mentions are marked as Italics.}
\label{tab:case_study}
% \vspace{-0.5cm}
\end{table}


\subsection{Performance Comparison}

% \input{relationClassification}
Given the experimental setup described above, the averaged evaluation scores in 10 runs of relation classification and relation extraction on two datasets are summarized in Table~\ref{table:relation_extraction}.
% We can observe that our method outperforms all baselines on both relation extraction task and relation classification task. 

From the comparison, it shows that NL strategy yields better performance than TD strategy, since the true labels inferred by Investment are actually wrong for many instances. 
On the other hand, as discussed in Sec.~\ref{subsec:case}, our method introduces context-awareness to true label discovery, while the inferred true label guides the relation extractor achieving the best performance. 
This observation justifies the motivation of avoiding the source consistency assumption and the effectiveness of proposed true label discovery model.

One could also observe the difference between \our and the compared methods is more significant on the NYT dataset than on the Wiki-KBP dataset. This observation accords with the fact that the NYT dataset contains more conflicts than KBP dataset (see Table~\ref{tab:data_conflicts}), and the intuition is that our method would have more advantages on more conflicting labels.

% Among four tasks, the relation classification of Wiki-KBP dataset has highest label quality (least conflicted label ratio), but with least number of  training instances. 
% CoType-RM performs well on this task while other methods achieve relative low accuracy. 
% This may because the representation learning gives better generalization ability of CoType-RM. 
% On the contrary, other methods like DSL perform better than CoType-RM on other tasks. 
% This is likely due to CoType-RM's lack of true label discovery component, which makes it vulnerable to noisy labels.
% And our method employs embedding techniques to get better generalization ability, and integrates context-aware true label discovery to infer true label from noisy labels, thus achieves best performance on all tasks.

Among four tasks, the relation classification of Wiki-KBP dataset has highest label quality, i.e. conflicting label ratio, but with least number of training instances.
And CoType-RM and DSL reach relatively better performance among all compared methods.
CoType-RM performs much better than DSL on Wiki-KBP relation classification task, while DSL gets better or similar performance with CoType-RM on other tasks. 
This may be because the representation learning method is able to generalize better, thus performs better when the training set size is small. 
However, it is rather vulnerable to the noisy labels compared to DSL. 
Our method employs embedding techniques, and also integrates context-aware true label discovery to de-noise labels, making the embedding method rather robust, thus achieves the best performance on all tasks.

% \subsection{Case Study}

% \begin{table}[t]
% \begin{small}
% \caption{On the effect of two kinds of labeling functions(Accuracy is reported)}
% \label{tab:twoKLF}
% \centering
% \begin{tabular}{r||c|c|c}
% \hline
% \textbf{Category} & \textbf{KBP} & \textbf{NYT} & \textbf{BioInfer} \\
% \hline
% KB & 0.0000 & &\\
% \hline
% Pattern & 0.0000 & & \\
% \hline
% KB \& Pattern & 0.0000 & & \\
% \hline
% \end{tabular}
% \end{small}
% % \vspace{-0.7cm}
% \end{table}

% \subsubsection{Effect of Using Two Kinds of Labeling Function}

% As introduced in Section~\ref{subsubsec:lf}, we employ two kinds of labeling functions as heterogeneous supervision in our experiments. Now, we verify our intuition about heterogeneous supervision by comparing the effect of these two kinds of labeling functions. First, we extract labeling functions of the first kind, which are based on knowledge base, and use them as heterogeneous supervision to conduct relation classification. Similarly, labeling functions of the second kind, which are based on handcrafted patterns, are grouped together and used to perform relation classification. The results are summarized in Table~\ref{tab:twoKLF}. 
% %some discussion

\subsection{Case Study}
\label{subsec:case}

% \smallskip
% \noindent
% \textsf{\small\textbf{Relation Extraction. }} 


\smallskip
\noindent
\textsf{\small\textbf{Context Awareness of True Label Discovery. }} 

Although Universal Schemas does not adopted the source consistency assumption, but it's conducted in document-level, and is context-agnostic in our sentence-level setting. Similarly, most true label discovery methods adopt the source consistency assumption, which means if they trust a labeling function, they would trust it on all annotations. And our method infers true labels in a context-aware manner, which means we only trust labeling functions on matched contexts.

For example, Investment and Universal Schemas refer \texttt{None} as true type for all four instances in Table~\ref{tab:case_study}. And our method infers \texttt{born-in} as the true label for the first two relation mentions; after replacing the matched contexts (\textbf{born}) with other words (\textbf{elected} and \textbf{examined}), our method no longer trusts \texttt{born-in} since the modified contexts are no longer matched, then infers \texttt{None} as the true label. In other words, our proposed method infer the true label in a context aware manner.

\smallskip
\noindent
\textsf{\small\textbf{Effectiveness of True Label Discovery. }} 
We explore the effectiveness of the proposed context-aware true label discovery component by comparing \our to its variants \ourtd and \ourus, which uses Investment or Universal Schemas to resolve conflicts. The averaged evaluation scores are summarized in Table~\ref{tab:context-awareTD}. 
We can observe that \our significantly outperforms its variants. 
Since the only difference between \our and its variants is the model employed to resolve conflicts, this gap verifies the effectiveness of the proposed context-aware true label discovery method.


\begin{table}[t]
% \vspace{-1.1ex}
\centering
\begin{small}
\begin{tabular}{l|l|c|c|c|c}
\hline
\multicolumn{2}{c|}{\textbf{\scriptsize Dataset \& Method}} &\textbf{Prec} &\textbf{Rec} & \textbf{F1} & \textbf{Acc} \\
\hline
\hline
\multirow{3}{*}{\textbf{\scriptsize Wiki-KBP}} 
& Ori & \textbf{0.3677} & 0.4933 & \textbf{0.4208} & \textbf{0.7277} \\
\cline{2-6}
& TD & 0.3032 & \textbf{0.5279} & 0.3850 & 0.7271\\
\cline{2-6}
& US & 0.3380 & 0.4779 & 0.3960 & 0.7268\\
\hline
\multirow{3}{*}{\textbf{\scriptsize NYT}} & Ori & \textbf{0.4122} & \textbf{0.5726} & \textbf{0.4792} & \textbf{0.8381} \\
\cline{2-6}
& TD & 0.3758 & 0.4887 & 0.4239 & 0.7387 \\
\cline{2-6}
& US & 0.3573 & 0.5145 & 0.4223 & 0.7362\\
\hline
\end{tabular}
\end{small}
% \vspace{-0.4cm}
\caption{\small Comparison among \our (Ori), \ourus(US) and \ourtd (TD) on relation extraction and relation classification}
\label{tab:context-awareTD}
% \vspace{-0.2cm}
\end{table}

% \subsection{Case Study}

% \subsubsection{Learned Representations}

% \subsubsection{Label Distributions on Training Set and Test Set}
% \label{subsubsec:label_distribution}

% \begin{figure}[]
%   \centering
%   \centerline{
%     \includegraphics[width=\columnwidth]{Dist_diff}
%   }
%   \caption{$\R$ on KBP}
%   \label{fig:label_dist}
% % \vspace{-0.3cm}
% \end{figure}

% As mentioned in Section~\ref{subsubsec:type_infer}, test sets' label distributions are different from training sets'. Specifically, table~\ref{tab:twoKLF} shows proportions of \texttt{None} in training set and test set. We can observe obvious difference between training sets' ratio with testing sets'.

% On the other hand, we calculate $\frac{n_i}{\sum_{r_j \in \mathcal{R}} n_j}$ for all relation types $r_i \in \mathcal{R}$, where $n_i$ is the number of relation mentions annotated as $r_i$. This value demonstrates the composition of labels in some dataset. And we use these values to investigate the difference between training set and test set. In Fig.~\ref{fig:label_dist}, we visualize this difference for Wiki-KBP, the `simplest' dataset which has least relation types. Obvious difference can be observe between the values for training set and those for test set. Benefit from the high-quality representation learned by our framework, our method can learn the conditional probability precisely, and doesn't suffer a lot from this shift.