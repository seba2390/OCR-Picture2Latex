%!TEX root = ms.tex

%d \in \D
% s

\section{The \our Framework}
\label{sect:method}

\input{notation}

Here, we present \our, a novel framework to infer true labels from automatically generated noisy labels, and categorize unlabeled instances into a set of relation types.
Intuitively, errors of annotations ($\O$) come from  mismatch of contexts, e.g., in Fig.~\ref{fig:Framework}, $\lambda_1$ annotates $c_1$ and $c_2$ with 'true' labels but for mismatched contexts `killing' and 'killed'. 
Accordingly, we should only trust labeling functions on \emph{matched} context, e.g., trust $\lambda_1$ on $c_3$ due to its context `was born in', but not on $c_1$ and $c_2$. 
On the other hand, relation extraction can be viewed as \emph{matching} appropriate relation type to a certain context.
These two \emph{matching} processes are closely related and can enhance each other, while context representation plays an important role in both of them.
% Specifically, our model is based on the following intuitive properties:
% \begin{enumerate}\setlength{\itemsep}{0pt}
% \item \label{prop:1} Text features occurring in the same contexts tend to have similar meanings (also known as distributional hypothesis\cite{harris1954distributional});
% \item \label{prop:2} Relation mentions' semantic meaning can be derived from their text features;
% \item \label{prop:2.5} Annotations $\C$ are generated without randomness, their errors came from the mismatch from labeling functions to real relation types;
%  % instead of some random error in generation. 
% \item \label{prop:3} If $\lambda_i$ annotates a relation mention correctly / wrongly, $\lambda_i$ would tend to annotate other similar relation mentions correctly / wrongly;
% \item \label{prop:4} Relation type can be identified based on relation mentions' semantic meaning;
% \end{enumerate}

% We plot the overview of our framework in Fig.~\ref{fig:Framework}, while context representation generation is summarized in Fig.~\ref{fig:context_rep} and notions are summarized in Table~\ref{tab:notation}. After extracting the text features 
% We describe the \our framework as follows (also plotted in Fig.~\ref{fig:Framework}). 
\smallskip
\noindent
\textsf{\textbf{\small Framework Overview. }}
% As plotted in Fig.~\ref{fig:Framework}, we employ embedding method to represent context, bridge relation extraction with truth discovery, and allow them to enhance each other.
% After extracted from context, text features are projected to a low dimension space while retaining their semantic meanings.
% Moreover, we use these text feature embeddings to calculate relation mention embeddings (see Fig.~\ref{fig:context_rep}).
% With relation mention embedding, we would infer true labels by calculating the reliability of labeling functions in a context-aware manner.
% And the inferred true labels would further `supervise' all components and enrich the representation of text feature, relation mention, labeling function and relation type.
% We now proceed by introducing these components of the model in further detail. 
We propose a general framework to learn the relation extractor from automatically generated noisy labels. 
As plotted in Fig.~\ref{fig:Framework}, distributed representation of context bridges relation extraction with true label discovery, and allows them to enhance each other.
Specifically, it follows the steps below:
% \vspace{-0.05cm}
\begin{enumerate}[fullwidth,itemindent=0em,label=\arabic*.]\setlength{\itemsep}{-0.05cm}
\item
After being extracted from context, text features are embedded in a low dimension space by representation learning (see Fig.~\ref{fig:context_rep});

\item
Text feature embeddings are utilized to calculate relation mention embeddings (see Fig.~\ref{fig:context_rep});

\item
With relation mention embeddings, true labels are inferred by calculating labeling functions' reliabilities in a context-aware manner (see Fig.~\ref{fig:Framework});

\item
Inferred true labels would `supervise' all components to learn model parameters (see Fig.~\ref{fig:Framework}).
\end{enumerate}
% \vspace{-0.05cm}
We now proceed by introducing these components of the model in further details. 

% is described as follows.

% \begin{enumerate}[label=(\alph*)]{}
% \noindent
% \textbf{(a)} For a POS-tagger corpus $\D$, generate relation mentions $\C$ and text features (see also Fig.~\ref{fig:context_rep});
%  % Utilize POS-tagger and entity detector to identify relation mentions $\C$ from corpus $\D$ and generate text features (see also Fig.~\ref{fig:context_rep});

% \noindent
% \textbf{(b)} Apply labeling functions $\Lambda$, on $\C$. The resulting annotation is recorded as $\ob$, where $\ob_{d, i} = \lambda_i(d), \forall d \in \C_l, \lambda_i \in \Lambda$ (see also Fig.~\ref{fig:context_rep});

% \noindent
% \textbf{(c)} Learn relation extraction model under heterogeneous supervision (see also Fig.~\ref{fig:Framework}). The inferred true label is marked as $\ob_c^*, \forall d \in \C_l$;

% \noindent
% \textbf{(d)} Estimate the relation type $\ob_d^*$ for $\forall d \in \C_u$ through the relation extractor learnt before.
% \end{enumerate}

\input{feature_table}

\subsection{Modeling Relation Mention }
As shown in Table~\ref{table:features}, we extract abundant lexical features \cite{ren2016cotype,mintz2009distant} to characterize relation mentions. 
However, this abundance also results in the gigantic dimension of original text features ($\sim10^7$ in our case). 
In order to achieve better generalization ability, we represent relation mentions with low dimensional ($\sim10^2$) vectors. 
In Fig.~\ref{fig:context_rep}, for example, relation mention $c_3$ is first represented as bag-of-features. 
After learning text feature embeddings, we use the average of feature embedding vectors to derive the embedding vector for $c_3$.

% \smallskip
% \noindent
% \textsf{\textbf{Text Feature Extraction. }}
% % \subsection{Text Feature Extraction}
% Similar to \cite{ren2016cotype,mintz2009distant,chan2010exploiting}, we employed lexical features summarized in Table~\ref{table:features} to capture the syntax and semantics of relation mentions. We mark text features as $\F = \{f_1, \cdots, f_{|\F|}\}$, and record the feature set for $\forall c \in \C$ as $\f_c$.
% To capture the syntax and semantics of relation mentions, we generate several different kinds of lexical features (summarized in Table~\ref{table:features} and referred as $\F$), which are similar to those used in \cite{ren2016cotype,mintz2009distant,chan2010exploiting}. We record $d$'s feature as $\f_d, \forall d \in \D$.

\smallskip
\noindent
\textsf{\textbf{\small Text Feature Representation. }} 
% As shown in Table~\ref{table:features}, we extract abundant lexical features \cite{ren2016cotype,mintz2009distant,chan2010exploiting} to characterize relation mentions. 
% However, this abundance also results in the overwhelming dimension of original text features ($\sim10^7$ in our case). 
% In order to achieve better generalization ability, we embed these features in a low dimension space while retaining their semantic meaning. 
Similar to other principles of embedding learning, we assume text features occurring in the same contexts tend to have similar meanings (also known as distributional hypothesis\cite{harris1954distributional}). Furthermore, we let each text feature's embedding vector to predict other text features occurred in the same relation mentions or context. Thus, text features with similar meaning should have similar
 embedding vectors. Formally, we mark text features as $\F = \{f_1, \cdots, f_{|\F|}\}$, record the feature set for $\forall c \in \C$ as $\f_c$, and represent the embedding vector for $f_i$ as $\v_i \in \mathbb{R}^{n_v}$, and we aim to maximize the following log likelihood: {\small $\sum_{c \in \C_l} \sum_{f_i, f_j \in \f_c} \log \: p(f_i | f_j)$}, where {\small $p(f_i | f_j) = \exp(\v_i^T\v_j^*) / \sum_{f_k \in \F} \exp(\v_i^T \v_k^*)$}.

However, the optimization of this likelihood is impractical because the calculation of $\nabla p(f_i|f_j)$ requires summation over all text features, whose size exceeds $10^7$ in our case. 
In order to perform efficient optimization, we adopt the negative sampling technique \cite{mikolov2013distributed} to avoid this summation. 
Accordingly, we replace the log likelihood with Eq.~\ref{eqn:neg_sampling} as below:

\begin{small}
\begin{equation}
\mathcal{J}_E = \sum_{\substack{c \in \C_l \\ f_i,f_j\in \f_c}} (\log \: \sigma(\v_i^T \v_j^*) - \sum_{k=1}^V \E_{f_{k'} \sim \hat{P}} [\log \: \sigma(-\v_i^T \v_{k'}^*)]) \label{eqn:neg_sampling}
\end{equation}
\end{small}
where $\hat{P}$ is noise distribution used in~\cite{mikolov2013distributed}, $\sigma$ is the sigmoid function and $V$ is number of negative samples.
% \mathcal{J}_E = \sum_{\substack{d \in \C_l \\ f_i, f_j \in \f_d}} (log \: \sigma(\v_i^T \v_j^*) - \sum_{k=1}^V \E_{f_{k'} \sim P_n(f)}[log \: \sigma(-\v_i^T \v_{k'}^*)]) \label{eqn:neg_sampling}


\smallskip
\noindent
\textsf{\textbf{\small Relation Mention Representation.}} 
With text feature embeddings learned by Eq.~\ref{eqn:neg_sampling}, a naive method to represent relation mentions is to concatenate or average its text feature embeddings.
However, text features embedding may be in a different semantic space with relation types.
Thus, we directly learn a mapping $g$ from text feature representations to relation mention representations  \cite{van2016learning,van2016unsupervised} instead of simple heuristic rules like concatenate or average (see Fig.~\ref{fig:context_rep}):

\begin{small}
\begin{equation}
\z_c = g(\f_c)= \text{tanh}(W \cdot \frac{1}{|\f_c|} \sum_{f_i \in \f_c} \v_i) \label{eqn:activate}
\end{equation}
\end{small}
where  $\z_c$ is the representation of $c \in \C_l$, $W$ is a $n_z \times n_v$ matrix, $n_z$ is the dimension of relation mention embeddings and tanh is the element-wise hyperbolic tangent function.

In other words, we represent bag of text features with their average embedding,
then apply linear map and hyperbolic tangent to transform the embedding from text feature semantic space to relation mention semantic space.
The non-linear tanh function allows non-linear class boundaries in other components, and also regularize relation mention representation to range $[-1, 1]$ which avoids numerical instability issues.

\subsection{True Label Discovery }
% \vspace{-0.3cm}
\begin{figure}[t]
  \centering
  \centerline{
    \includegraphics[width=0.8\columnwidth]{graphical_model.pdf}
  }
% \vspace{-0.2cm}
  \caption{Graphical model of $o_{c, i}$'s correctness}
  \label{fig:graphicalModel}
% \vspace{-0.3cm}
\end{figure}

Because heterogeneous supervision generates labels in a discriminative way, we suppose its errors follow certain underlying principles, i.e., if a labeling function annotates a instance correctly / wrongly, it would annotate other similar instances correctly / wrongly. 
For example, $\lambda_1$ in Fig.~\ref{fig:Framework} generates wrong annotations for two similar instances $c_1$, $c_2$ and would make the same errors on other similar instances.
Since context representation captures the semantic meaning of relation mention and would be used to identify relation types, we also use it to identify the mismatch of context and labeling functions. 
Thus, we suppose for each labeling function $\lambda_i$, there exists an proficient subset $\S_i$ on $\R^{n_z}$, containing instances that $\lambda_i$ can precisely annotate. 
In Fig.~\ref{fig:Framework}, for instance, $c_3$ is in the proficient subset of $\lambda_1$, while $c_1$ and $c_2$ are not.
Moreover, the generation of annotations are not really random, and we propose a probabilistic model to describe the level of mismatch from labeling functions to real relation types instead of annotations' generation.
% , or the generation of $\rho_{c, i} = \delta(\ob_{c, i} = \ob_c^*)$, the correctness of annotations.

% Furthermore, annotations are precisely generated by labeling function based on additional information, different from crowdsourcing, the generation of these annotations are accurate, in a discriminative way, while the randomness exists in the correctness of annotations. 
% Furthermore, as stated in  the generation of annotations $\ob_{c, i}$ is based on additional information and do not have much randomness. Accordingly, we propose a probabilistic model to describe the level of mismatch from labeling function to real relation types, or the generation of $\rho_{c, i} = \delta(\ob_{c, i} = \ob_c^*)$, the correctness of annotations.
% Thus, we utilize a probabilistic model for the correctness of annotations $e_{c, i}$ instead of annotations $\ob_{c, i}$.

As shown in Fig.~\ref{fig:graphicalModel}, we assume the indicator of whether $c$ belongs to $\S_i$, $s_{c, i} = \delta(c \in \S_i)$, would first be generated based on context representation 
% \begin{small}
\begin{align}
p(s_{c, i} = 1 | \z_c, \l_i) = p(c \in \S_i) = \sigma(\z_c^T \l_i) \label{eqn:gene_d}
% &p(\ob_{c,i} = \ob_c^*) \nonumber\\
% =& \sum_{s_{c, i} \in \{ 0, 1\}} p(\ob_{c,i} = \ob_c^* , s_{c, i}) \nonumber\\
% =& \sum_{s_{c, i} \in \{ 0, 1\}} p(\ob_{c,i} = \ob_c^* | s_{c, i}) * p(s_{c, i}) \nonumber\\
% =& p(\ob_{c,i} = \ob_c^* | d \in S_i) * p(d \in S_i) + p(\ob_{c,i} = \ob_c^* | d \not\in S_i) * p(d \not\in S_i) \nonumber \\
% =& \phi_1 * p(s_{c, i} = 1) + \phi_0 s* p(s_{c, i} = 0) \label{eqn:gene_r}
 % p(e_{c, i} | s_{c, i}) = p(\ob_{c,i} = \ob_c^* | s_{c, i})^{\delta(\ob_{d, i} = \ob_c^* )} \nonumber\\
 % \cdot ( 1-  p(\ob_{c,i} = \ob_c^* | s_{c, i}))^{\delta(\ob_{c, i} \neq \ob_c^* )}
\end{align}
% \end{small}

Then the correctness of annotation $\ob_{c, i}$, $\rho_{c, i} = \delta(\ob_{c, i} = \ob_c^*)$, would be generated. Furthermore, we assume $p(\rho_{c, i} = 1 | s_{c, i} = 1) = \phi_1$ and $p(\rho_{c, i} = 1 | s_{c, i} = 0) = \phi_0$ to be constant for all relation mentions and labeling functions.

Because $s_{c, i}$ would not be used in other components of our framework, we integrate out $s_{c, i}$ and write the log likelihood as

% \begin{align}
% \mathcal{J}_T =&\sum_{\ob_{c, i} \in \O} log(\sigma(\z_c^T \l_i)\phi_1^{e_{c, i}}(1-\phi_1)^{1 - e_{c, i}}\nonumber\\
% &+(1-\sigma(\z_c^T \l_i))\phi_0^{e_{c, i}}(1-\phi_0)^{1 - e_{c, i}}) \label{eqn:truth_ciscovery}
% \end{align}
\begin{small}
\begin{align}
&\mathcal{J}_T =\sum_{\ob_{c, i} \in \O} \log(\sigma(\z_c^T \l_i)\phi_1^{ \delta(\ob_{c, i} = \ob_c^*)}(1-\phi_1)^{\delta(\ob_{c, i} \neq \ob_c^*)}\nonumber\\
&+(1-\sigma(\z_c^T \l_i))\phi_0^{\delta(\ob_{c, i} = \ob_c^*)}(1-\phi_0)^{\delta(\ob_{c, i} \neq \ob_c^*)}) \label{eqn:truth_ciscovery}
\end{align}
\end{small}

Note that $\ob_c^*$ is a hidden variable but not a model parameter, and $\mathcal{J}_T$ is the likelihood of $\rho_{c, i} = \delta(\ob_{c, i} = \ob_c^*)$. 
Thus, we would first infer $\ob_c^* = \argmax_{\ob_c^*} \mathcal{J}_T$, then train the true label discovery model by maximizing $\mathcal{J}_T$.


\begin{table}[t]
% \begin{small}
\centering
\begin{tabular}{r|c|c}
\hline
\textbf{Datasets} & \textbf{NYT} & \textbf{Wiki-KBP} \\
\hline
\hline
\% of \texttt{None} in Training & 0.6717 & 0.5552\\
\hline
\% of \texttt{None} in Test & 0.8972 & 0.8532\\
\hline
\end{tabular}
% \vspace{-0.3cm}
\caption{Proportion of \texttt{None} in Training/Test Set}
\label{tab:twoKLF}
% \vspace{-0.4cm}
% \end{small}
\end{table}

\subsection{Modeling Relation Type }
We now discuss the model for identifying relation types based on context representation. For each relation mention $c$, its representation $\z_c$ implies its relation type, and the distribution of relation type can be described by the soft-max function:

\begin{small}
\begin{equation}
p(r_i | \z_c) = \frac{\exp(\z_c^T\t_i)}{\sum_{r_j \in \R\cup\{\texttt{None}\}} \exp(\z_c^T \t_j)} \label{eqn:rel_softmax}
\end{equation}
\end{small}
where $\t_i \in \mathbb{R}^{v_z}$ is the representation for relation type $r_i$. Moreover, with the inferred true label $\ob_c^*$, the relation extraction model can be trained as a multi-class classifier. Specifically, we use Eq.~\ref{eqn:rel_softmax} to approach the distribution

\begin{small}
\begin{equation}
p(r_i |\ob_c^*) = \left\{
\begin{aligned}
1 && r_i = \ob_c^* \\
0 && r_i \neq \ob_c^*
\end{aligned}
\right.
\label{eqn:one_hot_dist}
\end{equation}
\end{small}

Moreover, we use KL-divergence to measure the dissimilarity between two distributions, and formulate model learning as maximizing $\mathcal{J}_R$:

\begin{small}
\begin{align}
\mathcal{J}_R &= - \sum_{c \in \C_l} KL(p(. | \z_c)|| p(. |\ob_c^*)) \label{eqn:kl-divergence}
% = \sum_{c \in \C_l} log(p(\ob_c^* | \z_c))
% \nonumber\\
% &=\sum_{c \in \C_l}(\z_c^T\t_{\ob_c^*} - log(\sum_{r_j \in \R}exp(\z_c^T \t_j))) \label{eqn:kl-divergence}
\end{align}
\end{small}
where {\small $KL(p(. | \z_c)|| p(. |\ob_c^*))$} is the KL-divergence from {\small  $p(r_i |\ob_c^*)$ } to {\small $p(r_i | \z_c)$}, {\small $p(r_i | \z_c)$} and {\small $p(r_i |\ob_c^*)$} has the form of Eq.~\ref{eqn:rel_softmax} and Eq.~\ref{eqn:one_hot_dist}.



\subsection{Model Learning}

Based on Eq.~\ref{eqn:neg_sampling}, Eq.~\ref{eqn:truth_ciscovery} and Eq.~\ref{eqn:kl-divergence}, we form the joint optimization problem for model parameters as

\begin{small}
\begin{align}
&\min_{W, \v, \v^*, \l, \t, \ob^*} \mathcal{J} = -\mathcal{J}_R - \lambda_1 \mathcal{J}_E - \lambda_2 \mathcal{J}_T \nonumber\\
&\text{s.t. } \forall c \in \C_l, \ob_c^* = \argmax_{\ob_c^*} \mathcal{J}_T, \z_c = g(\f_c) \label{eqn:joint}
\end{align}
\end{small}
Collectively optimizing Eq.~\ref{eqn:joint} allows heterogeneous supervision guiding all three components, while these components would refine the context representation, and enhance each other.

% \subsection{Model Learning and Relation Type Inference}

% \subsection{Model Learning}
In order to solve the joint optimization problem in Eq.~\ref{eqn:joint} efficiently, we adopt the stochastic gradient descent algorithm to update $\{W, \v, \v^*, \l, \t\}$ iteratively, and $\ob_c*$ is estimated by maximizing $\mathcal{J}_T$ after calculating $\z_c$. Additionally, we apply the widely used dropout techniques~\cite{srivastava2014dropout} to prevent overfitting and improve generalization performance. 

The learning process of \our is summarized as below. In each iteration, we would sample a relation mention $c$ from $\C_l$, then sample $c$'s text features and conduct the text features' representation learning. After calculating the representation of $c$, we would infer its true label $\ob_c^*$ based on our true label discovery model, and finally update model parameters based on $\ob_c^*$.

% \begin{algorithm}[t]
%   \caption{\our's Model Learning\label{alg:overall}}
%   \KwIn{Relation Mentions, $\C_l$; }
%   \myinput{Noisy annotations, $\O$.}
%   \KwOut{Relation type embeddings, $\t$;}
%   \myoutput{Text feature embeddings, $\c$;}
%   \myoutput{Proficient subset embeddings, $\l$;}
%   \myoutput{Weight matrix of mapping $g(.)$, $W$;}
%   \myoutput{Inferred true labels, $\ob^*$.}
%   \For{iter count $\in [1, I]$}{
%     sample $c$ from $\C_l$\;
%     \For{sample count $\in [1, U]$} {
%       sample $f_i, f_j$ from $\f_c$\;
%       sample $V$ negative samples\;
%       update $\v$ and $\v^*$ based on step size and $-\frac{\partial \lambda_1\mathcal{J}_E}{\partial \v}$, $-\frac{\partial \lambda_1 \mathcal{J}_E}{\partial \v^*}$.
%     }
%     calculate $\z_c$ based on Eq.~\ref{eqn:activate}\;
%     infer $\ob_c^*$ as $\argmax_{\ob_c^* \in \{r_i | \exists j, \o_{c, j} = r_i\}} \mathcal{J}_T$\;
%     update $W, \t, \v$ and $\l$ based on step size and $\frac{\partial \mathcal{J}_R - \lambda_2 \mathcal{J}_T}{\partial W}$, $\frac{\partial \mathcal{J}_R - \lambda_2 \mathcal{J}_T}{\partial \t}$, $\frac{\partial \mathcal{J}_R - \lambda_2 \mathcal{J}_T}{\partial \v}$, $\frac{\partial \mathcal{J}_R - \lambda_2 \mathcal{J}_T}{\partial \l}$\;
%   }
% \end{algorithm}

\subsection{Relation Type Inference}
\label{subsec:type_infer}
We now discuss the strategy of performing type inference for $\C_u$.
As shown in Table~\ref{tab:twoKLF}, the proportion of \texttt{None} in $\C_u$ is usually much larger than in $\C_l$. 
Additionally, not like other relation types in $\R$, \texttt{None} does not have a coherent semantic meaning. Similar to~\cite{ren2016cotype}, we introduce a heuristic rule: identifying a relation mention as \texttt{None} when (1) our relation extractor predict it as \texttt{None}, or (2) the entropy of {\small $p(. | \z_c)$} over $\R$ exceeds a pre-defined threshold $\eta$. 
The entropy is calculated as {\small $H(p(. | \z_c)) = - \sum_{r_i \in \R} p(r_i | \z_c) log (p(r_i | \z_c))$}.
And the second situation means based on relation extractor this relation mention is not likely belonging to any relation types in $\R$. 

% We identify a relation mention $d$ as \texttt{None} if all probabilities $p(r_i|\z_c) \forall r_i \in \R$ are relative small. This is because predicting a relation mention as \texttt{None} is equivalent to judging its relation type is out of $\R$. Furthermore, we use the information entropy to describe this idea. We modify the soft-max defined in Eq.~\ref{eqn:softmax} to a distribution on $\R$ instead of $\R \cup \{\texttt{None}\}$
% \begin{equation}
% \bar{p}(r_i | \z_c) = \frac{exp(\z_c^T\t_i)}{\sum_{r_j \in \R} exp(\z_c^T \t_j)} \label{eqn:new_softmax}
% \end{equation}
% Then we would classify a relation mention as \texttt{None} if its entropy exceeds a pre-defined threshold $\eta$. The entropy is calculated as $H(\bar{p}(. | \z_c)) = - \sum_{r_i \in \R} p(r_i | \z_d) log (p(r_i | \z_c))$.