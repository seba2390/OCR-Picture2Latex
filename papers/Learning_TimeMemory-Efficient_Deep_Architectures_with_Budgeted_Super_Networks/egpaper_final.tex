\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{authblk}

\usepackage{algpseudocode}


% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\graphicspath{ {img/} }
\newtheorem{mytheo}{Proposition}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{3809} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}
%%%%%%%%% TITLE
\title{Learning Time/Memory-Efficient Deep Architectures with Budgeted Super Networks}

%\author{  $^1$ \and $^{1,2}$\and Ludovic Denoyer$^{1,2}$\and Ludovic Denoyer$^{1,2}$\and Ludovic %Denoyer$^{1,2}$\and {Ludovic Denoyer$^{1,2}$}\\ \\
%  $^1$\\
%  $^2$Criteo Research\\
%  \texttt{tom.veniat@lip6.fr, ludovic.denoyer@lip6.fr}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%}


\author[*]{Tom V\'eniat}
\author[*,\dag]{Ludovic Denoyer}
\affil[*]{Sorbonne Universit\'e, LIP6, F-75005, Paris, France}
\affil[ \dag]{Criteo Research}
\affil[ ]{\texttt{\{tom.veniat, ludovic.denoyer\}@lip6.fr}}

\maketitle
\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
 We propose to focus on the problem of discovering neural network architectures efficient in terms of both prediction quality and cost. For instance, our approach is able to solve the following tasks: learn a neural network able to predict well in less than 100 milliseconds or learn an efficient model that fits in a 50 Mb memory. Our contribution is a novel family of models called Budgeted Super Networks (BSN). They are learned using gradient descent techniques applied on a budgeted learning objective function which integrates a maximum authorized cost, while making no assumption on the nature of this cost. We present a set of experiments on computer vision problems and analyze the ability of our technique to deal with three different costs: the computation cost, the memory consumption cost and a distributed computation cost. We particularly show that our model can discover neural network architectures that have a better accuracy than the ResNet and Convolutional Neural Fabrics architectures on CIFAR-10 and CIFAR-100, at a lower cost. 

\end{abstract}

\section{Introduction}

In the Deep Learning community, finding the best Neural Network architecture for a given task is a key problem that is mainly addressed \textit{by hand} or using validation techniques. For instance, in computer vision, this has lead to particularly well-known models like GoogleNet \cite{DBLP:journals/corr/SzegedyLJSRAEVR14} or ResNet \cite{DBLP:journals/corr/HeZRS15}. More recently, there is a surge of interest in developing techniques able to automatically discover efficient neural network architectures. Different algorithms have been proposed including evolutionary methods \cite{DBLP:conf/gecco/StanleyM02a, DBLP:journals/corr/MiikkulainenLMR17, DBLP:journals/corr/RealMSSSLK17} or reinforcement learning-based approaches \cite{DBLP:journals/corr/ZophL16}. But in all cases, this selection is usually based solely on a final predictive performance of the model such as the accuracy. 

When facing real-world problems, this predictive performance is not the only measure that matters. Indeed, learning a very good predictive model with the help of a cluster of GPUs might lead to a neural network that can be incompatible with low-resource mobile devices. Another example concerns distributed models in which one part of the computation is made \textit{in the cloud} and the other part is made \textit{on the device}. In such situations, an efficient architecture would have to predict accurately while minimizing the amount of exchanged messages between the cloud and the device. One important research direction is thus to propose models that can learn to take into account the inference cost in addition to the quality of the prediction. 

We formulate this issue as a problem of automatically learning a neural network architecture under budget constraints. To tackle this problem, we propose a \textit{budgeted learning} approach that integrates a maximum cost directly in the learning objective function. The main originality of our approach with respect to state-of-the-art is the fact that it can be used with any type of costs, existing methods being usually specific to particular constraints like inference speed or memory consumption -- see Section \ref{sec_related_work} for a review of state-of-the-art. In our case, we investigate the ability of our method to deal with three different costs: (i) the \textit{computation cost} reflecting the inference speed of the resulting model, (ii) \textit{the memory consumption cost} that measures the final size of the model, and the (iii) \textit{distributed computation cost} that measures the inference speed when computations are distributed over multiple machines or processors. 


\begin{figure*}[ht]
\begin{center}
\begin{subfigure}[t]{.49\textwidth} 
	\centering    
    \includegraphics[width=0.6\linewidth]{ResNets_Fabric.png}
    \caption{\textbf{ResNet Fabric}: The ResNet Fabric is a super network that includes the ResNet model as a particular sub-graph. Each row corresponds to a particular size and number of feature maps. Each edge represents a simple \textit{building block} (as described in\cite{DBLP:journals/corr/HeZRS15}) i.e two stacked convolution layers + a shortcut connection. We use projection shortcuts (with 1x1 convolutions) for all connections going across different feature map sizes (green edges). Note that here, the subgraph corresponding to the bold edges is a ResNet-20. By increasing the width of the ResNet Fabric, we can include different variants of ResNets (from ResNet-20 with width 3 up to Resnet-110 with a width of 18). }
	\label{fig1a}
\end{subfigure}
\hfill
\begin{subfigure}[t]{.49\textwidth}
	\centering
	\includegraphics[width=0.85\linewidth]{CNF_8x6.png}         
	\caption{\textbf{Convolutional Neural Fabrics} \cite{DBLP:journals/corr/SaxenaV16}: Each row corresponds to a particular resolution of feature maps. The number of features map is constant across the whole network. Each edge represents a convolution layer. The color of an edge represents the difference between input and output maps resolutions. Blue edges keep the same resolution, green edges decrease the resolution (stride $>$ 1) and red edges increase the resolution (upsampling). Feature maps are aggregated (by addition) at each node before being sent to the next layers. }
	\label{fig1b}
\end{subfigure}

\end{center}
%\vspace{-0.3cm}
   \caption{This figure illustrates the two Super Networks on top of which cost-constrained architectures will be discovered. The ResNet Fabric is a generalization of ResNets\cite{DBLP:journals/corr/HeZRS15}, while CNF has been proposed in \cite{DBLP:journals/corr/SaxenaV16}. In both cases, our objective is to discover architectures that are efficient in both prediction quality and cost, by sampling edges over these S-networks.}
\label{fig:SuperNetworks}
\label{SuperNetworks_imgs}
\end{figure*}


Our model called \textit{Budgeted Super Network} (BSN) is based on the following principles: (i) the user provides a (big) \textit{Super Network} (see Section \ref{secsn}) defining a large set of possible final network architectures as well as a maximum authorized cost. (ii) Since finding the best architecture that satisfies the cost constraint is an intractable combinatorial problem (Section \ref{s3}), we relax this optimization problem and propose a stochastic model (called \textit{Stochastic Super Networks} -- Section \ref{s4}) that can be optimized using policy gradient-inspired methods (Section \ref{s5}). We show that the optimal solution of this stochastic problem corresponds to the optimal constrained network architecture (Proposition \ref{prop1}) validating our approach. At last, we evaluate this model on various computer vision tasks. We particularly show that, by taking inspiration from the \textit{Residual Networks} (ResNet) \cite{DBLP:journals/corr/HeZRS15} and \textit{Convolutional Neural Fabrics} (CNF) \cite{DBLP:journals/corr/SaxenaV16}, our model is able to discover new neural network architectures that outperform these baselines at a lower computation/memory/distributed cost (Section \ref{section_experiments}) on CIFAR-10 and CIFAR-100. The related work is presented in Section \ref{sec_related_work}.

% With respect to existing models, our approach has one strong advantage that it can handle any observable computation cost. Example of such costs include the number of operations made by the network as in \cite{DBLP:journals/corr/HowardZCKWWAA17,DBLP:journals/corr/DongHYY17,DBLP:journals/corr/HuangCLWMW17} (Section \ref{section_classif}), but also the time spent (in milliseconds) that include any operations (disk/memory) made by the network and a particular platform. In that case, the resulting architecture will take into account the particular properties of the platform on which it has been trained (see Section \ref{XX}). It also allows to learn architectures that can be parallelized on distributed platforms and thus to discover for instance interesting alternatives to the ResNet\footnote{Reset is particularly inefficient on distributed platforms since it involves a sequence of transformations that cannot be parallelized.}\cite{DBLP:journals/corr/HeZRS15} architecture (Section \ref{parallel_section}). 





\section{Super Networks}
\label{secsn}
We consider the classical supervised learning problem defined by an input space $\mathcal{X}$ and an output space $\mathcal{Y}$. In the following, input and output spaces correspond to multi-dimensional real-valued spaces. The training set is denoted as $\mathcal{D}=\{ (x^1,y^1), ..., (x^\ell,y^\ell) \}$ where $x^i \in \mathcal{X}$, $y^i \in \mathcal{Y}$ and $\ell$ is the number of supervised examples.  At last, we consider a model $f:  \mathcal{X} \rightarrow \mathcal{Y}$ that predicts an output given a particular input. 

We first describe a family of models called \textit{Super Networks} (S-networks)\footnote{The name \textit{Super Network} comes from \cite{DBLP:journals/corr/FernandoBBZHRPW17} which presents an architecture close to ours for a completely different purpose. } since our contribution presented in Section \ref{section_super_net} will be a stochastic extension of this model. Note that the principle of Super Networks is not new and similar ideas have been already proposed in the literature under different names, e.g Deep Sequential Neural Networks \cite{DBLP:journals/corr/DenoyerG14}, Neural Fabrics \cite{DBLP:journals/corr/SaxenaV16}, or even PathNet \cite{DBLP:journals/corr/FernandoBBZHRPW17}.

A Super Network is composed of a set of layers connected together in a direct acyclic graph (DAG) structure. Each edge is a (small) neural network, the S-Network corresponds to a particular combination of these neural networks and defines a computation graph. Examples of S-networks are given in Figure \ref{SuperNetworks_imgs}. More formally, let us denote $l_1,....,l_N$ a set of layers, $N$ being the number of layers, such that each layer $l_i$ is associated with a particular representation space $\mathcal{X}_i$ which is a  multi-dimensional real-valued space. $l_1$ will be the \textit{input layer} while $l_N$ will be the \textit{output layer}. We also consider a set of (differentiable) functions $f_{i,j}$ associated to each possible pair of layers such that $f_{i,j}: \mathcal{X}_i\rightarrow \mathcal{X}_j$. Each function $f_{i,j}$ will be referred as a \textit{module} in the following: it takes data from $\mathcal{X}_i$ as inputs and transforms these data to  $\mathcal{X}_j$. Note that each $f_{i,j}$ will make disk/memory/network operations having consequences on the inference speed of the S-network. Each $f_{i,j}$ module is associated with parameters in $\theta$, $\theta$ being implicit in the notation for sake of clarity.

On top of this structure, a particular architecture $E=\{e_{i,j}\}_{(i,j) \in [1;N]^2}$ is a binary adjacency matrix over the $N$ layers such that $E$ defines a DAG with a single source node $l_1$ and a single sink node $l_N$. Different matrices $E$ will thus correspond to different super network architectures. A S-network will be denoted $(E,\theta)$ in the following, $\theta$ being the parameters of the different \textit{modules}, and $E$ being the architecture of the super network.

\paragraph{Predicting with S-networks:} The computation of the output $f(x,E,\theta)$ given an input $x$ and a S-network $(E,\theta)$ is made through a classic forward algorithm, the main idea being that the output of modules $f_{i,j}$ and $f_{k,j}$ leading to the same layer $l_j$ will be added in order to compute the value of $l_j$. Let us denote $l_i(x,E,\theta)$ the value of layer $l_i$ for input $x$, the computation is recursively defined as: 
\begin{equation}
\begin{aligned}
\text{Input:} l_1(x,E,\theta) &= x \\
\text{Layer Computation: } l_i(x,E,\theta) &= \sum\limits_k e_{k,i} f_{k,i}(l_k(x,E,\theta))
\end{aligned}
\end{equation}
 In this configuration, learning of $\theta$ can be made using classical back-propagation and gradient-descent techniques.

\section{Learning Cost-constrained architectures}
\label{section_super_net}

Our main idea is the following: we now consider that the structure $E$ of the S-network $(E,\theta)$ describes not a single neural network architecture but a set of possible architectures. Indeed, each sub-graph of $E$ (subset of edges)  corresponds itself to a S-network and will be denoted $H \odot E$, where $H$ corresponds to a binary matrix used as a mask to select the edges in $E$ and $\odot$ is the Hadamard product. Our objective will thus be to identify the best matrix $H$  such that the corresponding S-network $(H \odot E,\theta)$ will be a network efficient in terms of both predictive quality and computation/memory/... cost. 

The next sections are organized as follows: (i) First, we formalize this problem as a combinatorial problem where one wants to discover the best matrix $H$ in the set of all possible binary matrices of size $N \times N$. Since this optimization problem is intractable, we propose a new family of models called \textit{Stochastic Super Networks} where $E$ is sampled following a parametrized distribution $\Gamma$ before each prediction. We then show that the resulting budgeted learning problem is continuous and that its solution corresponds to the optimal solution of the initial budgeted problem (Proposition 1). We then propose a practical learning algorithm to learn $\Gamma$ and $\theta$ simultaneously using gradient descent techniques.

\subsection{Budgeted Architectures Learning} 
\label{s3}
Let us consider $H$ a binary matrix of size $N \times N$. Let us denote $C(H \odot E) \in \mathbb{R}^+$ the cost\footnote{Note that we consider that the cost only depends on the network architecture. The model could easily be extended to costs that depend on the input $x$ to process, or to stochastic costs -- see appendix} associated to the computation of the S-Network $(H \odot E,\theta)$. Let us also define $\mathbf{C}$ the maximum cost the user would allow. For instance, when solving the problem of \textit{learning a model with a computation time lower than 200 ms} then $\mathbf{C}$ is equal to $200 ms$. We aim at solving the following soft constrained budgeted learning problem:
% \begin{equation}
% %\begin{aligned}
% %\begin{gather} \label{eqb}
% \begin{gathered}
% H^*,\theta^* = \arg \min\limits_{H,\theta} \frac{1}{\ell} \sum\limits_i \Delta(f(x^i,H \odot E, \theta),y^i) \\
% \text{under constraints: }  C(H \odot E)\leq\mathbf{C}
% \end{gathered}
% %\end{gather} 
% %\end{aligned}
% \end{equation}

% In this article, we propose to focus on a soft version of the problem written as:
%\begin{equation}
%\begin{aligned}
\begin{multline} \label{objective}
H^*,\theta^* = \arg \min\limits_{H,\theta} \frac{1}{\ell} \sum\limits_i \Delta(f(x^i,H \odot E, \theta),y^i)
\\ + \, \lambda \max(0,C(H \odot E)-\mathbf{C})
%\end{aligned}
\end{multline}
%\end{equation}
where $\lambda$ corresponds to the importance of the cost penalty. Note that the evaluated cost is specific to the particular infrastructure on which the model is ran. For instance, if $\mathbf{C}$ is the cost in milliseconds, the value of $C(H \odot E)$ will not be the same depending on the device on which the model is used. Note that the only required property of $C(H \odot E)$ is that this cost can be measured during training.

%In other words, solving this problem on a mobile device will produce a different structure that the one obtained when solving this problem on a cluster of GPUs which is an important property of our model. 
Finding a solution to this learning problem is not trivial since it involves the computation of all possible architectures which is prohibitive ($\mathcal{O}(2^N)$ in the worst case). We explain in the next section how this problem can be solved using \textit{Stochastic Super Networks}. 

\begin{figure}[t]
\centering
\includegraphics[width=1.1\linewidth]{Cifar10_ResNetFab_flops_light} 
\caption{Accuracy/Time trade-off using B-ResNet on CIFAR-10.}
\label{plot_cif10_flop_rnf}
\end{figure}

\subsection{Stochastic Super Networks}
\label{secion_ssn}
\label{s4}

% \begin{algorithm}[t] 
% \caption{Stochastic Super Network forward algorithm}
% \begin{algorithmic}[1]
% \Procedure{SSN-forward}{$x,E,\Gamma,\theta$}
% \State $H \sim \Gamma$ \Comment{as explained in Section \ref{secion_ssn}}
% \State \textbf{For} $i \in [1..N]$, $l_i \gets $\O ~~~~~~    \textbf{EndFor} 
% \State $l_1 \gets x$
% \State \textbf{For} $i \in [2..N]$,$l_i \gets \sum\limits_{k<i} e_{k,i} h_{k,i} f_{k,i}(l_k)$~~~~ \textbf{EndFor}
% \State \textbf{return} $l_N$


% \EndProcedure
% \end{algorithmic}
% \label{algo2}
% \end{algorithm}

Now, given a particular architecture $E$, we consider the following stochastic model -- called \textbf{Stochastic Super Network} (SS-network) -- that computes a prediction in two steps: 
\begin{enumerate}
\item A binary matrix $H$ is sampled based on a distribution with parameters $\Gamma$. This operation is denoted $H \sim \Gamma$ 
\item The final prediction is made using the associated sub-graph i.e. by computing $f(x,H \odot E,\theta)$. 
\end{enumerate}
A SS-network is thus defined by a triplet $(E,\Gamma,\theta)$, where both $\Gamma$ and $\theta$ are learnable parameters. 

We can rewrite the budgeted learning objective of Equation \ref{objective} as:
\begin{multline} \label{stochobjective}
%\begin{aligned}
 \Gamma^* , \theta^* = \arg \min\limits_{\Gamma,\theta} \frac{1}{\ell} \sum\limits_i \mathbb{E}_{H \sim \Gamma}\left[ \Delta(f(x^i,H \odot E, \theta),y^i) \right. \\+ \left. \lambda \max (0,C(H \odot E)-\mathbf{C}) \vphantom{\Delta}\right]
%\end{aligned}
\end{multline}

\begin{mytheo}
\label{prop1} (proof in Appendix) 
When the solution of Equation \ref{stochobjective} is reached, then the models sampled following $(\Gamma^*)$ and using parameters $\theta^*$ are optimal solution of the problem of Equation \ref{objective}. 
\end{mytheo}

Said otherwise, solving the stochastic problem will provide a model that has a good predictive performance under the given cost constraint. 
%The demonstration of the proposition is given in supplementary material.  
% \paragraph{Demonstration of Proposition 1}
% \textcolor{red}{Si on garde ici: faire attention aux numéros}

% Let us consider the stochastic optimization problem  defined in Equation 4. The schema of the proof is the following:
% \begin{itemize}
% \item First, we lower bound the value of Equation 4 by the optimal value of Equation 3
% \item Then we show that this lower bound can be reached by some particular values of $\Gamma$ and $\theta$ in Equation 4. Said otherwise, the solution of Equation 4 is equivalent to the solution of 3.
% \end{itemize}

% Let us denote:
% \begin{equation} 
% \begin{aligned}
% B(H \odot E, \theta,\lambda) &= \frac{1}{\ell} \sum_i \Delta(f(x^i,H \odot E, \theta),y^i) 
% &+ \lambda \max (0,C(H \odot E)-C)
% \end{aligned}
% \end{equation}


% Given a value of $\Gamma$, let us denote $supp(\Gamma)$ all the $H$ matrices that can be sampled following $\Gamma$. The objective function of Equation 4 can be written as:
% \begin{equation}
% \begin{aligned}
% E_{H \sim \Gamma} [B(H, E, \theta,\lambda)] &=\sum\limits_{H \in supp(\Gamma)} B(H \odot E, \theta,\lambda) P(H|\Gamma) \\
% & \geq \sum\limits_{H \in supp(\Gamma)} B((H \odot E)^*, \theta^*,\lambda) P(H|\Gamma) \\
% & = B((H \odot E)^*, \theta^*,\lambda)
% \end{aligned}
% \label{cucu}
% \end{equation}
% where $(H \odot E)^*$ and $\theta^*$ correspond to the solution of:
% \begin{equation}
% (H \odot E)^*,\theta^* = \arg \min_{H,\theta} B(H \odot E,\theta,\lambda)
% \label{eqt}
% \end{equation}


% Now, it is easy to show that this lower bound can be reached by considering a value of $\Gamma^*$ such that $\forall H \in supp(\Gamma), H \odot E = (H \odot E)^*$. This corresponds to a value of $\Gamma$ where all the probabilities associated to edges in $E$ are equal to $0$ or to $1$.

\paragraph{Edge Sampling: } In order to avoid inconsistent architectures where the input and the output layers are not connected, we sample $H$ using the following procedure: For each layer $l_i$ visited in the topological order of $E$ (from the first layer to the last one) and for all $k<i$:  If $l_k$ is connected to the input layer $l_1$ based on the previously sampled edges, then $h_{k,i}$ is sampled following a Bernoulli distribution with probability\footnote{Note that $\gamma_{k,i}$ is obtained by applying a \textit{logistic} function over a continuous parameter value, but this is made implicit in our notations.} $\gamma_{k,i}$. In the other cases, $h_{k,i}=0$. 

\subsection{Learning Algorithm}
\label{s5}

We consider the generic situation where the cost-function $C(.)$ is unknown and can be observed at the end of the computation of the model over an input $x$. Note that this case also includes stochastic costs where $C$ is a random variable, caused by some network latency during distributed computation for example. We now describe  the case where $C$ is deterministic, see appendix  for its stochastic extension.

Let us denote $\mathcal{D}(x,y,\theta,E,H)$ the quality of the S-Network $(H \odot E,\theta)$ on a given training pair $(x,y)$: 
\begin{multline} \label{D_def}
\mathcal{D}(x,y,\theta,E,H)=\Delta(f(x,H \odot E,\theta),y) \\+ \lambda \max(0, C(H \odot E) - \mathbf{C})
\end{multline}
We propose to use a policy gradient inspired algorithm as in \cite{DBLP:journals/corr/DenoyerG14,DBLP:journals/corr/BengioBPP15} to learn $\theta$ and $\Gamma$. Let us denote $\mathcal{L}(x,y,E,\Gamma,\theta)$ the expectation of $\mathcal{D}$ over the possible sampled matrices $H$:
\begin{equation}
\mathcal{L}(x,y,E,\Gamma,\theta) = \mathbb{E}_{H \sim \Gamma} \mathcal{D}(x,y,\theta,E,H)
\end{equation}

The gradient of $\mathcal{L}$ can be written as\footnote{details provided in appendix}:
\begin{multline}
\nabla_{\theta,\Gamma} \mathcal{L}(x,y,E,\Gamma,\theta) \\ = \sum\limits_H P(H|\Gamma) \left[(\nabla_{\theta,\Gamma}  \log P(H|\Gamma)) \mathcal{D}(x,y,\theta,E,H) \right] \\+ \sum\limits_H P(H|\Gamma) \left[ \nabla_{\theta,\Gamma} \Delta(f(x,H \odot E,\theta),y) \right]
\end{multline}
The first term corresponds to the gradient over the log-probability of the sampled structure $H$ while the second term is the gradient of the prediction loss given the sampled structure $H \odot E$.

Learning can be made using back-propagation and stochastic-gradient descent algorithms as it is made in Deep Reinforcement Learning models. Note that in practice, in order to reduce the variance of the estimator, the update is made following:
\begin{multline}
\nabla_{\theta,\Gamma} \mathcal{L}(x,y,E,\Gamma,\theta) \\ \approx (\nabla_{\theta,\Gamma} \log P(H|\Gamma)) (\mathcal{D}(x,y,\theta,E,H)-\tilde{\mathcal{D}} )  \\+ \nabla_{\theta,\Gamma} \Delta(f(x,H \odot E,\theta),y)
\end{multline}
where $H$ is sampled following $\Gamma$, and where $\tilde{\mathcal{D}}$ is the average value of $\mathcal{D}(x,y,\theta,E,H)$ computed on a batch of learning examples. 



\section{Experiments}\label{section_experiments}

\subsection{Implementation}\label{implem}

We study two particular S-Network architectures:

\textbf{ResNet Fabric} (Figure \ref{fig1a}) which is used for \textbf{image classification}. This S-Network is inspired by the ResNet\cite{DBLP:journals/corr/HeZRS15} architecture on which extra modules (i.e. edges) have been added. The underlying idea is that a particular sub-graph of the ResNet Fabric corresponds exactly to a ResNet model. We thus aims at testing the ability of our approach to discover ResNet-inspired efficient architectures, or at least to converge to a ResNet model that is known to be efficient.

\textbf{Convolutional Neural Fabrics} (CNF) which has been proposed in \cite{DBLP:journals/corr/SaxenaV16} (Figure \ref{fig1b}). It is a generic architecture that can be used for both \textbf{image classification} and \textbf{image segmentation}. The layers of the CNF super network are organized in a $W \times H$ matrix. We always use $W=8$ when running our budgeted algorithm. Different values of $W$ (as in \cite{DBLP:journals/corr/SaxenaV16}) are used as baselines. 

Image classification has been tested on CIFAR-10 and CIFAR-100 \cite{Krizhevsky09learningmultiple} while the image segmentation has been performed on the Part Label dataset \cite{GLOC_CVPR13}. 

For these two architectures denoted \textit{B-ResNet} and \textit{B-CNF}, we consider three different costs functions: the first one is the (i) \textit{computation cost} computed as the number of operations \footnote{The number of Mult-Add operations required to fully evaluate a network.} made by the S-Network as used in \cite{DBLP:journals/corr/DongHYY17} or \cite{DBLP:journals/corr/HuangCLWMW17}. Note that this cost is highly correlated with the execution time\footnote{Expressing constraint directly in \textit{milliseconds} has been also investigated, with results similar to the ones obtain using the computation cost, and are not presented here.}. The second one is the \textit{memory consumption cost}, measured as the number of parameters of the resulting models. At last, the third cost (iii) is the \textit{distributed computation cost} which is detailed in Section \ref{parallel_section} and corresponds to the ability of a particular model to be efficiently computed over a distributed environment. 

\subsection{Experimental Protocol and Baselines}


Each model is trained with various values for the objective cost $\mathbf{C}$. For the image classification problem, since we directly compare to ResNet, we select values of $\mathbf{C}$ that corresponds to the cost of the ResNet-20/32/44/56/110 architectures. This allows us to compare the performance of our method at the same cost level as the ResNet variants. When dealing with the B-CNF model, we select $\mathbf{C}$ to be the cost of different versions of the CNF model, having different width W. The height H being fixed by the resolution of the input image.

For each experiment, multiple versions of the different models are evaluated over the validation set during learning. Since our evaluation now involves both a cost and an accuracy, we select the best models using the pareto front on the cost/accuracy curve on the validation set. The reported performance are then obtained by evaluating these selected models over the test set. The detailed procedure of the hyper-parameters and model selection are given in Appendix and the source code of our implementation is open source\footnote{\url{https://github.com/TomVeniat/bsn}}. The learning is done using a classical stochastic gradient-descent algorithm for all parameters with learning rate decay, momentum of 0.9 and weight decay of $10^{-4}$ for $\theta$. 


\input{results/cifar10_allmodels_flops.tex}

For each experiment, we give the performance of both reference models (ResNet \cite{DBLP:journals/corr/HeZRS15} and CNF \cite{DBLP:journals/corr/SaxenaV16}), and of related existing models
i.e Low Cost Collaborative Layer (LCCL)\cite{DBLP:journals/corr/DongHYY17} and MSDNet \cite{DBLP:journals/corr/HuangCLWMW17} (under the anytime classification settings). Note that the baselines methods have been designed to reduce exclusively the \textit{computation cost}, while our technique is able to deal with any type of cost. We provide the performance of our budgeted version of ResNet (B-ResNet) and of our budgeted version of CNF (B-CNF). Note that, for a fair comparison, we present the published results of ResNet and CNF, but also the ones that we have obtained by training these models by ourselves, \textbf{our results being of better quality} than the previously published performance.

% \subsection{Learning Efficient architectures}

% In the first set of experiments, we consider the \textit{flop cost} which measures the number of operations made by the resulting neural network. Our main objective is to evaluate the ability of BSN to reduce this cost while keeping a high accuracy. The \textit{flop cost} is computed as the theoretical number of operations needed by the different modules used in the neural network as done in \cite{XX}. 

% We test our model with two different architectures: 
% \begin{itemize}
% \item The CNF architecture \cite{DBLP:journals/corr/SaxenaV16}, see Figure \ref{fig1} (a), where the layers are organized in a $W=8 \times H=6$ matrix. Each level in $H$ corresponds to a particular feature map size. Edges correspond to a 2D-convolution layer with kernels of size 3, stride and padding depending on the position of the module in the network. Each layer is followed by a Batch Normalization (BN) module and a ReLU activation \cite{DBLP:journals/corr/IoffeS15}.
% We compare our architecture with 2 baselines: (i) Random search, where random architectures are sampled and trained to convergence. This can be seen as selecting several precise architectures of all sizes within the Neural Fabric and using these network as a reference. (ii) Smaller Convolutional Neural Fabrics, by training to convergence fabrics with W=4 and W=2. Allowing to compare our deeper learnt architectures with shallow but wider networks having a similar budget.
% [Note here or in ccl that random search is favorised in very low budget as it doesn't have to find an architecture, its given one respecting the constraint.]

% \item (ii) A modification of the ResNet\cite{DBLP:journals/corr/HeZRS15} architecture where some residual blocks are added and connect each block the the blocks of same depth D, D-1 and D+1 from the layer having larger map sizes(wrong, see supp materials for more details) These added connection are arbitrary and spending enough time in searching a best Super-Network would likely lead to better results. This choice has been made with the intuition that such a super network would allow our BSN to search an architecture in the output map size space, as in the CNF super network, but also in the depth space, benefiting from the residual connections and these added layer to find an appropriate depth while respecting the given budget.  (iii) [The MDSNet architecture \cite{XX} allowing to have the multiscale benefits from the CNF and the skip-connections ability of ResNet.]
% We use here classical ResNets (110, 56, 44, 32, 20) as baselines. (and maybe MSDNET ...)
% \end{itemize} 
% Note that in each base network we use classic 3x3 convolutions and let the exploration of the performance gain using more complex basic block (Bottleneck blocks, depthwise seprable convolutions, ...) for further work. 

\subsubsection{Experimental results}\label{section_classif}

% \begin{figure}[ht]
% \begin{subfigure}[h]{0.5\textwidth}
% \includegraphics[width=1\textwidth]{Cif10_RNF_Flop} 
% \includegraphics[width=1\textwidth]{Plot_1} 
% \includegraphics[width=1\textwidth]{Cifar10_ResNetFab_flops} 
% \includegraphics[width=1\textwidth]{Cifar10_ResNetFab_flops_light_size2} 
% \includegraphics[width=1\textwidth]{Cifar10_ResNetFab_flops_light_size3} 
% Mettre courbe d'apprentissage accuracy/cost
% \end{subfigure}
% \caption{Accuracy/Cost curve for different models.}
% \end{figure}

\begin{figure*}[ht]
\begin{center}

\begin{subfigure}[t]{.3\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Arch_ResNet_fabric}         
	\caption{B-ResNet}
	\label{arch_resnet}
\end{subfigure}
\begin{subfigure}[t]{.3\textwidth} 
	\centering
    \includegraphics[width=\linewidth]{Arch_cnf_flops_w9_allnodes}
    \caption{B-CNF \& computation cost}
	\label{arch_cnf_flop}
\end{subfigure}
\begin{subfigure}[t]{.3\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Arch_cnf_params_w9_allnodes}  
	\caption{B-CNF \& memory consumption cost}
	\label{arch_cnf_param}
\end{subfigure}

\end{center}
   \caption{Discovered architectures: \textbf{(Left)} is a low \textit{computation cost} B-ResNet where dashed edges correspond to connections in which the two convolution layers have been removed (only shortcut or projection connections are kept). \textbf{(Center)} is a low \textit{computation cost} B-CNF where high-resolution operations have been removed. \textbf{(Right)} is a low \textit{memory consumption cost} B-CNF: the algorithm has mostly kept all high resolution convolutions since they allow fine-grained feature maps and have the same number of parameters than lower-resolution convolutions. It is interesting to note that our algorithm, constrained with two different costs, automatically learned two different efficient architectures.} 
\label{fig:DiscoveredNetworks}
\end{figure*}

\input{results/cifar100_resnetfab_flops.tex}

\paragraph{Reducing the computation cost:} Figure \ref{plot_cif10_flop_rnf} and Table \ref{cif10_allmodels_flop} show the performance of different models over CIFAR-10. Each point corresponds to a model evaluated both in term of accuracy and \textit{computation cost}. When considering the B-ResNet model, and by fixing the value of $\mathbf{C}$ to the computation cost of the different ResNet architectures, we obtain budgeted models that have approximatively the same costs than the ResNets, but with a higher accuracy. For example, ResNet-20 obtains an accuracy of 92.19\% at a cost of $40.9 \times 10^6$ flop, while B-ResNet is able to discover an architecture with 92.39\% accuracy at a slightly lower cost ($39.25 \times 10^6$ flop). Moreover, the B-ResNet model also outperforms existing approaches like MSDNet or LCCL, particularly when the \textit{computation cost} is low i.e for architectures that can be computed at a high speed. When comparing CNF to B-CNF, one can see that our approach is able to considerably reduce the computation cost while keeping a high accuracy. For example, one of our learned models obtained an accuracy of 93.14\% with a cost of $103 \times 10^6$ flop while CNF has an accuracy of 92.54\% for a cost of $406 \times 10^6$ flop. Note that the same observations can be drawn for CIFAR-100 (Table \ref{cif100_resnetfab_flop}). 

Figure \ref{arch_resnet} and \ref{arch_cnf_flop} illustrate two architectures discovered by B-ResNet and B-CNF with a low \textit{computation cost}. One can see that B-ResNet has converged to an architecture which is a little bit different than the standard ResNet architecture, explaining why its accuracy is better. On the CNF side, our technique is able to extract a model that has a minimum of high-resolution convolutions operations, resulting in a high speedup. 


\input{results/cifar10_allmodels_params.tex}
\vspace{-0.3cm}
\paragraph{Reducing the memory consumption:} Similar experiments have been made considering the \textit{memory consumption cost} that measures the number of parameters of the learned architectures. We want to demonstrate here the ability of our technique to be used with a large variety of costs, and not only to reduce the computation time. Table \ref{cif10_allmodels_params} illustrates the results obtained on CIFAR-10. As with the \textit{computation cost}, one can see that our approach is able to discover architectures that, given a particular memory cost, obtain a better accuracy. For example, for a model which size is $\approx 0.47$ millions parameters, ResNet-32 has a classification error of 7.81\%  while B-ResNet only makes 6.58\% error with $\approx 0.48$ million parameters. 


\paragraph{Image Segmentation: }

%\begin{figure}[ht]
%\begin{figure}[h]{0.5\textwidth}
%\centering

%\end{subfigure}
% \begin{subfigure}[h]{0.5\textwidth}
% \centering
% \includegraphics[width=\textwidth]{arch_cnf_part_w9} 
% \label{fig:partlabel_arch}
% \caption{Discovered Architecture}
% \end{subfigure}
%\caption{Part Label dataset.}
%\end{figure}

We also perform experiments on the image segmentation task using the Part Label dataset with CNF and B-CNF (Table \ref{tab:partlabels}). In this task, the model computes a map of pixel probabilities, the output layer being now located at the top-right position of the CNF matrix. It is thus more difficult to reduce the overall computation cost. On the Part Label dataset, we are able to learn a BSN model with a computation gain of $40\%$. Forcing the model to reduce further the computation cost by decreasing the value of $\mathbf{C}$ results in inconsistent models. At a computation gain of $40 \%$, BSN obtains an error rate of $4.57 \%$, which can be compared with the error of $4.94\%$ for the full model. The B-CNF best learned architecture is given in appendix.
\vspace{-0.2cm}
\paragraph{Learning Dynamics: }


Figure \ref{fig:dynamics} illustrates the learning dynamics of B-CNF and CNF. First, one can see (entropy curve) that the model becomes deterministic at the end of the learning procedure, and thus converges to a unique architecture. Moreover, the training speed of B-CNF and CNF are comparable showing that our method does not result in a slower training procedure. Note that the figure illustrates the fact that during a burn-in period, we don't update the edges probabilities, which allows us to obtain a faster convergence speed (see appendix).

\input{results/part_cnf_flops.tex}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.8\linewidth]{error_entropy}
\end{center}

\caption{Evolution of the loss function and the entropy of $\Gamma$ during training. The period between epoch 0 and 50 is the burn-in phase. The learning rate is divided by 10 after epoch 150 to increase the convergence speed.}
\label{fig:dynamics}
\end{figure}





\subsection{Learning Distributed Architectures}\label{parallel_section}

\begin{figure*}[h]
\centering
    \begin{subfigure}[t]{0.65\textwidth}
        \includegraphics[width=1.0\linewidth]{Cifar10_ResNetFab_para_all}
        \caption{Accuracy/number of operation for different number of cores on CIFAR-10 using B-ResNet.}
     \end{subfigure} 
     \hspace{.7em}
     \begin{subfigure}[t]{0.30\textwidth}
     	\vspace{-15em}
        \includegraphics[width=1.0\linewidth]{Arch_cnf_p1p4_w9}
        \caption{Architectures discovered with B-CNF for different number of cores: $n=1$ (top) and $n=4$ (bottom)}
        \label{fig:para_arch}
     \end{subfigure}
\caption{Architectures discovered on CIFAR-10 for different number of distributed cores $n$.}
\label{fig:para_res_arch}
\end{figure*}

%\begin{figure}[ht]
%\centering
%        \includegraphics[width=9cm]{parallel_demo}
%\caption{Two networks that have a close \textit{flop cost} but two highly different \textit{parallelization costs}. The blue network is composed by 9 computational modules but as a distribution cost of 5 XXXX while the red network is composed of 10 modules but highly parallelizable.}
%\label{parallel_ex}
%\end{figure}

%\input{results/cifar10_allmodels_allpara.tex}
%\input{results/cifar100_resnetfab_para1.tex}

At last, we perform a third set of experiments focused on distributed computing where different edges can be computed simultaneously on different computers/processors of a distributed platform. We thus evaluate the quality of an architecture by its ability to be efficiently parallelized. The \textit{distributed computation cost} corresponds to the number of steps needed to compute the output of the network e.g on an architecture with $n=2$ computers, depending on the structure of the network, two edges could be computed simultaneously. If the architecture is a sequence of layers, then this parallelization becomes impossible. Theses experiments allow us to measure the ability of BSN to handle complex costs that cannot be decomposed as a sum of individual modules costs as it is usually done in related works. 

Results and corresponding architectures are illustrated in Figure \ref{fig:para_res_arch} for the CIFAR-10 dataset and for both the B-ResNet and the B-CNF architectures. Note that ResNet is typically an architecture that cannot be efficiently distributed since it is a sequences of modules. One can see that our approach is able to find efficient architectures for $n=2$ and $n=4$. Surprisingly, when $n=4$ the discovered architectures are less efficient, which is mainly due to an over fitting of the training set, the cost constraint becomes too large and stop acting as a regularizer on the network architecture. On Figure \ref{fig:para_arch}, one can see two examples of architectures discovered when $n=1$ and $n=4$. The shape of the architecture when $n=4$ clearly confirm that BSN is able to discover parallelized architectures, and to 'understand' the structure of this complex cost.  

\section{Related Work}\label{sec_related_work}

\textbf{Learning cost-efficient models: } One of the first approaches to learn efficient models is to \textit{a posteriori}  compress the learned network, typically by pruning some connections. The oldest work is certainly the Optimal Brain Surgeon \cite{Hassibi} which removes weights in a classical neural network. The problem of network compression can also be seen as a way to speed up a particular architecture, for example by using quantization of the weights of the network \cite{Vanhoucke11}, or by combining pruning and quantization  \cite{han}. Other algorithms include the use of hardware efficient operations that allow a high speedup \cite{hard}. 

 \textbf{Efficient architectures: }Architecture improvements have been widely used in CNN to improve cost efficiency of network components, some examples are the bottleneck units in the ResNet model \cite{DBLP:journals/corr/HeZRS15}, the use of depthwise separable convolution in Xception \cite{DBLP:journals/corr/Chollet16a} and the lightweight  MobileNets\cite{DBLP:journals/corr/HowardZCKWWAA17} or the combinaison of pointwise group convolution and channel shuffle in ShuffleNet\cite{DBLP:journals/corr/ZhangZLS17}. %[This technique is powerful but requires a strong expertise to find the best optimization and is also highly specific to each architecture and/or hardware. Note that the computation speed can be reduced by using parallelized inference approaches which is not the topic of this paper.] Our model making no assumption on the architecture of the base network, these highly effective computational blocks can be used without any modification in combination with our model to improve further the computational efficiency of deep neural networks.

\textbf{End-to-end approaches: } A first example of end-to-end approaches is the usage of quantization at training time: different authors trained models using binary weight quantization coupled with full precision arithmetic operations  \cite{DBLP:journals/corr/CourbariauxBD15},\cite{DBLP:journals/corr/Lu17c}. Recently, \cite{DBLP:journals/corr/abs-1710-03740} proposed an method using half precision floating numbers during training. Another technique proposed by \cite{DBLP:journals/corr/HintonVD15}, \cite{DBLP:journals/corr/RomeroBKCGB14} and used in \cite{2017arXiv170510924Z,2017arXiv170510194N} is the distillation of knowledge, which consists of training a smaller network to imitate the outputs of a larger network. %\textcolor{red}{Je comprends pas les 4/5 mots qui suivent}
Other approaches are dynamic networks which conditionally select the modules to respect a budget objective.\cite{DBLP:journals/corr/BolukbasiWDS17, DBLP:journals/corr/OdenaLO17,DBLP:journals/corr/HuangCLWMW17,DBLP:journals/corr/BengioBPP15,DBLP:journals/corr/McGillP17}. 
%while keeping several important differences. First, the model is based on an explicit computation cost formulation while our approach can be used for any type of cost. Moreover, while our model converges to a static architecture, allowing to keep only the blocks used by the trained model, their model learns dynamic routes, causing an overhead in the computation cost and forcing to keep the whole network to do further inferences. At last, the number of possible architectures explored is smaller in this approach than in ours.

\textbf{Architecture Search: } Different authors have proposed to provide networks with the ability to learn to select the computations that will be applied i.e choosing the right architecture for a particular task. This is the case for example for classification in \cite{DBLP:journals/corr/DenoyerG14,DBLP:journals/corr/ZophL16} based on Reinforcement learning techniques, in \cite{DBLP:journals/corr/SrivastavaGS15} based on gating mechanisms,  in \cite{DBLP:journals/corr/RealMSSSLK17} based on  evolutionary algorithms or even in \cite{DBLP:journals/corr/FernandoBBZHRPW17} based on both RL and evolutionary techniques.
%One strong difference with our approach is that these models are only guided by the final predictive performance of the network. For example in \cite{DBLP:journals/corr/ZophL16}, a controller neural net can propose a “child” model architecture, which can then be trained and evaluated before trying a new architecture. The process is repeated iteratively until a good architecture is obtained. Moreover, in this approach each generated architecture is learned to convergence on the training set, resulting in a very low training speed while our model learns the parameters of the modules and the architecture simultaneously.

The strongest difference w.r.t. existing methods is that we do not make any assumption concerning the nature of the cost. Our model is thus more generic than existing techniques and allow to handle a large variety of problems.






\section{Conclusion and Perspectives}

We proposed a new model called \textit{Budgeted Super Network} able to automatically discover cost-constrained neural network architectures by specifying a maximum authorized cost. The experiments in the computer vision domain show the effectiveness of our approach. Its main advantage is that BSN can be used for any costs (computation cost, memory cost, etc.) without any assumption on the shape of this cost. A promising research direction is now to study whether this model could be adapted in order to reduce the training time (instead of the test computation time). This could for example be done using meta-learning approaches. 


\section*{Acknowledgments}
This work has been funded in part by grant ANR-16-CE23-0016 ``PAMELA'' and grant ANR-16-CE23-0006 ``Deep in France''.



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\newpage
\clearpage

\thispagestyle{empty}
\section*{Supplementary Material}

\subsection*{Demonstration of Proposition 1}

Let us consider the stochastic optimization problem defined in Equation \ref{stochobjective}. The schema of the proof is the following:
\begin{itemize}
\item First, we lower bound the value of Equation \ref{stochobjective} by the optimal value of Equation \ref{objective}.
\item Then we show that this lower bound can be reached by some particular values of $\Gamma$ and $\theta$ in Equation \ref{stochobjective}. Said otherwise, the solution of Equation \ref{stochobjective} is equivalent to the solution of \ref{objective}.
\end{itemize}

Let us denote:
\begin{multline}
B(H \odot E, \theta,\lambda) = \frac{1}{\ell} \sum_i \Delta(f(x^i,H \odot E, \theta),y^i)  \\+ \lambda \max (0,C(H \odot E)-C)
\end{multline}


Given a value of $\Gamma$, let us denote $supp(\Gamma)$ all the $H$ matrices that can be sampled following $\Gamma$. The objective function of Equation \ref{stochobjective} can be written as:
\begin{equation}
\begin{aligned}
E_{H \sim \Gamma} [B(H \odot E, \theta,\lambda)] &=\sum\limits_{H \in supp(\Gamma)} B(H \odot E, \theta,\lambda) P(H|\Gamma) \\
& \geq \sum\limits_{H \in supp(\Gamma)} B((H \odot E)^*, \theta^*,\lambda) P(H|\Gamma) \\
& = B((H \odot E)^*, \theta^*,\lambda)
\end{aligned}
\label{cucu}
\end{equation}
where $(H \odot E)^*$ and $\theta^*$ correspond to the solution of:
\begin{equation}
(H \odot E)^*,\theta^* = \arg \min_{H,\theta} B(H \odot E,\theta,\lambda)
\label{eqt}
\end{equation}


Now, it is easy to show that this lower bound can be reached by considering a value of $\Gamma^*$ such that $\forall H \in supp(\Gamma), H \odot E = (H \odot E)^*$. This corresponds to a value of $\Gamma$ where all the probabilities associated to edges in $E$ are equal to $0$ or to $1$.


\newpage


\subsection*{Gradient computation}

%\begin{multline}
%\mathcal{D}(x,y,\theta,E,H)=\Delta(f(x,H \odot E,\theta),y) \\+ \lambda \max(0, C(H \odot E) - \mathbf{C})
%\end{multline}

%\begin{equation}
%\mathcal{L}(x,y,E,\Gamma,\theta) = \mathbb{E}_{H \sim \Gamma} \mathcal{D}(x,y,\theta,E,H)
%\end{equation}

\begin{equation}
\nabla_{\theta,\Gamma} \mathcal{L}(x,y,E,\Gamma,\theta) = \nabla_{\theta,\Gamma} \mathbb{E}_{H \sim \Gamma} \mathcal{D}(x,y,\theta,E,H)
\end{equation}

\begin{equation}
= \nabla_{\theta,\Gamma} \sum\limits_H P(H|\Gamma) \mathcal{D}(x,y,\theta,E,H)
\end{equation}

\begin{equation}
= \sum\limits_H \nabla_{\theta,\Gamma} (P(H|\Gamma) \mathcal{D}(x,y,\theta,E,H))
\end{equation}

\begin{multline}
= \sum\limits_H \nabla_{\theta,\Gamma} (P(H|\Gamma))\mathcal{D}(x,y,\theta,E,H) \\+ P(H|\Gamma) \nabla_{\theta,\Gamma} \mathcal{D}(x,y,\theta,E,H)
\end{multline}

\begin{multline}
= \sum\limits_H P(H|\Gamma)\nabla_{\theta,\Gamma} \log P(H|\Gamma) \mathcal{D}(x,y,\theta,E,H) \\+ P(H|\Gamma) \nabla_{\theta,\Gamma} \mathcal{D}(x,y,\theta,E,H)
\end{multline}

Using Equation \ref{D_def}:

\begin{multline}
= \sum\limits_H P(H|\Gamma) ((\nabla_{\theta,\Gamma} \log P(H|\Gamma)) \mathcal{D}(x,y,\theta,E,H) \\+ \nabla_{\theta,\Gamma} \Delta(f(x,H \odot E,\theta),y) )
\end{multline}

\begin{multline}
= \sum\limits_H P(H|\Gamma) \left[(\nabla_{\theta,\Gamma}  \log P(H|\Gamma)) \mathcal{D}(x,y,\theta,E,H) \right] \\+ \sum\limits_H P(H|\Gamma) \left[ \nabla_{\theta,\Gamma} \Delta(f(x,H \odot E,\theta),y) \right]
\end{multline}

\begin{multline}
= \sum\limits_H P(H|\Gamma) \left[ (\nabla_{\theta,\Gamma}  \log P(H|\Gamma)) \Delta(f(x,H \odot E,\theta),y) \right]\\
+ \lambda \sum\limits_H P(H|\Gamma) \left[ (\nabla_{\theta,\Gamma}  \log P(H|\Gamma)) \max(0, C(H \odot E) - \mathbf{C}) \right] \\
+ \sum\limits_H P(H|\Gamma) \left[ \nabla_{\theta,\Gamma} \Delta(f(x,H \odot E,\theta),y) \right]
\end{multline}

\newpage


\subsection*{Segmentation architecture}
\begin{figure}[ht]
%\begin{figure}[h]{0.5\textwidth}
\centering
\includegraphics[width=\linewidth]{arch_cnf_part_w9}
\caption{Segmentation architecture}
\label{fig:partlabel_arch}
\end{figure}
Figure \ref{fig:partlabel_arch} is an example of segmentation architecture discovered on the Part Label dataset using the \textit{flop cost}. It is interesting to note that only one layer with 256x256 input and output is kept and that most of the computations are done at lower less-expensive layers.

\subsection*{Model Selection Protocol}
\begin{figure}[ht]
\centering
\includegraphics[width=1.1\linewidth]{val_test_costs}
\caption{Model selection}
\label{fig:model_selection}
\end{figure}

The selection of reported models is obtained by learning many different models, computing the Pareto front of the accuracy/cost curve on the validation set, and reporting the performance obtained on the test set. This is illustrated in figure \ref{fig:model_selection} where many different models are reported on the validation set(blue circles) with the corresponding performance on the test set (red crosses).

\newpage




\section*{Considering non-differentiable costs}
\subsection*{Stochastic costs in the REINFORCE algorithm: } As explained previously, the proposed algorithm can also be used when the cost $C(H \odot E)$ is a stochastic function that depends on the environment e.g the network latency, (or even on the input data $x$). Our algorithm is still able to learn with such stochastic costs since the only change in the learning objective is that the expectation is now made on both $H$ and $C$ (and $x$ if needed). 
This property is interesting since it allows to discover efficient architecture on stochastic operational infrastructure. 

%\subsection*{Complex costs}
%In some contexts, an architecture cost can't be seen as the sum of its components costs. This class of costs are generally non differentiable and therefore requires the usage of a gradient estimator to be optimized. We will present in this section an example of such cost function and the results of our model when searching architectures.


\paragraph*{Distributed computation cost} Taking the real-life example of a network which will, once optimized, have to run on a given computing infrastructure, the \textit{distributed computation cost} is a measure of how "parallelizable" an architecture is. This cost function takes the following three elements as inputs (i)A network architecture (represented as a graph for instance), (ii)An allocation algorithm and (iii) a maximum number of concurrent possible operations. The cost function then returns the number of computation cycles required to run the architecture given the allocation strategy.
\begin{figure}[ht]
	\centering
    \includegraphics[width=9cm]{parallel_demo}
	\caption{Two networks illustrating the need to have a cost function evaluating the global architecture of a network. Considering an environment with $n=2$ machines performing computations in parallel, the blue network composed of 9 computational modules has a \textit{distributed computation cost} of 6 while the red network, composed of 10 modules, has a smaller cost of 5.}
\label{parallel_ex}
\end{figure}

%This particular objective is similar to the objective of \cite{2017arXiv170604972M}, where they learn a placement for the different parts  of a given neural network across a set of available devices. 



\subsection*{Additional Architecture Details}
\subsubsection*{ResNet Fabric}
Based on the ResNet architecture, the structure of a ResNet Fabric is a stack of $k$ groups of layers, each group being composed of $2n$ layers where $n$ represents the width of the Fabric. The feature maps size and number of filters stay constant across the layers of each group and are modified between groups.

Due to its linear structure, the standard ResNet architecture spans a limited number of possible (sub-)architectures. In order to increase the size of the search space, we add several connections between groups as shown in \ref{fig1a}: each block in the second to last groups receives two (for the first and last block of each group) or three (for every other block) inputs from preceding groups. To stay consistent with the rest of the network, each connection is a \textit{basic block} \cite{DBLP:journals/corr/HeZRS15} composed of 2 convolutional layers and a shortcut connection.

In our experiments, we use stacks of $k=3$ blocks and $n=\{3, 5, 7, 9, 18\}$ to respectively include the ResNet-$\{20,32,44,56,110\}$ in the Fabric. Between each block, the feature maps size is reduced by a factor of 2 and the number of feature maps is doubled.

\subsubsection*{Convolutional Neural Fabric}
The second network we use in our experiments is based on the dense Convolutional Neural Fabrics, which can be seen as a multi-layer and multi-scale convolutional neural network.  As shown in Figure \ref{fig1b}, this architecture has 2 axis: The first axis represents the different columns (or width) $W$ of the network while the second axis corresponds to different scales (or height) $H$ of output feature maps, the first scale being the size of the input images, each subsequent scale being of a size reduced by a factor of 2 up to the last scale corresponding to a single scalar.

Each layer $(l, s)$ in this fabric takes its input from three different layers of the preceding column: (i) One with a finer scale $(l-1, s-1)$ on which a convolution with stride 2 is applied to obtain feature maps having half the size of the input, (ii) one with the same scale $(l-1, s)$ on which a convolution with stride 1 is applied to obtain feature map of the same resolution as the input and (iii) one with a coarser scale $(l-1, s+1)$ on which  convolution with stride 1 is applied after a factor 2 up-sampling to obtain feature maps having twice the size of the input. The three feature blocks are then added before passing through the ReLU activation function to obtain the final output of this layer $(l, s)$. 

The first and last columns are the only two which have vertical connections within scales of the same layer (as can be seen in Figure \ref{fig1b}). This is made to allow the propagation of the information to all nodes in the first column and to aggregate the activations of the last column to compute the final prediction. A more detailed description of this architecture can be found in the CNF original article.

We used two different Convolutional Neural Fabrics in our experiments: One for the classification task (CIFAR-10 and CIFAR-100) with $W=8$ columns, $H=6$ scales and 128 filters per convolution and one for the segmentation task (Part Label) with $W=8$ layers, $H=9$ scales (from 256x256 to 1x1 feature map sizes) and 64 filters per convolution.

\newpage
\subsection*{Additional Learning Details}

\subsubsection*{Datasets}

\paragraph*{CIFAR-10.}
The CIFAR-10 dataset consists of 60k 32x32 images with 10 classes and 6000 images per class. The dataset is decomposed in 50k training and 10k testing images. We split the training set following the standard, i.e 45k training samples and 5k validation samples. We use two data augmentation techniques: padding the image to 36x36 pixels before extracting a random crop of size 32x32 and horizontally flipping. Images are then normalized in the range [-1,1].  

\paragraph*{CIFAR100.}
The CIFAR-100 dataset is similar to CIFAR-10, with 100 classes and 600 images per class. We use the same train/validation split and data augmentation technique as with CIFAR-10.  

\paragraph*{Part Labels.}
The Part Labels dataset is a subset of the LFW dataset composed of 2927 250x250 face images in which each pixel is labeled as one of the Hair/Skin/Background classes. The standard split contains 1500 training samples, 500 validation samples and 927 test samples. Images are zero-padded from 250x250 to 256x256. We use horizontal flipping as data augmentation. Images are then normalized in the range [-1,1].

\subsubsection*{Learning procedure}
When training our budgeted models, we first train the network for 50 "warm-up" epochs during which no sampling is done (The whole super network is trained). After this warm-up phase, the probability of each edge is initialized and we start sampling architectures.

The real-valued distribution parameter associated with each layer (and used to generate the probability of sampling the edge) are all initialized to 3, resulting in a $\approx 0.95$ initial probability once passed through the sigmoid activation function.

On CIFAR-10 and CIFAR-100 datasets we train all models for 300 epochs. We start with a learning rate of $10^{-1}$ and divide it by 10 after 150 and 225 epochs. On Part Label dataset all models are trained for 200 epochs with a learning rate initialized to $10^{-1}$ and divided by 10 after 130 epochs.

For all models and all cost functions, we select the $\lambda$ hyper-parameter based on the order of magnitude $m$ of the maximum authorized cost $\mathbf{C}$. $\lambda$ is determined using cross-validation on values logarithmically spaced between $10^{m-1}$ and $10^{m+1}$.

\newpage

\subsection*{Forward algorithm}
Given the SS-Network $(E,\Gamma, \theta)$ and input $x$, the evaluation of $f(x,E, \Gamma,\theta)$ is done as follow :

\begin{algorithm}[ht] 
\caption{Stochastic Super Network forward algorithm}
\begin{algorithmic}[1]
\Procedure{SSN-forward}{$x,E,\Gamma,\theta$}
\State $H \sim \Gamma$ \Comment{as explained in Section \ref{secion_ssn}}
\For{$i \in [1..N]$}
\State $l_i \gets $\O 
\EndFor
\State $l_1 \gets x$
\For{$i \in [2..N]$}
\State $l_i \gets \sum\limits_{k<i} e_{k,i} h_{k,i} f_{k,i}(l_k)$
\EndFor 
\State \textbf{return} $l_N$
\EndProcedure
\end{algorithmic}
\label{algo2}
\end{algorithm}


\subsection*{Additional results}

\input{results/cifar10_allmodels_para4.tex}

\input{results/cifar10_allmodels_para1.tex}
\input{results/cifar100_resnets_para1.tex}

\input{results/cifar10_allmodels_para2.tex}
\input{results/cifar100_resnets_para2.tex}



\end{document}


