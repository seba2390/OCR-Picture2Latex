\documentclass{article} % For LaTeX2e
\usepackage[table]{xcolor}
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{pifont}


\title{Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{
    \textbf{Jinhua Liang\textsuperscript{1}}, 
    \textbf{Xubo Liu\textsuperscript{2}}, 
    \textbf{Wenwu Wang\textsuperscript{2}}, 
    \textbf{Mark D.~Plumbley\textsuperscript{2}},
    \textbf{Huy Phan\textsuperscript{3,*}}, 
    \textbf{Emmanouil Benetos\textsuperscript{1,4}} \\
    $\textsuperscript{1}$ Centre for Digital Music (C4DM), Queen Mary University of London, UK \\ 
    $\textsuperscript{2}$ Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, UK \\
    $\textsuperscript{3}$ Amazon, Cambridge, MA, USA \hspace{0.2cm} 
    $\textsuperscript{4}$ The Alan Turing Institute, UK \\
    \texttt{jinhua.liang@qmul.ac.uk}
}

% \author{     
%     \parbox{\linewidth}{\centering 
%     Xubo Liu\textsuperscript{1,$\dagger$},
%     \textbf{Zhongkai Zhu\textsuperscript{2}},
%     \textbf{Haohe Liu\textsuperscript{1,*}}, 
%     \textbf{Yi Yuan\textsuperscript{1,*}},
%     \textbf{Meng Cui\textsuperscript{1}}, 
%     \textbf{Qiushi Huang\textsuperscript{1}},} \\
%     \parbox{\linewidth}{\centering 
%     ~\textbf{Jinhua Liang\textsuperscript{3}}, 
%     \textbf{Yin Cao\textsuperscript{4}}, 
%     \textbf{Qiuqiang Kong\textsuperscript{5}},
%     \textbf{Mark D. Plumbley\textsuperscript{1}},
%     \textbf{Wenwu Wang\textsuperscript{1}}} \\\\
%     \parbox{\linewidth}{\centering 
%     ~\textsuperscript{1} University of Surrey ~\textsuperscript{2} Independent Researcher ~\textsuperscript{3} Queen Mary University of London}  \\
%     \parbox{\linewidth}{\centering 
%     ~\textsuperscript{4} Xi’an Jiaotong Liverpool University ~\textsuperscript{5} The Chinese University of Hong Kong} \\\\
%     \parbox{\linewidth}{\centering 
%     ~\textsuperscript{$\dagger$} Project Lead ~\textsuperscript{*} Equal Contribution}\\\\
%     \parbox{\linewidth}{\centering \url{https://Audio-AGI.github.io/WavJourney_demopage}}
% }

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. In this work, we introduce \textbf{A}coustic \textbf{P}rompt \textbf{T}urning (APT), a new adapter extending LLMs and VLMs to the audio domain by soft prompting only. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as language model inputs. To mitigate the data scarcity in the audio domain, a multi-task learning strategy is proposed by formulating diverse audio tasks in a sequence-to-sequence manner. Moreover, we improve the framework of audio language model by using interleaved audio-text embeddings as the input sequence. This improved framework imposes zero constraints on the input format and thus is capable of tackling more understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate the reasoning ability of audio networks, we propose natural language audio reasoning (NLAR), a new task that analyses across two audio clips by comparison and summarization. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. We finally demonstrate the APT's ability in extending frozen VLMs to the audio domain without finetuning, achieving promising results in the audio-visual question and answering task. Our code and model weights are released at \url{https://github.com/JinhuaLiang/APT}.\let\thefootnote\relax\footnotetext{\textsuperscript{*}The work does not relate to H.P.'s position at Amazon.}
\end{abstract}

\section{Introduction} \label{sec:introduction}
Auditory stimuli contribute to shaping the overall human perception experience. While visual language models (VLMs)~\citep{li_blip-2_2023,liu_visual_2023,dai_instructblip_2023,shukor_unified_2023,han_imagebind-llm_2023} that are capable of solving diverse down-streaming tasks have emerged driven by the advent of large language models (LLMs) and massive visual-text pretraining, only few of them~\citep{shukor_unified_2023,han_imagebind-llm_2023} can be adapted to the audio domain while maintaining their performance in the image/video domain.

To embrace more than two modalities, few works recently attempted to explore the diversity and heterogeneity of tasks and modalities. UniVAL~\citep{shukor_unified_2023} unified input/output format, model architecture, and training objective, and therefore, learned a shared encoder-decoder LLM with multi-modal curriculum learning. ImageBind-LLM~\citep{han_imagebind-llm_2023} adopted ImageBind, a cross-modal encoder bundling six modalities (including images) to a shared embedding space, and adapted the LLM with a frozen image encoder. While both works extended visual LLMs to other domains, in addition to the considerable amount of training data, they are bundled to a specific architecture, hindering the ability to adapt them to a new modality.

Meanwhile, following the VLM framework, a few works proposed audio-only LLMs where a pair of audio clip and text token are used as inputs for text generation. LTU~\citep{gong_listen_2023} bridged audio with language modalities by end-to-end finetuning on an instruction-based dataset. Pengi~\citep{deshmukh_pengi_2023} applied multi-task learning to leverage off-the-shelf datasets, alleviating the data-scarcity issue. Still, they are restricted to two domains (i.e., audio and language). They also cannot address tasks beyond the [audio, question, answer] format, e.g., few-shot audio classification~\citep{liang_adapting_2023}. One question thereby arises: \textit{Can we adapt LLMs/VLMs to the audio domain by simply encoding sound clips as acoustic prompts?}

In this work, we introduce APT (\textbf{A}coustic \textbf{P}rompt \textbf{T}uning), an acoustic adapter that extends LLMs and VLMs to audio understanding and reasoning tasks using soft prompts only. Specifically, APT encodes audio clips into audio feature maps and then uses an audio aligner to generate acoustic prompts conditioned on both input instructions and the audio feature maps. When training APTs, a multi-task learning strategy is adapted by formulating diverse audio tasks in a sequence-to-sequence format. Besides popular audio tasks (such as audio tagging and audio captioning), APT makes full use of publicly-available datasets by training on three new tasks, namely query-based sound event detection, temporal event retrieval, and sound event counting, to learn fine-grained audio features. In addition, we improve the audio language model framework by juxtaposing acoustic prompts with text embeddings. Rather than applying soft prompts as a prefix to the input texts, the improved framework exerts no constraints on the format of the input sequence. Therefore, the APT-enhanced LLMs, namely APT-LLM, can analyse multiple audio clips in a single feed-forward process, facilitating more audio understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate models' reasoning ability, we propose a new task referred to as natural language audio reasoning (NLAR) which is devised to distinguish, compare, and summarise two audio clips. Experiments on existing audio understanding tasks, including audio tagging, audio captioning, and few-shot audio classification, show that APT-LLM achieves performance on par with those obtained by audio language models or even domain-expert models. APT also yields a good performance on the proposed NLAR, indicating its capacity to comprehend over a single audio clip. Finally, quantitative studies are conducted to demonstrate that APT improves the performance of a VLM in the audio-visual question and answering (AVQA) task.

Our contributions are summarized as below:
\begin{itemize}
    \item An acoustic adapter is introduced to extend LLMs and VLMs to the audio modality by soft prompting. To mitigate data scarcity in the audio domain, we improve the present multi-task training approach by devising new tasks and their corresponding prompts during training. Leveraging the annotations in off-the-shelf databases, APT-LLM learns acoustic embeddings with fine-grained features from task discrepancy.
    
    \item APT formulates diverse audio tasks as a sequence-to-sequence task where generated text is conditioned on interleaved audio-text tokens. Without any constraints on the input format, APT-LLM is not only able to solve different tasks according to the diverse instructions, but also to exploit the correlation among different audio clips in the same sequence. To the best of our knowledge, APT-LLM is the first audio-language model reasoning beyond a single audio clip.
    
    \item Natural language audio reasoning, a new audio comprehension task, is proposed to distinguish, compare, and summarise two audio clips. Compared to existing audio tasks, this new task not only evaluates model ability to understand an audio clip, but also requires models to analyse the content of two recordings by comparison and summarisation. APT-LLM is then benchmarked on this task.
    
    \item BLIP-2~\citep{li_blip-2_2023} coupled with APT (namely APT-BLIP-2) is studied qualitatively and quantitatively on the audio-visual question and answering task~\citep{yang_avqa_2022}. Without further finetuning, APT-BLIP-2 can work with the visual modality directly, showcasing an efficient approach for extending multi-modal LLMs to a new modality.

\end{itemize}
\section{Related works} \label{sec:related_works}
\textbf{Multimodal language models.}
From recent advances, LLMs~\citep{touvron_llama_2023,chiang_vicuna_2023,openai_gpt-4_2023} has exhibited astonishing comprehending and reasoning capacity. Driven by the open-world knowledge in LLMs, a variety of visual language models have been proposed with different alignment methods to integrate image/video data to text tokens~\citep{alayrac_flamingo_2022,li_blip-2_2023,dai_instructblip_2023,zhang_llama-adapter_2023}. However, most of them are restricted to the visual domain, largely due to the lack of training data in other domains (such as audio) and modality discrepancies. Recently, ImageBind-LLMs~\citep{han_imagebind-llm_2023} bridged the image encoder of ImageBind~\citep{girdhar_imagebind_2023}, a six-modality language model, with an LLM and used visual tokens as soft prompts within the language model. UniVAL~\citep{shukor_unified_2023} uniformed the input/output, the architecture, and the training object of multimodal LLMs and then devised a curriculum learning for gradual exposure to new modality. While both works adapted VLMs to other domains, they demands massive multimodal data to train the overall networks from scratch. Instead, this work investigates a domain-specific adapter that can be applied to extend any existing VLM/LLM to an additional modality (such as audio). 

\textbf{Audio language models.} Following VLM, some works built audio language models for sound-only tasks. SpeechGPT~\citep{zhang_speechgpt_2023} collected a speech-text instruction dataset, thereby learned to perceive and generating speech content in the audio. LTU~\citep{gong_listen_2023} rendered an open-end dataset, containing 3.7M [audio, question, answer] tuples, and learned with a perception-to-understanding curriculum. While the aforementioned models achieved a good audio comprehension ability, they required a uniform input format as a triplet tuple. To work around this question, Pengi~\citep{deshmukh_pengi_2023} proposed a multi-task framework where an audio language model is trained with off-the-shelf audio datasets by prompted with different predefined questions. This work differs from these prior works in three-fold: 1) Rather than an audio-only language model, APT explores how to adapt existing VLMs and LLMs to the sound domain; 2) APT-LLM improves the multi-task framework by designing three new training tasks. By accessing existing datasets from different aspects, APT-LLM learns a fine-grained audio representation, and 3) APT-LLM re-frames the present input format, namely [audio, question, answer], to let audio and text alternate in a sequence. In this way, APT-LLM is able to ingest more than one audio clip in a single feed-forward, unleashing it to more audio tasks. To the best of the knowledge, APT-LLM is the first model that integrates in-context learning with multi-task training.

\section{Method} \label{sec:APT LLMs}
Current audio LLMs~\citep{gong_listen_2023,deshmukh_pengi_2023} learned to bridge audio with language by framing popular audio tasks (e.g., classification and captioning tasks) to the audio-conditioned text generation problem. Going beyond the [audio, question, answer] format, APT-LLM encodes multiple audio clips in one feed-forward process and juxtaposes them with text embeddings without any order constraint. The more flexible training paradigm mitigates the need for high-quality data and massive databases, and thus reduces required computations. Moreover, juxtaposing audio clips with texts enables APT-LLM to address more comprehensive reasoning tasks, such as natural language audio reasoning. We first discuss the overall architecture of APT-LLM in Section~\ref{subsec:architecture}, and then elaborates APT-LLM learning objective in Section~\ref{subsec:learning_objective} and the training recipe in Section~\ref{subsec:multi_task_learning}. In Section~\ref{subsec:audio_reasoning_task}, we define the natural language audio reasoning task, a new task to evaluate the audio comprehension ability of models.
 
\subsection{Architecture} \label{subsec:architecture}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.8]{src/lam.pdf}
    \caption{Illustration of the proposed APT-LLM. APT-LLM is constituted by three components: an audio encoder, an audio aligner, and a LLM. The audio encoder extracts audio feature maps from input spectrograms. The audio aligner then projects each audio feature map to 32 acoustic embeddings according to the input text. These acoustic embeddings, together with the added embeddings of the audio token ``$<$AUDIO$>$'', are juxtaposed with text embeddings. The interleaved audio-text embeddings are fed into the LLM to generate the output text. APT-LLM can ingest multiple audio clips in a sequence and thus benefit from diverse tasks during training.}
    \label{fig.:lam}
    \vspace{-0.3cm}
\end{figure}

The overall structure of APT-LLM is illustrated in Figure~\ref{fig.:lam}, with main components of an audio encoder, an audio aligner, and a large language model. APT-LLM alternates audio clips with text tokens without any format constraints and thus benefits from task diversity and large-scale pretraining.

\textbf{Audio encoder: from spectrograms to feature maps.} We use Audio-MAE~\citep{huang_masked_2022}, a vanilla 12-layer transformer encoder that learns to reconstruct randomly-masked spectrogram patches during training, as the audio encoder. Rather than using the last layer that finetuned for classification tasks, we apply the output feature map from the penultimate block of an Audio-MAE to encode fine-grained patterns in the sound.

\textbf{Audio aligner: from 10-second feature maps to a fixed number of audio tokens.} This module connects the audio encoder to the frozen language model as shown in Figure~\ref{fig.:lam}. It ingests a text prompt together with a variable number of audio feature maps extracted by the audio encoder as input and produces a fixed number of acoustic embeddings. Following the implementation of \citep{li_blip-2_2023}, four transformer blocks constitute our audio aligner where 32 trainable embeddings attend to the input text tokens and extract the relevant information from the audio feature maps. Resampling a varying-length of audio embeddings to 32 acoustic embeddings, APT aligner reduces the computational complexity in following attention mechanism while filtering out the information irrelevant to the input text. 

\textbf{Large language model: from interleaved audio-text tokens to generated text.} The language model predicts the output text by taking into account the previous generated texts and the input audio-text tokens. We freeze all parameters in the language model during training. In addition to existing works~\citep{li_blip-2_2023,gong_listen_2023}, we add before each audio clip a learnable audio token, ``$<$AUDIO$>$'', as a special token to indicate the beginning of audio tokens. We find this token helps the language model to distinguish audio tokens from text tokens when interleaving them together.

\subsection{Learning objective}  \label{subsec:learning_objective}
In order to motivate our training framework, we first present the learning objective used in existing work~\citep{deshmukh_pengi_2023}. Let an audio-text pair in [audio, question, answer] format be referred to as $(a, t, g)$ where $a$, $t$, $g$ are the audio clip, input text, and output text, respectively, and $\mathbf{X}$ be input sequential embeddings to the language model. To align the audio modality to the language modality, an audio encoder $\mathcal{A}$ and an audio aligner $\mathcal{M}$ project the audio $a$ into a sequence $\mathbf{X}_{\mathrm{audio}}$:
\begin{equation} \label{eqn.:extract_audio}
    \mathbf{X}_{\mathrm{audio}} = \mathcal{M}_{\theta}(\mathcal{A}_{\phi}(a, t)),
\end{equation}
where $\phi$ and $\theta$ are the parameters of the audio encoder $A$ and the aligner $\mathcal{M}$. The audio embeddings are used as a prefix and then concatenated with the input text embeddings as
\begin{equation} \label{eqn.:concat}
    \mathbf{X}_{\mathrm{audio;text}} =  \mathcal{C}(\mathbf{X}_{\mathrm{audio}}, \mathbf{X}_{\mathrm{text}}) = \mathcal{C}(\mathcal{M}_{\mathcal{\theta}}(\mathcal{A}_{\phi}(a)), \mathcal{W}_{\psi}(t)),
\end{equation}
where $\mathcal{C}$ is a concatenating function and $\psi$ denotes the parameters of the word embedding layer $\mathcal{W}$ in the language model. Assuming the length of the concatenated embeddings $\mathbf{X}_{\mathrm{audio;text}}$ be $L$, the parameters of the audio LLM are optimised by measuring the probability distribution of the next token conditioned on its previous tokens:
\begin{equation} \label{eqn.:next_token_pred}
    p(\mathbf{X}_{\mathrm{pred}}|\mathbf{X}_{\mathrm{audio}}; \mathbf{X}_{\mathrm{text}}) = \prod_{i=L+1}^{L+|g|}p_{\phi,\theta,\psi}(\mathbf{x}_i|X_{\mathrm{audio;text},<i}; \mathbf{X}_{\mathrm{pred},<i}),
\end{equation}
In this way, prevailing LLMs are able to unify many audio-to-text tasks in a sequence-to-sequence manner. However, not all understanding tasks can be fitted into the format of [audio, question, answer] (e.g., to learn a new concept using a handful of labelled audio examples), calling for a new paradigm that can exploit diverse tasks in a uniform input/output format.

We thereby propose a new learning framework in which interleaved audio-text embeddings are used as the LLM's input such that the model is able to leverage and learn from more diverse tasks during training. Let $\mathbf{a}$ and $\mathbf{t}$ be audio clips and input text, and $g$ still be output text. Assuming both $\mathbf{a}$ and $\mathbf{t}$ have $N$ different elements, the input audio-text pairs are denoted as $[(a^i, t^i)]_{i=1}^N$ where $a^i$ and $t^i$ are the $i$-th audio clip and input text, respectively. Eqn. (\ref{eqn.:concat}) can be re-written as
\begin{equation} \label{eqn.:interleave}
    \mathbf{X}_{audio;text}\!=\!\mathcal{I}(\mathbf{X}_{audio}, \mathbf{X}_{text}) = [\mathcal{M}(\mathcal{A}_{\phi}(a_{1}, t_{1})), T_{\psi}(t_{1}), \ldots, \mathcal{M}(\mathcal{A}_{\phi}(a_{N}, t_{N})), T_{\psi}(t_{N})],
\end{equation}

where $\mathcal{I}$ is the function that alternating acoustic embeddings with text embeddings. In this way, APT-LLM can integrate multiple audio clips in the input sequence, enabling itself to learn from more audio understanding tasks.

% Before introducing our training framework, we summarise the learning objective in the previous work. Let the $L$ audio-text pairs in [audio, question, answer] format be referred to as $[(a^i, t^i, g^i)]_{i=1}^L$ where $a^i$, $t^i$, $g^i$ are the audio clip, input text, and output text of the $i^{th}$ tuple, respectively, and $\mathbf{X}$ be the input sequence to the language model. To align the audio modality to the language modality, an audio encoder $E_{\phi}$ and an audio aligner $m$ project the audio $a^i$ into a sequence $\mathbf{X}_{\mathrm{audio}}$ of $k$ embeddings:
% \begin{equation} \label{eqn.:extract_audio}
%     \mathbf{X}_{\mathrm{audio}} = [X_1^i,\cdots,X_k^i] = m(E_{\phi}(a^i)),
% \end{equation}
% The audio embeddings are used as a prefix and then concatenated with input text embeddings as
% \begin{equation} \label{eqn.:concat}
%     \mathbf{X}_{\mathrm{audio;text}} =  \mathcal{C}(\mathbf{X}_{\mathrm{audio}}, \mathbf{X}_{\mathrm{text}}) = \mathcal{C}(m(E_{\phi}(a^i)), E_{\epsilon}(t^i)),
% \end{equation}
% where $\mathcal{C}$ is a function concatenating two elements together and $E_{\epsilon}$ is the word embedding layer of the language model.
% The learning objective of the audio LLM is thus measured as
% \begin{equation} \label{eqn.:next_token_pred}
%     p(\mathbf{X}_{\mathrm{pred}}|\mathbf{X}_{\mathrm{audio}}; \mathbf{X}_{\mathrm{text}}) = \prod_{i=1}^{L}p_{\theta}(\mathbf{x}_i|X_{\mathrm{audio;text}},<i; \mathbf{X}_{\mathrm{pred}},<i),
% \end{equation}
% In this way, existing LLMs unify the diverse audio tasks and learn to predict the next token. However, the format of [audio, question, answer] cannot cover all understanding tasks (e.g., to learn a new concept using a handful of labelled audio examples), calling for a new paradigm that can exploit more tasks in a uniform input/output format.

% We propose a new learning framework where the text tokens alternated with audio embeddings are used as the LLM input such that the model is able to learn from more comprehensive tasks during training. Assume that an audio LLM learns in a $N$-way 1-shot setting, i.e. there are $N$ categories to classify and one labelled example is associated with one category. The audio-text pairs are then denoted as $\mathbf{a}^i$, $\mathbf{t}^i$, $g^i$ where $\mathbf{a}^i$ and $\mathbf{t}^i$ consist of $N$ different audio clips and texts, respectively, and $g^i$ is the targeted answer of the $(N+1)$-th data. Eqn. (\ref{eqn.:concat}) can be re-written as:
% \begin{equation} \label{eqn.:interleave}
%     \mathbf{X}_{audio;text}\!=\!\mathcal{I}(\mathbf{X}_{audio}, \mathbf{X}_{text}) = [m(E_{\phi}(a_{1}^i)), E_{\epsilon}(t_{1}^i), \ldots, m(E_{\phi}(a_{N}^i)), E_{\epsilon}(t_{N}^i)],
% \end{equation}

% where $\mathcal{I}$ is the function that alternating acoustic tokens with texts.
% Of note, the function $\mathcal{I}$ follows the order of input sequence and thus not necessarily puts audio recordings and texts in the stagger arrangement.

\begin{table}[]
\centering
\caption{Multi-task learning strategy adopted by APT-LLMs. ``\#Audio samples'' denote the number of audio clips in the dataset. Stage 0-2 denotes audio-text alignment, learning from single audio clips, and learning from multiple clips, separately.}
\label{tab.:multi-task_learning}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multirow{2}{*}{Task} &
  \multicolumn{3}{c}{Training stages} &
  \multirow{2}{*}{Dataset} &
  \multirow{2}{*}{\#Audio samples} &
  \multirow{2}{*}{Durations} &
  \multirow{2}{*}{Setup} \\ \cmidrule(lr){2-4}
                                  & 0 & 1 & 2 &             &      &       &            \\ \midrule
Audio tagging                     & \ding{51}  & \ding{51}  & \ding{51}  & AudioSet    & 2M   & 5.8kh & train/test \\ \midrule
                                  &   &   &   & Wavcaps     & 400k & 7.6kh &            \\
Audio captioning                  & \ding{51}  & \ding{51}  & \ding{51}  & AudioCaps   & 39k  & 108h  & train/test \\
                                  &   &   &   & Clotho v2   & 7k   & 31h   &            \\ \midrule
Audio question and answering      &   & \ding{51}  & \ding{51}  & Clotho AQA  & 2k   & 12h   & train      \\ \midrule
Query-based sound event detection &   & \ding{51}  & \ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\ \midrule
Temporal event retrieval          &   & \ding{51}  & \ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\ \midrule
Sound event counting              &   & \ding{51}  & \ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\ \midrule
Few-shot audio classification     &   &   & \ding{51}  & AudioSet    & 2M   & 5.8kh & train/test \\ \midrule
Natural language audio reasoning  &   &   & \ding{51}  & NLAR        & 0.2k & 1.2h  & train/test \\ \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}

\subsection{Multi-task learning strategy}  \label{subsec:multi_task_learning}
With the uniform input/output format, APT-LLM is able to learn from a large variety of audio tasks and thus benefiting from the diverse training datasets. As shown in Fig.~\ref{tab.:multi-task_learning}, instead of passing through all training data directly, APT-LLM is trained through:

\textbf{Audio-text alignment.} Before coupled with a LLM, we pretrain APT audio aligner to bridge the audio modality and the text modality. To this end, we freeze the other components and optimise parameters of the audio aligner with audio-text pairs from AudioSet~\citep{gemmeke_audio_2017} and WavCaps~\citep{mei_wavcaps_2023}. During training, a fixed number of acoustic embeddings are learnt to extract relevant information from the audio feature maps according to the input text tokens. Following~\cite{li_blip-2_2023}, the audio aligner learns with triplet training objectives: audio-text matching, Audio-grounded text generation, and audio-text contrastive (See more in Appendix~\ref{appendix:audio-text_alignment})

\textbf{Learning from single audio clip.} 
After APT has extracted acoustic embeddings according to the input text, the following LLM learns to project these tokens to the word embeddings of the targeted LLM. APT-LLM is thus trained with multiple tasks using various prompts (see more in Appendix~\ref{appendix:multitask_prompt}). In addition to existing audio tasks, namely audio tagging, audio captioning, and audio question and answering, we design three new tasks: (1) \textit{Query-based sound event detection} that aims to train a model to predict the onset and offset time of a specific sound event; (2) \textit{Temporal event retrieval} that is to recognise sound events occurred in a specific period, and (3) \textit{Sound event counting} that requires a model to count the frequency of a specific sound event in a recording. Instead of rendering datasets, we exploit the publicly-available AudioSet with strong labels~\citep{hershey_benefit_2021} using different prompts (see more in~\ref{appendix:multitask_prompt}). This multi-task framework facilitates APT-LLM's learning from diverse datasets, including AudioSet~\citep{gemmeke_audio_2017}, WavCaps~\citep{mei_wavcaps_2023}, AudioSet with strong labels~\citep{hershey_benefit_2021}, Clotho~\citep{drossos_clotho_2020}, AudioCaps~\citep{kim_audiocaps_2019}, and Clotho-AQA~\citep{lipping_clotho-aqa_2022}.

\textbf{Learning from multiple audio clips.}
In addition to the aforementioned tasks, APT-LLM learns from two additional tasks by juxtaposing more than one audio clips with input text. Specifically, few-shot audio classification and natural language audio reasoning are added to the multi-task training framework in this stage. On the one hand, for the few-shot audio classification, APT-LLM predicts labels of sound events by exploiting the correlation between input audio clips. On the other hand, APT-LLM is required to compare and summarise two different sounds in the natural language audio reasoning task (see the following Section~\ref{subsec:audio_reasoning_task}). Trained on these two tasks, APT-LLM learns to analyse beyond a single recording and answer questions as per input questions. We adopts AudioSet~\citep{gemmeke_audio_2017} and the proposed datasets for few-shot audio classification and natural language audio reasoning, respectively.
% \subsection{Training recipe} \label{subsec:unified_pretraining_tasks}
% With the uniform input/output format, APT is able to learn from a large variety of audio tasks and thus benefiting from the diverse training datasets. Instead of passing through all training data directly, APT is trained through three stages:

% \textbf{Stage 1: Audio-text alignment.} 
% APT is pretrained with AudioSet~\citep{gemmeke_audio_2017} and WavCaps~\citep{mei_wavcaps_2023} such that the audio information extracted from query tokens is aligned with the input textual embeddings in the audio aligner. Following the implementation in~\citep{li_blip-2_2023}, only APT's audio aligner gets updated while the rest of its components are frozen in this stage. APT's parameters are optimised using triplet losses, including Audio-Text Matching (ATM) loss, Audio-Text Contrastive (ATC) loss, and Audio Grounded-Text Generation (AGTG) loss (pretraining loss details can be found in Appendix~\ref{appendix:APT_pretraining}.)

% \textbf{Stage 2: Multi-task training.} 
% After APT has extracted acoustic embeddings according to the input text, it learns to project these tokens to the word embeddings of the targeted LLM. APT is thus trained with multiple tasks using various prompts (see more in Appendix~\ref{appendix:multitask_prompt}). In addition to existing audio tasks, namely audio tagging, audio captioning, and audio question and answering, we design three new tasks: (1) \textit{Query-based sound event detection} that aims to train a model to predict the onset and offset time of a specific sound event; (2) \textit{Temporal event retrieval} that is to recognise sound events occurred in a specific period, and (3) \textit{Sound event counting} that requires a model to count the frequency of a specific sound event in a recording. Instead of rendering datasets, we exploit the publicly-available AudioSet with strong labels~\citep{hershey_benefit_2021} using different prompts (see more in~\ref{appendix:multitask_prompt}). This multi-task framework facilitates APT's learning from diverse datasets, including AudioSet~\citep{gemmeke_audio_2017}, WavCaps~\citep{mei_wavcaps_2023}, AudioSet with strong labels~\citep{hershey_benefit_2021}, Clotho~\citep{drossos_clotho_2020}, AudioCaps~\citep{kim_audiocaps_2019}, and Clotho-AQA~\citep{lipping_clotho-aqa_2022}.

% \textbf{Stage 3: Learning to analyse multiple recordings.}
% Based on the second-stage pretraining, APT learns from two additional tasks by juxtaposing more than one audio clips with input text. Specifically, few-shot audio classification and natural language audio reasoning are added to the multi-task training framework in this stage. On the one hand, for the few-shot audio classification, APT predicts labels of sound events by exploiting the correlation between input audio clips. On the other hand, APT is required to compare and summarise two different sounds in the natural language audio reasoning task (see the following Section~\ref{subsec:audio_reasoning_task}). Trained on these two tasks, SWIFT learns to analyse beyond a single recording and answer questions as per input questions. We adopts AudioSet~\citep{gemmeke_audio_2017} and the proposed datasets(Appendix~\ref{appendix:nlar}) for few-shot audio classification and natural language audio reasoning, respectively.

\subsection{Natural language audio reasoning task} \label{subsec:audio_reasoning_task}

\begin{table}[t]
\centering
\caption{An example demonstrating APT-LLM's capacity of audio reasoning. It requires audio networks to comprehend recordings and reasoning across multiple recordings.}
\label{tab.:nlar_example}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{2}{l}{Natural Language Audio Reasoning (NLAR) example: \textit{``Where is the sudden sound?''}} \\ \midrule
User         &  \\
             & \includegraphics[width=1.0\columnwidth]{src/wav.pdf} \\
             & Question: Which recording has a more sudden and startling sound event?             \\  \midrule
APT-LLM      & First.      \\
Ground truth & first       \\ \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}

One of the complex reasoning ability of human is to learns across different pieces of sounds, understanding what happening in each audio and analysing the content of different audio by comparison and summarisation. However, existing audio tasks focus on analysing acoustic scenarios in an independent recording by recognising the inside sound events~\cite{kong_panns_2020} and/or retrieval their spatio-temperal information~\cite{politis_starss22_2022}. We thus propose natural language audio reasoning (NLAR), a new task where the model is required to answer questions by explicitly comparing or summarising two different audio recordings. Table~\ref{tab.:nlar_example} showcases an example in the NLAR task. An audio system takes two audio clips together with a free-form text query as input and is expected to answer the question by taking into consideration the both audio. For details of the design process and examples of the proposed natural language audio reasoning task, please refer to Appendix~\ref{appendix:nlar}. Compared to existing audio tasks, the proposed audio reasoning task features three notable differences:

\textbf{Comprehension of multiple audio clip}: This task requires a model to answer open-ended questions by comparing or summarising the content of two different audio clips. The model must first comprehend the two audio clips as per the raised question separately and answer the question by taking into account the two audio inputs. An example of the audio reasoning task can be found in Table~\ref{tab.:nlar_dataset}. 

\textbf{Diverse question types}: Questions for natural language audio reasoning task assess diverse auditory aspects, such as the presence, the frequency, and acoustic features of sound events. Therefore, the model should not only ground the sound events in the recordings, but also retrieve relevant information as per the input question. 

\textbf{Effects of the chronological order}: Compared to existing audio tasks, e.g.,~\citep{li_blip-2_2023} and~\citep{gong_listen_2023}, the proposed audio reasoning task emphasises the order of the audio recordings in a sequence. In other word, the answer associated with the audio pair ``[Audio A, Audio B]'' could be different with the answer associated with  ``[Audio B, Audio A]'' when their questions are the same. In this way, we expect audio understanding models to be able to attend to different portions of the input sequence when the question vary. 

By evaluating audio language models on the natural language audio reasoning task, we achieve more comprehensive assessment of audio language models.

\section{Experiments} \label{sec:experiments}
APT was first coupled with LLMs (i.e., APT-LLM) and evaluated as a general-purposed audio learner on a variety of existing audio-related benchmarks, including audio tagging, audio captioning, and few-shot audio classification. To further assess its ability in comprehending two audio clips of interest, APT-LLM was further benchmarked on the natural language audio reasoning task. In addition to audio comprehension, we also experimented and analysed (quantitatively and qualitatively) APT as an zero-shot adapter to BLIP-2~\citep{li_blip-2_2023,dai_instructblip_2023}, a state-of-the-art VLM.

\begin{table}[t]
\centering
\caption{Zero-shot performance comparison with audio language models. We group the methods in terms of their training strategy. ``\#Params.'' denotes the number of trainable parameters and ``\#Pairs'' represents the number of audio-text pairs. $\uparrow$ indicates the higher number, the better performance.}
\vspace{1em}
\label{tab.:existing_tasks}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llllcc@{}}
\toprule
Model & \#Params. & \#Pairs & AudioSet (mAP$\uparrow$) & AudioCaps (SPICE$\uparrow$) & Clotho (SPICE$\uparrow$) \\ \midrule
\textit{Audio-language models trained with the contrastive loss} &        &      &      &      &      \\
AudioCLIP~\citep{guzhov_audioclip_2022}                 & 30M    & 2M   & 25.9 & -    & -    \\
CLAP~\citep{elizalde_clap_2023}                         & 190M   & 128k & 5.8  & -    & -    \\ \midrule
\textit{One-for-all models for various audio tasks}              &        &      &      &      &      \\
LTU~\citep{gong_listen_2023}                            & 96M    & 5.7M & 18.5 & 17.0 & 11.9 \\
Pengi~\citep{deshmukh_pengi_2023}                       & $>$191M & 3.4M & -    & 18.2 & 12.6 \\
\rowcolor{lightgray!60}APT-LLM                          & 101M   & 2.6M & 14.7 & 17.1 & 11.6 \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Experiment setup} \label{subsec:experiment_setup}
Our models were implemented relying on the BLIP-2 framework~\citep{li_blip-2_2023}. We used Audio-MAE~\citep{huang_masked_2022} as the audio encoder in all APT models we developed. Considering Audio-MAE only contains 100M parameters, we used a two-layer transformer as the aligner to bridge the audio and text domains. Without an explicit statement, we coupled APT with Vicuna 7B v1.1~\citep{chiang_vicuna_2023} for evaluation. We testified APT-LLM with two close-ended datasets: AudioSet~\citep{gemmeke_audio_2017} and ESC-50~\citep{piczak_esc_2015}; and four open-ended datasets: Clotho~\citep{drossos_clotho_2020}, AudioCaps~\citep{kim_audiocaps_2019}, natural language audio reasoning (in Section~\ref{subsec:audio_reasoning_task}), and audio-visual question and answering (AVQA)~\citep{yang_avqa_2022}.

Adam optimiser was used for model training. We applied Warmup strategy in the first 2K steps and used a cosine linear learning rate in the following steps. We trained the APT models using three NVIDIA A100 (40G) GPUs. The audio-text alignment pretraining and multi-task training took 5 days separately.

\subsection{Comparison with existing approaches} \label{subsec:comparison_with_existing_approaches}
We compare APT-LLM against the state-of-the-art specialised systems (i.e., the networks trained with task-specific data) and previous audio language models on existing tasks, including audio tagging, audio captioning, and few-shot audio classification.

\textbf{Audio tagging} requires models to predict classes of test samples from a predefined label set. We evaluated the models on the AudioSet dataset~\citep{gemmeke_audio_2017}. During inference, APT-LLM was prompted using the sentence ``\textit{Summarize the audio with key words.}'' Since APT generates free-form texts directly, we used the APT text encoder pretrained in the stage 1 to encode generated answers and the given classes names to text embeddings. Afterwards, cosine similarity is calculated as the classification probably.
Consistent with the findings in previous work~\citep{gong_listen_2023}, Table~\ref{tab.:existing_tasks} shows a performance gap between audio language models and task-specific models. This is expected since the latter addresses the classification task as a close-end problem, with much lower complexity than open-ended problem where models need to search across the entire word embedding space. In addition, we found that the performance of the text encoder greatly impacts the classification result when evaluating the generated answers. This finding can be explained by the fact that  word embeddings of different classes should be sparse enough to when measuring their distance to the embeddings of generated answers.

\begin{table}[t]
\centering
\caption{Performance comparison in audio captioning tasks. $\uparrow$ indicates the higher number, the better performance.}
\label{tab.:existing_tasks}
\vspace{1em}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
Model                                                   & \multicolumn{2}{c}{AudioCaps}   & \multicolumn{2}{c}{Clotho}      \\ \cmidrule(l){2-5} 
                                                        & SPICE $\uparrow$ & SPIDEr $\uparrow$ & SPICE $\uparrow$ & SPIDEr $\uparrow$ \\ \midrule
\textit{Specialised systems trained with task-specific examples} &                &                &                &                \\
PANNs-BART~\citep{xu_investigating_2021}                & 0.153          & 0.183          & 0.083          & 0.127          \\
CNN-GPT2~\citep{kim_prefix_2023}                        & 0.167          & \textbf{0.438} & 0.111          & 0.215          \\
WSAC+PD~\citep{kouzelis_weakly-supervised_2023}         & 0.173          & 0.403          & 0.123          & 0.247          \\ \midrule
\textit{One-for-all models for various audio tasks}              &                &                &                &                \\
\rowcolor{lightgray!60}APT-LLM                          & \textbf{0.191} & 0.402          & \textbf{0.132} & \textbf{0.248} \\ \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}

\textbf{Audio captioning} is the task where models are supposed to generate free-form description according to an input recording. The sentence ``\textit{Describe the audio clip concisely}.'' is applied as the input prompt. We finetune APT-LLM two epochs on the training split of AudioCaps~\citep{kim_audiocaps_2019} and Clotho~\citep{drossos_clotho_2020} datasets and compare it with the captioning models trained on the both tasks. As shown in Table~\ref{tab.:existing_tasks}, APT-LLM achieves the best performance on both AudioCaps and Clotho datasets in terms of SPICE and SPIDEr.

% \begin{table}[h]
% \centering
% \caption{Performance comparison in the audio tagging task. The best results are highlighted in \textbf{bold}.}
% \begin{tabular}{@{}lll@{}}
% \toprule
% Model                 & ESC-50 (Acc.) & AudioSet-2M (mAP) \\ \midrule
% Best fully-supevised  & 97.0          & 47.3              \\
% AudioCLIP             & 69.4          & 25.9              \\
% CLAP                  & 82.6          & 5.8               \\
% LTU                   & 82.8          & 18.5              \\
% LTU (w/o LLM adapter) & 70.8          & 14.7              \\
% Pengi                 & 92.0          &                   \\
% APT LLMs            &               & 14.7              \\ \bottomrule
% \end{tabular}
% \end{table}


% \begin{table}[h]
% \caption{Performance comparison in the audio captioning task. The best results are highlighted in \textbf{bold}.}
% \centering
% \begin{tabular}{@{}lll@{}}
% \toprule
%                        & AudioCaps & Clotho \\ \midrule
% Best supervised models & 17.7      & 13.5   \\
% LTU                    & 17.0      & 11.9   \\
% Pengi                  & 18.2      & 12.6   \\
% Our purposed           & 17.1      &        \\ \bottomrule
% \end{tabular}
% \end{table}

\begin{table}[h]
\centering
\caption{Accuracy (\%) of various methods on ESC-50 in the few-shot settings.}
\label{tab.:few_shot}
\begin{tabular}{llc}
\toprule
                                          & \multicolumn{2}{c}{Accuracy$\uparrow$} \\ \hline
                                          & 5-way          & 12-way         \\ \cline{2-3}
\multicolumn{3}{l}{\textit{Specialised systems trained with task-specific examples}} \\
ProtoNet~\citep{snell_prototypical_2017}   & 88.2          & 77.7          \\
MatchNet~\citep{vinyals_matching_2016}     & 86.8          & 71.8          \\
HPN~\citep{liang_leveraging_2022}          & 88.7          & 78.7          \\ \midrule
\multicolumn{3}{l}{\textit{Audio language models trained with constractive learning}} \\
TIP-adapter~\citep{zhang_tip-adapter_2022} & 97.5          & 95.6          \\
Treff adapter~\citep{liang_adapting_2023}  & 98.5          & 96.3          \\ \midrule
\multicolumn{3}{l}{\textit{One-for-all models for various audio tasks}} \\
\rowcolor{lightgray!60}
APT-LLM                                & 91.0          & 54.2          \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Few-shot audio classification} is to classify test audio clips using labelled audio examples. Models were evaluated in the $N$-way $K$-shot problem where: (1) there are $N$ classes in the classification task, and (2) each class contains $K$ different audio examples. Following previous works~\citep{liang_adapting_2023} in this task, we tested APT-LLM in the 5/12-way 5-shots settings. In our free-form query design, we prompt the few shot classification question by adding the query audio clip together with the input text ``\textit{This is a sound of}'' to the end of the sequence of labelled audio examples and their corresponding label texts. We implemented the same evaluation protocol to all few-shot learners for a fair comparison. As shown in Table~\ref{tab.:few_shot}, APT-LLM outperforms the task-specific models~\citep{snell_prototypical_2017,vinyals_matching_2016,liang_leveraging_2022} in the 5-way 5-shot setting while having a competitive performance compared to CLAP Adapters~\citep{zhang_tip-adapter_2022,liang_adapting_2023}. In the 12-way 5-shot problem, however, we can observe a performance degradation of APT. We suspect this may be due to the limitation of attention mechanism in LLMs when addressing very long sequences (12-way 5-shot modelling results in a sequence of roughly 2420 tokens). It should be noted that while APT-LLM was trained with 4-way 1-shot tasks, it can generalise to other few-shot settings, suggesting that APT-LLM learns to act as a few-shot classifier rather than memorising the expected answers.

\subsection{Evaluation on natural language audio reasoning} \label{subsec:evaluation_on_nlar}
\begin{wraptable}{r}{6.3cm}
    % \centering
    \caption{Benchmarking APT on the natural language audio reasoning task.}
    \label{tab.:audio_reasoning}
    \begin{tabular}{lc}
    \toprule
    Model               & Accuracy$\uparrow$ (\%)         \\ \hline
    the baseline        & 29.9              \\
    APT-Vicuna v1.1 & 62.9              \\
    APT-Vicuna v1.5 & \textbf{63.8}     \\ \bottomrule
    \end{tabular}
\end{wraptable}

Since APT-LLM is able to ingest multiple audio clips in a single feed-forward process, we investigated APT-LLM with natural language audio reasoning for which a model is expected to distinguish, compare, and summarise two audio clips (see Appendix~\ref{appendix:nlar}). To the best of the knowledge, there is no previous work evaluating model ability to comprehend more than one recording. We thus contrast APT-LLM to the baseline where predictions are fixed to a specific answer (we used ``yes'' as the fixed answer after several attempts). Table~\ref{tab.:audio_reasoning} demonstrates that APT-Vicuna v1.5 achieves 63.78\% mAP score, outperforming the baseline by a large margin. This result suggests that APT-LLM is able to not only comprehend the content in an audio clip but also analyse more than one audio recordings by comparison and summarisation. It is worth noting that there is marginal improvement when upgrading Vicuna from v1.1 to v1.5, indicating the performance of language models is not the bottleneck in this task, at least for the two used in our study.

\subsection{Evaluation on zero-shot audio-visual tasks} \label{subsec:evaluation_on_audio-visual_tasks}

\begin{wraptable}{r}{7.5cm}
    \caption{Performance comparison between different modalities in audio-visual learning.}
    \label{tab.:av_learning}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    Model                         & Modal       & Accuracy$\uparrow$         \\ \midrule
    BLIP-2~\citep{li_blip-2_2023} & Video-only  & 42.9             \\
    APT-LLM             & Audio-only  & 27.7                  \\
    APT-BLIP-2                  & Audio-video & \textbf{59.7}    \\ \bottomrule
    \end{tabular}
\end{wraptable}

APT was also experimented as an audio adapter for an existing VLM, BLIP-2~\citep{li_blip-2_2023}. BLIP-2 consists of a frozen image encoder, a Qformer, a projection layer, and a frozen Vicuna v1.1. Therefore, we integrated the APT trained with the same language model to BLIP-2 by interleaving acoustic prompts with text embeddings. We refer to the APT-enhanced BLIP-2 as APT-BLIP-2. Of note, although we selected BLIP-2 as our backbone model, APT can be easily adapted to another language model. APT-BLIP-2, together with other multimodal language models, was investigated on a audio-visual question and answering dataset where models are expected to choose one out of four options by using both audio and video modalities. We experimented APT-BLIP-2 on the subset of the AVQA dataset~\citep{yang_avqa_2022} as many video links associated with the AVQA test segmentation were no longer available on the internet at the time of the experiment. As shown in Table~\ref{tab.:av_learning}, APT-BLIP-2 yielded a better performance than video-only and audio-only counterparts, indicating the adaptation to the audio domain benefits models' learning from the content of video. 

\subsection{Limitations} \label{subsec:limitations}
In this work, we devised APT to align acoustic embeddings with text embeddings of language models. Now that the word embeddings change when switching to a different language model, even if their architectures remain the same, each language model calls for a dedicated APT for adaptation. In addition, APT-LLM was not trained with instruction-based datasets, and thus, has limited ability to response to questions excluded from the training set. Finally, we purposely focused APT-LLM training and experimentation on general-purpose audio understanding tasks, therefore, unlikely it can understand speech and music audios.

\section{Conclusions} \label{sec:conclusions}
We proposed APT, a general-purpose acoustic adapter that extends LLM/VLM to the audio domain. We showed that LLM coupled with APT is a multi-task audio learner that not only achieved a competitive performance across various audio understanding tasks but also be capable of in-context learning when fed with a few labelled examples. We also benchmarked APT-LLM's audio comprehension ability via the natural language audio reasoning task, a new task that requires a model to distinguish, compare, and summarise two different audio clips. Last but not least, it is evident from our study on audio-visual learning that encoding sound clips as word tokens is an efficient approach to adapt LLM/VLM to the audio domain. Future works can extend audio language models into comprehension of music and speech audio, and make them more aligned with human perception via instruction tuning. It is also interesting to investigate how audio language models handle errors in the in-context learning.

%\section*{Acknowledgements} \label{sec:acknowledgement}
%The research utilised Queen Mary's Apocrita HPC facility, supported by QMUL Research-IT, http://doi.org/10.5281/zenodo.438045. J. Liang supported by the Engineering and Physical Sciences Research Council [grant number EP/T518086/1]. E. Benetos is supported by a RAEng/Leverhulme Trust Research Fellowship [grant number LTRF2223-19-106]. This research was partly supported by Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 “AI for Sound”. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising.


\bibliography{references}
\bibliographystyle{iclr2024_conference}

\newpage
\appendix
\section{Appendix}
% \subsection{Representation learning in the stage 1} \label{appendix:APT_pretraining}
% \begin{figure}[h]
%     \centering
%     \includegraphics[scale=0.8]{src/qformer.pdf}
%     \caption{Illustration of APT pretraining in the stage 1.}
%     \label{fig.:pretraining}
% \end{figure}

% The aim of APT stage-1 pretraining is to bridge the audio modality and the text modality. To this end, a fixed number of acoustic embeddings are learnt to extract relevant information from the audio feature maps according to the input text tokens. Following the implementation in BLIP-2~\citep{li_blip-2_2023}, three objectives are jointly learnt by varying different attention masks on the sequences of audio and text as shown in Figure~\ref{fig.:pretraining}. Specifically, APT is optimised through three distinct losses:

% \textbf{Audio-Text Matching (ATM) loss.} The goal of ATM loss is to learn a token-level alignment between acoustic and text tokens. APT is trained to distinguish whether a pair of an audio recording and a text sentence are from the same source. A bi-directional self-attention mask is applied to the input sequence such that acoustic and textual tokens can attend to each other. APT's output embeddings $Z$ thus carry both audio and text information. Each vector of its embeddings $Z$ is then fed into a binary classifier, and the prediction is produced by averaging the binary logits over all vectors.

% \textbf{Audio-Grounded Text Generation (AGTG) loss.} AGTG loss shapes APT's learning to generate texts, given the input audio tokens. The information for generating the text is first extracted from the audio tokens, and then passed to the text tokens via self-attention layers. In APT, a multimodal causal self-attention mask is used to control audio-text interaction, where each text token can attend to all audio tokens and its previous text tokens.

% \textbf{Audio-Text Contrastive (ATC) loss.} The ATC loss enforces alignment between audio representation and text representation such that their mutual information is maximized. It achieves so by contrasting the audio-text similarity of a positive pair against those of negative pairs. APT aligns acoustic embeddings outputed by the audio encoder with text tokens. To avoid information leak, we employ a unimodal self-attention mask, where the acoustic and text tokens are not allowed to see each other. 

\subsection{Audio-text alignment} \label{appendix:audio-text_alignment}
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{src/qformer.pdf}
    \caption{Illustration of APT audio aligner pretrained with audio-text pairs. The rest components are frozen in the pretraining. Using different self-attention masks, the parameter of the audio aligner is optimised with triplet learning objectives: Audio-Text Matching (ATM), Audio-Grounded Text Generation (AGTG), and Audio-Text Contrastive (ATC).}
    \label{fig.:pretraining}
\end{figure}

Before coupled with a LLM, we pretrain APT audio aligner to bridge the audio modality and the text modality. Figure~\ref{fig.:pretraining} shows the implementation of audio-text alignment pretraining. Following~\cite{li_blip-2_2023}, we freeze the other components and optimise parameters of the audio aligner. During training, a fixed number of acoustic embeddings are learnt to extract relevant information from the audio feature maps according to the input text tokens. In the pretraining, the audio aligner learns with triplet training objectives:

\textbf{Audio-text matching (ATM).} The goal of ATM is to learn a token-level alignment between acoustic and text tokens. APT is trained to distinguish whether a pair of an audio recording and a text sentence are from the same source. A bi-directional self-attention mask is applied to the input sequence such that acoustic and textual tokens can attend to each other. APT's output embeddings $Z$ thus carry both audio and text information. Each vector of its embeddings $Z$ is then fed into a binary classifier, and the prediction is produced by averaging the binary logits over all vectors.

\textbf{Audio-grounded text generation (AGTG).} AGTG shapes APT's learning to generate texts, given the input audio tokens. The information for generating the text is first extracted from the audio tokens, and then passed to the text tokens via self-attention layers. In APT, a multimodal causal self-attention mask is used to control audio-text interaction, where each text token can attend to all audio tokens and its previous text tokens.

\textbf{Audio-text contrastive (ATC).} The ATC enforces alignment between audio representation and text representation such that their mutual information is maximized. It achieves so by contrasting the audio-text similarity of a positive pair against those of negative pairs. APT aligns acoustic embeddings outputed by the audio encoder with text tokens. To avoid information leak, we employ a unimodal self-attention mask, where the acoustic and text tokens are not allowed to see each other.

\subsection{Prompts for various tasks} 
\label{appendix:multitask_prompt}
Tables~\ref{tab.:tagging_captioning_template} and~\ref{tab.:proposed_task_template} detail the prompts we used to prompt APT-LLMs during training and inference.
\begin{table}[h]
\centering
\caption{Templates of the question and answer for audio tagging and audio captioning.}
\label{tab.:tagging_captioning_template}
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{1}{c}{audio tagging}                                                                      & \multicolumn{1}{c}{audio captioning}                                                                      \\ \midrule
Summarize the audio with key words.                                                                    & Summarize the audio succinctly.                                                                           \\\\
\begin{tabular}[c]{@{}l@{}}What sound events can be heard in \\ the audio clip?\end{tabular}           & \begin{tabular}[c]{@{}l@{}}Present a short overview of the\\  provided audio samples.\end{tabular}        \\\\
\begin{tabular}[c]{@{}l@{}}What auditory incidents can be \\ recognized in the recording?\end{tabular} & \begin{tabular}[c]{@{}l@{}}Provide a compact summary of\\  the auditory content.\end{tabular}             \\\\
\begin{tabular}[c]{@{}l@{}}Which auditory occurrences can be\\ detected?\end{tabular}                  & \begin{tabular}[c]{@{}l@{}}Offer a brief outline of the audio\\  clips that have been given.\end{tabular} \\\\
\begin{tabular}[c]{@{}l@{}}Which sound occurrences can be \\ perceived?\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Render a compressed version of\\ the audio's main points.\end{tabular}         \\\\
\begin{tabular}[c]{@{}l@{}}Present a concise breakdown of\\ the given audio clips.\end{tabular}        & Describe the audio clip concisely.                                                                        \\\\
\begin{tabular}[c]{@{}l@{}}List the sound events in the audio \\ clip.\end{tabular}                    & \begin{tabular}[c]{@{}l@{}}Explain the audio clip in a brief \\ and straightforward manner.\end{tabular}  \\\\
\begin{tabular}[c]{@{}l@{}}Describe the recording with names\\ of sound events.\end{tabular}           & \begin{tabular}[c]{@{}l@{}}Write a terse but informative \\ summary of the sound.\end{tabular}            \\\\
\begin{tabular}[c]{@{}l@{}}Enumerate the audio events present\\  in the audio.\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Give a quick overview of the \\ provided audio excerpts.\end{tabular}          \\\\
\begin{tabular}[c]{@{}l@{}}Name the auditory incidents in the\\  audio sample.\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Outline the given audio samples \\ briefly.\end{tabular}                       \\\\ \midrule
\#Output: \{LABEL\}                                                                        & \#Output: \{CAPTION\}                                                                                           \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Template of the question and answer for query-based sound event detection, temporal event retrieval, and sound event counting.}
\label{tab.:proposed_task_template}
\begin{tabular}{@{}lll@{}}
\toprule
\multicolumn{1}{c}{query-based sound event detection}                                                         & \multicolumn{1}{c}{temporal event retrieval}                                                                                                        & sound event counting                                                                                         \\ \midrule
\begin{tabular}[c]{@{}l@{}}Pinpoint the presence of \\ \{LABEL\} with the time \\ stamps.\end{tabular}        & \begin{tabular}[c]{@{}l@{}}Summarize the audio with \\ key words in the interval \\ of \{STT\} seconds to \{EDT\} \\ seconds.\end{tabular}          & \begin{tabular}[c]{@{}l@{}}How many times can the \\ sound \{LABEL\} be heard?\end{tabular}                  \\
\begin{tabular}[c]{@{}l@{}}Indicate the start and end \\ time of the audio event \\ \{LABEL\}.\end{tabular}   & \begin{tabular}[c]{@{}l@{}}What sound events can be \\ heard from \{STT\} seconds \\ to \{EDT\} seconds?\end{tabular}                               & \begin{tabular}[c]{@{}l@{}}How many instances of \\ the sound \{LABEL\} can be \\ perceived?\end{tabular}    \\
\begin{tabular}[c]{@{}l@{}}Document the exact times \\ the sound \{LABEL\} taking \\ place.\end{tabular}      & \begin{tabular}[c]{@{}l@{}}What auditory incidents can \\ be recognized in the recording \\ from \{STT\} seconds to \{EDT\} \\ seconds\end{tabular} & \begin{tabular}[c]{@{}l@{}}What is the number of \\ times the sound \{LABEL\} is\\  detectable?\end{tabular} \\
\begin{tabular}[c]{@{}l@{}}Specify the time stamps for\\ \{LABEL\} occurrence.\end{tabular}                   & \begin{tabular}[c]{@{}l@{}}Which auditory occurrences \\ can be detected during \{STT\} \\ seconds to \{EDT\} seconds?\end{tabular}                 & \begin{tabular}[c]{@{}l@{}}How frequently can one \\ hear the sound \{LABEL\}?\end{tabular}                  \\
\begin{tabular}[c]{@{}l@{}}When the sound \{LABEL\} \\ happens?\end{tabular}                                  & \begin{tabular}[c]{@{}l@{}}Which sound occurrences \\ can be perceived between \\ \{STT\} seconds and \{EDT\} \\ seconds?\end{tabular}              & \begin{tabular}[c]{@{}l@{}}How often can the sound \\ \{LABEL\} be perceived?\end{tabular}                   \\
\begin{tabular}[c]{@{}l@{}}Capture the exact times \\ when \{LABEL\} is \\ happening.\end{tabular}            & \begin{tabular}[c]{@{}l@{}}Present a concise breakdown \\ of the recording from \{STT\} \\ seconds to \{EDT\} seconds.\end{tabular}                 &                                                                                                              \\
\begin{tabular}[c]{@{}l@{}}Describe the time intervals \\ during which \{LABEL\} \\ takes place.\end{tabular} & \begin{tabular}[c]{@{}l@{}}List the sound events in the \\ interval of \{STT\} seconds to \\ \{EDT\} seconds.\end{tabular}                          &                                                                                                              \\
\begin{tabular}[c]{@{}l@{}}State the precise moment \\ at which \{LABEL\} occurs.\end{tabular}                & \begin{tabular}[c]{@{}l@{}}Name the auditory incidents \\ within the \{STT\} to \{EDT\} \\ seconds timeframe.\end{tabular}                          &                                                                                                              \\
\begin{tabular}[c]{@{}l@{}}What time does the sound \\ event \{LABEL\} take place?\end{tabular}               & \begin{tabular}[c]{@{}l@{}}Enumerate the audio events \\ present between \{STT\} seconds \\ and \{EDT\} seconds.\end{tabular}                       &                                                                                                              \\
\begin{tabular}[c]{@{}l@{}}Capture the beginning and \\ end time of the sound \\ \{LABEL\}.\end{tabular}      & \begin{tabular}[c]{@{}l@{}}Describe the recording with \\ names of sound events within \\ the \{STT\} to \{EDT\} seconds \\ timeframe.\end{tabular} &                                                                                                              \\ \midrule
\#Output: \{STT\}s-\{EDT\}s                                                                                   & \#Output: \{LABEL\}                                                                                                                                     & \# Output: \{NUMBER\}                                                                                        \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Natural language audio reasoning} \label{appendix:nlar}
Natural language audio reasoning (NLAR) is proposed to evaluate model's ability in comprehending across audio clips as per the text instruction. We progressively render the NLAR dataset using the Clotho-AQA dataset, and every stage is described in the following:
\begin{itemize}
    \item \textbf{Data collection}: We filter out some audio samples from the Clotho-AQA by the quality of the audio and the understandably of the annotations. To avoid data leakage, the test split of NLAR is collected from the test split of Clotho-AQA only;
    \item \textbf{Data cleaning}: The annotation in the Clotho-AQA is noisy. For instance, annotators might not reach an agreement on the number of bird chirping in the recordings. Therefore, we manually re-annotate the audio files again by focusing on the ``controversal'' annotations. We notice that it is fallible to calculate the frequency of sound events in some cases, and even we can not annotate their frequency, such as raining. In this case, we will annotate the presence of these activities throughout the recordings by using tags such as"in the beginning", ``in the middle'', ``in the end''  and ``throughout the recording''.
    \item \textbf{Describe acoustic features}: We applied ChatGPT-turbo~\citep{openai_gpt-4_2023} to describe the acoustic feature of each sound event with the prompt ``describe the acoustic characteristic of \{SOUND\} precisely with a sentence less than 10 words'' where \{SOUND\} refers to as the name of the sound event. We inspect each audio features to ensure they are comprehensible.
    \item  \textbf{Create audio-text pairs}: We applied ChatGPT-turbo to generate five question-answer pairs according to their associated sound events together with the temporal information, and audio description of the two audio recordings.
    \item \textbf{Inspect the rendered data}: We manually check the generated question-answer pairs using the annotations of the recordings and call ChatGPT again if the number of qualified pairs is lower than five. We dropped the generated pairs only if they contains some factual errors. We noticed that during data rendering, some answers to the questions could be "unclear". We didn't exclude this from the dataset as it is also important for models to learn what cannot be perceived from the audio. 
\end{itemize}

\begin{table}[]
\centering
\caption{An example of rendering the NLAR dataset}
\label{tab.:nlar_dataset}
\begin{tabular}{@{}l@{}}
\toprule                                                                      
\begin{tabular}[c]{@{}l@{}}Based on the following two audio clips, generate 5 different questions that must be derived by \\ summarising or comparing both audios and is prohibited to contain the information indicating \\ its answer. The following information is provided: the sound events appear in the audio clip, \\ together with its acoustic features, and corresponding onset and offset time stamps or frequency \\ in the recordings. The answer should be either a binary one that can be responded by 'yes' or \\ 'no', or an open-ended one that can be reply by a number or a single word.\\ \\ Audios:\\ \hspace{0.4cm} First: \\ \hspace{0.8cm} wood creaking (rustic, rhythmic, and creaky) {[}6 times{]};\\ \hspace{0.4cm} second: \\ \hspace{0.8cm} thunder (explosive, rumbling, and reverberating) {[}1 times{]}; \\ \hspace{0.8cm} rain (gentle, pitter-patter, rhythmic) {[}throughout the recording{]}.\\ Questions and answers:\\ \hspace{0.4cm}1. are there 6 wood creaking and 2 thunder sounds in total? - no\\ \hspace{0.4cm} 2. how many wood creaking or thunder are there in total? - 7\\ \hspace{0.4cm} 3. does the second recording more likely to make one feel calm? - yes\\ \hspace{0.4cm} 4. in which recording the events are more frequent? - first\\ \hspace{0.4cm} 5. the frequency of wood creaking in the first recording is 6 times \\ \hspace{0.8cm} more than the frequency of thunder in the second one. - yes\\ \\ Audios:\\ \hspace{0.4cm} First: \\ \hspace{0.8cm} tapping glass (crisp, clear, and tingling sound) {[}9 times{]}; \\ \hspace{0.4cm} second: \\ \hspace{0.8cm} water (rapid, and draining water sound) {[}throughout the recording{]}.\\ Questions and answers:\\ \hspace{0.4cm} 1. are there 9 tapping glass sounds and 3 water sounds in total? - no\\ \hspace{0.4cm} 2. how many tapping glass or water sounds are there in total? - 10\\ \hspace{0.4cm} 3. does the second recording create a continuous sound throughout? - yes\\ \hspace{0.4cm} 4. in which recording are the sound events more repetitive? - first\\ \hspace{0.4cm} 5. does the second recording sound more dynamic compared to the \\ \hspace{0.8cm} first recording? - yes\\ \\ Audios:\\ \hspace{0.4cm} First: \\ \hspace{0.8cm} wood creaking (rustic, rhythmic, and creaky) {[}6 times{]};\\ \hspace{0.4cm} second: \\ \hspace{0.8cm} shower (droplets, soothingly cascading) {[}throughout the recording{]}.\\ Questions and answers: \end{tabular} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Analysis on few-shot audio classification} \label{appendix:few-shot}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{src/few-shot.pdf}
    \caption{APT-LLM performance in few-shot audio classification when: (a) the number of classes increases, and (b) the number of examples per class increases. In both cases, the increase in the number of classes/examples leads to the rapid increase of the input sequence's length.}
    \label{fig.:few_shot}
\end{figure}

Figure~\ref{fig.:few_shot} (a) and (b) show APT-LLM performance in various few-shot settings. In both settings, the input sequence becomes longer when the number of shots/classes increases. It is interesting to observe that as the number of shots increases, the overall performance becomes worse. This is contrasting to the behavior of few-shot learners based on back propagation. One possible reason is that increasing the number of classes/shots rapidly increases the length of the input sequence, which is detrimental to APT-LLM global attention mechanism.

\end{document}