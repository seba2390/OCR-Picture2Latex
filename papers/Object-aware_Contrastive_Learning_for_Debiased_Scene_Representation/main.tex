\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[nonatbib,final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 font
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table]{xcolor}         % colors
\usepackage[sort&compress,numbers]{natbib}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{lipsum}
\usepackage[figuresleft]{rotating}

\frenchspacing

\definecolor{citecolor}{HTML}{0071bc}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}
\definecolor{brickred}{rgb}{0.6,0,0}
\definecolor{royalblue}{rgb}{0,0,0.8}
\definecolor{tdgreen}{rgb}{0,0.4,0.7}

\usepackage[breaklinks=true,colorlinks,citecolor=citecolor]{hyperref}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\stdv}[1]{\scriptsize$\pm$#1}
\newcommand{\up}[1]{\color{blue}\scriptsize(+#1)}
\newcommand{\down}[1]{\color{red}\scriptsize(-#1)}
\newcommand{\cuparrow}{\color{blue}$\pmb{\uparrow}$}
\newcommand{\cdownarrow}{\color{red}$\pmb{\downarrow}$}

\usepackage[table]{xcolor}
\usepackage{array} % for "\extrarowheight" macro
\usepackage{siunitx}

\newcommand{\tbd}[1]{{\color{red}{\textbf{TBD}: #1}}}
\newcommand{\edited}[1]{{\color{red}{#1}}}

\usepackage{etoolbox}
\usepackage[textsize=scriptsize,textwidth=2.2cm]{todonotes}

\newtoggle{showtodos}
\toggletrue{showtodos} % comment this out to remove comments.

\iftoggle{showtodos}{
\newcommand{\kihyuks}[1]{\todo[color=purple!40]{Kihyuk: #1}}
\newcommand{\chunliang}[1]{\todo[color=purple!40]{CL: #1}}
\newcommand{\swmo}[1]{\todo[color=red]{SW: #1}}
}{
\newcommand{\kihyuks}[1]{}
\newcommand{\chunliang}[1]{}
\newcommand{\swmo}[1]{}
}

\title{
Object-aware Contrastive Learning for \\
Debiased Scene Representation
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
Sangwoo Mo\thanks{Equal contribution}$\:\:^1$, Hyunwoo Kang$^{*1}$, Kihyuk Sohn$^2$, Chun-Liang Li$^2$, Jinwoo Shin$^1$\\
$^1$KAIST \quad $^2$Google Cloud AI\\
\texttt{\{swmo,hyunwookang,jinwoos\}@kaist.ac.kr}, \texttt{\{kihyuks,chunliang\}@google.com}
}

\begin{document}

\maketitle
  
\setcounter{footnote}{0}

\begin{abstract}
 Contrastive self-supervised learning has shown impressive results in learning visual representations from unlabeled images by enforcing invariance against different data augmentations. However, the learned representations are often contextually biased to the spurious scene correlations of different objects or object and background, which may harm their generalization on the downstream tasks. To tackle the issue, we develop a novel object-aware contrastive learning framework that first (a) localizes objects in a self-supervised manner and then (b) debias scene correlations via appropriate data augmentations considering the inferred object locations. For (a), we propose the contrastive class activation map (ContraCAM), which finds the most discriminative regions (e.g., objects) in the image compared to the other images using the contrastively trained models. We further improve the ContraCAM to detect multiple objects and entire shapes via an iterative refinement procedure. For (b), we introduce two data augmentations based on ContraCAM, object-aware random crop and background mixup, which reduce contextual and background biases during contrastive self-supervised learning, respectively. Our experiments demonstrate the effectiveness of our representation learning framework, particularly when trained under multi-object images or evaluated under the background (and distribution) shifted images.\footnote{Code is available at \url{https://github.com/alinlab/object-aware-contrastive}.}
\end{abstract}


\section{Introduction}
\label{sec:intro}

Self-supervised learning of visual representations from unlabeled images is a fundamental task of machine learning, which establishes various applications including object recognition \citep{he2020momentum,chen2020simple}, reinforcement learning \citep{anand2019unsupervised,srinivas2020curl}, out-of-distribution detection \citep{tack2020csi,sohn2021learning}, and multimodal learning \citep{radford2021learning,afouras2021self}. Recently, contrastive learning \citep{oord2018representation,wu2018unsupervised,misra2020self,he2020momentum,chen2020simple,caron2020unsupervised,tian2020contrastive,grill2020bootstrap,chen2021exploring} has shown remarkable advances along this line. The idea is to learn invariant representations by attracting the different views (e.g., augmentations) of the same instance (i.e., positives) while contrasting different instances (i.e., negatives).\footnote{Some recent works (e.g., \citep{grill2020bootstrap,chen2021exploring}) attract the positives without contrasting the negatives. While we mainly focus on contrastive learning with negatives, our method is also applicable to the positive-only methods.}

Despite the success of contrastive learning on various downstream tasks \citep{zhao2021makes}, they still suffer from the generalization issue due to the unique features of the training datasets \citep{hermann2020origins,geirhos2020surprising,purushwalkam2020demystifying} or the choice of data augmentations \citep{purushwalkam2020demystifying,tian2020makes,xiao2021should}. In particular, the co-occurrence of different objects and background in randomly cropped patches (i.e., positives) leads the model to suffer from the \textit{scene bias}. For example, Figure~\ref{fig:intro-crop} presents two types of the scene bias: the positive pairs contain different objects (e.g., giraffe and zebra), and the patches contain adjacent object and background (e.g., zebra and safari). Specifically, the co-occurrence of different objects is called contextual bias \citep{singh2020don}, and that of object and background is called background bias \citep{xiao2021noise}. Attracting the patches in contrastive learning makes the features of correlated objects and background indistinguishable, which may harm their generalization (Figure~\ref{fig:intro-multi}) because of being prone to biases (Figure~\ref{fig:intro-bg}).

\input{resources/fig_intro}

\textbf{Contribution.}
We develop a novel object-aware contrastive learning framework that mitigates the scene bias and improves the generalization of learned representation. The key to success is the proposed \textit{contrastive class activation map} (ContraCAM), a simple yet effective self-supervised object localization method by contrasting other images to find the most discriminate regions in the image. We leverage the ContraCAM to create new types of positives and negatives. First, we introduce two data augmentations  for constructing the \textit{positive} sample-pairs of contrastive learning: \textit{object-aware random crop} and \textit{background mixup} that reduce contextual and background biases, respectively. Second, by equipping ContraCAM with an iterative refinement procedure, we extend it to detect multiple objects and entire shapes, which allows us to generate masked images as effective \emph{negatives}.


We demonstrate that the proposed method can improve two representative contrastive (or positive-only) representation learning schemes, MoCov2 \citep{chen2020improved} and BYOL \citep{grill2020bootstrap}, by reducing contextual and background biases as well as learning object-centric representation. In particular, we improve:
\begin{itemize}[topsep=1.0pt,itemsep=1.0pt,leftmargin=5.5mm]
    \item The representation learning under multi-object images, evaluated on the COCO \citep{lin2014microsoft} dataset, boosting the performance on the downstream tasks, e.g., classification and detection.
    \item The generalizability of the learned representation on the background shifts, i.e., objects appear in the unusual background (e.g., fish on the ground), evaluated on the Background Challenge \citep{xiao2021noise}.
    \item The generalizability of the learned representation on the distribution shifts, particularly for the shape-biased, e.g., ImageNet-Sketch~\citep{wang2019learning}, and corrupted, e.g., ImageNet-C \citep{hendrycks2019robustness} datasets.
\end{itemize}
Furthermore, ContraCAM shows comparable results with the state-of-the-art unsupervised localization method (and also with the supervised classifier CAM) while being simple.


\section{Object-aware Contrastive Learning}
\label{sec:method}

We first briefly review contrastive learning in Section~\ref{sec:method-prelim}. We then introduce our object localization and debiased contrastive learning methods in Section~\ref{sec:method-concam} and Section~\ref{sec:method-debias}, respectively.


\subsection{Contrastive learning}
\label{sec:method-prelim}


Contrastive self-supervised learning aims to learn an encoder $f(\cdot)$ that extracts a useful representation from an unlabeled image $x$ by attracting similar sample $x^+$ (i.e., positives) and dispelling dissimilar samples $\{x^-_i\}$ (i.e., negatives). In particular, instance discrimination \citep{wu2018unsupervised} defines the same samples of different data augmentations (e.g., random crop) as the positives and different samples as negatives. Formally, contrastive learning maximizes the contrastive score:
\begin{align}
s_{\texttt{con}}(x; x^+, \{x^-_n\})
:= \log \frac{\exp(\mathrm{sim}(z(x), \bar{z}(x^+)) / \tau)}{\exp(\mathrm{sim}(z(x), \bar{z}(x^+)) / \tau) + \sum_{x^-_n} \exp(\mathrm{sim}(z(x), \bar{z}(x^-_n)) / \tau)},
\label{eq:con-score}
\end{align}
where $z(\cdot)$ and $\bar{z}(\cdot)$ are the output and target functions wrapping the representation $f(x)$ for use, $\mathrm{sim}(\cdot,\cdot)$ denotes the cosine similarity, and $\tau$ is a temperature hyperparameter.
The specific form of $z(\cdot),\bar{z}(\cdot)$ depends on the method. For example, MoCov2 \citep{chen2020improved} sets $z(\cdot) = g(f(\cdot)), \bar{z}(\cdot) = g_m(f_m(\cdot))$ where $g(\cdot)$ is a projector network to indirectly match the feature $f(x)$ and $f_m(\cdot),g_m(\cdot)$ are the momentum version of the encoder and projectors. 
On the other hand, BYOL \citep{grill2020bootstrap} sets $z(\cdot) = h(g(f(\cdot))), \bar{z}(\cdot) = g_m(f_m(\cdot))$, where $h(\cdot)$ is an additional predictor network to avoid collapse of the features because it only  
maximizes the similarity score $s_{\texttt{sim}}(x; x^+) := \mathrm{sim}(z(x), \bar{z}(x^+))$~\citep{grill2020bootstrap,chen2021exploring}.


\textbf{Scene bias in contrastive learning.}
Despite the success of contrastive learning, they often suffer from the \textit{scene bias}: entangling representations of co-occurring (but different) objects, i.e., contextual bias \citep{singh2020don}, or adjacent object and background, i.e., background bias \citep{xiao2021noise}, by attracting the randomly cropped patches reflecting the correlations (Figure~\ref{fig:intro-crop}). The scene bias harms the performance (Figure~\ref{fig:intro-multi}) and generalization of the learned representations on distribution shifts (Figure~\ref{fig:intro-bg}). To tackle the issue, we propose object-aware data augmentations for debiased contrastive learning (Section~\ref{sec:method-debias}) utilizing the object locations inferred from the contrastively trained models (Section~\ref{sec:method-concam}). 


\subsection{ContraCAM: Unsupervised object localization via contrastive learning}
\label{sec:method-concam}

We aim to find the most discriminative region in an image, such as objects for scene images, compared to the other images. To this end, we extend the (gradient-based) class activation map (CAM) \citep{zhou2016learning,selvaraju2017grad}, originally used to find the salient regions for the prediction of classifiers. Our proposed method, \textit{contrastive class activation map} (ContraCAM), has two differences from the classifier CAM. First, we use the contrastive score instead of the softmax probability. Second, we discard the negative signals from the similar objects in the negative batch since they cancel out the positive signals and hinder the localization, which is crucial as shown in Table~\ref{tab:loc-main} and Appendix~\ref{sec:add-loc-neg}).

Following the classifier CAM, we define the saliency map as the weighted sum of spatial activations (e.g., penultimate feature before pooling), where the weight of each activation is given by the importance, the sum of gradients, of the activation for the score function. Formally, let $\mathbf{A} := [A_{ij}^k]$ be a spatial activation of an image $x$ where $1 \le i \le H, 1 \le j \le W, 1 \le k \le K$ denote the index of row, column, and channel, and $H, W, K$ denote the height, width, and channel size of the activation. Given a batch of samples $\mathcal{B}$, we define the score function of the sample $x$ as the contrastive score $s_\texttt{con}$ in Eq.~\eqref{eq:con-score} using the sample $x$ itself as a positive\footnote{It does not affect the score but is defined for the notation consistency with the iterative extension.} and the remaining samples $\mathcal{B} \setminus x$ as negatives. Then, the weight of the $k$-th activation $\alpha_k$ and the CAM mask $\texttt{CAM} := [\texttt{CAM}_{ij}] \in [0,1]^{H \times W}$ are:
\begin{align}
\texttt{CAM}_{ij} = \texttt{Normalize}\left( \texttt{ReLU}\left( \sum_k \alpha_k A_{ij}^k \right) \right), ~\alpha_k = {\color{red}\texttt{ReLU}} \left( \frac{1}{HW} \sum_{i,j} \frac{\partial {\color{red} s_\texttt{con}(x; x, \mathcal{B} \setminus x)}}{\partial A^k_{i,j}} \right),
\label{eq:concam}
\end{align}
where $\texttt{Normalize}(x) := \frac{x - \min{x}}{\max{x} - \min{x}}$ is a normalization function that maps the elements to $[0,1]$. We highlight the differences from the classifier CAM with the red color. Note that the \texttt{ReLU} used to compute $\alpha_{k}$ in Eq.~\eqref{eq:concam} discards the negative signals. The negative signal removal trick also slightly improves the classifier CAM \cite{bae2020rethinking} but much effective for the ContraCAM.

\input{resources/fig_method_iconcam}


We further improve the ContraCAM to detect multiple objects and entire shapes with an iterative refinement procedure \citep{wei2017object}: cover the salient regions of the image with the (reverse of) current CAM, predict new CAM from the masked image, and aggregate them (see Figure~\ref{fig:method-iconcam}). It expands the CAM regions since the new CAM from the masked image detects the unmasked regions. Here, we additionally provide the masked images in the batch (parellely computed) as the negatives: they are better negatives by removing the possibly existing similar objects. Also, we use the original image $x$ as the positive to highlight the undetected objects. Formally, let $\texttt{CAM}^t$ be the CAM of iteration $t$ and $\overline{\texttt{CAM}}^t := [\overline{\texttt{CAM}}^t_{ij}] = [\max_{l \le t} \texttt{CAM}^l_{ij}]$ be the aggregated CAM mask. Also, let $x^t$ be the image softly masked by the (reverse of) current aggregated mask, i.e., $x^t := (1 - \overline{\texttt{CAM}}^{t-1}) \odot x$ for $t \ge 2$ and $x^1 = x$ where $\odot$ denotes an element-wise product, and $\mathcal{B}^t := \{x^t_n\}$ be the batch of the masked images. Then, we define the score function for iteration $t$ as:
\begin{align}
s_\texttt{con}^t(x) := s_\texttt{con}(x^t; x, \cup_{l \le t} (\mathcal{B}^l \setminus x^l)).
\label{eq:cam-score-it}
\end{align}
We substitute the contrastive score $s_\texttt{con}$ in Eq.~\eqref{eq:concam} with the $s_\texttt{con}^t$ in Eq.~\eqref{eq:cam-score-it} to compute the CAM of iteration $t$, and use the final aggregated mask after $T$ iterations. We remark that the CAM results are not sensitive to the number of iterations $T$ if it is large enough; CAM converges to the stationary value since soft masking $x^t$ regularizes the CAM not to be keep expanded (see Appendix~\ref{sec:add-loc-iter}). We provide the pseudo-code of the entire Iterative ContraCAM procedure in Appendix~\ref{sec:algorithm}.

Note that contrastive learning was known to be ineffective at localizing objects \citep{zhao2021distilling} with standard saliency methods (using a classifier on top of the learned representation) since attracting the randomly cropped patches makes the model look at the entire scene. To our best knowledge, we are the first to extend the CAM for the self-supervised setting, relaxing the assumption of class labels. \citet{selvaraju2021casting} considered CAM for contrastive learning, but their purpose was to regularize CAM to be similar to the ground-truth masks (or predicted by pre-trained models) and used the similarity of the image and the masked image (by ground-truth masks) as the score function of CAM.


\subsection{Object-aware augmentations for debiased contrastive learning}
\label{sec:method-debias}

We propose two data augmentations for contrastive learning that reduce contextual and background biases, respectively, utilizing the object locations inferred by ContraCAM. Both augmentations are applied to the \textit{positive} samples before other augmentations; thus, it is applicable for both contrastive learning (e.g., MoCov2 \citep{chen2020improved}) and positive-only methods (e.g., BYOL \citep{grill2020bootstrap}).


\textbf{Reducing contextual bias.}
We first tackle the contextual bias of contrastive learning, i.e., entangling the features of different objects. To tackle the issue, we propose a data augmentation named \textit{object-aware random crop}, which restricts the random crop around a single object and avoids the attraction of different objects. To this end, we first extract the (possibly multiple or none) bounding boxes of the image from the binarized mask\footnote{Threshold the mask or apply a post-processing method, e.g., conditional random field (CRF) \citep{lafferty2001conditional}.} of the ContraCAM. We then crop the image around the box, randomly chosen from the boxes, before applying other augmentations (e.g., random crop). Here, we apply augmentations (to produce positives) to the same cropped box; thus, the patches are restricted in the same box. Technically, it only requires a few line addition of code:

\input{resources/alg_multi}

\citet{purushwalkam2020demystifying} considered a similar approach using ground-truth bounding boxes applied on MoCov2. However, we found that cropping around the ground-truth boxes often harms contrastive learning (see Table~\ref{tab:multi-main}). This is because some objects (e.g., small ones) in ground-truth boxes are hard to discriminate (as negatives), making contrastive learning hard to optimize. In contrast, the ContraCAM produces more discriminative boxes, often outperforming the ground-truth boxes (see Appendix~\ref{sec:add-multi-stability}). Note that the positive-only methods do not suffer from the issue: both ground-truth and ContraCAM boxes work well. On the other hand, \citet{selvaraju2021casting} used a pre-trained segmentation model to constrain the patches to contain objects. It partly resolves the false positive issue by avoiding the attraction of background-only patches but does not prevent the patches with different objects; in contrast, the object-aware random crop avoids both cases.


\input{resources/fig_method_bg}

\textbf{Reducing background bias.}
We then tackle the background bias of contrastive learning, i.e., entangling the features of adjacent object and background. To this end, we propose a data augmentation named \textit{background mixup}, which substitutes the background of an image with other backgrounds. Intuitively, the positive samples share the objects but have different backgrounds, thus reducing the background bias. Formally, background mixup blends an image $x_1$ and a background-only image $x_2^\texttt{bg}$ (generated from an image $x_2$) using the ContraCAM of image $x_1$ as a weight, i.e.,
\begin{align}
x_1^\texttt{bg-mix} := \texttt{CAM}(x_1) \odot x_1 + (1 - \texttt{CAM}(x_1)) \odot x_2^\texttt{bg},
\label{eq:bg-mixup}
\end{align}
where $\odot$ denotes an element-wise product. Here, the background-only image $x_2^\texttt{bg}$ is generated by tiling the background patch of the image $x_2$ inferred by the ContraCAM. Precisely, we choose the largest rectangle in the zeros of the binarized CAM mask for the region of the background patch. The overall procedure of the background mixup is illustrated in Figure~\ref{fig:method-bg}.

Prior works considered the background bias for contrastive learning \citep{zhao2021distilling,ryali2021leveraging} but used a pre-trained segmentation model and copy-and-pasted the objects to the background-only images using binary masks. We also tested the copy-and-paste version with the binarized CAM, but the soft version in Eq.~\eqref{eq:bg-mixup} performed better (see Appendix~\ref{sec:add-bg-soft}); one should consider the confidence of the soft masks since they are inaccurate. Furthermore, the background mixup improves the generalization on distribution shifts, e.g., shape-biased \citep{geirhos2019imagenet,wang2019learning,hendrycks2020many} and corrupted \citep{hendrycks2019robustness} datasets (see Table~\ref{tab:bg-domain}). Remark that the background mixup often outperforms the Mixup \citep{zhang2018mixup} and CutMix \citep{yun2019cutmix} applied for contrastive learning \citep{lee2021mix}. Intuitively, the background mixup can be viewed as a saliency-guided extension \citep{kim2020puzzle,uddin2021saliencymix} of mixup but not mixing the targets (positives), since the mixed patch should be only considered as the positive of the patch sharing foreground, not the one sharing background.


\section{Experiments}
\label{sec:exp}

We first verify the localization performance of ContraCAM in Section~\ref{sec:exp-loc}. We then demonstrate the efficacy of our debiased contrastive learning: object-aware random crop improves the training under multi-object images by reducing contextual bias in Section~\ref{sec:exp-multi}, and background mixup improves generalization on background and distribution shifts by reducing background bias in Section~\ref{sec:exp-bg}.

\input{resources/tab_loc_main}

\textbf{Common setup.}
We apply our method on two representative contrastive (or positive-only) learning models: MoCov2 \citep{chen2020improved} and BYOL \citep{grill2020bootstrap}, under the ResNet-18 and ResNet-50 architectures \citep{he2016deep}. We train the models for 800 epochs on COCO \citep{lin2014microsoft} and ImageNet-9 \citep{xiao2021noise}, and 2,000 epochs on CUB \citep{welinder2010caltech} and Flowers \citep{nilsback2006visual} datasets with batch size 256. For object localization experiments, we train the vanilla MoCov2 and BYOL on each dataset and compute the CAM masks. For representation learning experiments, we first train the vanilla MoCov2 and BYOL to pre-compute the CAM masks (and corresponding bounding boxes); then, we retrain MoCov2 and BYOL, applying our proposed augmentations using the fixed pre-computed masks (and boxes). Here, we retrain the models from scratch to make the training budgets fair. We also retrained (i.e., third iteration) the model using the CAM masks from our debiased models but did not see the gain (see Appendix~\ref{sec:add-multi-second}). We follow the default hyperparameters of MoCov2 and BYOL, except the smaller minimum random crop scale of 0.08 (instead of the original 0.2) since it performed better, especially for the multi-object images. We run a single trial for contextual bias and three trials for background bias experiments.

We use the penultimate spatial activations to compute the CAM results. At inference, we follow the protocol of \citep{choe2020evaluating} that doubly expands the resolution of the activations to detect the smaller objects through decreasing the stride of the convolutional layer in the final residual block. Since it produces the smaller masks, we use more iterations (e.g., 10) for the Iterative ContraCAM. Here, we apply the conditional random field (CRF) using the default hyperparameters from the pydensecrf library \citep{krahenbuhl2011efficient} to produce segmentation masks and use the opencv \citep{opencv_library} library to extract bounding boxes. We use a single iteration of the ContraCAM without the expansion trick for background bias results; it is sufficient for single instance images. Here, we binarize the masks with a threshold of 0.2 to produce background-only images. We provide the further implementation details in Appendix~\ref{sec:details}.

\textbf{Computation time.}
The training of the baseline models on the COCO ($\sim$100,000 samples) dataset takes $\sim$1.5 days on 4 GPUs and $\sim$3 days on 8 GPUs for ResNet-18 and ResNet-50 architectures, respectively, using a single machine with 8 GeForce RTX 2080 Ti GPUs; proportional to the number of samples and training epochs for other cases. The inference of ContraCAM takes a few minutes for the entire training dataset, and generating the boxes using CRF takes dozens of minutes. Using the pre-computed masks and boxes, our method only slightly increases the training time.


\input{resources/tab_loc_cam}

\subsection{Unsupervised object localization}
\label{sec:exp-loc}

We check the performance of our proposed self-supervised object localization method, ContraCAM. Figure~\ref{fig:loc-cam} shows the examples of the ContraCAM on various image datasets, including CUB, Flowers, COCO, and ImageNet-9 datasets. ContraCAM even detects multiple objects in the image. We also quantitatively compare ContraCAM with the state-of-the-art unsupervised object localization method, ReDo \citep{chen2019unsupervised}. Table~\ref{tab:loc-main} shows that the ContraCAM is comparable with ReDO, in terms of the the mask mean intersection-over-unions (mIoUs). One can also see that the negative signal removal, i.e., \texttt{ReLU} in Eq.~\eqref{eq:con-score}, is a critical to the performance (see Appendix~\ref{sec:add-loc-neg} for the visual examples).

We also compare the localization performance of ContraCAM (using MoCov2) and classifier CAM (using a supervised model). Table~\ref{tab:loc-cam} shows the results where all models are solely trained from the target dataset and evaluated on the same dataset. Interestingly, ContraCAM outperforms the classifier CAM on CUB and Flowers. We conjecture this is because CUB and Flowers have few training samples; the supervised classifier is prone to overfitting. On the other hand, Table~\ref{tab:loc-cam-transfer} shows the results on the transfer setting, i.e., the models are trained on the ImageNet \citep{deng2009imagenet} using the ResNet-50 architecture. We use the publicly available supervised classifier \citep{paszke2019pytorch} and MoCov2, and follow the MaxBoxAccV2 evaluation protocol \citep{choe2020evaluating}. The ContraCAM often outperforms the classifier CAM, especially for the unseen images (e.g., CUB). This is because the classifiers project out the features unrelated to the target classes, losing their generalizability on the out-of-class samples.

We provide additional analysis and results in Appendix~\ref{sec:add-loc}. Appendix~\ref{sec:add-loc-iter} shows the ablation study on the number of iterations of ContraCAM. One needs a sufficient number of iterations since too few iterations often detect subregions. Since ContraCAM converges to the stationary values for more iterations, we simply choose 10 for all datasets. Appendix~\ref{sec:add-loc-batch} shows the effects of the negative batch of ContraCAM. Since ContraCAM finds the most discriminative regions compared to the negative batch, one needs to choose the negative batch different from the target image. Using a few randomly sampled images is sufficient. Appendix~\ref{sec:add-loc-cam} provides additional comparison of ContraCAM and classifier CAM. Finally, Appendix~\ref{sec:add-loc-saliency} provides a comparison with the gradient-based saliency methods \citep{sundararajan2017axiomatic,smilkov2017smoothgrad} using the same contrastive score. CAM gives better localization results.


\subsection{Reducing contextual bias: Representation learning from multi-object images}
\label{sec:exp-multi}

\input{resources/tab_multi_main}
\input{resources/tab_multi_shift}

We demonstrate the effectiveness of the object-aware random crop (OA-Crop) for representation learning under multi-object images by reducing contextual bias. To this end, we train MoCov2 and BYOL on the COCO dataset, comparing them with the models that applied the OA-Crop using the ground-truth (GT) bounding boxes or inferred ones from the ContraCAM.

We first compare the linear evaluation \citep{kolesnikov2019revisiting}, test accuracy of a linear classifier trained on top of the learned representation, in Table~\ref{tab:multi-main}. We report the results on the COCO-Crop, i.e., the objects in the COCO dataset cropped by the GT boxes, CIFAR-10 and CIFAR-100 \citep{krizhevsky2009learning}, CUB, Flowers, Food \citep{bossard2014food}, and Pets \citep{parkhi2012cats} datasets. OA-Crop significantly improves the linear evaluation of MoCov2 and BYOL for all tested cases. Somewhat interestingly, OA-Crop using the ContraCAM boxes even outperforms the GT boxes for MoCov2 under the ResNet-50 architecture. This is because the GT boxes often contain objects hard to discriminate (e.g., small objects), making contrastive learning hard to optimize; in contrast, ContraCAM finds more distinct objects. Note that BYOL does not suffer from this issue and performs well with both boxes. See Appendix~\ref{sec:add-multi-stability} for the detailed discussion.

We also compare the detection (and segmentation) performance measured by mean average precision (AP), an area under the precision-recall curve of the bounding boxes (or segmentation masks), on the COCO detection and segmentation tasks in Table~\ref{tab:multi-det}. Here, we fine-tune the MoCov2 and BYOL models using the ResNet-50 architecture. Remark that OA-Crop using the ContraCAM boxes outperforms the baselines, while the GT boxes are on par or worse. This is because the GT boxes solely focus on the objects while ContraCAM also catches the salient scene information.

In addition, we present the generalization performance of learned representations under the distribution shifts in Table~\ref{tab:multi-shift}. To this end, we evaluate the models trained on the COCO dataset to various 9 superclass (370 classes) subsets of ImageNet, whose details will be elaborated in the next section. ImageNet-9 contains natural images like COCO, but other datasets contain distribution-shifted (e.g., shape-biased or corrupted) images. Note that OA-Crop performs on par with the vanilla MoCov2 and BYOL on the original ImageNet-9 but performs better on the distribution-shifted dataset. It verifies that the OA-Crop improves the generalizability of the learned representation.

We provide additional analysis and results in Appendix~\ref{sec:add-multi}. Appendix~\ref{sec:add-multi-bias} provides an additional analysis that OA-Crop indeed reduces the contextual bias. Specifically, the representation learned from OA-Crop shows better separation between the co-occurring objects, giraffe and zebra. Appendix~\ref{sec:add-multi-supervised} provides the comparison with the supervised representation, learned by Faster R-CNN \citep{ren2015faster} and Mask R-CNN \citep{he2017mask}, using ground-truth bounding boxes or segmentation masks. OA-Crop significantly reduces the gap between self-supervised and supervised representation. Appendix~\ref{sec:add-multi-class-wise} presents the class-wise accuracy on CIFAR10 that OA-Crop consistently improves the accuracy over all classes. Appendix~\ref{sec:add-multi-imagenet} presents the linear evaluation performance of MoCov2 and BYOL trained on a 10\% subset of ImageNet for readers comparing with the results with the ImageNet-trained models.


\subsection{Reducing background bias: Generalization on background and distribution shifts}
\label{sec:exp-bg}

\input{resources/tab_bg_main}

We demonstrate the effectiveness of the background mixup (BG-Mixup) for the generalization of the learned representations on background and distribution shifts by reducing background bias and learning object-centric representation. To this end, we train MoCov2 and BYOL (and BG-Mixup upon them) on the \textsc{Original} dataset from the Background Challenge \citep{xiao2021noise}, a 9 superclass (370 classes) subset of the ImageNet \citep{deng2009imagenet}. We then train a linear classifier on top of the learned representation using the \textsc{Original} dataset. Here, we evaluate the classifier on the Background Challenge datasets for the background shift results, and the corresponding 9 superclasses of the ImageNet-Sketch~\citep{wang2019learning}, Stylized-ImageNet~\citep{geirhos2019imagenet}, ImageNet-R \citep{hendrycks2020many}, and ImageNet-C~\citep{hendrycks2019robustness} datasets, denoted by putting `-9' at the suffix of the dataset names, for the distribution shift results (see Appendix~\ref{sec:details-bg} for details).

We additionally compare BG-Mixp with the hard background mixing (i.e., copy-and-paste) using ground-truth masks (BG-HardMix (GT)) for the background shift experiments, and Mixup \citep{zhang2018mixup} and CutMix \citep{yun2019cutmix} (following the training procedure of \citep{lee2021mix}) for the distribution shift experiments. We also tested the BG-HardMix using the binarized CAM but did not work well (see Appendix~\ref{sec:add-bg-soft}). On the other hand, the BG-Mixup often makes contrastive learning hard to be optimized by producing hard positives; thus, we apply BG-Mix with probability $p_\texttt{mix} < 1$, independently applied on the patches. We tested $p_\texttt{mix} \in \{0.2,0.3,0.4,0.5\}$ and choose $p_\texttt{mix} = 0.4$ for MoCov2 and $p_\texttt{mix} = 0.3$ for BYOL. Note that MoCov2 permits the higher $p_\texttt{mix}$, since finding the closest sample from the (finite) batch is easier than clustering infinitely many samples (see Appendix~\ref{sec:add-bg-augp} for details).

Table~\ref{tab:bg-main} presents the results on background shifts: BG-Mixup improves the predictions on the object-focused datasets (e.g., \textsc{Mixed-Rand}) while regularizing the background-focused datasets (e.g., \textsc{Only-BG-T}). Table~\ref{tab:bg-domain} presents the results on distribution shifts: BG-Mixup mostly outperforms the Mixup and the CutMix. We also provide the BG-HardMix (GT) results on distribution shifts in Appendix~\ref{sec:add-bg-shift} and the mixup results on background shifts in Appendix~\ref{sec:add-bg-mixup}. The superiority of BG-Mix on both background and distribution shifts shows that its merits come from both object-centric learning via reducing background and the saliency-guided input interpolation. In addition, we provide the corruption-wise classification results on ImageNet-9-C in Appendix~\ref{sec:add-bg-imagenetc}, and additional distribution shifts results on ObjectNet~\citep{barbu2019objectnet} and SI-Score~\citep{djolonga2021robustness} in Appendix~\ref{sec:add-bg-datasets}.


\section{Related work}
\label{sec:related}


\textbf{Contrastive learning.}
Contrastive learning (or positive-only method) \citep{he2020momentum,chen2020simple,grill2020bootstrap} is the state-of-the-art method for visual representation learning, which incorporates the prior knowledge of invariance over the data augmentations. However, they suffer from an inherent problem of matching false positives from random crop augmentation. We tackle this scene bias issue and improve the quality of learned representation. Note that prior work considering the scene bias for contrastive learning \cite{purushwalkam2020demystifying,selvaraju2021casting,zhao2021distilling,ryali2021leveraging} assumed the ground-truth object annotations or pre-trained segmentation models, undermining the motivation of self-supervised learning to reduce such supervision. In contrast, we propose a fully self-supervised framework of object localization and debiased contrastive learning. Several works \citep{pirk2019online,romijnders2021representation} consider an object-aware approach for video representation learning, but their motivation was to attract the objects of different temporal views and require a pretrained object detector.

\textbf{Bias in visual representation.}
The bias (or shortcut) in neural networks \citep{geirhos2020shortcut} have got significant attention recently, pointing out the unintended over-reliance on texture \citep{geirhos2019imagenet}, background \citep{xiao2021noise}, adversarial features \citep{ilyas2019adversarial}, or conspicuous inputs \citep{moon2021masker}. Numerous works have thus attempted to remove such biases, particularly in an unsupervised manner \citep{wang2019learning,minderer2020automatic,nam2020learning}. Our work also lies on this line: we evoke the scene bias issue of self-supervised representation learning and propose an unsupervised debiasing method. Our work would be a step towards an unbiased, robust visual representation.

\textbf{Unsupervised object localization.}
The deep-learning-based unsupervised object localization methods can be categorized as follow.
(a) The generative-based \citep{chen2019unsupervised,bielski2019emergence,arandjelovic2019object} approaches train a generative model that disentangles the objects and background by enforcing the object-perturbed image to be considered as real. (b) The noisy-ensemble \citep{nguyen2019deepusps,zhang2018deep,zhang2017supervision} approaches train a model using handcrafted predictions as noisy targets. Despite the training is unsupervised, they initialize the weights with the supervised model. (c) \citet{voynov2020big} manually finds the `salient direction' from the noise (latent) of the ImageNet-trained BigGAN \citep{brock2019large}. Besides, scene decomposition (e.g., \citep{engelcke2021genesis}) aims at a more ambitious goal: fully decompose the objects and background, but currently not scale to the complex images. To our best knowledge, the generative-based approach is the state-of-the-art method for fully unsupervised scenarios. Our proposed ContraCAM could be an alternative in this direction.

\textbf{Class activation map.}
Class activation map \citep{zhou2016learning,selvaraju2017grad} has been used for the weakly-supervised object localization (WSOL), inferring the pixel- (or object-) level annotations using class labels. Specifically, classifier CAM finds the regions that are most salient for the classifier score. ContraCAM further expands its applicability from weakly-supervised to unsupervised object localization by utilizing the contrastive score instead of the classifier score. We think ContraCAM will raise new interesting research questions, e.g., one could adopt the techniques from CAM to the ContraCAM.


\section{Conclusion and Discussion}
\label{sec:conclusion}

We proposed the ContraCAM, a simple and effective self-supervised object localization method using the contrastively trained models. We then introduced two data augmentations upon the ContraCAM that reduce scene bias and improve the quality of the learned representations for contrastive learning. We remark that the scene bias is more severe for the uncurated images; our work would be a step towards strong self-supervised learning under real-world scenarios \citep{goyal2021self,tian2021divide}.

\textbf{Limitations.}
Since the ContraCAM finds the most salient regions, it can differ from the desiderata of the users, e.g., the ContraCAM detects both the birds and branches in the CUB \citep{welinder2010caltech} dataset, but one may only want to detect the birds. Also, though the ContraCAM identifies the disjoint objects, it is hard to separate the occluded objects. Incorporating the prior knowledge of the objects and designing a more careful method to disentangle objects would be an interesting future direction.

\textbf{Potential negative impacts.}
Our proposed framework enforces the model to focus on the ``objects'', or the salient regions, to disentangle the relations of the objects and background. However, ContraCAM may over-rely on the conspicuous objects and the derived data augmentation strategy by ContraCAM could potentially incur imbalanced performance across different objects. We remark that the biases in datasets and models cannot be entirely eliminated without carefully designed guidelines. While we empirically observe our proposed learning strategies mitigate contextual and background biases on certain object types, we still need a closer look at the models, interactively correcting them.

\section*{Acknowledgements}
This work was partly supported by Institute of Information \& Communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST); No. 2019-0-01396, Development of framework for analyzing, detecting, mitigating of bias in AI model and training data; No.2017-0-01779, A machine learning and statistical inference framework for explainable artificial intelligence), and partly by the Defense Challengeable Future Technology Program of the Agency for Defense Development, Republic of Korea. We thank Jihoon Tack, Jongjin Park, and Sihyun Yu for their valuable comments.


{\small
\bibliographystyle{unsrtnat}  % custom unsrt+abbrv
\bibliography{ref}
}

% \newpage
% \input{checklist}

\newpage
\appendix

\input{appendix/algorithm}

\newpage
\input{appendix/details}

\newpage
\input{appendix/additional_loc}

\newpage
\input{appendix/additional_multi}

\newpage
\input{appendix/additional_bg}

\end{document}