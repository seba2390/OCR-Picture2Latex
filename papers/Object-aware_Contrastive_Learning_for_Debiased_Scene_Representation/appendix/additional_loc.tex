\section{Additional localization results}
\label{sec:add-loc}


\subsection{Visualization of ContraCAM without negative signal removal}
\label{sec:add-loc-neg}

Figure~\ref{fig:loc-neg} shows the examples of ContraCAM without negative signal removal. The negative signals from similar objects in different images disturb the localization results by canceling positive signals; spread in random locations. Therefore, removing these signals improves the localization results.

\input{resources_appx_loc/fig_loc_neg}


\subsection{Ablation study on the number of iterations}
\label{sec:add-loc-iter}

We present the ablation study on the number of iterations for the ContraCAM in Table~\ref{tab:loc-iter}. One needs a sufficient number of iterations (e.g., 5) since too small numbers of iterations often detect subregions or miss some objects. Also, note that the CAM shows stable results for large numbers (e.g., 20) of iterations and converges to some stationary values, though it slightly harms the best value of 10.

\input{resources_appx_loc/tab_loc_iter}


\newpage
\subsection{Ablation study on the choice of negative batch}
\label{sec:add-loc-batch}

We study the effects of the negative batch for the ContraCAM. Recall that the ContraCAM finds the most discriminative regions compared to the negative batch; it assumes that images have similar backgrounds but different objects. For a sanity check, we construct a negative batch containing similar objects. Figure~\ref{fig:batch-giraffe} shows an example of the ContraCAM using a giraffe-only and random batch as the negatives. ContraCAM highlights background when compared to the giraffe-only batch.

\input{resources_appx_loc/fig_loc_batch_giraffe}

However, the pathological selection of the negative batch rarely occurs in practice; using a small number of random samples can alleviate the issue. Table~\ref{tab:loc-batch} shows the effects of the negative batch size for the ContraCAM. Using a small batch (e.g., of size 4) almost match the performance of the larger batch (e.g., of size 64). We use the random batch of size 64 for all our experiments.

\input{resources_appx_loc/tab_loc_batch}




\newpage
\subsection{Comparison with the Classifier CAM}
\label{sec:add-loc-cam}

We compare the ContraCAM and classifier CAM under the publicly available supervised classifier and MoCov2 trained on the ImageNet dataset. Somewhat interestingly, the ContraCAM often outperforms the classifier CAM on the transfer setting, e.g., when transferred to the CUB dataset, as shown in Figure~\ref{fig:loc-clfcon}. This is because some samples of the CUB dataset are out-of-class of the ImageNet, and the classifier fails to understand the important features unrelated to the original classes.

\input{resources_appx_loc/fig_loc_cam}

To check whether the superiority of the ContraCAM comes from the score function or better backbone, we also train a linear classifier on top of the MoCov2 backbone using the ImageNet dataset and test the classifier CAM. Table~\ref{fig:loc-clfcon} shows that the ContraCAM on MoCov2 even outperforms the classifier CAM on the same backbone for the ImageNet to CUB transfer scenario.

On the other hand, the table shows that the double expansion trick \citep{choe2020evaluating} of the resolution of penultimate spatial activations is more effective for MoCov2 while degrading the supervised classifier; MoCov2 is trained with stronger augmentations, making CAM robust to the modification of the architecture. Thus, we only apply the expansion trick for the MoCov2 results in Table~\ref{tab:loc-cam}.

\input{resources_appx_loc/tab_loc_cam}


\newpage
\subsection{Comparison with the gradient-based saliency methods}
\label{sec:add-loc-saliency}

We compare the ContraCAM and gradient-based saliency methods using the contrastive score Eq.~\eqref{eq:con-score}. All methods use the same score function but only differ from localization: the weighted sum of activations (i.e., CAM) or directly propagate the gradients to the input space (i.e., gradient-based saliencies). We choose two representative gradient-based saliency methods: Integrated Gradients (IntGrad) \citep{sundararajan2017axiomatic} and SmoothGrad \citep{smilkov2017smoothgrad}, which ensembles multiple gradients for better saliency detection. Specifically, IntGrad ensembles the gradients of the linear interpolation of the image and the zero image, and SmoothGrad ensembles the gradients of the image added by random Gaussian noises. We average ten gradients, either interpolation or Gaussian noises, for both methods.

Figure~\ref{fig:loc-saliency} and Table~\ref{tab:loc-saliency} present the visual examples and quantitative results measured by MaxBoxAccV2, respectively. The gradient-based saliencies provide sparse points as outputs, which can be hard to aggregate as segmentation masks. In contrast, ContraCAM provides smooth maps which are more interpretable and easily used for applications, e.g., post-process to bounding boxes. Furthermore, the gradient-based saliencies detect larger regions than ContraCAM. We think it is due to the negative signals: unlike ContraCAM, it is non-trivial to remove them for the gradient-based saliencies.

\input{resources_appx_loc/tab_loc_saliency}
