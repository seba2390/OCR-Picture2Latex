\section{Implementation details}
\label{sec:details}

We build our code upon the PyTorch \citep{paszke2019pytorch} and PyTorch Lightning\footnote{\url{https://github.com/PyTorchLightning/pytorch-lightning}} library. Further implementation details and additional libraries for each experiment are stated in the remaining subsections.


\subsection{Implementation details for object localization results}
\label{sec:details-loc}

We train MoCov2 under the ResNet-18 architecture on CUB, Flowers, COCO, and ImageNet-9 datasets for the segmentation results. We train the models with batch size 256, COCO, and ImageNet-9 for 800 epochs and CUB and Flowers for 2,000 epochs since the latter has few samples. We follow the augmentations of \citet{he2020momentum}: color jitter with strength (0.4,0.4,0.4,0.1), random grayscale with probability 0.2, and Gaussian blur with kernel size 23 and standard deviation sampled from (0.1,2.0) with probability 0.5; except random crop patches with size (0.08,1.0) instead of the original (0.2,1.0) as it performed better for images with small objects. We use a learning rate of 0.03 with a cosine annealing schedule. These training configurations are applied for all experiments.

We apply the expansion trick \citep{choe2020evaluating}: doubly expand the resolution of penultimate spatial activations by decreasing the stride of the convolutional layer in the final residual block to detect small objects with CAM. Note that we only apply this trick at inference time and do not change the training; namely, the model is trained with the original 7$\times$7 resolution but inferred with the expanded 14$\times$14 of the spatial activations. We also tried training the models using the modified 14$\times$14 resolution but did not see much gain. We run ten iterations for the Iterative ContraCAM and apply the conditional random field following the default hyperparameters\footnote{\url{https://github.com/lucasb-eyer/pydensecrf}} from the pydensecrf library \citep{krahenbuhl2011efficient}. We report the mask mean intersection-over-union (mIoU) between the predicted and ground-truth segmentation masks.

For the comparison of the classifier CAM and ContraCAM, we use the publicly available supervised classifier\footnote{\url{https://pytorch.org/vision/stable/models.html}} and MoCov2\footnote{\url{https://github.com/facebookresearch/moco}} trained on the ImageNet dataset under the ResNet-50 architecture. Here, we do not apply the expansion trick and run a single iteration for the ContraCAM. We report the MaxBoxAccV2 \citep{choe2020evaluating}: averages the ratios of the bounding boxes whose mean intersection-over-unions (mIoUs) are larger than 30\%, 50\%, and 70\% where the boxes for each mIoU percentages are generated by the CAM binarized by the optimal thresholds, on the ImageNet, CUB, Flowers, VOC, and OpenImages dataset following the official evaluation code.\footnote{\url{https://github.com/clovaai/wsolevaluation}} Recall that we report the transfer performance of the predicted CAMs from the ImageNet-trained models for these experiments.


\subsection{Implementation details for contextual bias results}
\label{sec:details-multi}

We train MoCov2 and BYOL under the ResNet-18 and ResNet-50 architectures on the COCO dataset for 800 epochs with batch size 256. We extract the bounding boxes from the binarized CAM masks using the \texttt{findContours} function in the OpenCV library \citep{opencv_library}. We compute the boxes with MoCov2 trained on ResNet-18 and ResNet-50 architectures and use them for the debiased MoCov2 and BYOL using the same architectures. We found that giving some margin for the boxes slightly improves the performance by observing more object boundaries. Specifically, we expand the boxes with 20\% of margins (width for left-and-right and height for up-and-down) found from the experiments using the ground-truth boxes and use the same margins for the CAM boxes. We also remove the small boxes, specifically smaller than 1\% of the image size, to remove vague low-resolution objects.

We follow the linear evaluation scheme of \citet{chen2020simple}: train a $\ell_2$-regularized multinomial logistic regression classifier on top of the pre-computed representation using the L-BFGS \citep{liu1989limited} optimizer. We compute the representation with the center cropped images and choose the $\ell_2$-regularization parameter from ($10^{-6}$,$10^5$) spaced with a range of 45 logarithmically. We evaluate the transfer performance on the COCO-Crop (crop objects of the COCO dataset with 20\% of margins), CIFAR-10, CIFAR-100, CUB, Flowers, Food, and Pets datasets using the linear classifier trained and tested on each dataset. For detection experiments, we follow the fine-tuning configuration of \citet{he2020momentum} evaluated on the COCO dataset. We use the Detectron\footnote{\url{https://github.com/facebookresearch/Detectron}} library for the detection experiments.


\newpage
\subsection{Implementation details for background bias results}
\label{sec:details-bg}

We provide the visual examples of the Background Challenge \citep{xiao2021noise} in Figure~\ref{fig:bg-challenge} and distribution-shifted datasets of ImageNet \citep{deng2009imagenet}: ImageNet-Sketch \citep{wang2019learning}, Stylized-ImageNet \citep{geirhos2019imagenet}, ImageNet-R \citep{hendrycks2020many}, and ImageNet-C \citep{hendrycks2019robustness} datasets in Figure~\ref{fig:dist-shifts}. We train the models on the ImageNet-9 \citep{xiao2021noise}, i.e., the \textsc{Original} dataset of the Background Challenge, which contains 9 superclass (370 class) of the full ImageNet for both background and distribution shifts experiments. Thus, we use the the corresponding 9 superclass subsets of the distribution-shifted datasets, denoted by putting ‘-9’ at the suffix of the dataset names. 

\input{resources_appx/fig_details_bg_challenge}
\input{resources_appx/fig_details_dist_shifts}

We train MoCov2 and BYOL under the ResNet-18 architecture on the \textsc{Original} dataset of the Background Challenge for 800 epochs with batch size 256. We use the ContraCAM masks from MoCov2 to train debiased MoCov2 and BYOL for debiased BYOL. We threshold the CAM values with a threshold of 0.2 to find the largest contour, find the largest rectangle outside the contour to create the background patch and tile it for the background-only image. We train a linear classifier on the \textsc{Original} dataset and evaluate test accuracy on the Background Challenge and distribution-shifted ImageNet 9 superclass subsets for the background and distribution shift results.


