%----------------------------------------------------------------------
%%% First-Order ALGORITHM
%----------------------------------------------------------------------
% !TEX root = ./HBAConicMain.tex

In this section we introduce a first-order potential reduction method for solving \eqref{eq:Opt} that uses a barrier $h \in\scrH_{\nu}(\setK)$ and potential function \eqref{eq:potential}. %Our first-order method employs a quadratic regularization strategy for the linearization of $F_{\mu}(x)$ using the local norm at the current position $x$. 
%We are thus following classical numerical optimization ideas, related to the Levenberg–Morrison–Marquardt techniques \cite{NocWri00}, and also more recently used in \cite{CarGouToi11}. \PD{I'm afraid that this will give an impression that our results are simple and classical.}
We assume that we are able to compute an approximate analytic center at low computational cost. Specifically, our algorithm relies on the availability of a $\nu$-analytic center, i.e. a point $x^{0}\in\setX$ such that 
\begin{equation}\label{eq:analytic_center}
h(x)\geq h(x^{0})-\nu\qquad\forall x\in\setX. 
\end{equation}
To obtain such a point $x^{0}$, one can apply interior point methods to the convex programming problem $\min_{x\in \feas}h(x)$. Moreover, since $\nu \geq 1$ we do not need to solve it with high precision, making the application of computationally cheap first-order method, such as \cite{Dvurechensky:2022tu}, an appealing choice for this preprocessing step. 

\subsection{Local properties}
Given $x\in\setX$, define the set of \emph{feasible directions} as $\scrF_{x}=\{v\in\setE\vert x+v\in\feas\}.$ Lemma \ref{lem:Dikin} implies that 
\begin{equation}\label{eq:Dikinv}
\scrT_{x}=\{v\in\setE\vert \bA v=0,\norm{v}_{x}<1\}\subseteq\scrF_{x}.
\end{equation}
Upon defining $d=[H(x)]^{1/2}v$ for $v\in\scrT_{x}$, we obtain a point  $d \in \R^{{\rm dim}(\setE)}$ satisfying $\bA[H(x)]^{-1/2}d=0$ and $\norm{d}=\norm{v}_{x}$. Hence, for $x\in\setK$, we can equivalently characterize the set $\scrT_{x}$ as $\scrT_{x}=\{[H(x)]^{-1/2}d\vert \bA[H(x)]^{-1/2}d=0,\norm{d}<1\}$.\\ 
Our complexity analysis relies on the ability to control the behavior of the objective function along the set of feasible directions and with respect to the local norm. 
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{assumption}[Local smoothness]
\label{ass:gradLip}
$f:\setE\to\R\cup\{+\infty\}$ is continuously differentiable on $\feas$ and there exists a constant $M>0$ such that for all $x\in\feas$ and $v\in\scrT_{x}$ we have 
\begin{equation}\label{eq:gradLip}
f(x+v) - f(x) - \inner{\nabla f(x),v} \leq \frac{M}{2}\norm{v}_x^2.
\end{equation}
\end{assumption}
\noindent
\begin{remark}
\label{rem:bounded1}
If the set $\bar{\setX}$ is bounded, we have $\lambda_{\min}(H(x)) \geq \sigma$ for some $\sigma >0$. In this case, assuming $f$ has an $M$-Lipschitz continuous gradient, the classical descent lemma \cite{Nes18} implies Assumption \ref{ass:gradLip}. Indeed,
\[
f(x+v) - f(x) - \inner{\nabla f(x),v} \leq \frac{M}{2}\norm{v}^2 \leq \frac{M}{2\sigma}\norm{v}_x^2.
\]
\close
\end{remark}
\begin{remark}
We emphasize that the local Lipschitz smoothness condition \eqref{eq:gradLip} does not require global differentiability. Consider the composite non-smooth and non-convex model \eqref{eq:composite} on $\bar{\setK}_{\text{NN}}$, with $\varphi(s)=s$ for $s \geq 0$. This means $\sum_{i=1}^{n}\varphi(x_{i}^{p})=\norm{x}_{p}^{p}$ for $p\in(0,1)$ and $x\in\bar{\setK}_{\text{NN}}$. As a concrete example for the smooth part of the problem let us consider the $L_{2}$-loss $\ell(x)=\frac{1}{2}\norm{\bN x-\bp}^{2}$. This gives rise to the $L_{2}-L_{p}$ minimization problem, an important optimization formulation arising in phase retrieval, mathematical statistics, signal processing and image recovery \cite{Fou09, GeJiaYe11,Chen:2014wx,LiLiuYaoYe17}. For $x\in\setK_{\text{NN}}$, set $M=\lambda_{\max}(\bN^{\ast}\bN)$, so that 
\[
\ell(x^{+})\leq \ell(x)+\inner{\nabla\ell(x),x^{+}-x}+\frac{M}{2}\norm{x^{+}-x}^{2},
\]
Since $t\mapsto t^{p}$ is concave for $t>0$ and $p\in(0,1)$, we have 
\[
(x_{i}^{+})^{p}\leq  x^{p}_{i}+px_{i}^{p-1}(x^{+}_{i}-x_{i})\qquad i=1,\ldots,n.
\]
Adding all these inequalities together, we immediately arrive at condition \eqref{eq:gradLip} in terms of the Euclidean norm. Over a bounded feasible set $\bar{\setX}$, Remark \ref{rem:bounded1} makes it clear that this implies Assumption \ref{ass:gradLip}. At the same time, $f$ is not differentiable at zero. \close
\end{remark}
We  emphasize that in Assumption \ref{ass:gradLip} the constant $M$ is in general either unknown or is a very conservative upper bound. Therefore, adaptive techniques should be used to estimate it and are likely to improve the practical performance of the method. 

Considering $x\in\setX,v\in\scrT_{x}$ and combining eq. \eqref{eq:gradLip} with eq. \eqref{eq:Dbound} (with $d=v$ and $t=1< \frac{1}{\norm{v}_x} \stackrel{\eqref{eq:boundzeta}}{\leq} \frac{1}{\zeta(x,v)}$) reveals a suitable quadratic model, to be used in the design of our first-order algorithm.
\begin{lemma}[Quadratic Overestimation]
For all $x\in\setX,v\in\scrT_{x}$ and $L\geq M$, we have 
\begin{equation}\label{eq:descentFOM}
F_{\mu}(x+v)\leq F_{\mu}(x)+\inner{\nabla F_{\mu}(x),v}+\frac{L}{2}\norm{v}^{2}_{x}+\mu\norm{v}^{2}_{x}\omega(\zeta(x,v)).
\end{equation}
\end{lemma}


\subsection{Algorithm description and its complexity}
\label{S:FO_descr}
Let $x \in \feas$ be given. Our first-order method employs a quadratic model $ Q^{(1)}_{\mu}(x,v)$ to compute a search direction $v_{\mu}(x)$, given by 
\begin{equation}\label{eq:search}
v_{\mu}(x) \eqdef \argmin_{v\in\setE:\bA v=0} \left\{  Q^{(1)}_{\mu}(x,v) \eqdef F_{\mu}(x) + \inner{\nabla F_{\mu}(x),v}+\frac{1}{2}\norm{v}_{x}^{2} \right\}.
\end{equation}
%\begin{equation}\label{eq:model1}
%Q^{(1)}_{\mu}(x,v) \eqdef F_{\mu}(x) + \inner{\nabla F_{\mu}(x),v}+\frac{1}{2}\norm{v}_{x}^{2}.
%\end{equation}
%We construct a search direction $v_{\mu}(x)$ as the solution of the strongly convex subproblem 
For the above problem, we have the following system of optimality conditions involving the dual variable $y_{\mu}(x)\in\R^{m}$:
\begin{align}
\nabla F_{\mu}(x) + H(x)v_{\mu}(x) - \bA^{\ast} y_{\mu}(x) &= 0, \label{eq:finder_1} \\
\bA  v_{\mu}(x) &=0. \label{eq:finder_2}
\end{align}
Since $H(x)\succ 0$ for $x\in\feas$, any standard solution method \citep{NocWri00} can be applied for the above linear system.
Moreover, this system can be solved explicitly.
Indeed, since $H(x)\succ 0$ for $x\in\feas$, and $\bA$ has full column rank, the linear operator $\bA[H(x)]^{-1}\bA^{\ast}$ is invertible. Hence, $v_{\mu}(x)$  is given explicitly as
\begin{equation*}%\label{eq:v_explicit}
v_{\mu}(x)= - ([H(x)]^{-1}\bA^{\ast}(\bA[H(x)]^{-1}\bA^{\ast})^{-1}\bA[H(x)]^{-1} - [H(x)]^{-1} ) \nabla F_{\mu}(x) \eqdef-\bS_{x}\nabla F_{\mu}(x).
\end{equation*}
To give some intuition behind this expression, observe that we can give an alternative representation of $\bS_{x}$ as $\bS_{x}v = [H(x)]^{-1/2}\Pi_{x}[H(x)]^{-1/2}v$, where
\[
\Pi_{x}v\eqdef v-[H(x)]^{-1/2}\bA^{\ast}(\bA[H(x)]^{-1}\bA^{\ast})^{-1}\bA[H(x)]^{-1/2}v.
\]
This shows that $\bS_{x}$ is just the $\norm{\cdot}_{x}$-orthogonal projection operator onto $\ker(\bA[H(x)]^{-1/2})$. Hence, we can always find a scalar $t>0$ such that $t v_{\mu}(x)\in\setL_{0}$ and $\norm{t v_{\mu}(x)}_{x}<1$. Any such scalar will be a suitable candidate for a step size. To determine an acceptable step-size, consider a point $x \in\setX$, the  search direction $v_{\mu}(x)$ gives rise to a family of parameterized arcs $x^{+}(t)\eqdef x+tv_{\mu}(x)$, where $t\geq 0$. Our aim is to choose this step-size to ensure feasibility of the iterates and decrease of the potential. By \eqref{eq:step_length_zeta} and \eqref{eq:finder_2}, we know that $x^{+}(t)\in\feas$ for all $t\in I_{x,\mu} \eqdef [0,\frac{1}{\zeta(x,v_{\mu}(x))})$. Multiplying \eqref{eq:finder_1} by $v_{\mu}(x)$ and using \eqref{eq:finder_2}, we obtain 
$\inner{\nabla F_{\mu}(x),v_{\mu}(x)}=-\norm{v_{\mu}(x)}_{x}^{2}$. Choosing $t \in  I_{x,\mu}$, we bound
\[
t^{2}\norm{v_{\mu}(x)}_{x}^{2}\omega(t\zeta(x,v_{\mu}(x))) \stackrel{\eqref{eq:omega_upper_bound}}{\leq}  \frac{t^{2}\norm{v_{\mu}(x)}_{x}^{2}}{2(1-t\zeta(x,v_{\mu}(x)))}. 
\]
Therefore, if $t\zeta(x,v_{\mu}(x))\leq 1/2$, we readily see from \eqref{eq:descentFOM} that
%\footnote{Note that since $t\zeta(x,v_{\mu}(x))=\zeta(x,tv_{\mu}(x))$ and we assumed that $t\zeta(x,v_{\mu}(x))\leq 1/2$, it is sufficient to make Assumption \ref{ass:gradLip} on a potentially smaller set $\widetilde{\scrT}_{x}\eqdef \{v\in\setE\vert \bA v=0,\zeta(x,v)\leq 1/2\}$ instead of $\scrT_x$ defined in \eqref{eq:Dikinv}.} 
\begin{align}
F_{\mu}(x^{+}(t))-F_{\mu}(x)&\leq -t\norm{v_{\mu}(x)}_{x}^{2}+\frac{t^{2}M}{2}\norm{v_{\mu}(x)}_{x}^{2}+\mu t^{2}\norm{v_{\mu}(x)}_{x}^{2} \nonumber\\
&= -t \norm{v_{\mu}(x)}_{x}^{2}\left(1-\frac{M+2\mu}{2}t\right) \eqdef -\eta_{x}(t).\label{eq:success}
\end{align}
The function $t \mapsto \eta_{x}(t)$ is strictly concave with the unique maximum at $ \frac{1}{M+2\mu}$, and two real roots at $t\in\left\{0,\frac{2}{M+2\mu}\right\}$. 
Thus, maximizing the per-iteration decrease $\eta_{x}(t)$  under the restriction $0\leq t\leq\frac{1}{2\zeta(x,v_{\mu}(x))}$, we choose the step-size
\begin{equation*}
%\label{eq:}
\ct_{\mu,M}(x)\eqdef \min \left\{\frac{1}{M+2\mu},\frac{1}{2\zeta(x,v_{\mu}(x))}\right\}.
\end{equation*}
This step-size rule, however, requires knowledge of the parameter $M$. To boost numerical performance, we employ a backtracking scheme in the spirit of \cite{NesPol06} to estimate the constant $M$ at each iteration. This procedure generates a sequence of positive numbers $(L_{k})_{k\geq 0}$ for which the local Lipschitz smoothness condition \eqref{eq:gradLip} holds. More specifically, suppose that $x^{k}$ is the current position of the algorithm with the corresponding initial local Lipschitz estimate $L_{k}$ and $v^{k}=v_{\mu}(x^{k})$ is the corresponding search direction. To determine the next iterate $x^{k+1}$, we iteratively try step-sizes $\alpha_k$ of the form $\ct_{\mu,2^{i_k}L_k}(x^{k})$ for $i_k\geq 0$ until the local smoothness condition \eqref{eq:gradLip} holds with $x=x^{k}$, $v= \alpha_k v^{k}$ and local Lipschitz estimate $M=2^{i_k}L_k$, see \eqref{eq:LS}. This process must terminate in finitely many steps, since when $2^{i_k}L_k \geq M$, inequality \eqref{eq:gradLip} with $M$ changed to $2^{i_k}L_k$, i.e., \eqref{eq:LS}, follows from Assumption \ref{ass:gradLip}. Combining the search direction finding problem \eqref{eq:search} with the just outlined backtracking strategy, yields an \underline{A}daptive first-order \underline{H}essian-\underline{B}arrier \underline{A}lgorithm ($\AHBA$, Algorithm \ref{alg:AHBA}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t]
\caption{ \underline{A}daptive first-order \underline{H}essian-\underline{B}arrier \underline{A}lgorithm  - $\AHBA(\mu,\eps,L_{0},x^{0})$}
\label{alg:AHBA}
\SetAlgoLined
\KwData{ $h \in\scrH_{\nu}(\setK)$, $\mu>0,\eps>0,L_0>0,x^{0}\in\setX$.
}
\KwResult{$(x^{k},y^{k},s^{k},L_{k})\in\setX\times\R^{m}\times\setK^{\ast}\times\R_{+}$, where $s^{k}=\nabla f(x^{k}) -\bA^{\ast}y^{k}$, and $L_{k}$ is the last estimate of the Lipschitz constant.}
%Set $L_0 > 0$ -- initial guess for $M$, 
Set $k=0$\;
\Repeat{
		$\norm{v^k}_{x^k} < \tfrac{\eps}{\nu}$ 
	}{	
		Set $i_k=0$. Find $v^k\eqdef v_{\mu}(x^k)$ and  the corresponding dual variable $y^k\eqdef y_{\mu}(x^k)$ as the solution to
		\begin{equation}\label{eq:finder}
		\min_{v\in\setE:\bA v=0}\{F_{\mu}(x^k) + \inner{\nabla F_{\mu}(x^k),v}+\frac{1}{2}\norm{v}_{x^{k}}^{2}\}. 
		\end{equation}
		\Repeat{
			\begin{equation}				
				f(z^{k}) \leq f(x^{k}) + \inner{\nabla f(x^{k}),z^{k}-x^{k}}+2^{i_{k}-1}L_{k}\norm{z^{k}-x^{k}}^{2}_{x^{k}}.
				\label{eq:LS}
			\end{equation}
		}
		{
			\begin{equation}\label{eq:alpha_k}
				\alpha_k \eqdef  \min \left\{\frac{1}{2^{i_k}L_{k} + 2 \mu},\frac{1}{2\zeta(x^k,v^k)} \right\},  	\text{where $\zeta(\cdot,\cdot)$ as in \eqref{eq:zeta}}	
			\end{equation}
			%%%%%%%%%%%%%%%
			Set $z^{k}=x^{k} + \alpha_k v^k$, $i_k=i_k+1$\;%, $i_k=i_k+1$\;
		}
		Set $L_{k+1} = 2^{i_k-1}L_{k}$, $x^{k+1}=z^{k}$, $k=k+1$\;%i_k-2 is since when we find a suitable $i_k$ we still set $i_k=i_k+1$
	}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our main result on the iteration complexity of Algorithm \ref{alg:AHBA} is the following Theorem, whose proof is given in Section \ref{sec:ProofFOM}. 
\begin{theorem}
\label{Th:AHBA_conv}
Let Assumptions \ref{ass:1}-\ref{ass:gradLip} hold. Fix the error tolerance $\eps>0$, the regularization parameter $\mu=\frac{\eps}{\nu}$, and some initial guess $L_0>0$ for the Lipschitz constant. Let $(x^{k})_{k\geq 0}$ be the trajectory generated by $\AHBA(\mu,\eps,L_{0},x^{0})$, where $x^{0}$ is a $\nu$-analytic center satisfying \eqref{eq:analytic_center}. Then the algorithm stops in no more than 
\begin{equation}
\label{eq:FO_main_Th_compl}
\K_{I}(\eps,x^{0})= \ceil[\bigg]{4(f(x^{0}) - f_{\min}(\setX)+ \eps) \frac{\nu^{2}(\max\{M,L_0\}+\eps/\nu)}{\eps^{2}}}
\end{equation}
outer iterations, and the number of inner iterations is no more than $2(\K_{I}(\eps,x^{0})+1)+\max\{\log_{2}(M/L_{0}),0\}$. Moreover, the last iterate obtained from $\AHBA(\mu,\eps,L_{0},x^{0})$ constitute a $2\eps$-KKT point for problem \eqref{eq:Opt} in the sense of Definition \ref{def:eps_KKT}.
%\begin{align}
%&\|\nabla f(x^{k}) - \bA^{\ast}y^{k} - s^{k} \| = 0 \leq 2\eps, \label{eq:FO_main_Th_eps_KKT_1} \\
%& |\inner{s^{k},x^{k}}| \leq 2\eps, \label{eq:FO_main_Th_eps_KKT_2}  \\
%& \bA x^{k}=b, s^{k} \in \setK^{\ast}, x^{k}\in\setK.   \label{eq:FO_main_Th_eps_KKT_3}
%\end{align}  
\end{theorem}
 %%%%%%%%
\begin{remark}
The line-search process of finding the appropriate $i_k$ is simple since only recalculating $z^k$ is needed, and repeatedly solving problem \eqref{eq:finder} is not required. Furthermore, the sequence of constants $L_k$ is allowed to decrease along subsequent iterations, which is achieved by the division by the constant factor 2 in the final updating step of each iteration. This potentially leads to longer steps and faster decrease of the potential.
\close
\end{remark}
%%%%%%%%%%%%%%%%%
\begin{remark}
\label{rem:FO_complexity_simplified}
Since $\nu \geq 1$, $f(x^{0}) - f_{\min}(\setX)$ is expected to be larger than $\eps$, and the constant $M$ is potentially large, we see that the main term in the complexity bound \eqref{eq:FO_main_Th_compl} is $O\left(\frac{M\nu^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)=O(\frac{\nu^{2}}{\eps^{2}})$, i.e. has the same dependence on $\eps$ as the standard complexity bounds  \cite{CarDucHinSid19b,CarDucHinSid19,lan2020first} of first-order methods for non-convex problems under the standard Lipschitz-gradient assumption, which on bounded sets is subsumed by our Assumption \ref{ass:gradLip}. Further, if the function $f$ is quadratic, Assumption \ref{ass:gradLip} holds with $M=0$ and we can take $L_0=0$. In this case, the complexity bound \eqref{eq:FO_main_Th_compl} improves to $O\left(\frac{\nu(f(x^{0}) - f_{\min}(\setX))}{\eps}\right)$. 

Just like classical interior-point methods, the iteration complexity of $\AHBA$ depends on the barrier parameter $\nu\geq 1$. For conic domains, the characterization of this barrier parameter has thus been an active research line. \cite{GulTun98} demonstrated that for symmetric cones, the barrier parameter is equivalent to algebraic properties of the cone and identified it with the rank of the cone (see \cite{FarKor94} for a definition of the rank of a symmetric cone). This deep analysis gives an exact characterization of the optimal barrier parameter for the most important conic domains in optimization. For $\setK_{\text{NN}}$ and $\setK_{\text{SDP}}$, it is known that $\nu=n$ is optimal, whereas for $\setK_{\text{SOC}}$ the optimal barrier parameter is $\nu=2$ (and therefore independent of the ambient dimension $n$). 
\close
\end{remark}
%\begin{remark}\label{rem:KKT1st}
%Consider the special case of problem \eqref{eq:Opt} with non-negativity constraints, i.e., with $\bar{\setK}=\bar{\setK}_{\text{NN}}$. In this case, our algorithm generates $(x^{k},y^{k},s^{k})$ satisfying $x^{k}\in\setK_{\text{NN}},s^{k}=\nabla f(x^k) -\bA^{\ast}y^{k}\in\setK_{\text{NN}}$, i.e., $x^{k},s^{k}\geq 0$, and approximate complementary slackness $\sum_{i=1}^{n}\abs{x_{i}^{k}s_{i}^{k}}=\sum_{i=1}^{n}x_{i}^{k}s_{i}^{k}\leq \eps$ after $O\left(\frac{M \nu^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ iterations. 
%A natural alternative formulation of the approximate complementary slackness condition is $\abs{\hat{x}_{i}\hat{s}_{i}}\leq \delta$ for all $i=1,\ldots,n$, or equivalently $\max_{1\leq i\leq n}\abs{\hat{x}_{i}\hat{s}_{i}}\leq \delta$, where $\hat{s}=\nabla f(\hat{x})-\bA^{\ast}\hat{y}$. This condition is imposed in \cite{HaeLiuYe18}, where, under an assumption similar to our Assumption \ref{ass:gradLip}, they guarantee that $\max_{1\leq i\leq n}\abs{\hat{x}_{i}\hat{s}_{i}}\leq \delta$ in $O\left(\frac{M (f(x^{0}) - f_{\min}(\setX))}{\delta^2}\right)$ iterations. We see that our notion of complementarity is stronger since $\max_{1\leq i\leq n}\abs{\hat{x}_{i}\hat{s}_{i}} \leq \sum_{i=1}^{n}\abs{\hat{x}_{i}\hat{s}_{i}} \leq n \max_{1\leq i\leq n}\abs{\hat{x}_{i}\hat{s}_{i}}$, where both equalities are achievable. Moreover,  to match our stronger guarantee, one has to take $\delta=\eps/n$ in the algorithm of \cite{HaeLiuYe18}, which leads to complexity $O\left(\frac{M n^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ similar to ours since in this case $\nu=n$.	Further benefit of our algorithm is that it is designed for general cones, rather than only for $\bar{\setK}_{\text{NN}}$.
%%
%%
%%For problem \eqref{eq:Opt} on the non-negative orthant $\bar{\setK}_{\text{NN}}$, a natural formulation of the approximate complementary slackness condition is $\abs{\hat{x}_{i}[\nabla f(\hat{x})-\bA^{\ast}\hat{y}]_{i}}\leq \delta$ for all $i=1,\ldots,n$. This condition is imposed in \cite{HaeLiuYe18}, where, under the assumption similar to our Assumption \ref{ass:gradLip}, they guarantee that $\max_{1\leq i\leq n}\abs{\hat{x}_{i}[\nabla f(\hat{x})-\bA^{\ast}\hat{y}]_{i}}\leq \delta$ in $O\left(\frac{M (f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ iterations. In the same setting, our notion of complementarity is stronger. Indeed, since our first-order $\eps$-KKT point $(x^{k},y^{k},s^{k})$ satisfies $x^{k}\in\setK_{\text{NN}},s^{k}=\nabla f(x^k) -\bA^{\ast}y^{k}\in\setK_{\text{NN}}$, and $\sum_{i=1}^{n}x_{i}^{k}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}\leq \eps$ after $O\left(\frac{M \nu^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$. Since $\sum_{i=1}^{n}x_{i}^{k}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}\leq n \max_{1\leq i\leq n}\abs{\hat{x}_{i}[\nabla f(\hat{x})-\bA^{\ast}\hat{y}]_{i}}\leq \delta$ (and the equality is achievable), we have that to match our guarantee, one has to take $\delta=\eps/n$ in the algorithm of \cite{HaeLiuYe18} which leads to complexity $O\left(\frac{M n^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ similar to ours since in this case $\nu=n$.	
%%
%%
%%in terms of the uniform norm $\max_{1\leq i\leq n}\abs{x_{i}[\nabla f(x)-\bA^{\ast}y]_{i}}\leq\eps$. In the worst case this could mean that $x_{i}[\nabla f(x)-\bA^{\ast}y]_{i}=\eps$ for all $i=1,\ldots,n$. Our complementarity measure uses the inner product between primal and dual variables. Hence, on the non-negative orthant, our first-order $\eps$-KKT point $(x^{k},y^{k},s^{k})$ satisfies $x^{k}\in\setK_{\text{NN}},s^{k}=\nabla f(x^k) -\bA^{\ast}y^{k}\in\setK_{\text{NN}}$, and $\sum_{i=1}^{n}x_{i}^{k}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}\leq \eps$. Since $n\max_{1\leq i\leq n}\abs{x^{k}_{i}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}}\leq \sum_{i=1}^{n}x^{k}_{i}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}$, we see that our definition is stronger by a factor $1/n$.
%%
%%
%%For problem \eqref{eq:Opt} on the non-negative orthant $\bar{\setK}_{\text{NN}}$, a natural formulation of the approximate complementary slackness condition is $\abs{x_{i}[\nabla f(x)-\bA^{\ast}y]_{i}}\leq \eps$ for all $i=1,\ldots,n$. This condition is imposed in \cite{HaeLiuYe18} in terms of the uniform norm $\max_{1\leq i\leq n}\abs{x_{i}[\nabla f(x)-\bA^{\ast}y]_{i}}\leq\eps$. In the worst case this could mean that $x_{i}[\nabla f(x)-\bA^{\ast}y]_{i}=\eps$ for all $i=1,\ldots,n$. Our complementarity measure uses the inner product between primal and dual variables. Hence, on the non-negative orthant, our first-order $\eps$-KKT point $(x^{k},y^{k},s^{k})$ satisfies $x^{k}\in\setK_{\text{NN}},s^{k}=\nabla f(x^k) -\bA^{\ast}y^{k}\in\setK_{\text{NN}}$, and $\sum_{i=1}^{n}x_{i}^{k}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}\leq \eps$. Since $n\max_{1\leq i\leq n}\abs{x^{k}_{i}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}}\leq \sum_{i=1}^{n}x^{k}_{i}[\nabla f(x^k) -\bA^{\ast}y^{k}]_{i}$, we see that our definition is stronger by a factor $1/n$.
%\close
%\end{remark}

\paragraph{Connection with interior point flows on polytopes.}
Consider $\bar{\setK}=\bar{\setK}_{\text{NN}}$, and $\setX=\setK_{\text{NN}}\cap\setL$. We are given a function $f:\bar{\setX}\to\R$ which is the restriction of a smooth function $f:\Rn\to\R$. %Let $\nabla f(x)=[\partial_{1}f(x),\ldots,\partial_{n}f(x)]^{\top}$ denote the gradient in $\Rn$ at $x\in\setX$. 
The canonical barrier for this setting is $h(x)=-\sum_{i=1}^{n}\ln(x_{i})$, so that $H(x)=\diag[x_{1}^{-2},\ldots,x_{n}^{-2}]=\XX^{-2}$ for $x\in\setX$. Applying our first-order method on this domain gives the search direction 
$v_{\mu}(x)=-\bS_{x}\nabla F_{\mu}(x)=-\XX(\bI-\XX\bA^{\top}(\bA\XX^{2}\bA^{\top})^{-1}\bA\XX)\XX\nabla F_{\mu}(x)$. This explicit formula yields various interesting connections between our approach and classical methods. For $\bA=\1_{n}^{\top}$, the feasible set $\setX$ reduces to the relative interior of the $(n-1)$-dimensional unit simplex. In this case, the vector field $v_{\mu}(\cdot)$ simplifies further to 
\[
[v_{\mu}(x)]_{i}=[\XX^{2}\nabla F_{\mu}(x)]_{i}-\frac{x_{i}^{2}}{\sum_{j=1}^{n}x_{j}^{2}}\sum_{j}[\XX^{2}\nabla F_{\mu}(x)]_{j} \quad 1\leq i\leq n,
\]
Observe that $v_{\mu}(x)\in (\1_{n})^{\bot}=\ker(\1_{n}^{\top})$. For $f(x)=c^{\top}x$ and $\mu=0$, we further obtain from this formula the search direction employed in \emph{affine scaling} methods for linear programming \cite{BayLag89,BayLag89II,AdlMont91,TseBomSch11}. 
%\[
%\bS_{x}c=\XX^{2} c-\XX^{2}\bA^{\top}(\bA\XX^{2}\bA^{\top})^{-1}\bA\XX^{2}c.
%\]
%This map is the search direction in \emph{affine scaling} methods for linear programming \cite{BayLag89,BayLag89II,AdlMont91}, a fundamental class of interior-point methods. Extensions to quadratic programming have been studied in various papers, notably by \cite{Ye92,Tse04,TsuMon96,Ye98}.
%
\cite{HBA-linear} partly motivated their algorithm as a discretization of the Hessian-Riemannian gradient flows introduced in \cite{ABB04} and \cite{BolTeb03}. Heuristically, we can therefore interpret $\AHBA$ as an Euler discretization (with non-monotone adaptive step-size policies) of the gradient-like flow $\dot{x}(t)=-\bS_{x(t)}\nabla F_{\mu}(x(t))$, which resembles very much the class of dynamical systems introduced in \cite{BolTeb03}. This gives an immediate connection to a large class of interior point flows on polytopes, heavily studied in control theory \cite{HelMoo96}.
%%Proof%%%
\subsection{Proof of Theorem \ref{Th:AHBA_conv}}
\label{sec:ProofFOM}
Our proof proceeds in several steps. First, we show that procedure $\AHBA(\mu,\eps,L_{0},x^{0})$ produces points in $\setX$, and, thus, is indeed an interior-point method. Next, we show that the line-search process of finding appropriate $L_k$ in each iteration is finite, and estimate the total number of trials in this process. Then we enter the core of our analysis where we prove that if the stopping criterion does not hold at iteration $k$, i.e. $\norm{v^{k}}_{x^{k}} \geq \tfrac{\eps}{\nu}$, then the objective $f$ is decreased by a quantity $O(\eps^{2})$, and, since the objective is globally lower bounded, we conclude that the method stops in at most $O(\eps^{-2})$ iterations. Finally, we show that when the stopping criterion holds, the method has generated an $\eps$-KKT point. 

\subsubsection{Interior-point property of the iterates}
\label{S:FO_correct}
By construction $x^{0}\in\setX$. Proceeding inductively, let $x^{k}\in\setX$ be the $k$-th iterate of the algorithm, delivering the search direction $v^{k}=v_{\mu}(x^{k})$. 
By eq. \eqref{eq:alpha_k}, the step-size $\alpha_k$ satisfies $\alpha_{k}\leq \frac{1}{2\zeta(x^{k},v^{k})}$, and, hence, $\alpha_{k}\zeta(x^{k},v^{k})\leq 1/2$ for all $k\geq 0$. 
Thus, by \eqref{eq:step_length_zeta} $x^{k+1}=x^{k}+\alpha_{k}v^{k} \in\setK$. Since, by \eqref{eq:finder}, $\bA v^{k} =0$, we have that $x^{k+1} \in \setL$. Thus, $x^{k+1}\in\setK \cap \setL=\setX$. By induction, we conclude that $(x^{k})_{k\geq 0}\subset\setX$. 

\subsubsection{Bounding the number of backtracking steps}
\label{sec:backtrack1}
Let us fix iteration $k$. Since the sequence $2^{i_k} L_k $ is increasing as $i_k$ is increasing, and Assumption \ref{ass:gradLip} holds, we know that when $2^{i_k} L_k \geq \max\{M,L_k\}$, the line-search process for sure stops since inequality \eqref{eq:LS} holds. 
%Hence, if $L_0 \leq M$, we have that $2^{i_k} L_k \leq 2M$. Otherwise, if $L_0 > M$, we have 
Hence, $2^{i_k} L_k \leq 2\max\{M,L_k\}$ must be the case, and, consequently, $L_{k+1} = 2^{i_k-1} L_k \leq \max\{M,L_k\}$, which, by induction, gives $L_{k+1} \leq \bar{M}\eqdef\max\{M,L_0\}$. 
%By construction we have $L_{k+1}=\frac{1}{2}2^{i_{k}}L_{k}$. 
At the same time, $\log_{2}\left(\frac{L_{k+1}}{L_{k}}\right)= i_{k}-1$, $\forall k\geq 0$. Let $N(k)$ denote the number of inner line-search iterations up to the $k-$th iteration of $\AHBA(\mu,\eps,L_{0},x^{0})$. Then, using that $L_{k+1} \leq \bar{M}=\max\{M,L_0\}$, 
\begin{align*}
N(k)&=\sum_{j=0}^{k}(i_{j}+1)=\sum_{j=0}^{k} (\log_{2}(L_{j+1}/L_{j})+2 ) \leq 2(k+1)+\max\{\log_{2}(M/L_{0}),0\}.
\end{align*}
This shows that on average the inner loop ends after two trials. 

\subsubsection{Per-iteration analysis and a bound for the number of iterations}
%Given $\eps>0$, we choose $\mu=\eps/\nu$ and $x^{0}$ a $\nu$-analytic center. 
Let us fix iteration counter $k$. Since $L_{k+1} = 2^{i_k-1}L_k$, the step-size \eqref{eq:alpha_k} reads as $\alpha_{k}=\min \left\{\frac{1}{2L_{k+1} + 2 \mu},\frac{1}{2\zeta(x^k,v^k)} \right\}$. Hence, $\alpha_{k}\zeta(x^{k},v^{k})\leq 1/2$, and \eqref{eq:success} with the identification $t=\alpha_k = \ct_{\mu,2L_{k+1}}(x^{k})$, $M=2L_{k+1}$, $x=x^{k}$, $v_{\mu}(x^{k}) \eqdef v^k$ gives:  
\begin{equation}
\label{eq:FO_per_iter_proof_2}
F_{\mu}(x^{k+1})-F_{\mu}(x^{k})\leq -\alpha_k \norm{v^k}_{x^k}^{2}\left(1-(L_{k+1}+\mu)\alpha_k\right) \leq -\frac{\alpha_k \norm{v^k}_{x^k}^{2}}{2},
\end{equation}
where we used that $\alpha_k \leq \frac{1}{2(L_{k+1}+\mu)}$ in the last inequality. 
Substituting into \eqref{eq:FO_per_iter_proof_2} the two possible values of the step-size $\alpha_k$ in \eqref{eq:alpha_k} gives
%Next, we consider two possible cases of the value of the step-size $\alpha_k$ and substitute it into \eqref{eq:FO_per_iter_proof_2}.
\begin{equation}
\label{eq:per_iter_decr_0}
F_{\mu}(x^{k+1})-F_{\mu}(x^{k})\leq 
\left\{
\begin{array}{ll}
- \frac{\norm{v^{k}}_{x^{k}}^{2} }{4(L_{k+1}+\mu)} & \text{if }  \alpha_k=\frac{1}{2(L_{k+1}+\mu)}\\

- \frac{\norm{v^{k}}_{x^{k}}^{2} }{4\zeta(x^k,v^k)}  \stackrel{\eqref{eq:boundzeta}}{\leq} - \frac{\norm{v^{k}}_{x^{k}}}{4} & \text{if }  \alpha_k=\frac{1}{2\zeta(x^k,v^k)}.
\end{array}\right.
\end{equation}
Recalling $L_{k+1} \leq \bar{M}$ (see section \ref{sec:backtrack1}), we obtain that 
\begin{equation}
\label{eq:per_iter_decr}
F_{\mu}(x^{k+1}) - F_{\mu}(x^{k}) \leq  -\frac{\norm{v^{k}}_{x^{k}}}{4} \min\left\{1,  \frac{\norm{v^{k}}_{x^{k}}}{\bar{M}+\mu}\right\}=-\delta_{k}.
\end{equation}
Rearranging and summing these inequalities for $k$ from $0$ to $K-1$ gives
\begin{align}
&K\min_{k=0\ldots,K-1} \delta_{k} \leq  \sum_{k=0}^{K-1}\delta_{k}\leq F_{\mu}(x^{0})-F_{\mu}(x^{K}) \notag \\
& \quad\stackrel{\eqref{eq:potential}}{=} f(x^{0}) - f(x^{K}) + \mu (h(x^{0}) - h(x^{K})) \leq f(x^{0}) - f_{\min}(\setX) + \eps, \label{eq:FO_per_iter_proof_6} 
\end{align}
where we used that, by the assumptions of Theorem \ref{Th:AHBA_conv}, $x^{0}$ is a $\nu$-analytic center defined in \eqref{eq:analytic_center} and $\mu = \eps/\nu$, implying that $h(x^{0}) - h(x^{K}) \leq \nu = \eps/\mu$.
Thus, up to passing to a subsequence, $\delta_{k}\to 0$, and consequently $\norm{v^{k}}_{x^{k}} \to 0$ as $k \to \infty$. This shows that the stopping criterion in Algorithm \ref{alg:AHBA} is achievable.

Assume now that the stopping criterion $\norm{v^k}_{x^k} < \frac{\eps}{ \nu}$ does not hold for $K$ iterations of $\AHBA$. Then, for all $k=0,\ldots,K-1,$ it holds that 
$\delta_{k}\geq \min\left\{\frac{\eps}{4 \nu},\frac{\eps^{2}}{4 \nu^{2}(\bar{M}+\mu)}\right\}$. 
Together with the parameter coupling $\mu=\frac{\eps}{\nu}$, it follows from \eqref{eq:FO_per_iter_proof_6} that
\[
K\frac{\eps^{2}}{4 \nu^{2}(\bar{M}+\eps/\nu)} = K \min\left\{\frac{\eps}{4 \nu},\frac{\eps^{2}}{4 \nu^{2}(\bar{M}+\eps/\nu)}\right\}\leq f(x^{0})-f_{\min}(\setX)+\eps.
\]
Hence, recalling that $\bar{M}=\max\{M,L_0\}$,
\[
K \leq 4(f(x^{0}) - f_{\min}(\setX)+ \eps) \cdot \frac{\nu^{2}(\max\{M,L_0\}+\eps/\nu)}{\eps^{2}},%\max\left\{\frac{\nu}{\eps},\frac{\nu^{2}(M+\eps/\nu)}{\eps^{2}}\right\},
\] 
i.e., the algorithm stops for sure after no more than this number of iterations. This, combined with the bound for the number of inner steps in Section \ref{sec:backtrack1}, proves the first statement of Theorem \ref{Th:AHBA_conv}.

\subsubsection{Generating $\eps$-KKT point}
To finish the proof of Theorem \ref{Th:AHBA_conv}, we now show that when Algorithm \ref{alg:AHBA} stops for the first time, it returns a $2\eps$-KKT point of \eqref{eq:Opt} according to Definition \ref{def:eps_KKT}.

Let the stopping criterion hold at iteration $k$. By the optimality condition \eqref{eq:finder_1} and the definition of the potential \eqref{eq:potential}, we have
\begin{equation}
\label{eq:FO_KKT_proof_0}
\nabla f(x^{k})-\bA^{\ast}y^{k}+\mu \nabla h(x^{k}) =-H(x^{k})v^{k}\iff [H(x^{k})]^{-1}\left(\nabla f(x^{k})-\bA^{\ast}y^{k}+\mu \nabla h(x^{k}) \right)=-v^{k}.
\end{equation}
Denoting $g^{k}\eqdef-\mu\nabla h(x^{k})$, multiplying both equations, and using the stopping criterion $\norm{v^{k}}_{x^{k}} < \frac{\eps}{\nu}$, we conclude 
\begin{equation}
\label{eq:FO_KKT_proof_00}
\norm{\nabla f(x^{k})-\bA^{\ast}y^{k}-g^{k}}^{\ast}_{x^{k}}=\norm{v^{k}}_{x^{k}}<\frac{\eps}{\nu}.
\end{equation}
Whence, setting $s^{k}\eqdef\nabla f(x^{k})-\bA^{\ast}y^{k}\in\setE^{\ast}$, we get, by the definition of the dual norm, 
\begin{align}
\frac{\eps}{\nu} > \norm{v^{k}}_{x^{k}}&=\norm{s^{k}-g^{k}}^{\ast}_{x^{k}} \label{eq:FO_KKT_proof_1}\\
&=\norm{s^{k}-g^{k}}_{[H(x^{k})]^{-1}}\stackrel{\eqref{eq:relations}}{=}\norm{s^{k}-g^{k}}_{\nabla^{2}h_{\ast}(-\nabla h(x^{k}))} \\
&=\norm{s^{k}-g^{k}}_{\nabla^{2}h_{\ast}(\frac{1}{\mu}g^{k})} = \mu\norm{s^{k}-g^{k}}_{\nabla^{2}h_{\ast}(g^{k})}, \notag
\end{align}
where in the last equality we used that since $h_{\ast}\in\scrH_{\nu}(\setK^{\ast})$, by \eqref{eq:log_hom_scb_hess_homog_prop}, 
%$\frac{1}{t^{2}}\nabla^{2}h_{\ast}(s)=\nabla^{2}h_{\ast}(ts)$. Therefore, 
$\nabla^{2}h_{\ast}(\frac{1}{\mu}g^{k})=\mu^{2}\nabla^{2}h_{\ast}(g^{k})$.
%, and we conclude $\norm{v^{k}}_{x^{k}}=\mu\norm{s^{k}-g^{k}}_{\nabla^{2}h_{\ast}(g^{k})}.$
%Whence, since the stopping criterion $\norm{v^{k}}_{x^{k}} < \frac{\eps}{\nu}$ holds at iteration $k$,
Thus, we arrive at
\begin{align}
\norm{s^{k}-g^{k}}_{\nabla^{2}h_{\ast}(g^{k})} = \frac{\norm{v^{k}}_{x^{k}}}{\mu} < \frac{\eps}{\mu \nu} = 1, \label{eq:FO_KKT_proof_2}
\end{align}
where in the last equality we used that, by the assumptions of Theorem \ref{Th:AHBA_conv}, $\mu=\frac{\eps}{\nu}$.
Thus, since, by \eqref{eq:relations_1}, $g^{k}=-\mu\nabla h(x^{k})\in \setK^{\ast}$, we get that $s^{k}\in\setK^{\ast}$. By construction, $x^{k}\in \setK$ and $\bA x^{k} = b$. Thus, \eqref{eq:eps_optim_equality_cones} holds.
Furthermore, $\|\nabla f(x^{k})-\bA^{\ast}y^{k} - s^{k}\|=0\leq 2 \eps$, meaning that \eqref{eq:eps_optim_grad} holds. 
Finally, since $(x^{k},s^{k})\in\setK\times\setK^{\ast}$, we see 
\begin{align}
0 \leq \inner{s^{k},x^{k}}&=\inner{s^{k}-g^{k},x^{k}}+\inner{g^{k},x^{k}} \notag\\
&\leq \norm{s^{k}-g^{k}}_{x^{k}}^{\ast}\cdot\norm{x^{k}}_{x^{k}}-\mu\inner{\nabla h(x^{k}),x^{k}} \notag \\
&\stackrel{\eqref{eq:FO_KKT_proof_1},\eqref{eq:log_hom_scb_norm_prop},\eqref{eq:log_hom_scb_hess_prop}}{=}\norm{v^{k}}_{x^{k}}\sqrt{\nu}+\mu\nu \notag \\
&<\sqrt{\nu}\frac{\eps}{\nu}+\eps\leq 2\eps, \label{eq:FO_KKT_proof_3}
\end{align}
where the last inequality uses $\nu\geq 1$. Hence, the complementarity condition \eqref{eq:eps_optim_complem} holds as well. This finishes the proof of Theorem \ref{Th:AHBA_conv}.

\subsection{Discussion}
\label{sec:FO_discussion}
\paragraph{Strengthened KKT condition.}
%The main technical challenge in our analysis is to guarantee the approximate complementarity condition \eqref{eq:eps_optim_complem}. For this, we need to show that the dual variable $s^{k}$ belongs to the dual cone $\setK^{\ast}$, and second, we need to show that \eqref{eq:eps_optim_complem} holds. Another challenge is to translate the condition \eqref{eq:FO_KKT_proof_00} stated in terms of the local norm to the condition \eqref{eq:eps_optim_grad} formulated in terms of the standard Euclidean norm. In our setting of general, potentially non-symmetric, cones all this is more difficult than for the prominent (symmetric) cone case $\bar{\setK}=\bar{\setK}_{\text{NN}}$, as studied in \cite{HaeLiuYe18}. % where a first-order algorithm is also proposed. 
%Let us compare the bounds reported in that references with ours. 
For $\bar{\setK}=\bar{\setK}_{\text{NN}}$, \cite{HaeLiuYe18} consider a first-order potential reduction method employing the standard log-barrier $h(x)=-\sum_{i=1}^n \ln(x_i)$ using a trust-region subproblem for obtaining the search direction. For $x\in\setK_{\text{NN}}$, we have $\nabla h(x)=[-x_{1}^{-1},\ldots,-x_{n}^{-1}]^{\top}$, $H(x)=\diag[x_{1}^{-2},\ldots,x_{n}^{-2}]=\XX^{-2}$. Combining \eqref{eq:FO_KKT_proof_0}, the information $[H(x^{k})]^{-1/2}\nabla h(x^{k})= - \1_{n},\;\nu=n$, and the stopping criterion of Algorithm \ref{alg:AHBA} at iteration $k$, saying that $\norm{v^{k}}_{x^{k}}<\frac{\eps}{\nu}$, we see 
%\begin{equation}
%\label{eq:FO_remarks_1}
\begin{align*}
\norm{H(x^{k})^{-\frac{1}{2}} (\nabla f(x^{k})-\bA^{\ast}y^{k}) - \mu \1_{n}}_{\infty}& \leq\norm{H(x^{k})^{-\frac{1}{2}} (\nabla f(x^{k})-\bA^{\ast}y^{k}) - \mu \1_{n}  }\\
& = \norm{-H(x^{k})^{\frac{1}{2}} v^{k}} < \frac{\eps}{n}.
\end{align*}
Therefore, since $\mu=\eps/n$ and $s^{k}=\nabla f(x^{k})-\bA^{\ast}y^{k}\in\setK^{\ast}_{\text{NN}}=\Rn_{++}$, we obtain from the triangle inequality
\[
0<\norm{\XX^{k}s^{k}}_{\infty}\leq \norm{H(x^{k})^{-1/2} s^{k}-\mu\1_{n}}_{\infty}+\mu\leq\frac{2\eps}{n}.
\]
By Remark \ref{rem:FO_complexity_simplified}, these inequalities are achieved after $O\left(\frac{M n^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ iterations of $\AHBA$, and they are seen to be by the factor $\frac{1}{n}$ sharper than the complementarity measure employed in \cite{HaeLiuYe18}. Conversely, in order to attain an approximate KKT point with the same strength as in \cite{HaeLiuYe18}, the above calculations suggest that we can weaken our tolerance from $\eps$ to $\eps\cdot n$, which results in an overall iteration complexity of   
$O\left(\frac{M  (f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$, and a complementarity measure $\norm{\XX^{k}s^{k}}_{\infty}\leq 2\eps$. %
%
Thus, in the particular case of non-negativity constraints our general algorithm is able to obtain results similar to \cite{HaeLiuYe18}, but under weaker assumptions. At the same time, our algorithm ensures a stronger measure of complementarity. Indeed, our algorithm guarantees that $x^{k}\in\setK_{\text{NN}},s^{k}=\nabla f(x^k) -\bA^{\ast}y^{k}\in\setK_{\text{NN}}$, i.e., $x^{k},s^{k}\geq 0$, and approximate complementary $0\leq \sum_{i=1}^{n}\abs{x_{i}^{k}s_{i}^{k}}=\sum_{i=1}^{n}x_{i}^{k}s_{i}^{k}\leq 2\eps$ after $O\left(\frac{M n^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ iterations, which is stronger than $\max_{1\leq i\leq n}\abs{x_{i}^{k}s_{i}^{k}} \leq 2\eps$ guaranteed by \cite{HaeLiuYe18}. Indeed, $\max_{1\leq i\leq n}\abs{x_{i}^{k}s_{i}^{k}} \leq \sum_{i=1}^{n}\abs{x_{i}^{k}s_{i}^{k}} \leq n \max_{1\leq i\leq n}\abs{x_{i}^{k}s_{i}^{k}}$, and both equalities are achievable. Moreover,  to match our stronger guarantee, one has to change $\eps \to\eps/n$ in the complexity bound of \cite{HaeLiuYe18}, which leads to the same $O\left(\frac{M n^2(f(x^{0}) - f_{\min}(\setX))}{\eps^2}\right)$ complexity bound. Besides this important insights, our algorithm is designed for general cones, rather than only for $\bar{\setK}_{\text{NN}}$. Therefore, we provide a unified approach for essentially all conic domains of relevance in optimization. Finally, our method does not rely on the trust-region techniques as in \cite{HaeLiuYe18} that may slow down the convergence in practice since the radius of the trust region is no grater than $O(\eps)$ leading to short steps.

\paragraph{Exploiting problem structure.}
In \eqref{eq:per_iter_decr_0} we can clearly observe the benefit of the use of $\nu$-SSB in our algorithm, whenever $\setK$ is a symmetric cone. Indeed, when $\alpha_k=\frac{1}{2\zeta(x^k,v^k)}$, the per-iteration decrease of the potential is $\frac{\norm{v^{k}}_{x^{k}}^{2} }{4\zeta(x^k,v^k)} \geq \frac{ \eps \norm{v^{k}}_{x^{k}}}{4\nu\zeta(x^k,v^k)} $ which may be large if $\zeta(x^k,v^k)=\sigma_{x^k}(-v^k) \ll \norm{v^{k}}_{x^{k}}$.


\paragraph{The role of the potential function.}
Next, we discuss more explicitly, how the algorithm and complexity bounds depend on the parameter $\mu$. The first observation is that from \eqref{eq:FO_KKT_proof_2}, to guarantee that $s^{k} \in \setK^{\ast}$, we need the stopping criterion to be $\norm{v^{k}}_{x^k} < \mu$, which by \eqref{eq:FO_KKT_proof_3} leads to the error $2 \mu \nu$ in the complementarity conditions. From the analysis following equation \eqref{eq:FO_per_iter_proof_6}, we have that  
\[
K\frac{\mu^{2}}{4  (\bar{M}+\mu)} = K \min\left\{\frac{\mu}{4},\frac{\mu^{2}}{4 \ (\bar{M}+\mu)}\right\}\leq f(x^{0})-f_{\min}(\setX)+\mu \nu.
\]
Whence, recalling that $\bar{M}=\max\{M,L_0\}$,
\[
K \leq 4(f(x^{0}) - f_{\min}(\setX)+ \mu \nu) \cdot \frac{\max\{M,L_0\}+\mu}{\mu^{2}}.%\max\left\{\frac{\nu}{\eps},\frac{\nu^{2}(M+\eps/\nu)}{\eps^{2}}\right\},
\]
Thus, we see that after $O(\mu^{-2})$ iterations the algorithm finds a $(2 \mu \nu)$-KKT point, and  if $\mu \to 0$, we have convergence to a KKT point, but the complexity bound tends to infinity and becomes non-informative. At the same time, as it is seen from \eqref{eq:finder}, when $\mu \to 0$, the algorithm itself converges to a preconditioned gradient method since $F_{\mu}(x)=f(x) + \mu h(x) \to  f(x)$. 
We also see from the above explicit expressions in terms of $\mu$ that the design of the algorithm requires careful balance between the desired accuracy of the approximate KKT point expressed mainly by the complementarity condition, stopping criterion, and complexity. Moreover, the step-size should be also taken carefully to ensure the feasibility of the iterates, and the standard for first-order methods step-size $1/M$ may not work. 





\subsection{Anytime convergence via restarting $\AHBA$}
\label{sec:path-following}
The analysis of Algorithm \ref{alg:AHBA} is based on the a-priori fixed tolerance $\eps>0$ and the parameter coupling $\mu=\eps/\nu$. This coupling allows us to embed Algorithm \ref{alg:AHBA} within a restarting scheme featuring a decreasing sequence $\{\mu_{i}\}_{i\geq 0}$, followed by restarts of $\AHBA$. This restarting strategy frees Algorithm \ref{alg:AHBA} from hard-coded parameters and connects it well to traditional barrier methods.% In this way, we obtain, to the best of our knowledge, the first path-following method with complexity guarantees for non-convex problems with linear and conic constraints. \MS{Not sure if this can not be interpreted as overselling.}

To describe this double-loop algorithm, we fix $\eps_{0}>0$ and select the starting point $x_0^{0}$ as a $\nu$-analytic centre of $\setX$ with respect to $h\in\scrH_{\nu}(\setK)$. We let $i \geq 0$ denote the counter for the restarting epochs at the start of which the value $\mu_{i}$ is decreased. In epoch $i$, we generate a sequence $\{x^{k}_{i}\}_{k=0}^{K_{i}}$ by calling $\AHBA(\mu_{i},\eps_{i},L_0^{(i)},x^{0}_{i})$ until the stopping condition is reached. This will take at most $\K_{I}(\eps_{i},x^{0}_{i})$ iterations, specified in eq. \eqref{eq:FO_main_Th_compl}. We store the last iterate $\hat{x}_{i}=x^{K_{i}}_{i}$ and the last estimate of the Lipschitz modulus $\hat{M}_{i}=L_{K_i}^{(i)}$ obtained from procedure $\AHBA(\mu_{i},\eps_{i},L_0^{(i)},x^{0}_{i})$ and then restart the algorithm using the ``warm starts'' $x^{0}_{i+1}=\hat{x}_{i}$, $L_{0}^{(i+1)}=\hat{M}_{i}/2$, $\eps_{i+1}=\eps_{i}/2$, $\mu_{i+1}=\eps_{i+1}/\nu$. If $\eps \in (0,\eps_0)$ is the target accuracy of the final solution, it suffices to perform $ \lceil \log_{2}(\eps_{0}/\eps)\rceil+1$ restarts since, by construction, $\eps_{i} = \eps_0 \cdot 2^{-i}$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h]
\caption{Restarting $\AHBA$}
\label{alg:RestartHBA}
\SetAlgoLined
\KwData{ $h \in\scrH_{\nu}(\setK)$, $\eps_{0}>0$, $x_0^{0}\in\setX$ satisfying \eqref{eq:analytic_center}, $L_0^{(0)}>0$.}
\KwResult{Point $\hat{x}_i$, dual variables $\hat{y}_i$, $\hat{s}_i = \nabla f(\hat{x}_i) -\bA^{\ast}\hat{y}_i$.}
%Set $I=\lceil\log_{2}(\eps_{0}/\eps)\rceil+1$\;
\For{$i=0,1,\ldots$} 
{ Set $\eps_{i}=2^{-i}\eps_{0}$, $\mu_{i}=\frac{\eps_i}{\nu}$\; 
 Obtain $(\hat{x}_{i},\hat{y}_{i},\hat{s}_{i},\hat{M}_{i})$ from $\AHBA(\mu_{i},\eps_i,L_0^{(i)},x^{0}_{i})$\;
 %
% $(x_{i}^{K_{i}},L_{i}^{K_{i}})$ from $\AHBA(\mu_{i},\eps_i,x^{0}_{i})$\; 
 Set $x_{i+1}^{0}=\hat{x}_{i}$ and $L_0^{(i+1)}=\hat{M}_{i}/2$.
	}
\end{algorithm}
%%%%%%%%%%%%%%%%%	
\begin{theorem}\label{th:ComplexityPathfollowing}
Let Assumptions \ref{ass:1}-\ref{ass:gradLip} hold. 
Then, for any $\eps \in (0,\eps_0)$, Algorithm \ref{alg:RestartHBA} finds a $2\eps$-KKT point for problem \eqref{eq:Opt} in the sense of Definition \ref{def:eps_KKT} after
no more than $I(\eps):=\lceil \log_{2}(\eps_{0}/\eps)\rceil+1$ restarts and at most 
$
\left\lceil \frac{64}{3\eps^2}(f(x^{0})-f_{\min}(\setX)+\eps_0)\nu^2(\max\{M,L_0^{(0)}\}+\eps_0/\nu)\right\rceil
$
 iterations of $\AHBA$.
\end{theorem}
\begin{proof}
Let us consider a restart $i \geq 0$ and repeat the proof of Theorem \ref{Th:AHBA_conv} with the change $\eps \to \eps_i$,  $\mu \to \mu_i = \eps_i/\nu$, $L_0 \to L_0^{(i)}=\hat{M}_{i-1}/2$, $\bar{M}=\max\{M,L_0\} \to \bar{M}_i=\max\{M,L_0^{(i)}\}$, $x^0 \to x^{0}_{i}=\hat{x}_{i-1}$.
Let $K_i$ be the last iteration of $\AHBA(\mu_{i},\eps_i,L_0^{(i)},x^{0}_{i})$ meaning that $\norm{v^{K_i}}_{x^{K_i}} < \frac{\eps_i}{\nu}$ and $\norm{v^{K_i-1}}_{x^{K_i-1}} \geq \frac{\eps_i}{\nu}$. From the analysis following equation \eqref{eq:FO_per_iter_proof_6}, we have that
\begin{align}
&K_i \frac{\eps_i^{2}}{4 \nu^{2}(\bar{M}_i+\eps_i/\nu)} \leq K_i \min_{k=0\ldots,K_i-1} \delta_{k}^i \leq  \sum_{k=0}^{K_i-1}\delta_{k}^i\leq F_{\mu_i}(x^{0}_i)-F_{\mu_i}(x^{K_i}_i). \label{eq:PF_proof_1} 
\end{align}
Further, using the fact that $\mu_i$ is a decreasing sequence and \eqref{eq:analytic_center}, it is easy to deduce
\begin{align}
F_{\mu_{i+1}}(x^{0}_{i+1})&=F_{\mu_{i+1}}(x^{K_i}_{i})\stackrel{\eqref{eq:potential}}{=}f(x^{K_i}_{i}) + \mu_{i+1} h(x^{K_i}_{i}) \stackrel{\eqref{eq:potential}}{=}F_{\mu_{i}}(x^{K_i}_{i}) + (\mu_{i+1} - \mu_{i}) h(x^{K_i}_{i}) \notag \\
&\stackrel{\eqref{eq:analytic_center}}{\leq} F_{\mu_{i}}(x^{K_i}_{i}) + (\mu_{i+1} - \mu_{i}) (h(x_0^0)-\nu)\notag \\
& \stackrel{\eqref{eq:PF_proof_1}}{\leq}F_{\mu_i}(x^{0}_i)-K_i \frac{\eps_i^{2}}{4 \nu^{2}(\bar{M}_i+\eps_i/\nu)} + (\mu_{i+1} - \mu_{i}) (h(x_0^0)-\nu).
\label{eq:PF_proof_2} 
\end{align}
Letting $I\equiv I(\eps):=\left\lceil \log_2 (\frac{\eps_0}{\eps}) \right\rceil+1$, by Theorem \ref{Th:AHBA_conv} applied to the restart $I-1$, we see that $\AHBA(\mu_{I-1},\eps_{I-1},L_0^{(I-1)},x^{0}_{I-1})$ outputs a $2\eps$-KKT point for problem \eqref{eq:Opt} in the sense of Definition \ref{def:eps_KKT}.
%Clearly, to achieve any desired accuracy $\eps$, i.e., find $2\eps$-KKT point for problem \eqref{eq:Opt}, it is sufficient to make $I=I_I(\eps)=\left\lceil \log_2 \frac{\eps_0}{\eps} \right\rceil+1$ restarts $i=0,...,I-1$. 
Summing inequalities \eqref{eq:PF_proof_2} for all the performed restarts $i=0,...,I-1$ and rearranging the terms, we obtain
\begin{align}
\sum_{i=0}^{I-1} K_i \frac{\eps_i^{2}}{4 \nu^{2}(\bar{M}_i+\eps_i/\nu)} & \leq F_{\mu_0}(x^{0}_0) - F_{\mu_{I}}(x^{0}_{I}) + (\mu_{I} - \mu_{0}) (h(x_0^0)-\nu) \notag \\
& \stackrel{\eqref{eq:potential}}{=} f(x^{0}_0) + \mu_0 h(x^{0}_0)- f(x^{0}_{I}) - \mu_{I}h(x^{0}_{I}) + (\mu_{I} - \mu_{0}) (h(x_0^0)-\nu) \notag \\
& \stackrel{\eqref{eq:analytic_center}}{\leq} f(x^{0}_0) - f_{\min}(\setX) + \mu_0 h(x^{0}_0) - \mu_{I}h(x^{0}_{0}) + \mu_{I} \nu + (\mu_{I} - \mu_{0}) (h(x_0^0)-\nu) \notag \\ 
& \leq f(x^{0}_0) - f_{\min}(\setX) + \mu_0 \nu = f(x^{0}_0) - f_{\min}(\setX) + \eps_0.
\label{eq:PF_proof_3} 
\end{align}
Moreover, based on our updating choice $L_0^{(i+1)}=\hat{M}_{i}/2$, it holds that 
\begin{align*}
\bar{M}_i &= \max\{M,L_0^{(i)}\} = \max\{M,\hat{M}_{i-1}/2\}\\
& = \max\{M,L_{K_{i-1}}^{(i-1)}/2\} \leq \max\{M,\bar{M}_{i-1}\} \leq ... \leq \max\{M, \bar{M}_{0}\} \leq  \max\{M,L_0^{(0)}\}.
\end{align*}
Hence, 
%$\bar{M}_i = \max\{M,L_0^{(i)}\} = \max\{M,\hat{M}_{i-1}/2\} = \max\{M,L_{K_i-1}^{i-1}/2\} \leq \max\{M,\bar{M}_{i-1}\} \leq ... \leq \max\{M, \bar{M}_{0}\}=  \max\{M,L_0\}$
%$\bar{M}_i = \max\{M,\hat{M}_i\}\leq \max\{M,L_0\}=\bar{M}$, we obtain for each $i=0,...,I-1$ that
\begin{align}
K_i \leq  4(f(x^{0}) - f_{\min}(\setX)+  \eps_0) \cdot \frac{\nu^{2}(\bar{M}_i+\eps_i/\nu)}{\eps_i^{2}} \leq \frac{C}{\eps_i^2},
\label{eq:PF_proof_4} 
\end{align}
where $C\equiv 4(f(x^{0})-f_{\min}(\setX)+\eps_0)\nu^2(\max\{M,L_0^{(0)}\}+\eps_0/\nu)$. Finally, we obtain that the total number of iterations of procedures $\AHBA(\mu_{i},\eps_{i},L_0^{(i)},x_{i}^{0}),0\leq i \leq I-1$, to reach accuracy $\eps$ is at most
\begin{align*}
\sum_{i=0}^{I-1}K_{i}&\leq \sum_{i=0}^{I-1} \frac{C}{\eps_i^2} \leq \frac{C}{\eps_0^2} \sum_{i=0}^{I-1} (2^i)^2 
\leq \frac{C}{3\eps_0^2} \cdot (4^{2+\log_2(\frac{\eps_0}{\eps})}) = \frac{16C}{3\eps^2}\\
&=\frac{64(f(x^{0})-f_{\min}(\setX)+\eps_0)\nu^2(\max\{M,L_0^{(0)}\}+\eps_0/\nu)}{3\eps^{2}}.
\end{align*}
\end{proof}