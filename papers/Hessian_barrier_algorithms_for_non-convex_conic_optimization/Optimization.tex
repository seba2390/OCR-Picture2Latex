%----------------------------------------------------------------------
%%% Optimization
%----------------------------------------------------------------------
% !TEX root = ./HBAConicMain.tex
%
%In this section we define our notion of approximate first- and second-order KKT points. The following assumptions are in place throughout the paper. 
%\begin{assumption}\label{ass:barrier}
%$\bar{\setK}$ is a regular cone admitting an efficient barrier setup $h\in\scrH_{\nu}(\setK)$. By this we mean that at a given query point $x\in\setK$, we can construct an oracle that returns to us information about the values $h(x),\nabla h(x)$ and $H(x)=\nabla^{2}h(x)$, with low computational efforts. 
%\end{assumption}
%%%%
%\begin{assumption}\label{ass:full_rank}
%The linear operator $\bA$ has full rank: $\image(\bA)=\R^{m}$. 
%\end{assumption}
%%%%%
%Note that this assumption is not restrictive. If the linear operator maps a point $x$ to a lower-dimensional subset, then it is possible to eliminate redundant constraints, or we are working with an inconsistent system $\setL=\emptyset$. The latter is excluded from our considerations, so in fact Assumption \ref{ass:full_rank} is without loss of generality. 
%%%%%%%%%%%%%%%
%\begin{assumption}\label{ass:solution_exist}
%Problem \eqref{eq:Opt} admits a solution. We let $f_{\min}(\setX)\eqdef \argmin\{f(x)\vert x\in\bar{\setX}\}$.
%\end{assumption}
%%%%%%%%%%%%%%%%%%
%\begin{assumption}\label{ass:C1}
%The objective function $f:\setE\to\R$ is continuously differentiable on an open set containing $\setX$.
%\end{assumption}
%%%%%%%%%%%%%%%%%%%
%\begin{assumption}\label{ass:C2}
%The objective function $f:\setE\to\R$ is twice continuously differentiable on an open set containing $\setX$.
%\end{assumption}
%%%%%%%%%%%%%%%%%%%%%
The next definition specifies our notion of an approximate first-order KKT point for problem \eqref{eq:Opt}.
%%%%%%%%%%%
\begin{definition}\label{def:eps_KKT}
%Let Assumptions \ref{ass:1} and \ref{ass:full_rank} hold. 
Given $\eps \geq 0$, we call a triple $(\bar{x},\bar{y},\bar{s})\in\setE\times\R^{m}\times\setE^{\ast}$ an $\eps$-KKT point for problem \eqref{eq:Opt} if
\begin{align}
& \bA\bar{x}=b, \bar{x}\in\setK, \bar{s} \in\setK^{\ast}, \label{eq:eps_optim_equality_cones} \\
&\|\nabla f(\bar{x}) - \bA^{\ast}\bar{y}-\bar{s} \|\leq \eps,  \label{eq:eps_optim_grad} \\
& \inner{\bar{s},\bar{x}} \leq \eps. \label{eq:eps_optim_complem} 
\end{align}
\end{definition}
%%%%%%%%%%%%%%
To justify this definition, let $x^{\ast}$ be a local solution of problem \eqref{eq:Opt}. Then, for $\delta>0$ sufficiently small, the point $x^{\ast}$ is the unique global solution to the perturbed optimization problem with ball restriction $\overline{\ball(x^{\ast};\delta)}\eqdef\{x\in\setE\vert\; \norm{x-x^{\ast}}\leq\delta\}$:
\begin{equation}
\label{eq:limit_optimality_proof_0}
	\min_{x\in\setX\cap\overline{\ball(x^{\ast};\delta)}} f(x) + \frac{1}{4}\norm{x-x^{*}}^4.
\end{equation}
Next, using the barrier $h\in\scrH_{\nu}(\setK)$, we absorb the constraint $x\in\setK$ in the penalty $\mu_k h(x)$, where $\mu_k> 0$, $\mu_{k}\downarrow 0$ is a given sequence. This leads to the barrier formulation
\begin{equation}\label{eq:limit_optimality_proof_1}
\min_{x\in\setX\cap\overline{\ball(x^{\ast};\delta)}}\varphi_{k}(x)\eqdef F_{\mu_{k}}(x)+\frac{1}{4}\norm{x-x^{\ast}}^{4},\quad F_{\mu_{k}}(x)= f(x)+\mu_{k}h(x).
\end{equation}
From the classical theory of interior penalty methods \cite{FiacMcCo68}, it is known that a global solution $x^k$ exists for this problem for all $k$ and that cluster points of $x^k$ are global solutions of \eqref{eq:limit_optimality_proof_0}. Clearly, $x^{k}\in \setX\cap\overline{\ball(x^{\ast},\delta)}$ for all $k$ and $x^{k}\to x^{\ast}$. Setting $s^{k}= -\mu_{k}\nabla h(x^{k})$, which belongs to $\setK^{\ast}$ by eq. \eqref{eq:relations_1}, and exploiting the properties of the barrier function $h\in\scrH_{\nu}(\setK)$, we see that 
$
\inner{s^{k},x^{k}}=-\mu_{k}\inner{\nabla h(x^{k}),x^{k}} \stackrel{\eqref{eq:log_hom_scb_hess_prop}}{=}\mu_{k}\nu.
%=-\mu_{k}\inner{[H(x^{k})]^{-1/2}\nabla h(x^{k}),[H(x^{k})]^{1/2}x^{k}}\\
%&\overset{\eqref{eq:log_hom_scb_hess_prop}}{=}\mu_{k}\inner{[H(x^{k})]^{1/2}x^{k},[H(x^{k})]^{1/2}x^{k}}\\
%&=\mu_{k}\norm{x^{k}}^{2}_{x^{k}}=\mu_{k}\nu.
$
Consequently, $\lim_{k\to\infty}\inner{s^{k},x^{k}}=0$. Since $x^{k}\to x^{\ast}$, the restriction $x^{k}\in\overline{\ball(x^{\ast};\delta)}$ will automatically hold for $k$ sufficiently large. By the full-rank assumption, the first-order optimality conditions of problem \eqref{eq:limit_optimality_proof_1} reads as 
\[
\nabla f(x^{k})-\bA^{\ast}y^{k}-s^{k}-\norm{x^{k}-x^{\ast}}^{2}\cdot(x^{k}-x^{\ast})=0,
\]
for all $k$ large enough. Hence, setting $\delta\leq\eps^{1/3}$, $\mu_k \leq \eps/\nu$, and $\bar{x}=x^{k},\bar{s}=s^{k},\bar{y}=y^{k}$, we obtain a triple satisfying conditions \eqref{eq:eps_optim_equality_cones}-\eqref{eq:eps_optim_complem}.
%%%%%%%%%%%%%%%%%%%%%
%\begin{remark}
%For $\eps=0$ the conditions of Definition \ref{def:eps_KKT} reduce to the exact first-order optimality conditions of \cite{FayLu06}.
%\end{remark}
%%%%%%%%%%%%%%%%%%%%%%%%%
\\

Assuming twice continuous differentiability of $f$ on $\setX$, our notion of an approximate second-order KKT point for problem \eqref{eq:Opt} is defined as follows.
\begin{definition}\label{def:eps_SOKKT}
%Let Assumptions \ref{ass:1}, and \ref{ass:full_rank} hold. 
Given $\eps_1,\eps_2 \geq 0$, we call a triple $(\bar{x},\bar{y},\bar{s})\in\setE\times\R^{m}\times\setE^{\ast}$ an $(\eps_1,\eps_2)$-2KKT point for problem \eqref{eq:Opt} if
\begin{align}
& \bA\bar{x}=b, \bar{x}\in\setK, \bar{s} \in\setK^{\ast}, \label{eq:eps_SO_optim_equality_cones} \\
&\|\nabla f(\bar{x}) - \bA^{\ast}\bar{y}-\bar{s} \|\leq \eps_1,  \label{eq:eps_SO_optim_grad} \\
&\inner{\bar{s},\bar{x}} \leq \eps_1, \label{eq:eps_SO_optim_complem} \\
& \nabla^2f(\bar{x}) + \sqrt{\eps_2} H(\bar{x}) \succeq 0 \;\; \text{on} \;\; \setL_0.   \label{eq:eps_SO_optim_SO}
\end{align}
\end{definition}
The first three conditions are the same as for the $\eps$-KKT point. 
The last one can be justified as follows. Using the full-rank condition, the second-order optimality condition for problem \eqref{eq:limit_optimality_proof_1} says that $x^{k}$ satisfies 
\[
\inner{(\nabla^{2}f(x^{k})+\mu_{k}H(x^{k}))d,d} \geq -2\inner{x^{k}-x^{\ast},d}^{2} - \norm{x^{k}-x^{\ast}}^2\norm{d}^2\geq -3\delta^{2}\norm{d}_{2}^{2}\qquad\forall d\in\setL_{0}.
\]
%says that $x^{k}$ is an isolated local solution to problem \eqref{eq:limit_optimality_proof_1} for $k$ sufficiently large if $\nabla^{2}\varphi_{k}(x^{k})\succ 0$ on $\setL_{0}\eqdef\setL-\setL$. This gives directly
%\[
%\inner{(\nabla^{2}f(x^{k})+\mu_{k}H(x^{k}))d,d}> -2\inner{x^{k}-x^{\ast},d}^{2}> -\delta^{2}\norm{d}_{2}^{2}\qquad\forall d\in\setL_{0}.
%\]
Setting $\mu_k \leq \sqrt{\eps_2}$ and $\delta \leq (\eps_2/9)^{1/4}$, we see that $x^{k}$ satisfies  $\inner{(\nabla^{2}f(x^{k})+\sqrt{\eps_2}(H(x^{k})+\bI))d,d}\geq 0,\forall d\in\setL_{0}$, which is clearly implied by \eqref{eq:eps_SO_optim_SO}. 
%Letting $\delta\to 0$, then for a given $\eps_{2}>0$ we see that condition \eqref{eq:eps_SO_optim_SO} applies for all $k$ sufficiently large. 
%\begin{remark}
%\label{rem:foopt}
%We note that both proposed definitions are stronger than the ones in \cite{HaeLiuYe18}, who use the weaker infinity norm in conditions \eqref{eq:eps_optim_grad} and \eqref{eq:eps_SO_optim_grad}, respectively. Moreover, they also use the weaker infinity norm in conditions \eqref{eq:eps_optim_complem} and \eqref{eq:eps_SO_optim_complem}. 
%Since in our case $\bar{x}\in\setK, \bar{s} \in\setK^{\ast}$, in our case we have for each $i=1,...,\dim(\setE)$, $ 0 \leq \bar{s}_i\bar{x}_i \leq \inner{\bar{s},\bar{x}} \leq \eps$. We discuss this in more details in Section \ref{sec:FO_discussion}, where we compare the complexity bounds for our first-order method and the first-order method in \cite{HaeLiuYe18}, as well as in Section \ref{sec:SO_discussion}, where we compare the complexity bounds for our second-order method and the second-order method in \cite{HaeLiuYe18}. \MS{We talk about this in laters sections anyhow so I think we do not need to put it here.}
%\end{remark}
%%%%%
\begin{remark}\label{rem:soopt}
To compare our second-order condition with the ones previously formulated in the literature, we consider the particular case $\bar{\setK}=\bar{\setK}_{NN}$ as in \cite{HaeLiuYe18,NeiWr20} with the log-barrier setup giving $H(x)=\diag[x_{1}^{-2},\ldots,x_{n}^{-2}]\eqdef\XX^{-2}$. Within this setup, our second-order condition \eqref{eq:eps_SO_optim_SO} becomes, after multiplication by  $[H(x)]^{-1/2}=\XX$ from left and right,
\[
\XX\nabla^{2}f(x)\XX+\sqrt{\eps_{2}}\bI\succeq 0\qquad \text{on the set }\{d\in\setE\vert \bA\XX d=0\}.%\ker(\bA\XX).
\] 
This is equivalent to Proposition 2(c) in \cite{HaeLiuYe18}, modulo our use of $\sqrt{\eps_{2}}$ instead of $\eps$ in \cite{HaeLiuYe18}, as well as equation (1.6d) in \cite{NeiWr20}, modulo our use of $\sqrt{\eps_{2}}$ instead of $\eps_H$ in \cite{NeiWr20}.
%On the other hand, our condition \eqref{eq:eps_SO_optim_SO} does not rely on the coupling $[H(\bar{x})]^{-1/2}=\XX$ which may not hold for general cones and is formulated in terms %of the Hessian $H(x)$ without the use of $[H(\bar{x})]^{-1/2}$ which may be tricky to define.
%
%
%This representation of the second-order optimality condition makes our set of KKT conditions easier to compare with the ones previously formulated in the literature, e.g. in \cite{HaeLiuYe18}. They consider the case $\bar{\setK}=\bar{\setK}_{NN}$ with the log-barrier setup. This gives $H(x)=\diag[x_{1}^{-2},\ldots,x_{n}^{-2}]\eqdef\XX^{-2}$. Within this setup, the second-order KKT condition becomes 
%\[
%\XX\nabla^{2}f(x)\XX+\sqrt{\eps_{2}}\bI\succeq 0\qquad \text{on }\ker(\bA\XX).
%\] 
%This corresponds to Proposition 2(c) in \cite{HaeLiuYe18}, modulo our use of $\sqrt{\eps_{2}}$ instead of $\eps$ in \cite{HaeLiuYe18}, as well as equarion (1.6d) in \cite{NeiWr20}, modulo our use of $\sqrt{\eps_{2}}$ instead of $\eps_H$ in \cite{NeiWr20}.
%
%Condition \eqref{eq:eps_SO_optim_SO} is equivalent to 
%\[
%[H(\bar{x})]^{-1/2}\nabla^2f(\bar{x})[H(\bar{x})]^{-1/2} + \sqrt{\eps_2}\bI \succeq 0 \quad \text{on }\ker(\bA[H(\bar{x})]^{-1/2}).
%\]
\close
\end{remark}
\begin{remark}
If $\bar{\setK}$ is a symmetric cone, the complementarity conditions \eqref{eq:eps_optim_complem} and \eqref{eq:eps_SO_optim_complem} are equivalent to complementarity notions formulated in terms of the multiplication $\circ$ under which $\setK$ becomes an Euclidean Jordan algebra. \citep[][Prop. 2.1]{LouFukMas18} shows that $x\circ y=0$ if and only $\inner{x,y}=0$, where $\inner{\cdot,\cdot}$ is the inner product of the ambient space $\setE$. Moreover, if $\bar{\setK}$ is a primitive symmetric cone, then by \citep[][Prop. III.4.1]{FarKor94}, there exists a constant $a>0$ such that $a\tr(x\circ y)=\inner{x,y}$ for all $x,y\in\setK$. In view of this relation, our complementarity notions could be specialized to the condition $\bar{s}\circ \bar{x}\leq \eps$. Hence, our approximate KKT conditions reduce to the ones reported in \cite{AndFukHaeSanSec21}. In particular, for $\bar{\setK}=\bar{\setK}_{\text{NN}}$ we recover the standard complementary slackness condition $s^{k}_{i}x^{k}_{i}\to 0$ as $k\to\infty$ for all $i$, as in this case the Jordan product $\circ$ gives rise to the Hadamard product. See \cite{Andreani:2019uf} for more details.
\close
\end{remark}

\subsection{On the relation to scaled critical points}
In absence of differentiability at the boundary, a popular formulation of necessary optimality conditions involves the definition of scaled-critical points. Indeed, at a local minimizer $x^{\ast}$, the scaled first-order optimality condition $x_{i}^{\ast}[\nabla f(x^{\ast})]_{i}=0,1\leq i\leq n$ holds, where the product is taken to be $0$ when the derivative does not exist. Based on this characterization, one may call a point $x\in\setK_{\text{NN}}$ with $\abs{x_{i}[\nabla f(x)]_{i}}\leq\eps$ for all $i=1,\ldots,n$ and $\eps$-scaled first-order point. Algorithms designed to produce $\eps$-scaled first-order points, with some small $\eps>0$, have been introduced in \cite{BiaCheYe15} and \cite{BiaChe15}. As reported in \cite{HaeLiuYe18}, there are several problems associated with this weak definition of a critical point. First, when derivatives are available on $\bar{\setK}_{\text{NN}}$, the standard definition of a critical point would entail the inclusion $\inner{\nabla f(x),x'-x}\geq 0$ for all $x'\in \bar{\setK}_{\text{NN}}.$ Hence, $[\nabla f(x)]_{i}=0$ for $x_{i}>0$ and $[\nabla f(x)]_{i}\geq 0$ for $x_{i}=0$. It follows, $\nabla f(x)\in\bar{\setK}_{\text{NN}}$, a condition that is absent in the definition of a scaled critical point. Second, scaled critical points come with no measure of strength, as they holds trivially when $x=0$, regardless of the objective function. Third, there is a general gap between local minimizers and limits of $\eps$-scaled first-order points, when $\eps\to 0^{+}$ (see \cite{HaeLiuYe18}). Similar remarks apply to the scaled second-order condition, considered in \cite{BiaChe15}. Our definition of approximate KKT points overcome these issues. In fact, our definitions of approximate first- and second-order KKT points is continuous in $\eps$, and therefore in the limit our approximate KKT points coincide with the classical first- and second-order KKT conditions for a local minimizer. This is achieved without assuming global differentiability of the objective function or performing an additional smoothing of the problem data as in \cite{Bian:2013vd,BiaChe15}. 




