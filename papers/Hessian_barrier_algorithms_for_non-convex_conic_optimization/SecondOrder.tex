%----------------------------------------------------------------------
%%% 2nd-order
%----------------------------------------------------------------------
% !TEX root = ./HBAConicMain.tex
%

In this section we introduce a second-order potential reduction method for problem \eqref{eq:Opt} under the assumption that the second-order Taylor expansion of $f$ on the set of feasible directions $\scrT_{x}$ defined in \eqref{eq:Dikinv} is sufficiently accurate in the geometry induced by $h \in\scrH_{\nu}(\setK)$. 
%%%%%%%%%%%%
\begin{assumption}[Local second-order smoothness]
\label{ass:2ndorder}
$f:\setE\to\R\cup\{+\infty\}$ is twice continuously differentiable on $\feas$ and there exists a constant $M>0$ such that, for all $x\in\feas$ and $v\in\scrT_{x}$, we have
\begin{equation}\label{eq:SO_Lipschitz_Gradient}
\norm{\nabla f(x+v)-\nabla f(x)-\nabla^{2}f(x)v}^{\ast}_{x}\leq\frac{M}{2}\norm{v}^{2}_{x}.
\end{equation}
\end{assumption}
A sufficient condition for \eqref{eq:SO_Lipschitz_Gradient} is the following local counterpart of the global Lipschitz condition on
the Hessian of $f$:
\begin{equation}\label{eq:LipHess}
(\forall x\in\setX)(\forall u,v\in\scrF_{x}):\;\norm{\nabla^{2}f(x+u)-\nabla^{2}f(x+v)}_{\text{op},x}\leq M\norm{u-v}_{x},
\end{equation}
where 
$\norm{\BB}_{\text{op},x}\eqdef\sup_{u:\norm{u}_{x}\leq 1}\left\{\frac{\norm{\BB u}_{x}^{\ast}}{\norm{u}_{x}}\right\}$ is the induced operator norm for a linear operator $\BB:\setE\to\setE^{\ast}$. Indeed, this condition implies \eqref{eq:SO_Lipschitz_Gradient}:
\begin{align*}
&\norm{\nabla f(x+v)-\nabla f(x)-\nabla^{2}f(x)v}^{\ast}_{x}=\norm{\int_{0}^{1}(\nabla^{2}f(x+tv)-\nabla^{2}f(x))v\dif t }^{\ast}_{x}\\
&\leq \int_{0}^{1}\norm{\nabla^{2}f(x+tv)-\nabla^{2}f(x)}_{\text{op},x}\cdot \norm{v}_{x}\dif t   \leq \frac{M}{2}\norm{v}^{2}_{x}.
\end{align*}
%\begin{align*}
%&\norm{\nabla f(x+v)-\nabla f(x)-\nabla^{2}f(x)v}^{\ast}_{x}=\norm{\int_{0}^{1}(\nabla^{2}f(x+tv)-\nabla^{2}f(x))v\dif t }^{\ast}_{x}\\
%&\quad \leq\int_{0}^{1}\norm{(\nabla^{2}f(x+tv)-\nabla^{2}f(x))v}_{x}^{\ast}\dif t \leq \int_{0}^{1}\norm{\nabla^{2}f(x+tv)-\nabla^{2}f(x)}_{\text{op},x}\cdot \norm{v}_{x}\dif t  \\
%&\quad \quad \leq \frac{M}{2}\norm{v}^{2}_{x}.
%\end{align*}
Further, \eqref{eq:SO_Lipschitz_Gradient} in turn implies another important estimate 
\begin{equation}\label{eq:cubicestimate}
f(x+v)-\left[f(x)+\inner{\nabla f(x),v}+\frac{1}{2}\inner{\nabla^{2}f(x)v,v}\right]\leq\frac{M}{6}\norm{v}^{3}_{x}.
\end{equation}
Indeed, for all $x\in\setX$ and $v\in\scrT_{x}$,
\begin{align*}
&\abs{f(x+v)-f(x)-\inner{\nabla f(x),v}-\frac{1}{2}\inner{\nabla^{2}f(x)v,v}}=\abs{\int_{0}^{1}\inner{\nabla f(x+tv)-\nabla f(x)-\frac{1}{2}\nabla^{2}f(x)v,v}\dif t}\\
&\quad\leq \int_{0}^{1}\norm{\nabla f(x+tv)-\nabla f(x)-\frac{1}{2}\nabla^{2}f(x)v}_{x}^{*}\dif t\cdot\norm{v}_{x} 
\leq \frac{M}{6}\norm{v}^{3}_{x}.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}
\label{Rm:Hess_Lip}
Assumption \ref{ass:2ndorder} subsumes, when $\bar{\setX}$ is bounded, the standard Lipschitz-Hessian setting since 
if the Hessian of $f$ is Lipschitz with modulus $M$ with respect to the Euclidean norm, we have by \cite[Eq. (2.2)]{NesPol06}.
\[
\norm{\nabla f(x+v)-\nabla f(x)-\nabla^{2}f(x)v}\leq\frac{M}{2}\norm{v}^{2}.
\]
Since $\bar{\setX}$ is bounded, one can observe that $\lambda_{\max}([H(x)]^{-1})^{-1}=\lambda_{\min}(H(x)) \geq \sigma$ for some $\sigma >0$, and \eqref{eq:SO_Lipschitz_Gradient} holds. Indeed, denoting $g=\nabla f(x+v)-\nabla f(x)-\nabla^{2}f(x)v$, we obtain
\[
(\norm{g}_x^*)^2 \leq \lambda_{\max}([H(x)]^{-1})\norm{g}^{2} \leq \frac{M^2}{4\lambda_{\min}(H(x))}\norm{v}^{4} \leq \frac{M^2}{4\sigma^{3}}\norm{v}_x^4.
\]
\close
\end{remark}

\begin{remark}
The cubic overestimation of the objective function in \eqref{eq:cubicestimate} does not rely on global second order differentiability assumptions. To illustrate this we invoke again the structured composite optimization problem \eqref{eq:composite}, assuming that the data fidelity function $\ell$ is twice continuously differentiable on an open neighborhood containing $\setX$, with Lipschitz continuous Hessian $\nabla^{2}\ell$ with modulus $\gamma$ w.r.t. the Euclidean norm. On the domain $\setK_{\text{NN}}$ we employ the canonical barrier $h(x)=-\sum_{i=1}^{n}\ln(x_{i})$, with $H(x)=\diag\{x_{1}^{-2},\ldots,x_{n}^{-2}\}=\XX^{-2}$. This means, for all $x,x^{+}\in\setX$, we have 
\[
\ell(x^{+})\leq\ell(x)+\inner{\nabla\ell(x),x^{+}-x}+\frac{1}{2}\inner{\nabla^{2}\ell(x)(x^{+}-x),x^{+}-x}+\frac{\gamma}{6}\norm{x^{+}-x}^{3}.
\]
As penalty function, we again consider the $L_{p}$ regularizer with $p\in(0,1)$. For any $t,s>0$, one has 
\[
t^{p}\leq s^{p}+ps^{p-1}(t-s)+\frac{p(p-1)}{2} s^{p-2}(t-s)^{2}+\frac{p(p-1)(p-2)}{6}s^{p-3}(t-s)^{3}.
\]
Since $v\in\scrT_{x}$ if and only if $v=[H(x)]^{-1/2}d=\XX d$ for some $d\in\R^{\dim(\setE)}$ satisfying $\bA\XX d=0$ and $\norm{d}<1$. Since $p(1-p)\leq 1/4$, it follows $p(1-p)(2-p)\leq 1/2$. Thus, using $x^{+}=x+v=x+\XX d$, we get 
\begin{align*}
f(x^{+}) &- \left((f(x)+\inner{\nabla f(x),\XX d}+\frac{1}{2}\inner{\nabla^{2}f(x)\XX d,\XX d}\right) \leq \frac{\gamma}{6}\norm{\XX d}^{3}+\frac{1}{12}\sum_{i=1}^{n}x^{p}_{i}d_{i}^{3}\\
&\leq \frac{\gamma}{6}\norm{\XX d}^{3}+\frac{1}{12}\norm{x}^{p}_{\infty}\sum_{i=1}^{n}d_{i}^{3} \leq \frac{1}{6}\left(\gamma\norm{x}_{\infty}^{3}+\frac{1}{2}\norm{x}^{p}_{\infty}\right)\norm{d}^{3}.
\end{align*}
%\begin{align*}
%f(x^{+})&\leq f(x)+\inner{\nabla f(x),\XX d}+\frac{1}{2}\inner{\nabla^{2}f(x)\XX d,\XX d}+\frac{\gamma}{6}\norm{\XX d}^{3}+\frac{1}{12}\sum_{i=1}^{n}x^{p}_{i}d_{i}^{3}\\
%&\leq f(x)+\inner{\nabla f(x),\XX d}+\frac{1}{2}\inner{\nabla^{2}f(x)\XX d,\XX d}+\frac{\gamma}{6}\norm{\XX d}^{3}+\frac{1}{12}\norm{x}^{p}_{\infty}\sum_{i=1}^{n}d_{i}^{3}\\
%&\leq f(x)+\inner{\nabla f(x),\XX d}+\frac{1}{2}\inner{\nabla^{2}f(x)\XX d,\XX d}+\frac{1}{6}\left(\gamma\norm{x}_{\infty}^{3}+\frac{1}{2}\norm{x}^{p}_{\infty}\right)\norm{d}^{3}
%\end{align*}
Assuming that $\bar{\setX}$ is bounded, there exists a universal constant $M>0$ such that $\gamma\norm{x}_{\infty}^{2}+\frac{1}{2}\norm{x}^{p}_{\infty}\leq M$. Combining this with Remark \ref{Rm:Hess_Lip}, we obtain a cubic overestimation as in eq. \eqref{eq:cubicestimate}. Importantly, $f(x)$ is not differentiable for $x \in \{x_i=0,\text{ for some } i\}$.  
\close
\end{remark}

We emphasize that in Assumption \ref{ass:2ndorder} the constant $M$ is in general unknown or may be a conservative upper bound. Therefore, adaptive techniques should be used to estimate it and are likely to improve the practical performance of the method. Assumption \ref{ass:2ndorder} also implies, by \eqref{eq:cubicestimate} and \eqref{eq:Dbound} (with $d=v$ and $t=1< \frac{1}{\norm{v}_x} \stackrel{\eqref{eq:boundzeta}}{\leq} \frac{1}{\zeta(x,v)}$), the following upper bound for the potential function $F_{\mu}$. 
\begin{lemma}[Cubic Overestimation]
\label{lem:cubic}
For all $x\in\setX,v\in\scrT_{x}$ and $L\geq M$, we have 
\begin{equation}\label{eq:cubicdecrease}
F_{\mu}(x+v)\leq F_{\mu}(x)+\inner{\nabla F_{\mu}(x),v}+\frac{1}{2}\inner{\nabla^{2}f(x)v,v}+\frac{L}{6}\norm{v}^{3}_{x}+\mu\norm{v}^{2}_{x}\omega(\zeta(x,v)).
\end{equation}
\end{lemma}

\subsection{Algorithm description and its complexity theorem}
\label{S:SO_alg_descr}
Let $x\in\setX$ be given. In order to find a search direction, we choose a parameter $L>0$, construct a cubic-regularized model of the potential $F_{\mu}$ \eqref{eq:potential}, and minimize it on the linear subspace $\setL_{0}$:
\begin{equation}\label{eq:cubicproblem}
v_{\mu,L}(x)\in\Argmin_{v\in\setE:\bA v=0}\left\{ Q^{(2)}_{\mu,L}(x,v)\eqdef F_{\mu}(x)+\inner{\nabla F_{\mu}(x),v}+\frac{1}{2}\inner{\nabla^{2}f(x)v,v}+\frac{L}{6}\norm{v}_{x}^{3} \right\},
\end{equation}
where by $\Argmin$ we denote the set of global minimizers. The model consists of three parts: linear approximation of $h$, quadratic approximation of $f$, and a cubic regularizer with penalty parameter $L>0$. Since this model and our algorithm use the second derivative of $f$, we call it a second-order method.
Our further derivations rely on the first-order optimality conditions for the problem \eqref{eq:cubicproblem}, which say that there exists $y_{\mu,L}(x)\in\R^{m}$ such that $v_{\mu,L}(x)$ satisfies
\begin{align}
\nabla F_{\mu}(x)+\nabla^{2}f(x)v_{\mu,L}(x)+\frac{L}{2}\norm{v_{\mu,L}(x)}_{x}H(x)v_{\mu,L}(x) - \bA^{\ast} y_{\mu,L}(x) &= 0,\label{eq:opt1}\\
 - \bA v_{\mu,L}(x)&=0. \label{eq:opt2}
\end{align}
We also use the following extension of \cite[Prop. 1]{NesPol06} to our setting with the local norm induced by $H(x)$.
\begin{proposition}
For all $x\in\feas$ it holds 
\begin{equation}\label{eq:PD}
\nabla^{2}f(x)+\frac{L}{2}\norm{v_{\mu,L}(x)}_{x}H(x)\succeq 0\qquad\text{ on }\;\;\setL_{0}.
\end{equation}
\end{proposition}
\begin{proof}
The proof follows the same strategy as Lemma 3.2 in \cite{CarGouToi11a}. Let $\{z_{1},\ldots,z_{p}\}$ be an orthonormal basis of $\setL_{0}$ and the linear operator $\bZ:\R^{p}\to\setL_{0}$ be defined by $\bZ w=\sum_{i=1}^{p}z_{i}w^{i}$ for all $w=[w^{1};\ldots;w^{p}]^{\top}\in\R^{p}$. With the help of this linear map, we can absorb the null-space restriction, and formulate the search-direction finding problem \eqref{eq:cubicproblem} using the projected data
\begin{equation}\label{eq:dataKernel}
\gbold\eqdef\bZ^{\ast}\nabla F_{\mu}(x),\; \bJ\eqdef\bZ^{\ast}\nabla^{2}f(x)\bZ,\;\bH\eqdef\bZ^{\ast}H(x)\bZ \succ 0.
\end{equation}
We then arrive at the cubic-regularized subproblem to find $u_{L}\in\R^{p}$ s.t.
\begin{equation}\label{eq:cubicauxiliary}
u_{L}\in  \Argmin_{u\in\R^{p}}\{\inner{\gbold,u}+\frac{1}{2}\inner{\bJ u,u}+\frac{L}{6}\norm{u}^{3}_{\bH}\},
\end{equation}
where  $\norm{\cdot}_{\bH}$ is the norm induced by the operator $\bH$. From \cite[Thm. 10]{NesPol06} we deduce  
\[
\bJ+\frac{L\norm{u_{L}}_{\bH}}{2}\bH\succeq 0.
\]
%where $u_{L}\in\R^{p}$ represents one global solution of problem \eqref{eq:cubicauxiliary}. 
Denoting $v_{\mu,L}(x) = \bZ u_{L}$, we see
\begin{align*}
\norm{u_{L}}_{\bH}=\inner{\bZ^{\ast}H(x)\bZ u_{L},u_{L}}^{1/2}=\inner{H(x)(\bZ u_{L}),\bZ u_{L}}^{1/2}&=\norm{v_{\mu,L}(x)}_{x}, \text{  and} \\
\bZ^{\ast}\left(\nabla^{2}f(x)+\frac{L}{2}\norm{v_{\mu,L}(x)}_{x}H(x)\right)\bZ&\succeq 0,
\end{align*}
which implies $\nabla^{2}f(x)+\frac{L}{2}\norm{v_{\mu,L}(x)}_{x}H(x)\succ 0$ over the null space $\setL_{0} = \{ v\in\setE:\bA v=0\}$. 
\end{proof}
\noindent
The above proposition gives some ideas on how one could numerically solve problem \eqref{eq:cubicproblem} in practice. In a preprocessing step, we once calculate matrix $\bZ$ and use it during the whole algorithm execution. At each iteration we calculate the new data using \eqref{eq:dataKernel}, leaving us with a standard \textit{unconstrained} cubic subproblem  \eqref{eq:cubicauxiliary}. \cite{NesPol06} show how such problems can be transformed to a \emph{convex} problem to which fast convex programming methods could in principle be applied. However, we can also solve it via recent efficient methods based on Lanczos' method \cite{CarGouToi11a,Jia22}. Whatever numerical tool is employed, we can recover our search direction by $v_{\mu,L}(x)$ by the matrix vector product $\bZ u_{L}$ in which $u_{L}$ denotes the solution obtained from this subroutine.
%%%%%%%%%%%%%%%%%%%%%
%\begin{remark}
%Our framework is formulated under the unrealistic assumption that the search direction can be computed exactly. An important extension of the current analysis will be to consider inexact computations. We leave this important question for future research.
%\close
%\end{remark}
%%%%%%%%%%%%%%

%step-size
Our next goal is to construct an admissible step-size policy,  given the search direction $v_{\mu,L}(x)$. 
Let $x\in\setX$ be the current position of the algorithm. Define the parameterized family of arcs $x^{+}(t)\eqdef x+t v_{\mu,L}(x)$, where $t\geq 0$ is a step-size. 
By \eqref{eq:step_length_zeta} and since $v_{\mu,L}(x)\in\setL_{0}$ by \eqref{eq:opt2}, we know that $x^{+}(t)$ is in $\setX$ provided that $t\in I_{x,\mu,L}\eqdef[0,\frac{1}{\zeta(x,v_{\mu,L}(x))})$. For all such $t$, Lemma \ref{lem:cubic} yields  
\begin{equation}\label{eq:SO_potent_upper_bound}
\begin{split}
F_{\mu}(x^{+}(t))\leq F_{\mu}(x)&+t\inner{\nabla F_{\mu}(x),v_{\mu,L}(x)} + \frac{t^2}{2}\inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)} \\
&+ \frac{Mt^3}{6}\norm{v_{\mu,L}(x)}^{3}_{x}  +\mu t^{2}\omega(t\zeta(x,v_{\mu,L}(x))).
\end{split}
\end{equation}
Since $v_{\mu,L}(x) \in \setL_0$, multiplying \eqref{eq:PD} with $v_{\mu,L}(x)$ from the left and the right, and multiplying \eqref{eq:opt1} by $v_{\mu,L}(x)$ and combining with \eqref{eq:opt2}, we obtain
\begin{align}
&\inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)}\geq-\frac{L}{2}\norm{v_{\mu,L}(x)}^{3}_{x},\label{eq:descent1}\\
&\inner{\nabla F_{\mu}(x),v_{\mu,L}(x)}+\inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)}+\frac{L}{2}\norm{v_{\mu,L}(x)}^{3}_{x}=0.\label{eq:normal}
\end{align}
Under the additional assumption that $t\leq 2$ and $L\geq M$, we obtain
\begin{align*}
&t\inner{\nabla F_{\mu}(x),v_{\mu,L}(x)} + \frac{t^2}{2}\inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)} + \frac{Mt^3}{6}\norm{v_{\mu,L}(x)}^{3}_{x}\\
&\stackrel{\eqref{eq:normal}}{=} - t \left(\inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)}+\frac{L}{2}\norm{v_{\mu,L}(x)}^{3}_{x}\right) &\\
&+ \frac{t^2}{2}\inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)} + \frac{Mt^3}{6}\norm{v_{\mu,L}(x)}^{3}_{x} \\
& = \left(\frac{t^2}{2} - t \right) \inner{\nabla^{2}f(x)v_{\mu,L}(x),v_{\mu,L}(x)} - \frac{Lt}{2}\norm{v_{\mu,L}(x)}^{3}_{x} + \frac{Mt^3}{6}\norm{v_{\mu,L}(x)}^{3}_{x} \\
& \stackrel{\eqref{eq:descent1},t\leq 2}{\leq} \left(\frac{t^2}{2} - t \right) \left(- \frac{L}{2}\norm{v_{\mu,L}(x)}^{3}_{x} \right) - \frac{Lt}{2}\norm{v_{\mu,L}(x)}^{3}_{x} + \frac{Mt^3}{6}\norm{v_{\mu,L}(x)}^{3}_{x} \\
& = - \norm{v_{\mu,L}(x)}^{3}_{x} \left(\frac{Lt^2}{4} - \frac{Mt^3}{6} \right) 
\stackrel{L \geq M}{\leq} - \norm{v_{\mu,L}(x)}^{3}_{x} \frac{Lt^2}{12} \left(3 - 2t \right).
\end{align*}
Substituting this into \eqref{eq:SO_potent_upper_bound}, we arrive at
\begin{align*}
F_{\mu}(x^{+}(t))&\leq F_{\mu}(x) - \norm{v_{\mu,L}(x)}^{3}_{x} \frac{Lt^2}{12} \left(3 - 2t \right)+\mu t^{2}\omega(t\zeta(x,v_{\mu,L}(x)))\\ 
&\stackrel{\eqref{eq:omega_upper_bound}}{\leq}  F_{\mu}(x) - \norm{v_{\mu,L}(x)}^{3}_{x} \frac{Lt^2}{12} \left(3 - 2t \right)+\mu \frac{t^{2}\norm{v_{\mu,L}(x)}_{x}^{2}}{2(1-t\zeta(x,v_{\mu,L}(x))}. 
\end{align*} 
for all  $t \in I_{x,\mu,L}$. Therefore, if $t\zeta(x,v_{\mu,L}(x))\leq 1/2$, we readily see%\footnote{
%Note that since $t\zeta(x,v_{\mu,L}(x))=\zeta(x,tv_{\mu,L}(x))$ and we assumed that $t\zeta(x,v_{\mu,L}(x))\leq 1/2$, it is sufficient to make Assumption \ref{ass:2ndorder} on a potentially smaller set $\widetilde{\scrT}_{x}\eqdef \{v\in\setE\vert \bA v=0,\zeta(x,v)\leq 1/2\}$ instead of $\scrT_x$ defined in \eqref{eq:Dikinv}.
%}  
\begin{align}
F_{\mu}(x^{+}(t))-F_{\mu}(x)&\leq - \frac{Lt^2\norm{v_{\mu,L}(x)}^{3}_{x} }{12} \left(3 - 2t \right)+\mu t^{2}\norm{v_{\mu,L}(x)}_{x}^{2} \nonumber \\
&= - \norm{v_{\mu,L}(x)}^{3}_{x} \frac{Lt^2}{12}\left(3 - 2t - \frac{12 \mu}{L\norm{v_{\mu,L}(x)}_{x}} \right) \eqdef -\eta_{x}(t).\label{eq:progressSOM}
\end{align}
Maximizing the above function $\eta_{x}(t)$ and finding a lower bound for its optimal value is technically quite challenging. Instead, we adopt the following step-size rule
\begin{equation}
\label{eq:SO_t_opt_def}
\ct_{\mu,L}(x) \eqdef \frac{1}{\max\{1,2\zeta(x,v_{\mu,L}(x))\}} = \min \left\{1, \frac{1}{2\zeta(x,v_{\mu,L}(x))}  \right\}.
\end{equation}  
Note that $\ct_{\mu,L}(x) \leq 1$ and $\ct_{\mu,L}(x)\zeta(x,v_{\mu,L}(x))\leq 1/2$. Thus, this choice of the step-size is feasible to derive \eqref{eq:progressSOM}. 
%%%%%%%%%%%%

Just like Algorithm \ref{alg:AHBA}, our second-order method employs a line-search procedure to estimate the Lipschitz constant $M$ in \eqref{eq:SO_Lipschitz_Gradient}, \eqref{eq:cubicestimate} in the spirit of \cite{NesPol06,CarGouToi12}. More specifically, suppose that $x^{k}\in\setX$ is the current position of the algorithm with the corresponding initial local Lipschitz estimate $M_{k}$. To determine the next iterate $x^{k+1}$, we solve problem \eqref{eq:cubicproblem} with $L= L_k = 2^{i_k}M_{k}$ starting with $ i_k =0$, find the corresponding search direction $v^{k}=v_{\mu,L_{k}}(x^{k})$ and the new point $x^{k+1} = x^{k} + \ct_{\mu, L_{k}}(x^{k})v^{k}$. 
Then, we check whether the inequalities \eqref{eq:SO_Lipschitz_Gradient} and \eqref{eq:cubicestimate} hold with  $M=L_{k}$, $x=x^{k}$, $v = \ct_{\mu, L_{k}}(x^{k})v^{k}$, see \eqref{eq:SO_LS_2} and \eqref{eq:SO_LS_1}. 
If they hold, we make a step to $x^{k+1}$. 
Otherwise, we increase $i_k$ by 1 and repeat the procedure. Obviously, when $L_{k} = 2^{i_k}M_k \geq M$, both inequalities \eqref{eq:SO_Lipschitz_Gradient} and \eqref{eq:cubicestimate} with $M$ changed to $L_k$, i.e., \eqref{eq:SO_LS_2} and \eqref{eq:SO_LS_1}, are satisfied and the line-search procedure ends. For the next iteration we set $M_{k+1} = \max\{2^{i_k-1}M_{k},\underline{L}\}=\max\{L_{k}/2,\underline{L}\}$, so that the estimate for the local Lipschitz constant on the one hand can decrease allowing larger step-sizes, and on the other hand is bounded from below. 
The resulting procedure gives rise to a \underline{S}econd-order \underline{A}daptive \underline{H}essian-\underline{B}arrier \underline{A}lgorithm ($\SAHBA$, Algorithm \ref{alg:SOAHBA}).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[t!]
\caption{\underline{S}econd-order \underline{A}daptive \underline{H}essian-\underline{B}arrier \underline{A}lgorithm - $\SAHBA(\mu,\eps,M_{0},x^{0})$}
\label{alg:SOAHBA}
\SetAlgoLined
\KwData{ $h \in\scrH_{\nu}(\setK)$, $\mu>0,\eps>0,M_0\geq 144  \eps,x^{0}\in\setX$.}
\KwResult{$(x^{k},y^{k-1},s^{k},M_{k})\in\setX\times\R^{m}\times\setK^{\ast}\times\R_{+}$, where $s^{k}=\nabla f(x^{k}) -\bA^{\ast}y^{k-1}$, and $M_{k}$ is the last estimate of the Lipschitz constant.}
Set $\underline{L} \eqdef 144  \eps$, $k=0$\;
%Set $144  \eps \eqdef \underline{L} < M_0$ -- initial guess for $M$\;% $\mu = \frac{\eps}{4\nu}$, $k=0$, $x^{0}\in\setX$ -- $4\nu$-analytic center (see \eqref{eq:analytic_center})
\Repeat{
		$\norm{v^{k-1}}_{x^{k-1}} < \Delta_{k-1}\eqdef\sqrt{\frac{\eps}{4L_{k-1}\nu}}$ \textnormal{ and }$\|v^{k}\|_{x^{k}} < \Delta_{k}\eqdef\sqrt{\frac{\eps}{4L_{k}\nu}}$
	}{	
		Set $i_k=0$.
		
		\Repeat{
		\begin{align}
			& f(z^{k}) \leq f(x^{k}) + \inner{\nabla f(x^{k}),z^{k}-x^{k}}+\frac{1}{2}\inner{\nabla^{2}f(x^{k})(z^{k}-x^{k}),z^{k}-x^{k}} \notag \\
		&\qquad\qquad\qquad\qquad\qquad+\frac{L_{k}}{6}\norm{z^{k}-x^{k}}^{3}_{x^{k}}, \quad \textnormal{and } \label{eq:SO_LS_1}\\
		&  \norm{\nabla f(z^{k})-\nabla f(x^{k})-\nabla^{2}f(x^{k})(z^{k}-x^{k})}^{\ast}_{x^{k}}\leq\frac{L_{k}}{2}\norm{z^{k}-x^{k}}^{2}_{x^{k}}. \label{eq:SO_LS_2}
		\end{align}
		}
		{
			Set $L_k = 2^{i_k}M_k$. Find $v^k \eqdef v_{\mu,L_k}(x^{k})$ and $y^k \eqdef y_{\mu,L_k}(x^{k})$ as a global solution to
			\begin{align}
				&\hspace{-2em} \min_{v:\bA v=0} \left\{F_{\mu}(x^k)+\inner{\nabla F_{\mu}(x^k),v}+\frac{1}{2}\inner{\nabla^{2}f(x^k)v,v}+\frac{L_{k}}{6}\norm{v}_{x^k}^{3} \right\}. \label{eq:SO_finder} \\
				&\hspace{-1em} \text{Set }\;\; \alpha_k\eqdef\min \left\{1, \frac{1}{2\zeta(x^k,v^k)}  \right\},  	\text{where $\zeta(\cdot,\cdot)$ as in \eqref{eq:zeta}}. \label{eq:SO_alpha_k}
			\end{align}
					
			Set $z^{k}=x^{k} + \alpha_k v^k$, $i_k=i_k+1$\;
		}
		Set $M_{k+1} =\max\{\frac{L_{k}}{2},\underline{L}\}$, %\max\{2^{i_k-1}M_{k},\underline{L}\}$,  
		$x^{k+1}=z^{k}$, $k=k+1$%i_k-2 is since when we find a suitable $i_k$ we still set $i_k=i_k+1$
	}
\end{algorithm}
Our main result on the iteration complexity of Algorithm \ref{alg:SOAHBA} is the following Theorem, whose proof is given in Section \ref{sec:ProofSOM}. 
%
\begin{theorem}
\label{Th:SOAHBA_conv}
Let Assumptions \ref{ass:1}, \ref{ass:barrier}, and \ref{ass:2ndorder} hold. Fix the error tolerance $\eps>0$, the regularization parameter $\mu= \frac{\eps}{4\nu}$, and some initial guess $M_0>144\eps$ for the Lipschitz constant. Let $(x^{k})_{k\geq 0}$ be the trajectory generated by $\SAHBA(\mu,\eps,M_{0},x^{0})$, where $x^{0}$ is a $4\nu$-analytic center satisfying \eqref{eq:analytic_center}.
Then the algorithm stops in no more than 
\begin{equation}
\label{eq:SO_main_Th_compl}
\K_{II}(\eps,x^{0})= \ceil[\bigg]{\frac{192 \nu^{3/2} \sqrt{2\max\{M,M_0\}}(f(x^{0}) - f_{\min}(\setX)+ \eps)}{\eps^{3/2} }}
\end{equation}
outer iterations, and the number of inner iterations is no more than $2(\K_{II}(\eps,x^{0})+1)+2\max\{\log_{2}(2M/M_{0}),1\}$. Moreover, the output of $\SAHBA(\mu,\eps,M_{0},x^{0})$ constitute an $(\eps,\frac{\max\{M,M_0\}\eps}{8\nu})$-2KKT point for problem \eqref{eq:Opt} in the sense of Definition \ref{def:eps_SOKKT}.
%\begin{align}
%&\|\nabla f(x^{k}) - \bA^{\ast}y^{k-1} - s^{k} \| = 0 \leq \eps, \label{eq:SO_main_Th_eps_KKT_1} \\
%& |\inner{s^{k},x^{k}}| \leq \eps,  \label{eq:SO_main_Th_eps_KKT_2}  \\
%& \bA x^{k}=b, s^{k} \in \setK^{\ast}, x^{k}\in\setK \label{eq:SO_main_Th_eps_KKT_3}  \\
%& \nabla^2f(x^k)  + H(x^k) \sqrt{\frac{M\eps}{\nu}}  \succeq 0 \;\; \text{on} \;\; \setL_0. \label{eq:SO_main_Th_eps_KKT_4} 
%\end{align}  
\end{theorem}
\begin{remark}
\label{rem:SO_complexity_simplified}
Since $f(x^{0}) - f_{\min}(\setX)$ is expected to be larger than $\eps$, and the constant $M$ is potentially large, we see that the main term in the complexity bound \eqref{eq:SO_main_Th_compl} is $O\left(\frac{\nu^{3/2}\sqrt{M}(f(x^{0}) - f_{\min}(\setX))}{\eps^{3/2}}\right)=O((\frac{\nu}{\eps})^{3/2})$.
Note that the complexity result  $O(\max\{\eps_1^{-3/2},\eps_2^{-3/2}\})$ reported in \cite{CarDucHinSid19b,CarDucHinSid19} to find an $(\eps_1,\eps_2)$-2KKT point for arbitrary $ \eps_1,\eps_2 > 0$, is known to be optimal for unconstrained smooth non-convex optimization by second-order methods under the standard Lipschitz-Hessian assumption. It can be easily obtained from our theorem by setting $\eps=\max\{\eps_1^{-3/2},\eps_2^{-3/2}\}$. 
\close
\end{remark}
%\begin{remark}
%An interesting observation is that our algorithm can be interpreted as a damped version of a cubic-regularized Newton's method. When $h \in\scrH_{\nu}(\setK)$ and we consider it as an element of $\scrH_{\nu}(\setK)$, we have that $\zeta(x^k,v^k) = \norm{v^k}_{x^k}$ and the stepsize $\alpha_k$ satisfies $\alpha_k= \min \left\{1, \frac{1}{2\norm{v^k}_{x^k}}  \right\}$. At the initial phase, when the algorithm is far from an $(\eps_1,\eps_2)$-2KKT point, we have $\norm{v^k}_{x^k} > 1/2$ and $\alpha_k = \frac{1}{2\norm{v^k}_{x^k}}<1$. When the algorithm is getting closer to $(\eps_1,\eps_2)$-2KKT point, $\norm{v^k}_{x^k}$ becomes smaller and the algorithm automatically switches to full steps $\alpha_k=1$. 
%
%At the same time our algorithm is completely different from cubic-regularized Newton's method \cite{NesPol06} applied to minimize the potential $F_{\mu}$. Indeed, we regularize by the cube of the local norm, rather than cube of the standard Euclidean norm, and we do not form a second-order Taylor expansion of $F_{\mu}$. These adjustments are needed to align the search direction subproblem with the local geometry of the feasible set. Moreover, for our algorithm, the analysis of the cubic-regularized Newton's method is not directly applicable since it relies on the stepsize 1, which may lead to infeasible iterates in our case.
%\close
%\end{remark}




\subsection{Proof of Theorem \ref{Th:SOAHBA_conv}}
\label{sec:ProofSOM}
%In this subsection we analyze the convergence of $\SAHBA$ and prove Theorem \ref{Th:SOAHBA_conv}. 
The main steps of the proof are similar to the analysis of Algorithm \ref{alg:AHBA}. We start by showing the feasibility of the iterates and correctness of the line-search process. Next, we analyze the per-iteration decrease of $F_{\mu}$ and $f$ and show that if the stopping criterion does not hold at iteration $k$, then the objective function is decreased by the value $O(\eps^{3/2})$. From this, since the objective is globally lower bounded, we conclude that the algorithm stops in  $O(\eps^{-3/2})$ iterations. Finally, we show that when the stopping criterion holds, the primal-dual pair $(x^{k}$, $y^{k-1})$ resulting from solving the cubic subproblem \eqref{eq:SO_finder} yields a dual slack variable $s^{k}$ such that this triple constitutes an second-order KKT point. 

\subsubsection{Interior point property of the iterates}
By construction $x^{0}\in\setX$. Proceeding inductively, let $x^{k}\in\setX$ be the $k$-th iterate of the algorithm, with the search direction $v^{k}\equiv v_{\mu,L}(x^{k})$.  By eq. \eqref{eq:SO_alpha_k}, the step-size $\alpha_k$ satisfies $\alpha_{k}\leq \frac{1}{2\zeta(x^{k},v^{k})}$. Consequently, $\alpha_{k}\zeta(x^{k},v^{k})\leq 1/2$ for all $k\geq 0$, and using \eqref{eq:step_length_zeta} as well as $\bA v^{k} =0$, we have that $x^{k+1}=x^{k}+\alpha_{k}v^{k}\in\setX$. By induction, it follows that $x^{k}\in\setX$ for all $k\geq 0$.

\subsubsection{Bounding the number of backtracking steps}
\label{sec:backtrack2}
%To bound the number of cycles involved in the line-search process for finding appropriate constants $L_{k}$, we proceed as in Section \ref{sec:backtrack1}. Since the derivations are analogous to the ones performed there, we skip the details, and just report here that the number of line-search iterations up to iteration $k$ of $\SAHBA(\mu,\eps,M_0,x^{0})$ is bounded as  
%\begin{align*}
%N(k)&=\sum_{j=0}^{k}(i_{j}+1)\leq i_0+1 + \sum_{j=1}^{k}\left(\log_{2}\left(\frac{L_{j}}{L_{j-1}}\right)+2\right) 
%\leq 2(k+1) + 2 \log_2\left(\frac{2\bar{M}}{M_0}\right),
%\end{align*}
%since $L_{k} \leq 2\bar{M} \eqdef 2\max\{M_0,M\}$ in the last step. Thus, on average, the inner loop ends after two trials. 
To bound the number of cycles involved in the line-search process for finding appropriate constants $L_{k}$, we proceed as in Section \ref{sec:backtrack1}. 
Let us fix an iteration $k$. The sequence $L_k = 2^{i_k} M_k$ is increasing as $i_k$ is increasing, and Assumption \ref{ass:2ndorder} holds. 
This implies \eqref{eq:cubicestimate}, and thus when $L_k = 2^{i_k} M_k \geq \max\{M,M_k\}$, the line-search process for sure stops since inequalities \eqref{eq:SO_LS_1} and \eqref{eq:SO_LS_2} hold.	
Hence, $L_k=2^{i_k} M_k \leq 2\max\{M,M_k\}$ must be the case, and, consequently, $M_{k+1}=\max\{L_k/2, \underline{L}\} \leq  \max\{\max\{M,M_k\}, \underline{L}\} = \max\{M,M_k\}$, which, by induction, gives $M_{k} \leq \bar{M}\equiv \max\{M,M_0\}$ and $L_k  \leq 2\bar{M}$. 
%
%
%$L_{k+1} = 2^{i_k-1} L_k \leq \max\{M,L_k\}$, which, by induction, gives $L_{k+1} \leq \bar{M}\eqdef\max\{M,L_0\}$. 
%Moreover, from this observation it follows that $L_k = 2^{i_k} M_k \leq 2\bar{M}=2\max\{M_0,M\}$. 
%
%
At the same time, by construction, $M_{k+1}= \max\{2^{i_k-1}M_{k},\underline{L}\} = \max\{L_k/2,\underline{L}\} \geq L_k/2 $. Hence, $L_{k+1} = 2^{i_{k+1}} M_{k+1} \geq 2^{i_{k+1}-1} L_k$ and therefore $\log_{2}\left(\frac{L_{k+1}}{L_{k}}\right)\geq i_{k+1}-1$, $\forall k\geq 0$. At the same time, at iteration $0$ we have $L_0=2^{i_0} M_0 \leq 2\bar{M}$, whence, $i_0 \leq \log_2\left(\frac{2\bar{M}}{M_0}\right)$.
Let $N(k)$ denote the number inner line-search iterations up to iteration $k$ of $\SAHBA$. Then, 
\begin{align*}
N(k)&=\sum_{j=0}^{k}(i_{j}+1)\leq i_0+1 + \sum_{j=1}^{k}\left(\log_{2}\left(\frac{L_{j}}{L_{j-1}}\right)+2\right) 
\leq 2(k+1) + 2 \log_2\left(\frac{2\bar{M}}{M_0}\right),
\end{align*}
since $L_{k} \leq 2\bar{M}= 2\max\{M,M_0\}$ in the last step. Thus, on average, the inner loop ends after two trials. 




%Next, we give a bound on the number of cycles involved in the line-search process for finding appropriate constants $L_k$. This allows us to connect the iteration complexity estimate to the number of function evaluations in the worst-case. Let us fix an iteration $k$. The sequence $L_k = 2^{i_k} M_k$ is increasing as $i_k$ is increasing and Assumption \ref{ass:2ndorder} holds. This implies \eqref{eq:cubicestimate}, and thus when $L_k = 2^{i_k} M_k \geq M$, the line-search process for sure stops since inequalities \eqref{eq:SO_LS_1} and \eqref{eq:SO_LS_2} hold.	
%Moreover, from this observation it follows that $L_k = 2^{i_k} M_k \leq 2\bar{M}=2\max\{M_0,M\}$. This also allows us to estimate the total number of backtracking steps after $k$ iterations. By construction, $M_{k+1}= \max\{2^{i_k-1}M_{k},\underline{L}\} = \max\{L_k/2,\underline{L}\} \geq L_k/2 $. Hence, $L_{k+1} = 2^{i_{k+1}} M_{k+1} \geq 2^{i_{k+1}-1} L_k$ and therefore $\log_{2}\left(\frac{L_{k+1}}{L_{k}}\right)\geq i_{k+1}-1$, $\forall k\geq 0$. At the same time, at iteration $0$ we have $L_0=2^{i_0} M_0 \leq 2\bar{M}$, whence, $i_0 \leq \log_2\left(\frac{2\bar{M}}{M_0}\right)$.
%Let $N(k)$ denote the number inner line-search iterations up to iteration $k$ of $\SAHBA$. Then, 
%\begin{align*}
%N(k)&=\sum_{j=0}^{k}(i_{j}+1)\leq i_0+1 + \sum_{j=1}^{k}\left(\log_{2}\left(\frac{L_{j}}{L_{j-1}}\right)+2\right) 
%\leq 2(k+1) + 2 \log_2\left(\frac{2\bar{M}}{M_0}\right),
%\end{align*}
%since $L_{k} \leq 2\bar{M}$ in the last step. Thus, on average, the inner loop ends after two trials. 

\subsubsection{Per-iteration analysis and a bound for the number of iterations}
Let us fix iteration counter $k$. The main assumption of this subsection is that the stopping criterion is not satisfied, i.e. either
$\|v^{k}\|_{x^{k}} \geq \Delta_{k}$ or $\|v^{k-1}\|_{x^{k-1}} \geq \Delta_{k-1}$. 
Without loss of generality, we assume that the first inequality holds, i.e., $\|v^{k}\|_{x^{k}} \geq \Delta_{k}$, and consider iteration $k$. Otherwise, if the second inequality holds, the same derivations can be made considering the iteration $k-1$ and using the second inequality $\|v^{k-1}\|_{x^{k-1}} \geq \Delta_{k-1}$. Thus, at the end of the $k$-th iteration 
\begin{equation}
\label{eq:SO_per_iter_proof_1}
\|v^{k}\|_{x^{k}} \geq \Delta_{k}=\sqrt{\frac{\eps}{4L_{k}\nu}}.
\end{equation}
Since the step-size $\alpha_k= \min\{1,\frac{1}{ 2\zeta(x^k,v^k)}\} = \ct_{\mu,L_k}(x^{k})$ in \eqref{eq:SO_alpha_k} satisfies $\alpha_k \leq 1$ and $\alpha_k \zeta(x^k,v^k) \leq 1/2$ (cf. \eqref{eq:SO_t_opt_def} and a remark after it), we can repeat the derivations of Section \ref{S:SO_alg_descr}, 
changing  \eqref{eq:cubicestimate} to  \eqref{eq:SO_LS_1}.
%changing \eqref{eq:SO_Lipschitz_Gradient} and \eqref{eq:cubicdecrease} to \eqref{eq:SO_LS_2} and \eqref{eq:SO_LS_1} respectively. 
In this way we obtain the following counterpart of \eqref{eq:progressSOM} with $t=\alpha_k$, $L=L_k$, $x=x^k$, $v_{\mu,L_k}(x^{k}) \eqdef v^k$: 
\begin{align}
F_{\mu}(x^{k+1})-F_{\mu}(x^{k})&\leq - \norm{v^{k}}^{3}_{x^{k}} \frac{L_k\alpha_k^2}{12}\left(3 - 2\alpha_k - \frac{12 \mu}{L_k\norm{v^{k}}_{x^{k}}} \right)\nonumber\\
& \leq - \norm{v^{k}}^{3}_{x^{k}} \frac{L_k\alpha_k^2}{12}\left(1 - \frac{12 \mu}{L_k\norm{v^{k}}_{x^{k}}} \right)\label{eq:SO_per_iter_proof_2},
\end{align}
where in the last inequality we used that $\alpha_k \leq 1$ by construction. 
Substituting $\mu = \frac{\eps}{4\nu}$, and using \eqref{eq:SO_per_iter_proof_1}, we obtain
\begin{align*}
1 - \frac{12 \mu}{L_k\norm{v^{k}}_{x^{k}}} &= 1 - \frac{12 \eps}{4\nu L_k\norm{v^{k}}_{x^{k}}}  %\stackrel{\eqref{eq:SO_per_iter_proof_1}}{\geq} 1 - \frac{3 \beta^2\eps}{\nu L_k\Delta_k} \\
\stackrel{\eqref{eq:SO_per_iter_proof_1}}{\geq} 1 - \frac{3 \eps}{\nu L_k\sqrt{\frac{\eps}{4L_{k}\nu}}} \\
&= 1 - \frac{6 \sqrt{\eps}}{ \sqrt{\nu L_k}} \geq 1 - \frac{6 \sqrt{\eps}}{ \sqrt{ 144 \nu \eps	}}  \geq \frac{1}{2},
\end{align*}
using that, by construction, $L_k =2^{i_k}M_k \geq \underline{L} = 144 \eps$ and that $\nu \geq 1$. 
Hence, from \eqref{eq:SO_per_iter_proof_2}, 
\begin{equation}
\label{eq:SO_per_iter_proof_3}
F_{\mu}(x^{k+1})-F_{\mu}(x^{k})\leq  - \norm{v^{k}}^{3}_{x^{k}} \frac{L_k\alpha_k^2}{24}.
\end{equation}
%Consider two possible cases of the value of the step-size $\alpha_k$ in \eqref{eq:SO_alpha_k} and substitute it into \eqref{eq:SO_per_iter_proof_3}:
Substituting into \eqref{eq:SO_per_iter_proof_3} the two possible values of the step-size $\alpha_k$ in \eqref{eq:SO_alpha_k} gives
\begin{equation}
\label{eq:SO_per_iter_proof_8}
F_{\mu}(x^{k+1})-F_{\mu}(x^{k})\leq 
\left\{
\begin{array}{ll}
- \norm{v^{k}}^{3}_{x^{k}} \frac{L_k}{24}, & \text{if }  \alpha_k=1,\\
 -  \frac{L_k\norm{v^{k}}^{3}_{x^{k}}}{96 (\zeta(x^k,v^k))^2} \stackrel{\eqref{eq:boundzeta}}{\leq} -  \frac{L_k\norm{v^{k}}_{x^{k}}}{96}& \text{if } \alpha_k=\frac{1}{2\zeta(x^k,v^k)}.
\end{array}\right.
\end{equation}
This implies
%From  \eqref{eq:SO_per_iter_proof_4} and \eqref{eq:SO_per_iter_proof_6}, we obtain that
\begin{equation}
\label{eq:SO_per_iter_proof_7}
F_{\mu}(x^{k+1})-F_{\mu}(x^{k}) \leq - \frac{L_k\norm{v^{k}}_{x^{k}}}{96} \min\left\{1, 4\norm{v^{k}}_{x^{k}}^2 \right\} \eqdef-\delta_{k}.
\end{equation}
Rearranging and summing these inequalities for $k$ from $0$ to $K-1$, and using that $L_k \geq \underline{L}$, we obtain
\begin{align}
%\label{eq:}
K\min_{k=0,...,K-1} &\frac{\underline{L}\norm{v^{k}}_{x^{k}}}{96} \min\left\{1, 4\norm{v^{k}}_{x^{k}}^2\right\} \leq  \sum_{k=0}^{K-1} \delta_k 
\leq F_{\mu}(x^{0})-F_{\mu}(x^{K}) \notag \\
&\stackrel{\eqref{eq:potential}}{=} f(x^{0}) - f(x^{K}) + \mu (h(x^{0}) - h(x^{K})) \leq f(x^{0}) - f_{\min}(\setX) + \eps, \label{eq:SO_per_iter_proof_9}
\end{align}
where we used that, by the assumptions of Theorem \ref{Th:SOAHBA_conv}, $x^{0}$ is a $4\nu$-analytic center defined in \eqref{eq:analytic_center} and $\mu = \frac{\eps}{4\nu}$, implying that $h(x^{0}) - h(x^{K}) \leq 4\nu = \eps/\mu$.
Thus, up to passing to a subsequence, we have $\norm{v^{k}}_{x^{k}} \to 0$ as $k \to \infty$, which makes the stopping criterion in Algorithm \ref{alg:SOAHBA} achievable.

Assume now that the stopping criterion does not hold for $K$ iterations of $\SAHBA$. 
Then, for all $k=0,\ldots,K-1,$ it holds that 
\begin{align}
\delta_k &= \frac{L_k}{96} \min\left\{\norm{v^{k}}_{x^{k}}, 4\norm{v^{k}}_{x^{k}}^3 \right\} 
\stackrel{\eqref{eq:SO_per_iter_proof_1}}{\geq} \frac{L_k}{96} \min\left\{  \sqrt{\frac{\eps}{4L_{k}\nu}} ,  \frac{4 \eps^{3/2}}{4^{3/2}L_{k}^{3/2}\nu^{3/2}}  \right\} \notag \\
&\stackrel{L_k \leq 2\bar{M}, \nu \geq 1}{\geq}  \frac{1}{96} \min\left\{  \frac{L_k \sqrt{\eps}}{\sqrt{8\bar{M}} \nu^{3/2}} ,  \frac{ \eps^{3/2}}{2 L_{k}^{1/2}\nu^{3/2}}  \right\}  \notag \\
&\stackrel{L_k \leq 2\bar{M},L_k \geq 144  \eps}{\geq} \frac{1}{96} \min\left\{ \frac{(144 \eps) \cdot \sqrt{\eps}}{\sqrt{8\bar{M}} \nu^{3/2}} ,  \frac{ \eps^{3/2}}{\sqrt{8\bar{M}}\nu^{3/2}}  \right\}  = \frac{\eps^{3/2}}{192 \nu^{3/2} \sqrt{2\bar{M}} },
\end{align}
Thus, from \eqref{eq:SO_per_iter_proof_9}.
\begin{align*}
K \frac{\eps^{3/2}}{192 \nu^{3/2} \sqrt{2\bar{M}} } \leq f(x^{0}) - f_{\min}(\setX) + \eps. 
\end{align*}
Hence, reacalling that $\bar{M}=\max\{M_0,M\}$, $K \leq \frac{192 \nu^{3/2} \sqrt{2\max\{M_0,M\}}(f(x^{0}) - f_{\min}(\setX)+ \eps)}{\eps^{3/2} }$,
i.e. the algorithm stops for sure after no more than this number of iterations. This, combined with the bound for the number of inner steps in Section \ref{sec:backtrack2}, proves the first statement of Theorem \ref{Th:SOAHBA_conv}.


\subsubsection{Generating a $(\eps_{1},\eps_{2})$-2KKT point}
In this section, to finish the proof of Theorem \ref{Th:SOAHBA_conv}, we show that if the stopping criterion in Algorithm \ref{alg:SOAHBA} holds, i.e. $\norm{v^{k-1}}_{x^{k-1}} < \Delta_{k-1}$ and $\norm{v^{k}}_{x^{k}} < \Delta_{k}$, then the algorithm has generated an $(\eps_{1},\eps_{2})$-2KKT point of \eqref{eq:Opt} according to Definition \ref{def:eps_SOKKT}, with $\eps_{1}=\eps$ and $\eps_{2}=\frac{\max\{M_{0},\bar{M}\}\eps}{8\nu}$.

Let the stopping criterion hold at iteration $k$. Using the first-order optimality condition \eqref{eq:opt1} for the subproblem \eqref{eq:SO_finder} solved at iteration $k-1$, there exists a dual variable $y^{k-1}\in\R^{m}$ such that \eqref{eq:opt1} holds. Now, expanding the definition of the potential \eqref{eq:potential} and adding $\nabla f(x^{k})$ to both sides, we obtain
\begin{align*}
 \nabla f(x^{k}) - & \bA^{\ast}y^{k-1} + \mu \nabla h(x^{k-1}) \\
&=\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1} - \frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}H(x^{k-1})v^{k-1}.
\end{align*}
Setting $s^{k}\eqdef \nabla f(x^{k})-\bA^{\ast}y^{k-1} \in \setE^{\ast}$ and $g^{k-1}\eqdef-\mu\nabla h(x^{k-1})$, after multiplication by $[H(x^{k-1})]^{-1}$, this is equivalent to 
\begin{align*}
[H(x^{k-1})]^{-1}\left(s^{k}-g^{k-1}\right)&=[H(x^{k-1})]^{-1}\left(\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1}\right)\\
& - \frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}v^{k-1}.
\end{align*}
Multiplying both of the above equalities, we arrive at 
\begin{align*}
\left(\norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}\right)^{2}
&=\left( \left\|\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1}  - \frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}H(x^{k-1})v^{k-1} \right\|_{x^{k-1}}^*\right)^{2}.
%&+\frac{L^{2}_{k-1}}{4}\norm{v^{k-1}}^{4}_{x^{k-1}}\\
%&-2\inner{\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1},\frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}v^{k-1}}.
\end{align*}
Taking the square root and applying the triangle inequality, we obtain
%Using the trivial inequality $-2\inner{a,b}\leq \norm{a}^{2}+\norm{b}^{2}$, it follows 
%\begin{align*}
%\left(\norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}\right)^{2}&=2\left(\norm{\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1}}^{\ast}_{x^{k-1}}\right)^{2}+\frac{L^{2}_{k-1}}{2}\norm{v^{k-1}}^{4}_{x^{k-1}}.
%\end{align*}
%Whence, 
\begin{align*}
\norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}&\leq\norm{\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1}}^{\ast}_{x^{k-1}}+\frac{L_{k-1}}{2}\norm{v^{k-1}}^{2}_{x^{k-1}}\\
&\stackrel{\eqref{eq:dualnorm},\eqref{eq:SO_LS_2},\eqref{eq:localnorm}}{\leq} \frac{L_{k-1}}{2}\norm{\alpha_{k-1}v^{k-1}}^{2}_{x_{k-1}}+\frac{L_{k-1}}{2}\norm{v^{k-1}}^{2}_{x^{k-1}}.
\end{align*}
Since the stopping criterion holds, at iteration $k-1$ we have
\begin{align}
\hspace{-1em}\zeta(x^{k-1},v^{k-1}) &\stackrel{\eqref{eq:boundzeta}}{\leq} \|v^{k-1}\|_{x^{k-1}} < \Delta_{k-1} = \sqrt{\frac{\eps}{4L_{k-1}\nu}} \leq \sqrt{\frac{\eps}{4 \cdot 144 \eps \nu}} < \frac{1}{2}\label{eq:SO_eps_KKT_proof_0},
\end{align}
where we used that, by construction, $L_{k-1} \geq \underline{L} = 144 \eps$ and that $\nu \geq 1$. Hence, by \eqref{eq:SO_alpha_k}, we have that $\alpha_{k-1}=1$ and $x^{k}=x^{k-1}+v^{k-1}$. This, in turn, implies that 
\begin{equation}
\label{eq:SO_eps_KKT_proof_1}
\norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}\leq  L_{k-1}\norm{v^{k-1}}^{2}_{x^{k-1}}.
\end{equation}
Now we follow the analysis of the first-order method by noting that $\norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}=\mu\norm{s^{k}-g^{k-1}}_{\nabla^{2}h_{\ast}(g^{k-1})}$ and $\mu=\frac{\eps}{4\nu}$, 
which implies
\begin{equation}
\label{eq:SO_eps_KKT_proof_2}
\norm{s^{k}-g^{k-1}}_{\nabla^{2}h_{\ast}(g^{k-1})}\leq\frac{L_{k-1}}{\mu}\norm{v^{k-1}}^{2}_{x^{k-1}}<\frac{L_{k-1}}{\mu}\Delta_{k-1}^{2} = \frac{L_{k-1}}{\frac{\eps}{4\nu}} \cdot \frac{\eps}{4L_{k-1}\nu} = 1.
\end{equation}
%where we used the stopping criterion and that, by the assumptions of Theorem \ref{Th:SOAHBA_conv}, $\mu=\frac{\eps}{4\nu}$.
Thus, since, by \eqref{eq:relations_1}, $g^{k-1}=-\mu\nabla h(x^{k-1})\in \setK^{\ast}$, we get that $s^{k}\in\setK^{\ast}$. By construction, $x^{k}\in \setK$ and $\bA x^{k} = b$. Thus, \eqref{eq:eps_SO_optim_equality_cones} holds. We also have that, by construction, $\|\nabla f(x^{k})-\bA^{\ast}y^{k-1} - s^{k}\|=0\leq \eps$, meaning that \eqref{eq:eps_SO_optim_grad} holds with $\eps_1=\eps$. To finish the analysis of the first-order condition, it remains to check the complementarity condition \eqref{eq:eps_SO_optim_complem}. 
We have
\begin{align*}
\inner{s^{k},x^{k}}=\inner{s^{k},x^{k-1}+v^{k-1}}=\inner{s^{k},x^{k-1}}+\inner{s^{k},v^{k-1}}.
\end{align*}
We estimate each of the two terms in the r.h.s. separately. First, 
\begin{align*}
0 \leq \inner{s^{k},x^{k-1}}&%=\inner{\nabla f(x^{k})-\bA^{\ast}y^{k-1},x^{k-1}}=
=  \inner{s^{k}-g^{k-1},x^{k-1}} + \inner{g^{k-1},x^{k-1}} \\
&\leq \norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}\cdot\norm{x^{k-1}}_{x^{k-1}}-\mu\inner{\nabla h(x^{k-1}),x^{k-1}} \\
&\stackrel{\eqref{eq:SO_eps_KKT_proof_1},\eqref{eq:log_hom_scb_norm_prop},\eqref{eq:log_hom_scb_hess_prop}}{\leq}  L_{k-1}\norm{v^{k-1}}^{2}_{x^{k-1}}\sqrt{\nu}+\mu\nu. %\Delta^{2}_{k-1}
\end{align*}
Second, 
\begin{align*}
 \inner{s^{k},v^{k-1}}&\leq \norm{s^{k}}^{\ast}_{x^{k-1}}\cdot\norm{v^{k-1}}_{x^{k-1}}\leq \left(\norm{s^{k}-g^{k-1}}^{\ast}_{x^{k-1}}+\norm{g^{k-1}}^{\ast}_{x^{k-1}}\right)\cdot \norm{v^{k-1}}_{x^{k-1}}\\
&\stackrel{\eqref{eq:SO_eps_KKT_proof_1},\eqref{eq:log_hom_scb_norm_prop},\eqref{eq:log_hom_scb_hess_prop}}{\leq}  \left( L_{k-1}\norm{v^{k-1}}^{2}_{x^{k-1}}+\mu\sqrt{\nu}\right)\Delta_{k-1}.
\end{align*}
Summing up, using the stopping criterion $\norm{v^{k-1}}_{x^{k-1}}<\Delta_{k-1}$ and that, by \eqref{eq:SO_eps_KKT_proof_0}, $\Delta_{k-1} \leq 1\leq\sqrt{\nu}$, we obtain
\begin{align}
 0 \leq \inner{s^k,x^k} = \inner{s^k,x^{k-1}+v^{k-1}} \leq  2L_{k-1}\Delta_{k-1}^2 \sqrt{\nu} + 2\mu \nu = 
2 L_{k-1} \frac{\eps}{4L_{k-1}\nu}  \sqrt{\nu} + 2\frac{\eps}{4\nu} \nu \leq  \eps, \label{eq:SO_eps_KKT_proof_3}
\end{align}
i.e., \eqref{eq:eps_SO_optim_complem} holds with $\eps_1=\eps$.\\
Finally, we show the second-order condition \eqref{eq:eps_SO_optim_SO}.
%It remains to show the second-order approximate stationarity \eqref{eq:SO_main_Th_eps_KKT_4}. 
By inequality \eqref{eq:PD} for subproblem \eqref{eq:SO_finder} solved at iteration $k$, we obtain on $\setL_0$
\begin{align}
\nabla^2 f(x^{k})  &\succeq - \frac{L_{k}\norm{v^{k}}_{x^{k}}}{2} H(x^{k}) \succeq -\frac{L_{k} \Delta_k}{2} H(x^{k}) \notag \\
& =  - \frac{L_{k}}{2}\sqrt{\frac{\eps}{4L_{k}\nu}} H(x^{k}) = - \frac{\sqrt{L_k \eps}}{4\nu^{1/2}}H(x^{k})  
\succeq - \frac{ \sqrt{2\bar{M}\eps}}{4\nu^{1/2}}H(x^{k}) \label{eq:SO_eps_KKT_proof_4}
\end{align}
where we used the second part of the stopping criterion, i.e. $\norm{v^{k}}_{x^{k}}< \Delta_k$ and that $L_k \leq 2\bar{M}=2\max\{M,M_0\}$ (see Section \ref{sec:backtrack2}). Thus, \eqref{eq:eps_SO_optim_SO} holds with $\eps_2=\frac{\max\{M,M_0\}\eps}{8\nu}$, which finishes the proof of Theorem \ref{Th:SOAHBA_conv}.
%Using the 	second-order optimality condition \eqref{eq:PD} for the subproblem \eqref{eq:cubicproblem} solved at iteration $k$, we obtain, on $\setL_0$
%\begin{equation}
%%\label{eq:}
%[H(x^{k})]^{-1/2} \nabla^2 f(x^{k}) [H(x^{k})]^{-1/2} \succeq - \frac{L_{k}\norm{v^{k}}_{x^{k}}}{2} I \succeq -\frac{L_{k} \Delta_k}{2} I =  - \frac{L_{k}}{2} \beta\sqrt{\frac{\eps}{4L_{k}\sqrt{\nu}}} I = - \frac{\beta \sqrt{L_k \eps}}{4\nu^{1/4}}I  \succeq - \frac{\beta \sqrt{2M\eps}}{4\nu^{1/4}}I
%\end{equation}
%where we used the second part of the stopping criterion, i.e. $\norm{v^{k}}_{x^{k}}< \Delta_k$ and that $L_k \leq 2M$. This finishes the proof of \eqref{eq:SO_main_Th_eps_KKT_4} and the proof of Theorem \ref{Th:SOAHBA_conv}.



\subsection{Discussion}
\label{sec:SO_discussion}
%The analysis of $\SAHBA$ requires to overcome similar technical challenges to that of the analysis of $\AHBA$. Namely, we need to guarantee that $s^{k} \in\setK^{\ast}$, show that \eqref{eq:eps_SO_optim_complem} holds, and obtain the condition \eqref{eq:eps_SO_optim_grad}  formulated in terms of the standard Euclidean norm based on inequality \eqref{eq:SO_eps_KKT_proof_1} formulated in terms of the local norm. Additionally, we need to guarantee that the second-order condition \eqref{eq:eps_SO_optim_SO} holds. And again, in our setting of general, potentially non-symmetric, cones all this is more difficult than for the particular case of the non-negativity constraints considered in \cite{HaeLiuYe18,NeiWr20}, where a second-order algorithm and a first-order implementation of a second-order algorithm are proposed respectively. 
\paragraph{Strengthened KKT condition.}
As in Section \ref{sec:FO_discussion}, our aim in this section is to compare our result with those available in the contemporary literature. We therefore onsider the special case $\bar{\setK}=\bar{\setK}_{\text{NN}}$, endowed with the standard log-barrier $h(x)=-\sum_{i=1}^n \ln(x_i)$. Recall that for this barrier setup we have $\nabla h(x)=[-x_{1}^{-1},\ldots,-x_{n}^{-1}]^{\top}$, $H(x)=\diag[x_{1}^{-2},\ldots,x_{n}^{-2}]=\XX^{-2}$. Assume that the stopping criterion applies at iteration $k$. Using the first-order optimality condition \eqref{eq:opt1} for the subproblem \eqref{eq:SO_finder} solved at iteration $k-1$ and expanding the definition of the potential \eqref{eq:potential}, there exists a dual variable $y^{k-1}\in\R^{m}$ such that \eqref{eq:opt1} holds, i.e., 
\begin{align*}
 \nabla f(x^{k-1}) + \mu \nabla h(x^{k-1})  + \nabla^2 f(x^{k-1})v^{k-1} - \bA^{\ast}y^{k-1} =  - \frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}H(x^{k-1})v^{k-1}.
\end{align*}
Multiplying both sides by $H(x^{k-1})^{-1/2}$, using the stopping criterion $\norm{v^{k-1}}_{x^{k-1}}<\sqrt{\frac{\eps}{4\nu L_{k-1}}}$, since $H(x^{k-1})^{-1/2}\nabla h(x^{k-1})= - \1_{n}$ and $\nu=n$, we obtain
\begin{align}
&\norm{\XX^{k-1}(\nabla^2 f(x^{k-1})v^{k-1} + \nabla f(x^{k-1})-\bA^{\ast}y^{k-1}) - \mu \1_{n}}_{\infty} \notag \\
& \leq \norm{\XX^{k-1} (\nabla^2 f(x^{k-1})v^{k-1} + \nabla f(x^{k-1})-\bA^{\ast}y^{k-1}) - \mu \1_{n}  } = \frac{L_{k-1}}{2} \norm{-\XX^{k} v^{k-1}}^2\notag\\
& < \frac{\eps}{8n}. \label{eq:SO_remarks_1}
\end{align}
Whence, since $\mu=\frac{\eps}{4n}$, the above bound \eqref{eq:SO_remarks_1} combined with the triangle inequality yields
\begin{align}
& \norm{\XX^{k-1}(\nabla^2 f(x^{k-1})v^{k-1} +\nabla f(x^{k-1})-\bA^{\ast}y^{k-1})}_{\infty} \notag \\
&\leq \norm{\XX^{k-1} (\nabla^2 f(x^{k-1})v^{k-1} + \nabla f(x^{k-1})-\bA^{\ast}y^{k-1}) - \mu \1_{n}  } +\norm{\mu\1_{n}} \notag\\
&=\frac{L_{k-1}}{2} \norm{-\XX^{k} v^{k-1}}^2+\norm{\mu\1_{n}}<\frac{3\eps}{8n}. \label{eq:SO_remarks_2}
\end{align}
Let $\mathbf{V}^{k-1} = \diag(v^{k-1})$. Using the fact that $x^{k}=x^{k-1}+v^{k-1}$ shown after \eqref{eq:SO_eps_KKT_proof_0}, we obtain
\begin{align*}
& \norm{\XX^{k}(\nabla f(x^{k})-\bA^{\ast}y^{k-1})}_{\infty} \notag \\
& = \norm{(\XX^{k-1}+\mathbf{V}^{k-1})(\nabla^2 f(x^{k-1})v^{k-1} +\nabla f(x^{k-1})-\bA^{\ast}y^{k-1} + \nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1})}_{\infty} \notag  \\
& \leq \norm{\XX^{k-1}(\nabla^2 f(x^{k-1})v^{k-1} +\nabla f(x^{k-1})-\bA^{\ast}y^{k-1} )}_{\infty}\\ %\label{eq:SO_remarks_3} \\
& \hspace{1em} + \norm{\XX^{k-1} (\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1})}_{\infty}\\ %\label{eq:SO_remarks_4} \\
& \hspace{1em} + \norm{\mathbf{V}^{k-1}(\nabla^2 f(x^{k-1})v^{k-1} +\nabla f(x^{k-1})-\bA^{\ast}y^{k-1} )}_{\infty}\\ %\label{eq:SO_remarks_5}\\
& \hspace{1em} + \norm{ \mathbf{V}^{k-1}(\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1})}_{\infty}\\ %\label{eq:SO_remarks_6}\\
&=I+II+III+IV.
\end{align*}
Let us estimate each of the four terms $I-IV$, using two technical facts \eqref{eq:technical_1}, \eqref{eq:technical_2} proved in Appendix \ref{sec:Appendix}. We have:
\begin{align*}
 I& \stackrel{\eqref{eq:SO_remarks_2}}{<} \frac{3\eps}{8n}, \\
II& \leq \norm{\XX^{k-1} (\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1})}\\
& = \norm{\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1}}_{x^{k-1}}^* \stackrel{\eqref{eq:SO_LS_2}}{\leq} \frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}^2<\frac{\eps}{8n}, \\
III&  \stackrel{\eqref{eq:technical_2}}{\leq} \norm{v^{k-1}}_{x^{k-1}} \cdot \norm{\XX^{k-1} (\nabla^2 f(x^{k-1})v^{k-1} +\nabla f(x^{k-1})-\bA^{\ast}y^{k-1} )}_{\infty}  \stackrel{\eqref{eq:SO_eps_KKT_proof_0},\eqref{eq:SO_remarks_2}}{<}\frac{3\eps}{8n},
\end{align*}
where we have used $x^{k}=z^{k-1}=x^{k-1}+v^{k-1}$ in bounding $II$, and the last bound for expression $III$ uses $\norm{v^{k-1}}_{x^{k-1}}<1$, which is implied by eq. \eqref{eq:SO_eps_KKT_proof_0}. Finally, we also obtain
\begin{align*}
IV& \stackrel{\eqref{eq:technical_1}}{\leq} \norm{v^{k-1}}_{x^{k-1}} \cdot \norm{\nabla f(x^{k}) - \nabla f(x^{k-1}) - \nabla^2 f(x^{k-1})v^{k-1}}_{x^{k-1}}^{*}\\
&\stackrel{\eqref{eq:SO_eps_KKT_proof_0},\eqref{eq:SO_LS_2}}{\leq} \frac{L_{k-1}}{2}\norm{v^{k-1}}_{x^{k-1}}^2 <\frac{\eps}{8n}.
\end{align*}
Summarizing, we arrive at
\begin{equation}
\label{eq:SO_remarks_7}
\norm{\XX^{k}(\nabla f(x^{k})-\bA^{\ast}y^{k-1})}_{\infty} \leq \frac{\eps}{n}. 
\end{equation}
Further, by Theorem \ref{Th:SOAHBA_conv}, we have that $\nabla f(x^{k})-\bA^{\ast}y^{k-1} = s^k \in \setK^{\ast}_{\text{NN}}=\Rn_{++}$, and
\[
\nabla^2f(x^k)  + H(x^k) \sqrt{\frac{M\eps}{n}}  \succeq 0 \;\; \text{on} \;\; \setL_0.
\] 
By Remark \ref{rem:SO_complexity_simplified}, these inequalities are achieved after $O\left(\frac{\sqrt{M} n^{3/2}(f(x^{0}) - f_{\min}(\setX))}{\eps^{3/2}}\right)$ iterations. Assuming that $M\geq 1$, if we change $\eps \to \tilde{\eps}=\min\{n\eps, n\eps/M\}$, we obtain from these inequalities that in 
$O\left(\frac{\sqrt{M} n^{3/2}(f(x^{0}) - f_{\min}(\setX))}{\tilde{\eps}^{3/2}}\right) = O\left(\frac{M^2 (f(x^{0}) - f_{\min}(\setX))}{\eps^{3/2}}\right)$ iterations $\SAHBA$ guarantees
\begin{align*}
& x^{k} > 0, \; \nabla f(x^{k})-\bA^{\ast}y^{k-1} >0 \\
&\norm{\XX^{k}(\nabla f(x^{k})-\bA^{\ast}y^{k-1})}_{\infty} \leq \frac{\tilde{\eps}}{n} \leq \eps, \\
& \nabla^2f(x^k)  + H(x^k) \sqrt{\eps}  \succeq \nabla^2f(x^k)  + H(x^k) \sqrt{\frac{M\tilde{\eps}}{n}}  \succeq 0 \;\; \text{on} \;\; \setL_0.
\end{align*}
In contrast, the second-order algorithm of \cite{HaeLiuYe18} requires an additional assumption that the level set of the objective $f$ is bounded in the $L_{\infty}$-norm, gives a slightly worse guarantee $\nabla f(x^{k})-\bA^{\ast}y^{k-1} > -\eps$, and requires a larger number of iterations $O\left(\frac{ \max\{M,R\}^{7/2}(f(x^{0}) - f_{\min}(\setX))}{\eps^{3/2}}\right)$ ($R$ denoting the $L_{\infty}$ upper bound of the level set corresponding to $x^{0}$).  We also can repeat the same remark as in Section \ref{sec:FO_discussion} that our measure of complementarity $0\leq \inner{s^{k},x^{k}}\leq \eps$ is stronger than $\max_{1\leq i\leq n}\abs{x_{i}^{k}s_{i}^{k}}$ used in \cite{HaeLiuYe18,NeiWr20}. Furthermore, our algorithm is applicable to general cones admitting an efficient barrier setup, rather than only for $\bar{\setK}_{\text{NN}}$. For more general cones we can not use the coupling $H(x)^{-\frac{1}{2}} = \XX$, which was seen to be very helpful in the derivations of the bound \eqref{eq:SO_remarks_7} above. Thus, to deal with general cones, we had to find and exploit suitable properties of the barrier class $\scrH_{\nu}(\setK)$ and develop a new analysis technique that works for general, potentially non-symmetric, cones. Finally, our method does not rely on the trust-region techniques as in \cite{HaeLiuYe18} that may slow down the convergence in practice since the radius of the trust region is no grater than $O(\sqrt{\eps})$ leading to short steps.

\paragraph{Exploiting problem structure.}
We note that in \eqref{eq:SO_per_iter_proof_8} we can clearly observe the benefit of the use of $\nu$-SSB in our algorithm. When $\alpha_k=\frac{1}{2\zeta(x^k,v^k)}$, the per-iteration decrease of the potential is  $\frac{L_k\norm{v^{k}}^{3}_{x^{k}}}{96 (\zeta(x^k,v^k))^2} \geq \frac{ \sqrt{\eps L_k} \norm{v^{k}}_{x^{k}}^2}{96 \sqrt{4\nu} (\zeta(x^k,v^k))^2} $ which may be large if $\zeta(x^k,v^k)=\sigma_{x^k}(-v^k) \ll \norm{v^{k}}_{x^{k}}$.

\paragraph{Dependence on parameters.}
Next, we discuss more explicitly, how the algorithm and complexity bounds depend on the parameter $\mu$. The first observation is that from \eqref{eq:SO_eps_KKT_proof_2}, to guarantee that $s^{k} \in \setK^{\ast}$, we need the stopping criterion to be $\norm{v^{k-1}}_{x^{k-1}} < \Delta_{k-1}= \sqrt{\mu/L_{k-1}}$, which by \eqref{eq:SO_eps_KKT_proof_3} leads to the error $4 \mu \nu$ in the complementarity conditions and by \eqref{eq:SO_eps_KKT_proof_4} leads to the error $\sqrt{\mu/\bar{M}}$ in the second-order condition. From the analysis following equation \eqref{eq:SO_per_iter_proof_8}, we have that  
\[
K\frac{\mu^{3/2}}{24  \sqrt{\bar{M}}} %= K \min\left\{\frac{\mu}{4},\frac{\mu^{2}}{4 \ (\bar{M}+\mu)}\right\}
\leq f(x^{0})-f_{\min}(\setX)+\mu \nu.
\]
Whence, recalling that $\bar{M}=\max\{M,M_0\}$,
\[
K \leq 24(f(x^{0}) - f_{\min}(\setX)+ \mu \nu) \cdot \frac{\sqrt{2\max\{M,M_0\}}}{\mu^{3/2}}.%\max\left\{\frac{\nu}{\eps},\frac{\nu^{2}(M+\eps/\nu)}{\eps^{2}}\right\},
\]
Thus, we see that after $O(\mu^{-3/2})$ iterations the algorithm finds a $(4 \mu \nu,\mu/\bar{M})$-KKT point, and  if $\mu \to 0$, we have convergence to a KKT point, but the complexity bound tends to infinity and becomes non-informative. At the same time, as it is seen from \eqref{eq:SO_finder}, when $\mu \to 0$, the algorithm resembles a cubic-regularized Newton method, but with the regularization with the cube of the local norm. We also see from the above explicit expressions in terms of $\mu$ that the design of the algorithm requires careful balance between the desired accuracy of the approximate KKT point expressed mainly by the complementarity conditions, stopping criterion, and complexity. Moreover, the step-size must be selected carefully to ensure the feasibility of the iterates.

\subsection{Anytime convergence via restarting $\SAHBA$}
Similarly to the restarted $\AHBA$ (Algorithm \ref{alg:RestartHBA}), we can obtain anytime convergence envoking a restarted method that uses $\SAHBA$ as an inner procedure. We fix $\eps_{0}>0$ and select the starting point $x_0^{0}$ as a $4\nu$-analytic center of $\setX$ in the sense of eq. \eqref{eq:analytic_center}. In epoch $i\geq 0$ we generate a sequence 
$\{x_{i}^{k}\}_{k=0}^{K_{i}}$  by calling $\SAHBA(\mu_{i},\eps_{i},M_0^{(i)},x^{0}_{i})$ with $\mu_{i}=\frac{\eps_{i}}{4\nu}$ until the stopping condition is reached. We know that this inner procedure terminates after at most $\K_{II}(\eps_{i},x^{0}_{i})$ iterations. Store the values $x^{K_{i}}_{i}$ and $M_{K_{i}}^{(i)}$, and set $x^{i+1}_{0}\equiv x^{K_{i}}_{i}$, as well as $M_{0}^{(i+1)}\equiv M_{K_{i}}^{(i)}/2$. Updating the parameters to $\mu_{i+1}$ and $\eps_{i+1}$, we restart by calling procedure $\SAHBA(\mu_{i+1},\eps_{i+1},M_0^{(i+1)},x^{0}_{i+1})$ anew. This is formalized in Algorithm \ref{alg:RestartSAHBA}. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{algorithm}[h!]
\caption{Restarting $\SAHBA$}
\label{alg:RestartSAHBA}
\SetAlgoLined
\KwData{ $h \in\scrH_{\nu}(\setK)$, $\eps_{0}>0$, $x_0^{0}\in\setX$ -- $4\nu$-analytic center, $M_0^{(0)}\geq 144 \eps_0$.}
\KwResult{Point $\hat{x}_i$, dual variables $\hat{y}_i$, $\hat{s}_i = \nabla f(\hat{x}_i) -\bA^{\ast}\hat{y}_i$.}
%Set $L_{0}^{0} > 0$ -- initial guess for $M$\; 
%$\hat{x}_{0}=x^{0},\hat{M}_{0}=L_{0}$, $\mu_{0}=\frac{\eps_0}{4\nu}$\;
\For{$i=0,1,\ldots$} 
{ Set $\eps_{i}=2^{-i}\eps_{0}$, $\mu_{i}=\frac{\eps_i}{4\nu}$\; 
Obtain $(\hat{x}_{i},\hat{y}_{i},\hat{s}_{i},\hat{M}_{i})$ from $\SAHBA(\mu_{i},\eps_i,M_0^{(i)},x^{0}_{i})$\;
 %
% $(x_{i}^{K_{i}},L_{i}^{K_{i}})$ from $\AHBA(\mu_{i},\eps_i,x^{0}_{i})$\; 
 Set $x_{i+1}^{0}=\hat{x}_{i}$ and $M_0^{(i+1)}=\hat{M}_{i}/2$.
	}
\end{algorithm}
%%%%%%%%%%%%%%%%%	
\begin{theorem}\label{th:ComplexityPathfollowingSAHBA}
Let Assumptions \ref{ass:1}, \ref{ass:barrier}, \ref{ass:2ndorder} hold. 
Then, for any $\eps \in (0,\eps_0)$, Algorithm \ref{alg:RestartSAHBA} finds an $(\eps,\frac{\max\{M,M_0^{(0)}\}\eps}{8\nu})$-2KKT point for problem \eqref{eq:Opt} in the sense of Definition \ref{def:eps_SOKKT} after no more than $I(\eps)\eqdef\lceil \log_{2}(\eps_{0}/\eps)\rceil+1$ restarts and at most 
$
\left\lceil 841(f(x^{0})-f_{\min}(\setX)+\eps_0)\nu^{3/2}\eps^{-3/2}\sqrt{2\max\{M,M_0^{(0)}\}}\right\rceil
$
%841 \frac{1536}{(2\sqrt{2}-1)
 iterations of $\SAHBA$.
\end{theorem}
\begin{proof}
Let us consider a restart $i \geq 0$ and mimic the proof of Theorem \ref{Th:SOAHBA_conv} with the substitution $\eps \to \eps_i$,  $\mu \to \mu_i = \eps_i/(4\nu)$, $M_0 \to M_0^{(i)}=\hat{M}_{i-1}/2$, $\underline{L} = 144  \eps \to \underline{L}_i = 144  \eps_i$, $\bar{M}=\max\{M,M_0\} \to \bar{M}_i=\max\{M,M_0^{(i)}\}$, $x^0 \to x^{0}_{i}=\hat{x}_{i-1}$. Note that $M_0^{(i)} \geq 144 \eps_i=\underline{L}_i$ for $i\geq 0$. We verify this via induction. By construction $M_0^{(0)}\geq 144 \eps_0$. Assume the bound holds for some $i\geq 1$. Then,  $M_0^{(i+1)}=M_{K_{i}}^{(i)}/2=\max\{L_{K_{i}-1}^{(i)}/2,\underline{L}_{i}\}/2\geq 144 \eps_{i}/2=144 \eps_{i+1}$, where we have used the induction hypothesis and the definition of the sequence $\eps_{i}$.\\
Let $K_i$ be the last iteration of $\SAHBA(\mu_{i},\eps_i,M_0^{(i)},x^{0}_{i})$ meaning that the stopping criterion does not hold at the inner iterations $k=0,\ldots,K_i-1$. From the analysis following equation \eqref{eq:SO_per_iter_proof_7}, we obtain
\begin{equation} \label{eq:SO_PF_proof_1} 
K_i \frac{\eps_i^{3/2}}{192 \nu^{3/2} \sqrt{2\bar{M}_i}} \leq F_{\mu_i}(x^{0}_i)-F_{\mu_i}(x^{K_i}_i).
\end{equation}
Using that $\mu_i$ is a decreasing sequence and $x_{0}^{0}$ is a $4\nu$-analytic center, we see
\begin{align}
F_{\mu_{i+1}}(x^{0}_{i+1})&=F_{\mu_{i+1}}(x^{K_i}_{i})=f(x^{K_i}_{i}) + \mu_{i+1} h(x^{K_i}_{i})=F_{\mu_{i}}(x^{K_i}_{i}) + (\mu_{i+1} - \mu_{i}) h(x^{K_i}_{i}) \notag \\
&\stackrel{\eqref{eq:analytic_center}}{\leq} F_{\mu_{i}}(x^{K_i}_{i}) + (\mu_{i+1} - \mu_{i}) (h(x_0^0)-4\nu)\notag \\
& \stackrel{\eqref{eq:SO_PF_proof_1}}{\leq}F_{\mu_i}(x^{0}_i)-K_i \frac{\eps_i^{3/2}}{192 \nu^{3/2} \sqrt{2\bar{M}_i} }  + (\mu_{i+1} - \mu_{i}) (h(x_0^0)-4\nu).
\label{eq:SO_PF_proof_2} 
\end{align}
Let $I=I(\eps)=\left\lceil \log_2 \frac{\eps_0}{\eps} \right\rceil+1$. By Theorem \ref{Th:SOAHBA_conv} applied to the restart $I-1$, we see that $\SAHBA(\mu_{I-1},\eps_{I-1},M_0^{(I-1)},x^{0}_{I-1})$ outputs an $(\eps_{I-1},\frac{\bar{M}_{I-1}\eps_{I-1}}{8\nu})$-2KKT point for problem \eqref{eq:Opt} in the sense of Definition \ref{def:eps_SOKKT}. Since $\eps_{I-1}=\eps$ and, for all $i\geq 1$,
\begin{align}
\bar{M}_i &= \max\{M,M_0^{(i)}\} = \max\{M,\hat{M}_{i-1}/2\} = \max\{M,M_{K_{i-1}}^{(i-1)}/2\} \notag\\
&= \max\{M,\max\{L_{K_{i-1}-1}^{(i-1)}/2,\underline{L}_{i-1}\}/2\} \notag\\
&\leq  \max\{M,\max\{\bar{M}_{i-1},M_0^{(i-1)}\}/2\} \leq  \max\{M,\bar{M}_{i-1}\} \notag\\
&\leq ... \leq \max\{M,\bar{M}_{0}\} \leq  \max\{M,M_0^{(0)}\}.\label{eq:SO_PF_proof_3} 
\end{align}
it follows that actually we generate an $(\eps,\frac{\max\{M,M_{0}^{(0)}\}\eps}{8\nu})$-2KKT point. Summing inequalities \eqref{eq:SO_PF_proof_2} for all the performed restarts $i=0,...,I-1$ and rearranging the terms, we obtain
\begin{align*}
\sum_{i=0}^{I-1} K_i \frac{\eps_i^{3/2}}{192 \nu^{3/2} \sqrt{2\bar{M}_i} }  & \leq F_{\mu_0}(x^{0}_0) - F_{\mu_{I}}(x^{0}_{I}) + (\mu_{I} - \mu_{0}) (h(x_0^0)-4\nu) \notag \\
&=f(x^{0}_0) + \mu_0 h(x^{0}_0)- f(x^{0}_{I}) - \mu_{I}h(x^{0}_{I}) + (\mu_{I} - \mu_{0}) (h(x_0^0)-4\nu) \notag \\
& \stackrel{\eqref{eq:analytic_center}}{\leq} f(x^{0}_0) - f_{\min}(\setX) + \mu_0 h(x^{0}_0) - \mu_{I}h(x^{0}_{0}) + 4\mu_{I} \nu + (\mu_{I} - \mu_{0}) (h(x_0^0)-4\nu) \notag \\ 
& \leq f(x^{0}_0) - f_{\min}(\setX) + 4\mu_0 \nu = f(x^{0}_0) - f_{\min}(\setX) + \eps_0,
\end{align*}
where in the last steps we have used the coupling $\mu_{0}=\eps_{0}/\nu$. From this inequality, using \eqref{eq:SO_PF_proof_3}, we obtain
\begin{align}
K_i \leq  (f(x^{0}) - f_{\min}(\setX)+  \eps_0) \cdot \frac{192\nu^{3/2}\sqrt{2\bar{M}_i}}{\eps_i^{3/2}} \leq \frac{C}{\eps_i^{3/2}},
\label{eq:SO_PF_proof_4} 
\end{align}
where $C \equiv 192(f(x^{0})-f_{\min}(\setX)+\eps_0)\nu^{3/2}\sqrt{2\max\{M,M_0^{(0)}\}}$. Finally, we obtain that the total number of iterations of procedures $\SAHBA(\mu_{i},\eps_{i},M_0^{(i)},x_{i}^{0}),0\leq i \leq I-1$, to reach accuracy $\eps$ is at most
\begin{align*}
\sum_{i=0}^{I-1}K_{i}&\leq \sum_{i=0}^{I-1} \frac{C}{\eps_i^{3/2}} \leq \frac{C}{\eps_0^{3/2}} \sum_{i=0}^{I-1} (2^i)^{3/2} \\
&\leq \frac{C}{\eps_0^{3/2}} \cdot \frac{2^{3/2\cdot(2+\log_2(\frac{\eps_0}{\eps}))}-1}{2^{3/2}-1}\leq \frac{8C}{(\sqrt{8}-1)\eps^{3/2}}\\
&<\frac{841(f(x^{0})-f_{\min}(\setX)+\eps_0)\nu^{3/2}\sqrt{2\max\{M,M_0^{(0)}\}}}{\eps^{3/2}}.
\end{align*} 
\end{proof}
