\section{Related Work and Discussion}
In this paper, we focus on improving the robustness of machine learning models to subpopulation shifts and domain shifts. Here, we discuss related approaches from the following three categories:

\textbf{Learning Invariant Representations.} Motivated by unsupervised domain adaptation~\citep{ben2010theory,ganin2016domain}, the first category of works learns invariant representations by aligning representations across domains. % across domains. 
The major research line of this category aims to eliminate the domain dependency by minimizing the divergence of feature distributions with different distance metrics, e.g., maximum mean discrepancy~\citep{tzeng2014deep,long2015learning}, an adversarial loss~\citep{ganin2016domain,li2018domain}, Wassertein distance~\citep{zhou2020domain}. Follow-up works applied data augmentation to (1) generate more domains and enhance the consistency of representations during training~\citep{yue2019domain,zhou2020deep,xu2020adversarial,yan2020improve,shu2021open,wang2020heterogeneous,yao2021meta} or (2) generate new domains in an adversarial way to imitate the challenging domains without using training domain information~\citep{zhao2020maximum,qiao2020learning,volpi2018generalizing}. Unlike these latter methods, LISA instead focuses on learning invariant predictors without restricting the internal representations, leading to stronger empirical performance.


\textbf{Learning Invariant Predictors.} Beyond using domain alignment to learning invariant representations, recent work aims to further enhance the correlations between the invariant representations and the labels~\citep{koyama2020out}, leading to invariant predictors. \yao{Representatively, motivated by casual inference, invariant risk minimization (IRM)~\citep{arjovsky2019invariant} and its variants~\citep{guo2021out,khezeli2021invariance,ahuja2021invariance} aim to find a predictor that performs well across all domains through regularizations. Other follow-up works leverage regularizers to penalize the variance of risks across all domains~\citep{krueger2021out}, to align the gradient across domains~\citep{koyama2020out}, to smooth the cross-domain interpolation paths~\citep{chuang2021fair}, or to involve game-theoretic invariant rationalization criterion~\citep{chang2020invariant}. Instead of using regularizers, LISA instead learns domain-invariant predictors via data interpolation.}

\yao{\textbf{Group Robustness.} The last category of methods combating spurious correlations and are particularly suitable for subpopulation shifts. These approaches include directly optimizing the worst-group performance with Distributionally Robust Optimization~\citep{sagawa2019distributionally,zhang2020coping,zhou2021examining}, generating samples around the minority groups~\citep{goel2020model}, and balancing the majority and minority groups via reweighting~\citep{sagawa2020investigation} or regularizing~\citep{cao2019learning,cao2020heteroskedastic}. A few recent approaches in this category target on subpopulation shifts without annotated group labels~\citep{nam2020learning,liu2021just,zhang2021correct,creager2021environment,lee2022diversify}. LISA proposes a more general strategy that is suitable for both domain shifts and subpopulation shifts.}
