\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{courier}
\usepackage{multirow}
\usepackage{color}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage[noend]{algorithmic} 
% \usepackage[noend,ruled]{algpseudocode}
\usepackage{amsmath,nccmath}
\usepackage{amsmath,amsfonts,amssymb,bbm, bm}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}

\usepackage{bbm}
\usepackage[accepted]{icml2022}

\setlength{\abovecaptionskip}{1pt}
\setlength{\belowcaptionskip}{1pt}
\setlength{\textfloatsep}{5pt plus 1pt minus 1pt}
\setlength{\intextsep}{1pt plus 1pt minus 1pt}

\icmltitlerunning{Improving Out-of-Distribution Robustness via Selective Augmentation}

\usepackage{hyperref}
\usepackage{url}
\newcommand{\linjun}[1]{\textcolor{cyan}{(Linjun: #1)}}

\newcommand{\james}[1]{\textcolor{blue}{(James: #1)}}
\newcommand{\chelsea}[1]{\textcolor{purple}{(Chelsea: #1)}}

\newcommand{\yao}[1]{\textcolor{black}{#1}}

\def\gam{\gamma}
\def\Sig{\Sigma}
\def\eps{\epsilon}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{condition}{Condition}
\newtheorem{lemma}{Lemma}


% Improving Robustness via Selective Augmentation: A Simple Approach for Distribution Shifts
%%CF.9.23: It would be good to brainstorm on titles a bit. I think that this title is not great because augmentation makes you think of things like random crops, and "learning invariant representations via augmentation" could literally just mean doing data augmentation. In this sense, I also think that the current method name is not great either. It's more that the paper is doing selective augmentation or selective mix-up. I might also consider trying to get "robust" or "robustness" into the title


% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\def\R{\mathbbm{R}}
\def\P{\mathbbm{P}}
\def\E{\mathbbm{E}}
\def\lam{\lambda}
\def\Sig{\Sigma}
\def\tx{\tilde{x}}
\def\ty{\tilde{y}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\twocolumn[
\icmltitle{Improving Out-of-Distribution Robustness via Selective Augmentation}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Huaxiu Yao}{equal,stanford}
\icmlauthor{Yu Wang}{equal,ucsd}
\icmlauthor{Sai Li}{ruc}
\icmlauthor{Linjun Zhang}{rutgers}
\icmlauthor{Weixin Liang}{stanford}
\\
\icmlauthor{James Zou}{stanford}
\icmlauthor{Chelsea Finn}{stanford}
\end{icmlauthorlist}

\icmlaffiliation{ucsd}{University of California San Diego, CA, USA}
\icmlaffiliation{ruc}{Renmin University of China, Beijing, China}
\icmlaffiliation{rutgers}{Rutgers University, NJ, USA}
\icmlaffiliation{stanford}{Stanford University, CA, USA}

\icmlcorrespondingauthor{Huaxiu Yao}{huaxiu@cs.stanford.edu}
\icmlcorrespondingauthor{Sai Li}{saili@ruc.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in

]

\printAffiliationsAndNotice{\icmlEqualContribution. This work was done when Yu Wang was mentored by Huaxiu Yao remotely.}

\input{abstract}
\input{introduction}
\input{preliminary}
\input{method}
\input{experiment}
\input{analysis}
\input{relatedwork}
\input{conclusion}
\section*{Acknowledgement}
We thank Pang Wei Koh for the many insightful discussions. This research was funded in part by JPMorgan Chase \& Co. Any views or opinions expressed herein are
solely those of the authors listed, and may differ from the views and opinions expressed by JPMorgan Chase
\& Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities
LLC. This material should not be construed as an individual recommendation for any particular client and is
not intended as a recommendation of particular securities, financial instruments or strategies for a particular
client. This material does not constitute a solicitation or offer in any jurisdiction. The research was also supported by Apple and Juniper Networks. The research of Linjun Zhang is partially supported by  NSF DMS-2015378.


\bibliography{ref}
\bibliographystyle{icml2022}
\onecolumn
\input{appendix_experiment}
\input{appendix}



\end{document}
