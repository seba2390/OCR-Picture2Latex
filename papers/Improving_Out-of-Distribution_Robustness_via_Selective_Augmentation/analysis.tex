\section{Theoretical Analysis}
\label{sec:theory}
In this section, we provide some theoretical understandings that explain several of the empirical phenomena from the previous experiments and theoretically compare the worst-group errors of three methods: the proposed LISA, ERM, and vanilla mixup. Specifically, we consider a Gaussian mixture model with subpopulation and domain shifts, which has been widely adopted in theory to shed light upon complex machine learning phenomenon such as in \cite{montanari2019generalization,zhang2021and, liu2021self}. We   note here that despite the popularity of mixup in practice, the theoretical analysis of how mixup (w/ or w/o the selective augmentation strategies) affects the misclassification error is still largely unexplored in the literature even in the simple models. As discussed in Section~\ref{sec:prelim}, here, we define \begin{small}$y\in\{0,1\}$\end{small} as the label, and \begin{small}$d\in\{R,G\}$\end{small} as the domain information. For \begin{small}$y\in\{0,1\}$\end{small} and \begin{small}$d\in\{R,G\}$\end{small}, we consider the following model:
\begin{equation}
\small
\label{m1}
    x_i|y_i=y,d_i=d\sim N(\mu^{(y,d)},\Sigma^{(d)}), i=1,\dots,n^{(y,d)},
\end{equation}
where \begin{small}$\mu^{(y,d)}\in\R^p$\end{small} is the conditional mean vector and \begin{small}$\Sigma^{(d)}\in\R^{p\times p}$\end{small} is the covariance matrix. Let \begin{small}$n=\sum_{y\in\{0,1\},d\in\{R,G\}}n^{(y,d)}$\end{small}. 
%Notice that given $d_i=d$, $(x_i,y_i)$ follows a two-component Gaussian mixture model. 
Let \begin{small}$\pi^{(y,d)}=\mathbbm{P}(y_i=y,d_i=d)$\end{small}, \begin{small}$\pi^{(y)}=\mathbbm{P}(y_i=y)$\end{small}, and \begin{small}$\pi^{(d)}=\mathbbm{P}(d_i=d)$\end{small}.

To account for the spurious correlation brought by domains, we consider \begin{small}$\mu^{(y,R)}\neq \mu^{(y,G)}$\end{small} in general for \begin{small}$y\in\{0,1\}$\end{small} and the imbalanced case where \begin{small}$\pi^{(0,R)}, \pi^{(1,G)}<1/4$\end{small}. Moreover, we assume there exists some invariance across different domains. Specifically, we assume 
\begin{equation*}
   \resizebox{\hsize}{!}{$\mu^{(1,R)}-\mu^{(0,R)}= \mu^{(1,G)}-\mu^{(0,G)}:=\Delta~~\text{and}~~\Sigma^{(G)}=\Sigma^{(R)}:=\Sigma$}.
\end{equation*}
According to Fisher's linear discriminant analysis \citep{anderson1962introduction,tony2019high,cai2021convex}, the optimal classification rule is linear with slope \begin{small}$\Sig^{-1}\Delta$\end{small}. The assumption above implies that \begin{small}$(\Sig^{-1}\Delta)^\top x$\end{small} is the (unknown) invariant prediction rule for model \eqref{m1}. 

Suppose we use some method \begin{small}$A$\end{small} and obtain a linear classifier \begin{small}$x^Tb+b_0>0$\end{small} from a training data, we will apply it to a test data and compute the worst-group misclassification error, where the mis-classification error for domain $d$ and class $y$ is \begin{small}$E^{(y,d)}(b,b_0):=  \mathbbm{P}(\mathbbm{1}(x_i^Tb+b_0>\frac{1}{2})\neq y|d_i=d,y_i=y)$\end{small}, 
and we denote the worst-group error with the method $A$ as
\begin{equation}
\nonumber
\small
E^{(wst)}_A= \max_{d\in\{R,G\},y\in\{0,1\}}E^{(y,d)}(b_A,b_{0,A}),
\end{equation}
where \begin{small}$b_A$\end{small} and \begin{small}$b_{0,A}$\end{small} are the slope and intercept based on the method \begin{small}$A$\end{small}. Specifically, \begin{small}$A=\textup{ERM}$\end{small} denotes the ERM method (by minimizing the sum of squares loss on the training data altogether), \begin{small}$A=\textup{mix}$\end{small} denotes the vanilla mixup method (without any selective augmentation strategy), and \begin{small}$A=\textup{LISA}$\end{small} denotes the mixup strategy for LISA. We also denote its finite sample version by \begin{small}$\hat E_A^{(wst)}$\end{small}.


Let \begin{small}$\widetilde{\Delta}=\E[x_i|y_i=1]-\E[x_i|y_i=0]$\end{small} denote the marginal difference and \begin{small}$\xi=\frac{\Delta^T\Sig^{-1}\widetilde{\Delta}}{\|\Delta\|_{\Sig}\|\widetilde{\Delta}\|_{\Sig}}$\end{small} denote the correlation operator between the domain-specific difference \begin{small}$\Delta$\end{small} and the marginal difference \begin{small}$\widetilde{\Delta}$\end{small} with respect to \begin{small}$\Sig$\end{small}. We see that smaller \begin{small}$\xi$\end{small} indicates larger discrepancy between the marginal difference and the domain-specific difference and therefore implies stronger spurious correlation between the domains and labels. We present the following theorem showing that our proposed LISA algorithm outperforms the ERM and vanilla mixup in the subpopulation shifts setting. 

\begin{theorem}[Error comparison with subpopulation shifts]
\label{thm1}

Consider $n$ independent samples generated from model (\ref{m1}),  \begin{small}$\pi^{(R)}=\pi^{(1)}=1/2$\end{small}, \begin{small}$\pi^{(0,R)}=\pi^{(1,G)}=\alpha<1/4$\end{small}, \begin{small}$\max_{y,d}\|\mu^{(y,d)}\|_2\leq C$\end{small}, and \begin{small}$\Sig$\end{small} is positive definite. Suppose \begin{small}$(\xi,\alpha)$\end{small} satisfies that \begin{small}$\xi < \min\{\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},\frac{\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}}\}-C\alpha$\end{small}
 for some large enough constant \begin{small}$C$\end{small} and \begin{small}$\E[\lam_i^2]/\max\{var(\lam_i),1/4\}\geq \|\widetilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}$\end{small}.
Then for any \begin{small}$p_{sel}\in[0,1]$\end{small},
\begin{small}
$$\widehat{E}_{\textup{LISA}}^{(wst)}< \min\{\widehat{E}_{\textup{ERM}}^{(wst)},\widehat{E}_{\textup{mix}}^{(wst)}\}+O_P\left(\frac{p\log n}{n}+\frac{p}{\alpha n}\right).$$
\end{small}
\end{theorem}
In Theorem \ref{thm1}, $\lambda_i$ is the random mixup coefficient for the $i$-th sample. If $\lambda_i=\lambda$ are the same for all the samples in a mini-batch, the results still hold. 
Theorem \ref{thm1} implies that when \begin{small}$\xi$\end{small} is small (indicating that the domain has strong spurious correlation with the label) and \begin{small}$p=o(\alpha n)$\end{small}, the worst-group classification errors of LISA are asymptotically smaller than that of ERM and vanilla mixup. {\color{black} In fact, our analysis shows that LISA yields a classification rule closer to the invariant classification rules by leveraging the domain information.}

In the next theorem, we present the mis-classification error comparisons with domain shifts. That is, consider samples from a new unseen domain: 
\begin{equation}
\small
    x_i^{(0,*)}\sim N(\mu^{(0,*)},\Sigma),~~x_i^{(1,*)}\sim N(\mu^{(1,*)},\Sigma).
\end{equation}
% \[
  
% \]
% \end{small}
%Assume that $\mu^{(0,*)}-\mu^{(1,*)}=\Delta$.
Let \begin{small}$\widetilde{\Delta}^*=2(\mu^{(0,*)}-\E[x_i])$\end{small}, where \begin{small}$\E[x_i]$\end{small} is the mean of the training distribution, and assume \begin{small}$\mu^{(1,*)}-\mu^{(0,*)}=\Delta$\end{small}. Let \begin{small}$\xi^*=\frac{\widetilde{\Delta}^T\Sig^{-1}\widetilde{\Delta}^*}{\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}}$\end{small} and \begin{small}$\gamma=\frac{\Delta^T\Sig^{-1}\widetilde{\Delta}^*}{\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}}$\end{small} denote the correlation for \begin{small}$(\widetilde{\Delta}^*,\widetilde{\Delta})$\end{small} and for \begin{small}$(\widetilde{\Delta}^*,\Delta)$\end{small}, respectively, with respect to \begin{small}$\Sig^{-1}$\end{small}. 
Let \begin{small}$E^{(wst*)}_A= \max_{y\in\{0,1\}}E^{(y,*)}(b_A,b_{0,A})$\end{small} and its sample version be \begin{small}$\hat E^{(wst*)}_A$\end{small}.

\begin{theorem}[Error comparison with domain shifts]
\label{thm3}
Suppose $n$ samples are independently generated from model (\ref{m1}), \begin{small}$\pi^{(R)}=\pi^{(1)}=1/2,\pi^{(0,R)}=\pi^{(1,G)}=\alpha<1/4$\end{small},  \begin{small}$\max_{y,d}\|\mu^{(y,d)}\|_2\leq C$\end{small} and \begin{small}$\Sig$\end{small} is positive definite.
Suppose that \begin{small}$(\xi,\xi^*,\gamma)$\end{small} satisfy that \begin{small}$0\leq \xi^*\leq \gamma\xi$\end{small} and \begin{small}$\xi < \min\{\frac{\gamma}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},\frac{\|\Delta\|_{\Sig}}{\|\tilde{\Delta}\|_{\Sig}}\}-C\alpha$\end{small}
 for some large enough constant $C$ and \begin{small}$\E[\lam_i^2]/\max\{var(\lam_i),1/4\}\geq \|\widetilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}$\end{small}.
Then for any \begin{small}$p_{sel}\in[0,1]$\end{small},
\begin{small}$$\widehat{E}_{\textup{LISA}}^{(wst*)}< \min\{\widehat{E}_{\textup{ERM}}^{(wst*)},\widehat{E}_{\textup{mix}}^{(wst*)}\}+O_P\left(\frac{p\log n}{n}+\frac{p}{\alpha n}\right).$$
\end{small}
\end{theorem}

Similar to Theorem~\ref{thm1}, this result shows that when domain has strong spurious correlation with the label (corresponding to small $\xi$), such a spurious correlation leads to the downgraded performance of ERM and vanilla mixup, while our proposed LISA method is able to mitigate such an issue by selective data interpolation. Proofs of Theorem~\ref{thm1} and Theorem~\ref{thm3} are provided in Appendix~\ref{sec:app_proof}.