\section{Experiments}
\label{sec:experiments}
\begin{table*}[h]
\small
\caption{Dataset Statistics for Subpopulation Shifts. All datasets are binary classification tasks and we use the worst group accuracy as the evaluation metric.}
\label{tab:subpopulation_data}
\begin{center}
\begin{tabular}{l|ccc}
\toprule
Datasets  & Domains  & Model Architecture & Class Information \\\midrule
CMNIST & 2 digit colors  & ResNet-50 & digit (0,1,2,3,4) v.s. (5,6,7,8,9)\\
Waterbirds & 2 backgrounds  & ResNet-50 & waterbirds v.s. landbirds\\
CelebA & 2 hair colors & ResNet-50 & man v.s. women\\
CivilComments & 8 demographic
identities & DistilBERT-uncased & toxic v.s. non-toxic \\\bottomrule
\end{tabular}
\end{center}
\vspace{-1.5em}
\end{table*}
In this section, we conduct comprehensive experiments to evaluate the effectiveness of LISA. Specifically, we aim to answer the following questions: \textbf{Q1}: Compared to prior methods, can LISA improve robustness to subpopulation shifts and domain shifts (Section~\ref{sec:exp_main_sub} and Section~\ref{sec:exp_main_domain})? \textbf{Q2}: Which aspects of LISA are the most important for improving robustness (Section~\ref{sec:exp_compare_augmentation})? \textbf{Q3}: Does LISA successfully produce more invariant predictors (Section~\ref{sec:invariance})? \textbf{Q4}: How does LISA perform with varying degrees of distribution shifts (Section~\ref{sec:exp_degree})? 

To answer Q1, we compare to ERM, IRM~\citep{arjovsky2019invariant}, IB-IRM~\citep{ahuja2021invariance}, V-REx~\citep{krueger2021out}, CORAL~\citep{li2018domain}, DRNN~\citep{ganin2015unsupervised}, GroupDRO~\citep{sagawa2019distributionally}, DomainMix~\citep{xu2020adversarial}, and Fish~\citep{shi2021gradient}. Upweighting (UW) is particularly suitable for subpopulation shifts, so we also use it for comparison. We adopt the same model architectures for all approaches. The strategy selection probability $p_{sel}$ is selected via cross-validation.

\subsection{Evaluating Robustness to Subpopulation Shifts}
\label{sec:exp_main_sub}
\textbf{Evaluation Protocol.} In subpopulation shifts, we evaluate the performance on four binary classification datasets, including Colored MNIST (CMNIST), Waterbirds~\citep{sagawa2019distributionally}, CelebA~\citep{liu2015faceattributes}, and Civilcomments~\citep{borkan2019nuanced}. We detail the data descriptions of subpopulation shifts in Appendix~\ref{sec:app_sub_data} and report the detailed data statistics in Table~\ref{tab:subpopulation_data}, covering domain information, model architecture, and class information. Following~\citet{sagawa2019distributionally}, in subpopulation shifts, we use the worst-group accuracy to evaluate the performance of all approaches. In these datasets, the domain information is highly spurious correlated with the label information. For example, as suggested in Figure~\ref{fig:method_illustration}, 80\% images in the CMNIST dataset have the same color in each specific class, i.e., green color for label [1, 0] and red color for label [0, 1]. 

In CMNIST, Waterbirds, and CelebA, we find that $p_{sel}=0.5$ works well for choosing selective augmentation strategies, while in CivilComments, we set $p_{sel}$ as $1.0$ . This is not surprising because it might be more beneficial to use intra-label LISA more often to eliminate domain effects when there are more domains, i.e., eight domains in CivilComments v.s. two domains in others. The rest hyperparameter settings and training details are listed in Appendix~\ref{sec:app_sub_training}.


\begin{table*}[h]
\small
\caption{Results of subpopulation shifts. Here, we show the average and worst group accuracy. We repeat the experiments three times and put full results with standard deviation in Table~\ref{tab:subpopulation_main_full}.}
\label{tab:subpopulation_main}
\begin{center}
\begin{tabular}{l|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c|}{CMNIST} & \multicolumn{2}{c|}{Waterbirds} & \multicolumn{2}{c|}{CelebA} & \multicolumn{2}{c}{CivilComments} \\
& Avg. & Worst  & Avg. & Worst  & Avg. & Worst & Avg. & Worst \\\midrule
ERM & 27.8\% & 0.0\% & 97.0\% & 63.7\% & 94.9\% & 47.8\% & 92.2\% & 56.0\% \\
UW  & 72.2\% & 66.0\% &  95.1\% & 88.0\% & 92.9\% & 83.3\% & 89.8\% & 69.2\% \\
IRM & 72.1\% & 70.3\% & 87.5\% & 75.6\% & 94.0\% & 77.8\% & 88.8\% & 66.3\% \\
IB-IRM & 72.2\% & 70.7\% & 88.5\% & 76.5\% & 93.6\% & 85.0\% & 89.1\% & 65.3\%\\
V-REx & 71.7\% & 70.2\% & 88.0\% & 73.6\% & 92.2\% & 86.7\% &  90.2\% & 64.9\% \\
CORAL & 71.8\% & 69.5\% & 90.3\% & 79.8\% & 93.8\% & 76.9\% & 88.7\% & 65.6\% \\
GroupDRO & 72.3\% & 68.6\% & 91.8\% & \textbf{90.6\%} & 92.1\% & 87.2\% & 89.9\% & 70.0\%  \\
DomainMix & 51.4\% & 48.0\% & 76.4\% & 53.0\% & 93.4\% & 65.6\% & 90.9\% & 63.6\%\\
Fish & 46.9\% & 35.6\% & 85.6\% & 64.0\% & 93.1\% & 61.2\% &  89.8\% & 71.1\% \\
\midrule
\textbf{LISA (ours)} & 74.0\% & \textbf{73.3\%} & 91.8\% & 89.2\% & 92.4\% & \textbf{89.3\%} & 89.2\% &  \textbf{72.6\%} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2em}
\end{table*}

\begin{table*}[h]
\small
\caption{Main domain shifts results. LISA outperforms prior methods on all five datasets. Following the instructions of~\citet{koh2021wilds}, we report the performance of Camelyon17 over 10 different seeds and the results of other datasets are obtained over 3 different seeds.}
\label{tab:res_domain}
\begin{center}
\begin{tabular}{l|cccccc}
\toprule
\multirow{2}{*}{}  & Camelyon17 & FMoW & RxRx1 & Amazon &  MetaShift\\\cmidrule{2-6}
& Avg. Acc. & Worst Acc. & Avg. Acc. & 10-th Per. Acc. & Worst Acc.\\\midrule
ERM & 70.3 $\pm$ 6.4\% & 32.3 $\pm$ 1.25\%  & 29.9 $\pm$ 0.4\% & 53.8 $\pm$ 0.8\% & 52.1 $\pm$ 0.4\% \\
IRM  & 64.2 $\pm$ 8.1\% & 30.0 $\pm$ 1.37\% & 8.2 $\pm$ 1.1\% &  52.4 $\pm$ 0.8\% & 51.8 $\pm$ 0.8\% \\
IB-IRM & 68.9 $\pm$ 6.1\% & 28.4 $\pm$ 0.90\% & 6.4 $\pm$ 0.6\% & 53.8 $\pm$ 0.7\% & 52.3 $\pm$ 1.0\% \\
V-REx & 71.5 $\pm$ 8.3\% & 27.2 $\pm$ 0.78\% & 7.5 $\pm$ 0.8\% & 53.3 $\pm$ 0.0\% & 51.6 $\pm$ 1.8\% \\
CORAL & 59.5 $\pm$ 7.7\% & 31.7 $\pm$ 1.24\% & 28.4 $\pm$ 0.3\% & 52.9 $\pm$ 0.8\% & 47.6 $\pm$ 1.9\%\\
GroupDRO & 68.4 $\pm$ 7.3\% & 30.8 $\pm$ 0.81\% & 23.0 $\pm$ 0.3\% & 53.3 $\pm$ 0.0\% & 51.9 $\pm$ 0.7\% \\
DomainMix & 69.7 $\pm$ 5.5\% & 34.2 $\pm$ 0.76\% & 30.8 $\pm$ 0.4\% & 53.3 $\pm$ 0.0\% & 51.3 $\pm$ 0.5\% \\
Fish & 74.7 $\pm$ 7.1\% & 34.6 $\pm$ 0.18\% & 10.1 $\pm$ 1.5\%  & 53.3 $\pm$ 0.0\% & 49.2 $\pm$ 2.1\% \\\midrule
\textbf{LISA (ours)} & \textbf{77.1 $\pm$ 6.5\%} & \textbf{35.5 $\pm$ 0.65\%} & \textbf{31.9 $\pm$ 0.8\%} & \textbf{54.7 $\pm$ 0.0\%} & \textbf{54.2 $\pm$ 0.7\%} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2em}
\end{table*}

\textbf{Results.} 
In Table~\ref{tab:subpopulation_main}, we report the overall performance of LISA and other methods. According to Table~\ref{tab:subpopulation_main}, we \yao{observe that the performance of approaches that learn invariant predictors with explicit regularizers (e.g., IRM, IB-IRM, V-REx) is not consistent across datasets. For example, IRM and V-REx outperform UW on CMNIST, but they fail to achieve better performance than UW on Waterbirds. The results corroborate our hypothesis that designing widely effective regularizers is challenging, and that inappropriate regularizers may even hurt the performance. LISA instead consistently outperforms other invariant learning methods (e.g., IRM, IB-IRM, V-REx, CORAL, DomainMix, Fish) in all datasets. LISA further shows the best performance on CMNIST, CelebA, and CivilComments. In Waterbirds, it is slightly worse than GroupDRO, but the performance is comparable. These results demonstrate the effectiveness of LISA in improving robustness to subpopulation shifts.}

\textbf{Effects of Intra-label and Intra-domain LISA.} For CMNIST, Waterbirds and CelebA, both intra-label and intra-domain LISA are used (i.e., $p_{sel}=0.5$), we illustrate the separate results in Figure~\ref{fig:intraonly} and observe that both variants contribute to the final performance. In addition, intra-domain LISA performs slightly better than intra-label LISA, corroborating our assumption that intra-domain LISA benefits more when domain information is highly spuriously correlated with the label (see the discussion of the strength of spurious correlation in Appendix~\ref{sec:app_spurious_strength}). 

\begin{figure}[h]
    \centering
    \small
    \includegraphics[width=0.32\textwidth]{figures/variant_effects.pdf}
    \vspace{-0.8em}
    \caption{Effects of intra-label and intra-domain LISA in CMNIST, Waterbirds and CelebA. The experiments are repeated three times with different seeds.}
    \label{fig:intraonly}
\end{figure}

\subsection{Evaluating Robustness to Domain Shifts}
\label{sec:exp_main_domain}
\begin{table*}[h]
\caption{Dataset Statistics for Domain Shifts.}
\vspace{0.5em}
\small
\label{tab:domian_data}
\begin{center}
\begin{tabular}{l|cccc }
\toprule
Datasets  & Domains & Metric & Base Model & Num. of classes \\\midrule
Camelyon17 & 5 hospitals & Avg. Acc. & DenseNet-121 & 2\\
FMoW & 16 years x 5 regions & Worst-group Acc. & DenseNet-121 & 62\\
RxRx1 & 51 experimental batches & Avg. Acc. & ResNet-50 & 1,139\\
Amazon & 7,676 reviewers & 10th Percentile Acc. & DistilBERT-uncased & 5\\
MetaShift & 4 backgrounds & Worst-group Acc. & ResNet-50 & 2 \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-2em}
\end{table*}
\textbf{Experimental Setup.} In domain shifts, we study five datasets. Four of them (Camelyon17, FMoW, RxRx1, and Amazon) are selected from WILDS~\citep{koh2021wilds}, covering natural distribution shifts across diverse domains (e.g., health, language, and vision). Besides the WILDS data, we also apply LISA on the MetaShift datasets~\citep{metadataset}, constructed using the real-world images and natural heterogeneity of Visual Genome~\citep{krishnavisualgenome}. We summarize these datasets in Table~\ref{tab:domian_data}, including domain information, evaluation metric, model architecture, and the number of classes. Detailed dataset descriptions and other training details are discussed in Appendix~\ref{sec:app_domain_data} and~\ref{sec:app_domain_training}, respectively.

\yao{The strategy selection probability $p_{sel}$ is set as $1.0$ for these domain shifts datasets, i.e., only intra-label LISA is used. Additionally, we only interpolate samples with the same labels without considering the domain information in Camelyon17, FMoW, and RxRx1, which empirically leads to the best performance. One potential reason is that the spurious correlations between labels and domains are not very strong in datasets with natural domain shifts under the existing domain partitions. Here, to evaluate the strength of spurious correlation, we adopt Cramér's V~\citep{cramer2016mathematical} (see the detailed definition in Appendix~\ref{sec:app_spurious_strength}) to measure the association between the domain set $\mathcal{D}$ and the label set $\mathcal{Y}$, where the results are reported in Table~\ref{tab:spurious_strength} of Appendix~\ref{sec:app_spurious_strength}. The Cramér's V values in Camelyon17, FMoW, and RxRx1 are significantly smaller than other datasets, indicating relatively weak spurious correlations. Under this setting, enlarging the interpolation scope by directly interpolating samples within the same class regardless of existing domain information may bring more benefits.}

\yao{\textbf{Results.}
We report the results of domain shifts in Table~\ref{tab:res_domain}, where full results that include validation performance and other metrics are listed in Appendix~\ref{sec:app_domain_full_results}. Aligning with the observation in subpopulation shifts, the performance of prior regularization-based invariant predictor learning methods (e.g., IRM, IB-IRM, V-REx) is still unstable across different datasets. For example, V-REx outperforms ERM on Camelyon17, while it fails in RxRx1. However, LISA consistently outperforms all these methods on five datasets regardless of the model architecture and data types (i.e., image or text), indicating its effectiveness in improving robustness to domain shifts with selective augmentation.}








\subsection{Are the Performance Gains of LISA from Data Augmentation?}

\begin{table*}[h]
\small
\caption{Compared LISA with substitute mixup strategies in domain shifts.}
\vspace{0.5em}
\label{tab:domain_ablation}
\begin{center}
% \setlength{\tabcolsep}{1.mm}{
\begin{tabular}{l|cccccc}
\toprule
\multirow{2}{*}{}  & Camelyon17 & FMoW & RxRx1 & Amazon &  MetaShift\\\cmidrule{2-6}
& Avg. Acc. & Worst Acc. & Avg. Acc. & 10-th Per. Acc. & Worst Acc.\\\midrule
ERM & 70.3 $\pm$ 6.4\% & 32.8 $\pm$ 0.45\%  & 29.9 $\pm$ 0.4\% & 53.8 $\pm$ 0.8\% & 52.1 $\pm$ 0.4\% \\
Vanilla mixup & 71.2 $\pm$ 5.3\% & 34.2 $\pm$ 0.45\% & 26.5 $\pm$ 0.5\% & 53.3 $\pm$ 0.0\% & 51.3 $\pm$ 0.7\% \\
In-group mixup & 75.5 $\pm$ 6.7\% & 32.2 $\pm$ 1.18\% & 24.4 $\pm$ 0.2\% & 53.8 $\pm$ 0.6\% & 52.7 $\pm$ 0.5\%\\\midrule
\textbf{LISA (ours)} & \textbf{77.1 $\pm$ 6.5\%} & \textbf{35.5 $\pm$ 0.65\%} & \textbf{31.9 $\pm$ 0.8\%} & \textbf{54.7 $\pm$ 0.0\%} & \textbf{54.2 $\pm$ 0.7\%} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1em}
\end{table*}

\begin{table*}[h]
\small
\caption{Compared LISA with substitute mixup strategies in subpopulation shifts. UW represents upweighting. Full results with standard deviation is listed in Table~\ref{tab:subpopulation_ablation_full}.}
\label{tab:subpopulation_ablation}
\begin{center}
\begin{tabular}{l|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c|}{CMNIST} & \multicolumn{2}{c|}{Waterbirds} & \multicolumn{2}{c|}{CelebA} & \multicolumn{2}{c}{CivilComments}\\
& Avg. & Worst  & Avg. & Worst  & Avg. & Worst & Avg. & Worst \\\midrule
ERM & 27.8\% & 0.0\% & 97.0\% & 63.7\% & 94.9\% & 47.8\% & 92.2\% & 56.0\% \\
Vanilla mixup & 32.6\% & 3.1\% &  81.0\% & 56.2\% & 95.8\% & 46.4\% & 90.8\% & 67.2\% \\
Vanilla mixup + UW & 72.2\% & 71.8\% & 92.1\% & 85.6\% & 91.5\% & 88.0\% & 87.8\% & 66.1\% \\
In-group mixup & 33.6\% & 24.0\% & 88.7\% & 68.0\% & 95.2\% & 58.3\% & 90.8\% & 69.2\% \\
In-group mixup + UW & 72.6\% & 71.6\% & 91.4\% & 87.1\%  & 92.4\%  & 87.8\% & 84.8\% & 69.3\%  \\
\midrule
\textbf{LISA (ours)} & 74.0\% & \textbf{73.3\%} & 91.8\% & \textbf{89.2\%} & 92.4\% & \textbf{89.3\%} & 89.2\% & \textbf{72.6\%} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-1em}
\end{table*}
\begin{table*}[h]
\small
\caption{Results of the analysis of learned invariant predictors. Accuracy of domain prediction ($\mathrm{IP}_{adp}$) and pairwise divergence of prediction among all domains ($\mathrm{IP}_{kl}$) are used to measure the invariance. Smaller values denote stronger invariance.}
\label{tab:prediction_invariance}
\begin{center}
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\toprule
& \multicolumn{4}{c|}{$\mathrm{IP}_{adp} \downarrow$} & \multicolumn{4}{c}{$\mathrm{IP}_{kl} \downarrow$} \\\cmidrule{2-9}
  & CMNIST & Waterbirds & Camelyon17  & MetaShift & CMNIST & Waterbirds  & Camelyon17 & MetaShift\\\midrule
ERM & 82.85\% & 94.99\% &  49.43\% & 67.98\%  & 6.286 & 1.888 & 1.536 & 1.205 \\
Vanilla mixup & 92.34\% & 94.49\% &  52.79\% & 69.36\% & 4.737 & 2.912 & 0.790 & 1.171\\
IRM & 69.42\% & 95.12\% & 47.96\% & 67.59\% & 7.755 & 1.122 & 0.875 & 1.148 \\
IB-IRM & 74.72\% & 94.78\% &  48.37\% & 67.39\% & 1.004 & 3.563 & 0.756 & 1.115 \\
V-REx & 63.58\% & 93.32\%  & 61.38\% & 68.38\% & 3.190 & 3.791 & 1.281 & 1.094 \\
\midrule
\textbf{LISA (ours)} & \textbf{58.42\%} & \textbf{90.28\%} & \textbf{45.15\%} & \textbf{66.01\%}  &  \textbf{0.567} & \textbf{0.134} & \textbf{0.723}  & \textbf{1.001} \\ 
\bottomrule
\end{tabular}
}
\vspace{-2em}
\end{center}
\end{table*}

\label{sec:exp_compare_augmentation}
In LISA, we apply selective augmentation strategies on samples either with the same label but different domains or with the same domain but different labels. Here, we explore two substitute interpolation strategies: 
\begin{itemize}[leftmargin=*]
    \item \emph{Vanilla mixup}: in Vanilla mixup, we do not add any constraints on the sample selection, i.e., the mixup is performed on any pairs of samples.
    \item \emph{In-group mixup}: this strategy applies data interpolation on samples with the same labels and from the same domains. 
\end{itemize}
Notice that all substitute interpolation strategies use the same variant of mixup as LISA (e.g., mixup/CutMix). Finally, as upweighting (UW) small groups significantly improves performance in subpopulation shifts, we evaluate UW combined with Vanilla/In-group mixup.

The results of substitute interpolation strategies on domain shifts and subpopulation shifts are in Table~\ref{tab:domain_ablation} and Table~\ref{tab:subpopulation_ablation}, respectively. Furthermore, we also conduct experiments on datasets without spurious correlation in Table~\ref{tab:app_no_spurious} of Appendix~\ref{sec:app_no_spurious}. From the results, we make the following three key observations. \emph{First}, compared with Vanilla mixup, the performance of LISA verifies that selective data interpolation indeed improve the out-of-distribution robustness by canceling out the spurious correlations and encouraging learning invariant predictors rather than simple data augmentation. These findings are further strengthened by the results in Table~\ref{tab:app_no_spurious} of Appendix~\ref{sec:app_no_spurious}, where Vanilla mixup outperforms LISA and ERM without spurious correlations but LISA achieves the best performance with spurious correlations. \emph{Second}, the superiority of LISA over In-group mixup verifies that only interpolating samples within each group is incapable of eliminating out the spurious information, where In-group mixup still performs the role of data augmentation. \emph{Third}, though incorporating UW significantly improves the performance of Vanilla mixup and In-group mixup in subpopulation shifts, LISA still achieves larger benefits than these enhanced substitute strategies, demonstrating its stronger power in improving OOD robustness.

\subsection{Does LISA Lead to More Invariant Predictors?}
\label{sec:invariance}

\yao{We further analyze the model invariance learned by LISA. Specifically, for each sample {($x_i$, $y_i$, $d$)} in domain $d$, we denote the unscaled output (i.e., logits) of the model as $g_{i,d}$. We use two metrics to measure the invariance (see Appendix~\ref{sec:app_additional_predictor_invariance} for additional metrics and the corresponding results): }
\vspace{-1em}
\begin{itemize}[leftmargin=*]
    \item \yao{\textbf{Accuracy of domain prediction} ($\mathrm{IP}_{adp}$). In the first metric, we use the unscaled output to predict the domain. Concretely, the entire dataset is re-split into training, validation, and test sets, where logits are used as features and labels represent the corresponding domain ID. A logistic regression model is trained to predict the domain.}
    \vspace{-0.5em}
    \item \yao{\textbf{Pairwise divergence of prediction} ($\mathrm{IP}_{kl}$). We calculate the KL divergence of the distribution of logits among all domains, where kernel density estimation is used to estimate the probability density function $P(g_{d}^y)$ of logits from domain $d$ with label $y$. The pairwise divergence of the predictions is defined as \begin{small}$\frac{1}{|\mathcal{Y}||\mathcal{D}|^2}\sum_{y\in \mathcal{Y}}\sum_{d',d\in \mathcal{D}}\mathrm{KL}(P(g_D^y\mid D=d)|P(g_D^y\mid D=d'))$\end{small}.}
\end{itemize}
\vspace{-1em}



\yao{Small values of $\mathrm{IP}_{adp}$ and $\mathrm{IP}_{kl}$ represent strong function-level invariance. In Table~\ref{tab:prediction_invariance}, we report the results of LISA and other approaches on CMNIST, Waterbirds, Camelyon17 and MetaShift. The results verify that LISA learns predictors with greater domain invariance. Besides having more invariant predictors, we observe that LISA also leads to more invariant representations, as detailed in Appendix~\ref{app:sec_representation_invariance}.}


\subsection{Effect of the Degree of Distribution Shifts}
\label{sec:exp_degree}
We investigate the performance of LISA with respect to the degree of distribution shifts. Here, we use MetaShift to evaluate performance, where the distance between training and test domains is measured as the node similarity on a meta-graph~\citep{metadataset}. To vary the distance between training and test domains, we change the backgrounds of training objects (see full experimental details in Appendix~\ref{sec:app_domain_data}). The performance with varied distances is illustrated in Table~\ref{tab:vary_distance}, where the top four best methods (i.e., ERM, IRM, IB-IRM, GroupDRO) are reported for comparison. We observe that LISA consistently outperforms other methods under all scenarios. Another interesting finding is that LISA achieves more substantial improvements with the increases of distance. A potential reason is that the effects of eliminating domain information is more obvious when there is a larger distance between training and test domains. 

\begin{table}[h]
\small
\vspace{-0.5em}
\caption{Effects of the degree of distribution shifts w.r.t. the performance on the MetaShift benchmark. Distance represents the distribution distance between training and test domains. Best B/L represents best baseline.}
\label{tab:vary_distance}
\begin{center}
\setlength{\tabcolsep}{1.mm}{
\begin{tabular}{l|cccc}
\toprule
Distance & 0.44 & 0.71 & 1.12 & 1.43 \\\midrule
ERM & 80.1\% & 68.4\% & 52.1\% & 33.2\% \\
IRM & 79.5\% & 67.4\% & 51.8\% & 32.0\% \\
IB-IRM & 79.7\% & 66.9\% & 52.3\% & 33.6\% \\
GroupDRO & 77.0\% & 68.9\% & 51.9\% & 34.2\% \\
LISA (ours) & \textbf{81.3\%} & \textbf{69.7\%} & \textbf{54.2\%} & \textbf{37.5\%} \\\midrule
LISA v.s. Best B/L & \textcolor{Green}{+1.5\%} & \textcolor{Green}{+1.2\%} & \textcolor{Green}{+3.6\%} & \textcolor{Green}{+9.6\%}\\
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

