\section{Learning Invariant Predictors with Selective Augmentation}
\label{sec:method}
This section presents LISA, a simple way to improve robustness to subpopulation shifts and domain shifts. The key idea behind LISA is to encourage the model to learn invariant predictors by selective data interpolation, which could also alleviates the effects of domain-related spurious correlations. Before detailing how to select interpolated samples, we first provide a general formulation for data interpolation.

In LISA, we perform linear interpolation between training samples. Specifically, given samples $(x_i, y_i, d_i)$ and $(x_j, y_j, d_j)$ drawn from domains $d_i$ and $d_j$, we apply mixup~\citep{zhang2017mixup}, a simple data interpolation strategy, separately on the input features and corresponding labels as:
\begin{equation}
\label{eq:mix}
    x_{mix}=\lambda x_i + (1-\lambda) x_j,\; y_{mix} = \lambda y_i + (1-\lambda) y_j,
\end{equation}
where the interpolation ratio $\lambda \in [0,1]$ is sampled from a Beta distribution $\rm{Beta}(\alpha, \beta)$ and $y_i$ and $y_j$ are one-hot vectors for classification problem. Notice that the mixup approach in~\eqref{eq:mix} can be replaced by CutMix~\citep{yun2019cutmix}, which shows stronger empirical performance in vision-based applications. In text-based applications, we can use Manifold Mixup~\citep{verma2019manifold}, interpolating the representations of a pre-trained model, e.g., the output of BERT~\citep{devlin2018bert}.

After obtaining the interpolated features and labels, we replace the original features and labels in ERM with the interpolated ones. Then, the optimization process in~\eqref{eq:erm} is reformulated as:
\begin{equation}
\label{eq:erm_mix}
\theta^{*} := \arg\min_{\theta \in \Theta} \mathbb{E}_{\{(x_i,y_i,d_i), (x_j, y_j,d_j)\}\sim \hat{P}} [\ell(f_{\theta}(x_{mix}), y_{mix})].
\end{equation}
Without additional selective augmentation strategies, vanilla mixup will regularize the model and reduce overfitting~\citep{zhang2020does}, allowing it to attain good in-distribution generalization. However, vanilla mixup may not be able to cancel out spurious correlations, causing the model to still fail at attaining good OOD generalization (see empirical comparisons in Section~\ref{sec:exp_compare_augmentation} and theoretical discussion in Section~\ref{sec:theory}). In LISA, we instead adopt a new strategy where mixup is only applied across specific domains or groups, which leans towards learning invariant 
predictors and thus better OOD performance. Specifically, the two kinds of selective augmentation strategies are presented as:

\noindent \textbf{Intra-label LISA (LISA-L): Interpolating samples with the same label.}
Intra-label LISA interpolates samples with the same label but different domains (i.e., $d_i \neq d_j$, $y_i=y_j$). As shown in Figure~\ref{fig:method_illustration}(a), this produces datapoints that have both domains partially present, effectively eliminating spurious correlations between domain and label in cases where the pair of domains correlate differently with the label. As a result, intra-label LISA should learn domain-invariant predictors for each class and thus achieve better OOD robustness. 
% \yao{Empirically, if the spurious correlations are not very strong in some datasets with natural domain shifts (see the discussion of the strength of spurious correlations in Appendix~\ref{sec:app_spurious_strength}), we can even enlarge the interpolation scope by only interpolating samples within the same class regardless of domain information (i.e., $y_i=y_j$). More discussions are presented in Section~\ref{sec:experiments}.}
%%CF.1.24: I find this text to be pretty confusing, especially since it is not self-contained (i.e. there's a reference to the appendix). Can you work on making this more clear? It seems like you are saying that the strategy changes completely in some cases.

\noindent \textbf{Intra-domain LISA (LISA-D): Interpolating samples with the same domain.}
Supposing domain information is highly spuriously correlated with the label information, intra-domain LISA (Figure~\ref{fig:method_illustration}(b)) applies the interpolation strategy on samples with the same domain but different labels, i.e., $d_i=d_j$, $y_i\neq y_j$. Intuitively, even within the same domain, the model is supposed to generate different predicted labels since the interpolation ratio $\lambda$ is randomly sampled, corresponding to different labels $y_{mix}$. This causes the model to make predictions that are less dependent on the domain, again improving OOD robustness.

In this paper, we randomly perform intra-label or intra-domain LISA during the training process with probability $p_{sel}$ and $1-p_{sel}$, where $p_{sel}$ is treated as a hyperparameter and determined via cross-validation. \yao{Intuitively, the choice of $p_{sel}$ depends on the number of domains and the strength of the spurious correlations. Empirically, using intra-label LISA brings more benefits when there are more domains or when the the spurious correlations are not very strong. Intra-domain LISA benefits performance when domain information is highly spuriously correlated with the label. The pseudocode of LISA is in Algorithm~\ref{alg:ilsa}.}
%, where we find a balanced ratio (i.e., $p_{sel}=0.5$) performs the best. }

\begin{algorithm}[H]
    \caption{Training Procedure of LISA}
    \label{alg:ilsa}
    \begin{algorithmic}[1]
    \REQUIRE Training data $\mathcal{D}$, step size $\eta$, learning rate $\gamma$, shape parameters $\alpha$, $\beta$ of Beta distribution
    \WHILE{not converge}
    \STATE Sample $\lambda \sim \mathrm{Beta}(\alpha, \beta)$
    % \STATE Sample a set of samples $B_1$ uniformly from the training data
    \STATE Sample minibatch $B_1 \sim \mathcal{D}$
    \STATE Initialize $B_2 \leftarrow \{\}$
    \STATE Select strategy $s\sim Bernoulli(p_{sel})$
    \IF{$s$ is True} \comment{intra-label LISA}
    \FOR{$(x_i, y_i, d_i) \in B_1$}
        \STATE Randomly sample $(x_j, y_j, d_j) \sim \{(x,y,d) \in \mathcal{D}\}$ which satisfies $(y_i=y_j)$ and $(d_i\neq d_j)$. 
        \STATE Put $(x_j, y_j, d_j)$ into $B_2$. 
    \ENDFOR
    \ELSE \comment{intra-domain LISA}
    \FOR{$(x_i, y_i, d_i) \in B_1$}
        \STATE Randomly sample $(x_j, y_j, d_j) \sim \{(x,y,d) \in \mathcal{D}\}$ which satisfies $(y_i\neq y_j)$ and $(d_i= d_j)$. 
        \STATE Put $(x_j, y_j, d_j)$ into $B_2$. 
    \ENDFOR
    % \STATE Randomly select intra-label or intra-domain LISA with the probability $p_{sel}$ and $1-p_{sel}$
    % \IF{use intra-label LISA (LISA-L)}
    % \STATE For each sample ($x_i$, $y_i$, $d_i$), sample another datapoint ($x_j$, $y_j$, $d_j$) from the dataset with the same label ($y_i=y_j$) but different domains ($d_i\neq d_j$), and construct set $B_2$. 
    % \ELSIF{use intra-domain LISA (LISA-D)}
    % \STATE Randomly sample a domain $d_i$
    % \STATE For each sample ($x_i$, $y_i$, $d_i$) in domain $d_i$, find another one ($x_j$, $y_j$, $d_j$) from the same domain ($d_i=d_j$) but different labels ($y_i\neq y_j$), constructing set $B_2$. 
    \ENDIF
    \STATE Update $\theta$ with data $\lambda B_1 + (1-\lambda) B_2$ with learning rate $\gamma$. 
    \ENDWHILE
    \end{algorithmic}
\end{algorithm}
%%CF.1.24: I think it would be a good idea to write this algorithm out mathematically rather than in English. e.g. 
% use $\mathcal{D}$ for the training data notation
% Sample minibatch $B_1 \sim \mathcal{D}$
% Initialize minibatch $B_2 \leftarrow {}$
% Sample strategy $s \sim Bernoulli(p_sel)$
% \IF{ $s$ is True } \comment{intra-label LISA}
% for $(x_i, y_i, d_i) \in B_i$
%    randomly sample (x_j, y_j, d_j) \sim {(x,y,d) \in \mathcal{D} for d=d_i}
% and so on
%%CF.1.24: I think it's important to write it out mathematically because there are some parts of the algorithm that are open to interpretation and not very precise in the current version