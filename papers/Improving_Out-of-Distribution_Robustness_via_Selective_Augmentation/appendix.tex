\renewcommand{\baselinestretch}{1.0}

\def\R{\mathbbm{R}}
\def\P{\mathbbm{P}}
\def\E{\mathbbm{E}}
\def\lam{\lambda}
\def\Sig{\Sigma}
\def\gam{\gamma}
\def\tx{\tilde{x}}
\def\ty{\tilde{y}}
\section{Proofs of Theorem \ref{thm1} and Theorem \ref{thm3}}
\label{sec:app_proof}
\textbf{Outline of the proof}. We will first find the mis-classification errors based on the population version of OLS with different mixup strategies. Next, we will develop the convergence rate of the empirical OLS based on $n$ samples towards its population version. These two steps together give us the empirical mis-classification errors of different methods. We will separately show that the upper bounds in Theorem \ref{thm1} and Theorem \ref{thm3} hold for two selective augmentation strategies of LISA and hence hold for any $p_{sel}\in[0,1]$. Let LL denote intra-label LISA and LD denote intra-domain LISA.

Let $\pi_1=\P(y_i=1)$ and $\pi_0=\P(y_i=0)$ denote the marginal class proportions in the training samples. Let $\pi_R=\P(d_i=R)$ and $\pi_G=\P(d_i=G)$ denote the marginal subpopulation proportions in the training samples. Let $\pi_{G|1}=\P(d_i=G|y_i=1)$ and define $\pi_{G|0}$, $\pi_{R|1}$, and $\pi_{R|0}$ similarly.

We consider the setting where $\alpha:=\pi^{(1,G)}=\pi^{(0,R)}$ is relatively small and $\pi^{(1)}=\pi^{(0)}=\pi^{(G)}=\pi^{(R)}=1/2$.

\subsection{Decomposing the loss function}
Recall that $\Delta=\mu^{(1,G)}-\mu^{(0,G)}=\mu^{(1,R)}-\mu^{(0,R)}$. We further define $\widetilde{\Delta}=\mu^{(1)}-\mu^{(0)}$, $\theta^{(G)}=\mu^{(0,G)}-\E[x_i]$, and $\theta^{(R)}=\mu^{(0,R)}-\E[x_i]$.

For the mixup estimators, we will repeatedly use the fact that $\lam_i$ has a symmetric distribution with support $[0,1]$.

For ERM estimator based on $(X,y)$, where $b_0=\frac{1}{2}-\E[x_i]^Tb$, we have
\begin{align*}
(\mu^{(0,G)})^Tb+b_0&=(\mu^{(0,G)}-\E[x_i])^Tb+\frac{1}{2}\\
&=(\theta^{(G)})^Tb+\E[y_i]\\
(\mu^{(1,G)})^Tb+b_0&=(\mu^{(1,G)}-\E[x_i])^Tb+\frac{1}{2}\\
&=\Delta^Tb+(\theta^{(G)})^Tb+\E[y_i],
\end{align*}

Notice that based on the estimator $b,b_0$, for $d\in\{G,R\}$,
\begin{align}
    \label{err-orginal}
 E^{(1,d)}(b,b_0)=\Phi(\frac{-\Delta^Tb -(\theta^{(d)})^Tb}{\sqrt{b^T\Sig b}})~~\text{and}~~ E^{(0,d)}(b,b_0)=\Phi(\frac{ (\theta^{(d)})^Tb}{\sqrt{b^T\Sig b}}).
\end{align}

In the extreme case where $\pi_{0,R}=\pi_{1,G}=0$, we have 
\[
\widetilde{\Delta}=\mu^{(1,R)}-\mu^{(0,G)},\theta^{(G)}=-\frac{1}{2}\widetilde{\Delta},\theta^{(R)}=\frac{1}{2}\widetilde{\Delta}-\Delta,~\text{and}~\Delta_0:=\mu^{(0,G)}-\mu^{(0,R)}=\Delta-\widetilde{\Delta}.
\]

Hence,
\begin{align}
\label{err0}
    E_0^{(wst)}
    &=\max \{\Phi\left(\frac{(\frac{1}{2}\widetilde{\Delta}-\Delta)^Tb}{\sqrt{b^T\Sig b}}\right),\Phi\left(\frac{-\frac{1}{2}\widetilde{\Delta}^Tb}{\sqrt{b^T\Sig b}}\right)\}.
\end{align}

\subsection{Classification errors of four methods with infinite training samples}
We first provide the limit of the classification errors when $n\rightarrow \infty$.

\subsubsection{Baseline method: ERM}
For the training data, it is easy to show that
\begin{align*}
    var(x)&=\E[var(x|y)]+var(\E[x|y])\\
     &=\Sigma+\E[var(\E[x|y,D]|y)]+var((\mu^{(1)}-\mu^{(0)})y)\\
    &=\Sigma+\E[var(\mu^{(0,R)}-\mu^{(0,G)})\mathbbm{1}(D=R)|y)]+\widetilde{\Delta}^{\otimes 2}\pi^{(1)}\pi^{(0)}\\
    &=\Sigma+\frac{1}{2}(\mu^{(0,R)}-\mu^{(0,G)})^{\otimes 2}(\pi_{R|1}\pi_{G|1}+\pi_{R|0}\pi_{G|0})+\widetilde{\Delta}^{\otimes 2}\pi^{(1)}\pi^{(0)}\\
    cov(x,y)&=cov(\E[x|y],y)\\
    &=cov(\mu^{(0)}+\widetilde{\Delta}y,y)\\
    &=cov(\widetilde{\Delta}y,y)=\widetilde{\Delta}\pi^{(1)}\pi^{(0)}
\end{align*}

For $a_0=\frac{1}{2}(\pi_{R|1}\pi_{G|1}+\pi_{R|0}\pi_{G|0})$ and $\Delta_0=\mu^{(0,G)}-\mu^{(0,R)}$, the ERM has slope and intercept being
\begin{align*}
    b&=var(x)^{-1}cov(x,y)\\
    &\propto (\Sig+ a_0\Delta_0^{\otimes 2})^{-1}\widetilde{\Delta}\\
    &=\Sig^{-1}\widetilde{\Delta}-\Sig^{-1}\Delta_0\cdot \frac{a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}{1+a_0\Delta_0^T\Sig^{-1}\Delta_0}\\
    b_0&=\E[y]-\E[x^Tb].
\end{align*}


\subsubsection{Baseline method: Vanilla mixup}
The vanilla mixup does not use the group information. Let $i_1$ be a random draw from $\{1,\dots, n\}$. Let $i_2$ be a random draw from $\{1,\dots, n\}$ independent of $i_1$.
Let
\[
   \tilde{y}_i=\lam_i y_{i_1}+(1-\lam_i)y_{i_2}
\]
and
\[
  \tilde{x}_i=\lam_i x_{i_1}+(1-\lam_i)x_{i_2}.
\]
We can find that
\begin{align*}
    cov(\tx_i,\ty_i)&=cov(\lam_i x_{i_1}+(1-\lam_i)x_{i_2},\lam_i y_{i_1}+(1-\lam_i)y_{i_2})\\
    &=cov(\lam_i x_{i_1},\lam_i y_{i_1})+cov((1-\lam_i)x_{i_2},(1-\lam_i)y_{i_2})\\
    &=(\E[\lam_i^2]+\E[(1-\lam_i)^2])cov(x_i,y_i).\\
    cov(\tx_i)&=(\E[\lam_i^2]+\E[(1-\lam_i)^2])cov(x_i).
\end{align*}
Hence, the population-level slope is the same as the slope in the benchmark method. It is easy to show that the population-level intercept is also the same. Hence,
\begin{align*}
E_{\textup{mix}}^{(wst)}=E_0^{(wst)}.
\end{align*}


\subsection{Intra-label LISA (LISA-L): mixup across domain}

Define
\[
  x_i^{(\lam)}=\lam_i x_{i_1}^{(y_i,G)}+(1-\lam_i)x_{i_2}^{(y_i,R)},
\]
where $i_1$ is a random draw from $\{l: y_l=y_i,D_l=G\}$ and $i_2$ is a random draw from $\{l: y_l=y_i,D_l=R\}$. Then we perform OLS based on $(x_i^{(\lam)},y_i),i=1,\dots,n$.

We can calculate that
\begin{align*}
cov(x_i^{(\lam)},y_i)&=cov(\E[x_i^{(\lam)}|y_i],y_i)=cov(\frac{1}{2}\mu^{(y_i,G)}+\frac{1}{2}\mu^{(y_i,R)},y_i)\\
& =var(y_i)\Delta=\pi^{(1)}\pi^{(0)}\Delta\\
cov(x_i^{(\lam)})&=\E[cov(x_i^{(\lam)}|y_i,\lam_i)]+cov(\E[x_i^{(\lam)}|y_i,\lam_i])\\
&=2\E[\lam_i^2]\Sig+cov(\lam_i(\mu^{(0,G)}-\mu^{(0,R)})+\Delta y_i)\\
&=2\E[\lam_i^2]\Sig+var(\lam_i)(\mu^{(0,G)}-\mu^{(0,R)})^{\otimes 2}+\pi^{(1)}\pi^{(0)}\Delta^{\otimes 2}.
\end{align*}


\subsection{Intra-domain LISA (LISA-D): mixup within each domain}

The interpolated sample can be written as
\begin{align*}
  &(\ty_i,\tx_i)=(\lam_i,\lam_ix_{i_1}^{(1,G)}+(1-\lam_i)x_{i_2}^{(0,G)}) ~\text{if}~d_i=G\\
&(\ty_i,\tx_i)=(\lam_i,\lam_ix_{i_1}^{(1,R)}+(1-\lam_i)x_{i_2}^{(0,R)}) ~\text{if}~d_i=R,
\end{align*}
where $i_1$ is a random draw from $\{l: d_l=d_i,y_i=1\}$ and $i_2$ is a random draw from $\{l: d_l=d_i,y_i=0\}$.

We consider regress $\ty_i$ on $\tx_i$. 
\begin{align*}
cov(\tx_i,\ty_i|d_i=G)&=cov(\E[\tx_i|\ty_i,d_i=G],\ty_i|d_i=G)=var(\ty_i)(\mu^{(1,G)}-\mu^{(0,G)})\\
var(\tx_i|d_i=G)&=\E[var(\tx_i|,\lam_i,D_i=G)|d_i=G]+var(\E[\tx_i|,\lam_i,d_i=G]|D_i=G]\\
&=2\E[\lam_i^2]\Sig+var(\lam_i\mu^{(1,G)}+(1-\lam_i)\mu^{(0,G)}|d_i=G)\\
&=2\E[\lam_i^2]\Sig+var(\ty_i)\Delta^{\otimes 2}.
\end{align*}
We further have
\begin{align*}
    cov(\tx_i,\ty_i)&=\E[cov(\tx_i,\ty_i|d_i)]+cov(\E[\tx_i|d_i],\E[\ty_i|d_i])\\
&=cov(\tx_i^{(G)},\ty_i^{(G)})\pi^{(G)}+cov(\tx_i^{(R)},\ty_i^{(R)})\pi^{(R)}\\
&=var(\ty_i)(\mu^{(1,G)}-\mu^{(0,G)})\pi^{(G)}+var(\ty_i)(\mu^{(1,R)}-\mu^{(0,R)})\pi^{(R)}\\
&=var(\ty_i)\Delta.
\end{align*}
Moreover,
\begin{align*}
    var(\tx_i)&=\E[var(\tx_i|d_i)]+var(\E[\tx_i|d_i])\\
    &=var(\tx_i^{(G)})\pi^{(G)}+var(\tx^{(R)})\pi^{(R)}+(\E[\tx^{(G)}]-\E[\tx^{(R)}])^{\otimes 2}\pi^{(G)}\pi^{(R)}\\
    &=2\E[\lam_i^2]\Sig+var(\lam_i)\Delta^{\otimes 2}+(\mu^{(0,G)}-\mu^{(0,R)})^{\otimes 2}\pi^{(G)}\pi^{(R)}.
\end{align*}


Slope:
\begin{align*}
    b&=var(\tx_i)^{-1}cov(\tx_i,\ty_i)\\
    &\propto (\Sig+a_{\textup{LD}}\Delta_0^{\otimes 2})^{-1}\Delta\\
    &=\Sig^{-1}\Delta-\Sig^{-1}\Delta_0\cdot\frac{a_{\textup{LD}}(\Delta_0)^T\Sig^{-1}\Delta}{1+a_{\textup{LD}}(\Delta_0)^T\Sig^{-1}\Delta_0}\\
    &\propto \Sig^{-1}\tilde{\Delta}+c_{\textup{LD}}\Sig^{-1}\Delta,
\end{align*}
where $a_{\textup{LD}}=\frac{\pi^{(R)}\pi^{(G)}}{2\E[\lam_i^2]}$ and 
 \[
    c_{\textup{LL}}=\frac{1+a_{\textup{LD}}\Delta_0^T\Sig^{-1}\Delta_0-a_{\textup{LD}}\Delta^T\Sig^{-1}\Delta_0}{a_{\textup{LD}}\Delta^T\Sig^{-1}\Delta_0}.
 \]

Moreover, $b_0=\E[\ty_i]-\E[\tx_i]^Tb=\frac{1}{2}-\E[\tx_i]^Tb$.
Notice that
\begin{align*}
    \E[\tx_i]&=\frac{1}{4}(\mu^{(0,G)}+\mu^{(1,G)}+\mu^{(0,R)}+\mu^{(1,R)})\\
    &=\frac{1}{4}(2\mu^{(0,G)}+\Delta+2\mu^{(1,R)}-\Delta)\\
    &=\frac{1}{2}(\mu^{(0,G)}+\mu^{(1,R)})=\E[x_i].
\end{align*}



\underline{\textbf{Method comparison}}.
We only need to compare $E^{(wst)}_{\textup{ERM}}, E^{(wst)}_{LL}, E^{(wst)}_{LD}$.  

For the ERM, $0\leq a_0\leq 2\alpha$ and 
\begin{align*}
b_{\textup{ERM}}&=(1+\frac{a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}{1+a_0\Delta_0^T\Sig^{-1}\Delta_0})\Sig^{-1}\widetilde{\Delta}-\frac{a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}{1+a_0\Delta_0^T\Sig^{-1}\Delta_0}\Sig^{-1}\Delta\\
&\propto \Sig^{-1}\widetilde{\Delta}-\frac{a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}{1+a_0\Delta_0^T\Sig^{-1}\Delta_0+a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}\Sig^{-1}\Delta\\
&\propto \Sig^{-1}\widetilde{\Delta}-\frac{a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}{1+a_0\Delta^T\Sig^{-1}\Delta_0}\Sig^{-1}\Delta.
\end{align*}
Let $c_0=\frac{a_0\widetilde{\Delta}^T\Sig^{-1}\Delta_0}{1+a_0\Delta^T\Sig^{-1}\Delta_0}$ and $c_1=|c_0|\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}$. For simplicity, let $\|v\|_{\Sig}=v^T\Sig^{-1}v$. We first lower bound it via
\begin{align*}
  cor(b_{\textup{ERM}},\widetilde{\Delta})=   \frac{b^T\widetilde{\Delta}}{\|\widetilde{\Delta}\|_{\Sig}\sqrt{b^T\Sig b}}&=\frac{\widetilde{\Delta}^T\Sig^{-1}\widetilde{\Delta}-c_0\Delta^T\Sig^{-1}\widetilde{\Delta}}{\|\widetilde{\Delta}\|_{\Sig}\sqrt{b^T\Sig b}}\\
    &\geq \frac{\widetilde{\Delta}^T\Sig^{-1}\widetilde{\Delta}}{\|\widetilde{\Delta}\|_{\Sig}(\|\widetilde{\Delta}\|_{\Sig}+|c_0|\|\Delta\|_{\Sig})}-\frac{|c_0\Delta^T\Sig^{-1}\widetilde{\Delta}|}{\|\widetilde{\Delta}\|_{\Sig}\sqrt{b^T\Sig b}}\\
    &\geq \frac{1}{1+|c_0|\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}}-\frac{c_0\xi\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}-c_0\|\Delta\|_{\Sig}}\\
    &\geq \frac{1-(1+\xi)c_1-c_1^2}{1-c_1^2}=1-C\alpha.
\end{align*}

Similarly, we have
\begin{align*}
   cor(b_{\textup{ERM}},\Delta)=  \frac{b^T\Delta}{\|\Delta\|_{\Sig}\sqrt{b^T\Sig b}}&=\frac{\Delta^T\Sig^{-1}\widetilde{\Delta}-c_0\Delta^T\Sig^{-1}\Delta}{\|\Delta\|_{\Sig}\sqrt{b^T\Sig b}}\\
    &\leq  \frac{\widetilde{\Delta}^T\Sig^{-1}\Delta}{\|\Delta\|_{\Sig}(\|\widetilde{\Delta}\|_{\Sig}\pm c_0\|\Delta\|_{\Sig})}+\frac{|c_0\Delta^T\Sig^{-1}\Delta|}{(\|\widetilde{\Delta}\|_{\Sig}- c_0\|\Delta\|_{\Sig})\|\Delta\|_{\Sig}}\\
    &\leq \frac{1}{1\pm c_0\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}}\xi+\frac{c_0\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}}{1-c_0\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}}\\
    &\leq (\frac{\xi}{1\pm c_1}-\frac{c_1}{1-c_1})\|\Delta\|_{\Sig}.
\end{align*}

Hence,
\begin{align}
\label{err0-lb}
    E_{\textup{ERM}}^{(wst)}\geq \max\left\{\Phi((\frac{1}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}),\Phi((-\frac{1}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig})\right\}
\end{align}
 for some constant $C$ depending on the true parameters.


For method LISA-L, using the fact that $\Delta_0=\Delta-\widetilde{\Delta}$, for $a_{\textup{LL}}=var(\lam_i)/(2\E[\lam_i^2)])$,
\begin{align*}
 b_{\textup{LL}}&\propto\Sig^{-1}\Delta+\frac{-a_{\textup{LL}}\Delta^T\Sig^{-1}\Delta_0}{1+a_{\textup{LL}}\Delta_0^T\Sig^{-1}\Delta_0}\Sig^{-1}\widetilde{\Delta}\\
 &\propto \Sig^{-1}\widetilde{\Delta}+c_{\textup{LL}}\Sig^{-1}\Delta
 \end{align*}
 for 
 \[
    c_{\textup{LL}}=\frac{1+a_{\textup{LL}}\Delta_0^T\Sig^{-1}\Delta_0-a_{\textup{LL}}\Delta^T\Sig^{-1}\Delta_0}{a_{\textup{LL}}\Delta^T\Sig^{-1}\Delta_0}=\frac{1-a_{\textup{LL}}\tilde{\Delta}^T\Sig^{-1}\Delta_0}{a_{\textup{LL}}\Delta^T\Sig^{-1}\Delta_0}.
 \]
 Hence,
 \begin{align*}
cor(b_{\textup{LL}},\widetilde{\Delta})&=\frac{ \widetilde{\Delta}^T b_{\textup{LL}}}{\|\widetilde{\Delta}\|_{\Sig}\sqrt{b_{\textup{LL}}^T\Sig b_{\textup{LL}}}} =\frac{\|\widetilde{\Delta}\|_{\Sig}+c_{\textup{LL}}\xi\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}}\\
 cor(b_{\textup{LL}},\Delta)&=\frac{ b_{\textup{LL}}^T\Delta}{\|\Delta\|_{\Sig}\sqrt{b_{\textup{LL}}^T\Sig b_{\textup{LL}}}}=\frac{\xi\|\widetilde{\Delta}\|_{\Sig}+c_{\textup{LL}}\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}}.
\end{align*}

To have $E_{\textup{LL}}^{(wst)}< E_{\textup{ERM}}^{(wst)}$, it suffices to require that $(-\frac{1}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}< (\frac{1}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}$ and
\begin{align*}
   &\frac{1}{2} cor(b_{\textup{LL}},\widetilde{\Delta})\|\widetilde{\Delta}\|_{\Sig}-cor(b_{\textup{LL}},\Delta)\|\Delta\|_{\Sig}\leq (\frac{1}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}\\
   &-\frac{1}{2} cor(b_{\textup{LL}},\widetilde{\Delta})\|\widetilde{\Delta}\|_{\Sig}\leq (\frac{1}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}.
\end{align*}


A sufficient condition is
\begin{align*}
    & \xi < (\frac{1}{2}+\frac{1}{2}cor(b_{\textup{LL}},\widetilde{\Delta}))\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha,~cor(b_{\textup{LL}},\Delta)\geq \xi+C\alpha,~~cor(b_{\textup{LL}},\widetilde{\Delta})\leq 1-2C\alpha.
\end{align*}
We can find that a further sufficient condition is
\begin{align}
   & \xi < \frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha, c_{\textup{LL}}> 0, \xi \le \frac{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}-\|\widetilde{\Delta}\|_{\Sig}}{c_{\textup{LL}}\|\Delta\|_{\Sig}}-\eps_1\alpha\label{cond1}\\
    &\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}\geq \|\widetilde{\Delta}\|_{\Sig},~\xi\leq \frac{c_{\textup{LL}}\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}-\|\widetilde{\Delta}\|_{\Sig}}-\eps_1\alpha\label{cond2}\\
   & \xi \leq (\frac{1}{2}+\frac{1}{2}cor(b_{\textup{LL}},\widetilde{\Delta}))\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha.\label{cond3}
\end{align}
We first find sufficient conditions for the statements in (\ref{cond1}) and (\ref{cond2}).
Parameterizing $t=c_{\textup{LL}}\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}$, we further simplify the condition in (\ref{cond1}) and (\ref{cond2}) as
\begin{align*}
   & \xi < \min\{\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},1\}-C\alpha,t(t+2\xi)>0\\
   &\xi\leq \frac{\sqrt{1+t^2+2t\xi}-1}{t}-\eps_1\alpha,~~\xi\leq \frac{1+\sqrt{1+t^2+2t\xi}}{t+2\xi}-\eps_1\alpha.
\end{align*}
We only need to require
\[
  t\geq \max\{0,-2\xi\}~\text{and}~\xi \leq \min\{\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},1\}-C\alpha.
\]
Some tedious calculation shows that $t\geq \max\{0,-2\xi\}$ can be guaranteed by
\[
  a_{\textup{LL}}\leq\frac{1}{\|\tilde{\Delta}\|_{\Sig}^2+\|\tilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}}~\text{and}~\xi\leq \frac{\|\Delta\|_{\Sig}}{\|\tilde{\Delta}\|_{\Sig}}
\]
It is left to consider the constraint in (\ref{cond3}).
Notice that it holds for any $\xi\leq 0$. When $\xi>0$, we can see
\begin{align*}
cor(b_{\textup{LL}},\widetilde{\Delta})&=\frac{\|\widetilde{\Delta}\|_{\Sig}+\xi c_{\textup{LL}}\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}}=\frac{1+t\xi}{\sqrt{1+t^2+2t\xi}}\\
&\geq \frac{1+t\xi}{1+t}\geq \xi.
\end{align*}
Hence, it suffices to guarantee that 
\[
   (1-\frac{1}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}})\xi<\frac{1}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha.
\]
If $\|\widetilde{\Delta}\|_{\Sig}/\|\Delta\|_{\Sig}\geq 2$, then LHS is negative and it holds. If $1\leq \|\widetilde{\Delta}\|_{\Sig}/\|\Delta\|_{\Sig}< 2$, then the inequality becomes $\xi< 1-C\alpha$. If $\|\widetilde{\Delta}\|_{\Sig}/\|\Delta\|_{\Sig}<1$, then the inequality becomes $\xi\leq \frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha$.
Because we have required $\xi < \min\{\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},1\}-C\alpha$ for some large enough $C$, the constraint (\ref{cond3}) always holds.
To summarize, $E_{\textup{LL}}< E_{\textup{ERM}}$ given that $\xi \leq \min\{\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},\frac{\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}}\}-C\alpha$ for some large enough $C$ and $a_{\textup{LL}}\leq 1/(\|\widetilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig})$.



For \textbf{method LISA-D}, we can similarly show that $E_{\textup{LD}}\leq E_{\textup{ERM}}$ given that $\xi < \min\{\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},\frac{\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}}\}-C\alpha$ for some large enough $C$ and $a_{\textup{LD}}\leq 1/(\|\widetilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig})$.
%%%%%%%%%%%%%%

\subsection{Finite sample analysis}
The empirical loss can be written as
\begin{align}
    \label{loss-em}
    &\P(\mathbbm{1}((x_i^{G)})^T\hat{b}+\hat{b}_0>\frac{1}{2})\neq y_i^{(G)})\\
   & =\frac{1}{2}\P((x_i^{G)})^T\hat{b}+\hat{b}_0>\frac{1}{2}|y_i^{(G)}=0)+\frac{1}{2}\P((x_i^{G)})^T\hat{b}+\hat{b}_0<\frac{1}{2}|y_i^{(G)}=1),\nonumber
\end{align}
where
\begin{align*}
    &\P((x_i^{G)})^T\hat{b}+\hat{b}_0>\frac{1}{2}|y_i^{(G)}=0)=\Phi(-\frac{\frac{1}{2}-(\mu^{(0,G)})^T\hat{b}-\hat{b}_0}{\sqrt{\hat{b}^T\Sig \hat{b}}}).\\
    &\P((x_i^{G)})^T\hat{b}+\hat{b}_0<\frac{1}{2}|y_i^{(G)}=1)=\Phi(\frac{\frac{1}{2}-(\mu^{(1,G)})^T\hat{b}-\hat{b}_0}{\sqrt{\hat{b}^T\Sig \hat{b}}}).
\end{align*}

First notice that
\[
  \hat{b}_0=\bar{y}-\bar{x}^T\hat{b}.
\]

We have
\begin{align*}
(\mu^{(0,G)})^T\hat{b}+\hat{b}_0&=(\mu^{(0,G)}-\bar{x})^T\hat{b}+\bar{y}\\
&=(\mu^{(0,G)}-\E[x_i])^T\hat{b}+\frac{1}{2}+\underbrace{\{(\bar{y}-\bar{x}^T\hat{b})-(\E[y_i]-\E[x_i]^T\hat{b})\}}_{R_1}\\
(\mu^{(1,G)})^T\hat{b}+\hat{b}_0&=(\mu^{(1,G)}-\bar{x})^T\hat{b}+\bar{y}\\
&=\Delta^T\hat{b}+(\mu^{(0,G)}-\E[x_i])^T\hat{b}+\frac{1}{2}+R_1.
\end{align*}


Therefore, according to (\ref{loss-em}),
\begin{align*}
   & \frac{1}{2}\Phi(-\frac{\frac{1}{2}-(\mu^{(0,G)})^T\hat{b}-\hat{b}_0}{\sqrt{\hat{b}^T\Sig \hat{b}}})+\frac{1}{2}\Phi(\frac{\frac{1}{2}-(\mu^{(1,G)})^T\hat{b}-\hat{b}_0}{\sqrt{\hat{b}^T\Sig \hat{b}}})\\
    =&\frac{1}{2}\Phi(\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})+\frac{1}{2}\Phi(-\frac{\Delta+(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})\\
    =&\frac{1}{2}\Phi(\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})+\frac{1}{2}\Phi(-\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})\\
    &-\left\{\frac{1}{2}\Phi(-\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})-\frac{1}{2}\Phi(-\frac{\Delta+(\Theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})\right\}\\
    =&\frac{1}{2}-\left\{\frac{1}{2}\Phi(-\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})-\frac{1}{2}\Phi(-\frac{\Delta^T\hat{b}+(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})\right\}.
\end{align*}


Then the mis-classification error can be written as
\begin{align}
\label{loss2}
    \frac{1}{2}-\frac{1}{2}\underbrace{\left\{\Phi(\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})-\Phi(\frac{(\theta^{(G)})^T\hat{b}-\Delta^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}})\right\}}_{\widehat{L}(\hat{b})}.
\end{align}
Larger the $\widehat{L}(\hat{b})$, smaller the mis-classification error.

We first find that
\begin{align*}
    \widehat{L}(\hat{b})-L(b)\leq C\underbrace{|\frac{(\theta^{(G)})^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}}-\frac{(\theta^{(G)})^Tb}{\sqrt{b^T\Sig b}}|}_{T_1}+C\underbrace{|\frac{(\theta^{(G)})^T\hat{b}-\Delta^T\hat{b}+R_1}{\sqrt{\hat{b}^T\Sig \hat{b}}}-\frac{(\theta^{(G)})^Tb-\Delta^Tb}{\sqrt{b^T\Sig b}}|}_{T_2}.
\end{align*}
In the event that
\[
   \|\Sig^{1/2}(\hat{b}-b)\|_2=o(1) ~\max_{y,d}\|\mu^{(y,d)}\|_2\leq C,~\Sig~\text{is positive definite.}
\]
for the denominator, we have
\begin{align*}
|b^T\Sig b-\hat{b}^T\Sig\hat{b}|&\leq (2\|\Sig^{1/2} b\|_2+\|\Sig^{1/2}(\hat{b}-b)\|_2)\|\Sig^{1/2}(\hat{b}-b)\|_2\\
&\leq 2(1+o(1))\|\Sig^{1/2} b\|_2\|\Sig^{1/2}(\hat{b}-b)\|_2\\
  | \sqrt{\hat{b}^T\Sig\hat{b}}-\sqrt{b^T\Sig b}|&\leq \frac{|\hat{b}^T\Sig\hat{b}-b^T\Sig b|}{\sqrt{\hat{b}^T\Sig\hat{b}}+\sqrt{b^T\Sig b}}\\
  &\leq 2(1+o(1))\|\Sig^{1/2}(\hat{b}-b)\|_2.
\end{align*}

For the numerator, we have
\[
|\frac{1}{2}\widetilde{\Delta}^T\hat{b}+R_1-\frac{1}{2}\widetilde{\Delta}^Tb|\leq |R_1|+\frac{1}{2}\|\Sig^{-1/2}\widetilde{\Delta}\|_2\|\Sig^{1/2}(\hat{b}-b)\|_2.
\]

We arrive at
\begin{align*}
    T_1\leq (1+o(1))\frac{|R_1|+\frac{1}{2}\|\Sig^{-1/2}\widetilde{\Delta}\|_2\|\Sig^{1/2}(\hat{b}-b)\|_2}{\|\Sig^{1/2}b\|_2}+(1+o(1))\frac{|\widetilde{\Delta}^Tb|}{\sqrt{b^T\Sig b}}\frac{\|\Sig^{1/2}(\hat{b}-b)\|_2}{\sqrt{b^T\Sig b}}.
\end{align*}
\begin{align*}
    T_2\leq& (1+o(1))\frac{|R_1|+\frac{1}{2}(\|\Sig^{-1/2}\widetilde{\Delta}\|_2+\|\Sig^{-1/2}\Delta\|_2)\|\Sig^{1/2}(\hat{b}-b)\|_2}{\|\Sig^{1/2}b\|_2}\\
    &+(1+o(1))\frac{|\frac{1}{2}\widetilde{\Delta}^Tb-\Delta^Tb|}{\sqrt{b^T\Sig b}}\frac{\|\hat{b}-b\|_2}{\sqrt{b^T\Sig b}}.
\end{align*}
Moreover $R_1\leq \|\hat{b}-b\|_2+O_P(\frac{1}{\sqrt{n}})$.
To summarize,
\[
 |\widehat{L}(\hat{b})-L(b)|\lesssim (1+o(1))(\|\hat{b}-b\|_2+\frac{1}{\sqrt{n}}).
\]

In the following, we will upper bound $\|\hat{b}-b\|_2$ for each method.
For the \underline{\textbf{ERM method}}, 
\begin{align*}
    \hat{b}=\{(X-\bar{X})^T(X-\bar{X})\}^{-1}(X-\bar{X})^T(y-\bar{y}).
\end{align*}
It is easy to show that
\[
 \|\hat{b}-b\|_2^2=O_P(\frac{p\sum_{i=1}^Nvar(y_i|x_i)}{N^2})=O_P(\frac{p}{N}).
\]

For the \underline{\textbf{vanilla mixup method}}, we first see that
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n\tx_i&=\frac{1}{n}\sum_{i=1}^n(\lam_i x_{i_1}+(1-\lam_i)x_{i_2})=\bar{x}+O_P(n^{-1/2})=\mu+O_P(n^{-1/2})\\
  \frac{1}{n}\sum_{i=1}^n\ty_i&=\pi^{(1)}+O_P(n^{-1/2}).
\end{align*}
Next,
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n\tx_i\ty_i&=  \frac{1}{n}\sum_{i=1}^n\left\{\lam_i^2x_{i_1}y_{i_1}+(1-\lam_i)^2x_{i_2}y_{i_2}+\lam_i(1-\lam_i)x_{i_1}y_{i_2}+\lam_i(1-\lam_i)x_{i_2}y_{i_i}\right\}\\
  \frac{1}{n}\sum_{i=1}^n\tx_i\ty_i- \E[\tx_i\ty_i]  &=\underbrace{\frac{1}{n}\sum_{i=1}^n \tx_i\ty_i-\E[\tx_i\ty_i|X,y]}_{E_1}+\underbrace{\E[\tx_i\ty_i|X,y]-\E[\tx_i\ty_i]}_{E_2}.
\end{align*}
For $E_2$,
\begin{align*}
    E_2&=\frac{2\E[\lam_i^2]}{n}\sum_{i=1}^nx_iy_i-\E[\tx_i\ty_i]=2\E[\lam_i^2]\E[x_iy_i].
\end{align*}
Hence,
\[
  \|E_2\|_2^2=O_P(\frac{p}{n}).
\]

For $E_1$, conditioning on $(X,y)$, $\lam_i^2x_{i_1}y_{i_1}-\frac{\E[\lam_i^2]}{n}\sum_{i=1}^nx_iy_i$ are independent sub-Gaussian vectors. The sub-Gaussian norm of $\frac{1}{N}\sum_{i=1}^n\lam_i^2x_{i_1,j}y_{i_1}-\frac{\E[\lam_i^2]}{n}\sum_{i=1}^nx_{i,j}y_i$ (conditioning on $(X,y)$) can be upper bounded by $c\max_{i\leq N}|x_{i,j}|/\sqrt{n}$. Hence
\begin{align*}
   \P(\|E_1\|_2\geq t|X,y)\leq 2\exp\{-\frac{c_2nt^2}{\max_{j=1}^p\max_{i\leq N}x_{i,j}^2}\}.
\end{align*}
As $x_{i,j}$ are Gaussian distributed, we know that
\[
\P(\sum_{j=1}^p\max_{i\leq n}x_{i,j}^2\geq p\log n)\leq \exp\{-c_3\log n\}.
\]
Hence,
with probability at least $1-\exp(-c_1\log n)$, 
\[
   E_1\leq \frac{Cp\log n}{n}.
\]
To summarize,
\[
   \left\|\frac{1}{n}\sum_{i=1}^n\tx_i\ty_i-(\frac{1}{n}\sum_{i=1}^n\tx_i)(\frac{1}{n}\sum_{i=1}^n\ty_i)-cov(\tx_i,\ty_i)\right\|_2^2=O_P(\frac{p\log n}{n}).
\]
Similarly, we can show that
\[
   \left\|\frac{1}{n}\sum_{i=1}^n\tx_i\tx_i^T-(\frac{1}{n}\sum_{i=1}^n\tx_i)(\frac{1}{n}\sum_{i=1}^n\tx_i)^T-cov(\tx_i)\right\|_2^2=O_P(\frac{p\log n}{n}).
\]
Hence,
\begin{align*}
    \|\hat{b}-b\|_2^2=O_P(\frac{p\log n}{n}).
\end{align*}

For the \underline{\textbf{LISA-L}}, we first see that
\begin{align*}
  \frac{1}{n}\sum_{i=1}^nx^{(\lam)}_i&=\frac{1}{n}\sum_{y_i=1}(\lam_i x_{i_1}^{(1,G)}+(1-\lam_i)x_{i_2}^{(1,R)})+\frac{1}{n}\sum_{y_i=0}(\lam_i x_{i_1}^{(0,G)}+(1-\lam_i)x_{i_2}^{(0,R)})\\
  &=\frac{1}{2}(\bar{x}^{(1,G)}+\bar{x}^{(1,R)})\hat{\pi}_1+\frac{1}{2}(\bar{x}^{(0,G)}+\bar{x}^{(0,R)})\hat{\pi}_0
\end{align*}
We have
\begin{align*}
    \frac{1}{n}(X^{(\lam)})^Ty- \bar{y}\frac{1}{n}\sum_{i=1}^nx^{(\lam)}_i-cov(x_i^{(\lam)},y_i)&=\underbrace{\    \frac{1}{n}(X^{(\lam)})^Ty- \bar{y}\frac{1}{n}\sum_{i=1}^nx^{(\lam)}_i-cov(x_i^{(\lam)},y_i|X,y)}_{E_1}\\&+\underbrace{cov(x_i^{(\lam)},y_i|X,y)-cov(x_i^{(\lam)},y_i)}_{E_2}
\end{align*}
For $E_2$,
\begin{align*}
    E_2&=\frac{\hat{\pi}_1}{2}(\bar{x}^{(1,G)}+\bar{x}^{(1,R)})-\hat{\pi}_1(\frac{1}{2}(\bar{x}^{(1,G)}+\bar{x}^{(1,R)})\hat{\pi}_1+\frac{1}{2}(\bar{x}^{(0,G)}+\bar{x}^{(0,R)})\hat{\pi}_0)-cov(x_i^{(\lam)},y_i)\\
    &=\frac{1}{2}(\bar{x}^{(1,G)}+\bar{x}^{(1,R)}-\bar{x}^{(0,G)}-\bar{x}^{(0,R)})\hat{\pi}_1\hat{\pi}_0-\pi^{(1)}\pi^{(0)}\Delta.
\end{align*}
It is easy to show that
\[
  \|E_2\|_2^2=O_P\left(\frac{p}{\min_{y,e}n^{(y,e)}}\right).
\]

For $E_1$, conditioning on $X$ and $y$, $x_i^{(\lam)}y_i-\E[x_i^{(\lam)}y_i|X,y]$ are independent sub-Gaussian vectors with mean zero. The sub-Gaussian norm of $\frac{1}{n}\sum_{i=1}^nx_{i,j}^{(\lam)}y_i$ (conditioning on $X$ and $y$) can be upper bounded by $c\max_{i\leq n} |x_{i,j}|/\sqrt{N}$.
\begin{align*}
   &\P( \|E_1\|_2\geq t|X,y)=\P\left(\sum_{j=1}^p|\frac{1}{n}\sum_{i=1}^n\{x_{i,j}^{(\lam)}y_i-\E[x_{i,j}^{(\lam)}y_i|X,y]\}|^2\geq t^2|X,y\right)\\
   &\leq 2\exp\left\{-\frac{c_2nt^2}{\sum_{j=1}^p\max_{i\leq n}x_{i,j}^2}\right\}.
\end{align*}
Hence,
\[
  E_1=O_P(\sqrt{\frac{\sum_{j=1}^p\max_{i\leq n}x_{i,j}^2}{n}})=O_P(\frac{p\log n}{n}).
\]
To summarize,
\[
  \|\frac{1}{n}(X^{(\lam)})^Ty-\E[x_i^{(\lam)}y_i]\|_2^2=O_P(\frac{p}{\min_{y,e}n^{(y,e)}}+\frac{p\log n}{n}).
\]

We can use similar analysis to bound
\[
 \|\frac{1}{N}(X^{(\lam)})^TX^{(\lam)}-\E[x_i^{(\lam)}(x_i^{(\lam)})^T]\|_2.
\]
The sub-exponential norm of $\frac{1}{N}\sum_{i=1}^Nx_{i,j}^{(\lam)}x_{i,k}^{(\lam)}$ (conditioning on $X$) can be upper bounded by $\max_{i\leq N}|x_{i,j}||x_{i,k}|/\sqrt{N}$.
We can show that
\begin{align*}
  \|\frac{1}{n}(X^{(\lam)})^TX^{(\lam)}-\E[x_i^{(\lam)}(x_i^{(\lam)})^T]\|_2=O_P(\frac{p}{\min_{y,e}n^{(y,e)}}+\frac{p\log n}{n}).
\end{align*}


For the \underline{\textbf{LISA-D}}, we first see that
\begin{align*}
  \frac{1}{n}\sum_{i=1}^n\tx_i&=\frac{1}{n}\sum_{D_i=G}(\lam_i x_{i_1}^{(1,G)}+(1-\lam_i)x_{i_2}^{(0,G)})+\frac{1}{n}\sum_{D_i=R}(\lam_i x_{i_1}^{(1,R)}+(1-\lam_i)x_{i_2}^{(0,R)})\\
  &=\frac{1}{2}(\bar{x}^{(1,G)}+\bar{x}^{(0,G)})\hat{\pi}_G+\frac{1}{2}(\bar{x}^{(1,R)}+\bar{x}^{(0,R)})\hat{\pi}_R\\
  \bar{\ty}=\frac{1}{2}.
\end{align*}

Next,
\begin{align*}
    \frac{1}{n}\sum_{i=1}^n\tx_i\ty_i&=  \frac{1}{n}\sum_{D_i=G}\left\{\lam_i^2x_{i_1}^{(1,G)}+\lam_i(1-\lam_i)x_{i_2}^{(0,G)}\right\}+\frac{1}{n}\sum_{D_i=R}\left\{\lam_i^2x_{i_1}^{(1,R)}+\lam_i(1-\lam_i)x_{i_2}^{(0,R)}\right\}\\
  \frac{1}{n}\sum_{i=1}^n\tx_i\ty_i -\bar{\tx}\bar{\ty}-cov(\tx,\ty) &=\underbrace{\frac{1}{n}\sum_{i=1}^n \tx_i\ty_i-\bar{\tx}\bar{\ty}-cov(\tx_i,\ty_i|X,y)}_{E_1}+\underbrace{cov(\tx_i,\ty_i|X,y)-cov(\tx_i,\ty_i)}_{E_2}.
\end{align*}
For $E_2$,
\begin{align*}
    E_2&=\hat{\pi}^{(G)}(\E[\lam_i^2](\bar{x}^{(1,G)}-\bar{x}^{(0,G)})+\frac{1}{2}\bar{x}^{(0,G)})+\hat{\pi}^{(R)}(\E[\lam_i^2](\bar{x}^{(1,R)}-\bar{x}^{(0,R)})+\frac{1}{2}\bar{x}^{(0,R)})-\\
   & \frac{1}{4}(\bar{x}^{(1,G)}+\bar{x}^{(0,G)})\hat{\pi}_G-\frac{1}{4}(\bar{x}^{(1,R)}+\bar{x}^{(0,R)})\hat{\pi}_R-var(\lam_i)\Delta\\
    & =\hat{\pi}^{(G)}var(\lam_i)(\bar{x}^{(1,G)}-\bar{x}^{(0,G)})+\hat{\pi}^{(R)}var(\lam_i)(\bar{x}^{(1,R)}-\bar{x}^{(0,R)})-var(\lam_i)\Delta.
\end{align*}
Notice that $E_2$ is a sub-Gaussian vector with sub-Gaussian norm upper bounded by
\[
   \frac{\hat{\pi}_G^2}{n^{(1,G)}}+\frac{\hat{\pi}_G^2}{n^{(0,G)}}+\frac{\hat{\pi}_R^2}{n^{(1,R)}}+\frac{\hat{\pi}_R^2}{n^{(0,R)}}\leq \frac{4}{n}\max_{y,d}\frac{\pi_d}{\pi_{y|d}}.
\]
Using sub-Gaussian concentration, we can show that
\begin{align*}
    E_2=O_P(\sqrt{\frac{p}{n}\max_{y,d}\frac{\pi_d}{\pi_{y|d}}}).
\end{align*}
Notice that $\max_{y,d}\frac{\pi_d}{\pi_{y|d}}\geq 1$.
For $E_1$, conditioning on $X$ and $y$ $\tx_i\ty_i-\E[\tx_i\ty_i|X,y]$ are independent sub-Gaussian vectors with mean zero. The sub-Gaussian norm of $\frac{1}{n}\sum_{i=1}^n\tx_{i,j}\ty_i$ conditioning on $X$ and $y$ can be upper bounded by $c\max_{i,j}|x_{i,j}|$. Similar analysis on $E_1$ leads to
\begin{align*}
     \frac{1}{n}\sum_{i=1}^n\tx_i\ty_i -\bar{\tx}\bar{\ty}-cov(\tx,\ty)=O_P(\sqrt{\frac{p\log n}{n}}+\sqrt{\frac{p}{n}\max_{y,d}\frac{\pi_d}{\pi_{y|d}}}).
\end{align*}

For the sample covariance matrix, we can also show that
\[
   \left\|\frac{1}{n}\sum_{i=1}^n\tx_i\tx_i^T-(\frac{1}{n}\sum_{i=1}^n\tx_i)(\frac{1}{n}\sum_{i=1}^n\tx_i)^T-cov(\tx_i)\right\|_2^2=O_P(\sqrt{\frac{p\log n}{n}}+\sqrt{\frac{p}{n}\max_{y,d}\frac{\pi_d}{\pi_{y|d}}}).
\]

\subsection{A $\xi$-dependent lower bound for $E^{(wst)}_{\textup{ERM}}-E^{(wst)}_{\textup{LL}}$}
Next, we provide a $\xi$-dependent lower bound for $E^{(wst)}_{\textup{ERM}}-E^{(wst)}_{\textup{LL}}$. Based on our previous analysis
\begin{align*}
 E^{(wst)}_{\textup{ERM}}-E^{(wst)}_{\textup{LL}}&\geq c_1\min\left\{(\frac{1}{2}-C\alpha-\frac{1}{2}cor(b_{\textup{LL}},\tilde{\Delta}))\|\tilde{\Delta}\|_{\Sig}+(cor(b_{\textup{LL}},\Delta)-\xi-C\alpha)\|\Delta\|_{\Sig},\right.\\
 &\quad\quad \left.(\frac{1}{2}-C{\alpha}+\frac{1}{2}cor(b_{\textup{LL}},\tilde{\Delta}))\|\tilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}\right\},   
\end{align*}
where $c_1$ is a positive constant given by the derivative of $\Phi(\cdot)$. 
Plugging in the expression of $cor(b_{\textup{LL}},\tilde{\Delta})$ and $cor(b_{\textup{LL}},\Delta)$, we have for the first term of $E^{(wst)}_{\textup{ERM}}-E^{(wst)}_{\textup{LL}}$, it is no smaller than
\begin{align*}
&\frac{1}{2}(1-2C\alpha-\frac{1+\xi t}{\sqrt{1+t^2+2t\xi}})\|\tilde{\Delta}\|_{\Sig}+(\frac{\xi+t}{\sqrt{1+t^2+2t\xi}}-\xi-C\alpha)\|\Delta\|_{\Sig}\\
&\geq \frac{1}{2}\frac{t^2}{(1+t)^2}(1-\xi^2)\|\tilde{\Delta}\|_{\Sig}+\frac{t^2}{(1+t)^2}(1-\xi)\|\Delta\|_{\Sig}-C\alpha(\|\Delta\|_{\Sig}+\|\tilde{\Delta}\|_{\Sig}),
\end{align*}
where the last step is due to the current constraint that $t>\max\{0,-2\xi\}$. For the second term, it is no smaller than
\[
  \|\tilde{\Delta}\|_{\Sig}-\xi\|\Delta\|_{\Sig}-C\alpha(\|\tilde{\Delta}_{\Sig}+\|\Delta\|_{\Sig}).
\]
Notice that $t^2/(1+t^2)\geq \min\{\frac{t^2}{4},\frac{1}{4}\}$. We can show that $t\geq \|\tilde{\Delta}\|_{\Sig}/\|\Delta\|_{\Sig}$, then
\begin{align*}
  E^{(wst)}_{\textup{ERM}}-E^{(wst)}_{\textup{LL}}\geq c_3\min\{(\frac{\|\tilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}} -\xi)\|\Delta\|_{\Sig},(1-\xi)\|\Delta\|_{\Sig},(1-\xi)\|\tilde{\Delta}\|_{\Sig}^2/\|\Delta\|_{\Sig}\}-c_4\alpha(\|\Delta\|_{\Sig}+\|\tilde{\Delta}\|_{\Sig}).
\end{align*}

%%%%%%%%%%%
\subsection{Domain shifts: Proof of Theorem \ref{thm3}}
It still holds that $\widetilde{\Delta}^*=2(\mu^{(0,*)}-\E[x_i^{(\lam)}])=2(\mu^{(0,*)}-\E[\tx_i])$.
It is easy to show that the worst group mis-classification error for this new environment is
\begin{align}
  E^{(wst,*)}_A=\max\left\{\Phi\left(-\frac{\frac{1}{2}(\widetilde{\Delta}^*)^Tb_A}{\sqrt{b_A^T\Sig b_A}}\right),\Phi\left(\frac{\frac{1}{2}(\widetilde{\Delta}^*)^Tb_A-\Delta^Tb_A}{\sqrt{b_A^T\Sig b_A}}\right)\right\},
\end{align}
where $A\in\{\textup{ERM},\textup{mix}, \textup{LL},\textup{LD}\}$.
Notice that
\[
 \widetilde{\Delta}^*=2\mu^{(0,*)}-(\mu^{(0,G)}+\mu^{(1,R)})=\widetilde{\Delta}+\mu^{(0,*)}-\mu^{(0,G)}
\]
We assume $\|\widetilde{\Delta}^*\|_2=\|\widetilde{\Delta}\|_2$.
Let $\xi^*=cor(\Delta,\widetilde{\Delta}^*)$ and $\gamma=cor(\widetilde{\Delta},\widetilde{\Delta}^*)$.
We have
\begin{align*}
    cor(b_{\textup{ERM}},\widetilde{\Delta}^*)&=\frac{\gam\|\widetilde{\Delta}\|_{\Sig}\|\widetilde{\Delta}^*\|_{\Sig}-c_0\xi^*\|\Delta\|_{\Sig}\|\widetilde{\Delta}^*\|_{\Sig}}{\|\widetilde{\Delta}^*\|_{\Sig}\|\widetilde{\Delta}+c_0\Delta\|_{\Sig}}\\
    &= \frac{\gam \|\widetilde{\Delta}\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}\pm \|c_0\Delta\|_{\Sig}}\pm\frac{|c_0\xi^*|\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}\pm \|c_0\Delta\|_{\Sig}}= \gam\pm C\alpha.
\end{align*}

Hence,
\begin{align}
\label{err0-lb}
    E_{\textup{ERM}}^{(wst)}\geq \max\left\{\Phi((\frac{\gam}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi-C\alpha)\|\Delta\|_{\Sig}),\Phi((-\frac{\gam}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig})\right\}
\end{align}
 for some constant $C$ depending on the true parameters.

 Hence,
 \begin{align*}
cor(b_{\textup{LL}},\widetilde{\Delta}^*)&=\frac{ (\widetilde{\Delta}^*)^T b_{\textup{LL}}}{\|\widetilde{\Delta}^*\|_{\Sig}\sqrt{b_{\textup{LL}}^T\Sig b_{\textup{LL}}}} =\frac{\gam\|\widetilde{\Delta}\|_{\Sig}+c_{\textup{LL}}\xi^*\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}}.
\end{align*}

To have $E_{\textup{LL}}^{(wst*)}< E_{\textup{ERM}}^{(wst*)}$, it suffices to require that $(-\frac{\gam}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}< (\frac{\gam}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}$ and
\begin{align*}
   &\frac{1}{2} cor(b_{\textup{LL}},\widetilde{\Delta}^*)\|\widetilde{\Delta}\|_{\Sig}-cor(b_{\textup{LL}},\Delta)\|\Delta\|_{\Sig}\leq (\frac{\gam}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}\\
   &-\frac{1}{2} cor(b_{\textup{LL}},\widetilde{\Delta}^*)\|\widetilde{\Delta}\|_{\Sig}\leq (\frac{\gam}{2}-C\alpha)\|\widetilde{\Delta}\|_{\Sig}-(\xi+C\alpha)\|\Delta\|_{\Sig}.
\end{align*}
A sufficient condition is
\begin{align*}
    & \xi < (\frac{\gam}{2}+\frac{1}{2}cor(b_{\textup{LL}},\widetilde{\Delta}^*))\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha,~cor(b_{\textup{LL}},\Delta)\geq \xi+C\alpha,~~cor(b_{\textup{LL}},\widetilde{\Delta}^*)\leq \gam-2C\alpha.
\end{align*}
We can find that a further sufficient condition is
\begin{align}
   & \xi < \frac{1+\gam}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha, c_{\textup{LL}}> 0, \xi^*\leq \frac{\gam(\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}-\|\widetilde{\Delta}\|_{\Sig})}{c_{\textup{LL}}\|\Delta\|_{\Sig}}-\eps_1\alpha\label{cond1-da}\\
    &\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}\geq \|\widetilde{\Delta}\|_{\Sig},~\xi\leq \frac{c_{\textup{LL}}\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}-\|\widetilde{\Delta}\|_{\Sig}}-\eps_1\alpha\label{cond2-da}\\
   & \xi \leq (\frac{\gam}{2}+\frac{1}{2}cor(b_{\textup{LL}},\widetilde{\Delta}^*))\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha.\label{cond3-da}
\end{align}

We first find sufficient conditions for the statements in (\ref{cond1}) and (\ref{cond2}).
Parameterizing $t=c_{\textup{LL}}\|\Delta\|_{\Sig}/\|\widetilde{\Delta}\|_{\Sig}$, we further simplify the condition in (\ref{cond1-da}) and (\ref{cond2-da}) as
\begin{align*}
   & \xi < \frac{1+\gam}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}}-C\alpha,t>0,~\xi^*\leq \frac{\gam(\sqrt{1+t^2+2t\xi}-1)}{t}-\eps_1\alpha,\\
   &-\frac{t}{2}\leq \xi\leq t,~~\xi\leq \frac{1+\sqrt{1+t^2+2t\xi}}{t+2\xi}-\eps_1\alpha.
\end{align*}
We only need to require
\[
  t\geq \max\{0,-2\xi\}~\text{and}~\xi < \min\{\frac{1+\gam}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},1\}-C\alpha, \xi^*\leq \gam\xi.
\]
Some tedious calculation shows that $t\geq\max\{0,-2\xi\}$ can be guaranteed by
\[
 a_{\textup{LL}}\leq \frac{1}{\|\tilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}}~\text{and}~\xi\leq \frac{\|\Delta\|_{\Sig}}{\|\tilde{\Delta}\|_{\Sig}}
\]
It is left to consider the constraint in (\ref{cond3-da}).
Notice that it holds for any $\xi\leq 0$. When $\xi>0$, we can see
\begin{align*}
cor(b_{\textup{LL}},\widetilde{\Delta}^*)&=\frac{\gam\|\widetilde{\Delta}\|_{\Sig}+\xi^* c_{\textup{LL}}\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}+c_{\textup{LL}}\Delta\|_{\Sig}}=\frac{\gam+t\xi^*}{\sqrt{1+t^2+2t\xi}}\\
&\geq \frac{\gam+t\xi^*}{1+t}.
\end{align*}
Hence, it suffices to guarantee that 
\[
  \xi^*+\gam\geq \frac{2\|\Delta\|_{\Sig}}{\|\widetilde{\Delta}\|_{\Sig}}\xi+C\alpha.
\]
To summarize, it suffices to require
\[
  a_{\textup{LL}}\leq \frac{1}{\|\tilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}},0\leq \xi^*\leq \gam\xi,\xi < \min\{\frac{\gam}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},\frac{\|\Delta\|_{\Sig}}{\|\tilde{\Delta}\|_{\Sig}}\}-C\alpha. 
\]

For LISA-D, we can similarly show that $E^{(wst*)}_{\textup{LD}}< E^{(wst*)}_{\textup{ERM}}$ given that
\[
 a_{\textup{LD}}\leq \frac{1}{\|\tilde{\Delta}\|_{\Sig}^2+\|\widetilde{\Delta}\|_{\Sig}\|\Delta\|_{\Sig}},0\leq \xi^*\leq \gam\xi,\xi < \min\{\frac{\gam}{2}\frac{\|\widetilde{\Delta}\|_{\Sig}}{\|\Delta\|_{\Sig}},\frac{\|\Delta\|_{\Sig}}{\|\tilde{\Delta}\|_{\Sig}}\}-C\alpha. 
\]
