\appendix
\section{Additional Experiments}
\subsection{Additional Experiments on Subpopulation Shifts}
\subsubsection{Dataset Details}
\label{sec:app_sub_data}

\textbf{Colored MNIST (CMNIST)}: We classify MNIST digits from 2 classes, where classes 0 and 1 indicate original digits (0,1,2,3,4) and (5,6,7,8,9). The color is treated as a spurious attribute. Concretely, in the training set, the proportion between red samples and green samples is 8:2 in class 0, while the proportion is set as 2:8 in class 1. In the validation set, the proportion between green and red samples is 1:1 for all classes. In the test set, the proportion between green and red samples is 1:9 in class 0, while the ratio is 9:1 in class 1. The data sizes of train, validation, and test sets are 30000, 10000, and 20000, respectively. Follow~\cite{arjovsky2019invariant}, we flip labels with probability 0.25.

\textbf{Waterbirds}~\citep{sagawa2019distributionally}: The Waterbirds dataset aims to classify birds as ``waterbird" or ``landbird", where each bird image is spuriously associated with the background ``water" or ``land". 
Waterbirds is a synthetic dataset where each image is composed by pasting a bird image sampled from CUB dataset~\citep{WahCUB_200_2011} to a background drawn from the Places dataset~\cite{zhou2017places}. 
The bird categories in CUB are stratified as land birds or water birds. Specifically, the following bird species are selected to construct the waterbird class: albatross, auklet, cormorant, frigatebird, fulmar, gull, jaeger, kittiwake, pelican, puffin, tern, gadwall, grebe, mallard, merganser, guillemot, or Pacific loon. All other bird species are combined as the landbird class. We define (land background, waterbird) and (water background, landbird) are minority groups. There are 4,795 training samples while only 56 samples are ``waterbirds on land" and 184 samples are ``landbirds on water". The remaining training data include 3,498 samples from ``landbirds on land", and 1,057 samples from ``waterbirds on water".

\textbf{CelebA}~\citep{liu2015faceattributes,sagawa2019distributionally}: 
For the CelebA data~\citep{liu2015faceattributes}, we follow the data preprocess procedure from~\cite{sagawa2019distributionally}.
CelebA defines a image classification task where the input is a face image of celebrities and the classification label is its corresponding hair color --  ``blond” or ``not blond.” The label is spuriously correlated with gender, i.e., male or female. In CelebA, the minority groups are (blond, male) and (not blond, female). The number of samples for each group are 71,629 ``dark hair, female'', 66,874 ``dark hair, male", 22,880 ``blond hair, female", 1,387 ``blond hair, male". 


\textbf{CivilComments}~\citep{borkan2019nuanced,koh2021wilds}: 
We use CivilComments from the WILDS benchmark~\citep{koh2021wilds}. 
CivilComments is a text classification task, aiming to predict whether an online comment is toxic or non-toxic. 
The spurious domain identifications are defined as the demographic features, including male, female, LGBTQ, Christian, Muslim, other religion, Black, and White. 
CivilComments contains 450,000 comments collected from online articles. The number of samples for training, validation, and test are 269,038, 45,180, and 133,782, respectively. The readers may kindly refer to Table 17 in~\cite{koh2021wilds} for the detailed group information. 


\subsubsection{Training Details}
\label{sec:app_sub_training}
We adopt pre-trained ResNet-50~\citep{he2016deep} and BERT~\citep{sanh2019distilbert} as the model for image data (i.e., CMNIST, Waterbirds, CelebA) and text data (i.e., CivilComments), respectively. In each training iteration, we sample a batch of data per group. 
For intra-label LISA, we randomly apply mixup on sample batches with the same labels but different domains. For intra-domain LISA, we instead apply mixup on sample batches with the same domain but different labels. The interpolation ratio $\lambda$ is sampled from the distribution $\mathrm{Beta}(2,2)$. All hyperparameters are selected via cross-validation and are listed in Table~\ref{tab:hyperameter_sub}.

\subsubsection{Additional Results}
In this section, we have added the full results of subpopulation shifts in Table~\ref{tab:subpopulation_main_full} and Table~\ref{tab:subpopulation_ablation_full}.

\begin{table}[ht]
    \centering
    \small
    \caption{Hyperparameter settings for the subpopulation shifts.}
    \label{tab:hyperameter_sub}
    \begin{tabular}{l|cccc}
    \toprule
        Dataset & CMNIST & Waterbirds & CelebA & CivilComments  \\
        \midrule
        Learning rate &  1e-3 & 1e-3 &  1e-4 & 1e-5\\
        Weight decay & 1e-4 & 1e-4 & 1e-4 & 0 \\
        Scheduler & n/a & n/a & n/a & n/a \\ 
        Batch size & 16 & 16 & 16 & 8 \\
        Type of mixup & mixup & mixup & CutMix & ManifoldMix \\
        Architecture & ResNet50 & ResNet50 & ResNet50 & DistilBert \\
        Optimizer & SGD & SGD & SGD & Adam  \\
        Maximum Epoch & 300 & 300 & 50 & 3 \\
        Strategy sel. prob. $p_{sel}$ & 0.5 & 0.5 & 0.5 & 1.0\\
        \bottomrule
    \end{tabular}
\end{table}


\begin{table*}[h]
\small
\caption{Full results of subpopulation shifts with standard deviation. All the results are performed with three random seed.}
\label{tab:subpopulation_main_full}
\begin{center}
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c|}{CMNIST} & \multicolumn{2}{c}{Waterbirds} \\
& Avg. & Worst  & Avg. & Worst \\\midrule
ERM & 27.8 $\pm$ 1.9\% & 0.0 $\pm$ 0.0\% & 97.0 $\pm$ 0.2\% & 63.7 $\pm$ 1.9\%\\
UW  & 72.2 $\pm$ 1.1\% & 66.0 $\pm$ 0.7\% &  95.1 $\pm$ 0.3\% & 88.0 $\pm$ 1.3\% \\
IRM & 72.1 $\pm$ 1.2\% &  70.3 $\pm$ 0.8\% &  87.5 $\pm$ 0.7\% & 75.6 $\pm$ 3.1\% \\
IB-IRM & 72.2 $\pm$ 1.3\% & 70.7 $\pm$ 1.2\% & 88.5 $\pm$ 0.6\%  & 76.5 $\pm$ 1.2 \% \\
V-REx & 71.7 $\pm$ 1.2\% & 70.2 $\pm$ 0.9\% & 88.0 $\pm$ 1.0\% & 73.6 $\pm$ 0.2\% \\
Coral & 71.8 $\pm$ 1.7\%  & 69.5 $\pm$ 0.9\% & 90.3 $\pm$ 1.1\% & 79.8 $\pm$ 1.8\% \\
GroupDRO & 72.3 $\pm$ 1.2\% & 68.6 $\pm$ 0.8\% & 91.8 $\pm$ 0.3\% & \textbf{90.6 $\pm$ 1.1\%} \\
DomainMix & 51.4 $\pm$ 1.3\% & 48.0 $\pm$ 1.3\% & 76.4 $\pm$ 0.3\% & 53.0 $\pm$ 1.3\% \\
Fish &  46.9 $\pm$ 1.4\% & 35.6 $\pm$ 1.7\% & 85.6 $\pm$ 0.4\% & 64.0 $\pm$ 0.3\%  \\
\midrule
\textbf{LISA} & 74.0 $\pm$ 0.1\% & \textbf{73.3 $\pm$ 0.2\%} & 91.8 $\pm$ 0.3\% & 89.2 $\pm$ 0.6\%  \\\midrule\midrule
& \multicolumn{2}{c|}{CelebA} & \multicolumn{2}{c}{CivilComments} \\
 & Avg. & Worst & Avg. & Worst \\\midrule
ERM  & 94.9 $\pm$ 0.2\% & 47.8 $\pm$ 3.7\% & 92.2 $\pm$ 0.1\% & 56.0 $\pm$ 3.6\% \\
UW & 92.9 $\pm$ 0.2\% & 83.3 $\pm$ 2.8\% & 89.8 $\pm$ 0.5\% & 69.2 $\pm$ 0.9\% \\
IRM &  94.0 $\pm$ 0.4\% & 77.8 $\pm$ 3.9\% &  88.8 $\pm$ 0.7\% & 66.3 $\pm$ 2.1\% \\
IB-IRM & 93.6 $\pm$ 0.3\% & 85.0 $\pm$ 1.8\% &  89.1 $\pm$ 0.3\% & 65.3 $\pm$ 1.5\%  \\ 
V-REx & 92.2 $\pm$ 0.1\% & 86.7 $\pm$ 1.0\%  & 90.2 $\pm$ 0.3\% & 64.9 $\pm$ 1.2\% \\
Coral &  93.8 $\pm$ 0.3\% & 76.9 $\pm$ 3.6\% & 88.7 $\pm$ 0.5\% & 65.6 $\pm$ 1.3\% \\
GroupDRO & 92.1 $\pm$ 0.4\% & 87.2 $\pm$ 1.6\% & 89.9 $\pm$ 0.5\% & 70.0 $\pm$ 2.0\% \\
DomainMix  & 93.4 $\pm$ 0.1\% & 65.6 $\pm$ 1.7\% & 90.9 $\pm$ 0.4\% & 63.6 $\pm$ 2.5\% \\
Fish &  93.1 $\pm$ 0.3\%  &  61.2 $\pm$ 2.5\% & 89.8 $\pm$ 0.4\% & 71.1 $\pm$ 0.4\%  \\\midrule
\textbf{LISA (ours)} & 92.4 $\pm$ 0.4\% & \textbf{89.3 $\pm$ 1.1\%} & 89.2 $\pm$ 0.9\% & \textbf{72.6 $\pm$ 0.1\%}\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}


\begin{table*}[h]
\small
\caption{Full table of the comparison between LISA and other substitute mixup strategies in subpopulation shifts. UW represents upweighting.}
\vspace{-1em}
\label{tab:subpopulation_ablation_full}
\begin{center}
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c|}{CMNIST} & \multicolumn{2}{c}{Waterbirds} \\
& Avg. & Worst  & Avg. & Worst   \\\midrule
ERM & 27.8 $\pm$ 1.9\% & 0.0 $\pm$ 0.0\% & 97.0 $\pm$ 0.2\% & 63.7 $\pm$ 1.9\%\\
Vanilla mixup & 32.6 $\pm$ 3.1\% & 3.1 $\pm$ 2.4\% &  81.0 $\pm$ 0.2\% & 56.2 $\pm$ 0.2\% \\
Vanilla mixup + UW & 72.2 $\pm$ 0.7\% & 71.8 $\pm$ 0.1\% & 92.1 $\pm$ 0.1\% & 85.6 $\pm$ 1.0\% \\
In-group Group & 33.6 $\pm$ 1.9\% & 24.0 $\pm$ 1.1\% & 88.7 $\pm$ 0.3\%  & 68.0 $\pm$ 0.4\% \\
In-group + UW & 72.6 $\pm$ 0.1\% & 71.6 $\pm$ 0.2\% & 91.4 $\pm$ 0.6\% & 87.1 $\pm$ 0.6\% \\
\midrule
\textbf{LISA (ours)} & 74.0 $\pm$ 0.1\% &\textbf{73.3 $\pm$ 0.2\%} & 91.8 $\pm$ 0.3\% & \textbf{89.2 $\pm$ 0.6\%}  \\
\midrule\midrule
& \multicolumn{2}{c|}{CelebA} & \multicolumn{2}{c}{CivilComments} \\
& Avg. & Worst & Avg. & Worst \\\midrule
ERM & 94.9 $\pm$ 0.2\% & 47.8 $\pm$ 3.7\% & 92.2 $\pm$ 0.1\% & 56.0 $\pm$ 3.6\% \\
Vanilla mixup & 95.8 $\pm$ 0.0\% & 46.4 $\pm$ 0.5\% & 90.8 $\pm$ 0.8\% & 67.2 $\pm$ 1.2\% \\
Vanilla mixup + UW & 91.5 $\pm$ 0.2\% & 88.0 $\pm$ 0.3\% & 87.8 $\pm$ 1.2\% & 66.1 $\pm$ 1.4\%  \\
Within Group & 95.2 $\pm$ 0.3\% & 58.3 $\pm$ 0.9\% & 90.8 $\pm$ 0.6\% & 69.2 $\pm$ 0.8\% \\
Within Group + UW & 92.4 $\pm$ 0.4\%  & 87.8 $\pm$ 0.6\% & 84.8 $\pm$ 0.7\% & 69.3 $\pm$ 1.1\% \\
\midrule
\textbf{LISA (ours)} & 92.4 $\pm$ 0.4\% & \textbf{89.3 $\pm$ 1.1\%} & 89.2 $\pm$ 0.9\% & \textbf{72.6 $\pm$ 0.1\%}\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}


\subsection{Additional Experimental Settings on Domain Shifts}
\subsubsection{Dataset Details}
\label{sec:app_domain_data}

In this section, we provide detailed descriptions of datasets used in the experiments of domain shifts and report the data statistics in Table~\ref{tab:domian_data}.
\paragraph{Camelyon17}
We use Camelyon17 from the WILDS benchmark~\citep{koh2021wilds,bandi2018detection}, which provides $450,000$ lymph-node scans sampled from $5$ hospitals. 
Camelyon17 is a medical image classification task where the input $x$ is a $96\times 96$ image and the label $y$ is whether there exists tumor tissue in the image. The domain $d$ denotes the hospital that the patch was taken from. The training dataset is drawn from the first $3$ hospitals, while out-of-distribution validation and out-of-distribution test datasets are sampled from the $4$-th hospital and $5$-th hospital respectively. 

\paragraph{FMoW}
The FMoW dataset is from the WILDS benchmark~\citep{koh2021wilds,christie2018functional} --- a satellite image classification task which includes $62$ classes and $80$ domains ($16$ years x $5$ regions). Concretely, the input $x$ is a $224 \times 224$ RGB satellite image, the label $y$ is one of the $62$ building or land use categories, and the domain $d$ represents the year that the image was taken as well as its corresponding geographical region -- Africa, the Americas, Oceania, Asia, or Europe. 
The train/test/validation splits are based on the time when the images are taken. Specifically, images taken before 2013 are used as the training set. Images taken between 2013 and 2015 are used as the validation set. Images taken after 2015 are used for testing. 



\paragraph{RxRx1}
RxRx1~\citep{koh2021wilds,taylor2019rxrx1} from the WILDS benchmark is a cell image classification task. In the dataset, some cells have been genetically perturbed by siRNA. The goal of RxRx1 is to predict which siRNA that the cells have been treated with. Concretely, the input $x$ is an image of cells obtained by fluorescent microscopy, the label $y$ indicates which of the $1,139$ genetic treatments the cells received, and the domain $d$ denotes the experimental batches. Here, $33$ different batches of images are used for training, where each batch contains one sample for each class. The out-of-distribution validation set has images from $4$ experimental batches. The out-of-distribution test set has $14$ experimental batches. The average accuracy on out-of-distribution test set is reported.

\paragraph{Amazon}
Each task in the Amazon benchmark~\citep{koh2021wilds,ni2019justifying} is a multi-class sentiment classification task. The input $x$ is the text of a review, the label $y$ is the corresponding star rating ranging from 1 to 5, and the domain $d$ is the corresponding reviewer. The training set has $245,502$ reviews from $1,252$ reviewers, while the out-of-distribution validation set has $100,050$ reviews from another $1,334$ reviewers. The out-of-distribution test set also has $100,050$ reviews from the rest $1,252$ reviewers. We evaluate the models by the 10th percentile of per-user accuracies in the test set.



\paragraph{MetaShift} 
We use the MetaShift~\citep{metadataset}, which is derived from Visual Genome~\citep{krishnavisualgenome}. 
MetaShift leverages the natural heterogeneity of Visual Genome to provide many distinct data distributions for a given class (e.g. “cats with cars” or “cats in bathroom” for the “cat” class). A key feature of MetaShift is that it provides explicit explanations of the dataset correlation and a distance score to measure the degree of distribution shift between any pair of sets.

We adopt the “Cat vs. Dog” task in MetaShift, where we evaluate the model on the “dog(\emph{shelf})” domain with 306 images, and the “cat(\emph{shelf})” domain with 235 images. The training data for the “Cat” class is the cat(\emph{sofa + bed}), including cat(\emph{sofa}) domain and cat(\emph{bed}) domain. MetaShift provides 4 different sets of training data for the “Dog” class in an increasingly challenging order, i.e., increasing the amount of distribution shift. Specifically, dog(\emph{cabinet + bed}), dog(\emph{bag + box}), dog(\emph{bench + bike}), dog(\emph{boat + surfboard}) are selected for training, where their corresponding distances to dog(\emph{shelf}) are 0.44, 0.71, 1.12, 1.43.





\subsubsection{Training Details}
\label{sec:app_domain_training}
Follow WILDS~\citet{koh2021wilds}, we adopt pre-trained DenseNet121~\citep{huang2017densely} for Camelyon17 and FMoW datasets, ResNet-50~\citep{he2016deep} for RxRx1 and MetaShift datasets, and DistilBert~\citep{sanh2019distilbert} for Amazon datasets.

In each training iteration, we first draw a batch of samples $B_1$ from the training set. With $B_1$, we then select another sample batch $B_2$ with same labels as $B_1$ for data interpolation. The interpolation ratio $\lambda$ is drawn from the distribution $\mathrm{Beta}(2,2)$. We use the same image transformers as~\citet{koh2021wilds}, and all other hyperparameters are selected via cross-validation and are listed in Table~\ref{tab:domain_parameter}.



\begin{table}[ht]
    \centering
    \small
    \caption{Hyperparameter settings for the domain shifts.}
    \begin{tabular}{l|ccccc}
    \toprule
        Dataset & Camelyon17 & FMoW & RxRx1 & Amazon & MetaShift \\
        \midrule
        Learning rate & 1e-4 & 1e-4 & 1e-3 & 2e-6 & 1e-3 \\
        Weight decay & 0 & 0 & 1e-5 & 0 & 1e-4 \\
        Scheduler & n/a & n/a & 
        \shortstack{Cosine Warmup} & n/a & n/a \\
        Batch size & 32 & 32 & 72 & 8 & 16 \\
        Type of mixup & CutMix & CutMix & CutMix & ManifoldMix & CutMix \\
        Architecture & DenseNet121 & DenseNet121 & ResNet50 & DistilBert & ResNet50 \\
        Optimizer & SGD & Adam & Adam & Adam & SGD \\
        Maximum Epoch & 2 & 5 & 90 & 3 & 100\\
        Strategy sel. prob. $p_{sel}$ & 1.0 & 1.0 & 1.0 & 1.0 & 1.0\\
        \bottomrule
    \end{tabular}
    \label{tab:domain_parameter}
\end{table}
















\subsection{Strength of Spurious Correlation}
\label{sec:app_spurious_strength}
\yao{In Section~\ref{sec:prelim}, the spurious correlation is defined as the association between the domain $d$ and label $y$, measured by  Cramér's V ~\citep{cramer2016mathematical}. Specifically, let $k_{y,d}$ be the number of samples from domain $d$ with label $y$. The Cramér's V is formulated as 
\begin{equation}
\label{eq:strength_spurious}
    V=\sqrt{\frac{\chi^2}{N\min(|Y-1|, |D-1|)}}=\sqrt{\frac{\sum_{y\in \mathcal{Y}, d\in \mathcal{D}}\frac{(k_{y,d}-\tilde k_{y,d})^2}{\tilde k_{y,d}}}{N\min(|\mathcal{Y}|-1|, |\mathcal{D}|-1|)}},
\end{equation}
where $N$ represents the number of samples in the entire dataset and $\tilde k_{y,d}=\frac{\sum_{y\in \mathcal{Y}} k_{y,d}\sum_{d\in \mathcal{D}}{ k_{y,d}}}{\sum_{y\in \mathcal{Y},d \in \mathcal{D}} k_{y,d}}$. Cramér's V varies from 0 to 1 and higher Cramér's V represents stronger correlation.}

\yao{According to Eq.~\eqref{eq:strength_spurious}, we calculate the strength of spurious correlations on all datasets used in the experiments and report the results in Table~\ref{tab:spurious_strength}. Compared with other datasets, the Cramér's V on Camelyon17, FMoW and RxRx1 are significantly smaller, indicating weaker spurious correlations.}


\begin{table*}[h]
\small
\caption{Analysis of the strength of spurious correlations on datasets with subpopulation shifts or domain shifts.}
\label{tab:spurious_strength}
\begin{center}
\begin{tabular}{cccc|ccccc}
\toprule
\multicolumn{4}{c|}{Subpopulation Shifts} & \multicolumn{5}{c}{Domain Shifts}\\
CMNIST & Waterbirds & CelebA & CivilComments & Camelyon17 & FMoW & RxRx1 & Amazon & MetaShift \\\midrule
0.6000 & 0.8672 & 0.3073 & 0.8723 & 0.0004 & 0.1114 & 0.0067 & 0.3377 & 0.4932 \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}





\subsection{Results on Datasets without Spurious Correlations}
\label{sec:app_no_spurious}
In order to analyze the factors that lead to the performance gains of LISA, we conduct experiments on datasets without spurious correlations. To be more specific, we balance the number of samples for each group under the subpopulation shifts setting. The results of ERM, Vanilla mixup and LISA on CMNIST, Waterbirds and CelebA are reported in Table \ref{tab:app_no_spurious}. The results show that LISA performs similarly compared with ERM when datasets do not have spurious correlations. If there exists any spurious correlation, LISA significantly outperforms ERM. Another interesting finding is that Vanilla mixup outperforms LISA and ERM without spurious correlations, while LISA achieves the best performance with spurious correlations. This finding strengthens our conclusion that the performance gains of LISA are from eliminating spurious correlations rather than simple data augmentation.


\begin{table}[h]
\small
\caption{Results on datasets without spurious correlations. LISA performs similarly to ERM when there are no spurious correlations. However, Vanilla mixup outperforms LISA and ERM when there are no spurious correlations while underperforms LISA on datasets with spurious correlations. The results further strengthen our claim that the performance gains of LISA are not from simple data augmentation.}
\label{tab:app_no_spurious}

\begin{center}
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{l|ccc}
    \toprule
        Dataset & CMNIST & Waterbirds & CelebA \\
        \midrule
        ERM & 73.67\% & 88.07\% & 86.11\% \\
        Vanilla mixup & 74.28\% & 88.23\% & 88.89\% \\
        LISA & 73.18\% & 87.05\% & 87.22\% \\
        \bottomrule
    \end{tabular}
}
\end{center}

\end{table}

\subsection{Additional Invariance Analysis}
\subsubsection{Additional Metrics of Invariant Predictor Analysis}
\label{sec:app_additional_predictor_invariance}
In Table~\ref{tab:additional_invariance_predictor}, we report two additional metrics to measure the invariance of predictors -- Risk Variance and Gradient Norm, which is defined as:
\begin{itemize}[leftmargin=*]
    \item \textbf{Risk Variance ($\mathrm{IP}_{var}$)}. Motivated by~\citet{krueger2021out}, we use the variance of test risks across all domains to measure the invariance, which is defined as $\mathrm{IP}_{var}=\mathrm{Var}(\{\mathcal{R}_1(\theta),\ldots, \mathcal{R}_D(\theta)\})$, where $D$ represents the number of test domains and $\mathcal{R}_d(\theta)$ represents the risk of domain $d$.
    \item \textbf{Gradient Norm ($\mathrm{IP}_{norm}$)}. Follow IRMv1~\cite{arjovsky2019invariant}, we use the gradient norm of the classifier to measure the optimality of the dummy classifier at each domain $d$. Assume the classifier is parameterized by $w$, $\mathrm{IP}_{norm}$ is defined as: $\mathrm{IP}_{norm}=\frac{1}{|\mathcal{D}|}\sum_{d\in \mathcal{D}}\| \nabla_{w|w=1.0} \mathcal{R}_d (\theta)\|^2$.
\end{itemize}


\begin{table}[h]
\small
\caption{Additional Invariance Metrics for Invariant Predictor Analysis. We report the results of risk variance ($\mathrm{IP}_{var}$) and gradient norm ($\mathrm{IP}_{norm}$), where smaller values indicate stronger invariance.}
\label{tab:additional_invariance_predictor}
\begin{center}
\setlength{\tabcolsep}{0.9mm}{
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\toprule
& \multicolumn{4}{c|}{$\mathrm{IP}_{var} \downarrow$} & \multicolumn{4}{c}{$\mathrm{IP}_{norm} \downarrow$} \\\cmidrule{2-9}
  & CMNIST & Waterbirds & Camelyon  & MetaShift & CMNIST & Waterbirds  & Camelyon & MetaShift\\\midrule
ERM  & 12.0486 & 0.2456 & 0.0150 & 1.8824  & 1.1162 & 1.5780  & 1.2959 & 1.0914\\
Vanilla mixup & 0.2769 & 0.1465 & 0.0180 & 0.2659  & 1.5347 & 1.8631 & 0.3993 & 0.1985 \\
IRM & 0.0112 & 0.1243 & 0.0201 & 0.8748 & 0.0908 & 0.9798  & 0.5266 & 0.2320 \\
IB-IRM & 0.0072 & 0.2069 & 0.0329 & 0.5680 & 0.6225 & 0.8814 & 0.6890 & 0.1683\\
V-REx & 0.0056 & 0.1257 & 0.0106 & 0.4220 & 0.0290 & 0.8329 & 0.9641 & 0.3680 \\
\midrule
\textbf{LISA (ours)} & \textbf{0.0012} & \textbf{0.0016} & \textbf{9.97e-5} & \textbf{0.2387} & \textbf{0.0039}  & \textbf{0.0538} & \textbf{0.3081} & \textbf{0.1354} \\ 
\bottomrule
\end{tabular}
}
\end{center}
\end{table}

Comparing LISA to other invariant learning methods, the results of $\mathrm{IP}_{var}$ and $\mathrm{IP}_{norm}$ further confirm that LISA does indeed improve predictor invariance.

\subsubsection{Analysis of Learned Invariant Representations}
\label{app:sec_representation_invariance}
\yao{In this section, we use pairwise divergence of representations ($\mathrm{IR}_{kl}$) to measure representation-level invariance. Specifically, assume the representation before classifier of each sample $(x_i, y_i, d)$ is $h_{i,d}$, we compute the KL divergence of the distribution of representations. Similarly, kernel density estimation is also used to estimate the probability density function $P(h_d^y)$ of representations from domain $d$ with label $y$. Formally, $\mathrm{IR}_{kl}$ is defined as $\mathrm{IR}_{kl}=\frac{1}{|\mathcal{Y}||\mathcal{D}|^2}\sum_{y\in \mathcal{Y}}\sum_{d',d\in \mathcal{D}}\mathrm{KL}(P(h_D^y\mid D=d)|P(h_{D}^y\mid D=d'))$. Smaller $\mathrm{IR}_{kl}$ values indicate more invariant representations with respect to the labels. We report the results on CMNIST, Waterbirds, Camelyon17 and MetaShift in Table~\ref{tab:invariance}. Our key observations are: (1) Compared with ERM, LISA learns stronger representation-level invariance. The potential reason is that a stronger 
invariant predictor implicitly includes stronger invariance representation; (2) LISA provides more invariant representations than other regularization-based invariant predictor learning methods, i.e., IRM, IB-IRM, V-REx, showing its capability in learning stronger invariance.}

\begin{table}[h]
\small
\caption{Results of representation-level invariance $\mathrm{IR}_{kl}$ ($\times 10^8$ for CMNIST), where smaller $\mathrm{IR}_{kl}$ value denotes stronger invariance.}
\centering
\label{tab:invariance}
\begin{tabular}{l|c|c|c|c}
\toprule
  & CMNIST & Waterbirds & Camelyon17 & MetaShift \\\midrule
ERM & 1.683 & 3.592 & 8.213 & 0.632 \\
Vanilla mixup & 4.392 & 3.935 & 7.786 & 0.634 \\
IRM  & 1.905 & 2.413 & 8.169 & 0.627  \\
IB-IRM & 3.178 & 3.306 & 8.824 & 0.646 \\
V-REx & 3.169 & 3.414 & 8.838 & 0.617\\
\midrule
\textbf{LISA (ours)} & \textbf{0.421} & \textbf{1.912} & \textbf{7.570} & \textbf{0.585} \\
\bottomrule
\end{tabular}
\end{table}

\yao{Besides the quantitative analysis, follow Appendix C in~\citet{lee2019meta}, we visualize the hidden representations for all test samples and the decision boundary on Waterbirds and illustrate the results in Figure~\ref{fig:boundary}. Compared with other methods, the representations of samples with the same label that learned by LISA are closer regardless of their domain information, which further demonstrates the promise of LISA in producing invariant representations.}
\begin{figure}[h]
\centering
  \includegraphics[width=0.7\textwidth]{figures/boundary_visualization.pdf}
  \vspace{-0.5em}
  \caption{Visualization of sample representations and decision boundaries on Waterbirds dataset.}\label{fig:boundary}
\end{figure}

\subsection{Full Results of WILDS data}
\label{sec:app_domain_full_results}
Follow~\citet{koh2021wilds}, we reported more results on WILDS datasets in Table~\ref{tab:camelyon_full} - Table~\ref{tab:amazon_full}, including validation performance and the results of other metrics. According to these additional results, we could see that LISA outperforms other baseline approaches in all scenarios. Particularly, we here discuss two additional findings: (1) In Camelyon dataset, the test data is much more visually distinctive compared with the validation data, resulting in the large gap ($\sim10\%$) between validation and test performance of ERM (see Table~\ref{tab:camelyon_full}). However, LISA significantly reduces the performance gap between the validation and test sets, showing its promise in improving OOD robustness; (2) In Amazon dataset, though LISA performs worse than ERM in average accuracy, it achieves the best accuracy at the 10th percentile, which is regarded as a more common and important metric to evaluate whether models perform consistently well across all users~\citep{koh2021wilds}.


\begin{table*}[h]
\small
\caption{Full Results of Camelyon17. We report both validation accuracy and test accuracy.}
\label{tab:camelyon_full}
\begin{center}
\begin{tabular}{l|cc}
\toprule
 & Validation Acc. & Test Acc.\\\midrule
ERM & 84.9 $\pm$ 3.1\% & 70.3 $\pm$ 6.4\%  \\
IRM  & \textbf{86.2 $\pm$ 1.4\%} & 64.2 $\pm$ 8.1\% \\
IB-IRM & 80.5 $\pm$ 0.4\% & 68.9 $\pm$ 6.1\% \\
V-REx  & 82.3 $\pm$ 1.3\% & 71.5 $\pm$ 8.3\% \\
Coral & \textbf{
86.2 $\pm$ 1.4\%} & 59.5 $\pm$ 7.7\% \\
GroupDRO & 85.5 $\pm$ 2.2\% & 68.4 $\pm$ 7.3\% \\
DomainMix & 83.5 $\pm$ 1.1\% & 69.7 $\pm$ 5.5\% \\
Fish & 83.9 $\pm$ 1.2\% & 74.7 $\pm$ 7.1\% \\
\midrule
\textbf{LISA (ours)} & 81.8 $\pm$ 1.3\% & \textbf{77.1 $\pm$ 6.5\%} \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}

\begin{table*}[h]
\small
\caption{Full Results of FMoW. Here, we report the average accuracy and the worst-domain accuracy on both validation and test sets.}
\label{tab:fmow_full}
\begin{center}
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{}  & \multicolumn{2}{c}{Validation} & \multicolumn{2}{c}{Test}\\
& Avg. Acc. & Worst Acc. & Avg. Acc. & Worst Acc. \\\midrule
ERM &  \textbf{59.5 $\pm$ 0.37\%} & 48.9 $\pm$ 0.62\% & \textbf{53.0 $\pm$ 0.55\%} & 32.3 $\pm$ 1.25\% \\
IRM  & 57.4 $\pm$ 0.37\% & 47.5 $\pm$ 1.57\% & 50.8 $\pm$ 0.13\% & 30.0 $\pm$ 1.37\% \\
IB-IRM & 56.1 $\pm$ 0.48\% & 45.0 $\pm$ 0.62\% & 49.5 $\pm$ 0.49\% & 28.4 $\pm$ 0.90\% \\
V-REx  & 55.3 $\pm$ 1.75\% & 44.7 $\pm$ 1.31\% & 48.0 $\pm$ 0.64\% & 27.2 $\pm$ 0.78\%\\
Coral & 56.9 $\pm$ 0.25\% & 47.1 $\pm$ 0.43\% & 50.5 $\pm$ 0.36\% & 31.7 $\pm$ 1.24\% \\
GroupDRO & 58.8 $\pm$ 0.19\% & 46.5 $\pm$ 0.25\% & 52.1 $\pm$ 0.50\% & 30.8 $\pm$ 0.81\% \\
DomainMix & 58.6 $\pm$ 0.29\% & 48.9 $\pm$ 1.15\% & 51.6 $\pm$ 0.19\% & 34.2 $\pm$ 0.76\% \\
Fish & 57.8 $\pm$ 0.15\% & \textbf{49.5 $\pm$ 2.34\%} & 51.8 $\pm$ 0.32\% & 34.6 $\pm$ 0.18\% \\
\midrule
\textbf{LISA (ours)} & 58.7 $\pm$ 0.92\% & 48.7 $\pm$ 0.74\% & \textbf{52.8 $\pm$ 0.94\%} & \textbf{35.5 $\pm$ 0.65\%} \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}


\begin{table*}[h]
\small
\caption{Full Results of RxRx1. ID: in-distribution; OOD: out-of-distribution}
\label{tab:rr1_full}
\begin{center}
\begin{tabular}{l|ccc}
\toprule
 & Validation Acc. & Test ID Acc. & Test OOD Acc.\\\midrule
ERM & 19.4 $\pm$ 0.2\% & 35.9 $\pm$ 0.4\% & 29.9 $\pm$ 0.4\% \\
IRM  & 5.6 $\pm$ 0.4\% & 9.9 $\pm$ 1.4\% & 8.2 $\pm$ 1.1\% \\
IB-IRM & 4.3 $\pm$ 0.7\% & 7.9 $\pm$ 0.5\% & 6.4 $\pm$ 0.6\% \\
V-REx  & 5.2 $\pm$ 0.6\% & 9.3 $\pm$ 0.9\% & 7.5 $\pm$ 0.8\%\\
Coral & 18.5 $\pm$ 0.4\% & 34.0 $\pm$ 0.3\% & 28.4 $\pm$ 0.3\% \\
GroupDRO & 15.2 $\pm$ 0.1\% & 28.1 $\pm$ 0.3\% & 23.0 $\pm$ 0.3\% \\
DomainMix & 19.3 $\pm$ 0.7\% & 39.8 $\pm$ 0.2\% & 30.8 $\pm$ 0.4\% \\
Fish & 7.5 $\pm$ 0.6\% & 12.7 $\pm$ 1.9\% & 10.1 $\pm$ 1.5\% \\
\midrule
\textbf{LISA (ours)} & \textbf{20.1 $\pm$ 0.4\%} & \textbf{41.2 $\pm$ 1.0\%} & \textbf{31.9 $\pm$ 0.8\%}  \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}


\begin{table*}[h]
\small
\caption{Full Results of Amazon. Both the average accuracy and the 10th Percentile accuracy are reported.}
\label{tab:amazon_full}
\begin{center}
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{}  & \multicolumn{2}{c}{Validation} & \multicolumn{2}{c}{Test}\\
& Avg. Acc. & 10-th Per. & Avg. Acc. & 10-th Per. Acc. \\\midrule
ERM &  \textbf{72.7 $\pm$ 0.1\%} & \textbf{55.2 $\pm$ 0.7\%} & 71.9 $\pm$ 0.1\%  & 53.8 $\pm$ 0.8\% \\
IRM  & 71.5 $\pm$ 0.3\% & 54.2 $\pm$ 0.8\% & 70.5 $\pm$ 0.3\% & 52.4 $\pm$ 0.8\% \\
IB-IRM & 72.4 $\pm$ 0.4\% & \textbf{55.1 $\pm$ 0.6\%} & \textbf{72.2 $\pm$ 0.3\%} & 53.8 $\pm$ 0.7\% \\
V-REx  & 72.7 $\pm$ 1.2\% & 53.8 $\pm$ 0.7\% & 71.4 $\pm$ 0.4\% & 53.3 $\pm$ 0.0\% \\
Coral & 72.0 $\pm$ 0.3\% & 54.7 $\pm$ 0.0\% & 70.0 $\pm$ 0.6\% & 52.9 $\pm$ 0.8\% \\
GroupDRO & 70.7 $\pm$ 0.6\% & 54.7 $\pm$ 0.0\% & 70.0 $\pm$ 0.6\% & 53.3 $\pm$ 0.0\% \\
DomainMix & 71.9 $\pm$ 0.2\% & 54.7 $\pm$ 0.0\% & 71.1 $\pm$ 0.1\% & 53.3 $\pm$ 0.0\% \\
Fish & 72.5 $\pm$ 0.0\% & 54.7 $\pm$ 0.0\% & 71.7 $\pm$ 0.1\% & 53.3 $\pm$ 0.0\% \\
\midrule
\textbf{LISA (ours)} & 71.6 $\pm$ 0.4\% & \textbf{55.1 $\pm$ 0.6\%} & 70.8 $\pm$ 0.3\% & \textbf{54.7 $\pm$ 0.0\%} \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}