\section{Conclusion}
To tackle distribution shifts, we propose LISA, a simple and efficient algorithm, to improve the out-of-distribution robustness. LISA aims to eliminate the domain-related spurious correlations among the training set with selective interpolation. We evaluate the effectiveness of LISA on nine datasets under subpopulation shifts and domain shifts settings, demonstrating its promise. Besides, detailed analyses verify that the performance gains caused by LISA result from encouraging learning invariant predictors and representations. Theoretical results further strengthen the superiority of LISA by showing smaller worst-group mis-classification error compared with ERM and vanilla data interpolation. 

While we have made progress in learning invariant predictors with selective augmentation, a limitation of LISA is how to make it compatible with problems in which it is difficult to obtain examples with the same label (e.g., object detection, generative modeling). It would be interesting to explore more general selective augmentation strategies in the future. Additionally, we empirically find that intra-label LISA works without domain information in some domain shift situations. Systematically exploring domain-free intra-label LISA with a theoretical guarantee would be another interesting future direction. 
%To tackle distribution shifts, we propose LISA, a simple and efficient algorithm, to improve the out-of-distribution robustness. LISA aims to eliminate the domain-related spurious correlations among the training set with selective interpolation. We evaluate the effectiveness of LISA on nine datasets under subpopulation shifts and domain shifts settings, demonstrating its promise. Besides, detailed analyses verify that the performance gains caused by LISA result from encouraging learning invariant predictors and representations.