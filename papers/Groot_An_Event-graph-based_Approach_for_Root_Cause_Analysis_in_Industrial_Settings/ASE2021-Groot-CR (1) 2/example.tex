\section{Motivating Examples}

\label{sec:example}

In this section, we demonstrate the effectiveness of event-based graph and adaptive customization strategies with two motivating examples.

\begin{figure*}[t]
\begin{subfigure}[b]{0.37\textwidth}
\centering
  \includegraphics[width=0.98\textwidth]{figures/example1_1.png}
  \caption{Dependency graph}
  \label{fig:ex1_dep}
\end{subfigure}
\hfill %%
\begin{subfigure}[b]{0.4\textwidth}
\centering
  \includegraphics[width=0.98\textwidth]{figures/example1_2.png}
  \caption{Causality graph}
  \label{fig:ex1_cas}
\end{subfigure}
\caption{Motivating example of event causality graph}
\label{fig:example1}
\end{figure*}

Figure~\ref{fig:example1} shows an abstracted real incident example with the dependency graph and the corresponding causality graph constructed by \system. The \emph{Checkout} service of our e-commerce system suddenly gets an additional latency spike due to a code deployment on the \emph{Service-E}. The service monitor is reporting \emph{API Call Timeout} detected by the ML-based anomaly detection system. The simplified sub-dependency graph consisting of 6 services is shown in Figure~\ref{fig:ex1_dep}. The initial alert is triggered on the \emph{Checkout} (entrance) service. The other nodes \emph{Service-*} are the internal services that the \emph{Checkout} service directly or indirectly depends on. The color of the nodes in Figure~\ref{fig:ex1_dep} indicates the severity/count of anomalies (alerts) reported on each service. We can see that \emph{Service-B} is the most severe one as there are two related alerts on it. The traditional graph-based approach~\cite{brandon2020graph,weng2018root} usually takes into account only the graph between services in addition to the severity information on each service. If the traditional approach got applied on Figure~\ref{fig:ex1_dep}, either \emph{Service-B}, \emph{Service-D}, or \emph{Service-E} could be a potential root cause, and \emph{Service-B} would have the highest possibility since it has two related alerts. Such results are not useful to the SRE teams. 

\system constructs the event-based causality graph as shown in Figure~\ref{fig:ex1_cas}. The events in each service are used as the nodes here. %Users can receive the correct root cause recommendation and quick understanding through the visualization. 
We can see that the \emph{API Call Timeout} issue in \emph{Checkout} is possibly caused by \emph{API Call Timeout} in \emph{Service-A}, which is further caused by \emph{Latency Spike} in \emph{DataCenter-A} of \emph{Service-C}. \system further tracks back to find that it is likely caused by \emph{Latency Spike} in \emph{Service-E}, which happens in the same data center. Finally \system figures out that the most probable root cause is a recent \emph{Code Deployment} event in \emph{Service-E}. The SRE teams then could quickly locate the root cause and roll back this code deployment, followed by further investigations.

There are no casual links between events in \emph{Service-B} and \emph{Service-A}, since no causal rules are matched. The \emph{API Call Timeout} event is less likely to depend on the event type \emph{High CPU} and \emph{High GC}. Therefore, the inference can eliminate \emph{Service-B} from possible root causes. This elimination shows the benefit of the event-based graph. Note that there is another event \emph{Latency Spike} in \emph{Service-D}, but not connected to \emph{Latency Spike} in \emph{Service-C} in the causality graph. The reason is that the \emph{Latency Spike} event in \emph{Service-C} happens in \emph{DataCenter-A}, not \emph{DataCenter-B}. %so even although both events are \emph{Latency Spike} and related, they would not connect in \system's causality graph, as they do not happen in the same data center.

\begin{figure}[t]
\centering
\resizebox{0.4\textwidth}{!}{ 
  \includegraphics[width=0.44\textwidth]{figures/example2_1.png}
  }
  \caption{Example of event type addition}
  \label{fig:ex2_n1}
\end{figure}

\begin{figure}[t]
\centering
\resizebox{0.44\textwidth}{!}{ 
  \includegraphics[width=0.44\textwidth]{figures/example2_2.png}
}
  \caption{Example of event and rule update}
  \label{fig:ex2_n2}
\end{figure}

Figures~\ref{fig:ex2_n1} and~\ref{fig:ex2_n2} show how SRE engineers can easily change \system to adapt to new requirements, by updating the events and rules. In Figure~\ref{fig:ex2_n1}, SRE engineers want to add a new type of deployment activity, \emph{ML Model Deployment}. Usually, the SRE engineers first need to select the anomaly detection model or set their own alerts and provide alert/activity data sources for the stored events. In this example, the event can be directly fetched from the ML model management system. Then \system also requires related properties (e.g., the detection time range) to be set for the new event type. Lastly, the SRE engineers add the rules for building the causal links between the new event type and existing ones. The blue box in Figure~\ref{fig:ex2_n1} shows the rule, which denotes the edge direction, target event, and target service (self, upstream, and downstream dependency). 

Figure~\ref{fig:ex2_n2} shows a real-world example of how \system is able to incorporate SRE insights and knowledge. More specifically, SRE engineers would like to change the rules to allow \system to distinguish the latency spikes from different data centers. As an example in Figure \ref{fig:ex1_cas}, \emph{Latency Spike} events propagate only within the same data center. During \system development, SRE engineers could easily add new property \emph{DataCenter} to the \emph{Latency Spike} event. Then they add the corresponding ``conditional'' rules to be differentiated with the ``basic'' rules in Figure~\ref{fig:ex2_n2}. In conditional rules, links are constructed only when the specified conditions are satisfied. %The ``conditional'' rule is based on one ``basic'' rule, but with additional conditions that need to be met. %Otherwise, the link would not be constructed.