\section{Experience}



\system currently supports daily SRE work. Figure~\ref{fig:UI} shows a live \system's ``bird's eye view'' UI on an actual simple checkout incident. Service $C$ has the root cause ($Error Spike$) and belongs to an external provider. Although the domain service $A$ also carries an error spike and gets impacted, \system correctly ignores the irrelevant deployment event, which has no critical impact. The events on $C$ are virtually created based on the dynamic rule. %, an API error spike of $B$, and a business alert. 
Note that all causal links (yellow) in the UI indicate ``is  cause of'', being the opposite of ``is caused by'' as  described in Section~\ref{sec:causality} to provide more intuitive UI  for users to navigate through. \system visualizes the dependency and event causality graph with extra information such as an error message. The SRE teams can quickly comprehend the incident context and derived root cause to investigate \system further. A mouseover can trigger ``event enrichment'' based on the event type to present details such as raw metrics and other additional information.%, or links to other tools.
\begin{figure}[t]
\centering
  \includegraphics[width=0.88\linewidth]{figures/GrootUI.png}
  \caption{\system UI in production}
  \label{fig:UI}
   \vspace{-3.0ex} 
\end{figure}





We next share two major kinds of experience:
%In addition, when mouse over any event, the support data pops out such as error message, figures of time series metrics, or contacts of developer activities. 
\begin{itemize}
   \item \textbf{Feedback from \system users and developers},  reflecting the general experience of two groups: (1) domain SRE teams who use \system to find the root cause, and 
% to investigate and mitigate; 
    (2) a centered SRE team who maintains \system to facilitate new requirements. 
%   \system is designed to be SRE-centric, and directly addressing the requirements of both centered and domain SRE teams is important. 
   \item \textbf{Lessons learned}, representing the lessons learned from deploying and adopting \system in production for the real-world RCA process.
\end{itemize}

\subsection{Feedback from \system Users and Developers}

We invite the SRE members who use \system for RCA in their daily work to the user survey. We call them users in this section. We also invite different SRE members responsible for maintaining \system to the developer survey. We call them developers in this section. In total, there are 14 users and 6 developers\footnote{The \system researchers and developers who are authors of this paper are excluded.} who respond to the surveys. 
%

For the user survey, we ask 14 users the following 5 questions (Questions 4-5 have the same choices as Question 1):
\begin{itemize}
    \item \textbf{Question 1.} When \system correctly locates the root cause, how does it help with your triaging experience? Answer choices: Helpful(4), Somewhat Helpful(3), Not Helpful(2), Misleading(1).
    \item \textbf{Question 2.} When \system correctly locates the root cause, how does it save/extend your or the team's triaging time? (Detection and remediation time not included) Answer choices: Lots Of Time Saved(4), Some Time Saved(3), No Time Saved(2), Waste Time Instead(1).
    \item \textbf{Question 3.} Based on your estimation, how much triage time \system would save on average when it correctly locates the root cause? (Detection and remediation time  not included) Answer choices: More than 50\%(4), 25-50\%(3), 10-25\%(2), 0-10\%(1), N/A(0).
    \item \textbf{Question 4.} When \system correctly locates the root cause, do you find that the result ``graph'' provided by \system helps you understand how and why the incident happens?
    \item \textbf{Question 5.} When \system does not correctly locate the root cause, does the result ``graph'' make it easier for your investigation of the root cause?
\end{itemize}


\begin{figure}
    \centering
    \begin{subfigure}[t]{0.45\linewidth}
    \begin{tikzpicture}[scale=0.45]
  \begin{axis}
    [
    ytick={1,2,3,4,5},
    yticklabels={Question 5, Question 4, Question 3, Question 2, Question 1},
    xmin=0
    ]
    \addplot+[
    boxplot prepared={
      average=3.43,
      upper quartile=4.0,
      lower quartile=3.0,
      upper whisker=4.0,
      lower whisker=2.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.64,
      upper quartile=4.0,
      lower quartile=3.0,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=2.79,
      upper quartile=3.25,
      lower quartile=2.0,
      upper whisker=4.0,
      lower whisker=2.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.71,
      upper quartile=4.0,
      lower quartile=3.0,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.79,
      upper quartile=4,
      lower quartile=3.75,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
  \end{axis}
\end{tikzpicture}
\caption{From 14 \system users}
\label{fig:survey1}
\end{subfigure}
~
\begin{subfigure}[t]{0.45\linewidth}
    \begin{tikzpicture}[scale=0.45]
  \begin{axis}
    [
    ytick={1,2,3,4,5},
    yticklabels={Question 5, Question 4, Question 3, Question 2, Question 1},
    xmin=0
    ]
    \addplot+[
    boxplot prepared={
      average=3.83,
      upper quartile=4,
      lower quartile=3.75,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.5,
      upper quartile=4.0,
      lower quartile=3.0,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.67,
      upper quartile=4.0,
      lower quartile=3.0,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.67,
      upper quartile=4.0,
      lower quartile=3.0,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
    \addplot+[
    boxplot prepared={
      average=3.83,
      upper quartile=4,
      lower quartile=3.75,
      upper whisker=4.0,
      lower whisker=3.0
    },
    ] coordinates {};
  \end{axis}
\end{tikzpicture}
\caption{From 6 \system developers}
\label{fig:survey2}
\end{subfigure}
\caption{Survey results}
 \vspace{-3.0ex} 
\end{figure}

Figure~\ref{fig:survey1} shows the results of the user survey. We can see that most users find \system very useful to locate the root cause. The average score for Question 1 is 3.79, and 11 out of 14 participants find \system very helpful. As for Question 3, \system saves the triage time by 25-50\%. Even in cases that \system cannot correctly locate the root cause, it is still helpful to provide information for further investigation with an average score of 3.43 in Question 5.

For the developer survey, we ask the 6 developers the following 5 questions (Questions 2-5 have the same choices as Question 1):
\begin{itemize}
    \item \textbf{Question 1.} Overall, how convenient is it to change and customize events/rules/domains while using  \system? Answer choices: Convenient(4), Somewhat Convenient(3), Not Convenient(2), Difficult(1).
    \item \textbf{Question 2.} How convenient is it to \emph{change/customize event models} while using \system? 
    \item \textbf{Question 3.} How convenient is it to \emph{add new domains} while using \system? 
    \item \textbf{Question 4.} How convenient is it to \emph{change/customize causality rules} while using \system? 
    \item \textbf{Question 5.} How convenient is it to change/customize \system compared to other SRE tools? 
\end{itemize}

Figure~\ref{fig:survey2} shows the results of the developer survey. Overall, most developers find it convenient to make changes on and customize events/rules/domains in \system. %all questions have very positive responses on average. 5 out of 6 participants find it convenient to make changes on events/rules/domains in \system, while one finds it somewhat convenient. 
% (here domain denotes adding services from different function fields). %We see 3 participants find it somehow convenient in rule customization, indicating that we should make the rule customization easier to use.

\subsection{Lessons learned}
In this section, we share the lessons learned in terms of technology transfer and adoption on using \system in production environments.

\emph{Embedded in Practice.} To build a successful RCA tool in practice, it is important to embed the R\&D efforts in the live environment with SRE experts and users. We have a 30-minute routine meeting daily with an SRE team to manually test and review every site incident. In addition, we actively reach out to the end users for feedback. For example, the users found our initial UI hard to understand. Based on their suggestions, we have introduced alert enrichment with the detailed context of most events, raw metrics, and links to other tools for the next steps. We also make the UI interactive and build user guides, training videos, and sections. As a result, \system has become increasingly practical and well adopted in practice. We believe that R\&D work on observability should be incubated and grown within daily SRE environments. It is also vital to bring developers with rich RCA experience into the R\&D team.

\emph{Vertical Enhancements.} High-confidence and automated vertical enhancements can empower great experiences. \system is enhanced and specialized in critical scenarios such as grouped related %connection stacking 
alerts across services or critical business domain issues, and large-scale scenarios such as infrastructure changes or database issues. Furthermore, the end-to-end automation is also built for integration and efficiency with anomaly detection, RCA, and notification. For notification, domain business anomalies and diagnostic results are sent through communication apps (e.g., slack and email) for better reachability and experience. Within 18 months of R\&D, \system now supports 18 business domains and sub-domains of the company. On average, \system UI supports more than 50 active internal users, and the service sends thousands of results every month. Most of these usages are around the vertical enhancements. 

%\emph{Vertical Enhancement}: During the continued improvements and validations, high accuracy and recurrence scenarios were discovered and enhanced. We created high confidence vertical enhancements that boosted the user's trust and adoption rate. One example here is to use \system for grouped similar connection stacking alerts across the applications or critical domain issues. Thanks to the high customizability of \system, we efficiently enhanced the vertical experiences, such as fine-tuning and specialized events. We then created an end-to-end flow that includes detection, RCA, and notification. Lots of essential incidents with complicated context were supported, such as infra change and database issues. Then, overall efficiency is improved through integrations with other critical site tools. We also attach diagnostic results in the communication apps (e.g., slack, email, and ServiceNow) for better visibility and reachability. Within 18 months of R\&D, we have hundreds of users per month access to the application for 18 domains and sub-domains RCA. We believe for any AIOPs product like \system - vertical enhancements which focus on the domain use cases are essential to success. 


\emph{Data and Tool Reliability.} Reliability is critical to \system itself and requires a lot of attention and effort. For example, if a critical event is missing, \system may infer a totally different root cause, which would mislead users. %the whole causality of an incident may be affected. In this case, it is not helpful that the user can assume only the related metrics/status are fine. 
We estimate the alert accuracy 
%($f1$) 
to be greater than 0.6 in order to be useful. Recall is even more important since \system can effectively eliminate false positive alerts based on the casual ranking. Since there are hundreds of different metrics supported in \system, we spend time to ensure a robust back end by adding partial and dynamic retry logic and high-efficiency cache. \system's unsuccessful cases can be caused by imperfect data, flawed algorithms, or simply code defects. To better trace the reason behind each unsuccessful case, we add a tracing component. Every \system request can be traced back to atomic actions such as retrieving data, data cleaning, and anomaly detection via algorithms.

\emph{Trade-off among Models.} The accuracy and scalability trade-off among anomaly detection models should be carefully considered and tested. In general, some algorithms such as deep-learning-based or ensemble models are more adaptive and accurate than typical ones such as traditional ML or statistical models. However, the former requires more computation resources, operational efforts, and additional system complexities such as training or model fine-tuning. Due to the actual complexities and fast-evolving nature of our context, it is not possible to scale each model (e.g., deep-learning-based models), nor have it deeply customized for every metric at every level. Therefore, while selecting models, we must make careful trade-off in aspects such as accuracy, scalability, efficiency, effort, and robustness. In general, we first set different ``acceptance'' levels by analyzing each event's impact and frequency, and then test different models in staging and pick the one that is good enough. For example, a few alerts such as ``high thread usage'' are defined by thresholds and work just fine even without a model. Some alerts such as  ``service client error'' are more stochastic and require coverage on every metric of every service, and thus we select fast and robust statistical models and actively conduct detection on the fly.  %Business and domain anomaly events can trigger our triaging flow and diagnostics automatically. SRE and monitoring teams spend more time for better and accurate results, such as exercising deep learning approaches, manually correlate critical signals and active improvements for the false positives.

\emph{Phased Incorporation of ML.} In the current industrial settings, ML-powered RCA products still require effective knowledge engineering. Due to the higher complexity and lower ``signal to noise ratio'' of real production incidents, many existing approaches cannot be applied in practice. We believe that the knowledge engineering capabilities can facilitate adoption of technologies such as AIOps. Therefore, \system is designed to be highly customizable and easy to infuse SRE knowledge and to achieve high effectiveness and efficiency. Moreover, a multi-scenario RCA tool requires various and interpretable events from different detection strategies. Auto-ML-based anomaly detection or unsupervised RCA for large service ecosystems is not yet ready in such context.
% resulting in that taking a general ML model is currently not practical in industrial settings.
As for the path of supervised learning, the training data is tricky to label and vulnerable to potential cognitive bias. 
% from SRE engineers
Lastly, the end users often require complete understanding to fully adopt new solutions, because there is no guarantee of correctness. Many recent ML algorithms (e.g., ensemble and deep learning) lack interpretability. Via the knowledge engineering and graph capabilities, \system is able to explain diversity and causality between ML-model-driven and other types of events. Moving forward, we are building a white-box deep learning approach with causal graph algorithms where the causal link weights are parameters and derivable. 
%, while some are manually tuned or detected by rules for certainty and interpretability. For root cause analysis in practice, taking diversified predictive ML anomalies as the major inputs into another ML layer for  root cause analysis, seems too ideal and difficult as mentioned in Section~\ref{sec:intro}. Additionally, there is also a ``chicken-and-egg'' issue for the supervised learning path: collecting training datasets first can be risky and is hard to happen in the SRE domain. Via a graph-based approach, \system is able to explain how diversified and low-level ML events and their connectivity. Also, it effectively and efficiently solve most of the challenge. Lastly, we are more ready for other intelligent solutions with the dataset of 1000+ real production incidents for training, and our AIOPs infrastructure.
