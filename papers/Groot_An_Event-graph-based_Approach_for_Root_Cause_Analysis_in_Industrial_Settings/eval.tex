\section{Evaluation}

We evaluate \system in two aspects: (1) \emph{effectiveness (accuracy)}, which assesses how accurate \system is in  detecting and ranking root causes, and (2) \emph{efficiency}, which assesses how long it takes for \system to derive root causes and conduct end-to-end analysis in action. Particularly, we intend to address the following research questions:
    %\item \emph{User Experience}, which represents how two groups of SREs experience while using \system: domain SREs which use \system to find the root cause to investigate and mitigate, infrastructure SREs which maintain \system to facilitate new requirements.


\begin{itemize}
    \item \textbf{RQ1.} What are the accuracy and efficiency of \system when applied on the collected dataset?
    \item \textbf{RQ2.} How does \system compare with baseline approaches in terms of accuracy?
    \item \textbf{RQ3.} What are the accuracy and efficiency of \system in an end-to-end scenario?
\end{itemize}

% RQ1 aims to see if given a clear triggering event, how would \system perform. RQ2 aims to see whether using adaptive event-driven approach has an advantage compared to baseline in this scenario. RQ3 puts \system in the real time end-to-end scenario, evaluates the \system's efficiency as a whole. 
%2) Conduct a user study to figure out whether \system is actually helpful and user-friendly to the end SRE users.

\subsection{Evaluation Setup}
\label{sec:evalset}
%To measure whether \system can deal with the challenges with industrial settings, we evaluate \system in an environment that is able to represent the real-time large scale distributed system that is running in real world
To evaluate \system in a real-world scenario, we deploy and apply \system  in eBay's e-commerce system that serves more than 159 million active buyers. In particular, we apply \system upon a microservice ecosystem that contains over 5,000 services on three data centers. These services are built on different tech stacks with different programming languages, including Java, Python, Node.js, etc. Furthermore, these services interact with each other by using different types of service protocols, including HTTP, gRPC,  and Message Queue. The distributed tracing of the ecosystem generates 147B traces on average per day.% with 2.8T spans~\cite{opentracing}%The most busy services in the system have more than 50,000 TPS (transactions per second) every day. It is both time-consuming and troublesome for site reliability engineers to manually solve different incidents without the support of tools like \system.  
\subsubsection{Data Set}
\label{sec:dataset}
The SRE teams at eBay help collect a labeled data set containing 952 incidents over 15 months (Jan 2020 - Apr 2021). Each incident data contains the input required by \system (e.g., dependency snapshot and events with details) and the root cause manually labeled by the SRE teams. %To create this data set, 
These incidents are grouped into two categories: 
%we evaluate over 1,500 incidents grouped into two categories: %and around ${40\%}$ cases are missing supported events or caused by external issues (e.g, regional network provider failures). These incidents are categorized as:
\begin{itemize}
\item \emph{Business domain incidents.} These incidents are detected mainly due to their business impact. For example, end users encounter failed interactions, and business or customer experience is impacted, similar to the example in  Figure~\ref{fig:example1}. 
\item \emph{Service-based incidents.} These incidents are detected mainly due to their impact on the service level, similar to the example in Figure~\ref{fig:dynamic_example}.
\end{itemize}

An internal incident may get detected early, and then likely get categorized as a service-based incident or even solved directly by owners without records. On the other hand, infrastructure-level issues or issues of external service providers (e.g., checkout and shipping services) may not get detected until business impact is caused. 

There are 782 business domain incidents and 170 service-based incidents in the data set. For each incident, the root cause is manually labeled, validated, and collected by the SRE teams, who handle the site incidents everyday. For a case with multiple interacting causes, only the most actionable/influential event is labelled as the root cause for the case. These actual root causes and incident contexts serve as the ground truth in our evaluation.


%To trigger the root cause analysis, w
%\begin{enumerate}
%\item\textbf{Business Monitoring}: Business metrics alerts are collected from here. There are around 200+ business time series are detected  by  anomaly detection service every minute.
%\item\textbf{Application Monitoring}:Different application alerts including TPS alerts, Error Spiking alerts, Latency alerts are collected from here. There are around several million time series are detected by  anomaly detection service every minute. 
%\item\textbf{Load Balance Monitoring}: VIP connection number spiking alerts are collected from here. There are around 60k time series are detected by  anomaly detection service every 5 secs. 
%\item\textbf{Deployment system}: Deployment events are collected from here.
%\item\textbf{Site Event system}: Application mark down events and DB mark down events  are collected from here. When a micro-service is not able to talk to its downstream service after retries, it will mark down the downstream service and send a service mark down event to site event system.  Similar to this, when a micro-service is not able to talk to Database, it will mark down the Database and send a DB mark down event to site event system.
%\end{enumerate}

%\subsubsection{Root cause distribution of collected cases}

%\begin{figure}[t]
%\centering
%  \includegraphics[width=\columnwidth]{figures/app_rootcause.png}
%  \caption{Ground-truth RCA distribution of Application-based anomalies}
%  \label{fig:app_rootcause}
%\end{figure}
%\begin{figure}[t]
%\centering
%  \includegraphics[width=\columnwidth]{figures/biz_rootcause.png}
%  \caption{Ground-truth RCA distribution of Business domain anomalies}
%  \label{fig:biz_rootcause}
%\end{figure}

%Figure ~\ref{fig:app_rootcause} and Figure ~\ref{fig:biz_rootcause} shows the root cause distribution of application-based and business domain anomalies in the dataset. From these two charts, we can see that distribution among application-based anomalies is quite diverse while in business domain anomalies, more than 70\% of the anomalies are due to third party errors. We think this imbalance of the distribution among two categories are due to the reason that we cannot monitor third party applications like we do on other parts of the system. So that in the well-monitored applications, the anomalies would be detected earlier by the abnormal metrics before resulting in severe business impact.



\subsubsection{\system Setup}

The \system production system is deployed as three microservices and federated in three data centers with nine 8-core CPUs, 20GB RAM pods each on Kubernetes.

% They are hosted by Gunicorn to scale up. Nginx is used as our routing manager to serve static files and reverse proxying to Gunicorn, and Supervisor to watch the Gunicorn servers in the background and start the processes on reboot.

\subsubsection{Baseline Approaches} 
\label{sec:baseline}
In order to compare \system with other related approaches, we design and implement two baseline approaches for the evaluation:  
\begin{itemize}
    \item \emph{Naive Approach.} This approach directly uses the constructed service dependency graph (Section~\ref{sec:appgraph}). The events are assigned a score by the severeness of the associated anomaly. Then a normalized score for each service is calculated summarizing all the events related to the service. Lastly, the PageRank algorithm is used to calculate the root cause ranking. 
    \item \emph{Non-adaptive Approach.} This approach is not context-aware. It replaces all special rules (i.e., conditional and dynamic ones) with their basic rule versions. Its other parts are identical to \system.
\end{itemize}
The non-adaptive approach can be seen as a baseline for reflecting a group of graph-based approaches (e.g.,  CauseInfer~\cite{chen2014causeinfer} and Microscope~\cite{lin2018microscope}). These approaches also specify certain service-level metrics but lack the context-aware capabilities of \system. Because the tools for these approaches are not publicly available, we implement the non-adaptive approach to approximate these approaches.%of existing graph-based approaches.

\subsection{Evaluation Results}

%Under the settings described in Section~\ref{sec:evalset}, we would be able to evaluate the efficiency and effectiveness of \system in real world scenarios. 

\subsubsection{RQ1}

\label{sec:rq1}

\begin{table}[t]
\centering
\caption{Accuracy of RCA by \system and baselines}
\resizebox{0.9\linewidth}{!}{ 
\begin{tabular}{l|r|r|r|r|r|r|}
\cline{2-7}
                                    & \multicolumn{2}{c|}{\system} & \multicolumn{2}{c|}{Naive} & \multicolumn{2}{c|}{Non-adaptive} \\ \cline{2-7} 
                                    & Top 3        & Top 1       & Top 3         & Top 1      & Top 3         & Top 1 \\ \hline
\multicolumn{1}{|c|}{Service-based}    & 92\%         & 74\%         & 25\%          & 16\%   & 84\% & 62\%       \\ \hline
\multicolumn{1}{|c|}{Business domain} & 96\%         & 81\%        & 2\%          & 1\%   & 28\% & 26\%       \\ \hline
\multicolumn{1}{|c|}{Combined} & 95\%         & 78\%        & 6\%          & 3\%   & 38\% & 33\%       \\ \hline
\end{tabular}
}
 \label{tab:accuracy}
  \vspace{-3.0ex} 
\end{table}

%\begin{figure}[t]
%\centering
%  \includegraphics[width=0.5\textwidth]{figures/app_wrongcause.png}
%  \caption{Ground-truth RCA distribution of Application-based anomalies which \system gives incorrect top-1 result}
%  \label{fig:app_wrongcause}
%\end{figure}
%\begin{figure}[t]
%\centering
%  \includegraphics[width=\columnwidth]{figures/biz_wrongcause.png}
%  \caption{Ground-truth RCA distribution of Business domain anomalies which \system gives incorrect top-1 result}
%  \label{fig:biz_wrongcause}
%\end{figure}

Table~\ref{tab:accuracy} shows the results of applying \system on the collected data set. We measure both top-1 and top-3 accuracy. The top-1 and top-3 accuracy is calculated as the percentage of cases where their ground-truth root cause is ranked within top 1 and top 3, respectively, in \system's results. %All joint rankings must be untied first for fairness. 
\system achieves high accuracy on both incident categories. For example, for business domain incidents, \system achieves 96\% top-3 accuracy.

% While after the causal links construction, the average number of services that are potentially targeted as root cause is reduced to 11.2 and 7.3 for service-based and business domain incidents respectively.
The unsuccessful cases that \system ranks the root cause after top 3 are mostly caused by missing event(s). %Therefore the causality graph is short of necessary causal link(s) or the root cause itself. 
More than one-third of these unsuccessful cases have been addressed by adding necessary events and corresponding rules over time. For example, initially, we had only an event type of general error spike, which mixes different categories of errors and thus causes high false-positive rate. We then have designed different event types for each category of the error metrics (including various internal and client API errors). %We include these cases to reflect a fair setting in production. 
In many cases that \system ranks the root cause after top 1, the labeled root cause is just one of the multiple co-existing root causes. But for fairness,  the SRE teams label only a single root cause in each case. According to the feedback from the SRE teams, \system still facilitates the RCA process for these cases.   %the RCA distribution of these incidents is quite different from the overall ground-truth RCA distribution. More specifically, we find that 

%Every second of an incident RCA process is valuable. 
Our results show that the runtime cost of applying \system is relatively low. For a service-based incident, the average runtime cost of \system is 1.06s while the maximum is 1.69s. For a business domain incident, the average runtime cost is 0.98s while the maximum is 1.14s. %We designed every step as lightweight as possible, for providing timely results in action.
%In order to answer RQ1, we take 219 real production incidents in eBay: 90 App-Search and 129 Domain-Search cases. These cases are handled by the site reliability engineering team or the technical duty officers of eBay. The app-search cases are from the production environment where the trigger anomaly was firstly observed at the application level. The domain-search is built for the most critical business domain - checkout flow. The trigger anomaly of domain search are generated by our ML anomaly detection systems which calculated from different business metrics and customer interaction metrics. Every root cause is manually labeled, and also collect application graph and event context for each ticket. 

%The results are present in table \ref{tab:accuracy}: The \system detects and ranked 100\% and 86\% correct root cause as Top Rank or Top 3 for app-search use cases. For Domain-search are 91\% (Top 1) and 87\% (Top 3). Obviously, our approach is significantly better than the baseline approach. Moreover, the validation for the baseline approach is less strict, the detection is at the application level. For \system accuracy we also restrict on providing the correct root cause event (e.g., code deploy).      

\subsubsection{RQ2}

\label{sec:rq2}

We additionally apply the baseline approaches on the  data set. Table~\ref{tab:accuracy} also shows the evaluation results. The results show that the accuracy of \system is substantially higher than that of the baseline approaches. In terms of the top-1 accuracy, \system achieves 78\% compared with 3\% and 33\% of the naive and non-adaptive approaches, respectively.  In terms of the top-3 accuracy, \system achieves 95\% compared with 6\% and 38\% of the naive and non-adaptive approaches, respectively. 
%In general, \system is 75\% and 45\% more, respectively, in the top-1 accuracy compared with the naive and non-adaptive baselines. \system has even larger advantages in the top-3 accuracy, being 89\% and 57\% more, respectively, compared with the two baselines. For example, \system achieves 81\% in the top-1 accuracy compared with 26\% accuracy of the non-adaptive approach for business domain incidents.%, so that SREs can save time from investigating unrelated services or events.

The naive approach performs worst in all settings, because it blindly propagates the score at service levels.
The accuracy of the non-adaptive approach is much worse for business domain incidents. The reason is that for a business domain incident, it often takes a longer propagation path since the incident is triggered by a group of services, and new dynamic dependencies may be introduced during the event collection, causing more inaccuracy for the non-adaptive approach. %As showed in Section~\ref{sec:rq1}, the causality construction helps to locate the root cause more effectively.  
There can be many non-critical or irrelevant error events in an actual production scenario, aka ``soft'' errors. We suspect that these non-critical or irrelevant events may be ranked higher by the non-adaptive approach since they are similar to injected faults and hard to be distinguished from the actual ones. \system uses dynamic and conditional rules to discover the actual causal links, building fewer links related to such non-critical or irrelevant events for leading to higher accuracy.  

\subsubsection{RQ3}

\begin{table}[]
\centering
\caption{Comparison of \system results on the dataset and end-to-end scenario}
\resizebox{0.95\linewidth}{!}{ 
\begin{tabular}{l|r|r|r|r|}
\cline{2-5}
                                             & \multicolumn{2}{c|}{Service-based} & \multicolumn{2}{c|}{Business Domain} \\ \cline{2-5} 
                                             & Dataset          & End-to-End          & Dataset         & End-to-End         \\ \hline
\multicolumn{1}{|l|}{Top-1 Accuracy}         & 74\%             & 73\%                & 81\%            & 73\%               \\ \hline
\multicolumn{1}{|l|}{Top-3 Accuracy}         & 92\%              & 91\%                & 96\%            & 87\%               \\ \hline
\multicolumn{1}{|l|}{Average Runtime Cost} & 1.06s            & 3.16s               & 0.98s           & 2.98s              \\ \hline
\multicolumn{1}{|l|}{Maximum Runtime Cost} & 1.69s            & 4.56s               & 1.14s           & 3.61s              \\ \hline
\end{tabular}
}
\label{tab:compare}
 \vspace{-3.0ex} 
\end{table}

To evaluate \system under an end-to-end scenario, we apply \system  upon actual incidents in action. Table~\ref{tab:compare} shows the results. The accuracy has a decrease of up to 9 percentage points in the end-to-end scenario, with some failures caused by production issues such as missing data and service/storage failures. In addition, the runtime cost is increased by up to nearly 3 seconds due to the time spent on fetching data from different data sources, e.g., querying the events for a certain time period.

%\system is currently deployed in production, and it helps to detect/confirm the root causes and boost the triaging speed in action for many cases by the time we submit this paper. There is one great case to share: around mid July 2020, there was an issue happened in one of core DB which impacted multiple services across multiple data centers. This caused more than 100 events including more than 40 database mark down events, along with many VIP connection number spike alerts, latency alerts, and error spike alerts happened around the same time. \system was successfully analyzing the root cause to be one problematic DB within a few seconds.

%\subsection{Case Study}
%During the time we put \system integrated into the production environment to work in the real time scenario, we found an interesting case that we would like to share. In mid July 2020, there was an issue happened in one of core DB which impacted multiple applications across multiple data centers. This caused more than 100 events including more than 40 database mark down events, along with many VIP connection number spike alerts, latency alerts, and error spike alerts happened around the same time. \system was successfully analyzing the root cause to be one problematic DB within a few seconds. 
%\system is currently deployed in production and being used for site reliability teams. According to engineers monitoring platform architect, \system explores a new approach to analyze the alerts and events in order to improve observability efficiency. 
%Another comment on \system comes from a senior reliability engineer who now uses \system everyday, "\system fasten our site issue triage a lot. For example, in connection stacking cases, \system clearly shows error chain (connection stacking chain) and points to the last unhealthy pool, which greatly saves our time to check logs of every pool on this chain to find out which pool is the next. And thanks to the \system feature that gathering almost all related events, we can identify which pools or events are related to the ongoing site issue instead of checking every dashboard first to collect pools health information first. Every second improving of issue resolving speed saves thousands of dollars."  

