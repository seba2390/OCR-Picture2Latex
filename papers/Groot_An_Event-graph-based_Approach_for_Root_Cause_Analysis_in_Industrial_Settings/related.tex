\section{Related Work}
\label{sec:related}
%The system recovery process contains two major steps: anomaly detection and root cause analysis. In this section, we would briefly introduce anomaly detection approaches and conduct a literature review on the state of art RCA approaches to point out their limitations in industrial settings.

%\subsection{Anomaly Detection}
%\label{sec:anomalyrelated}

\emph{Anomaly Detection.} Anomaly detection aims to detect potential issues in the system. Anomaly detection approaches using time series data can generally be categorized into three types: (1) batch-processing and historical analysis such as  Surus~\cite{nflxsurus}; (2) machine-learning-based, such as Donut~\cite{xu2018unsupervised}; (3) usage of adaptive concept drift, such as StepWise~\cite{ma2018robust}.

%Anomaly detection aims to detect the potential issues in the system. Anomaly detection approaches using time series data can generally be categorized into three types: The first is processing data in batches and performing historical analysis. These kinds of system has been used in industry such as Netflix's Surus~\cite{nflxsurus}. However it lacks the ability to deal with the real-time scenarios. The second type is based on ML, such as Donut proposed by Xu et al~\cite{xu2018unsupervised}. However, the lack of training data taints the performance of such system. The third type is leveraging adaptive concept drift that may not be robust since they only care about local information~\cite{ma2018robust}.

\system currently uses a combination of manually written thresholds, statistical models, and machine learning (ML) algorithms to detect  anomalies. Since our approach is event-driven, as long as fairly accurate alerts are generated, \system is able to incorporate them. 

%\subsection{Root Cause Analysis}

\emph{Root Cause Analysis.} Traditional RCA approaches (e.g., Adtributor~\cite{bhagwan2014adtributor} and HotSpot~\cite{sun2018hotspot}) find the multi-dimensional combination of attribute values that would lead to certain quality of service (QoS) anomalies. These approaches are effective at discrete static data. Once there are continuous data introduced by time series information, these approaches would be much less effective.

To tackle these difficulties, there are two categories of approaches based on ML and graph, respectively. 

\emph{ML-based RCA.} Some ML-based approaches use features such as time series information~\cite{ma2020diagnosing, weng2018root} and features extracted using textual and temporal information~\cite{zhao2020automatically}. Some other approaches~\cite{xu2018unsupervised} conduct deep learning by first constructing the dependency graph of the system and then representing the graph in a neural network. However, these ML-based approaches face the challenge of lacking training data. Gan et al.~\cite{gan2019seer} proposed Seer to make use of historical tracking data. Although Seer also focuses on the microservice scenario, it is designed to detect QoS violations while lacking support for other kinds of errors. There is also an effort to use unsupervised learning such as GAN~\cite{xu2018unsupervised}, but it is generally hard to simulate large, complicated distributed systems to give meaningful data.

\emph{Graph-based RCA.} A recent survey~\cite{sole2017survey} on RCA approaches categorizes more than 20 RCA algorithms by more than 10 theoretical models to represent the relationships between components in a microservice system.  
Nguyen et al.~\cite{nguyen2013fchain} proposed FChain, which introduces time series information into the graph, but they still use server/VM as nodes in the graph. Chen et al.~\cite{chen2014causeinfer}  proposed CauseInfer, which constructs a two-layered hierarchical causality graph. It applies metrics as nodes that indicate service-level dependency. Schoenfisch et al.~\cite{schoenfisch2018root} proposed to use Markov Logic Network to express conditional dependencies in the first-order logic, but still build dependency on the service level. Lin et al.~\cite{lin2018microscope} proposed Microscope, which targets the microservice scenario. It builds the graph only on service-level metrics so it cannot get full use of other information and lacks customization. Brandon et al. ~\cite{brandon2020graph} proposed to build the system graph using metrics, logs, and anomalies, and then use pattern matching against a library to identify the root cause. However, it is difficult to update the system to facilitate the changing requirements. Wu et al.~\cite{wu2020microrca} proposed MicroRCA, which models both services and machines in the graph and tracks the propagation among them. It would be hard to extend the graph from machines to the concept of other resources such as databases in our paper. 

%Pengfei et al proposed CauseInfer~\cite{chen2014causeinfer}, which constructs a two layered hierarchical causality graph. It applies metrics as nodes which advances service-level dependency. However only considering metrics would restrict the system from getting use of other forms of information which is addressed by the event-based model by \system. Schoenfisch et al proposed to use Markov Logic Network to express conditional dependencies in the first order logic~\cite{schoenfisch2018root}. However their approaches still build dependency on service level. Jinjin et al proposed Microscope~\cite{lin2018microscope} which targets microservices scenario. Despite taking in both communicating and non-communicating dependencies into account, it only builds the graph on service-level metrics so that it cannot get full use of other information and also lacks customization. Brandon et al. proposed an approach that builds the system graph using metrics, logs and anomalies and then use pattern matching against a library to identify the root cause. However it's difficult to update the system to facilitate the changing requirements~\cite{brandon2020graph}.  Li et al. proposed MicroRCA~\cite{wu2020microrca} which models both services and machines in the graph and tracks the propagation among them. It achieves higher accuracy than service-only models but it would be hard to extend the graph from machines to the concept of other resources such as DB in our paper.

As mentioned in Section~\ref{sec:intro}, by using the event graph, \system mainly overcomes the limitations of existing graph-based approaches in two aspects: (1) build a more accurate and precise causality graph use the event-graph-based model; (2) allow adaptive customization of link construction rules to incorporate domain knowledge in order to facilitate the rapid requirement changes in the microservice scenario.
%Our approach would fall into the category of probabilistic graphical model, along with a set of other approaches~\cite{brandon2020graph, schoenfisch2018root}. 

Our \system approach uses a customized page rank algorithm in the event ranking, and can also be seen as an unsupervised ML approach. Therefore, \system is complementary to other ML approaches as long as they can accept our event causality graph as a feature.

\begin{table}[]
\centering
\caption{The scale of experiments in existing RCA approachesâ€™ evaluations (QPS: Queries per second)}
\resizebox{0.45\textwidth}{!}{ 
\begin{tabular}{|l|l|l|l|}
\hline
Approach      & Year   & Scale    & Validated on Real Incidents?                             \\ \hline
FChain~\cite{nguyen2013fchain}        & 2013                  & \textless{}= 10 VMs         & No           \\ \hline
CauseInfer~\cite{chen2014causeinfer}    & 2014             & 20 services on 5 servers       & No       \\ \hline
MicroScope~\cite{lin2018microscope}    & 2018                       & 36 services, $\sim$5000 QPS & No          \\ \hline
APG~\cite{weng2018root}           & 2018                  & \textless{}=20 services on 5 VMs   & No   \\ \hline
Seer~\cite{gan2019seer}          & 2019   & \textless{}=50 services on 20 servers & Partially \\ \hline
MicroRCA~\cite{wu2020microrca}      & 2020                        & 13 services, $\sim$600 QPS     & No       \\ \hline
RCA Graph~\cite{brandon2020graph}     & 2020                          & \textless{}=70 services on 8 VMs  & No    \\ \hline
Causality RCA~\cite{qiu2020causality} & 2020                     & \textless{}=20 services        & No       \\ \hline
\end{tabular}
}
\label{tab:related}
\end{table}

%\begin{tabular}{|l|l|l|l|}
%\hline
%Approach      & Year  & Architecture  & Scale                                \\ \hline
%FChain~\cite{nguyen2013fchain}        & 2013 & VM-based model                   & \textless{}= 10 VMs                    \\ \hline
%CauseInfer~\cite{chen2014causeinfer}    & 2014 & VM-based model                   & 20 services on 5 servers              \\ \hline
%MicroScope~\cite{lin2018microscope}    & 2018 & Kubernetes                       & 36 services, $\sim$5000 QPS           \\ \hline
%APG~\cite{weng2018root}           & 2018 & OpenStack                        & \textless{}=20 services on 5 VMs      \\ \hline
%Seer~\cite{gan2019seer}          & 2019 & Various & \textless{}=50 services on 20 servers \\ \hline
%MicroRCA~\cite{wu2020microrca}      & 2020 & Kubernetes                       & 13 services, $\sim$600 QPS            \\ \hline
%RCA Graph~\cite{brandon2020graph}     & 2020 & DC/OS                            & \textless{}=70 services on 8 VMs      \\ \hline
%Causality RCA~\cite{qiu2020causality} & 2020 & Kubernetes                       & \textless{}=20 services               \\ \hline
%\end{tabular}

\emph{Settings and Scale.} The challenges of operational, scale, and monitoring complexities are observed, especially being substantial in the industrial settings. Hence, we believe that the target RCA approach should be validated at the enterprise scale and against actual incidents for effectiveness. %We find Seer~\cite{gan2019seer} can deal with various languages and frameworks. While both FChain~\cite{nguyen2013fchain} and CauseInfer~\cite{chen2014causeinfer} only take VMs as basic nodes without considering services, the other approaches target at specific architecture. 

Table~\ref{tab:related}  lists  the experimental  settings and scale in existing RCA approaches' evaluations.  All the listed existing approaches are evaluated in a relatively small scenario. 
% MicroScope~\cite{lin2018microscope} runs on a system that has relatively ~1/10 QPS of \system's busiest service (5,000 vs 50,000). However its experiment setting only contains 36 services. 
In contrast, our experiments are performed upon a system containing 5,000 production services on hundreds of thousands of VMs. On average, the sub-dependency graph (constructed in Section~\ref{sec:appgraph}) of our service-based data set is already 77.5 services, more than the total number in any of the listed evaluations. Moreover, 7 out of the 8 listed approaches are evaluated under simulative fault injection on top of existing benchmarks such as RUBiS, which cannot represent real-world incidents;  Seer~\cite{gan2019seer}  collects only the real-world results with no validations. Our data set contains 952 actual incidents collected from real-world settings.