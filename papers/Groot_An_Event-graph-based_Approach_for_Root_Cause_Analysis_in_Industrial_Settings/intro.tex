\section{Introduction}

\label{sec:intro}
Since the emergence of microservice  architecture~\cite{balalaie2016microservices}, it has been quickly adopted by many large companies such as Amazon, Google, and Microsoft.
Microservice architecture aims to improve the scalability, development agility, and reusability of these companies' business systems.
Despite these undeniable benefits, different levels of components in such a system can go wrong 
due to the fast-evolving and large-scale nature of microservices architecture~\cite{balalaie2016microservices}. 
Even if there are minimal human-induced faults in code, the system might still be at risk due to anomalies in hardware, configurations, etc. 
Therefore, it is critical to detect anomalies and then efficiently analyze the root causes of the associated incidents, subsequently helping the system reliability 
engineering (SRE) team take further actions to bring the system back to normal.
%Large-scale distributed systems are playing increasingly important roles nowadays due to the increase of global inter-connectivity.
%The reliability of large-scale distributed system is a major concern for both developers and users since different levels of components in such system can go wrong which would cause significant consequences. Even if there are minimal human-related faults, the system might still be at risk due to anomalies in hardware, configurations, etc. Therefore, it is critical to detect anomalies and then efficiently analyze the root causes of the associated incidents. Then the system reliability engineers (SREs) can take further actions to bring the system back to normal.

In the process of recovering a system, it is critical to conduct accurate and efficient root cause analysis (RCA)~\cite{sole2017survey}, the second one of a three-step process. In the first step, anomalies are detected with alerting mechanisms~\cite{zhao2020automatically, xu2017lightweight, tang2012optimizing} %For example, when a critical business metric value exceeds a certain threshold (e.g., customers'  checkout failure rate exceeds 0.1\%).
based on monitoring data such as logs~\cite{aguilera2003performance,zawawy2010log,nair2015learning,lu2017log,gan2019seer}, metrics/key performance indicators (KPIs)~\cite{mace2015pivot,xu2018unsupervised,ma2019ms, meng2020localizing, wu2020microrca}, or a combination thereof~\cite{kim2013root,wang2019grano}.  In the second step, when the alerts are triggered, RCA is performed to analyze the root cause of these alerts and additional events, and to propose recovery actions from the associated incident~\cite{baek2017cloudsight,da2016monitoring,aguilera2003performance}. RCA needs to consider multiple possible interpretations of potential causes for the incident, and these different interpretations could lead to different mitigation actions to be performed.  
In the last step, the SRE teams perform those mitigation actions and recover the system. 
%In the process of recovering the system, root cause analysis (RCA) is one of the most important steps among the three stages: Detection, Triage, and Remediation. In the detection stage, a common practice is to let developers monitor data such as logs~\cite{aguilera2003performance,zawawy2010log,nair2015learning,lu2017log,gan2019seer}, metrics/Key Performance Indicators (KPIs)~\cite{mace2015pivot,xu2018unsupervised,ma2019ms, meng2020localizing, wu2020microrca}, or a combination thereof~\cite{kim2013root,wang2019grano}, and then set up alerting mechanisms~\cite{zhao2020automatically, xu2017lightweight, tang2012optimizing}. %For example, when a critical business metric value exceeds a certain threshold (e.g., customers'  checkout failure rate exceeds 0.1\%). 
%When these alerts are triggered, the triage stage starts. The triage stage's goal is to analyze the root cause of these alerts and propose mitigating actions to recover~\cite{baek2017cloudsight,da2016monitoring,aguilera2003performance}. At last, the remediation stage comes to perform those mitigating actions and resolves the system incident. RCA happens in the triage stage. Multiple possible interpretations of potential causes for the incident need to be considered in RCA. And these different interpretations could lead to different mitigation actions to be performed. Thus, an accurate and efficient RCA is important~\cite{sole2017survey}.

%As the most complex and time-consuming part of the whole process, root cause analysis (RCA) in the triage stage plays a key role. Both the detection and remediation stages have clear conditions such as abnormal metrics or the feedback of performing mitigating actions, while it is highly likely for the triage stage to face a relatively puzzling situation. Usually multiple possible interpretations of potential causes for the anomalies happen and thus different mitigating actions have to be performed. Therefore, an accurate and efficient RCA framework is crucial for the recovery when anomalies happen. 

Based on our industrial SRE experiences, we find that RCA is difficult in industrial practice due to three complexities, particularly under microservice settings:%~\cite{balalaie2016microservices}:%, which existing approaches~\cite{sole2017survey} cannot handle well:
\begin{itemize}
    \item \textbf{\emph{Operational Complexity}}. For large-scale systems, there are typically centered (aka infrastructure) SRE and domain (aka embedded) SRE engineers~\cite{GoogleSRE}. Their communication is often ineffective or limited under the microservice scenarios due to a more diversified tech stack, granular services, and shorter life cycles than traditional systems. The knowledge gap between the centered SRE team and the domain SRE team gets further enlarged and makes RCA much more challenging. Centered SRE engineers have to learn from domain SRE engineers on how the new domain changes work to update the centralized RCA tools. Thus, adaptive and customizable RCA is required instead of one-size-fits-all solutions. %For example, microservice  architecture can involve more diversified tech stacks with shorter life cycles than a traditional system, resulting in larger knowledge gaps since experience on existing tech stack may not mitigate to the new one. Therefore, modern RCA systems must have better adaptability and customizability. However, none of existing approaches~\cite{sole2017survey} provide an easy way to achieve this purpose. 
        
    \item \textbf{\emph{Scale Complexity}}. There could be thousands of services simultaneously running in a large microservice system, resulting in a very high number of monitoring signals. A real incident could cause numerous alerts to be triggered across services. The inter-dependencies and incident triaging between the services are proportionally more complicated than a traditional system~\cite{wu2020microrca}. To detect root causes that may be distributed and many steps away from an initially observed anomalous service, the RCA approach must be scalable and very efficient to digest high volume signals. %Due to the enlarged graph size and complicated dependencies, a lot of existing graph-based approaches tend to construct the causality graph based on static information such as call graph~\cite{meng2020localizing, kim2013root}, which may cause inaccuracy against the complicated dependencies in such scenario.
    
    \item \textbf{\emph{Monitoring Complexity}}. A high quantity of observability data types (metrics, logs, and activities) need to be monitored, stored, and processed, such as intra-service and inter-service metrics. Different services in a system may produce different types of logs or metrics with different patterns. There are also various kinds of activities, such as code deployment or configuration changes. The RCA tools must be able to consume such highly diversified and unstructured data and make inferences. %When we build RCA system in practice, these types are hard to be managed and consumed structurally and effectively. 
    %On the other hand, approaches based on logs analysis would be hard to apply to new tech stack due to the frequent production of different log types. Especially those approaches~\cite{gan2019seer, nair2015learning} that involve learning algorithms may require extra time or larger training data set to deal with the new data type, which would be a burden for a real-time system.


    %\item \textbf{Agile Development with Heterogeneous technology stacks} Microservices architecture allows developers to adapt new technologies to boost productivity and performance, but it also increases the number of different tech stacks involved in the system which requires root cause analysis system to be more flexible. Detecting root causes that are distributed in a domain, or many steps away from an initial-observed anomaly service is harder than ever. This is because of the dynamic services and their dependencies under agile development, as well as the domain knowledge gap from many different decoupled services owners.


\end{itemize}


% The existing approaches~\cite{sole2017survey,meng2020localizing, kim2013root} have limited effectiveness in industrial settings due to the aforementioned complexities,
% and the existing work lacks reports of industrial RCA experiences (as shown in Section~\ref{sec:related}).
% (1) In terms of the operational complexity, few existing approaches~\cite{nguyen2013fchain,chen2014causeinfer,brandon2020graph} have good adaptability toward diversified tech stacks and customizability for different domain contexts and requirements.
% (2) In terms of the scale complexity, most of the related work~\cite{nguyen2013fchain,chen2014causeinfer,weng2018root,qiu2020causality} is evaluated in only a small-scale setting with a few services and without real incidents. For real large-scale system incidents, the inter-dependencies become much more complicated. RCA needs to be event-driven and context-aware, yet not supported by existing graph-based 
% approaches ~\cite{meng2020localizing, kim2013root}, due to the use of static call graphs.
% %  In addition, as the scale goes up, the dependencies among different services or other components in the system also get more complicated. For example, existing graph-based approaches typically construct a causality graph based on static information such as a call  graph~\cite{meng2020localizing, kim2013root}, which may suffer from inaccuracy against the complicated dependencies in industrial settings.
% (3) In terms of the monitoring complexity, existing approaches~\cite{aguilera2003performance,zawawy2010log,nair2015learning,lu2017log,gan2019seer} based on log analysis may not be adaptable to frequent context changes caused by fast-evolving tech stacks, such as different log data types. Especially those approaches~\cite{gan2019seer, nair2015learning} that involve learning algorithms may suffer from severe model drift or scalability issues, imposing runtime-overhead burden for a real-time system. (4) None of the existing work~\cite{sole2017survey} reports industrial RCA experiences, despite the high importance of RCA technology transfer and adoption. 
%These approaches usually capture information about the state of the system by instrumentation~\cite{aguilera2003performance, mace2015pivot, gan2019seer} or monitoring metrics~\cite{mace2015pivot,xu2018unsupervised,ma2019ms, meng2020localizing, wu2020microrca}. Then with techniques such as machine learning~\cite{nair2015learning,gan2019seer,xu2018unsupervised} or heuristics~\cite{wu2020microrca,brandon2020graph,meng2020localizing}, these approaches abstract the RCA problem into logical  constraints~\cite{zawawy2010log, mace2015pivot} or a dependency/causality graph~\cite{brandon2020graph, wu2020microrca, chen2014causeinfer, gan2019seer, schoenfisch2018root, aguilera2003performance, ma2019ms}. Graph models are popular since they can represent the dependencies/causalities between different components in a system. Existing work~\cite{aguilera2003performance,brandon2020graph,schoenfisch2018root,marvasti2013anomaly,weng2018root,wu2020microrca} has already attempted approaches based on probabilistic graphical models to describe the states of the system. These approaches often construct a dependency graph to represent the relationship between different services/applications in the system. Then these approaches run different inference algorithms over the graph such as using statistical models~\cite{schoenfisch2018root} or heuristics to calculate similarity scores~\cite{wu2020microrca,brandon2020graph} to calculate the possibility of each node to be the root cause.


%For example, Azure, one of the world's largest cloud service platform, experienced six major outages in June and July 2020~\cite{AzureHistory}. More importantly, these reliability issues are devastating. In March 2017, S3, another one of the largest cloud service platforms, experienced several hours of outage. This outage is estimated to cause more than \$150 million in revenue loss of related US companies~\cite{S3Outage}.

%Given the large impact caused by these anomalies, preventing them from happening would be ideal. However, prevention would be hard, if not impossible. As we can see in Amazon's official report of the preceding incident, the incident was caused by an S3 team member when she incorrectly entered one of the inputs to the commands, being part of a routine operational step~\cite{S3summary}, although Amazon added new safeguards to prevent such incident from happening again. It would be hard to predict similar issues due to hindsight bias as there could always be situations that are not considered by the developers in system design. Thus, it is important to efficiently detect and diagnose the anomalies and further take actions to bring the system back to normal.

%Moreover, as microservices aim to provide agility in software development~\cite{balalaie2016microservices,larrucea2018microservices}, new tech stack or infrastructure is introduced in the system frequently. In such scenario, experience on existing tech stack may or may not mitigate to the new one. SREs who use the RCA system will have to learn from how the new things work and use such domain knowledge to update the RCA system, for example, what metrics should be focused on the services using new tech stacks or what kind of new events can we mine from the logs data. However, none of the existing approaches provide an easy way for SREs to easily plug in their domain knowledge to allow the RCA system adapt to new tech stacks. 

%In the mean time, although quite some models and approaches have been proposed on RCA~\cite{sole2017survey,ma2020diagnosing,schoenfisch2018root,brandon2020graph,yoon2016dbsherlock,jeyakumar2019explainit,jayathilaka2017performance,zhao2020automatically,marvasti2013anomaly,weng2018root}, in large-scale distributed systems, RCA still poses two major challenges:
%\begin{itemize}
    %\item \textbf{The complexity of the architecture and the large scale of the system.} To achieve the availability and scalability to serve a large number of users, a real-world distributed system often contains thousands of different both logical (e.g., service, application\footnote{In microservices, a service represents a basic process that communicates over the Internet, while an application often consists of a set of services. In our paper, we treat service as synonym of application, and they both serve as a basic component in the application that communicates with other services/applications. We use the item \emph{pool} to represent this idea. By this design, our framework can serve both the microservices scenario and traditional cloud platforms.}, etc.) and physical components (e.g., virtual machine, data center etc.)~\cite{wang2019grano}. The system may not be fully accessible for system developers as it may contain external components provided by third-party developers. It would be time consuming and troublesome to find the root cause due to the large scale and complexity.
    %\item \textbf{The diversity of metrics and events.} There are a lot of metrics of different categories monitored in the system, such as performance metrics (latency, CPU usage or transactions per second (TPS), etc.), status metrics or events (the Java exceptions or HTTP errors threw by applications, etc.), and developer activities (code deploy or configuration change). These different metrics may require different detection strategies and could lead to different mitigating actions.
    %\item \textbf{The dynamic events and real-time demands of the system }
%\end{itemize}

To overcome the limited effectiveness of existing approaches~\cite{sole2017survey,nguyen2013fchain,chen2014causeinfer,ma2020diagnosing,schoenfisch2018root,brandon2020graph,yoon2016dbsherlock,jeyakumar2019explainit,jayathilaka2017performance,zhao2020automatically,marvasti2013anomaly,weng2018root,qiu2020causality,meng2020localizing, kim2013root} (as mentioned in Section~\ref{sec:related}) in industrial settings due to the aforementioned complexities, we propose \system, an event-graph-based RCA approach. %To the best of our knowledge, \system is the first RCA approach with the ability to adapt for diversified events, the scalability for real-world settings and the manageability for contextual information.
In particular, \system constructs an event causality graph, whose basic nodes are monitoring events such as performance-metric deviation events, status change events, and developer activity events. These events carry detailed information to enable accurate RCA. The events and the causalities between them are constructed using specified rules and heuristics (reflecting domain knowledge). In contrast to the existing fully learning-based approaches~\cite{gan2019seer, ma2020diagnosing, zhao2020automatically}, \system provides better transparency and interpretability. Such interpretability is critical in our industrial settings because a graph-based approach can offer visualized reasoning with causality links to the root cause and details of every event instead of just listing the results. Besides, our approach can enable effective tracking of cases and targeted detailed improvements, e.g., by enhancing the rules and heuristics used to construct the graph. 

\system has two salient advantages over existing graph-based approaches:
\begin{itemize}
\item \emph{\textbf{Fine granularity}} (events as basic nodes). First, unlike existing graph-based approaches, which directly use services~\cite{brandon2020graph} or hosts (VMs)~\cite{weng2018root} as basic nodes, \system constructs the causality graph by using monitoring events as basic nodes. Graphs based on events from the services can provide more accurate results to address the monitoring complexity. Second, for the scale complexity, \system can dynamically create hidden events or additional dependencies based on the context, such as adding dependencies to the external service providers and their issues. Third, to construct the causality graph, \system takes the detailed contextual information of each event into consideration for analysis with more depth. Doing so also helps \system incorporate SRE insights with the context details of each event to address the operational complexity.
\item \emph {\textbf{High diversity}} (a wide range of event types supported). First, the causality graph in \system supports various event types such as performance metrics, status logs, and developer activities to address the monitoring complexity. This multi-scenario graph schema can directly boost the RCA coverage and precision. For example, \system is able to detect a specific configuration change on a service as the root cause instead of performance anomaly symptoms, thus reducing triaging efforts and time-to-recovery (TTR). Second, \system allows the SRE engineers to introduce different event types that are powered by different detection strategies or from different sources. For the rules that decide causality between events, we design a grammar that allows easy and fast implementations of domain-specific rules, narrowing the knowledge gap of the operational complexity. Third, \system provides a robust and transparent ranking algorithm that can digest diverse events, improve accuracy, and produce results interpretable by visualization. 
%nstead of the commonly used service~\cite{brandon2020graph} or hosts (VM)~\cite{weng2018root} as the basic nodes of the causality graph, \system goes one level deeper and further, by using monitoring events of these system entities as basic nodes to better represent the complicated incident contexts, as well as precise and accurate root cause detection. The reason for using events as nodes is that each service/application could have multiple related events, and constructing edges between events would help the system to be more accurate among the complicated dependencies as some issues would be created via a ``dynamic'' dependency or propagated via a ``conditional'' dependency. Furthermore, when we construct the causality graph, \system uses rule-based strategies to take the detailed context information of each event into consideration when deciding whether to add a certain causality link. We show an example of such rule in Section~\ref{sec:example}. These two techniques help \system generate more accurate graph representation of the system. %Furthermore, our study in Section~\ref{sec:rq2} also shows that the event-level results and visualization has better interpretability for our end users.
%\item \textbf{Adaptive Customization}. To deal with high velocity development in microservice scenarios, our event-driven causality graph allows a wide range of event types including performance metrics, status logs, and developer activities. \system gives the SREs who maintain the RCA system the ability to define and customize the needed event types into the system and thus use different anomaly detection algorithms (e.g., ML, Statistical, Rule-based) and mitigating strategies against different events. For the rules deciding causality between events, we design the grammar that allows the SREs to easily customize domain-specific rules from their expertise. Furthermore, since we are allowing events such as developer activities (code deployment, etc.), our analysis could directly report the root cause to be a certain service developer activity so the developers of that service can quickly take actions. %into the code change causing the anomaly. 
\end{itemize}

To demonstrate the flexibility and effectiveness of \system, we evaluate it on eBay's production system that serves more than \textbf{159} million active users and features more than \textbf{5,000} services deployed over three data centers. %The TPS (transactions per second) of the most busy services in the system is more than 50,000 everyday, and the distributed tracing of the system generates 147B traces with 2.8T spans~\cite{opentracing} on average per day. 
We conduct experiments on a labeled and validated data set to show that \system achieves 95\% top-3 accuracy and 78\% top-1 accuracy for 952 real production incidents collected over 15 months. %\system outperforms two baseline approaches by 0.65/0.72 and 0.24/0.29, respectively.
Furthermore, \system is deployed in production %of a Fortune 500 e-commerce company 
for real-time RCA, and is used daily by both centered and domain SRE teams, with the achievement of 73\% top-1 accuracy in action. 
Finally, the end-to-end execution time of \system for each incident in our experiments is less than 5 seconds, demonstrating the high efficiency of \system.

We report our experiences and lessons learned when using \system to perform RCA in the industrial e-commerce system. We survey among the SRE users and developers of \system, who find \system easy to use and helpful during the triage stage. Meanwhile, the developers also find the \system design to be desirable to make changes and facilitate new requirements. We also share the lessons learned from adopting \system in production for SRE in terms of technology transfer and adoption.

In summary, this paper makes four main contributions:
\begin{itemize}
    \item An event-graph-based approach named \system for root cause analysis tackling challenges in industrial settings.
    \item Implementation of \system in an RCA framework for allowing the SRE teams to instill domain knowledge.
    \item Evaluation performed in eBay's production environment with more than 5,000 services, for demonstrating \system's effectiveness and efficiency.
    \item Experiences and lessons learned when deploying and applying \system in production.
\end{itemize}


