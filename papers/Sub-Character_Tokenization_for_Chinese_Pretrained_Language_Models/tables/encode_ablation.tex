\begin{table*}[t]
    \centering
    \small
    \addtolength{\tabcolsep}{-2pt}
    \setlength{\tabcolsep}{3.5pt}{
    \begin{tabular}{ lcccccccc }
    \toprule             & TNEWS & IFLY  & BQ    & WSC   & AFQMC & CSL   & OCNLI & AVG \\
    \hline
    SubChar-Pinyin       & 63.68 & 58.81 & 81.74 & 65.90 & 68.89 & 82.87 & 67.98 & 70.16 \\
    SubChar-Zhuyin       & 64.91 & 59.39 & 81.41 & 62.72 & 69.14 & 82.60 & 69.12 & 69.90 \\
    SubChar-Stroke       & 64.26 & 55.44 & 81.52 & 62.06 & 69.88 & 83.16 & 68.98 & 69.33 \\
    SubChar-Wubi         & 63.81 & 58.74 & 81.55 & 64.61 & 69.66 & 82.44 & 68.02 & 69.90 \\
    SubChar-Zhengma      & 63.86 & 59.51 & 81.59 & 63.27 & 70.47 & 82.91 & 69.03 & 70.09 \\
    SubChar-Cangjie      & 64.10 & 57.77 & 81.98 & 62.39 & 68.95 & 82.60 & 68.46 & 69.46 \\
    SubChar-Byte         & 63.58 & 59.55 & 81.65 & 63.60 & 68.60 & 82.66 & 67.93 & 69.65 \\
    SubChar-RandomIndex  & 64.11 & 59.16 & 81.64 & 63.93 & 68.53 & 82.86 & 69.39 & 69.95 \\
    \midrule
    SubChar-Pinyin (BPE) & 63.86 & 58.84 & 82.12 & 65.57 & 69.86 & 82.86 & 68.57 & 70.24 \\
    \bottomrule
    \end{tabular}}
    \caption{Results of SubChar tokenizers when using different encoding methods. The last row is a model with SubChar-Pinyin tokenizer using BPE as the subword tokenization algorithm, all previous rows are using unigram LM as the subword tokenization implementation. All models have 6-layers with the same hyper-parameters. The impact of different encoding methods on downstream performance is small, and the ULM and BPE versions of SubChar-Pinyin also achieve similar results.}
    \label{tab:encode_ablation}
    \end{table*}

