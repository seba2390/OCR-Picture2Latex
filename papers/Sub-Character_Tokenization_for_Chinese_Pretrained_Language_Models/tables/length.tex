\begin{table}[t]
    \centering
    \small
    \addtolength{\tabcolsep}{-4pt}
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{ lcc }
    \toprule 
     & iFLYTEK
     & TNEWS
     \\
\hline
CharTokenizer & 289.0 & 22.0 \\
Sub-word & 255.2 & 20.1 \\
% SubChar-Stroke & 228.1 & 18.3 \\
SubChar-Wubi & 183.2  & 15.8 \\
% SubChar-Zhengma & 208.1 & 18.0 \\
% SubChar-Cangjie & 183.0 & 15.8 \\
% SubChar-Zhuyin & 203.6 & 17.6 \\
SubChar-Pinyin & 185.2  &  16.1 \\
SubChar-Pinyin-NoIndex & \textbf{175.4} & \textbf{15.2} \\
% \midrule
% SubChar-Pinyin (BPE) & 184.4 & 15.9 \\
% SubChar-Wubi-CWS & 212.7 & 17.3 \\
% SubChar-Pinyin-CWS & 209.1 & 17.0 \\
% SubChar-Byte & 207.2 & 16.5 \\
% SubChar-RandomIndex & 216.4 & 17.0 \\
    \bottomrule
    \end{tabular}}
    \caption{Comparison of average length of tokenized sequences with different tokenizers. 
    % The last row uses BPE and others use unigram LM for vocabulary construction.
    SubChar tokenizers produce much shorter tokenized sequences than the baselines. SubChar-Pinyin-NoIndex tokenizer achieves the most length reduction. BPE and Unigram LM counterparts achieve similar speedup improvement. }
    \label{tab:output_lengths}
    \end{table}