\begin{table}[t]
    \centering
    \small
    \addtolength{\tabcolsep}{-2pt}
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{ lc }
    \toprule 
      & Tokenized Corpus Size 
     \\
\hline
CharTokenizer & 100.0\% \\
Sub-word & 91.4\%  \\
SubChar-Wubi & 77.2\% \\
SubChar-Pinyin & 78.4\% \\
SubChar-Pinyin-NoIndex & \textbf{74.7\%} \\

    \bottomrule
    \end{tabular}}
    \caption{Relative size (disk memory) of the tokenized pretraining corpus with different tokenizers. SubChar tokenizers produce much smaller tokenized corpus due to their ability to tokenize inputs into shorter sequences.}
    \label{tab:compression}
    \end{table}