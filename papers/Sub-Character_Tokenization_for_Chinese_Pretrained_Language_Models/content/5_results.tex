In this section, we present the experiment results and the main findings. We not only evaluate on a wide range of common Chinese NLU datasets, but also perform robustness evaluation on both synthetic and real-world noisy data. 

\subsection{Standard Evaluation}
\label{sec:standard_eval}

% For each tokenizer, we pretrain a 6-layer and 12-layer BERT style model using the Baidu Baike corpus~\cite{CPM} which has 2.2G of raw text before processing. 
% The model configuration is exactly the same for all models: 6 or 12 layers, 12 attention heads, intermediate size 3072, hidden size 768.
% For pretraining, we follow the original BERT paper's two-stage pretraining procedure where we first train with sequence length 128 for 8k steps and then train with sequence length 512 for 4k steps.  We only keep the masked language modeling objective in pretraining and discard the next sentence prediction objective as suggested in RoBERTa~\cite{RoBERTa}.

% During fine-tuning, we use the set of hyperparameters as shown in Table~\ref{tab:hyper_parameters}.
% For all experiments in this paper, we report results of the average run of three different random seeds.

We compare models trained with our SubChar tokenizers and the baseline tokenizers. There are multiple possible encoding methods for SubChar tokenizers as described in section~\ref{sec:method}. In this section, we choose two representative ones: \textit{Wubi} (glyph-based) and \textit{Pinyin} (pronunciation-based). We later show a full ablation of all different encoding methods in section~\ref{sec:encoding}.


% We train the baseline and proposed tokenizers and then use the tokenizers for training the corresponding BERT models, which are evaluated on the downstream datasets. In Table~\ref{tab:main_results}, we compare the performance of four BERT models, trained with Character Tokenizer (CharTokenizer), SentencePiece Tokenizer (SentencePiece), and our Sub-Character Tokenizer with Wubi (SubChar-Wubi) and Pinyin (SubChar-Pinyin) encodings.

% We compare the results of the baseline tokenizers (BERT-Chinese, \sentp{}-\ulm{}) with our proposed \jiezi{} (including four variants: \jiezi{}-Cangjie, \jiezi{}-Stroke, \jiezi{}-Zhengma, \jiezi{}-Wubi) and \shuowen{} (including two variants: \shuowen{}-Pinyin and \shuowen{}-Zhuyin) tokenizers in Table~\ref{tab:main_results}. 

Table~\ref{tab:main_results} shows the performance of BERT models with different tokenizers on downstream datasets. 
%
Examining the results of the 6-layer BERT models pretrained on the 2.3G Baidu Baike corpus, we observe that
%
despite some variation across different datasets, our proposed sub-character tokenizers can match the baselines on downstream datasets.
When scaling the 6-layer models to 12-layer, we observe moderate improvement on the average performance (70.75 $\rightarrow$ 72.23 for CharTokenizer and 71.42 $\rightarrow$ 72.87 for SubChar-Pinyin). 
% Notably, for the 6-layer model size, our \jiezi{}-Cangjie tokenizer obtains an average of 0.40 points over the BERT-Chinese tokenizer, for the 12-layer model size, our \shuowen{}-Zhuyin tokenizer achieves an average of 0.58 points of improvement over the BERT-Chinese tokenizer.
Besides, we discuss the impact of pretraining data size in section~\ref{sec:pretraining-data}.
These results demonstrate that on standard NLU benchmarks, our proposed tokenizers can serve as a very strong alternative.

\input{tables/noise_phonology.tex}

\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{figures/pinyin-input.png}
\caption{An actual interface of the popular \textit{pinyin} input method. The first line \textit{\underline{yi yi}} is the user input of the romanization sequence, all words with this same pronunciation are listed below for users to choose from.}
\label{fig:pinyin}
\end{figure}



\begin{figure}[t]
\centering
\includegraphics[width=0.49\textwidth]{figures/WubiBERT-Fig3.png}
\caption{Illustration of how our SubChar-Pinyin-NoIndex tokenizer is robust to any homophone typos. The possible homophone typos (characters in purple dashed boxes) are mapped into the same romanization sequence as the intended correct characters, and hence the resultant tokenization based on the romanized sequences would be the same.}
\label{fig:no_index}
\end{figure}


\subsection{Robustness Evaluation}
\label{sec:noisy_eval}

Apart from evaluating on the standard benchmarks, we also verify whether our proposed tokenization methods are better at handling noisy inputs.
We cover two major Chinese input methods: keyboard input and speech input. 
For keyboard input, we construct synthetic noise tests via character substitutions. 
For speech input, we use a noisy test set including inputs with diverse accents, which poses greater typo diversity.  
Our SubChar-Pinyin method shows advantage in both cases.

\paragraph{Synthetic Typos}
We simulate the homophone typos that are common in real-world Chinese writing systems, especially user-generated inputs. As shown in Figure~\ref{fig:pinyin}, \textit{pinyin} input is the most widely used keyboard input method for Chinese users.\footnote{\url{https://en.wikipedia.org/wiki/Chinese_input_methods_for_computers}} When users type in the romanization of the intended characters, the input interface will present all Chinese characters with the same romanization for the users to choose from. As a result, it is common for users to choose the wrong characters either by mistake or because they are unclear about the differences among these homophones. 


In such cases, our SubChar-Pinyin-NoIndex tokenizer (described in section~\ref{sec:char_encode}) has the advantage of being robust towards any such homophone typos. As illustrated in Figure~\ref{fig:no_index}, the character encoding will map all homophones of a character into the same romanization sequence before undergoing the sub-word tokenization. As a result, the tokenized output will be identical no matter what the typo character is as long as it is a homophone of the intended character.

% \begin{itemize}
%   \item Glyph-based noise: we replace original characters with other characters that have similar glyph but have different semantic meanings (\textit{e.g., 壁} (wall) and \textit{璧} (jade)). Specifically, we obtain a substitution candidate list for each character, where the candidates are selected so that they share at least one common radical with the original character. Then, we randomly sample a certain ratio $r\%$ of the original characters, for each of them, we randomly sample a substitution character from its candidate list for substitution.
% %   from the OpenAttack library~\cite{OpenAttack}. The substitution candidates are chosen via rendering the character as images and ranking the image similarity. 
% This simulates common noises when people use glyph-based input methods where these similar characters could be chosen since their input encoding are similar. 

 We inject synthetic noises into the test data and examine whether models trained on clean training data can perform well on these noisy data.
%
To construct the noisy data, we replace the original correct characters with their homophones, \textit{e.g.}, change `意'\ (sense) to `异'\ (different)' and `义'\ (meaning) to `议'\ (debate).\footnote{Interestingly, all these four characters have the same pronunciation but different meanings. Moreover, ``意义"\ (meaning) and ``异议"\ (objection) are homophone words.}
Specifically, we randomly sample a certain ratio $r\%$ of the original characters. For each of them, we replace it with a randomly sampled homophone from all its homophones obtained via a Pinyin dictionary (no replacement if it has no homophones).
%   This simulates the common noise when users use pronunciation-based input methods where the input encoding of these characters and their substitutions are the same.
% \end{itemize}
% We vary the ratio of homophone substitution in the test data to examine the impact.

% Intuitively, our \shuowen{} tokenizer could be robust to pronunciation-based noises and \jiezi{} tokenizer could be robust to glyph-based noises because the substitution characters share similar pronunciation or glyph components with the original characters, which may be captured by our tokenizers. 

% This noisy setup is reflective of real-life use cases where user queries often contain such noises. Since most Chinese people use either glyph-based input methods (\textit{e.g., wubi}) or pronunciation-based input methods (\textit{e.g., pinyin, zhuyin}), such mis-typed characters can be very common. This highlights the potential impact of our work.

% We perform the noisy evaluation on two datasets: TNEWS and OCNLI. 
% For glyph-based noises, we compare baselines BERT-Chinese and \sentp{}-\ulm{} with our \jiezi{}-Wubi. 
% The results are presented in Table~\ref{tab:noisy_results_glyph}. 
% We observe that when the noise ratio increases, the advantage of \jiezi{} is particularly large. For example, when $60\%$ characters are substituted, \jiezi{}-Wubi still performs close to the original performance, while other baselines suffer large drops in performance. On OCNLI, the gap can be as large as 18 points in accuracy. 


The results are shown in Table~\ref{tab:noisy_results_phonology}. 
We observe that there can be a significant drop in performance where there exist homophone typos in the test data. For example, the BERT model trained with CharTokenizer drops from $64.10\%$ accuracy on clean data to $25.20\%$ accuracy when $37.5\%$ of the characters in test inputs are replaced with homophone typos.
Overall, we find that the character tokenizer, sub-word tokenizer, as well as the vanilla SubChar-Pinyin tokenizer, cannot handle such noisy data. However, our SubChar-Pinyin-NoIndex tokenizer exhibits \textbf{no performance drop} under noises.
Moreover, despite learning a shared representation for homophones, the model with SubChar-Pinyin-NoIndex still performs competitively on the clean test sets, either match (on C3) or only a little worse than the baselines (on TNEWS and OCNLI).

\paragraph{Real-World Typos}
While the above synthetic typos aim to simulate typos in keyboard inputs, another major input method is through speech input where users speak to their devices (like mobile phones) and their speech input is then converted to text for downstream tasks. 
In order to evaluate model robustness in such scenarios, we use a realistically collected test set that captures such speech input typos. 
Specifically, we use the speech-noise version of the AFQMC test set from the READIN~\cite{READIN} benchmark. 
For each example in this noisy AFQMC test set, three annotators with different accents read the original input, and then the speech recordings are converted to text using commercial automatic speech recognition (ASR) software. We refer readers to the dataset description paper for more data construction details. 
When computing performance for each test example, we compute both the average across different annotations (Noisy-Average), as well as the worst performance across different annotations (Noisy-Worst), and then take the macro-average across all examples. The character-level error rate of the noisy test set is 30\% on average. 

%  Specifically, we recruit ten native Chinese speakers, all from different cities in China and with different accents (4 males and 6 females, ages ranging from 32 to 64). 
%  We ask them to read the examples from the AFQMC test set, and record their reading with phones.
%  We instruct the annotators to speak Mandarin while preserving their accents. Each example is recorded by 3 different annotators.
 %
%  After obtaining their recordings, 
%  the speech data are converted to text with a commercial automatic speech recognition (ASR) software iFlytek.~\footnote{\url{https://global.xfyun.cn/products/real-time-asr}} We choose this commercial software because it is optimized for Mandarin and outperforms other open-source toolkits that we explored in the pilot run in terms of character-level error rates.~\footnote{This new dataset, both raw recordings and converted text, will be released along with a more detailed description of statistics and collection protocols.} 

This AFQMC noisy test set contains not only homophone typos, but also a wide range of other types of real-world input noises due to both the accent variations and ASR errors. The greater diversity of typo types in the real-world test set makes it much more challenging to maintain robustness than the synthetic setting which only considers homophone typos. 
While the original AFQMC is a binary classification task that classifies whether the question pair is a paraphrase or not, we find that models trained on the AFQMC training set exploit spurious correlations like lexical overlap, even though we explicitly balanced the training set. In particular, when introducing typos in the test data, performance on positive examples drops drastically due to lower lexical overlap, while the performance on negative examples stays or even improves a little because of the lower lexical overlap caused by the typos. This is similar to previous findings on HANS~\cite{McCoy2019RightFT} and PAWS~\cite{Zhang2019PAWSPA}. Hence, we follow the evaluation practice when dealing with spurious correlation, which is to focus on improving the worst-group performance, and in this case, we focus on improving performance on the positive examples against the impact of typos. 


\input{tables/afqmc_noisy.tex}
The results are shown in Table~\ref{tab:afqmc_noisy} where we report performance on the AFQMC positive examples. All models are trained on the original clean data from AFQMC (we balanced the positive and negative classes during training). We evaluate on the original clean test set, the Noisy-Average performance (N-Average), and the Noisy-Worst performance (N-Worst). We can see that despite this more challenging speech typo setting, our SubChar-Pinyin model still outperforms the baselines. 

\input{tables/cws.tex}

These results highlight the robustness advantage of our Sub-Character tokenization method in both dealing with synthetic homophone typos as well as on more diverse real-world typos. 
 

\input{tables/cmrc_ner}


\subsection{Effect of CWS}

 We examine the impact of incorporating \cws{} in the tokenization as described in Section~\ref{sec:cws}.
We train tokenizers with and without \cws{} and compare the performance of the corresponding pretrained models. As shown in Table~\ref{tab:cws_results}, we can see that adding \cws{} as an additional step does not help downstream task performance. These results serve as empirical evidence that \cws{} is ineffective in the use of \plm{}s, complementary to the results of \citet{CWSnecessary} on models without pretraining.


\subsection{Character-Level Tasks}


The evaluation in Section~\ref{sec:standard_eval} is restricted to sequence-level classification tasks such as single-sentence classification, sentence-pair classification and machine reading comprehension. 

One might wonder how do SubChar tokenizers handle character-level tasks where classification is done on every single character, such as sequence labeling and span extraction. Since SubChar tokenizers may combine multiple characters into one token or split one character into sub-character tokens, directly adding a classification head on each token may cause discrepancy with the human annotation, which is done on the character level. For example, it is infeasible to evaluate the POS tag of a sub-character token.
% when only parts of a token belong to the label, it becomes impossible to make a correct prediction.

% Since Sub-Character Tokenizers may break a character into sub-characters, directly adding a classification head on each token may cause discrepancy with the human annotation (\textit{e.g.,} it doesn't make sense to predict the POS tag of a sub-character).  

% Since labels in span-extraction and named entity recognition tasks are given on character level, the tokens might not match the granularity of the labels. 

% To handle such situations, we obtain character level representations by first tokenizing each character individually, and then averaging the embedding of each sub-character token of the character as its character representation.

To handle such situations, we perform classification on the character level for these tasks.
To obtain the representation of each character, we average the representations of all its sub-character tokens. We apply this on the final layer of BERT and feed the character representation to a linear classifier for downstream tasks.

% by tokenizing each character individually, and let the model make predictions on the sub-character tokens.  To take into account information about multi-character words, we tokenize the entire sequence to obtain word-level tokens. Then each sub-chararacter token is added with the average of the embeddings of the word-level tokens that overlap with it.

We measure the performance of this approach on CMRC (span-extraction reading comprehension) and CLUENER (named entity recognition) and show the results in Table~\ref{tab:cmrc_ner}. The results show that our model can indeed handle character-level tasks with this simple adaptation. There might be better ways of adopting our model on character-level tasks, and we leave it to future work.

% tokenizing each character individually 
% in addition to tokenizing the entire sequence to get ordinary tokens, 
% we tokenize each character individually to get sub-character tokens. Then for each embedding of the sub-character tokens, we add the average of embeddings of the ordinary tokens that overlap with it. The model makes predictions on the resulting embeddings.