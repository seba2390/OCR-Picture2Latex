\begin{figure*}[t]
\centering
\includegraphics[trim={0.5cm 4.5cm 0.5cm 1.5cm},width=\textwidth]{figures/WubiBERT-Fig1.pdf}
\caption{Comparison of existing tokenizers (character tokenizer and sub-word tokenizer) and our sub-character tokenizers (SubChar-Wubi using glyph and SubChar-Pinyin using pronunciation encoding).
Different tokens produced by the tokenizers are separated by `|'. The numbers in \textit{(brackets)} indicate the number of tokens in the tokenized sequence.
Tokens in \textcolor{BrickRed}{orange} indicate character combinations, while tokens in \textcolor{OliveGreen}{green} indicate sub-character tokens. `\#' indicates the special separation symbol after each character, circled numbers (\circled{1}\circled{2}\circled{3}\circled{4}) indicate the intonation of characters.  (Figure best viewed in color.)}
\end{figure*}

% Pretrained Language Models (PLM) have achieved great success in NLP on a wide range of languages such as English~\cite{BERT}, Chinese~\cite{CPM}, French~\cite{FlauBERT}, and many others~\cite{XLM,XLM-R}. While these PLMs trained with corpus of different languages keep emerging, another active line of research focuses on improving the model design or training objectives, e.g., ALBERT~\cite{ALBERT}, ELECTRA~\cite{ELECTRA}, DeBERTa~\cite{DeBERTa}, ERNIE~\cite{ERNIE}, CharBERT~\cite{CharBERT}. 

% Large scale transformer-based pretrained language models (\plm s)~\cite[][\textit{inter alia}]{BERT,ALBERT,ELECTRA,CharBERT,DeBERTa} have achieved great success in recent years by dominating various benchmarks and have attracted wide interest in the research community.

Large-scale Transformer-based pretrained language models (\plm s)~\cite[][\textit{inter alia}]{BERT,RoBERTa,ALBERT,ELECTRA,DeBERTa} have achieved great success in recent years and attracted wide research interest, in which tokenization plays a fundamental role.

% Unfortunately, current tokenization methods are developed primarily for English~\cite{BPEsuboptimal}. 
The most popular type of tokenization adopted by \plm{}s is sub-word tokenization, such as byte pair encoding (BPE)~\cite{BPE}, WordPiece~\cite{WordPiece} and unigram language model segmentation~\cite{Kudo2018}. 
Recent Chinese \plm{}s such as CPM~\cite{CPM,CPM2} adopt this kind of sub-word tokenization.
Apart from sub-word tokenization, many other Chinese \plm{}s adopt a simple character tokenizer (CharTokenizer for short) that treats every single Chinese character as a token~\cite[][\textit{inter alia}]{ERNIE,WWM,MacBERT}.

% While the idea of sub-word tokenization is intuitive and effective for morphologically rich or synthetic languages, it is \textbf{not} the case for Chinese. 

However, we believe that both of these existing tokenizers are sub-optimal for Chinese. This is based on the observation that Chinese has unique linguistic characteristics:
%
% 1) The Chinese writing system is morphemic~\cite{hill2016typology}, 

1) Chinese has an opaque orthography with irregular grapheme-phoneme correspondence~\cite{phonology_new}. This is in contrast to transparent orthographies like Spanish and Finnish where each letter approximately represents one sound.
% This poses difficulty for character and sub-word tokenizers to capture the potentially useful pronunciation information. 
As a result, utilizing pronunciation information in Chinese requires explicit pronunciation encoding.
% poorly reflect the pronunciation, causing the conventional character or sub-word tokenization to miss the phonological information. 
%

2) Chinese does not have morphological inflection, unlike morphologically-rich languages like Russian~\cite{writing_system}. This renders sub-word tokenization less useful since the main advantage of sub-word tokenization comes from the fact that it can split common affixes and root words as separate tokens. In fact, Chinese characters are logograms, and their glyphs (the composition of radicals) also contain rich semantic information, which can only be captured at the sub-character level.

% 3) In Chinese writing, there is no natural word boundary like the spaces in English writing. Although it is possible to inject word boundaries via Chinese word segmentation (\cws), there is no prior study on how this works for Chinese \plm s.

Motivated by these observations, we propose the novel sub-character (SubChar) tokenization. It first encodes every Chinese character into a sequence of phonetic or stroke symbols, and then it uses a sub-word segmenter (such as BPE) to construct the vocabulary on all the encoded sequences. 
%
In this way, the resultant tokenizers can capture sub-character tokens that correspond to meaningful phonetic or morphemic units, which are absent from all existing Chinese tokenizers. As far as we know, this is the first attempt on leveraging the sub-character information for language models, especially in the context of Chinese NLP. 



% we devote this work to analyzing three unique linguistic characteristics of Chinese: 
% 1) The Chinese writing system is morphemic~\cite{hill2016typology}, which means Chinese characters poorly reflect the pronunciation, causing the conventional character-based tokenization to miss  phonological information. 
% 2) Modern Chinese words do not undergo morphological inflections~\cite{morphology_chinese}, rendering sub-word tokenization inapplicable. On the other hand, Chinese characters are mainly logograms, which means their glyphs, the composition of stokes and radicals, also contain rich semantic information~\cite{glyce}. 
% 3) In Chinese writing, there is no natural word boundary like the spaces in English writing. Although it is possible to inject word boundaries via Chinese word segmentation (\cws), there is no prior study on how this works for Chinese \plm s.

% %Xiaozhi: 要不要把用了什么encoding方式（仓颉、拼音、注音）什么的也在这段提一下？
% Targeting these three factors, we then explore three corresponding tokenization strategies: 
% 1) A pronunciation-based tokenizer family called \shuowen, which first romanizes Chinese characters based on their pronunciations, and then constructs the vocabulary with the romanized scripts using the unigram language model~\cite{sentencepiece}. 
% 2) A glyph-based tokenizer family called \jiezi, which decomposes characters into combinations of Chinese strokes or radicals, and then constructs the vocabulary with the stroke or radical sequences using the unigram language model (\ulm). 
% 3) A word segmented tokenizer family, which first uses a Chinese word segmenter to segment text into words, then  forming the vocabulary with two parts: the most frequent segmented words, and the sub-word vocabulary constructed by \ulm ~on the unsegmented parts of the text.

% However,  We believe that it is crucial to develop tailored tokenization techniques for the languages beyond English because there can be huge differences between different languages~\cite[\#BenderRule]{BenderRule}. To this end, in this work we take the first step towards a more efficient and robust tokenization method for Chinese. Specifically, \textbf{we challenge the conventional hypothesis that character is the smallest unit for Chinese NLP}. In doing so, we propose a new tokenization named Sub-Character Tokenization, which first encodes every character into a sequence of symbols (\textit{e.g.,} phonetic symbols), and then uses SentencePiece to construct the vocabulary. As a result, the resultant vocabulary consists of a mix of sub-character tokens, single character tokens, and character combination tokens such as words and phrases. Notably, this is first time that a Chinese tokenizer can produce tokens smaller than the character level. We use extensive experiments to demonstrate the benefit of doing so. 


To assess the effectiveness of our proposed method, we train a series of BERT-style \plm s using the existing and proposed tokenizers.
We evaluate these models on over ten datasets of various downstream natural language understanding (NLU) tasks. 
Through extensive evaluation, we find that models trained with SubChar tokenizers match models trained with character and sub-word tokenizers on downstream task performance. 
More importantly, SubChar tokenizers have two major advantages compared to existing tokenizers:

% \begin{enumerate}
  1) \textbf{SubChar tokenizers are more efficient.} We find that a small fraction of sub-character tokens in the vocabulary can compose a large variety of rare and complex characters, thus saving much space in the vocabulary for more character combination tokens such as words and phrases. The increased use of combination tokens leads to significantly decreased length of the tokenized sequences. For example, on the iFLYTEK long text classification dataset, with the same vocabulary size as the CharTokenizers, SubChar tokenizers can achieve as much as 40\% length reduction on the tokenized output. Such length reduction can significantly speed up both pretraining and finetuning.

 2) \textbf{SubChar tokenizers are more robust.} A common and unique type of typos in Chinese is caused by homophones where characters with different semantic meanings have exactly the same pronunciation. SubChar tokenizers based on pronunciation can map homophones into the same transliteration sequences, thus improving robustness against any homophone typos. This could be immensely useful when handling noisy inputs. 
% \end{enumerate}

% but achieving much better efficiency. This is achieved by the fact that a small fraction of sub-character tokens makes space for much more character-compositions (words, phrases) in the vocabulary, reducing the tokenized sequence length.
% Furthermore, \shuowen ~and \jiezi ~have the unique advantage of being robust on noisy inputs, where they are \textbf{invariant} to any pronunciation or glyph based character substitutions. This is particularly useful for applications where users may have typos when typing.  

% In summary, our linguistically informed Chinese tokenizers are much more efficient and robust than previous approaches, while maintaining competitiveness on standard benchmarks.
% Our positive results suggest that linguistically informed techniques based on the characteristics of different languages deserve more attention. We will release the code, pretrained models, and the tokenizers to serve as a better foundation for future research on Chinese \plm s.


% In summary, in this work, we proposed the idea of Sub-Character Tokenization. We trained a series of Sub-Character Tokenizers with different configurations and used them to train BERT-style models. Extensive empirical evaluation on downstream NLU tasks shows that Sub-Character tokenizers are much more efficient and robust than previous tokenizers while remaining competitive in standard NLU evaluation. 


We believe that our work is an important step towards more tailored techniques for languages beyond just English by effectively integrating the unique linguistic characteristics of the language~\cite[\#BenderRule]{BenderRule}. 

% To this end, in this work we take the first step towards a more efficient and robust tokenization method for Chinese. Specifically, \textbf{we challenge the conventional hypothesis that character is the smallest unit for Chinese NLP}. In doing so, we propose a new tokenization named Sub-Character Tokenization, which first encodes every character into a sequence of symbols (\textit{e.g.,} phonetic symbols), and then uses SentencePiece to construct the vocabulary. As a result, the resultant vocabulary consists of a mix of sub-character tokens, single character tokens, and character combination tokens such as words and phrases. Notably, this is first time that a Chinese tokenizer can produce tokens smaller than the character level. We use extensive experiments to demonstrate the benefit of doing so. 