% Past Chinese tokenization work: MVP-BERT, AMBERT, ZEN (https://arxiv.org/pdf/1911.00720.pdf), 
% glyph embedding (jiwei), wubi encoding for NMT, chinese word segmentation for Chinese NLP
\paragraph{Chinese \plm{}s.} Chinese BERT~\cite{BERT} is the first Chinese \plm, which adopts the character tokenization. Then, researchers have explored techniques to explicitly incorporate the word-level information into Chinese \plm{}s for better performance. \citet{MVP-BERT} and \citet{AMBERT} expand BERT vocabulary with Chinese words apart from Chinese characters and incorporate them in the pretraining objectives. \citet{WWM}, \citet{NEZHA}, and \citet{ERNIE-GRAM} consider coarse-grained information through masking whole words and $n$-grams during the masked language modeling pretraining. \citet{ZEN} incorporate word-level information via superimposing the character and word embeddings. \citet{Lattice-BERT} incorporate Chinese word lattice structures in pretraining. Different from these studies, we investigate the information in the sub-character level for Chinese \plm{}s.
  
\paragraph{Linguistically-Informed Techniques for Chinese NLP.} 
Before the era of \plm{}, many efforts have been made to incorporate linguistic knowledge, including both glyph~\cite{sun2014radical,yu2017joint,cw2vec} and pronunciation~\cite{zhang2019learning,chaudhary-etal-2018-adapting}, into word embedding~\cite{mikolov2013distributed}.
Beyond word-level representation, researchers explore the use of linguistic information to enhance sequential models~\cite{dong2016character,bharadwaj-etal-2016-phonologically,liu-etal-2017-learning}, especially BERT~\cite{glyce,sun-etal-2021-chinesebert}.
Compared to these works, we do not incorporate additional information from sources like images, instead, our proposed tokenization methods are drop-in replacements to existing tokenizers, without adding any extra layers or parameters. 
Besides, \cws{} is a common preprocessing step for Chinese NLP~\cite{THULAC}, 
\citet{CWSnecessary} empirically analyze whether \cws{} is helpful for Chinese NLP tasks before the era of \plm{}s and find that the answer is no in many cases. 
In our work, we also spend a section examining the impact of \cws{} specifically for \plm{}s. 
% \citet{glyce} incorporate glyph information of Chinese characters by adding an extra encoder to represent the images of Chinese characters and combining them with the character embeddings. 
% \citet{wubiNMT} explore to transform Chinese text into Wubi sequences that represent character glyph information for the task of machine translation.
Moreover, as shown by~\citet{huang-etal-2021-phmospell}, incorporating linguistic information also benefits spelling check.
Instead of explicitly using spelling check, our linguistically-informed tokenizations are robust to spelling errors.

\paragraph{Granularity of Tokenization.} Although sub-words are taken to be the default granularity of tokenization since the release of BERT, researchers also explore different granularities for \plm{}s. For instance, ELMo~\cite{ELMo}, the early pioneer of \plm{}s, starts by using character representation. \citet{CharBERT} combine character representations with sub-word representations for better performance and robustness. 
\citet{Nzeyimana2022KinyaBERTAM} incorporate a morphological analyzer for tokenization and achieve gains for the Kinyarwanda language model. 
More recently, there is a trend in tokenization-free methods, including Byte-BPE~\cite{BBPE}, CANINE~\cite{CANINE}, ByT5~\cite{ByT5}, and Charformer~\cite{Charformer}, which discard explicit tokenization and directly represent inputs as small units such as bytes. The downside of these tokenization-free approaches is obvious: the longer tokenized sequence lengths slow down both training and inference. Contrary to them, our sub-character tokenization encourages the use of more character combinations, which largely shortens the tokenized sequences. 

