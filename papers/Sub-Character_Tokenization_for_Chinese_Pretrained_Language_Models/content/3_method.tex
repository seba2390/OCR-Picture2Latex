\label{sec:method}

In this section, we describe our proposed SubChar tokenization in detail. We break it down into two steps: 1) Chinese character encoding; 2) vocabulary construction based on the encoded sequences. 

%% TODO: provide introduction and example of each input method


% \subsection{\shuowen: Pronunciation-based Tokenizers}
\subsection{Step 1: Character Encoding}
\label{sec:char_encode}

The core idea of this step is to encode every Chinese character into a sequence that characterizes its glyph or pronunciation, in order to provide additional inductive biases to the model. We explore several ways of encoding the characters.
% as summarised in Table~\ref{tab:all_encodings}.
% which are summarised in Table~\ref{tab:all_encodings}.
They can be categorised as pronunciation-based and glyph-based encoding.

% \input{tables/all_encodings.tex}

\paragraph{Pronunciation-based encoding}
% \label{sec:pronounce_encode}
%% TODO: give pages to these citations
% The Chinese writing system is morphemic~\cite{hill2016typology} and barely conveys phonological information. 
% However, the pronunciation of Chinese characters also reveals semantic patterns~\cite{phonology} and has long been widely used in the input methods of Chinese (\textit{e.g., Pinyin} input).
In order to capture pronunciation information of characters, we encode Chinese characters using transliteration, which uses IPA-inspired\footnote{IPA: International Phonetic Alphabet (\url{https://en.wikipedia.org/wiki/International_Phonetic_Alphabet})} phonetic scripts to characterize the pronunciation.

We explore two different transliteration methods: \textit{pinyin} and \textit{zhuyin (i.e., bopomofo)}.  
\textit{Pinyin} uses romanized transcription and four different tones (\={}, \'{}, \v{}, \`{})
% \footnote{The light tone is sometimes considered as the fifth tone but we omit it for simplicity.}
to transliterate characters, \textit{e.g., 魑魅魍魉} $\rightarrow$ \textit{Chi\={} Mei\`{} Wang\v{} Liang\v{}}. 
On the other hand, \textit{zhuyin} uses a set of graphemes nonexistent in English and the same four tones to transliterate the characters, \textit{e.g., 魑魅魍魉} $\rightarrow$ \textit{ㄔ ㄇㄟ\`{} ㄨㄤ\v{} ㄌㄧㄤ\v{}}. 
In \textit{zhuyin}, the first tone mark (\={}) is usually omitted.

We insert special separation symbols (\textit{\#}) after each character's transliterated sequence, \textit{e.g., Chi\={}\#Mei\`{}\#Wang\v{}\#Liang\v{}\#, ㄔ\#ㄇㄟ\`{}\#ㄨㄤ\v{}\#ㄌㄧㄤ\v{}\#}. This prevents cases where transliterated sequences of different characters are mixed together, especially when there are no tone markers to split them in \textit{zhuyin}.
  
 Different Chinese characters may have the same pronunciation even if they have different semantic meanings (\textit{i.e.,} homophones). For disambiguation, we append different indices after the encoded sequences for the homophonic characters, so as to allow a biunique mapping between each Chinese character and its transliteration sequence, \textit{e.g., Chi\={}33\#Mei\`{}24\#Wang\v{}25\#Liang\v{}13\#, ㄔ10\#ㄇㄟ\`{}3\#ㄨㄤ\v{}6\#ㄌㄧㄤ\v{}1\#}. 

It is unclear whether having such disambiguation of homophones is beneficial or not. To analyse the impact, we also experiment with a variant where we do not add the indices to disambiguate the homophones. We implement the tokenizer \textit{SubChar-Pinyin-NoIndex} to perform \textit{pinyin} encoding without disambiguation indices.
We will show that this variant also has the advantage of being robust to homophone typos~(section \ref{sec:noisy_eval}). 

\begin{comment}
On raw Chinese input text (\textit{e.g., 魑魅魍魉}), \shuowen{} performs the following steps:

\begin{enumerate}
  \item Romanize the text using Chinese transliteration systems. In this work, we explore two different transliteration methods: \textit{pinyin} and \textit{zhuyin (i.e., bopomofo)}. \textit{Pinyin} uses the Latin alphabet and four different tones (\={}, \'{}, \v{}, \`{})\footnote{The light tone is sometimes considered as the fifth tone but we omit it for simplicity.} to romanize pronunciations of characters, \textit{e.g., 魑魅魍魉} $\rightarrow$ \textit{Chi\={} Mei\`{} Wang\v{} Liang\v{}}. On the other hand, \textit{zhuyin} uses a set of self-invented characters and the same four tones to romanize the characters, \textit{e.g., 魑魅魍魉} $\rightarrow$ \textit{ㄔ ㄇㄟ\`{} ㄨㄤ\v{} ㄌㄧㄤ\v{}}. Note that in \textit{zhuyin}, the first tone mark (\={}) is usually omitted.
  
  \item Insert special separation symbols (\textit{+}) after each character's romanized sequence, \textit{e.g., Chi\={}+Mei\`{}+Wang\v{}+Liang\v{}+, ㄔ+ㄇㄟ\`{}+ㄨㄤ\v{}+ㄌㄧㄤ\v{}+}. This prevents cases where romanized sequences of different characters are mixed together, especially when there are no tone markers to split them in \textit{zhuyin}.
  
  \item Different Chinese characters may have the same pronunciation. For disambiguation, we append different indices after the romanized sequences for the homophonic characters, so as to allow a biunique mapping between each Chinese character and its romanized sequence, \textit{e.g., Chi\={}33+Mei\`{}24+Wang\v{}25+Liang\v{}13+, ㄔ10+ㄇㄟ\`{}3+ㄨㄤ\v{}6+ㄌㄧㄤ\v{}1+}. 
  
  \item Apply a unigram language model (\ulm{}) as in \citet{sentencepiece} on the romanized sequences to build the final vocabulary. 
  
\end{enumerate}

We do not set any constraint on the vocabulary other than the vocabulary size. The resultant vocabulary contains tokens corresponding to flexible combinations of characters and sub-characters.

\end{comment}
% Specifically, we first encode each Chinese character into a sequence that represents its pronunciation. We insert a special separation token after each character's pronunciation sequence to indicate character boundary. 
% We then apply unigram LM as in \citet{sentencepiece} to construct sub-word token vocabulary. In the resultant vocabulary, there exists: 1) sub-characters that represent part of a Chinese character; 2) single-character tokens; 3) multi-character tokens. 

% When converting each Chinese character into a sequence that represents its pronunciation, we resort to two classic phonology based input method: Pinyin and Zhuyin. Pinyin and Zhuyin are both romanization systems for standard Mandarin Chinese, where Pinyin is dominantly used in mainland China and Zhuyin is commonly used in places like Taiwan province.

% It is common that different Chinese character have the exact same pronunciation and hence the same Pinyin and Zhuyin encoding. We add a unique index at the end of the encoding sequence of each character for disambiguation of such homophone characters. 
 
% \subsection{\jiezi: Glyph-based Tokenizers}
\paragraph{Glyph-based encoding}
%% todo: ant linguistics paper to support?
The glyphs (\textit{i.e.}, shapes) of Chinese characters contain rich semantic information and can help NLP models~\cite{cw2vec}.  Most Chinese characters can be broken down into semantically meaningful radicals. Characters that share common radicals often have related semantic information, e.g., the four characters \textit{`魑魅魍魉'} share the same radical \textit{`鬼'} (meaning ``ghost''), and their meanings are indeed all related to ghosts and monsters.\footnote{The word \textit{`魑魅魍魉'} is in fact a Chinese idiom, which is often used to refer to bad people who are like monsters.} 
%
In order to capture glyph information, we explore four glyph-based encoding methods, namely \textit{Stroke, Wubi, Zhengma}, and \textit{Cangjie}.

% For example, characters containing the radical `\textit{木}' (wood) are often related to woods, like `\textit{树}' (tree), `\textit{林}' (forest),  `\textit{枝}' (branch).

% However, existing tokenization methods treat each character as the smallest unit in Chinese and hence do not allow the models to capture the shared semantics of characters with the same radicals. 

% propose the glyph-based tokenizer \jiezi{}, which performs the following steps on raw Chinese input (\textit{e.g., 魑魅魍魉}):

% \begin{enumerate}

% We convert each character into a stroke or radical sequence.

For \textit{stroke} encoding, we use the Latin alphabet to represent the set of Chinese strokes and convert the characters based on the standard stroke orders,\footnote{\url{https://en.wikipedia.org/wiki/Stroke_order}} \textit{e.g., 魑} $\rightarrow$ \textit{\underline{pszhshpzzn}nhpnzsszshn}; \textit{魅} $\rightarrow$ \textit{\underline{pszhshpzzn}hhspn} (underlined parts indicate shared stroke sequences across these characters). 

The other three glyph-based encoding methods encode characters into radical sequences instead,
by using glyph-based Chinese input methods: \textit{Wubi}, \textit{Zhengma} and \textit{Cangjie}. These input methods group strokes together in different ways to form radicals, and then decompose characters into radical sequences. We use the Latin alphabet to represent these radicals, \textit{e.g., 魑魅魍魉} $\rightarrow$ \textit{Wubi: \underline{rqc}c \underline{rqc}i \underline{rqc}n \underline{rqc}w}; \textit{Zhengma: \underline{nj}lz \underline{nj}bk \underline{nj}ld \underline{nj}oo}; \textit{Cangjie: \underline{hi}yub \underline{hi}jd \underline{hi}btv \underline{hi}mob} (\underline{underlined} parts indicate common radicals among them).

We append the same separation symbol (`\#') after each character, and also add the disambiguation indices for characters whose stroke sequences are identical (\textit{e.g., 人} (people) and \textit{八} (eight)). However, we note that there are very few cases where different characters have the same glyph encoding.
% \end{enumerate} 


% Please refer to Appendix~\ref{app:inputmethod} for a detailed explanation for the differences between these different input methods involved.



\subsection{Step 2: Vocabulary construction}

Once we have the encoded sequences, we can treat the encoding of each character as the equivalent of `word' in English and then apply sub-word segmentation to construct the vocabulary for our sub-character tokenizers. 

Sub-word segmentation typically forms sub-word tokens by merging frequent token bigrams, which often results in meaningful morphemes of the words when used in languages like English. On our encoded sequences, sub-word segmentation can capture shared sub-character sequences that correspond to shared radicals or phonetic sequences among similar characters. 
After running the sub-word segmentation step on the encoded sequences, the vocabulary of the resultant sub-character tokenizers consists of a mixture of sub-character tokens, character tokens, and character combination tokens. 

In this work, we use the unigram language model segmentation method~\cite{Kudo2018} implemented in SentencePiece~\cite{sentencepiece} as the default sub-word segmentation method. In section~\ref{sec:bpe}, we also perform an ablation study by setting the sub-word segmentation method to BPE, which results in similar performance and efficiency, illustrating that the gains of SubChar tokenization are insensitive to the specific choice of sub-word segmentation methods. 




\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{figures/CWS.png}
\caption{Illustration of the tokenization pipeline when incorporating CWS. After the first step of CWS, high-frequency words (words in the dashed box) directly become part of the final output sequence, the other words then go through SubChar tokenization.}
\label{fig:cws}
\end{figure}


% ZZY
% CWS is done before character encoding right? You can move this before 3.1, I will edit the intro paragraph accordingly.
\subsection{Optional Step: Chinese Word Segmentation}
\label{sec:cws}

Before the first step of character encoding, there is an optional step of Chinese word segmentation. 

Chinese word segmentation (\cws{}) is a common technique to split Chinese text chunks into a sequence of Chinese words. The resultant segmented words sometimes provide better granularity for downstream tasks~\cite{CWS-NMT}. However, the impact of \cws{} is unclear in the context of pretraining, especially its interplay with the tokenization. Hence, we propose a way to incorporate \cws{} into our SubChar tokenization and examine whether it is helpful. Our proposed tokenization pipeline is summarized in Figure~\ref{fig:cws}.

Given that the vocabulary of SubChar tokenizers consists of character combinations, characters, and sub-characters, we use \cws{} to construct the character combination part of the vocabulary. Compared to the character combination tokens generated by the statistical approach of sub-word tokenization, the combination tokens generated by a trained Chinese word segmenter have more linguistic prior knowledge. 
% For example, given a sentence ``这篇论文有意思。(This paper is interesting.)'', the segmenter would split it into three words: ``这篇\textbackslash 论文\textbackslash 有意思\textbackslash  。''. 

Specifically, to construct the vocabulary, we first segment the pretraining corpus into words. Then, we select the most frequent words as the character combination part of the SubChar tokenizer vocabulary. We then encode the text with one of the pronunciation- or glyph-based encoding methods and use sub-word tokenization on the encoded sequences to get the sub-character and character tokens of the vocabulary.  Finally, we merge these parts together as the vocabulary for the SubChar tokenizer. When tokenizing new inputs, we first segment them into words, if the words are in the vocabulary, they will be tokenized as word tokens; if not, they will be further processed by the SubChar tokenizer.
We control the ratio of word tokens in the vocabulary to be 80\% based on preliminary tuning and we use a state-of-the-art segmenter THULAC~\cite{THULAC,THULAC-repo} for word segmentation.

% Hence, we study word segmented tokenizers that perform the following process on raw Chinese input, e.g., ``这篇论文有意思。(this paper is interesting)'' :

% \begin{enumerate}
%   \item We use a state-of-the-art segmenter THULAC~\cite{THULAC} to segment the sentence into a sequence of words joined by spaces, \textit{e.g., `这篇\textbackslash 论文\textbackslash 有意思\textbackslash  。'} (We use \textbackslash to indicate a blank space for easier reading.)

%   \item We directly apply \ulm{}~on these space-joined sequences to construct the vocabulary.
  
% \end{enumerate}

% In other words, \cws{} is used as a preprocessing step on training corpora when we build the vocabulary using the above-mentioned methods. When we perform pretraining and finetuning, we conduct \cws{} as preprocessing before tokenization using the word segmented tokenizer. 