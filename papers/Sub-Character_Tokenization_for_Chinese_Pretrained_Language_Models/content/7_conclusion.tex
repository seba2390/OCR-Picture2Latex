In this work, we propose sub-character tokenization and conduct comprehensive experiments to illustrate its advantage over existing tokenization methods. 
%
% The idea of SubChar tokenization is simple: we encode every Chinese character into a short sequence of symbols and then construct the vocabulary on the encoded sequences with sub-word tokenization. 
%
Compared to treating each individual character as a token (CharTokenizer) or directly running sub-word tokenization on the raw Chinese text (sub-word tokenizer), our SubChar tokenizers not only perform competitively on downstream NLU tasks, more importantly, they can be much more efficient and robust.
%
We conduct a series of ablation and analysis to understand the reasons why SubChar tokenizers are more efficient, as well as the impact of linguistic and non-linguistic encoding.
%
Given the advantages of our SubChar tokenizers, we believe that they are better alternatives to all existing Chinese tokenizers, especially in applications where efficiency and robustness are critical. 
%
% We will release well-documented code, tokenizers, and pretrained models for easy usage. 
%
It is possible that our approach can be useful for other morphologically poor languages and more complicated methods could be developed based on SubChar tokenization for even better performance. We leave these interesting directions for future exploration.
%
On a broader level, our work makes an important attempt in developing more tailored methods for a language drastically different from English with promising results. We believe that this is a crucial future direction for the community given the language diversity in the world.
%
We hope that our work can inspire more such work in order to benefit language technology users from different countries and cultures.


% In this paper, we have explored three linguistically informed tokenization methods motivated by the unique linguistic characteristics of the Chinese writing system. Specifically, we find that pronunciation-based and glyph-based tokenizers can match or outperform the conventional Chinese tokenizers and Chinese word segmentation is not a useful addition for the tokenizer.
% Moreover, we find that our glyph-based tokenizers achieve large gains on noisy input as compared to the baselines, while our pronunciation-based tokenizers obtain limited success. This highlights the potential advantage of our proposed methods in real-life scenarios with noisy data.
% We believe that our work sets an important example of exploiting the unique linguistic property of a language beyond English to develop more tailored techniques, which should be an important direction for the global NLP community. 


