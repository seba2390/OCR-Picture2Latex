
\input{tables/stats.tex}

In this section, we introduce our baselines, datasets and experiment settings.

% \subsection{Baselines}

% We compare with two existing tokenization methods as baselines, namely single-character tokenization and sub-word tokenization.  
% We name this tokenizer \textit{CharTokenizer}.  

% This approach resembles the vocabulary of some recent multi-granularity Chinese \plm{} variants such as AMBERT~\cite{AMBERT} and Lattice-BERT~\cite{Lattice-BERT}. Different from them, we do not add any new model designs or pretraining objectives, but use the original BERT architecture and masked language modeling objective. 



\subsection{Baselines}

We compare two existing tokenization methods as baselines, namely single-character tokenization and sub-word tokenization.  
For a fair comparison, we set the same vocabulary size of $22,675$ for all tokenizers, including baselines and our proposed tokenizers. This is consistent with the vocabulary size of Chinese BERT~\cite{BERT}. 

% In this work, we use two strong baselines, single-character tokenization and sub-word tokenization, which are commonly used in Chinese \plm{}s. Single-character tokenization is adopted by BERT-Chinese and many other follow-up Chinese \plm{}~\cite[\textit{e.g.,}][]{WWM,MacBERT}. We name this tokenizer \textit{CharTokenizer}. For sub-word tokenization, we apply SentencePiece with \ulm~on the raw Chinese corpus to generate the vocabulary. As a result, the vocabulary contains both single characters and character combinations. We name this \textit{SentencePiece} for short. This approach resembles the vocabulary of some recent multi-granularity Chinese \plm{} variants such as AMBERT~\cite{AMBERT} and Lattice-BERT~\cite{Lattice-BERT}. Different from them, we do not add any new model designs or pretraining objectives, but use the original BERT architecture and masked language modeling objective. 

% We use two strong baseline tokenizers in this work (which as far as we know are the only existing tokenization methods for Chinese). The first one is the conventional single-character tokenizer as used in BERT-Chinese and many other follow-up Chinese \plm{}~\cite[\textit{e.g.,}][]{WWM,MacBERT}. We name this tokenizer CharTokenizer.

% For the second baseline, we directly apply SentencePiece with \ulm~on the raw Chinese corpus to generate the vocabulary. 
% As a result, the vocabulary contains both single characters and character combinations. We name this SentencePiece for short. 
% This approach resembles the vocabulary of some recent multi-granularity Chinese \plm{} variants such as AMBERT~\cite{AMBERT} and Lattice-BERT~\cite{Lattice-BERT}. Unlike them, we do not add any new model designs or pretraining objectives, but instead use the original BERT architecture and masked LM objective. 

\subsection{Pretraining Data}

% For a fair comparison, we set the same vocabulary size of $22,675$ for all tokenizers, following the setting of~\cite{BERT}. 

We use the same training corpus to train all the tokenizers in this work. 
The corpus consists of $2.3$ GB Chinese text from Baidu Baike.\footnote{\url{https://baike.baidu.com/}}

To evaluate the effectiveness of the tokenizers, we pretrain a BERT\footnote{Note that we mean BERT-style pretrained Transformers. Our models are not directly comparable with the original Chinese BERT since we use different pretraining data and hyper-parameters.} model using each tokenizer and compare their performance on downstream tasks. 
%
When pretraining the BERT models, we use the same pretraining corpus (\textit{i.e., } Baidu Baike) and the same set of hyper-parameters.
%
Notably, we also pretrain a new BERT model using the character tokenizer on our pretraining corpus instead of loading from existing checkpoints~\cite{BERT} so that it provides an apple-to-apple comparison with our proposed methods.
%
Since our proposed tokenizers are direct drop-in replacements for the baseline tokenizers, they do not incur any extra parameters. 
%
In summary, all the compared models have the same training corpus, hyper-parameters, and number of parameters, allowing for a truly fair comparison.



\subsection{Evaluation Data}

% We evaluate our models on 10 downstream NLU tasks, 8 from the CLUE benchmarks~\cite{CLUEbenchmark} and 2 from [...]. These include TNEWS (short text classification), IFLYTEK (long text classification), THUCNEWS (long text classification), BQ (question matching), CLUEWSC (coreference resolution), AFQMC (semantic similarity), CSL (keyword recognition), OCNLI (natural language inference), CHID (idiom cloze), C3 (multiple choice questions).

We finetune and evaluate the pretrained models with different tokenization methods on various downstream NLU datasets, including single-sentence classification, sentence-pair classification, and reading comprehension tasks. We briefly introduce each dataset below and present the dataset statistics in Table~\ref{tab:hyper_parameters}.

\noindent \textbf{TNEWS}~\cite{CLUEBenchmark} is a news title classification dataset containing 15 classes. %We use the split as released in \cite{CLUEBenchmark}.

\noindent \textbf{IFLYTEK}~\cite{CLUEBenchmark} is a long text classification dataset containing 119 classes. The task is to classify mobile applications into corresponding categories given their description.

\noindent \textbf{BQ}~\cite{BQ} is a sentence-pair question matching dataset extracted from an online bank customer service log. The goal is to evaluate whether two questions are semantically equivalent.

\noindent \textbf{THUCNEWS}~\cite{THUCNEWS} is a document classification dataset with 14 classes. The task is to classify news into the corresponding categories given their title and content.

\noindent \textbf{CLUEWSC}~\cite{CLUEBenchmark} is a coreference resolution dataset in the format of Winograd Schema Challenge~\cite{levesque2012winograd}. The task is to determine whether the given noun and pronoun in the sentence refer to the same entity.

\noindent \textbf{AFQMC}~\cite{CLUEBenchmark} is the Ant Financial Question Matching Corpus for the question matching task that aims to predict whether two sentences are semantically equivalent.

\noindent \textbf{CSL}\footnote{\url{https://github.com/P01son6415/CSL}} is the Chinese Scientific Literature dataset extracted from academic papers. Given an abstract and some keywords, the goal is to determine whether they belong to the same paper. It is formatted as a sentence-pair classification task.

\noindent \textbf{OCNLI}~\cite{OCNLI} is a natural language inference dataset. The task is to determine whether the relationship between the hypothesis and premise is entailment, neutral, or contradiction.

\noindent \textbf{CHID}~\cite{CHID} is a  cloze-style multiple-choice reading comprehension dataset. Given a context where some idioms are masked, the task is to select the appropriate idiom from a list of candidates.

\noindent \textbf{C3}~\cite{C3} is a multiple-choice reading comprehension dataset. The goal is to choose the correct answer for the questions given context. 

\noindent \textbf{CMRC}~\cite{CMRC} is a span-extraction reading comprehension dataset consisting of questions annotated from Wikipedia paragraphs.

\noindent \textbf{CLUENER2020}~\cite{CLUENER} is a named entity recognition dataset with 10 entity types. 

% \input{tables/noise_glyph.tex}



\subsection{Hyper-parameters}

We elaborate on all hyper-parameters involved for reproducibility (we also release all code, trained tokenizers and models). 


\textbf{Tokenizer Training}.
When training tokenizers with SentencePiece, we use a character coverage of $1.0$ and model type `unigram' for all tokenizers being compared. Other hyper-parameters follow the default of SentencePiece.

\input{tables/main.tex}

\textbf{BERT pretraining}. We follow the training procedure of BERT~\cite{BERT} except that the next sentence prediction objective is removed. The pretraining process consists of two stages. The first stage uses a maximum sequence length of $128$ with a batch size of $8$K for $8$K steps. The second stage uses a maximum sequence length of $512$ with a batch size of $4$K for $2$K steps. We experiment primarily with  $6$-layer Transformer~\cite{Transformer} models. To ablate the impact of model size, we also pretrain 12-layer Transformer models for the baseline CharTokenizer and proposed SubChar-Pinyin tokenizer. 
Other model configurations are the same for all models: $12$ attention heads, an intermediate size of $3072$, and a hidden size of $768$.


\textbf{BERT finetuning}.
For the finetuning on downstream datasets, 
we use a batch size of 32, maximum training epochs of 24 and tune max sequence length in \{96, 256, 512\}. Since the original test sets are not released, we use the original dev sets as the test sets and randomly hold-out 10\% of the training set as the dev sets. We select the best checkpoint on the dev sets and report performance on test sets.
% the set of hyperparameters as shown in the right part of Table~\ref{tab:hyper_parameters}. 
These hyper-parameters are consistent with previous work.
For all experiments in this paper, we report the results of the average run of three different random seeds.
All experiments are done on NVIDIA A100 GPUs.


