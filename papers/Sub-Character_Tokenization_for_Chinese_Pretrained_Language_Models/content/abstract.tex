% Tokenization is a fundamental component of pretrained language models (\plm{}s). 
Tokenization is fundamental to pretrained language models (\plm{}s).
% However, existing tokenization methods (e.g., Byte Pair Encoding) are only suited for languages with clear morphological patterns. As one of the only existing isolating and logographic languages in the world, Chinese has no morphological inflection nor word boundaries.
% Thus, directly applying existing tokenizers completely ignores the unique linguistic characteristics of Chinese. 
Existing tokenization methods for Chinese \plm{}s typically treat each character as an indivisible token. However, they ignore the unique feature of the Chinese writing system where additional linguistic information exists below the character level, \textit{i.e.}, at the sub-character level.
To utilize such information, we propose sub-character (SubChar for short) tokenization.
% In this work, we make a first step towards building better tokenizers for Chinese. 
Specifically, we first encode the input text by converting each Chinese character into a short sequence based on its glyph or pronunciation, and then construct the vocabulary based on the encoded text with sub-word segmentation. 
%
% We implement various sub-character tokenizers with different encoding schemes and conduct comprehensive analysis.
%
% We find that sub-character tokenizers have two main advantages over existing tokenizers: 1) Sub-character tokenizers can tokenize inputs into much shorter sequences, thus improving the computational efficiency. 2) The sub-character tokenizers based on pronunciation can encode Chinese homophones into the same transliteration sequences before tokenization, thus being robust to homophone typos.
Experimental results show that SubChar tokenizers have two main advantages over existing tokenizers: 1) They can tokenize inputs into much shorter sequences, thus improving the computational efficiency. 2) Pronunciation-based SubChar tokenizers can encode Chinese homophones into the same transliteration sequences and produce the same tokenization output, hence being robust to homophone typos.
At the same time, models trained with SubChar tokenizers perform competitively on downstream tasks. 
% Correspondingly, \plm{}s with the sub-character tokenizers perform competitively on downstream tasks.
% We release all of our code, tokenizers and model checkpoints to facilitate future work.
We release our code and models at \url{https://github.com/thunlp/SubCharTokenization} to facilitate future work.

% Tokenization is a fundamental part of any NLP system.
% Existing tokenization methods (e.g., Byte Pair Encoding) are only suited for languages with clear morphological patterns, while Chinese, one of the only existing isolating and logographic languages in the world, has no morphological inflection nor word boundaries. Thus, directly applying existing tokenizers completely ignores the unique linguistic characteristics of Chinese. 

%%clsi: better to differentiate tokenization vs tokenizer



% Old versions
%Xiaozhi: backup
%  Conventional tokenization methods for Chinese language models treat each character as a unique token in the vocabulary~\cite{BERT}, completely ignoring the rich linguistic information contained in the pronunciation and glyph (\textit{i.e.}, shape) of Chinese characters.  
%  In this work, based on the unique linguistic characteristics of the Chinese writing system, we analyse three key aspects: pronunciation, glyph and word boundary. 
%  Targeting these factors, we propose three corresponding types of tokenizers: 1) \shuowen~(\textit{说文}, meaning Talk Word), the pronunciation-based tokenizers; 
%  2) \jiezi~(\textit{解字}, meaning Solve Character), the glyph-based tokenizers;
%  3) Word Segmented Tokenizers, the tokenizers with Chinese Word Segmentation.
%  %
%  We empirically verify the effectiveness of each proposed tokenizer via pretraining a BERT-style language model and evaluating it on various downstream NLU tasks.
%  %
%  We find that pronunciation-based (\shuowen) and glyph-based (\jiezi) tokenizers can outperform conventional single-character tokenizers on multiple downstream tasks, while Chinese Word Segmentation has no benefit as a preprocessing step.
%  %
% Moreover, our proposed \shuowen~ and \jiezi~ tokenizers not only perform competitively on standard NLU benchmarks, they are also advantageous when handling noisy texts since they can learn useful information from sub-characters. 
% %
%  We will release our code and pretrained models and hope that our work will motivate more future works on  linguistically informed techniques for Chinese NLP.~\footnote{We refer readers to the Appendix for a detailed explanation on the Chinese meaning of \shuowen-\jiezi~ and the history behind.}



% Conventional tokenization methods for Chinese pretrained language models (\plm{}s) treat each character as an indivisible token~\cite{BERT}, which ignores the characteristics of the Chinese writing system. In this work, we comprehensively study the impact of three unique factors on the Chinese tokenization for \plm: pronunciation, glyph (\textit{i.e.}, shape) and word boundary.
% Correspondingly, we propose three kinds of tokenizers: 1) \shuowen~(\textit{说文}, meaning Talk Word), the pronunciation-based tokenizers; 
% 2) \jiezi~(\textit{解字}, meaning Solve Character), the glyph-based tokenizers; 
% and 3) Word segmented tokenizers, the tokenizers that incorporate Chinese word segmentation (\cws).
% To empirically compare the effectiveness of the studied tokenizers, we pretrain BERT-style language models with the different tokenizers and evaluate the models on various downstream NLU tasks.
% We find that (perhaps surprisingly) the use of pronunciation and glyph encoding, as well as injecting \cws ~information into the vocabulary, can all improve the efficiency of tokenization by largely reducing the tokenized sequence lengths (as large as 40\% length reduction on downstream datasets), while maintaining competitive performance.
% Moreover, the proposed \shuowen~and \jiezi~tokenizers achieve certified robustness against any glyph- and pronunciation-based character substitution, which simulates noisy user-generated inputs.
% %
% The code and pretrained models will be released to facilitate more linguistically informed Chinese NLP work.~\footnote{Please refer to Appendix~\ref{app:shuowenjiezi} for an enriching explanation of the historical meaning of \shuowen-\jiezi.}