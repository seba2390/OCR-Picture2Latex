\label{sec:analysis}

% \input{tables/encode_ablation}

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figures/WubiBERT-Fig2.png}
\caption{Breakdown of different types of tokens in the vocabularies of various tokenizers. We observe the clear trend that in our SubChar tokenizers, a small fraction of sub-character tokens saves the space to store much more character combination tokens (e.g., words and phrases).}
\label{fig:composition}
\end{figure}


In this section, we conduct various analyses to better understand the working mechanisms of SubChar tokenization, including illustrations of the efficiency improvement and ablations on different components of our tokenization pipeline. 

\subsection{Vocabulary Composition}

We break down the vocabulary of each tokenizer into three different categories: sub-character tokens, character tokens, and character combination tokens (words and phrases). 
%
As shown in Figure~\ref{fig:composition}, character tokenizers only have character tokens, while sub-word tokenizers have a small percentage of combination tokens. The main reason for the relatively small number of combination tokens in sub-word tokenizers is that unlike how English words are composed with $26$ alphabet letters, there are thousands of unique Chinese characters, which take up a large proportion of the vocabulary in order to maintain coverage.

In contrast, SubChar tokenizers use a very small fraction of sub-character tokens to compose many complex Chinese characters, therefore saving up a large percentage of the vocabulary to store combination tokens. This brings the advantage of having more words and phrases in the tokenized outputs, thus shortening the sequence lengths, as elaborated in the next section.

\subsection{Efficiency Improvement}

\input{tables/length}

\input{tables/finetune_time}

The direct consequence of having more character combinations in the vocabulary is that the tokenized sequences are shorter. Table~\ref{tab:output_lengths} shows the average sequence length by using different tokenizers on two downstream datasets. 
We observe that SubChar tokenizers can tokenize the inputs into much shorter sequences.
% We observe that different encoding methods do make a difference in terms of output length reduction. Encoding methods like Wubi, Cangjie, and Pinyin are particularly efficient. 
% Moreover, we find that the SubChar-Pinyin-NoIndex variant achieves even more output length reduction than the SubChar-Pinyin counterpart (40\% shorter on iFLYTEK and 31\% shorter on TNEWS compared to the CharTokenizer baseline). 
% This shows that SubChar-Pinyin-NoIndex is not only more robust, but also more efficient.

% During finetuning, the short sequence lengths allow us to reduce the maximum sequence length of each batch, leading to faster training.
Moreover, our SubChar tokenizers can speed up training for both pretraining and finetuning.
During finetuning, we can pack multiple sequences into one input sequence to reduce the computation waste introduced by sequence padding~\cite{kosec2021packing}, and shorter sequence lengths allow the sequences to be packed more densely, thus increasing the overall throughput.
% ~\footnote{For simplicity, we perform packing using the First-Fit Bin Packing algorithm.}

 \begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{figures/ifly_time.png}
\caption{Training curves on the iFLYTEK dataset with two different models. The y-axis indicates classification loss (cross-entropy), the x-axis indicates time (seconds). Our SubChar-Pinyin-NoIndex model gets a lower loss than the CharTokenizer baseline throughout training.}
\label{fig:ifly_loss_time}
\end{figure}

\input{tables/compression}

% packing啥的对于长序列也能做，感觉不用提
Table~\ref{tab:finetune_time} shows the model finetuning time relative to the CharTokenizer baseline. We observe significant speedup by SubChar tokenizers, finishing in as little as $68.9\%$ time on iFLYTEK with the SubChar-Pinyin-NoIndex tokenizer.
In Figure~\ref{fig:ifly_loss_time}, we plot the training curves for the CharTokenizer baseline and the SubChar-Pinyin-NoIndex model on the iFLYTEK dataset, we observe that our SubChar-Pinyin-Noindex model indeed converges much faster and achieves lower training loss in the end.  


The speedup on pretraining is also significant. While the running speed differs on different machines, the compression brought by the shorter tokenized outputs is hardware-invariant. In Table~\ref{tab:compression}, we show the relative size (disk memory) of the tokenized pretraining corpus. We observe that SubChar tokenizers can tokenize the raw pretraining texts into shorter sequences than the baselines, thus resulting in a much smaller pretraining data (\textit{e.g.}, as much as $25.3\%$ smaller than that of the CharTokenizer baseline with SubChar-Pinyin-NoIndex). In turn, this can translate to much faster pretraining on any training infrastructure.



% On the other hand, we observe that \cws{} and non-linguistic encoding methods like Byte and RandomIndex are not as efficient as most of the linguistic encoding methods.


\input{tables/vocab_size_ablation}


\input{tables/encode_ablation}


\subsection{Impact of Vocabulary Size}

Intuitively, when we increase the vocabulary size, there will also be more room to store combination tokens (\textit{e.g.}, words and phrases), leading to a decrease in tokenization length and thus better efficiency. Although we used the standard vocabulary size of 22675 in our previous experiments, to understand whether the efficiency benefits of SubChar tokenization wear off at larger vocabulary size, we perform an additional ablation on the impact of vocabulary size. 

As shown in Table~\ref{tab:vocab_size_ablation}, as we increase the vocabulary size, the efficiency advantage of SubChar tokenizers slightly diminishes. However, even at a very large vocab size of 60000, our SubChar-Pinyin tokenizer still tokenizes the inputs into significantly shorter sequences than the Sub-word baseline.  We thus conclude that the efficiency advantage of our SubChar tokenizers would hold in most practical cases where the vocabulary size is typically under 60000 (such as BERT and RoBERTa). 



\subsection{Impact of Pretraining Data Size}
\label{sec:pretraining-data}

To understand the impact of pretraining data size, we take the checkpoints of the 12-layer Transformer models pretrained on the 2.3G Baike corpus, and further pretrain them on a much larger corpus of 22.1GB text. This 22.1GB corpus is sampled from Chinese web text\footnote{\url{https://github.com/OpenBMB/CPM-Live}}, mainly consisting of books and web pages. We further pretrain for 8K steps with a maximum sequence length of 512. 

As shown in the bottom block of Table~\ref{tab:main_results}, further training on this larger corpus leads to small improvement on average performance (72.23 $\rightarrow$ 72.81 for CharTokenizer and 72.87 $\rightarrow$ 73.42 for SubChar-Pinyin), possibly because the original models trained on 2.3GB corpus are already close to being fully trained. More importantly, this result shows that even with pretraining on larger corpora, our proposed methods can still match or slightly outperform baselines on the downstream datasets. 

\subsection{Impact of Encoding Methods}
\label{sec:encoding}

As described in Section~\ref{sec:method}, 
we experiment with different types of encoding methods and compare their downstream performance to analyze the impact.

Our previous encoding methods are based on the hypothesis that linguistic information such as glyph or pronunciation provides useful inductive biases to the model. However, in the case where this hypothesis is not true, it is possible that non-linguistic encoding methods may work as well. To verify this, we add two encoding methods that do not consider any linguistic information: \textit{Byte Encoding} and \textit{Random Index Encoding}, for the purpose of ablation analysis.

In \textit{Byte Encoding}, we convert every character into its byte sequence, same as in ByT5~\cite{ByT5}. In cases where the byte sequence consists of multiple indices (each Chinese character has three byte indices), we concatenate them and append the character separation symbol as the encoding (\textit{e.g., 魑} $\rightarrow$ \textit{233\_173\_145\#}). 

In \textit{Random Index Encoding}, we map each character into a unique and randomly generated five-digit index and append the character separation symbol as the encoding ( \textit{e.g., 魑} $\rightarrow$ \textit{29146\#} ). 


We train SubChar tokenizers with all the different encoding methods and compare the corresponding BERT models using these tokenizers on downstream tasks. The results are presented in Table~\ref{tab:encode_ablation}. We observe that the differences between these different tokenizers are rather small in terms of the model performance on downstream datasets. Moreover, perhaps somewhat surprisingly, tokenizers with the non-linguistic encoding methods -- SubChar-Byte and SubChar-RandomIndex -- can also perform competitively despite the fact that they do not capture glyph or pronunciation information like the other tokenizers. 

These results suggest that linguistic encoding may not be necessary for SubChar tokenizers to achieve high performance on downstream tasks. However, the linguistic encoding methods can build more robust and efficient tokenizers as illustrated in previous sections.




% \subsection{Impact of Subword Tokenization Implementation}

% In previous experiments, we use the unigram LM method to construct the vocabulary based on the transliterated sequences. There are alternative implementations of the subword tokenization algorithm. In this section, we replace the unigram LM algorithm with byte-pair encoding (BPE) and analyze whether the specific choice of subword tokenization implementation matters for the overall performance. 

% We train a tokenizer on Pinyin transliterated corpus with BPE as the subword tokenization implementation. We pretrain a 6-layer transformer with this tokenizer and compare the performance with the unigram LM counterpart in Table~\ref{tab:encode_ablation} (comparing the first and last row of the results). 

% We further compare the vocabulary composition and tokenized sequence lengths between tokenizers using the unigram LM and BPE implementations, we share the detailed results in the appendix.


\subsection{Impact of Vocabulary Construction Algorithm}
\label{sec:bpe}

In previous experiments, we used the Unigram LM implementation in SentencePiece for vocabulary construction. We perform an additional ablation where we replace Unigram LM with Byte Pair Encoding (BPE) for vocabulary construction to train a pinyin-based tokenizer, while holding all other hyper-parameters constant. 

We compare the SubChar-Pinyin-BPE variant with the unigram LM (SubChar-Pinyin) tokenizer. We find that these two perform similarly. In terms of \textbf{efficiency}: SubChar-Pinyin-BPE tokenizes iFLYTEK to an average length of 184.4 and tokenizes TNEWS to an average length of 15.9. In comparison, SubChar-Pinyin tokenizes iFLYTEK to an average length of 185.2 and tokenizes TNEWS to an average length of 16.1. The vocabulary compositions of the two are also similar, where character combination takes up the majority of the space in the vocabulary for both BPE and unigram LM implementations. 
In terms of \textbf{performance}, we observe in Table~\ref{tab:encode_ablation} that the BPE implementation and the unigram LM implementation have little difference in downstream task performance.
Based on these results, we conclude that the choice of which vocabulary construction to use has a marginal impact on the tokenization efficiency and model performance. 

% \chenglei{TODO: Complete this part.}