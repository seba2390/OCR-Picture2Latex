Our experiments are focused on natural language understanding tasks. We recognize that adapting our SubChar tokenization to language generation tasks might require additional efforts, for example, we may want to avoid cases of predicting sub-character tokens that do not form complete characters. Also, evaluating the robustness of language generation models on real-world input noises may require additional benchmarks beyond those used in this paper. We leave such exploration as an interesting direction for future work. 

Another limitation is that our method is designed specifically for the Chinese language. While we hypothesize that our method can also bring benefits to other languages with ideographic symbols, such as Kanji in Japanese, we leave such investigation to future work. 

