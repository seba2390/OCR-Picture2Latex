\ncp\ncp\ncp
\section{Experiments}
\label{sec:eval}

\subsection{Benchmarks} 
We evaluate {\sysname} as well as the other baselines  on four  benchmark datasets: 1) \textit{Open AutoML Benchmark}~\cite{automl_benchmark}, a collection of 39 binary and multi-class \textit{classification} datasets (used by FLAML \cite{flaml}). The datasets are selected such that they are representative of the real world from a diversity of problem domains and of enough difficulty for the learning algorithms. 2) \textit{Penn Machine Learning Benchmark} (PMLB) \cite{pmlb}: Since Open AutoML Benchmark is limited to classification datasets, the authors of FLAML \cite{flaml} evaluated their system on 14 more \textit{regression} datasets selected from PMLB, such that the number of samples is more than 10,000. To demonstrate the generality of our approach, we include those datasets in our evaluation as well. 3) \textit{AL's datasets}: We also evaluate on the datasets used for AL's \cite{al} evaluation which include 6 Kaggle datasets (2 regression and 4 classification) and another 18  classification datasets (9 from PMLB and 9 from OpenML). Unlike other benchmarks, the Kaggle datasets include datasets with textual features. 
4) VolcanoML's datasets: finally, we evaluate {\sysname} on 44 more datasets used by VolcanoML \cite{VolcanoML}. The authors of VolcanoML evaluate their system on a total of 66 datasets from OpenML and Kaggle, from which 11 datasets are not specified, 10 datasets overlap with ours, and 1 dataset consists of image samples. 
\autoref{dataset_stats} includes a summary of all 121 benchmark datasets.
\hitext{The detailed statistics of all datasets are shown in the appendix. %of~\cite{kgpip}. 
These statistics include names, number of rows and columns, number of numerical, categorical, and textual features, number of classes, sizes, sources, and papers that evaluated on them.}


%Tables \ref{tab:dataset_stats} and \ref{tab:dataset_stats_volcano} in the appendix, which include names, number of rows and columns, number of numerical, categorical, and textual features, number of classes, sizes, sources, and papers that evaluated on them.

\input{tables/benchmark_stats.tex}



\input{figures/exp_1h_vs_fl_as.tex}

\subsection{Baselines}
We empirically validate {\sysname} against three AutoML systems: (1) Auto-Sklearn (v0.14.0) \cite{autosklearn} which is the overall winner of multiple challenges in the ChaLearn AutoML competition \cite{chalearn}, and one of the top 4 competitors reported in the Open AutoML Benchmark \cite{automl_benchmark}. (2) FLAML (v0.6.6) \cite{flaml}: an AutoML library designed with both accuracy and computational cost in mind. FLAML outperforms Auto-Sklearn among other systems on two AutoML benchmarks using a low computational budget, (3) AL \cite{al}: a meta-learning-based AutoML approach that utilizes dynamic analysis of Kaggle notebooks, an approach that has similarities to ours, and (4) VolcanoML (v0.5.0)~\cite{VolcanoML}, a recent AutoML approach which proposes efficient decomposition strategies for the large AutoML search spaces. \hitext{In all our experiments, we used the latest code provided by the authors for existing systems, the same exact hardware, time budget, and the parameters recommended by the authors of these systems.}

\subsection{Training Setup} 
Because our approach to mining historical pipelines from scripts is relatively cheap, we can apply it more easily on a wider variety of datasets to form a better base as more and more scripts get generated by domain experts on Kaggle competitions.  \hitext{In this work, we performed program analysis on 11.7K scripts associated with 142 datasets, and then selected those with estimators from \texttt{sklearn}, \texttt{XGBoost} and \texttt{LightGBM} since those were the estimators supported by the most AutoML systems for classification and regression.  This resulted in the selection of 2,046 notebooks for 104 datasets; a vast portion of the 11.7K programs were about exploratory data analysis, or involved libraries that were not supported by Auto-Sklearn \cite{autosklearn} or FLAML (e.g., PyTorch and Keras) \cite{flaml}.} 
We used Macro F1 for classification tasks to account for data imbalance, if any, and use $R^2$ for regression tasks, as in FLAML \cite{flaml}. We  also varied the time budget given to each system between 1 hour and 30 minutes, to measure how fast can {\sysname} find an efficient pipeline compared to other approaches. The time budget is end-to-end, from loading the dataset till producing the best AutoML pipeline. In all experiments, we report averages over 3 runs.


\subsection{Comparison with Existing Systems}
\label{baselines_comparison}

\input{figures/exp_1h_vs_fl_diff_graphs}

In this section, we evaluate {\sysname} against state-of-the-art approaches; FLAML \cite{flaml} and Auto-Sklearn \cite{autosklearn}. Figure \ref{1hr_exps} shows a radar graph of all systems when given a time budget of 1 hour. It shows the performance of all systems on the three tasks in all benchmarks, namely, binary classification, multi-class classification, and regression. For every dataset, the figure shows  the actual performance metric (F1 for classification and $R^2$ for regression) obtained from every system
 \footnote{The detailed scores for every system and dataset as well as the corresponding names of datasets are shown in tables \ref{tab:dataset_stats}-\ref{tab:detailed_scores_volcano} in the appendix.}. 
Therefore, the out most curve from the center of the radar graph has the best performance. In Figure \ref{1hr_exps}, both variations of {\sysname} achieve the best performance across all tasks, outperforming both FLAML and Auto-Sklearn. We also performed a \textit{two-tailed t-Test} between the performance obtained by {\sysname} compared to the other systems. The results show that {\sysname} achieves  significantly better performance than both FLAML and Auto-Sklearn with a t-Test value of $0.01$ and $0.0002$, respectively (both have $p < 0.05$). 

\input{tables/exp_avg_scores.tex}



Table ~\ref{averages_1h} also shows the average F1 and $R^2$ values for classification and regression tasks, respectively. The results show that both variations of {\sysname} achieve better performance compared to both FLAML and Auto-Sklearn over all tasks and datasets. 

\textit{Scalability of {\sysname}'s meta-learning against existing systems}: 
The AL meta-learning approach~\cite{al} mines pipelines using dynamic code analysis, which has high cost as discussed in Section~\ref{sec:static}. Thus, the authors of AL provided a pre-trained meta-learning model on 500 pipelines and 9 datasets, which does not scale to cover various cases. In contrast, we trained our meta-learning model using 2000 pipelines and 142 datasets. None of these datasets were included in the 77 datasets used in testing. AL failed in 22 and timed out in 38 datasets. This shows that the {\sysname} meta-learning approach, which is based on pipelines semantics and dataset representation learning, is more effective.
AL failed on many of the datasets during the fitting process. As the figure shows, {\sysname} still outperforms all other approaches, including AL, significantly. On these datasets, AL achieved the lowest F1 score on binary and multi-class classification tasks with values of 0.36 and 0.36, respectively. This compares to 0.74 and 0.75 by FLAML, 0.73 and 0.68 by Auto-Sklearn,  0.79 and  0.79 by {\sysname FLAML}, and 0.79 and 0.74 by {\sysname Auto-Sklearn}. 



\begin{figure}
\ncp\ncp
\centering
  \includegraphics[width=0.45\textwidth]{figures_raw/kgpip_vs_volcano.pdf}
  \ncp\ncp\ncp\ncp\ncp
  \caption{\hitext{Score difference between KGpipFLAML and VolcanoML on the 44 classification and regression datasets from VolcanoML with a time budget of 1 hour. For brevity, we removed from this Figure the 22 datasets on which both systems perform comparably (within a difference of $\leq 0.01$). 
%   and the datasets overlapping with the ones shown in Figure \ref{1hr_exps}.
  }}
  \label{volcanoML_exp}
%   \ncp\ncp\ncp\ncp\ncp\ncp
\end{figure}

\input{tables/exp_avg_scores_kgpip_vs_volcano}

\textit{VolcanoML Datasets}: VolcanoML used a variety of datasets that are not included in our 77 datasets of \autoref{1hr_exps}. Some of these datasets are quite large which are meant to test the the system scalability. Therefore, we also collected all 49 the datasets we could find in their paper and tested the best version of {\sysname} (KGpipFLAML) against VolcanoML on these datasets with a time budget of 1 hour. The performances of KGpipFLAML and VolcanoML are shown in Figure \ref{volcanoML_exp}. For brevity, we omitted from the figure all datasets on which the performance difference between both systems is $\leq 0.01$ \hitext{and the datasets overlapping with the ones shown in Figure \ref{1hr_exps}. On those datasets, KGpipFLAML found a valid pipeline for all of them, sometimes with a decent absolute difference in F1 or $R^2$ scores of  $ \geq 0.90$. Across all the 44 datasets, KGpipFLAML achieved significantly better average of scores compared to VolcanoML (statistical significance test of $p$ < 0.05), see Table~\ref{averages_1h_vs_volcano} for details.} 




\input{figures/exp_top_learners.tex}

\input{tables/nonabstracted}



\subsection{Ablation Study}
\label{sec:study}


\subsubsection{The effectiveness of {\GML}}
Our {\GML} approach manages to reduce dramatically the number of nodes and edges in the code graph.  Using the original graph obtained from static analysis, it produces the {\GML} graph that focuses on the core aspects needed to train a graph generation model for ML pipelines, such as data transformations, learner selection, and hyper-parameter selection. This experiment investigates the scalability of our graph generation model based on two different training sets, i.e., the sets of {\GML} graphs described in section~\ref{abstraction} vs. the original set of code graphs from static analysis for the same ML pipeline scripts. 




For this experiment, we use a small-scale training set of 82 pipeline graphs pertaining to one classification dataset. The original code graphs for these 82 pipelines include 29,139 nodes and 252,486 edges. Our {\GML} graph, however, includes 974 nodes and 1052 edges. This is a graph reduction rate of at least 96.6\%, Figure \ref{tab:nonabstracted} shows these detailed statistics. The main investigation here is whether this huge reduction ratio will help improving the accuracy and scalability of our graph generation model. We train one model on the original code graph and another on the {\GML} graphs. Both models are trained for 15 epochs with the same set of hyperparameters. It is worth noting that due to the huge time required to process the nodes and edges in the code graph, we had to reduce the number of epochs from 400 to 15.



We test the performance of {\sysname} when trained on both graphs on the most trivial binary and multi-class classification datasets in the AutoML benchmark. These are the  datasets where the F1 score of all the reported systems in section \ref{baselines_comparison} is above 0.9. The result is a total of 5 datasets (1 binary and 4 multi-class). Both models use Auto-Sklearn as the hyperparameter optimizer with a time budget of 15 minutes and 3 graphs. We take the average of three runs. The results are summarized in Table \ref{tab:nonabstracted}. For these trivial datasets, the model trained using code graphs did not manage to generate any valid ML pipeline. This means the model failed to capture the core aspects of ML pipelines, i.e., valid transformation or learners. Moreover, our {\GML} approach helps {\sysname} to reduce the training time by 99\%, as shown in Table \ref{tab:nonabstracted}. 





\subsubsection{The {\sysname} meta-learning quality}

This experiment tests the quality of our meta-learning component. We test the performance as we vary the number of graphs selected from the graph generation phase before feeding it to the hyper-parameter optimization module.  Table \ref{tab_top_graphs} shows the KGpipFLAML performance as we vary the number of predicted graphs between 3, 5 and 7. 
%The results indicate that {\sysname} achieves significantly better performance compared to FLAML with a t-Test value of 0.03 and 0.01 when using 5 and 7 graphs, respectively. 

\begin{table}[t]
\ncp\ncp\ncp
\caption{Performance of KGpipFLAML (mean and standard deviation) as we vary the number of predicted pipeline graphs within 30 minutes time limit. We obtained similar results for KGpipAutoSklearn, and hence omitted its results.}
\ncp\ncp\ncp
\begin{tabular}{llll}
\toprule
             & Binary  & Multi-Class  & Regression \\
\midrule
Top-3 graphs &     0.80 (0.14)                  &       0.70 (0.31)                     &    0.71 (0.23)        \\
Top-5 graphs &       0.81 (0.14)                &     0.73 (0.26)                       &      0.70 (0.23)      \\
Top-7 graphs &      0.81 (0.14)                 &    0.75 (0.24)                        &   0.71 (0.24) \\
\bottomrule
\end{tabular}
\label{tab_top_graphs}
\ncp\ncp\ncp\ncp\ncp\ncp
\end{table}

With only 3 graphs, {\sysname} is still outperforming FLAML (second best system after KGpipFLAML) , although the effect is weaker (t-Test value = 0.06). Compared to Auto-sklearn (third best system after KGpipFLAML), all variations have similar or better performance, but the difference is insignificant. 
This experiment shows that even with three graphs, {\sysname} outperforms FLAML and KGpipAuto-Sklearn, i.e., the correct pipelines often appear in the top 3. As another assessment of the quality of our predictions, we measure where in our ranked list of predicted pipelines the best pipeline turned out to be.  Ideally, the top pipeline would always be first, and we use Mean Reciprocal Rank (MRR) to measure how close to that our predictions are.  Across all runs, the MRR is 0.71, indicating that the top pipeline is typically very near the top.

\subsubsection{The {\sysname} meta-learning diversity} 
One question we addressed is whether {\sysname} produced different pipelines for the \textit{same dataset} across different runs. This gives us a sense of whether {\sysname} is deterministic, or whether it produces different pipelines to help with pruning the AutoML search space.  We took different runs for the exact same dataset, and created a list of learners and transformers produced for each dataset across runs.  The list was limited by the shortest number of learners and transformers produced across runs.  We then computed correlations for datasets across runs 1, 2, and 3.  The correlations ranged from 0.60 - 0.64, suggesting that the runs did not produce the same transformers and learners across runs.
We also examined the types of learners selected by {\sysname} for consideration.  
% Figure~\ref{varying operators chosen} shows the sorts of learners and transformers  selected by {\sysname}.
Figure~\ref{fig:diversity} shows the learners and transformers found at least 20 times in the training pipelines.  One can see from the figure that {\sysname} does not blindly output learners and transformers by counts. 
Figure~\ref{fig:coverage} shows more diversity in what was selected overall. So, a variety of methods are covered by {\sysname}. 



% \subsubsection{Dataset Embedding Quality}
% {\sysname} utilizes dataset embeddings to perform similarity search between datasets. These embeddings are based on the actual content of the datasets unlike pre-defined meta-features used by other AutoML systems such as Auto-Sklearn \cite{autosklearn} and AL \cite{al}. 
% In this experiment, we perform a qualitative analysis of the dataset embeddings used in {\sysname}. Figure \ref{tsne_plot} illustrates the t-SNE plot for 38 Kaggle datasets classified by their domains such as sales, financing, and customer reviews. As shown, datasets from the same domains have close embeddings despite never encountering these datasets when learning the embeddings. This shows the generality of our approach to dataset similarity search, which does not only eliminate the need for hand-crafted meta-features, but also results in a more accurate similarity search.

% \input{figures/dataset_embedding.tex}