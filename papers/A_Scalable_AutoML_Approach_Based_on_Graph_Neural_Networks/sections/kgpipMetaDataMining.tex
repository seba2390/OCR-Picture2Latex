\section{The {\sysname} Scalable Meta-Learning}
\label{sec:mining}

% Figure~\ref{fig:mining} shows an overview of {\sysname}'s approach. {\sysname} is based on mining large databases of ML pipelines associated with the used datasets.
Our meta-learning approach is based on mining large databases of ML pipelines associated with the used datasets, as illustrated in Figure~\ref{fig:mining}. 
The mining process uses static program analysis instead of executing the actual pipeline scripts or preparing the actual raw data. The {\sysname} meta-learning component enhances the search strategy of existing AutoML systems, such as AutoSklearn and FLAML, and allows these systems to handle ad-hoc datasets, i.e., unseen ones. 
%We use knowledge graph technologies to retain a maximal degree of flexibility by capturing metadata and semantics in a flexible graph format with deep learning models on them. 
To retain a maximal degree of flexibility, {\sysname} captures metadata and semantics in a flexible graph format, and relies on graph generator models as the database of pipelines. 

Unlike existing meta-learning approaches, our approach is designed to learn from a large scale database and achieve high degree of coverage and diversity. Several ML portals, such as Kaggle or OpenML~\cite{OpenML}, provide access to thousands of datasets associated with hundreds of thousands of public notebooks, i.e., ML pipelines/code. 
{\sysname} mines these large databases of datasets and pipelines using static analysis and filters them into ML pipelines customized for the learner selection problem.
The {\sysname} meta-learning approach leverages~\cite{graph4code} for code understanding via static analysis of scripts/code of ML pipelines. It extracts the semantics of these scripts as code and form an initial graph for each script. 

{\sysname} cleans the graphs generated by ~\cite{graph4code} to keep the semantic required for the ML meta-learning process. Furthermore, our approach introduces dataset nodes and interlinks the relevant pipeline semantic to them. So, our meta-learning approach produces MetaPip, a highly interconnected graph of seen datasets and pipelines applied to them. We also developed a deep embedding model to find the closest datasets to an unseen one, i.e., to effectively prune MetaPip. We then train a deep graph generator model~\cite{deepgmg} using MetaPip. This model is the core of our meta-learning component as illustrated in Figure~\ref{fig:mining} and discussed in the next section.  
 

\input{figures/architecture_mining}



\subsection{Graph Representation of Code Semantics}
\label{sec:static}

Static and dynamic program analysis techniques could be used to abstract the semantics of programs and extract language-independent representations of code. A program source code is examined in the static analysis without running the program. In contrast, dynamic analysis examines the source code during runtime to collect memory traces and more detailed statistics specific to the analysis technique. Unlike static analysis, dynamic analysis helps in capturing more rich semantics from programs with the high cost of execution and storing massive memory traces. ML portals, such as Kaggle, have hundreds of thousands of ML pipelines with no instructions for running or managing the environments of these pipelines. {\sysname} combines dataset embedding with static code analysis tools, such as {\sysGC}~\cite{graph4code}, to enrich the collected semantics of ML pipelines while avoiding the need to run them.  

\input{figures/running_example}

\input{figures/graphgen4code_graph.tex}

{\sysGC} is optimized to efficiently process millions of Python programs, performing interprocedural data flow and control flow analysis to examine for instance, what happens to data that is read from a Pandas dataframe, how it gets manipulated and transformed, and what transformers or estimators get called on the dataframe.  {\sysGC}'s graphs make it explicit what APIs and functions are invoked on objects without the need to model the used  libraries themselves; hence {\sysGC} can scale static analysis to millions of programs. 
Figures~\ref{running_ex} and \ref{static_analysis} show a small code snippet and its corresponding static analysis graph from {\sysGC}, respectively. As shown in Figure \ref{static_analysis}, the graph captures control flow (gray edges), data flow (black edges), as well as numerous other nodes and edges that are not shown in the figure. Examples of these nodes and edges include those capturing location of calls inside a script file and function call parameters. For example, {\sysGC} generates a graph of roughly 1600 nodes and 3700 edges for a Kaggle ML pipeline script of 72 lines of code. The number of nodes and edges dominate the complexity of training a graph generator model. 

\input{figures/filtered_graph}

\subsection{MetaPip: from Code to Pipeline Semantics}
\label{abstraction}

For AutoML systems, a pipeline is a set of data transformations, learner selection, and hyper-parameter optimization for each model that is selected. Mined data science notebooks often contain data analysis, data visualization, and model evaluation. Moreover, each notebook is associated with one or more datasets. Thus, it is essential for our meta-learning model to distinguish between different types of pipelines and realize this association with datasets. Existing systems for static code analysis extract general semantics of code and cannot link pipeline scripts to the used datasets.  Thus, the generated graphs by systems, such as {\sysGC}, are scattered and unlinked, i.e., a graph per an ML pipeline script. Moreover, each graph will have nodes and edges that are not relevant for the meta-learning process. \hitext{These irrelevant nodes and edges, i.e., triples, will add noise to the training data. Hence, a meta-learning model will not be able to learn from the abstracted graph pipelines generated by such tools, as shown in Table~\ref{tab:nonabstracted}. We developed a method to filter out this kind of triples from {\sysGC}'s graph and analyze ML pipelines to prepare a training set interconnecting repositories of ML pipeline scripts with their associated datasets. Moreover, our method cleans the noisy nodes and edges and calls to modules outside the target ML libraries. For example, our method will extract triples related to libraries, such as Scikit-learn, XGBoost, and LGBM. These libraries are the most popular among the top-scoring ML pipelines in ML portals. The code for the cleaning method is available at the {\sysname}'s repository.}

Our meta-learning component aims to pick learners and transformer for unseen datasets. Thus, {\sysname} links the filtered ML pipelines with the used datasets. The result of adding these dataset nodes is a highly interconnected graph for ML pipelines, we refer to it as \textit{{\GML}}. Our  {\GML} graph captures both the code and data aspects of ML pipelines. Hence, we can populate the {\GML} graph with datasets from different sources, such as OpenML and Kaggle, and pipelines applied on these datasets. Figure \ref{abstraction_figure} shows the {\GML} graph corresponding to the code snippet in Figure~\ref{running_ex}. {\sysname} utilizes {\GML} to train a model based on a large set of pipelines associated with similar datasets. For example, a \textit{pandas.read\_csv} node will be linked to the used table node, i.e., csv file. In some cases, the code, which reads a csv file, does not explicitly mention the dataset name. The pipelines are usually associated with datasets, such as Kaggle pipelines and datasets, as shown in Figure~\ref{fig:mining}. 



\subsection{Dataset Representation Learning}
Our approach efficiently guides the meta-learning process by linking the extracted semantics of pipelines to dataset nodes representing the used datasets. There is a sheer amount of datasets of variable sizes and we need to develop a scalable method for finding the most similar datasets for an unseen one. The pairwise comparison based of the actual content of datasets, i.e, tuples in CSV files, does not scale. Thus, we developed a dataset representation learning method to generate a fixed-size and dense embedding at the granularity of a dataset, e.g., a table or CSV file. The embedding of a dataset $\mathcal{D}$ is the average of its column embeddings, i.e.:
\begin{equation}
\ncp\ncp
\label{equation_dataset_embedding}
    h_{\theta}(\mathcal{D}) = \frac{1}{|\mathcal{D}|} \sum_{c \in \mathcal{D}}{h_\theta(c)}
\end{equation}
where $|\mathcal{D}|$ is the number of columns in $\mathcal{D}$. Our work generalizes the approach outlined in ~\cite{Mueller2019RecognizingVF} for individual column embeddings, where column embeddings are obtained by training a neural network on a binary classification task.  The model learns when two columns represent the same concept, but with different values, as opposed to columns representing different concepts.  Embeddings for an unseen dataset are produced by the last layer of the neural net.



{\sysname} reads datasets only once and leverages PySpark DataFrame to achieve high task and data parallelism.
We use the embeddings of datasets to measure their similarity. With these embeddings, we build an index of vector embeddings for all the datasets in our training set. We utilize efficient libraries~\cite{JDH17} for similarity search of dense vectors to retrieve the most similar dataset to a new input dataset based on its embeddings. Thus, our method scales well and leads to accurate results in capturing similarities between datasets.   


% For this component, we implemented an embedding model \cite{Mueller2019RecognizingVF} to generate fixed-size and dense columnar embeddings for each column in input datasets. 
% These embeddings are based on deep neural network models \cite{Mueller2019RecognizingVF}, which are more accurate and robust in capturing similarities between the raw values of columns than standard statistical techniques \cite{Mueller2019RecognizingVF}. 
% For instance, the embedding of a column \texttt{temp\_in\_celsius} is similar to that of \texttt{temp\_in\_fahrenheit}, despite having different distributions.

% The column embedding model jointly trains two neural networks $h_{\theta}$ and $g_{\psi}$, where $h_{\theta}$ generates column embeddings and $g_{\psi}$ adjusts the loss function to eliminate bias towards columns with common distributions such as booleans.
% The embedding of a column $\mathcal{C}$, i.e. $h_{\theta}(\mathcal{C})$ is an aggregation of applying $h_{\theta}$ over the column's raw values.
% We trained the models on a collection of 600 OpenML datasets, with each training sample as a triplet in the format $(\mathcal{C}_i, \mathcal{C}_j, y_{ij})$, where $y_{ij}\in\{0,1\}$ indicates whether the two columns are matching, i.e. have similar content semantics. 

% In {\sysname}, we define the embedding on the granuality of datasets, i.e. CSV files. The embedding of a dataset $\mathcal{D}$ is the average of its column embeddings, i.e.:
% \begin{equation}
% \label{equation_dataset_embedding}
%     h_{\theta}(\mathcal{D}) = \frac{1}{|\mathcal{D}|} \sum_{c \in \mathcal{D}}{h_\theta(c)}
% \end{equation}

% where $|\mathcal{D}|$ is the number of columns in $\mathcal{D}$. With these embeddings, we build an index of vector embeddings for all the datasets in our training set. We then use efficient libraries~\cite{JDH17} for similarity search of dense vectors to retrieve the most similar dataset to a new input dataset based on its embeddings.