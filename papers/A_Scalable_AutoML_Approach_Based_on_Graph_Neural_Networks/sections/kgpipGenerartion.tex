\section{The {\sysname} Pipeline Automation}
\label{sec:kgpipGen}

The {\sysname} workflow for pipeline automation is based on our meta-leaning model, as illustrated in Figure~\ref{fig:kgpipGen}. {\sysname} predicts the top-K pipeline skeletons, i.e., a specific set \{$P$, $E$\} of Preprocessor ($P$) and Estimators ($E$), for an unseen dataset ($D$) based on the most similar seen dataset ($SD$), i.e., the nearest neighbour dataset.
{\sysname} starts by finding $SD$ based on the embedding of the unseen dataset. Then, {\sysname} generates the top-K validated ML pipeline graphs $VG$ and converts them into ML pipeline skeletons $\{P,E\}$. Then, it performs hyperparameter optimization using systems, such as  FLAML \cite{flaml} and Auto-Sklearn \cite{autosklearn}, to find the optimum hyperparameters for each pipeline skeleton within a specific time budget.




\subsection{Graph Generation for ML Pipelines}
{\sysname} formulates the generation of ML pipelines as a graph generation problem. The intuition behind this idea is that a neural graph generator might capture more succinctly multiple pipelines seen in practice for a given dataset, and might also capture statistical similarities between different pipelines more effectively. To effectively use such a network, we add a single dataset node as the starting point for the filtered pipelines we generate from Python notebooks. The node is assumed to flow into a \texttt{read\_csv} call which is often the starting point for the pipelines. For generating an ML pipeline, we simply pass in a dataset node for the nearest neighbour of the unseen dataset, i.e., the most similar dataset based on content similarity, as shown in Figure~\ref{fig:kgpipGen}.

\input{figures/architecture_generation.tex}

Our meta-learning model generates ML pipeline graphs in a sequential node-by-node fashion. Algorithm~\ref{algorithm:graph_generation} illustrates the implementation of the graph generation model. For an empty graph $G$ and the most similar dataset $SD$, the algorithm starts by adding an edge between $SD$ and \texttt{pandas.read\_csv}. Then, the graph neural network $f_{AddNode}$ decides whether to add a new node of a certain type. The network $f_{AddEdge}$ decides whether to add an edge to the newly added node. Then, the network $f_{ChooseNode}$ decides the existing node to which the edge is to be added. The While loop at line 7 is repeated repeated until no more edges to be added.  The While loop at line 4 is repeated until no more nodes to be added. The three neural networks, namely $f_{AddNode}$, $f_{AddEdge}$, and  $f_{ChooseNode}$, utilize node embeddings that are learned throughout training via graph propagation rounds. These embeddings capture the structure of ML pipeline graphs.

\input{algorithms/graph_generation}

The generated graph  $G$ is not guaranteed to be a valid ML pipeline. Thus, \hitext{Algorithm~\ref{algorithm:graph_generation} at line 14} checks that $G$ is a valid ML pipeline graph. In {\sysname}, a graph $G$ is valid if 1) it contains at least one estimator matching the task, i.e., regression or classification, and 2) the estimator is supported by the hyperparameter optimizer (AutoSklearn or FLAML in our case). With these modifications, it is possible to generate ML pipelines for unseen datasets using the closest seen dataset node -- more specifically, its content embedding obtained from the dataset embedding module. We  built \hitext{Algorithm~\ref{algorithm:graph_generation} on top of the system proposed in~\cite{deepgmg}}. This system does not support conditional graph generation at test time by default, i.e., building a graph on top of a provided dataset node. We extended this system to generate valid ML pipeline graphs, as illustrated in \hitext{Algorithm~\ref{algorithm:graph_generation}}.  




\subsection{Hyperparameter Optimizion}

\hitext{{\sysname} maps the valid graphs into ML pipeline skeletons, where each skeleton is a set of pre-processors and an estimator with place holders for the optimal parameters. In {\sysname}, the hyperparameter optimizer is responsible for finding the optimal parameters for the pre-processors and learners on the target dataset. Then, {\sysname} replaces the place holders with these parameters. Finally, {\sysname} creates a python script using the pre-processors and estimator achieving the highest scores.}
{\sysname} is well designed to support both  numerical and non-numerical datasets.  Thus, {\sysname}  applies different pre-processing techniques on the given dataset ($D$) and produces a pre-processed dataset ($D'$).
Our pre-processing includes 1) detecting task type (i.e. regression or classification) automatically based on the distribution of the target column 2) automatically inferring accurate data types of columns, 3) \hitext{vectorizing textual columns using word embeddings~\cite{CER2018}}, and 4) imputing missing values in the dataset. In {\sysname}, the hyperparameter optimizer uses $D'$.



Similar to hyperparameter optimizers implemented in AutoML systems, such as FLAML or Auto-Sklearn, {\sysname} works within a provided time budget per dataset. We note here that the majority of the allotted time budget for ML pipeline generation is spent on the hyperparameter optimization; that is, if the user desires only to know what learners would work best for their dataset, {\sysname} can do that almost instantaneously. Given a time budget ($T$), {\sysname} calculates $t$, the time consumed in generating and validating the graphs. {\sysname} then divides the rest of the time budget between the $K$ graphs. Hence, the hyperparameter optimizer has a time limit of ($(T-t) / K$) to optimize each graph independently.   

The hyperparameter optimizer repeatedly applies the learners and pre-processors with different configurations while monitoring the target score metric throughout. {\sysname}  keeps updating its output with the best pipeline skeleton, i.e.,  learners and pre-processors, and its score.  
For example, if the predicted learner is LogisticRegression, it searches for the best combination of regularization type (L1 or L2) and regularization parameter. The difference between hyperparameter optimizers is the search strategy followed to arrive at the best hyperparameters within the allotted time budget. A naive approach would be to perform an exhaustive grid search over all combinations, while a more advanced approach would be to start with promising configurations first.  
We integrate {\sysname} with the hyperparameter optimizers of both FLAML \cite{flaml} and Auto-Sklearn \cite{autosklearn} to demonstrate the generality of {\sysname}. The integration of a hyperparameter optimizer into {\sysname} needs a JSON document of the particular preprocessors and estimators supported by the hyperparameter optimizer. Thus, the integration is relatively easy. Finally, our neural graph generation produces a diverse set of pipelines across runs, allowing for exploration and exploitation.








% \subsection{Graph Generation for ML Pipelines}
% {\sysname} formulates the generation of ML pipelines as a graph generation problem. The intuition behind this idea is that a neural graph generator might capture more succinctly multiple pipelines seen in practice for a given dataset, and might also capture statistical similarities between different pipelines more effectively. To effectively use such a network, we add a single dataset node as the starting point for the filtered pipelines we generate from Python notebooks. The node is assumed to flow into a \texttt{read\_csv} call which is often the starting point for the pipelines. For generating an ML pipeline, we simply pass in a dataset node for the nearest neighbour of the unseen dataset, i.e., the most similar dataset based on content similarity, as shown in Figure~\ref{fig:kgpipGen}.

% \input{algorithms/graph_generation}

% {\color{blue}
% Our neural graph generator is built on the work proposed by \citet{deepgmg}. 
% ML pipeline graphs are generated in a sequential node-by-node fashion. The generation process is shown in \autoref{algorithm:graph_generation} and can be summarized in the following steps: given an empty graph $G$ and the most similar dataset $SD$, (1) add an edge between $SD$ and \texttt{pandas.read\_csv} (2) a graph neural network $f_{AddNode}$ decides whether to add a new node of a certain type, if yes, (3) network $f_{AddEdge}$ decides whether to add an edge to the newly added node, if yes (4) network $f_{ChooseNode}$ decides the existing node to which the edge is to be added. Step (3) is repeated until no more edges to be added, then step (2) is repeated until no more nodes to be added. The three graph neural networks utilize node embeddings that are learned throughout training via graph propagation rounds. These embeddings capture the structure of ML pipeline graphs. 

% The neural graph generator proposed by \citet{deepgmg} does not guarantee the generated graphs to be valid ML pipelines. {\sysname} generates 1000 graphs and validates them based on predefined heuristics.
% A graph G is a valid ML pipeline if 1) it contains at least one estimator matching the task, i.e., regression or classification, and 2) the estimator is supported by the hyperparameter optimizer (AutoSklearn or FLAML in our case). With these modifications, it is possible to generate ML pipelines for unseen datasets using the closest seen dataset node -- more specifically, its content embedding obtained from the dataset embedding module. It also generates multiple competing ML pipeline graphs for an unseen dataset with a score (probability) of each graph.
% }