\section{{\sysname} System}
\label{overview}

This section highlights the main components and workflow of {\sysname}, as illustrated in Figure~\ref{fig:overview}. 

\subsection{Overview of {\sysname}}

{\sysname} aims to predict an optimized ML pipeline for an unseen dataset by mining large repositories of datasets and their associated ML pipelines. Several ML portals, such as Kaggle \footnote{www.kaggle.com}, provide access to thousands of datasets associated with hundreds of thousands of public notebooks, i.e., ML pipelines/code.
\highlight{
{\sysname} trained a neural graph generator to predict top-k graphs/pipelines, i.e., a set {P, E} of Preprocessor (P) and Estimators (E), for an unseen dataset (D) based on the most similar seen dataset (SD), as shown in Figure 1. Then, it applies hyperparameter optimizations to generate optimized pipelines. We observed that similar datasets share similar ML pipelines, i.e., a specific set of {P, E} perform well on these datasets. Thus, we construct a training set interconnecting repositories of ML pipeline scripts with their associated datasets by capturing metadata and semantics in a flexible graph format.}

{\sysname} mine a large set of datasets and pipelines using static analysis and filters them into ML pipelines customized for the learner selection problem. Generation of new pipelines is formulated as a graph generation problem.  Associations between dataset names and their related pipelines are captured in a graph generative model.  Conceptually, as shown in Figure~\ref{fig:overview}, the graph generator functions like a database of datasets and their associated pipelines. Given a new dataset, {\sysname} will convert the dataset into its embedding, obtain its top-k nearest neighbor datasets from our training set, and then use it as seeds for pipeline generation from the graph generator. It then  uses learners and transformers produced by the graph generator to search for hyperparameters. We discuss each of these components in detail below.

We leverage ~\cite{graph4code} and ~\cite{kglac} for code understanding via static analysis of graphs and dataset embeddings, respectively. Then, {\sysname} constructs the graph for use in ML by i) filtering the code graphs to extract a sub-graph representing mainly a flow of objects through transformation and modelling functions, and ii) connecting the filtered graph with the associated dataset, since this structure is apparent in Kaggle. We then train a deep graph generator model \cite{deepgmg} with the dataset node and its associated pipelines. For generation, we first find the nearest neighbour dataset based on the embedding of the unseen dataset, then start the generation process for the trained model based on a node that represents the nearest dataset name. Finally, {\sysname} is integrated with FLAML and Auto-Sklearn, to find the optimum hyperparameters for each generated ML pipeline based on the input dataset.

\begin{figure}
\ncp\ncp
 \centering
   \includegraphics[width=1\columnwidth]{figures/kgpipOverview} 
  \ncp\ncp\ncp\ncp\ncp\ncp\ncp
  \caption{An overview of {\sysname}'s main components and workflows for 1) training a graph generator using ML pipelines modeled as graphs and 2) ML pipeline generation for a given unseen dataset and certain time budget. {\sysname} utilizes systems for hyperparameter optimization, such as FLAML or Auto-Sklearn, to optimize {\sysname}'s predicted pipelines.}
  \ncp\ncp\ncp\ncp
  \label{fig:overview}
\end{figure}

\subsection{Dataset Embeddings}
In this component, we leverage existing techniques~\cite{kglac} to generate fixed-size, dense columnar embeddings for input datasets. 
\citet{kglac} provide a data discovery system that allows for connecting datasets based on state-of-the-art data profiling techniques \cite{dataprofiling} and deep learning models \cite{Mueller2019RecognizingVF}. 
It identifies multiple types of similarity relationships between data items, including semantic and content similarities. Two columns, for example, are semantically similar if they have similar columns names (based on word embeddings). The content similarity is calculated using dense vector representations (embeddings) of column values. Table embeddings are computed by pooling over their individual column embeddings.  We use these embeddings to measure the similarity between two columns or tables. 
With these embeddings, we build an index of vector embeddings for all the datasets in our training set. We then use efficient libraries \cite{JDH17} for similarity search of dense vectors to retrieve the most similar dataset to a new input dataset based on its embeddings.



\subsection{Graph Representation of Code Semantics}

\highlight{Static and dynamic program analysis techniques could be used to abstract the semantics of programs and extract language-independent representations of code. A program source code is examined in the static analysis without running the program. In contrast, dynamic analysis examines the source code during runtime to collect memory traces and more detailed statistics specific to the analysis technique. Unlike static analysis, dynamic analysis helps in capturing more rich semantics from programs with the high cost of execution and storing massive memory traces. ML portals, such as Kaggle or OpenML, have hundreds of thousands of ML pipelines with no instructions for running or managing the environments of these pipelines. {\sysname} combines dataset embedding with static code analysis, such as {\sysGC}~\cite{graph4code}, to enrich the collected semantics of ML pipelines while avoiding the need to run them.} 

{\sysGC} is optimized to efficiently process millions of Python programs, performing interprocedural data flow and control flow analysis to examine for instance, what happens to data that is read from a Pandas dataframe, how it gets manipulated and transformed, and what transformers or estimators get called on the dataframe.  {\sysGC}'s graphs make it explicit what APIs and functions are invoked on objects without the need to model the used  libraries themselves; hence {\sysGC} can scale static analysis to millions of programs. 
Figures~\ref{running_ex} and \ref{static_analysis} show a small code snippet and its corresponding static analysis graph from {\sysGC}, respectively. As shown in Figure \ref{static_analysis}, the graph captures control flow (gray edges), data flow (black edges), as well as numerous other nodes and edges that are not shown in the figure. Examples of these nodes and edges include those capturing location of calls inside a script file and function call parameters. For example, {\sysGC} generates a graph of roughly 1600 nodes and 3700 edges for a Kaggle ML pipeline script of 72 lines of code. The number of nodes and edges dominate the complexity of training a graph generator model. Moreover, the irrelevant nodes and edges will add noise to the modelling process. 

\begin{figure}
\ncp\ncp
 \centering
  \includegraphics[width=0.75\columnwidth]{figures/running_ex_code.pdf}
  \ncp\ncp\ncp
  \caption{An example from a data science notebook. 
  }
  \label{running_ex}
  \ncp\ncp\ncp\ncp
\end{figure}

\begin{figure}
 \centering
  \includegraphics[width=.6\columnwidth]{figures/figures_pg_4.pdf}
  \ncp\ncp\ncp\ncp
  \caption{Code graph corresponding to Figure \ref{running_ex} obtained with {\sysGC}.  The graph shows control flow with gray edges and data flow with black edges. Numerous other nodes, such as nodes for locations in code files, and edges, such as edges for parameters and documentation, are not shown.}
  \label{static_analysis}
  \ncp\ncp\ncp\ncp
\end{figure}

     


\subsection{Filtering Code Graphs}
\label{abstraction}

For AutoML systems, a pipeline is a set of data transformations, learner selection, and hyper-parameter selection for each model that is selected. On the one hand, {\sysGC} generates a rich code graph for various tasks that may benefit from static code analysis. The nodes and edges, which are not relevant for learner selection, will add noise to training data. This in turn will decrease the scalability of training our graph generation model, i.e., more time to generate embeddings for them and less accuracy due to their noise. Moreover, actual data science notebooks often contain data analysis, data visualization, and model evaluation. We filter out these types of nodes and edges, as well as calls to modules outside the target ML libraries, namely, Scikit-learn, XGBoost, and LGBM. We focus on these ML frameworks because they are the most popular among the top-scoring ML pipelines in ML portals, such as Kaggle, and also because these are the most popular libraries supported by most AutoML systems. 

\begin{figure}
\ncp\ncp
 \centering
  \includegraphics[width=0.45\columnwidth]{figures/figure_pg_5.pdf}
  \ncp\ncp\ncp\ncp\ncp\ncp\ncp
  \caption{The filtered graph of the graph from Figure \ref{static_analysis}. Our filtered graph contains at least 96\% less nodes and edges than the original graph while enhancing the overall quality of the graph generation process. {\sysname} interlinks the abstracted ML pipelines to dataset nodes (highlighted in Orange) representing datasets used by them. This interconnection is essential to train the {\sysname} graph generation model. 
  }
  \ncp\ncp\ncp\ncp\ncp\ncp
  \label{abstraction_figure}
\end{figure}

On the other hand, code graphs represent scattered and unlinked graphs, i.e., a graph per an ML pipeline script. We aim at preparing a training set interconnecting repositories of ML pipeline scripts with their associated datasets. Furthermore the goal of training is to pick learners and transformer for unseen datasets. {\sysname} links the filtered ML pipelines with a unique dataset name. Figure \ref{abstraction_figure} shows the filtered pipeline corresponding to the code snippet in Figure~\ref{running_ex} demonstrating the filtered view of the code graph that we feed to the generator.
The result of adding these dataset nodes is a highly interconnected graph for ML pipelines, we refer to it as {\GML}. Our  {\GML} captures both the code and data aspects of ML pipelines. Hence, we can populate {\GML} with datasets from different sources, such as OpenML~\cite{OpenML} and Kaggle, and ML pipelines applied on these datasets. {\sysname} utilizes {\GML} to train a model based on a large set of pipelines associated with similar datasets. For example, a \textit{pandas.read\_csv} node will be linked to the used table node, i.e., csv file. In some cases, the code, which reads a csv file, does not explicitly mention the dataset name. The ML pipelines are usually associated with datasets, such as Kaggle pipelines and datasets, as shown in Figure~\ref{fig:overview}. {\sysname} uses this information to predict a dataset node.

\subsection{Graph Generation for ML Pipelines}
{\sysname} formulates the generation of ML pipelines as a graph generation problem. The intuition behind this idea is that a neural graph generator might capture more succinctly multiple pipelines seen in practice for a given dataset, and might also capture statistical similarities between different pipelines more effectively. To effectively use such a network, we add a single dataset node as the starting point for the filtered pipelines we generate from Python notebooks. The node is assumed to flow into a \texttt{read\_csv} call which is often the starting point for the pipelines. For generating an ML pipeline, we simply pass in a dataset node for the nearest neighbour of the unseen dataset, i.e., the most similar dataset based on content similarity, as shown in Figure~\ref{fig:overview}.


Our neural graph generator produces graphs in a node-by-node fashion. The generation process can be summarized in the following sequential steps: (1) decide whether to add a new node of a certain type, if yes, (2) decide whether to add an edge to the newly added node, if yes (3) decide the existing node to which the edge to be added. It then repeats from step (2) until no more edges to be added, then repeats from step (1) until no more nodes to be added. The graph generator utilizes node embeddings that are learned throughout the training via graph propagation rounds. These embeddings capture the structure of the ML pipeline graphs. We built on the work proposed by \citet{deepgmg}, modifying it to support the same conditional graph generation process after training. That is, the graph generation starts with a subgraph instead of from scratch. During testing, {\sysname} starts from a subgraph including a dataset node connected to a node for a \texttt{read\_csv} call. With this modification, it is possible to generate graphs for unseen datasets using the closest seen dataset node -- more specifically, its content embedding obtained from the dataset embedding module. It also generates multiple competing ML pipeline graphs for an unseen dataset with a score (probability) of each graph.


\subsection{Hyperparameter Optimizion}

{\sysname} uses the trained graph generator to predict the top-k graphs of ML pipelines for the most similar dataset to the given unseen one ($D$), as shown in Figure~\ref{fig:overview}. {\sysname} maps these graphs into ML pipeline skeletons, where each skeleton is a set of pre-processors and an estimator.
In {\sysname}, the hyperparameter optimizer is responsible for finding the optimum settings for the predicted pre-processors and learners on the target dataset.

{\sysname} is well designed to support both  numerical and non-numerical datasets.  Thus, {\sysname}  applies different pre-processing techniques on the given dataset ($D$) and produces a pre-processed dataset ($D’$). Our pre-processing includes 1) detecting task type (i.e. regression or classification) automatically based on the distribution of the target column 2) automatically inferring accurate data types of columns, 3) vectoring textual columns using word embeddings, and 4) imputing missing values in the dataset. In {\sysname}, the hyperparameter optimizer uses $D’$.

Similar to hyperparameter optimizers implemented in AutoML systems, such as FLAML or Auto-Sklearn, {\sysname} works within a provided time budget per dataset. We note here that the majority of the allotted time budget for ML pipeline generation is spent on the hyperparameter optimization; that is, if the user desires only to know what learners would work best for their dataset, {\sysname} can do that almost instantaneously. Given a time budget ($T$), {\sysname} calculates $t$, the time consumed in generating and validating the graphs. {\sysname} then divides the rest of the time budget between the $K$ graphs. Hence, the hyperparameter optimizer has a time limit of ($(T-t) / K$) to optimize each graph independently.   


The hyperparameter optimizer repeatedly applies the learners and pre-processors with different configurations while monitoring the target score metric throughout. {\sysname}  keeps updating its output with the best pipeline skeleton, i.e.,  learners and pre-processors, and its score.  
For example, if the predicted learner is LogisticRegression, it searches for the best combination of regularization type (L1 or L2) and regularization parameter. The difference between hyperparameter optimizers is the search strategy followed to arrive at the best hyperparameters within the allotted time budget. A naive approach would be to perform an exhaustive grid search over all combinations, while a more advanced approach would be to start with promising configurations first.  

We integrate {\sysname} with the hyperparameter optimizers of both FLAML \cite{flaml} and Auto-Sklearn \cite{autosklearn} to demonstrate the generality of {\sysname}.
The integration of a hyperparameter optimizer into {\sysname} needs a JSON document of the particular preprocessors and estimators supported by the hyperparameter optimizer. Thus, the integration is relatively easy. Finally, our neural graph generation produces a diverse set of pipelines across runs, allowing for exploration and exploitation.

