

%  \vspace{-2mm}
% \subsection{``Which $\phi$ to propose?'' --- Generalized PTR with privacy calibration}
%     \vspace{-2mm}
%The optimism of all data-dependent analyses is to add less noise while maintaining the same or even stronger privacy guarantee compared to its data-independent counterpart. However, PTR approaches require a reasonable guess of the proposed parameter (e.g., $\lambda$ in the linear regression example). Otherwise, it may easily output $\perp$ and be less practical. Noting our generalized PTR takes an extra parameter $\phi$, one may hope to try out various $\phi$ (e.g., different noisy levels in private mean estimation) and return the one with the best utility. 
%In this section,  we propose an improved version of the generalized PTR such that the algorithm takes a list of parameters as input, runs PTR with each of the parameters, and only returns the output with the best utility. We show that the privacy guarantee in $\epsilon$ is independent of the number of $\phi$ that we try.  


% The main limitation of generalized PTR is that one needs to ``propose'' a good guess of parameter $\phi$.  Take the example of $\phi$ being the noise level in a noise-adding mechanism. Choosing too small a $\phi$ will result in a useless output $\perp$, while choosing too large a $\phi$ will add more noise than necessary.

% In this section,  we propose an improved version of generalized PTR such that the algorithm takes a list of parameters as input, runs PTR with each of the parameters, and only returns the output with the best utility. We show that the privacy guarantee in $\epsilon$ is independent of the number of $\phi$ that we try.  

% Formally, let $\phi_1, ..., \phi_k$ be a set of hyper-parameters and $\tilde{\theta}_i \in\{\perp, \text{Range}(\cM)\}$ denotes the output of running generalized PTR on a private dataset $X$ with $\phi_i$. 
% Let $X_{val}$ be a public validation set and $q(\tilde{\theta}_i)$ be the score of evaluating $\tilde{\theta}_i$ with $X_{val}$ (e.g., validation accuracy). The goal is to select a pair of DP model $\tilde{\theta}_i$ and its  hyper-parameter $\phi_i$, such that $\tilde{\theta}_i$ maximizes the validation score.

%(1) the system is $(\tilde{\epsilon}, \tilde{\delta})$-DP; (2) the validation score $q(\theta_i) \geq max_j q(\theta_j) -\alpha$ with high probability.


%The generalized PTR with privacy calibration is described in Algorithm~\ref{alg: parameter_ptr}. Specifically, $T$ denotes the computational limit. We assume an oracle access to the score function $(\tilde{\theta}_i, q(\tilde{\theta}_i)) \sim Q(X)$ by: (a) pick a candidate $\phi_i$ randomly; If there is an order between parameters (e.g., $\phi_i$ denotes the magnitude of noise level), then we can sort $\phi_i$ such that $\phi_1 < \phi_2 < ... < \phi_k$ and pick $\phi_i$ from the left; (b) Run
%Algorithm~\ref{alg:gen_ptr} to obtain $\tilde{\theta}_i$ and evaluate it on $X_{\text{val}}$. Finally, the algorithm outputs the highest scored candidates.


% Finally, the algorithm has a random stopping with probability $\gamma$. 
% state privacy guarantee, cite from 
% The generalized PTR framework with privacy calibration is described in Algorithm~\ref{alg: parameter_ptr}. The privacy guarantee of Algorithm~\ref{alg: parameter_ptr} is an application of \citet{liu2019private}.

% \begin{theorem}[ Theorem 3.4 \citet{liu2019private} ]
% Fix any $\tau \in [0, 1], \delta_2>0$ and let $T =\frac{1}{\tau} \log \frac{1}{\delta_2}$. If each oracle access to $Q(X)$ is $(\epsilon, \delta)$-DP, then
% Algorithm~\ref{alg: parameter_ptr} is $(3\epsilon + 3\sqrt{2\delta}, \sqrt{2\delta} T +\delta_2 )$-DP.%\yw{What if it does not reach $T$? Also in the algorithm it has $k$ right?  Also, did you define which score it is?}
% \end{theorem}
% The theorem implies that one can try a random number of $\phi$ while paying a constant $\epsilon$.
% In practice, we can roughly set $\tau = \frac{1}{10k}$ so that the algorithm is likely to test all $k$ parameters. We emphasize that the privacy and the utility guarantee (see Sec~\ref{sec: omit_gen_ptr} in Appendix) is not our contribution. But the idea of applying a generalized PTR to enforce a uniform DP guarantee over all choices of parameters with data-dependent analysis is new, and in our opinion, significantly broadens the applicability to generic hyperparameter tuning machinery from \citep{liu2019private}.




%\begin{remark}[Private hyper-parameter tuning]
%Our results directly generalize private hyper-parameter tuning. We remark that our PTR-based algorithm can tune privacy parameters (e.g., noise scale) adapted to the data-dependent DP while maximizing the utility. In contrast, prior work~\citep{papernot2021hyperparameter}\citep{liu2019private} on this problem requires a uniform DP guarantee of all base mechanisms, thus not applicable. 
%\end{remark}


%We can categorize hyper-parameters of privacy-preserving models into two sets --- utility parameters and privacy parameters. 

%Most prior work~\citep{liu2019private, papernot2021hyperparameter} on this problem assumes parameters of interests are privacy uncorrelated (e.g., regularizer parameter) and require a uniform DP bound for all of the mechanisms $\cM_i$.  \citet{liu2019private} shows that if each mechanism $\cM_i$ is $(\epsilon, 0)$-DP, then run a random number of mechanisms and report the best option satisfies $(3\epsilon, 0)$-DP.

 %On the other hand, for noise calibration with a fixed privacy budget, we note that one could naively apply a differential privacy accountant (e.g., Renyi DP accountant \citep{mironov2017renyi}) to apriori optimize the privacy parameters with a fixed privacy budget. However, this approach does not give us insights into the utility guarantee or take advantage of the data-dependent property. 

%We generalize the above results by: (1) considering a wide range of hyper-parameters that could be either privacy parameters or non-privacy parameters. (2) Tune parameters adapted to the dataset itself.

%\yw{The two paragraphs above are not clear (from ``on the other hand'' onwards). Should  revise so readers understand what you mean.} % i have remove it to the related work.


% \begin{algorithm}[t]
% 	\caption{PTR with hyper-parameter selection}
% 	\label{alg: parameter_ptr}
% 	\begin{algorithmic}[1]
% 	   \STATE {\textbf{Input}:  Privacy budget $\epsilon, \delta$, cut-off $T$, parameters $\phi_{1:k}$, flipping probability $\tau$ and the access to the validation score $q(\cdot)$. } 
% 		\STATE {Initialize the set $S=\varnothing$.}
% 		\STATE{Draw $G$ from a geometric distribution $\cD_\tau$ and let $\hat{T}=\text{min}(T, G)$.}
% 		\FOR{i = 1 ,..., $\hat{T}$}
% 		\STATE{ pick a random $\phi_i$ from $\phi_{1:k}$.}
% 		\STATE{evaluate $\phi_i$: $(\tilde{\theta}_i, q(\tilde{\theta}_i))\gets$ Algorithm~\ref{alg:gen_ptr}($\phi_i, (\epsilon, \delta)$).}
% 		\STATE {$S \gets S \cup \{\tilde{\theta}_i, q(\tilde{\theta}_i)\}$.}
% 		%\STATE{with probability $\gamma$, we output the highest scored candidate from $S$ and halt.}
% 		\ENDFOR	\vspace{-1mm}
% 	\STATE{Output the highest scored candidate from $S$.}
% 	\end{algorithmic}
% 	\vspace{-1mm}
% \end{algorithm}

% \begin{figure*}[t]
% 	\centering	
% 	\subfigure[Bike dataset]{
% 	\includegraphics[width=0.48\textwidth]{img/bike.pdf}\label{fig:bike}}
% 	\subfigure[Elevators dataset]{
% 	\includegraphics[width=0.48\textwidth]{img/elevator.pdf}\label{fig:ele}}
% 	\vspace{-2mm}
% 	\caption{Differential private linear regression algorithms on UCI datasets. $y$-axis reports the MSE error with confidence intervals. $\epsilon$ is evaluated with $\delta = 1e-6$. }
% 	\label{fig:selection}\vspace{-2mm}
% \end{figure*}

\textbf{Case study with private linear regression.}
We next apply Algorithm~\ref{alg: parameter_ptr} to the private linear regression task with UCI regression datasets. Standard z-scoring is applied and each data point is normalize with a Euclidean norm of 1.  We consider $(60\%,10\%,30\%)$ splits for training, validation and testing test.

\textbf{Baselines}
\begin{itemize}
\vspace{-2mm}
    \item Output Perturbation (Outpert)~\citep{chaudhuri2011differentially}: $\theta=(X^TX + \lambda I)^{-1}X^T\bf{y}$. Release $\hat{\theta} = \theta + \bf{b}$ with an appropriate $\lambda$ and $\bf{b}$ is a Gaussian random vector.\vspace{-2mm}
    \item Posterior sampling (OPS). Sample $\hat{\theta}\sim P(\theta)\propto e^{-\gamma(F(\theta)+ 0.5 \lambda ||\theta||^2)}$ with parameters $\gamma, \lambda$.\vspace{-1.5mm}
    \item Adaptive posterior sampling (AdaOPS)~\citep{wang2018revisiting}. Run OPS with $(\lambda, \gamma)$ chosen adapted to the dataset.\vspace{-1.5mm}
\end{itemize}
%\yw{\cite{gopi2022private} had a nice and clean Gaussian DP bound for OPS. I thought if we can privately test the strong convexity from the dataset, it should satisfy a stronger Gaussian DP. How are privacy accounted in the experiments now?} % use composition.  Each oracle call is (eps, delta). split (epsilon/2, delta/2) to release data dependent quantity, use the remaining to tune gamma for an input lambda.
Outpert and OPS serve as two non-adaptive baselines. In particular, we consider OPS-Balanced~\citep{wang2018revisiting}, which chooses $\lambda$ to minimize a data-independent upper bound of empirical risk and dominates other OPS variants. 
AdaOPS is one state-of-the-art algorithm for adaptive private regression, which  automatically chooses $\lambda$ by minimizing an upper bound of data-dependent empirical risk.


We implement OPS-PTR as follows: 
propose a list of $\lambda$ through grid search (we choose $k=30$ and $\lambda$ ranges from $[2.5, 2.5^{10}]$ on a logarithmic scale); instantiate Algorithm~\ref{alg: parameter_ptr} with $\tau =0.1k, T=\frac{1}{\tau} \log(1/\delta_2)$ and  $\delta_2 = 1/2 \delta$;
calibrate $\gamma$ to meet the privacy requirement for each $\lambda$. sample $\hat{\theta}$ using $(\lambda,\gamma)$ and return the one with the best validation accuracy. Notice that we use a ``no $\perp$'' variant of Algorithm~\ref{alg:gen_ptr} as the calibration of $\gamma$ is clear given a fixed $\lambda$ and privacy budget (see more details in the appendix). We can propose various combinations of $(\lambda, \gamma)$ for more general applications.


 Figure~\ref{fig:selection}  demonstrates how  the MSE error of the linear regression algorithms vary with the privacy budget $\epsilon$. OutPert suffers from the large global sensitivity of output $\theta$. OPS performs well but does not benefit from the data-dependent quantities. Alternatively, AdaOPS is able to choose $(\lambda, \gamma)$ adapted to the dataset, but suffers from the estimation error of the data-dependent empirical risk. On the other hand, OPS-PTR chooses a  $(\lambda, \gamma)$ pair that minimizes the empirical error on the validation set directly, and the privacy parameter $\gamma$ is adapted to the dataset thus achieving the best result.


