%\vspace{-1em}
\section{Preliminaries}
%\vspace{-1em}
\label{sec:preliminaries}
Datasets $X, X' \in \mathcal{X}$ are neighbors if they differ by no more than one datapoint -- i.e., $X \simeq X'$ if $d(X, X') \leq 1$. We will define $d(\cdot)$ to be the number of coordinates that differ between two datasets of the same size $n$: $d(X, Y) = \#\{i \in [n]: X_i \neq Y_i  \}$.

We use $||\cdot||$ to denote the radius of the smallest Euclidean ball that contains the input set, e.g. $||\mathcal{X}|| = \sup_{x \in \mathcal{X}} ||x||$.

The parameter $\phi$ denotes the privacy parameters associated with a mechanism (e.g. noise level, regularization). $\mathcal{M}_{\phi}$ is a mechanism parameterized by $\phi$.
For mechanisms with continuous output space, we will take $\text{Pr}[\mathcal{M}(X) = y]$ to be the probability density function of $\mathcal{M}(X)$ at $y$.



    \begin{definition}[Differential privacy \citep{dwork2006calibrating}] \label{def:dp}
        Fix $\epsilon, \delta \geq 0$. 
A randomized algorithm $\mathcal{M}: \mathcal{X} \rightarrow \mathcal{S}$ satisfies $(\epsilon, \delta)$-DP if for all neighboring datasets $X \simeq X'$ and for all measurable sets $S \subset \mathcal{S}$, 
            \[\text{Pr}\big[\mathcal{M}(X) \in S\big] \leq e^{\epsilon}\text{Pr}\big[\mathcal{M}(X') \in S\big] + \delta.\]
    \end{definition}
%\vspace{-3mm}
% We now define \emph{data-dependent} differential privacy that conditions on an input dataset $X$. 

% \begin{definition}[Data-dependent privacy\cite{papernot2018scalable}]\vspace{-1mm}
% \label{def:data_dep_dp}
% Suppose we have $\delta > 0$ and a function $\epsilon: \mathcal{X} \rightarrow \mathbb{R}$. We say that mechanism $\mathcal{M}$ satisfies ($\epsilon(X), \delta$) data-dependent DP for dataset $X$ if for all possible output sets $S$ and neighboring datasets $X'$,
% \begin{align*}
%     \text{Pr}\big[\mathcal{M}(X) \in S\big] &\leq e^{\epsilon(X)}\text{Pr}\big[\mathcal{M}(X') \in S\big] + \delta, \\
%       \text{Pr}\big[\mathcal{M}(X') \in S\big] &\leq e^{\epsilon(X)}\text{Pr}\big[\mathcal{M}(X) \in S\big] + \delta.
% \end{align*}
% \end{definition}

%\subsection{Additive Noise Mechanisms}
Suppose we wish to privately release the output of a real-valued function $f: \mathcal{X} \rightarrow \mathcal{R}$. We can do so by calculating the \emph{global sensitivity} $\Delta_{GS}$, calibrating the noise scale to the global sensitivity and then adding sampled noise to the output.



\begin{definition}[Local / Global sensitivity]
The local $\ell_*$-sensitivity of a function $f$ is defined as $\Delta_{LS}(X) = \max\limits_{X \simeq X'} || f(X) - f(X') ||_* $ and the global sensitivity of $f$ is $\Delta_{GS} = \sup_X \Delta_{LS}(X)$.
% The local $\ell_*$-sensitivity of a function $f: \cX \to \mathbb{R}^d$ is defined as $\Delta_{LS}(X) = \max\limits_{X \simeq X'} || f(X) - f(X') ||_* $ and the global sensitivity of $f$ is $\Delta_{GS} = \sup_X \Delta_{LS}(X)$.
\end{definition}
%\vspace{-2mm}
% The choice of $\ell_*$ depends on which kind of noise we use, e.g., $\ell_2$-norm is used for Gaussian noise.

\begin{comment}

\begin{definition}[Global sensitivity]
The global $\ell_*$-sensitivity of a function $f: \mathcal{X} \rightarrow \mathcal{R}^d$ is defined as
\begin{align*}
    \Delta_{GS} &= \max\limits_{X, X' \in \mathcal{X}:X \simeq X'} || f(X) - f(X') ||_*. 
\end{align*}
\end{definition}
\begin{definition}[Laplace mechanism]
The Laplace mechanism $\mathcal{M}: \mathcal{X} \rightarrow \mathbb{R}$ applied to a function $f$ is given as
\begin{align*}
    \mathcal{M}(X) &= f(X) + \text{Lap}\left(b \right).
\end{align*}
\end{definition}
\begin{theorem}
Suppose the function $f: \mathcal{X} \rightarrow \mathbb{R}$ has global $\ell_1$-sensitivity $\Delta_f$. Then the Laplace mechanism satisfies $\epsilon$-differential privacy with noise parameter $b = \Delta_f/\epsilon$.	 \vspace{-2mm}
\end{theorem}

\begin{definition}[Gaussian mechanism]
The Gaussian mechanism $\mathcal{M}: \mathcal{X} \rightarrow \mathbb{R}$ applied to a function $f$ is given as
\begin{align*}
    \mathcal{M}(X) &= f(X) + \mathcal{N}(0, \sigma^2).
\end{align*}
\end{definition}
\begin{theorem}
Suppose the function $f: \mathcal{X} \rightarrow \mathbb{R}$ has global $\ell_2$-sensitivity $\Delta_f$. Then the Gaussian mechanism satisfies $(\epsilon, \delta)$-differential privacy with noise parameter $\sigma = \Delta_f\sqrt{2 \log(1.25/\delta)}/\epsilon$.
\end{theorem}
Both the Laplace and Gaussian mechanisms generalize easily to releasing the output of a $d$-dimensional function $f$ by adding i.i.d. noise to each coordinate.
\begin{definition}[Local sensitivity]
The local $\ell_*$-sensitivity of a function $f: \mathcal{X} \rightarrow \mathbb{R}^d$ is defined as
\begin{align*}
    \Delta_{LS}(X) &= \max\limits_{X \simeq X'} || f(X) - f(X') ||_*. 
\end{align*}
\end{definition}
\end{comment}
% \todo{Define Global sensitivity}
%define global/local sensitivity
%Laplace, Gaussian mech
% explain how related to PTR and its generalization

%\vspace{-0.em}
\subsection{Propose-Test-Release}
%\vspace{-0.5em}
Calibrating the noise level to the local sensitivity $\Delta_{LS}(X)$ of a function would allow us to add less noise and therefore achieve higher utility for releasing private queries. However, the local sensitivity is a data-dependent function and na\"ively calibrating the noise level to $\Delta_{LS}(X)$ will not satisfy DP.

PTR resolves this issue in a three-step procedure: \textbf{propose} a bound on the local sensitivity, privately \textbf{test} that the bound is valid (with high probability), and if so calibrate noise according to the bound and \textbf{release} the output.

% \begin{figure}[t]
% \vspace{-1em}
% \centering
% \resizebox{0.95\columnwidth}{!}{%
% \begin{minipage}{0.50\textwidth}
% \begin{algorithm}[H]
% \caption{Propose-Test-Release \cite{dwork2009differential}}
% \label{alg:classic_ptr}
% \begin{algorithmic}[1]
% \STATE{\textbf{Input}: Dataset $X$; privacy parameters $\epsilon,\delta$; proposed bound $\beta$ on $\Delta_{LS}(X)$; query function $f: \mathcal{X} \rightarrow \mathbb{R}$.}
% \STATE{\textbf{Output}: $f^P(X)$ or $\perp$.}
% \STATE{Compute the distance $\gamma(X)$ to the nearest dataset $X''$ such that $ \Delta_{LS}(X'')> \beta$:
% $\gamma(X) = \min\limits_{X''} \{ \text{dist}(X, X''): \Delta_{LS}(X'')> \beta \}$.}
% \STATE{Privately release $\gamma^P(X) = \gamma(X) + \text{Lap}\left(\frac{1}{\epsilon}\right)$.}
% \IF{$\gamma^P(X) > \dfrac{\log(1/\delta)}{\epsilon}$}\vspace{-1pt}
% \STATE{Release $f^P(X) = f(X) + \text{Lap}\left(\frac{\beta}{\epsilon}\right)$.}\vspace{-2pt}
% \ELSE
% \STATE{Output $\perp$.}\vspace{-1pt}
% \ENDIF
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% \quad
% \begin{minipage}{0.46\textwidth}
% \begin{algorithm}[H]
% \caption{Generalized PTR}
% \label{alg:gen_ptr}
% \begin{algorithmic}[1]
% \STATE{{Input}: Proposed~parameter~$\phi$;~privacy~parameters~$\epsilon,  \hat{\epsilon}, \hat{\delta}$; dataset $X$;\blue{an $(\epsilon,\delta)$~DP test $\cT$; ~data-dependent~DP~function~$\epsilon_{\phi}(\cdot, \hat{\delta})$;~mechanism~$\mathcal{M}_{\phi}$.}}
% \STATE{\textbf{Output}: 
% $\mathcal{M}_{\phi}(X)$ or $\perp$.}
% \STATE{Let $\cT$ privately test if $\epsilon_\phi(X,\hat{\delta}) \leq \hat{\epsilon}$.}% with privacy limit $(\hat{\epsilon}, \hat{\delta})$ }.
% \IF{the test $\cT$ passes}
% \vspace{1pt}
% \STATE{Run $\theta = \mathcal{M}_{\phi}(X)$ and output $\theta$.}\vspace{2pt}

% \ELSE \vspace{2pt}

% \STATE{Output $\perp$.}\vspace{2pt}

% \ENDIF
% \end{algorithmic}
% \end{algorithm}
% \end{minipage}
% }
% \vspace{-1em}
% \end{figure}


% \begin{theorem} 
% Algorithm~\ref{alg:classic_ptr} satisfies ($2 \epsilon, \delta$)-DP.
% \cite{dwork2009differential}
% \end{theorem}

PTR privately computes the distance $\cD_{\beta}(X)$ between the input dataset $X$ and the nearest dataset $X''$ whose local sensitivity exceeds the proposed bound $\beta$:
\begin{align*}
    \cD_{\beta}(X) = \min\limits_{X''} \{ d(X, X''): \Delta_{LS}(X'')> \beta \}.
\end{align*}
%\vspace{-.8em}
% The $\epsilon$-DP "test" fails (with probability $\delta$) if PTR decides to release $f^P(X)$ when $\gamma(X) = 0$, i.e. when dataset $X$ has local sensitivity greater than $\beta$.

\begin{figure}[H]
\vspace{-1.4em}
\centering
% \resizebox{0.95\columnwidth}{!}{%
% \begin{minipage}{0.54\textwidth}
\begin{algorithm}[H]
\caption{Propose-Test-Release \citep{dwork2009differential}}
\label{alg:classic_ptr}
\begin{algorithmic}[1]
\STATE{\textbf{Input}: Dataset $X$; privacy parameters $\epsilon,\delta$; proposed bound $\beta$ on $\Delta_{LS}(X)$; query function $f: \mathcal{X} \rightarrow \mathbb{R}$.}
% \STATE{\textbf{Output}: $f^P(X)$ or $\perp$.}
% \STATE{Compute the distance $\gamma(X)$ to the nearest dataset $X''$ such that $ \Delta_{LS}(X'')> \beta$.}
% \STATE{Privately release $\gamma^P = \gamma(X; \beta) + \text{Lap}\left(\frac{1}{\epsilon}\right)$.}

\STATE{\textbf{if} $\cD_{\beta}(X) + \text{Lap}\left(\frac{1}{\epsilon}\right) \leq \frac{\log(1/\delta)}{\epsilon}$ \textbf{then} output $\perp$,}
\STATE{\textbf{else} release $f(X) + \text{Lap}\left(\frac{\beta}{\epsilon}\right)$.}
\end{algorithmic}
\end{algorithm}
\end{figure}
%\vspace{-.8em}
\begin{theorem} 
Algorithm~\ref{alg:classic_ptr} satisfies ($2 \epsilon, \delta$)-DP.
\citep{dwork2009differential}
\end{theorem}
%\vspace{-1em}
Rather than proposing an arbitrary threshold $\beta$, one can also privately release an upper bound of the local sensitivity and calibrate noise according to this upper bound. This was used for node DP in graph statistics \citep{kasiviswanathan2013analyzing}, and for fitting topic models using spectral methods \citep{decarolis2020end}.

%This gives a more efficient alternative and avoids the need to propose $\beta$. This variant is  the local sensitivity itself has a global sensitivity.

%there exist other variants of PTR --- e.g., compute a differentially private upper bound of the local sensitivity and calibrate noise according to this upper bound. This type of PTR requires a global sensitivity of the local sensitivity. We refer readers to the excellent summary of PTR in  section 3 of \citet{vadhan2017complexity}.

% We may mention other types of PTR: propose and release
% There are other variants of PTR... 
% \vspace{-1mm}
% \subsection{Motivation}
% \vspace{-1mm}
% Why do we want to generalize PTR beyond noise-adding mechanisms? For other mechanisms, the local sensitivity either does not exist or is only defined for specific data-dependent quantities (e.g., the sensitivity of the score function in the exponential mechanism) rather than the mechanism's output. We give a concrete example below. 

%In this section, we give a concrete example to demonstrate this limitation and motivate our generalization.
%This section discusses a few limitations of PTR approaches that motivated our work.
%Let us first ask, ``is PTR a general framework applicable to any mechanism with a data-dependent analysis?'' If so, we could explore other less costly approaches to privately test the local sensitivity.

%However, the answer is unfortunately ``no''.  The reasons are twofold. First, the framework above applies only to ``noise-adding'' mechanisms --- where we have a well-defined local sensitivity (of the output), and the noise scale is calibrated according to that. For other non-noise-adding mechanisms, the local sensitivity either does not exist or is only defined for specific data-dependent quantities (e.g., the sensitivity of the score function in the exponential mechanism) rather than the mechanism's output. Consider the difficulties of applying PTR to the following example.


% \begin{example}[Private posterior sampling]\label{exp: posterior}
% Let $\cM: \cX\times \cY \to \Theta $ be a private posterior sampling   mechanism~\citep{minami2016differential,wang2015privacy,gopi2022private} for approximately minimizing $F_{X}(\theta)$. % for linear regression problem, i.e., $\min_{\theta} \frac{1}{2}||y-X\theta||^2 + \lambda ||\theta||^2$. 
% $\cM$ samples $\theta \sim P(\theta)\propto e^{-\gamma(F_X(\theta)+ 0.5\lambda ||\theta||^2)}$ with parameters $\gamma, \lambda$. $\gamma,\lambda$ cannot be appropriately chosen for this mechanism to satisfy DP without going through a sensitivity calculation of $\arg\min F_X(\theta)$. In fact, the global and local sensitivity of the minimizer is unbounded even in linear regression problems, i.e., when $F_X(\theta) = \frac{1}{2}||y-X\theta||^2.$ 
% %The local sensitivity $\Delta:=||P_{X,y}(\theta)-P_{X', y'}(\theta)||$ is not well-defined  for the sampling algorithm, thus the standard PTR is not applicable.  
% \end{example}
% Output perturbation algorithms do work for the above problem when we regularize, but they are known to be suboptimal in theory and in practice \cite{chaudhuri2011differentially}.% do not achieve the level of utility in theory and in practice when comparing to posterior sampling. 
 

% Moreover, even in the cases of noise-adding mechanisms where PTR seems to be applicable, it does not lead to a tight privacy guarantee. Specifically, by an example of privacy amplification by post-processing (Example~\ref{exp: binary_vote} in the appendix), we demonstrate that the local sensitivity does not capture all sufficient statistics for data-dependent privacy analysis and thus is loose.

% Instead of identifying sufficient statistics of each mechanism, we develop a unified framework --- generalized PTR, offering the flexibility for any mechanism to exploit data-dependent quantities.

% \textbf{On data-dependent DP losses.} In addition to the above, there has been an increasing list of empirical DP work that fix the parameters of a randomized algorithm while reporting the resulting data-dependent DP losses $\epsilon(\text{Data})$ after running on a specific dataset \citep{ligett2017accuracy,papernot2018scalable,zhu2020private, feldman2021individual}. The data-dependent DP losses are often smaller than the worst-case DP losses, but technically speaking, these algorithms are not formally DP with DP guarantees any smaller than that of the worst-case. In addition, the data-dependent DP losses themselves are sensitive, and thus cannot be reported. A typical solution is to privately release $\epsilon(\text{Data})$, but it still does not satisfy DP as this would require a prescribed $(\epsilon,\delta)$-DP parameter to be satisfied for all input datasets. Part of our contribution is to resolve this conundrum by showing that a simple post-processing step of the privately released upper bound of $\epsilon(\text{Data})$ gives a formal DP algorithm.
%\yq{Shall we combine this part with the related work section?}

%Instead,  exploits data-dependent quantities by first privately choosing $\gamma, \lambda$ adapted to the dataset and then applying posterior sampling with the sanitized parameters.  
