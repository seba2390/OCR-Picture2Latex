%\vspace{-1em}
In this section, we apply the generalized PTR framework to solve an open problem from the Private Aggregation of Teacher Ensembles (PATE) \citep{papernot2017, papernot2018scalable} --- privately publishing the entire model through privately releasing data-dependent DP losses. Our algorithm makes use of the smooth sensitivity framework ~\citep{nissim2007smooth} and the Gaussian mechanism to construct a high-probability test of the data-dependent DP. The one-dimensional statistical nature of data-dependent DP enables efficient computations under the smooth sensitivity framework. Thus, this approach is generally applicable for other private data-adaptive analysis beyond PATE.

% shall we put this remark at the end of the section?


PATE  is a knowledge transfer framework for model-agnostic private learning. In this framework,  an ensemble of teacher models is trained on the disjoint private data and uses the teachers' aggregated consensus answers to supervise the training of a ``student'' model agnostic to the underlying machine-learning algorithms. By publishing only the aggregated answers and by the careful analysis of the ``consensus'', PATE has become a practical technique in recent private model training. 


The tight privacy guarantee of PATE heavily relies on a delicate data-dependent DP analysis, for which the authors of PATE use the smooth sensitivity framework  to privately publish the data-dependent privacy cost. However, it remains an open problem to show that the released model is DP under  data-dependent analysis. Our generalized PTR resolves this gap by  carefully testing a private upper bound of the data-dependent privacy cost. Our algorithm is fully described in Algorithm~\ref{alg: pate_ptr}, where the modification over the original PATE framework is highlighted in blue.


% put a pate framework here
 
Algorithm~\ref{alg: pate_ptr} takes the input of privacy budget $(\epsilon',\hat{\epsilon}, \delta)$, unlabeled public data $x_{1:T}$ and $K$ teachers' predictions on these data.  The parameter $\epsilon$ denotes the privacy cost of publishing the data-dependent DP and $\epsilon'$ is the predefined privacy budget for testing. $n_j(x_i)$ denotes the the number of teachers that agree on label $j$ for $x_i$ and $C$ denotes the number of classes. The goal is to privately release a list of plurality outcomes --- $\text{argmax}_{j\in[C]} n_j(x_i)$ for $i \in[T]$ --- and use these outcomes to supervise the training of a ``student'' model in the public domain. The parameter $\sigma_1$ denotes the noise scale for the vote count.


In their privacy analysis,  \citet{papernot2018scalable} compute the data-dependent $\rdp_{\sigma_1}(\alpha, X)$ of labeling the entire group of student queries.  $\rdp_{\sigma_1}(\alpha, X)$ can be orders of magnitude smaller than its data-independent version if there is a strong agreement among teachers. Note that $\rdp_{\sigma_1}(\alpha, X)$  is a function of the RDP order $\alpha$ and the dataset $X$, analogous to our  Definition~\ref{def:data_dep_dp}  but subject to RDP~\citep{mironov2017renyi}.
% We cite the comparison between two approaches for a special case.

\begin{theorem}[\citep{papernot2018scalable}]\label{thm: dep_gau}
If the top three vote counts of $x_i$ are $n_1>n_2>n_3$ and $n_1 -n_2, n_2-n_3\gg \sigma_1$, then the data-dependent RDP of releasing $\text{argmax}_j \{n_j +\cN(0, \sigma_1^2)\}$
satisfies $(\alpha, \exp\{-2\alpha/{\sigma_1^2}\}/\alpha)$-RDP and the data-independent RDP (using the Gaussian mechanism) satisfies $(\alpha, \frac{\alpha}{\sigma_1^2})$-RDP. 
\end{theorem}


\begin{algorithm}[H]
	\caption{PATE with generalized PTR}
	\label{alg: pate_ptr}
	\begin{algorithmic}[1]
		\STATE {\textbf{Input}: Unlabeled public data $x_{1:T}$, aggregated teachers prediction $n(\cdot)$, privacy parameter $\hat{\epsilon},\epsilon', \delta$, noisy parameter $\sigma_1.$}
		\STATE{Set  $\alpha =\frac{2\log(2/\delta)}{\hat{\epsilon}}+1 $, $\sigma_s = \sigma_2 = \sqrt{\frac{3\alpha + 2}{\hat{\epsilon}}}, \delta_2 = \delta/2, $ smoothness parameter $ \beta = \frac{0.2}{\alpha}$.}
		\STATE{Compute noisy labels: ${y_i}^p \gets \text{argmax}_{j\in [C]}\{n_j(x_i)+ \cN(0, \sigma_1^2)\} $ for all $i \in[1:T]$.}
		\STATE{$\rdp_{\sigma_1}(\alpha, X)\gets$ data-dependent RDP at the $\alpha$-th order.}
		\STATE {$SS_\beta(X) \gets$ the smooth sensitivity  of $\rdp_{\sigma_1}^{\text{upper}} (\alpha,X)$.}
		\STATE \blue{Privately release $\mu:=  \log(SS_\beta(X)) + \beta \cdot \cN(0, \sigma_2^2)+\sqrt{2\log(2/\delta_2)}\cdot \sigma_2 \cdot \beta$}
		\STATE{\blue{ $\rdp_{\sigma_1}^{\text{upper}}(\alpha)\gets $ an upper bound of data-dependent RDP through Lemma~\ref{lem: upperbound}}}.
		\STATE{\blue{$\epsilon_{\sigma_1} \gets$ DP guarantee converted from $\rdp_{\sigma_1}^{\text{upper}}(\alpha).$ }}
		%\gets \rdp_{\sigma_1} (\alpha, X)+ SS_{\beta}(X) \cdot \cN(0, \sigma_s^2)+ \blue{\sigma_s \cdot \sqrt{2\log(2/\delta_2)}e^{\mu} }+ \frac{\log(2/\delta)}{\alpha-1}$.}
		\STATE{\blue{If $\epsilon'\geq \epsilon_{\sigma_1}$ \textbf{return} a student model trained using $(x_{1:T}; y_{1:T}^p)$}}.
		\STATE{\blue{\text{Else return} $\perp$.}}
		%\STATE \blue{\textbf{if} $  \epsilon_{\sigma_1}(\alpha)^p + \frac{\log(2/\delta)}{\alpha-1} \leq \epsilon$ \textbf{return} a student model trained using $(x_{1:T}; y_{1:T}^p)$ \textbf{else return} $\perp.$}
	\end{algorithmic}
	%\vspace{-1mm}
\end{algorithm}



However, $\rdp_{\sigma_1}(\alpha, X)$ is data-dependent and thus cannot be revealed. The authors therefore privately publish the data-dependent RDP using the smooth sensitivity framework~\citep{nissim2007smooth}.
\begin{comment}
\begin{definition}[Smooth Sensitivity]\label{def: smooth}
	Given the smoothness parameter $\beta$, a $\beta$-smooth sensitivity of $f(X)$ is defined as 
	\[SS_\beta(X):= \max_{d\geq 0} e^{-\beta d} \cdot \max_{\tilde{X'}: dist(X, \tilde{X'})\leq d} \Delta_{LS}(\tilde{X}')\]
\end{definition}
\end{comment}
The smooth sensitivity calculates a smooth upper bound on the local sensitivity of $\rdp_{\sigma_1}(\alpha, X)$, denoted as $SS_\beta(X)$, such that $SS_\beta (X) \leq e^\beta SS_\beta(X')$ for any neighboring dataset $X$ and $X'$. By adding Gaussian noise scaled by the smooth sensitivity (i.e., release $\epsilon_{\sigma_1}(\alpha, X)+ SS_\beta(X)\cdot \cN(0, \sigma_s^2)$), the privacy cost is safely published. 


Unlike most noise-adding mechanisms, the standard deviation $\sigma_s$ cannot be published since $SS_\beta(X)$ is a data-dependent quantity. Moreover, this approach fails to provide a valid privacy guarantee of the noisy labels obtained through the PATE algorithm, as the published privacy cost could be smaller than the real privacy cost.
Our solution in Algorithm~\ref{alg: pate_ptr} looks like the following:
%\vspace{-2mm}
\begin{itemize}
    	%\vspace{-0.5em}
    \item Privately release an upper bound of the smooth sensitivity $SS_\beta(X)$ with $e^{\mu}$. % \vspace{-0.5em}
    \item Conditioned on a high-probability event of $e^{\mu}$, publish the data-dependent RDP with $\rdp_{\sigma_1}^{\text{upper}}(\alpha)$.
   % \vspace{-0.5em}
    \item Convert $\rdp_{\sigma_1}^{\text{upper}}(\alpha)$ back to the standard DP guarantee using RDP to DP conversion at $\delta/2$.  %\vspace{-0.5em}
    \item Test if the converted DP is above the predefined budget $\epsilon'$.
% \vspace{0.5em}
\end{itemize}
%\vspace{-2em}
The following lemma states that $\rdp_{\sigma_1}^{\text{upper}}(\alpha)$ is a valid upper bound of the data-dependent RDP.

\begin{figure*}[t]
	\centering	
	\subfigure[High consensus and strong data-dependent DP ]{
	\includegraphics[width=0.46\textwidth]{img/pate_high.pdf}\label{fig:high}}
	\subfigure[Low consensus and low data-dependent DP]{
	\includegraphics[width=0.46\textwidth]{img/pate_low.pdf}\label{fig:low}} %\vspace{-3mm}
	\caption{Privacy and utility tradeoffs with PATE. 
	When $\sigma_1$ is aligned, three algorithms provide the same utility. $y$-axis plots the privacy cost of labeling $T=200$ public data with $\delta = 10^{-5}$.
   The left figure considers the high-consensus case, where the data-adaptive analysis is preferred.} 
	\label{fig: exp_pate}
	%\vspace{-1em}
\end{figure*}

\begin{lemma}[Private upper bound of data-dependent  RDP]\label{lem: upperbound}
We are given a RDP function $\rdp(\alpha, X)$ and a $\beta$-smooth sensitivity bound $SS(\cdot)$ of $\rdp(\alpha, X)$. Let $\mu$ (defined in Algorithm~\ref{alg: pate_ptr}) denote the private release of $\log(SS_\beta(X))$. Let the $(\beta, \sigma_s, \sigma_2)$-GNSS mechanism be 
\[\scriptstyle
\rdp^{\text{upper}}(\alpha):=\rdp(\alpha, X) + SS_\beta(X) \cdot \cN(0, \sigma_s^2) + \sigma_s \sqrt{2\log(\frac{2}{\delta_2}) } e^{\mu} \]
	 Then, the release of $\rdp^{\text{upper}}(X)$ satisfies $(\alpha, \frac{3\alpha +2}{2\sigma_s^2})$-RDP for all $1<\alpha < \frac{1}{2\beta}$; w.p. at least $1-\delta_2$, $\rdp^{\text{upper}}(\alpha)$ is an upper bound of $\rdp(\alpha, X)$.
%\vspace{-2mm}
\end{lemma}


The proof (deferred to the appendix) makes use of the facts that: (1) the log of $SS_\beta(X)$ has a bounded global sensitivity $\beta$ through the definition of smooth sensitivity; (2) releasing $\rdp_{\sigma_1}(\alpha, X)+ SS_\beta(X)\cdot \cN(0, \sigma_s^2)$ is $(\alpha, \frac{\alpha+1}{\sigma_s^2})$-RDP (Theorem 23 from \citet{papernot2018scalable}).

%The theorem provides a universal solution to sani any smooth sensitivity based data-dependent DP.
% remark, the theorem provides a general approach

%Next, we convert $\epsilon_{\sigma_1}(\alpha)^p$ back to the standard DP guarantee, and compare it with the pre-defined privacy budget $\epsilon$. Once the test passes, Algorithm~\ref{alg: pate_ptr} returns the noise parameter as well as the noisy aggregated labels.


Now, we are ready to state the privacy guarantee of Algorithm~\ref{alg: pate_ptr}.





\begin{theorem}\label{thm: pate_ptr}
Algorithm~\ref{alg: pate_ptr} satisfies $(\epsilon'+\hat{\epsilon}, \delta)$-DP.
\end{theorem}
%\vspace{-2mm}
%short version
In the proof, the choice of $\alpha$ ensures that the cost of the $\delta/2$ contribution (used in the RDP-to-DP conversion) is roughly $\hat{\epsilon}/2$. Then  the release of $\rdp_{\sigma_1}^{\text{upper}}(\alpha)$ with  $\sigma_s =\sqrt{\frac{2+3\alpha}{\hat{\epsilon}}}$ accounts for another cost of $(\epsilon/2, \delta/2)$-DP.% Finally, the testing step is $\epsilon$-DP.

%In the proof, we set $\alpha =\frac{2\log(2/\delta)}{\epsilon}+1$ and use RDP to DP conversion with $\delta/2$ ensures that the cost of $\delta/2$ contribution to be roughly $\epsilon/2$. Then, the release of $\epsilon_{\sigma_1}^p(\alpha)$ with  $\sigma_s =\sqrt{\frac{2+3\alpha}{\epsilon}}$ counts for another $(\epsilon/2, \delta/2)$-DP.


%Regarding the  practical choice on $\epsilon$, we can set 
%$\epsilon = \frac{1}{4}\bigg(\frac{T}{\sigma_1^2}+2\sqrt{\frac{T\log(1/\delta)}{\sigma_1^2}}\bigg)$ (a quarter of the data-independent cost), so that the algorithm returns $\perp$ only when the data-dependent DP is close to its data-independent version.  Alternatively, we provide a no-$\perp$ version of Algorithm~\ref{alg: pate_ptr} by removing its testing step, which satisfies $(\epsilon_{\sigma_1}(\alpha)^p + \epsilon, \delta)$-DP. 

\textbf{Empirical results.} We next empirically evaluate Algorithm~\ref{alg: pate_ptr} (PATE-PTR) on the MNIST dataset. Following the experimental setup from \citet{papernot2018scalable}, we consider the training set to be the private domain, and the testing set is used as the public domain.  We first partition the training set into $400$ disjoint sets and $400$ teacher models, each trained individually. Then we select $T=200$ unlabeled data from the public domain, with the goal of privately labeling them. 
To illustrate the behaviors of algorithms under various data distributions, we consider two settings of unlabeled data, high-consensus and low-consensus. In the low-consensus setting, we choose $T$ unlabeled data such that there is no high agreement among teachers, so the advantage of data-adaptive analysis is diminished. We provide further details on the distribution of these two settings in the appendix.
%In the low-consensus setting, we apply a screening on public data and only use data whose maximum vote count is smaller than $150$, so the advantage of data-adaptive analysis is diminished. We provide the distribution of two settings in the appendix.
% mention how to compute the smooth sensitivity

\textbf{Baselines.}
We consider the Gaussian mechanism as a data-independent baseline, where the privacy guarantee is valid but does not take advantage of the properties of the dataset. The data-dependent DP (~\citet{papernot2018scalable}) serves as a non-private baseline, which requires further sanitation.  Note that these two baselines provide different privacy analyses of the same algorithm (see Theorem~\ref{thm: dep_gau}).




Figure~\ref{fig: exp_pate} plots privacy-utility tradeoffs between the three approaches by varying the noise scale $\sigma_1$. The purple region denotes a set of privacy budget choices ($\hat{\epsilon}+\epsilon'$ used in Algorithm~\ref{alg: pate_ptr}) such that the utility of the three algorithms is aligned under the same $\sigma_1$. In more detail, the purple region is lower-bounded by $\hat{\epsilon} +\epsilon_{\sigma_1}$.
We first fix $\sigma_s=\sigma_2=15$ such that $\hat{\epsilon}$ is fixed. Then we empirically calculate the average of $\epsilon_{\sigma_1}$ (the private upper bound of the data-dependent DP) over $10$ trials. Running Algorithm~\ref{alg: pate_ptr} with any choice of $\hat{\epsilon}+\epsilon'$ chosen from the purple region implies  $\epsilon'>\epsilon_{\sigma_1}$. Therefore, PATE-PTR will output the same noisy labels (with high probability) as the two baselines.


\textbf{Observation} 
As $\sigma_1$ increases, the privacy loss of the Gaussian mechanism decreases, while the data-dependent DP curve does not change much. This is because the data-dependent DP of each query is a complex function of both the noise scale and the data and does not monotonically decrease when $\sigma_1$ increases (see more details in the appendix). However, the data-dependent DP still dominates the Gaussian mechanism for a wide range of $\sigma_1$. Moreover, PATE-PTR nicely interpolates between the data-independent DP guarantee and the non-private data-adaptive DP guarantee.  In the low-consensus case, the gap between the data-dependent DP and the DP guarantee of the Gaussian mechanism unsurprisingly decreases. Meanwhile, PATE-PTR (the purple region) performs well when the noise scale is small but deteriorates when the data-independent approach proves more advantageous. 
This example demonstrates that using PTR as a post-processing step to convert the data-dependent DP to standard DP is effective when the data-adaptive approach dominates others.





 %Figure~\ref{fig: exp_pate} plots privacy-utility tradeoffs between three approaches by varying noisy scale $\sigma_1$. As $\sigma_1$ increases, the privacy loss of  Gaussian mechanism decreases, while the curve of data-dependent DP does not change much. This is because the data-dependent DP of each query is a function of $q$, where $q$ denotes an upper bound of the probability where the plurality output does not match the noisy output. $q$ is a complex function of both the noisy scale and data and is not monotonically decreasing when $\sigma_1$ is increasing. 







