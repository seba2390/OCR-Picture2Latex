\section{Limitations and Future Work}
One weakness of generalized PTR is that it requires a case-specific privacy analysis. Have we simply exchanged the problem of designing a data-adaptive DP algorithm with the problem of analyzing the data-dependent privacy loss? We argue that this limitation is inherited from classic PTR. In situations where classic PTR is not applicable, we've outlined several approaches to constructing the DP test for our framework (see Sections~\ref{subsections:test_construction} and~\ref{section:applications}).

Furthermore, the data-dependent privacy loss is often more straightforward to compute than local sensitivity, and often exists in intermediate steps of classic DP analysis already. Most DP analysis involves providing a high-probability tail bound of the privacy loss random variable. If we stop before taking the max over the input dataset, then we get a data-dependent DP loss right away (as in Example~\ref{examp:lap_mech}).

There are several exciting directions for applying generalized PTR to more problems. Sufficient statistics release and our private hyperparameter tuning (Algorithm~\ref{alg: parameter_ptr}) can be used to construct data-adaptive extensions of DP-PCA \citep{dwork2014analyze} and Sparse-DP-ERM \citep{kifer2012private}. For DP-PCA we could use our Algorithm~\ref{alg: parameter_ptr} to tune the variance of the noise added to the spectral gap; for Sparse-DP-ERM we would test the restricted strong convexity parameter (RSC), i.e. not adding additional regularization if the RSC is already large.


\section{Conclusion}

Generalized PTR extends the classic ``Propose-Test-Release'' framework to a more general setting by testing the data-dependent privacy loss of an input dataset, rather than its local sensitivity. In this paper we've provided several examples -- private linear regression with hyperparameter selection and PATE -- to illustrate how generalized PTR can enhance DP algorithm design via a data-adaptive approach.