Why do we want to generalize PTR beyond noise-adding mechanisms? Compared to classic PTR, the generalized PTR framework allows us to be more flexible in both the type of test conducted and also the type of mechanism whose output we wish to release. For many mechanisms, the local sensitivity either does not exist or is only defined for specific data-dependent quantities (e.g., the sensitivity of the score function in the exponential mechanism) rather than the mechanism's output. 

The following example illustrates this issue.

\begin{example}[Private posterior sampling]\label{exp: posterior}
Let $\cM: \cX\times \cY \to \Theta $ be a private posterior sampling   mechanism~\citep{minami2016differential,wang2015privacy,gopi2022private} for approximately minimizing $F_{X}(\theta)$.

$\cM$ samples $\theta \sim P(\theta)\propto e^{-\gamma(F_X(\theta)+ 0.5\lambda ||\theta||^2)}$ with parameters $\gamma, \lambda$. Note that $\gamma,\lambda$ cannot be appropriately chosen for this mechanism to satisfy DP without going through a sensitivity calculation of $\arg\min F_X(\theta)$. In fact, the global and local sensitivity of the minimizer is unbounded even in linear regression problems, i.e when $F_X(\theta) = \frac{1}{2}||y-X\theta||^2.$ 
%The local sensitivity $\Delta:=||P_{X,y}(\theta)-P_{X', y'}(\theta)||$ is not well-defined  for the sampling algorithm, thus the standard PTR is not applicable.  

\end{example}
Output perturbation algorithms do work for the above problem when we regularize, but they are known to be suboptimal in theory and in practice \citep{chaudhuri2011differentially}. In Section~\ref{subsections:private_linear_regression} we demonstrate how to apply generalized PTR to achieve a data-adaptive posterior sampling mechanism.

Even in the cases of noise-adding mechanisms where PTR seems to be applicable, it does not lead to a tight privacy guarantee. Specifically, by an example of privacy amplification by post-processing (Example~\ref{exp: binary_vote} in the appendix), we demonstrate that the local sensitivity does not capture all sufficient statistics for data-dependent privacy analysis and thus is loose.

% We've just seen an example where the local sensitivity is unbounded. In other cases, the local sensitivity exists and can be tested efficiently -- \emph{but} isn't sufficiently descriptive to make full use of data-dependent properties. Our next example demonstrates that considering only the local sensitivity leads to a loose privacy analysis.

% \begin{example}\label{exp: binary_vote}
% Consider a binary class voting problem: $n$ users vote for a binary class $\{0, 1\}$ and the goal is to output the class that is supported by the majority. Let $n_i$ denote the number of people who vote for the class $i$. We consider the report-noisy-max mechanism: $\cM(X): \text{argmax}_{i \in [0,1]} n_i(X)+ Lap(1/\epsilon)$,where $1/\epsilon$ denotes the scale of Laplace noise.

% For a dataset $X$, the gap between number of votes in each class is $t(X) = |n_0(X) - n_1(X)|$. Observe that if $t(X) > 1$, adding or removing one user will not change the majority class -- in other words, for a neighboring dataset $X'$ we have $\text{argmax}_{i \in [0,1]} n_i(X) = \text{argmax}_{i \in [0,1]} n_i(X')$. The local sensitivity $\Delta_{LS}(X)$ of the report-noisy-max mechanism is therefore $0$ if $t(X) >1$. 
% \end{example}