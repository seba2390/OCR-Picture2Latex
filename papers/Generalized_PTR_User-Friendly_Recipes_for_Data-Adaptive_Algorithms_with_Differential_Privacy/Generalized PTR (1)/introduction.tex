  %\vspace{-2em}
\section{Introduction}
   % \vspace{-1em}
\label{sec:introduction}
The guarantees of differential privacy (DP) \citep{dwork2006calibrating} are based on worst-case outcomes across all possible datasets. A common paradigm is therefore to add noise scaled by the \emph{global sensitivity} of a query $f$, i.e. the maximum change in $f$ between any pair of neighboring datasets.

A given dataset $X$ might have a \emph{local sensitivity} that is much smaller than the global sensitivity, in which case we can hope to add a smaller amount of noise (calibrated to the local rather than the global sensitivity) while achieving the same privacy guarantee. However, this must not be undertaken na\"{i}vely -- the local sensitivity is a dataset-dependent function and so calibrating noise to the local sensitivity could leak information about the dataset \citep{nissim2007smooth}.

The ``Propose-Test-Release'' (PTR) framework \citep{dwork2009differential} resolves this issue by introducing a test to privately check whether a proposed bound on the local sensitivity is valid. Only if the test ``passes'' is the output released with noise calibrated to the proposed bound on the local sensitivity. 

PTR is a powerful and flexible tool for designing data-adaptive DP algorithms, but it has several limitations. First, it applies only to noise-adding mechanisms which calibrate noise according to the sensitivity of a query. Second, the test in ``Propose-Test-Release'' is computationally expensive for all but a few simple queries such as privately releasing the median or mode.  Third, while some existing works\blue{~\citep{decarolis2020end, kasiviswanathan2013analyzing, liu2021differential}} follow the approach of testing ``nice'' properties of a dataset before exploiting these properties in a private release to PTR \footnote{We refer to these as PTR-like methods. }, 
there has not been a systematic recipe for \emph{discovering} which properties should be tested.
% \begin{enumerate}
%     \item It applies only to noise-adding mechanisms which calibrate noise according to the sensitivity of a query.
    
%     \item The test in ``Propose-Test-Release'' is computationally expensive for all but a few "nice" cases such as privately releasing the median or mode.
% \end{enumerate}

In this paper, we propose a generalization of PTR which addresses these limitations. The centerpiece of our framework is a differentially private test on the \emph{data-dependent privacy loss}.
%which meets a few requirements. 
This test does not directly consider the local sensitivity of a query and is therefore not limited to additive noise mechanisms. Moreover, in many cases, the test can be efficiently implemented by privately releasing a high-probability upper bound, thus avoiding the need to search an exponentially large space of datasets. Furthermore, the derivation of the test itself often spells out exactly what properties of the input dataset need to be checked, which streamlines the design of data-adaptive DP algorithms.

%Furthermore, the test need not search an exponentially large space of datasets and we illustrate how to design a test which privately checks requisite properties of the input dataset itself.


Our contributions are summarized as follows:
%\todo{to add }
\begin{enumerate}
    %\vspace{-1em}
    % \item We propose a generalization of PTR, which 
    % substantially broadens the applicability of the PTR framework \blue{on two fronts: (a) generalized PTR extends PTR to apply to any DP mechanisms (beyond noise-adding mechanisms); (b) generalized PTR allows us to plug in \emph{any} data-dependent DP analysis to construct a high-probability DP test that adapts to favorable properties of the input dataset -- without painstakingly designing each test from scratch.}
    \item We propose a generalization of PTR which can handle algorithms beyond noise-adding mechanisms. Generalized PTR allows us to plug in \emph{any} data-dependent DP analysis to construct a high-probability DP test that adapts to favorable properties of the input dataset -- without painstakingly designing each test from scratch.
   % \vspace{-0.7em}
    \item We demonstrate that many existing examples of PTR and PTR-like algorithms can be unified under the generalized PTR framework, sometimes resulting in a tighter analysis (see an example of report-noisy-max in Sec~\ref{sec:binary_vote}).   % \vspace{-0.7em}
    \item We show that one can publish a DP model through privately upper-bounding a one-dimensional statistic --- no matter how complex the output space of the mechanism is. We apply this result to solve an open problem from PATE \citep{papernot2017, papernot2018scalable}.    %\vspace{-1mm}
    \item Our results broaden the applicability of private hyper-parameter tuning \citep{liu2019private,papernot2021hyperparameter} in enabling joint-parameter selection of  DP-specific parameters (e.g., noise level) and native parameters of the algorithm (e.g., learning rate, regularization weight), which may jointly affect the data-dependent DP losses.    % \vspace{-0.7em}
\end{enumerate}

% \textbf{A remark on the DP test with case-specific study.}
% We concede that the data-dependent privacy loss might depend on the dataset in different ways, and thus the derivation of the test (used in the generalized PTR) requires a case-specific study. However, requiring a case-specific analysis for each problem seems to be a general problem that applies to all recipes for data-adaptive DP algorithm design. The required labor in the cases when standard PTR is applicable is equivalent to the work needed to derive and (privately) test the local sensitivity. When the classic PTR is not applicable, our framework provides a step-by-step guideline on what to do, which makes it more widely applicable.

% Moreover, we propose the following two approaches to construct the high-probability DP test.

% \begin{enumerate}
%     \item Private sufficient statistics release (used in the private linear regression example) specifies the data-dependent DP as a function of the dataset and privately releases each data-dependent component. 
%     \item The second approach (used in the PATE example) uses the smooth sensitivity framework to privately release the data-dependent DP as a whole, and then construct a high-confidence test using the Gaussian mechanism. 
    
% \end{enumerate}

% We believe these two approaches can cover most of the scenarios arising in data-adaptive analysis. For example, we demonstrate the merits of generalized PTR in handling data-adaptive private generalize linear models in the appendix using private sufficient statistics release. We believe this result is effective and not previously proposed in the literature. 
% Moreover, sufficient statistics release together with our private hyper-parameter tuning (Algorithm~\ref{alg: parameter_ptr} can be used to construct data-adaptive extensions of \blue{DP-PCA} and Sparse-DP-ERM (see details in the future work section).
% add citation