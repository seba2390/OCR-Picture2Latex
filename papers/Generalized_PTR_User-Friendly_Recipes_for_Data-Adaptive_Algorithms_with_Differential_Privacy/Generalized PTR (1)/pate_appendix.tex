\subsection{Details of PATE case study}

\begin{definition}[Renyi DP \citep{mironov2017renyi}]
    We say a randomized algorithm $\cM$ is $(\alpha, \epsilon_\cM(\alpha))$-RDP with order $\alpha \geq 1$ if for neighboring datasets $X, X'$
    \begin{align*}
    &\mathbb{D}_{\alpha}(\cM(X)||   \cM(X')):=\\
    & \frac{1}{\alpha-1}\log \mathbb{E}_{o \sim \cM(X')}\bigg[ \bigg( \frac{\pr[\cM(X)=o]}{\pr[\cM(X')=o]}\bigg)^\alpha \bigg]\leq \epsilon_\cM(\alpha).
    \end{align*}
\end{definition}
At the limit of $\alpha \to \infty$, RDP reduces to $(\epsilon, 0)$-DP. 
We now define the  data-dependent Renyi DP that conditioned on an input dataset $X$.
\begin{definition}[Data-dependent Renyi DP \citep{papernot2018scalable}]
    We say a randomized algorithm $\cM$ is $(\alpha, \epsilon_\cM(\alpha, X))$-RDP with order $\alpha \geq 1$ for dataset $X$ if for neighboring datasets $X'$
    \begin{align*}
    &\mathbb{D}_{\alpha}(\cM(X)||   \cM(X')):=\\
    & \frac{1}{\alpha-1}\log \mathbb{E}_{o \sim \cM(X')}\bigg[ \bigg( \frac{\pr[\cM(X)=o]}{\pr[\cM(X')=o]}\bigg)^\alpha \bigg]\leq \epsilon_\cM(\alpha, X).
    \end{align*}
\end{definition}

%we can take a functional view of RDP and use $\epsilon_\cM(\cdot)$ to denote the RDP as a lambda function of $\alpha$.  

RDP features two useful properties.
\begin{lemma}[Adaptive composition]
    $\epsilon_{(\cM_1, \cM_2)} = \epsilon_{\cM_1}(\cdot) + \epsilon_{\cM_2}(\cdot)$.
\end{lemma}
\begin{lemma}[From RDP to DP] If a randomized algorithm $\cM$ satisfies $(\alpha,\epsilon(\alpha))$-RDP, then $\cM$ also satisfies $(\epsilon(\alpha)+\frac{\log(1/\delta)}{\alpha-1},\delta)$-DP for any $\delta \in (0,1)$. \label{lem: rdp2dp}
\end{lemma}


\begin{definition}[Smooth Sensitivity]\label{def: smooth}
	Given the smoothness parameter $\beta$, a $\beta$-smooth sensitivity of $f(X)$ is defined as 
	\[SS_\beta(X):= \max_{d\geq 0} e^{-\beta d} \cdot \max_{\tilde{X'}: dist(X, \tilde{X'})\leq d} \Delta_{LS}(\tilde{X}')\]
\end{definition}


\begin{lemma}[Private upper bound of data-dependent  RDP, Restatement of Theorem~\ref{lem: upperbound}]]
Given a RDP function $\rdp(\alpha, X)$ and a $\beta$-smooth sensitivity bound $SS(\cdot)$ of $\rdp(\alpha, X)$. Let $\mu$ (defined in Algorithm~\ref{alg: pate_ptr}) denote the private release of $\log(SS_\beta(X))$. Let $(\beta, \sigma_s, \sigma_2)$-GNSS mechanism be 
\[\scriptstyle
\rdp^{\text{upper}}(\alpha):=\rdp(\alpha, X) + SS_\beta(X) \cdot \cN(0, \sigma_s^2) + \sigma_s \sqrt{2\log(\frac{2}{\delta_2}) } e^{\mu} \]
	 Then, the release of $\rdp^{\text{upper}}(X)$ satisfies $(\alpha, \frac{3\alpha +2}{2\sigma_s^2})$-RDP for all $1<\alpha < \frac{1}{2\beta}$; w.p. at least $1-\delta_2$, $\rdp^{\text{upper}}(\alpha)$ is an upper bound of $\rdp(\alpha, X)$.
\vspace{-2mm}
\end{lemma}


\begin{proof}[Proof sketch]
%Let $\epsilon_{\sigma_1}(\alpha)^p$ denotes the output of applying $(\beta,\sigma_s, \sigma_2)$-GNSS on $\epsilon_{\sigma_1}(X)$.
  
We first show that releasing the smooth sensitivity $SS_\beta$ with $e^\mu$ satisfies $(\alpha, \frac{\alpha}{2\sigma_2^2})$-RDP. Notice that the log of $SS_\beta(X)$ has a bounded global sensitivity $\beta$ (Definition~\ref{def: smooth} implies that $|\log SS_\beta(X)-\log SS_\beta(X')|\leq \beta $ for any neighboring dataset $X, X'$). By Gaussian mechanism, scaling noise with $\beta \sigma_2$ to $\log SS_\beta(X)$ is $(\alpha, \frac{\alpha}{2\sigma_2^2})$-RDP.
Therefore, the release of $\rdp(\alpha, X)$ is $(\alpha, \epsilon_s(\alpha)+\frac{\alpha}{2\sigma_2^2})$-RDP. Since the release  of $ f(X) + SS_\beta(X)\cdot \cN(0, \sigma_s^2)$ is $(\alpha, \frac{\alpha+1}{\sigma_s^2})$-RDP (Theorem 23 from \citet{papernot2018scalable}) for $\alpha<\frac{1}{2\beta}$, we have
$\epsilon_s(\alpha)+\frac{\alpha}{2\sigma_2^2}=\frac{3\alpha+2}{2\sigma_s^2}$.


We next prove the second statement. First, notice that with probability at least $1-\delta_2/2$, $e^\mu \geq SS_\beta(X)$ using the standard Gaussian tail bound.  Let $E$ denote the event that $e^{\mu}\geq SS_\beta(X)$. 


%We next prove that with probability at least $1-\delta_2$, $\epsilon_{\sigma_1}^p(\alpha)\geq 
%\epsilon_{\sigma_1}(\alpha, X)$. Let $E$ denote the event that $e^{\mu}\geq SS_\beta(X)$. 


\begin{align*}
   & \pr\bigg[\rdp^{\text{upper}}(\alpha)\leq \rdp(\alpha, X)\bigg] \\
   &=  \pr\bigg[\rdp^{\text{upper}}(\alpha) \leq \rdp(\alpha,X)|E\bigg] + \pr\bigg[\rdp^{\text{upper}}(\alpha)\leq \rdp(\alpha, X)|E^c\bigg]\\
   &\leq \pr\bigg[\rdp^{\text{upper}}(\alpha) \leq\rdp(\alpha, X)|E\bigg] + \delta_2/2\\
   &= \underbrace{\pr\bigg[\cN(0, \sigma_s^2)\cdot SS_{\beta(X)}\geq \sigma_s \cdot \sqrt{2\log(2/\delta_2)}e^{\mu} |E\bigg]}_{\text{denoted by} (*)} + \delta_2/2\\
\end{align*}



Condition on the event $E$, $e^{\mu}$ is a valid upper bound of $SS_\beta(X)$, which implies  \[ (*) \leq  \pr[\cN(0, \sigma_s^2)\cdot SS_\beta(X) \geq \sigma_s \cdot \sqrt{2\log(2/\delta_2)} SS_\beta(X) |E] \leq \delta_2/2\]
Therefore, with probability at least $1- \delta_2$, $\rdp^{\text{upper}}(\alpha) \geq \rdp(\alpha, X)$.
\end{proof}


\begin{theorem}[Restatement of Theorem~\ref{thm: pate_ptr}]
Algorithm~\ref{alg: pate_ptr} satisfies $(\epsilon'+\hat{\epsilon}, \delta)$-DP.
\end{theorem}
%short version

\begin{proof}
The privacy analysis consists of two components --- the privacy cost of releasing an upper bound of data-dependent RDP ($\epsilon_{\text{upper}}(\alpha):=\epsilon_s(\alpha)+\frac{\alpha}{2\sigma_2^2}$ and the valid upper bound $\epsilon_{\sigma_1}^p(\alpha)$.
First, set $\alpha =\frac{2\log(2/\delta)}{\epsilon}+1$ and use RDP to DP conversion with $\delta/2$ ensures that the cost of $\delta/2$ contribution to be roughly $\epsilon/2$ (i.e., $\frac{\log(2/\delta)}{\alpha-1} = \epsilon/2$). Second,  choosing $\sigma_s = \sqrt{\frac{2+3\alpha}{\epsilon}}$ gives us another $\epsilon/2$. 
\end{proof}
%by choosing $\beta = \frac{0.2}{\alpha}$, we can upper bound $\epsilon_{\text{upper}}(\alpha)$ with $\frac{2+3\alpha}{2\sigma_s^2}$ (the first term in $\epsilon_s(\alpha)$ will be the dominant term).Then,

%In the experiments, we consider a tighter DP version of Algorithm~\ref{alg: pate_ptr} by choosing
%$\sigma_s=\sigma_2$ as the input. The choices on other parameters $(\beta, \alpha, \delta_2)$ and the algorithm procedure remain unchanged. Then the algorithm satisfies $(\epsilon_{\sigma_1}^p(\alpha)+ \frac{\epsilon}{2} + \epsilon_s(\alpha)+ \frac{\alpha}{2\sigma_2^2}, \delta)$-DP. The only difference is that, we  use the  exact $\epsilon_s(\alpha)=\frac{\alpha \cdot e^{2\beta}}{\sigma_s^2} + \frac{\beta \alpha - 0.5 \ln(1-2\alpha \beta)}{ \alpha-1}$ without approximation. Then, the privacy cost of releasing the upper bound will be $\epsilon_s(\alpha)+\frac{\alpha}{2\sigma_2^2}$.

\textbf{Experimental details}
 $K=400$ teacher models are trained individually on the disjoint set using AlexNet model. We set $\sigma_2 = \sigma_s = 15.0$.   Our data-dependent RDP calculation and the smooth-sensitivity calculation follow \citet{papernot2018scalable}. Specifically, we use the following theorem (Theorem~6 from~\citet{papernot2018scalable}) to compute the data-dependent RDP of each unlabeled data $x$ from the public domain.

\begin{theorem}[data-dependent RDP ~\citet{papernot2018scalable}]
 Let $\tilde{q}\geq \pr[\cM(X)\neq \argmax_{j\in [C]} n_j(x)]$, i.e., an upper bound of the probability that the noisy label does not match the majority label. Assume $\alpha\leq \mu_1$ and $\tilde{q}\leq e^{(\mu_2 -1)\epsilon_2}/\bigg(\frac{\mu_1}{\mu_1 -1} \cdot \frac{\mu_2}{\mu_2 -1}\bigg)^{\mu_2}$, then we have:
 \[\epsilon_{\cM}(\alpha, X) \leq \frac{1}{\alpha-1}\log \bigg( (1-\tilde{q})\cdot A(\tilde{q}, \mu_2, \epsilon_2)^{\alpha-1} +\tilde{q}\cdot B(\tilde{q}, \mu_1, \epsilon_1)^{\alpha-1}\bigg)  \]
 where $A(\tilde{q}, \mu_2, \epsilon_2):= (1-\tilde{q})/\bigg(1-(\tilde{q}e^{\epsilon_2})^{\frac{\mu_2-1}{\mu_2}}\bigg)$, $B(\tilde{q},\mu_1, \epsilon_1)=e^{\epsilon_1}/\tilde{q}^{\frac{1}{\mu_1 -1}}, \mu_2=\sigma_1 \cdot \sqrt{\log(1/\tilde{q})}, \mu_1 = \mu_2 +1, \epsilon_1 = \mu_1/\sigma_1^2 $ and $\epsilon_2 = \mu_2/\sigma_2^2$.
    
\end{theorem}
 
In the experiments, the non-private data-dependent DP baseline is also based on the above theorem.  Notice that the data-dependent RDP of each query is a function of $\tilde{q}$, where $\tilde{q}$ denotes an upper bound of the probability where the plurality output does not match the noisy output. $\tilde{q}$ is a complex function of both the noisy scale and data and is not monotonically decreasing when $\sigma_1$ is increasing.  


\textbf{Simulation of two distributions.}
The motivation of the experimental design is to compare three approaches under different data distributions. 
Notice that there are $K=400$ teachers, which implies the number of the vote count for each class will be bounded by $400$. In the simulation of high-consensus distribution, we choose $T=200$ unlabeled public data such that the majority vote count will be larger than $150$ (i.e., $\max_{j\in[C]} n_j(x)>150$). For the low-consensus distribution, we choose to select $T$ unlabeled data such that the majority vote count will be smaller than $150$.





\section{Omitted proofs in private GLM}
\subsection{Per-instance DP of GLM}
\begin{theorem}[Per-instance differential privacy guarantee\label{thm: glm}]
	Consider two adjacent data sets $Z$ and $Z' =[Z, (x,y)]$, and denote the smooth part of the loss function $F_s =   \sum_{i=1}^n l(y_i,\langle x_i, \cdot\rangle) + r_s(\cdot)$ (thus $\tilde{F}_s = F_s  +  l(y,\langle x, \cdot \rangle)$.
	Let the local neighborhood be the line segment between $\theta^*$ and $\tilde{\theta}^*$. Assume 
	\begin{enumerate}
		\item the GLM loss function $l$ be convex, three-time continuous differentiable and $R$-generalized-self-concordant w.r.t. $\|\cdot\|_2$,
		\item $F_s$ is locally $\alpha$-strongly convex w.r.t. $\|\cdot\|_2$,
	%	\item Denote the maximum generalized leverage score in the local region $\mu  =  \sup_{\theta \in [\theta^*,\tilde{theta}^*]}  l''(y,x^T\theta) \|x\|_{H_{\theta^{-1}}^2}$
		\item and in addition, denote $L := \sup_{\theta\in [\theta^*,\tilde{\theta}^*]}|l'(y,x^T\theta)|$, $\beta := \sup_{\theta\in [\theta^*,\tilde{\theta}^*]}|l''(y,x^T\theta)|$.	
	%	$l(y,x^T\theta)$ be locally $L$-Lipschitz and $\beta$-smooth (namely, $|l'|\leq L$ and $l'' \leq \beta$).
	\end{enumerate}
	
	Then the algorithm obeys $(\epsilon,\delta)$-pDP for $Z$ and $z=(x,y)$ with any $0<\delta < 2/e$ and
$$
\epsilon \leq \epsilon_0(1+\log(2/\delta))  +  e^{\frac{RL\|x\|_2}{\alpha}} \left[\frac{\gamma L^2\|x\|_{H^{-1}}^2}{2} +  \sqrt{ \gamma L^2\|x\|_{H^{-1}}^2\log(2/\delta) }\right]
$$
%where $\epsilon_0 := (e^{\frac{\|v\|_2}{R}} -1)(1+\log(2/\delta))  +  2\mu_2 + \mu_1\log(2/\delta)$
where 
$\epsilon_0 \leq e^{\frac{RL\|x\|_2}{\alpha}} -1  + 2\beta \|x\|_{H_1^{-1}}^2 +  2\beta\|x\|_{\tilde{H}_2^{-1}}^2.$
If we instead assume that $l$ is $R$-self concordant. Then the same results hold, but with all $e^{\frac{RL\|x\|_2}{\alpha}}$ replaced with $(1-RL\|x\|_{H^{-1}})^2$.

\end{theorem}
	
	Under the stronger three-times continuous differentiable assumption, by mean value theorem, there exists $\xi$ on the line-segment between $\theta^*$ and $\ttheta^*$ such that 
	$$
	H = \left[\int_{t=0}^{1}\nabla^2 F_s((1-t)\theta^* + t\ttheta^*)  dt \right]  =  \nabla^2 F_s(\xi).
	$$
	
	The two distributions of interests are $\cN(\theta^*,  [\gamma \nabla^2 F_s(\theta^*)]^{-1})$ and $\cN(\ttheta^*, [\gamma \nabla^2 F_s(\ttheta^*) + \nabla^2l(y,x^T\ttheta^*)]^{-1}).$
	Denote $[\nabla^2 F_s(\theta^*)]^{-1} =: \Sigma$ and $[\nabla^2 F_s(\ttheta^*) + \nabla^2l(y,x^T\ttheta^*)]^{-1} =: \tilde{\Sigma}$.
	Both the means and the covariance matrices are different, so we cannot use multivariate Gaussian mechanism naively. Instead we will take the tail bound interpretation of $(\epsilon,\delta)$-DP and make use of the per-instance DP framework as internal steps of the proof. 
	
	First, we can write down the privacy loss random variable in analytic form
	\begin{align*}
	\log\frac{|\Sigma|^{-1/2}e^{- \frac{\gamma}{2}\|\theta -\theta^*\|_{\Sigma^{-1}}^2}}{|\tilde{\Sigma}|^{-1/2}e^{- \frac{\gamma}{2}\|\theta -\ttheta^*\|_{\tilde{\Sigma}^{-1}}^2}}
	=\underbrace{\frac{1}{2}\log \left(\frac{|\Sigma^{-1}|}{|\tilde{\Sigma}^{-1}|}\right)}_{(*)} +  \underbrace{\frac{\gamma}{2}\left[\|\theta -\theta^*\|_{\Sigma^{-1}}^2 - \|\theta -\ttheta^*\|_{\tilde{\Sigma}^{-1}}^2\right]}_{(**)}
	\end{align*}
	The general idea of the proof is to simplify the expression above and  upper bounding the two terms separately using self-concordance and matrix inversion lemma, and ultimately show that the privacy loss random variable is dominated by another random variable having an appropriately scaled shifted $\chi$-distribution, therefore admits a Gaussian-like tail bound.
	
	
	To ensure the presentation is readable, we define a few short hands. We will use $H$ and $\tilde{H}$ to denote the Hessian of $F_s$ and $F_s +  f$ respectively and subscript $1$ $2$ indicates whether the Hessian evaluated at at $\theta^*$ or $\ttheta^*$. $H$ without any subscript or superscript represents the Hessian of $F_s$ evaluated at $\xi$ as previously used.
	\begin{align*}
	(*)  = \frac{1}{2} \log  \frac{|H_1|}{ |H| }\frac{|H|}{|H_2|}\frac{|H_2|}{|\tilde{H}_2|}  \leq \frac{1}{2}\left[  	\log\frac{|H_1|}{ |H| }  + \log \frac{|H|}{|H_2|} + \log\frac{|H_2|}{|\tilde{H}_2|}\right]
	\end{align*}
	By the $R$-generalized self-concordance of $F_s$, we can apply Lemma~\ref{lem:selfconcordant-hessian}, 
	$$
-\|\theta^*-\xi\|_2R\leq \log\frac{|H_1|}{ |H| } \leq R\|\theta^*-\xi\|_2, \quad   -R\|\xi - \ttheta^*\|_2\leq \log\frac{|H|}{ |H_2| } \leq R\|\xi - \ttheta^*\|_2.
	$$
	The generalized linear model ensures that the Hessian of $f$ is rank-$1$:
	$$\nabla^2 f(\ttheta^*) =  l''(y,x^T\ttheta^*)  xx^T$$
	and we can apply Lemma~\ref{lem:determinant} in both ways (taking $A=H_2$ and $A=\tilde{H}_2$) and obtain
	$$
	\frac{|H_2|}{|\tilde{H}_2|}   =  \frac{1}{1 + l''(y,x^T\ttheta^*)x^T H_2^{-1}  x}  =  1- l''(y,x^T\ttheta^*)x^T\tilde{H}_2 x
	$$
	Note that $ l''(y,x^T\ttheta^*)x^T\tilde{H}_2^{-1} x$ is the in-sample leverage-score and $ l''(y,x^T\ttheta^*)x^T H_2^{-1}  x$ is the out-of-sample leverage-score of the locally linearized problem at $\ttheta^*$. We denote them by $\mu_2$ and $\mu'_2$ respectively (similarly, for the consistency of notations, we denote the in-sample and out of sample leverage score at $\theta^*$ by $\mu_1$ and $\mu'_1$ ). %Note that $\mu_2'\leq \mu_2 \leq \beta \|x\|_{H_2^{-1}}$ and $\mu_1'\leq \mu_1 \leq\beta \|x\|_{H_1^{-1}}$
	
Combine the above arguments we get
	\begin{align}\label{eq:der_part1}
	   (*)\leq&  R\|\theta^*-\xi\|_2 + R\|\xi - \ttheta^*\|_2  + \log (1 - \mu_2) \leq R\|\theta^*-\ttheta^*\|_2 + \log(1-\mu_2)\\
	   (*) \geq& -R\|\theta^*-\ttheta^*\|_2  - \log(1-\mu_2).
	\end{align}
	
We now move on to deal with the second part, where we would like to express everything in terms of $\|\theta-\theta^*\|_{H_1}$, which we know from the algorithm is $\chi$-distributed.
\begin{align*}
(**)  = \frac{\gamma}{2}\left[ \|\theta -\theta^*\|_{H_1}^2 - \|\theta -\theta^*\|_{H_2}^2  + \|\theta -\theta^*\|_{H_2}^2 - \|\theta -\ttheta^*\|_{H_2}^2+ \|\theta -\ttheta^*\|_{H_2}^2- \|\theta -\ttheta^*\|_{\tilde{H}_2}^2  \right]
\end{align*}
By the generalized self-concordance at $\theta^*$ %\|\theta -\theta^*\|_{H_1}^2 - 
\begin{align*}
e^{-R\|\theta^*-\ttheta^*\|_2}\|\cdot\|_{H_1}^2 \leq \|\cdot\|_{H_2}^2 \leq   e^{R\|\theta^*-\ttheta^*\|_2}\|\cdot\|_{H_1}^2
\end{align*}
This allows us to convert from $\|\cdot\|_{H_2}$ to $\|\cdot\|_{H_1}$, and as a consequence:
$$
\left|\|\theta -\theta^*\|_{H_1}^2 - \|\theta -\theta^*\|_{H_2}^2 \right|  \leq   [e^{R\|\theta^*-\ttheta^*\|_2} - 1]\|\theta -\theta^*\|_{H_1}^2.
$$
%\begin{align*}
%e^{-R\|\theta^*-\ttheta^*\|_2}\|\theta -\theta^*\|_{H_1}^2 \leq \|\theta -\theta^*\|_{H_2}^2 \leq   e^{R\|\theta^*-\ttheta^*\|_2}\|\theta -\theta^*\|_{H_1}^2
%\end{align*}
Also, 
\begin{align*}
 \|\theta -\theta^*\|_{H_2}^2 - \|\theta -\ttheta^*\|_{H_2}^2  &=  \left\langle \ttheta^* -\theta^* ,  2\theta - 2\theta^* + \theta^*-\ttheta^*  \right\rangle_{H_2}  =  2 \langle  \theta-\theta^*, \ttheta^* -\theta^* \rangle_{H_2} -  \|\theta^*-\ttheta^*\|_{H_2}^2
 \end{align*}
 Therefore
 \begin{align*}
 \left|  \|\theta -\theta^*\|_{H_2}^2 - \|\theta -\ttheta^*\|_{H_2}^2\right|  &\leq 2\|\theta - \theta^*\|_{H_2} \|\theta^*-\ttheta^*\|_{H_2}  +  \|\theta^*-\ttheta^*\|_{H_2}^2  \\
 &\leq 2e^{R\|\ttheta^* - \theta^*\|_2}\|\theta - \theta^*\|_{H_1} \|\theta^*-\ttheta^*\|_{H}  + e^{R\|\ttheta^* - \theta^*\|_2}\|\theta^*-\ttheta^*\|_{H}^2.
\end{align*}
Then lastly  we have
\begin{align*}
0\geq \|\theta -\ttheta^*\|_{H_2}^2- \|\theta -\ttheta^*\|_{\tilde{H}_2}^2 &=  -l''(y,x^T\ttheta^*)\left[ \langle x, \theta-\theta^* \rangle + \langle x,\theta^*-\ttheta^*\rangle\right]^2   \\
&\geq -2\beta \|x\|_{H_1^{-1}}^2\|\theta-\theta^*\|_{H_1}^2   -  2\beta \|x\|_{H^{-1}}^2\|\theta^*-\ttheta^*\|_{H}^2
\end{align*}
$$
\left|  \|\theta -\ttheta^*\|_{H_2}^2- \|\theta -\ttheta^*\|_{\tilde{H}_2}^2\right|  \leq 2\beta \|x\|_{H_1^{-1}}^2\|\theta-\theta^*\|_{H_1}^2   +  2\beta \|x\|_{H^{-1}}^2\|\theta^*-\ttheta^*\|_{H}^2
$$

Combine the above derivations, we get 
\begin{align}
\left|(**)\right|  \leq \frac{\gamma}{2}\left[  a \|\theta-\theta^*\|_{H_1}^2 + b \|\theta-\theta^*\|_{H_1}  +c\right] \label{eq:der_part2}
\end{align}
where 
\begin{align*}
a :=& \left[ e^{R\|\theta^*-\ttheta^*\|_2} -1  + 2\beta \|x\|_{H_1^{-1}}^2\right] \\
b:=& 2 e^{R\|\theta^*-\ttheta^*\|_2}   \|\theta^*-\ttheta^*\|_H \\
c:=& (e^{R\|\theta^*-\ttheta^*\|_2} + 2\beta \|x\|_{H^{-1}}^2)\|\theta^*-\ttheta^*\|_H^2
\end{align*}

Lastly, by \eqref{eq:der_part1} and $\eqref{eq:der_part2}$, 
$$
\left|  \log\frac{p(\theta|Z)}{p(\theta|Z')}  \right|  \leq R\|\theta^*-\ttheta^*\|_2  + \log(1-\mu_2)  +  \frac{\gamma}{2} [ a W^2 + bW + c].
$$
where according to the algorithm $W:= \|\theta-\theta^*\|_{H_1}$ follows a half-normal distribution with $\sigma=\gamma^{-1/2}$.

By standard Gaussian tail bound, we have for all $\delta<2/e$.
$$
\P(|W|\leq \gamma^{-1/2} \sqrt{\log(2/\delta)} )  \leq \delta.
$$
This implies that a high probability upper bound of the absolute value of the privacy loss random variable $\log \frac{p(\theta|Z)}{p(\theta|Z')}$ under $p(\theta|Z)$.
By the tail bound to privacy conversion lemma (Lemma~\ref{lem:tailbound2DP}), we get 
that for any set $S\subset \Theta$
$\P(\theta \in S | Z) \leq e^\epsilon \P(\theta \in S | Z') +\delta$
for any $0<\delta<2/e$ and 
$$
\epsilon  = R\|\theta^*-\ttheta^*\|_2  + \log(1-\mu_2)  + \frac{\gamma c}{2}  + \frac{a}{2}  \log(2/\delta)  +  \frac{\gamma^{1/2} b}{2}  \sqrt{\log(2/\delta)}.
$$
Denote $v:=  \theta^*-\ttheta^*$, by strong convexity
$$\|v\|_2\leq \|\nabla l(y,x^T\theta)[\ttheta^*]\|_2/\alpha  = |l'| \|x\|_2 / \alpha \leq L\|x\|_2/\alpha$$
and 
$$
\|v\|_H \leq \|\nabla l(y,x^T\theta)[\ttheta^*]\|_{H^{-1}}  =  |l'| \|x\|_{H^{-1}} \leq L\|x\|_{H^{-1}}.
$$
Also use the fact that $|\log(1-\mu_2)| \leq 2\mu_2$ for $\mu_2<0.5$ and $\mu_2\leq \beta\|x\|_{\tilde{H}_2^{-1}}^2 $, we can then combine similar terms and have a more compact representation.
%$$
%\epsilon=   \frac{\|v\|_2}{R} +  \log(1- \mu_2)  + (\mu_1  + e^{\frac{\|v\|_2}{R}} -1)\log(2/\delta)  +  e^{\frac{\|v\|_2}{R}} \left[\frac{\gamma \|v\|_H^2}{2} +  \sqrt{ \gamma \|v\|_H^2\log(1/\delta) }\right]
%$$
$$
\epsilon \leq \epsilon_0(1+\log(2/\delta))  +  e^{\frac{RL\|x\|_2}{\alpha}} \left[\frac{\gamma L^2\|x\|_{H^{-1}}^2}{2} +  \sqrt{ \gamma L^2\|x\|_{H^{-1}}^2\log(2/\delta) }\right]
$$
%where $\epsilon_0 := (e^{\frac{\|v\|_2}{R}} -1)(1+\log(2/\delta))  +  2\mu_2 + \mu_1\log(2/\delta)$
where 
$$\epsilon_0 \leq e^{\frac{RL\|x\|_2}{\alpha}} -1  + 2\beta \|x\|_{H_1^{-1}}^2 +  2\beta\|x\|_{\tilde{H}_2^{-1}}^2$$ 
is the part of the privacy loss that does not get smaller as $\gamma$ decreases.





\begin{proposition}\label{prop:generalnorm}
	Let $\|\cdot\|$ be a norm and $\|\cdot\|_*$ be its dual norm. Let $F(\theta)$, $f(\theta)$ and $\tilde{F}(\theta) = F(\theta) + f(\theta)$ be proper convex functions and $\theta^*$ and $\tilde{theta}^*$ be their minimizers, i.e., $0\in \partial F(\theta^*)$ and $0\in \partial \tilde{F}(\tilde{theta}^*)$.  If in addition, $F,\tilde{F}$ is $\alpha,\tilde{\alpha}$-strongly convex with respect to $\|\cdot\|$ within the restricted domain 
	$\theta \in \{  t\theta^* + (1-t)\tilde{\theta}^*  \;|\;  t\in[0,1]  \}$. 	Then there exists $g \in \partial f(\theta^*)$ and $\tilde{g}\in \partial f(\tilde{\theta}^*)$ such that
	$$
	\|\theta^*-\tilde{\theta}^*\| \leq\min\left\{\frac{1}{\alpha}\| \tilde{g}\|_*,  \frac{1}{\tilde{\alpha}}\| g\|_*\right\}.
	$$
	 %continuously differentiable with Lipschitz gradient. Let $\theta^*$ and $\ttheta^*$ be such that $0\in \partial F(\theta^*)$ and $0\in \partial \tilde{F}(\ttheta^*)$. If in addition $F(\theta)$ is $\alpha$-strongly convex with respect to $\|\cdot\|$ for all 
	%$\theta \in \{  t\theta^* + (1-t)\ttheta^*  \;|\;  t\in[0,1]  \}$. 
\end{proposition}
\begin{proof}
	Apply the first order condition to $F$ restricted to the line segment between $\tilde{\theta}^*$ and $\theta^*$, there are we get
	\begin{align}
	F(\tilde{\theta}^*) \geq F(\theta^*)  +  \langle \partial F(\theta^*),  \tilde{\theta}^*-\theta^* \rangle  + \frac{\alpha}{2}\|\tilde{\theta}^*-\theta^*\|^2\label{eq:strongcvx1} \\
	F(\theta^*) \geq F(\tilde{\theta}^*)  +  \langle \partial F(\tilde{\theta}^*),  \theta^*-\tilde{\theta}^* \rangle  + \frac{\alpha}{2}\|\tilde{\theta}^*-\theta^*\|^2 \label{eq:strongcvx2}
	\end{align}
	Note by the convexity of $F$ and $f$, $\partial\tilde{F}=  \partial F + \partial f$, where $+$ is the Minkowski Sum. Therefore, $0\in \partial\tilde{F}(\tilde{\theta}^*)$ implies that there exists $\tilde{g}$ such that $\tilde{g}\in \partial f(\tilde{\theta}^*)$ and $-\tilde{g}\in\partial F(\tilde{\theta}^*)$.
	Take $-\tilde{g}\in\partial F(\tilde{\theta}^*)$ in Equation~\ref{eq:strongcvx2} and $0 \in \partial F(\theta^*)$ in Equation~\ref{eq:strongcvx1}  and add the two inequalities, we obtain
	$$
		0\geq \langle -\tilde{g},  \theta^*-\tilde{\theta}^* \rangle  + \alpha \|\tilde{\theta}^* - \theta^*\|^2 \geq - \|\tilde{g}\|_* \|\theta^*-\tilde{\theta}^*\|  +  \alpha\|\tilde{\theta}^* - \theta^*\|^2. 
	$$
	For $\|\tilde{\theta}^* - \theta^*\|=0$ the claim is trivially true, otherwise, we can divide the both sides of the above inequality by $\|\tilde{\theta}^* - \theta^*\|$ and get
	$	\|\theta^*-\tilde{\theta}^*\| \leq \frac{1}{\alpha}\| \tilde{g}\|_*$. 
	
	It remains to show that $\|\theta^*-\tilde{\theta}^*\| \leq \frac{1}{\tilde{\alpha}}\|g\|_*$. This can be obtained by exactly the same arguments above but applying strong convexity to $\tilde{F}$ instead. Note that we can actually get something slightly stronger than the statement because the inequality holds for all $g\in \partial f(\theta^*)$.
\end{proof}



A consequence of (generalized) self-concordance is the spectral (\emph{multiplicative}) stability of Hessian to small perturbations of parameters.
\begin{lemma}[Stability of Hessian{\citep[Theorem~2.1.1]{nesterov1994interior}, \citep[Proposition~1]{bach2010self}}]\label{lem:selfconcordant-hessian}
	Let $H_\theta :=  \nabla^2F_s(\theta)$. If $F_s$ is $R$-self-concordant at $\theta$. Then for any $v$ such that $R \|v\|_{H_\theta} < 1$, we have that
	$$
	(1-R\|v\|_{H_\theta})^2 \nabla^2 F_s(\theta) 	\prec	\nabla^2 F_s(\theta+v) \prec  \frac{1}{(1-R\|v\|_{H_\theta})^2}   \nabla^2 F_s(\theta)  .
	$$
	If instead we assume $F_s$ is $R$-generalized-self-concordant at $\theta$ with respect to norm $\|\cdot\|$, then
	$$
	e^{-R\|v\|} \nabla^2 F_s(\theta) \prec  \nabla^2 F_s(\theta+v)  \prec e^{R\|v\|}  \nabla^2 F_s(\theta) 
	$$
\end{lemma}\label{stability}
The two bounds are almost identical when  $R\|v\|$ and $R\|v\|_{\theta}$ are close to $0$, in particular, for $x\leq 1/2$, $e^{-2x} \leq 1-x \leq e^{-x}$.