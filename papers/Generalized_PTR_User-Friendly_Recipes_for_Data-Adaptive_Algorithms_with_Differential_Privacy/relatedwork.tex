%\vspace{-1em}
\section{Related Work}
%\vspace{-1em}
\label{sec:related_work}

\textbf{Data-dependent DP algorithms.}
Privately calibrating noise to the local sensitivity is a well-studied problem. One approach is to add noise calibrated to the smooth sensitivity \citep{nissim2007smooth}, an upper bound on the local sensitivity which changes slowly between neighboring datasets. %Smoothed sensitivity considers the local sensitivity of all possible datasets, discounted by their distance to an input dataset $X$.
An alternative to this -- and the focus of our work -- is  Propose-Test-Release (PTR) \citep{dwork2009differential}, which works by calculating the distance $\mathcal{D}_{\beta}(X)$ to the nearest dataset to $X$ whose local sensitivity violates a proposed bound $\beta$. The PTR algorithm then adds noise to $\mathcal{D}_{\beta}(X)$ before testing whether this privately computed distance is sufficiently large.

PTR spin-offs abound. Notable examples include stability-based methods \citep{thakurta2013differentially} (stable local sensitivity of $0$ near the input data) and privately releasing upper bounds of local sensitivity \citep{kasiviswanathan2013analyzing,liu2021differential, decarolis2020end}. We refer readers to Chapter 3 of \citet{vadhan2017complexity} for a concise summary of these classical results. 
Recent work
\citep{wang2022renyi} has provided R\'enyi DP bounds for PTR and demonstrated its applications to robust DP-SGD. Our work (see Section~\ref{subsections:pate}) also considers applications of PTR in data-adaptive private deep learning: Instead of testing the local sensitivity of each gradient step as in \citet{wang2022renyi}, our PTR-based PATE algorithm tests the data-dependent privacy loss as a whole.

\citet{liu2021differential} proposed a new variant called High-dimensional Propose-Test-Release (HPTR). HPTR provides a systematic way of solving DP statistical estimation problems by using the exponential mechanism (EM) with carefully constructed scores based on certain one-dimensional robust statistics, which have stable local sensitivity bounds. HPTR focuses on designing data-adaptive DP mechanisms from scratch; our method, in contrast, converts existing randomized algorithms (including EM and even some that do not satisfy DP) into those with formal DP guarantees. Interestingly, our proposed method also depends on a one-dimensional statistic of direct interest: the data-dependent privacy loss.

 %algorithms.

% The High-dimensional Propose-Test-Release (HPTR) framework \cite{liu2021differential} establishes a unifying characterization of the statistical efficiency of differentially private statistical estimation problems. HPTR releases its output via the exponential mechanism, with a score function whose access to the data depends only on one-dimensional robust statistics (e.g. one-dimensional robust mean or variance). Because these one-dimensional robust statistics have low local sensitivity on resilient\footnote{Resilience of a dataset measures how sensitive the sample statistics are to removing a fraction of the datapoints.} % \footnote{Resilience of a dataset measures how sensitive the sample statistics are to removing a fraction of the datapoints, similar to the finite sample breakdown point of \cite{brunel2020propose}.} 
% datasets, \cite{liu2021differential} can then provide tight local sensitivity bounds and use the PTR mechanism to check that a given dataset satisfies the resilience property.

% Recent work \cite{brunel2020propose} has put forward a more general version of the PTR mechanism for the purpose of deriving concentration inequalities for differentially private median
% and mean estimators. Their algorithm introduces a notion of distance related to the finite sample breakdown point, a measure of statistical robustness which quantifies the smallest fraction of datapoints which can be arbitrarily changed before the estimator diverges. While this allows their PTR algorithm to generalize to a broad class of estimators, the choice of how to inject noise is still restricted to either the Laplace or Gaussian mechanism.



% Our work, in contrast, makes no assumptions about the statistical properties of an input dataset. Instead we focus on developing a flexible framework suitable for any choice of DP mechanism.




\textbf{Data-dependent DP losses.} The flip side of data-dependent DP algorithms is the study of data-dependent DP losses \citep{papernot2018scalable,soria2017individual,wang2017per}, which fix the randomized algorithm but parameterize the resulting privacy loss by the specific input dataset. For example: In the simple mechanism that adds Laplace noise with parameter $b$, data-dependent DP losses are $\epsilon(X) = \Delta_{\text{LS}}(X)/b$. The data-dependent DP losses are often much smaller than the DP loss, but they themselves depend on the data and thus may reveal sensitive information; algorithms satisfying a data-dependent privacy guarantee are not formally DP with guarantees any smaller than that of the worst-case. Existing work has considered privately publishing these data-dependent privacy losses \citep{papernot2018scalable,redberg2021privately}, but notice that privately publishing these losses does not improve the DP parameter of the given algorithm. Part of our contribution is to resolve this conundrum by showing that a simple post-processing step of the privately released upper bound of $\epsilon(\text{Data})$ gives a formal DP algorithm.

% \textbf{On data-dependent DP losses.} In addition to the above, there has been an increasing list of empirical DP work that fix the parameters of a randomized algorithm while reporting the resulting data-dependent DP losses $\epsilon(\text{Data})$ after running on a specific dataset \citep{ligett2017accuracy,papernot2018scalable,zhu2020private, feldman2021individual}. The data-dependent DP losses are often smaller than the worst-case DP losses, but technically speaking, these algorithms are not formally DP with DP guarantees any smaller than that of the worst-case. In addition, the data-dependent DP losses themselves are sensitive, and thus cannot be reported. A typical solution is to privately release $\epsilon(\text{Data})$, but it still does not satisfy DP as this would require a prescribed $(\epsilon,\delta)$-DP parameter to be satisfied for all input datasets. Part of our contribution is to resolve this conundrum by showing that a simple post-processing step of the privately released upper bound of $\epsilon(\text{Data})$ gives a formal DP algorithm.

\textbf{Private hyper-parameter tuning.}
%In addition to normal machine learning parameters (e.g., the learning rate) DP algorithms introduce a few new hyper-parameters, e.g., the clipping threshold in DP-SGD.
Our work has a nice connection with private hyper-parameter tuning. 
%Recent work~\citep{papernot2021hyperparameter} empirically demonstrates that repeat privacy-preserving training and tuning hyper-parameters does leak private information. 
Prior work~\citep{liu2019private, papernot2021hyperparameter} requires each candidate configuration to be released with the same DP (or R\'enyi DP) parameter set. Another hidden assumption is that the parameters must not be privacy-correlated
(i.e., parameter choice will not change the privacy guarantee). Otherwise we need to use the largest DP bound across all candidates.
For example, \citet{liu2019private} show that if each mechanism (instantiated with  one group of hyper-parameters) is $(\epsilon, 0)$-DP, then running a random number of mechanisms and reporting the best option satisfies $(3\epsilon, 0)$-DP. 
Our work directly generalizes the above results by (1) considering a wide range of hyper-parameters, either privacy-correlated or not; and (2) requiring only that individual candidates to have a \emph{testable} data-dependent DP.







