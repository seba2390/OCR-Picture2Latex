\subsection{An Improved Algorithm via LP Rounding}
\label{sec:lp}
As mentioned above, \g{} can hit its worst-case bound of $\Omega(s^2)\cdot \rand$ even when \opt{} is actually small. We know that for the case of $1$-Borda, no polynomial-time algorithm can do better. Now the question is, can a different algorithm do better in the case of $s$-Borda for $s = \omega(1)$? We answer this question in the affirmative by presenting an algorithm that is based on dependent rounding of an LP relaxation combined with uniform random sampling, which provides nontrivial improvement when $\opt$ is small. In particular, it achieves expected score at most $3\cdot \opt + O(s^{3/2}\log s)\cdot \rand$. 

\subsubsection{LP-Rounding-Based Algorithm}
The following linear program (based on~\cite{cornuejols1983,LinV,CharikarGTS,fault,Byrka}) is a natural relaxation for the $s$-Borda problem.
\begin{mini*}
{}{\sum_{i = 1}^n\sum_{\ell = 1}^s\sum_{j=1}^{m} x_{ij}^{\ell} \cdot r_{v_i}(c_j),}{}{}
\addConstraint{\sum_{j=1}^m\:} {y_j}{=k}
\addConstraint{\sum_{\ell = 1}^k\:}{x_{ij}^{\ell}}{\leq y_j,}{\mkern46mu i \in \{1, \ldots, n\}, \:\:j\in\{1 ,\ldots, m\}} 
\addConstraint{\sum_{j = 1}^m\:}{x_{ij}^{\ell}}{\geq 1,}{\mkern53mu i \in \{1, \ldots, n\},\:\: \ell\in \{1, \ldots, k\}}
\addConstraint{y_j, \:}{x_{ij}^{\ell}}{\in [0, 1],}{\mkern26mu i \in \{1, \ldots, n\}, \:\: j \in \{1, \ldots, m\}, \:\: \ell \in \{1, \ldots, k\}.}
\end{mini*}

Variable $y_j$ denotes how much candidate $c_j$ is chosen; integral values $1$ and $0$ mean choosing and not choosing candidate $c_j$, respectively. The first constraint encodes choosing exactly $k$ candidates. We copy each voter $k$ times, and the $\ell^{\text{th}}$ copy of the voter $v_i$ is assigned to the $\ell^{\text{th}}$-preferred chosen candidate. Variable $x_{ij}^{\ell}$ denotes how much the $\ell^{\text{th}}$ copy of voter $v_i$ is assigned to candidate $c_j$. The second constraint prevents a voter from being assigned to a candidate that is not chosen. The third constraint ensures that each copy of the voter is assigned to some candidate. The objective function computes the $s$-Borda score.

We will use dependent rounding~\cite{dependent} to round this LP solution. There is a catch though: Dependent rounding can cause a deficit in around $\tilde{O}(\sqrt{s})$ candidates from the top $s$ that are fractionally chosen by the LP. Since any solution must account for the top $s$ scores, we need to ensure these ``deficit'' candidates do not increase the score too much. Towards this end, we scale down the LP solution, and choose enough candidates uniformly at random so that these candidates can absorb the deficit. However, such scaling creates a further deficit that will have to be absorbed by random sampling. We find that the right trade-off is achieved by scaling down by a factor of $(1-\frac{1}{\sqrt{s}})$.

Without further ado, the overall algorithm works as follows:
\begin{enumerate}
    \item Solve the above linear program and let $\tilde{y}$ denote the optimal solution.
    \item For $j = 1,2,\ldots, m$, let $y_j = (1 - \frac{1}{\sqrt{s}}) \tilde{y}_j$. Note that $\sum_{j=1}^m y_j = k (1 - \frac{1}{\sqrt{s}})$.
    \item Apply dependent rounding~\cite{dependent} on the variables $\{y_j\}$ so that exactly $k (1-\frac{1}{\sqrt{s}})$ candidates are chosen. Let $T_1$ denote the set of chosen candidates.
    \item Finally choose a set $T_2$ of $\frac{k}{\sqrt{s}}$ candidates uniformly at random from $\C \setminus S$ and output $T = T_1 \cup T_2$. 
\end{enumerate}

We will show the following theorem:

\begin{theorem}
\label{thm:lp}
When $m = \omega(k)$, $k = \omega(s^{3/2})$, and $s = \omega(1)$, we have:
$$\E[r_{\V}(T)] \leq 3\opt +O(s^{3/2}\log s)\cdot \rand.$$
\end{theorem}


\subsubsection{Analysis: Proof of Theorem~\ref{thm:lp}}
First consider dependent rounding on $\{y_j\}$. Let $Y_j$ denote the random variable which returns $1$ if $y_j$ is rounded to $1$ and $0$ if $y_j$ is rounded to $0$. Note that $\E[Y_j] = y_j$ for all candidates $j \in \C$. The following lemma is an easy consequence of Chernoff bounds:

\begin{lemma}
\label{lem:bound}
For any subset of candidates $\{c_{j_1}, \ldots, c_{j_\ell}\}$, let $W = \sum_{t = 1}^\ell y_{j_t}$. If $W = \omega(1)$, then 
$$\Pr\left[\sum_{t = 1}^\ell Y_{j_t} \in \left(W - 9\sqrt{W\log W}, W + 9\sqrt{W\log W}\right)\right] \geq 1 - \frac{2}{W^3}.$$
\end{lemma}
%\begin{proof}
%Since  $\E[\sum_{t = 1}^\ell Y_{j_t}] = W$, by Chernoff Bounds applied to negatively dependent random variables, we have: $\Pr\left[ |\sum_{t = 1}^\ell Y_{j_t} - W | \geq 2 \sqrt{W\log W}\right] \leq \frac{2}{W^3}.$
%\qedhere
%\end{proof}

 We now compute the expected $s$-Borda score for each voter. Towards this end, we partition the candidates into buckets with geometrically decreasing sum of $y_i$ values, and account for the expected score generated by dependent rounding in each bucket against the LP value of the subsequent bucket. Lemma~\ref{lem:bound} will ensure the number of candidates chosen from each bucket is close to the LP value, and the deficit gets taken care of by the uniformly randomly chosen candidates.

For simplicity of notation, let $\eta = \log_2 \frac{\sqrt{s}}{2}$. Fix a voter $v_i$, and suppose its preference order is $c_{i_1} \succ c_{i_2} \succ \ldots \succ c_{i_m}$. Recall that $\{\tilde{y}, x\}$ is the LP solution. The values $x_{ij}$ in the LP are set as follows: Consider the prefix of the ordering such that $\sum_{t=1}^{\ell} \tilde{y}_{i_t} \le s$ and $\sum_{t=1}^{\ell+1} \tilde{y}_{i_t} > s$. The LP sets $x_{i i_t} = \tilde{y}_{i_t}$ for $t \le \ell$, and sets $x_{i i_{\ell+1}} = s - \sum_{t=1}^{\ell} \tilde{y}_{i_t}$. The contribution of $v_i$ to the LP objective is therefore
\begin{equation}
    \label{eq:opt1}
\opt_i =  \sum_{t=1}^{\ell} r_{v_i}(c_{i_t}) \tilde{y}_{i_t} + r_{v_i}(c_{i_{\ell+1}}) \left(s - \sum_{t=1}^{\ell} \tilde{y}_{i_t} \right) \ge \sum_{t=1}^{\ell} r_{v_i}(c_{i_t}) \tilde{y}_{i_t}.
\end{equation}

Consider the first $\ell$ candidates in the above ordering. We have $\sum_{j=1}^{\ell} \tilde{y}_j \ge s - 1$, so that
\begin{equation}
    \label{eq:sumy}
\sum_{j=1}^{\ell} y_j \ge (s-1) \left(1 - \frac{1}{\sqrt{s}} \right) \ge s - (\sqrt{s}+1) \ge s - 2 \sqrt{s}.
\end{equation}
We split these $\ell$ candidates into sets $\C_1, \ldots, \C_{\eta}$ as follows: We walk down the preference order of $v_i$. We take $\C_1$ as the set of candidates whose $y$-values sum to $\frac{s}{2}$;  $\C_2$ as the next set of candidates whose  $y$-values sum to $\frac{s}{4}$, and so on until $\C_{\eta}$, whose sum of $y$-values is $\frac{s}{2^\eta} = 2\sqrt{s}$.  Now the sum of $y$-values of all candidates in $\{\C_1, \ldots, \C_{\eta}\}$ is exactly $s - 2\sqrt{s}$. Formally, we define 
\[
\theta_0 = 0, \qquad \theta_h = \min\left\{q \ \bigg| \ \sum_{t = 1}^q y_{i_t} \geq \left(1 - \frac{1}{2^h}\right)s \right\}, \:\:\forall h \in \{1, \ldots, \eta\},
\]
and correspondingly define the sets $\{\C_1, \ldots, \C_{\eta}\}$ as:
$$\C_h = \{c_{i_{q}} \mid \theta_{h - 1} < q \leq \theta_h\}, \:\: \forall h \in \:\{1, \ldots, \eta\}.$$

For all $h \in \{1, \ldots, \eta\}$, let $y_{\C_h} = \sum_{c_j \in \C_h}y_j$ and  $Y_{\C_h} = \sum_{c_j \in \C_h}Y_j$. Note that $y_{\C_h}$ decreases by a factor of $2$ as $h$ increases. Now consider the outcome of the dependent rounding procedure for each of the sets $\C_1, \ldots \C_{\eta}$. We say the rounding fails for $v_i$ if there exists $h \in \{1, \ldots, \eta-1\}$ such that the number $Y_{\C_h}$ of chosen candidates  in $\C_h$ is not in range $y_{\C_h} \pm 9 \sqrt{y_{\C_h}\log y_{\C_h}}$. We will not consider $\C_{\eta}$ when defining failure, and will deal with this set separately. 

Let $\F$ denote the failure event. We now bound the probability of the event $\F$ for voter $v_i$.
\begin{lemma}
$\Pr[\F] \leq s^{-3/2}.$
\end{lemma}
\begin{proof}
By union bound applied to Lemma~\ref{lem:bound}, we have:
\begin{align*}
\Pr[\F] & \leq \sum_{h = 1}^{\eta-1}\frac{2}{y_{\C_h}^3} \leq \frac{3}{y_{\C_{\eta-1}}^3} \le s^{-3/2},
\end{align*}
where we have used that $\{y_{\C_h}\}$ is a geometrically decreasing sequence, and that $y_{\C_{\eta-1}} = 4 \sqrt{s}$. 
\end{proof}

We are now ready to compute the expected score for $v_i$ in our algorithm. Recall that $T$ denotes the set of chosen candidates and $\opt_i$ denotes the $s$-Borda score for $v_i$ in the LP solution. Let $\mathsc{Bad}$ denote the expected $s$-Borda score for $v_i$ in the event $\F$, and $\mathsc{Good}$ denote the expected score otherwise. We will bound these separately below.

\begin{lemma}
\label{lem:bad}
$\mathsc{Bad} \le O(s^{5/2})\cdot \rand$.
\end{lemma}
\begin{proof}
If $\F$ happens, the final solution is still at least as good as choosing the $\frac{k}{\sqrt{s}}$ random candidates in Step (4) of the algorithm. Note that since we assumed $k = \omega(s^{3/2})$, we have $\frac{k}{\sqrt{s}} \ge s$, so that we will have chosen enough random candidates to fill up at least $s$ positions for computing $s$-Borda score. Further, since we assume that $m =  \omega(k)$, the score of the solution will at most double had we assumed these $\frac{k}{\sqrt{s}}$ candidates are chosen randomly from the entire set of $m$ candidates instead of from the remaining $m - k(1 - \frac{1}{\sqrt{s}})$ candidates after dependent rounding. Thus, we have:
$$\mathsc{Bad} \leq 2 \E_{T \subseteq C, |T| = k/\sqrt{s}}[r_{v_i}(T)] \le 2 \frac{s(s+1)}{2} \frac{m+1}{\frac{k}{\sqrt{s}} + 1}\leq 4s^{3/2}(s + 1)\cdot \rand,$$
which yields that $\mathsc{Bad} \leq O(s^{5/2})\cdot \rand$.
\end{proof}

\begin{lemma}
\label{lem:good}
$\mathsc{Good} \leq 3\opt_i +O(s^{3/2}\log s)\cdot \rand$.
\end{lemma}
\begin{proof}
Suppose $\F$ does not happen. Denote the set of candidates chosen by the algorithm from $\{\C_1, \ldots, \C_{\eta-1}\}$ as $T_1$, and the randomly chosen $\frac{k}{\sqrt{s}}$ candidates as $T_2$. Therefore $T = T_1 \cup T_2$. From Eq~(\ref{eq:sumy}), we have 
$$\sum_{h=1}^{\eta-1} y_{\C_h} = s - 2\sqrt{s} - y_{\C_{\eta}} = s - 4\sqrt{s}.$$ 
Since $\F$ does not happen, we have:
$$|T_1| \geq s - 4 \sqrt{s} - \sum_{h = 1}^{\eta-1} \sqrt{y_{\C_h}\log y_{\C_h}} \geq s - 4 \sqrt{s} - \sum_{h = 1}^{\eta-1} \sqrt{\frac{s}{2^h}\log s} \geq s - 4 \sqrt{s} - (\sqrt{2} + 1)\sqrt{s\log s}.$$

Denote $u = 4 \sqrt{s} + (\sqrt{2} + 1)\sqrt{s\log s} = O(\sqrt{s\log s})$, so that $|T_1| \ge s - u$.  The quantity $u$ is the total ``deficit'' in candidates from the top $s$ that is caused by scaling the LP and dependent rounding. We make up this deficit using the set $T_2$. Specifically, consider the subsets, 
$$T_1^* = \argmin_{Q \subseteq T_1, |Q| = s - u}\sum_{c \in Q}r_{v_i}(c) \qquad \mbox{and} \qquad T_2^* = \argmin_{Q \subseteq T_2, |Q| = u}\sum_{c \in Q}r_{v_i}(c).$$ 
Note that $T_1^* \subseteq T_1$, and $T_2^* \subseteq T_2$. We will evaluate the score of these subsets of candidates, which will be an upper bound on the score of the algorithm. Towards this end, we define $\mu_h$ as the scaled LP score of $\C_{h}$, that is:
$$\mu_h = \frac{\sum_{j = \theta_{h - 1} + 1}^{\theta_h}r_{v_i}(c_{i_j})y_{i_j}}{y_{\C_h}}, \:\: \forall h \in \{1, \ldots, \eta\}.$$

Since $y_i \le \tilde{y}_i$, combining the previous inequality with Eq~(\ref{eq:opt1}), we have:
$$\sum_{h = 1}^{\eta}\mu_hy_{\C_h} \le \opt_i.$$

Since $y_{\C_h} \ge 2\sqrt{s} = \omega(1)$ for all $h \in \{1,2,\ldots, \eta\}$, we have:
$$|T_1 \cap \C_h| \le y_{\C_h} + 9 \sqrt{y_{\C_h} \log y_{\C_h}} \le \frac{3}{2} y_{\C_h}.$$ 

Since $\mu_h > r_{v_i}(c), \forall c \in \C_{h - 1}$ and since $y_{\C_h} \le 2 y_{\C_{h+1}}$, we can bound the expected score of $T_1^*$ as:
$$\sum_{c \in T_1^*}r_{v_i}(c) \leq \sum_{h = 1}^{\eta-1} \frac{3}{2} \cdot y_{\C_h}\mu_{h + 1} \le \sum_{h = 1}^{\eta-1} 3\cdot y_{\C_{h + 1}}\mu_{h + 1} \le 3\opt_i.$$

We can again assume that the $\frac{k}{\sqrt{s}}$ random candidates are chosen randomly from the entire set of $m$ candidates. This yields a bound on the score of $T_2^*$ as:
$$\sum_{c \in T_2^*}r_{v_i}(c) \leq \sum_{t = 1}^u t \cdot(\sqrt{s}\cdot \rand) = O(s^{3/2}\log s)\cdot \rand.$$
where we used $u = O(\sqrt{s\log s})$ to derive $\sum_{t = 1}^u t = O(s\log s)$. 

Therefore, we can bound $\mathsc{Good}$ as follows:
\[\mathsc{Good} \leq \sum_{c \in T_1^*}r_{v_i}(c) + \sum_{c \in T_2^*}r_{v_i}(c) \leq 3\opt_i + O(s^{3/2}\log s)\cdot \rand.\qedhere\]
\end{proof}

Synthesizing the bounds from Lemmas~\ref{lem:bad} and~\ref{lem:good}, we can conclude that:
\begin{align*}
\E[r_{v_i}(T)] &= \mathsc{Bad}\cdot \Pr[\F] + \mathsc{Good}\cdot (1-\Pr[\F])\\
%&\leq \mathsc{Bad}\cdot \Pr[\F] + \mathsc{Good} \\
&\leq  s^{-3/2} \cdot O(s^{5/2})\cdot \rand + 3\opt_i + O(s^{3/2}\log s)\cdot \rand \\
&= 3\opt_i + O(s^{3/2}\log s)\cdot \rand.
\qedhere
\end{align*}

Taking expectation over all voters, this yields Theorem~\ref{thm:lp}.
