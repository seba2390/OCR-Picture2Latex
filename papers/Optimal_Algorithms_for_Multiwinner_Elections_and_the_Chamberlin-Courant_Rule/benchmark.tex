\section{Hardness of $1$-Borda and Optimal Deterministic Algorithm}
\label{sec:lower}
Throughout this section, we focus on $1$-Borda score. We justify our choice of benchmark $\rand = \frac{m + 1}{k + 1}$, and show that a deterministic algorithm, \b{}, achieves this benchmark optimally. First, notice that if the input consists of one voter for each possible preference of $m$ candidates (thus $n = m!$), picking any committee has the same $1$-Borda score by symmetry, so \opt{} is just \rand. Thus, we have the following proposition:
\begin{proposition}
For any $m$ and $k$, there exist instances where $\opt = \rand$.
\end{proposition}

\subsection{Hardness Result for $1$-Borda: Theorem~\ref{thm:main_lb}}
We now show Theorem~\ref{thm:hardness}: Even if $\opt$ is very small, it is computationally hard to significantly beat $\rand$. 
%The following theorem is a restatement of Theorem~\ref{thm:main_lb}:
%\begin{theorem}
%Unless $\np = \zpp$, no polynomial-time algorithm can distinguish between instances with $\opt \ge (1 - o(1)) \cdot \frac{m+1}{k+1}$ from those with either:
%\begin{enumerate}
%    \item $\opt \le \left(\frac{m+1}{k+1}\right)^{\delta}$, where $\delta \in (0,1)$ is any constant; or
%    \item $\opt \le \frac{1}{k^{\alpha}} \cdot \frac{m+1}{k+1}$, where $\alpha > 0$ is a constant.
%\end{enumerate}
%Unless $\zpp = \np$, for any constant $\varepsilon > 0$, there is no polynomial-time (in $m,n, k$) algorithm that distinguishes between instances with optimal $1$-Borda score $\opt \ge (1 - \varepsilon) \cdot \rand$ and those with $\opt < \varepsilon \cdot \rand$.
%\label{thm:hardness}
%\end{theorem}
To prove this hardness result, we show a reduction from the decision version of the \rc{} problem. 
\begin{definition}
In \rc{}, these is a universe $U$ of $n$ elements $\{a_1, a_2, \ldots, a_n\}$, and a family $\mathcal{F} = \{S_1, S_2, \ldots, S_z\}$ of subsets of $U$. Each $S_i$ has the same size $\frac{n}{k}$. The {\em value} of an instance is the maximum size of the union of $k$ sets from $\mathcal{F}$. For any constant $\varepsilon > 0$, we consider the following decision version:
\begin{itemize}
    \item ``YES'' instances are those with value $n$. Therefore, there exist $k$ disjoint sets each of size $n/k$ that cover all the elements.
    \item ``NO'' instances are those with value at most $\frac{3}{4}n$.
\end{itemize}
\label{def:k_cover}
\end{definition}

%There is a size-$k$ subfamily $\mathcal{G}$ of $\mathcal{F}$ whose union covers $U$:
%\[
%\exists \mathcal{G} \subseteq \mathcal{F}: |\mathcal{G}| = k \text{ and } U = \bigcup_{S \in \mathcal{G}} S.
%\]
%An algorithm is said to be approximating \rc{} within a factor of $c$ if it always outputs a size-$k$ subfamily of $\mathcal{F}$ whose union covers at least $c \cdot n$ elements.

%\km{I don't think the computational version of Feige's result is regular. The decision version definitely is, so I changed the theorem statement.} 

The above problem known to be $\nph$ to approximate via the following lemma that is implicit in the proof of Theorem 5.3 in~\cite{Feige}.

\begin{lemma}[\cite{Feige}]
The decision version of \rc{} from Definition~\ref{def:k_cover} is $\nph$, that is, unless $\p = \np$, there is no polynomial time algorithm that can decide always answers ``YES'' for ``YES'' instances and answers ``NO'' for ``NO'' instances.
%it is $\nph$ to distinguish between instances of \rc{} with value $n$ and those with value at most $(1-1/e + \varepsilon)n$.  %within a factor of $1 - \frac{1}{e} + \varepsilon$ is $\nph$. \kn{Let me check this again.}
\label{lem:cited_hardness}
\end{lemma}

Note that if the instance has value $n$, there exist $k$ disjoint sets each of size $n/k$ that cover all the elements. This aspect will be crucial in our reduction. Also needed in our reduction, we state the following lemma for constructing a profile with polynomial number of voters, where the best solution with score $\opt$ has similar performance as $\rand$.

\begin{lemma}
Fix any $\varepsilon > 0$ and let $n \ge \left\lceil\frac{m(k + 1)^2}{\varepsilon^2}\right\rceil$. Consider the instance where the preference of each voter is an independent and uniformly random permutation. Let $\opt'$ denote the expected value of the optimum score, and $\rand' = \frac{m+1}{k+1}$, then $\Pr[\opt \leq (1 - \varepsilon) \cdot \rand] < \frac{1}{2}$, where the probability is over the randomness in the permutations.
\label{lem:random_construction}
\end{lemma}
\begin{proof}
Fix any committee $T$ of size $k$. Notice that $\E[r_{\V}(T)] = \rand'$ since the preferences are uniformly random. We have
\begin{align*}
\Pr[r_{\V}(T) - \rand' \leq -\varepsilon \cdot \rand'] &= \Pr\left[\frac{1}{n}\sum_{v \in \V} r_v(T) - \rand' > \varepsilon \cdot \rand'\right]\\
& \leq \exp\left(\frac{-2n (\varepsilon \cdot \rand')^2}{m^2}\right) \leq \exp\left(\frac{-2n \varepsilon^2}{(k + 1)^2}\right),
\end{align*}
where the second step comes from Hoeffding's inequality. By union bound,
\begin{align*}
\Pr[\opt' \leq (1 - \varepsilon) \cdot \rand'] &\leq \binom{m}{k} \cdot \Pr[r_{\V}(T) - \rand' \leq -\varepsilon \cdot \rand']\\
&\leq \exp\left(\frac{-2n \varepsilon^2}{(k + 1)^2} + m\right)\leq e^{-m} < \frac{1}{2}. \qedhere
\end{align*}
\end{proof}

Now we are ready to prove Theorem~\ref{thm:hardness}.
\begin{proof}[Proof of Theorem~\ref{thm:hardness}]
Fix a $\varepsilon > 0$ and let $\varepsilon' = 10 \varepsilon$.  We will choose $\varepsilon$ appropriately later. Given any instance of \rc{} with $n$ elements and $z$ sets each of size $n/k$ (as in Definition~\ref{def:k_cover}), we construct the following instance for our problem:
\begin{itemize}
    \item There are $N = n R$ voters $v_{ij}$ where $i \in [n]$ and $j \in [R]$.  We have $m = \frac{2}{\varepsilon'} k z$ candidates. The first $z$ candidates $\{c_1, c_2, \ldots, c_z\}$ are ``critical'' candidates, and the other $m-z$ candidates are ``dummy'' candidates. Each voter corresponds to an element in the universe and each critical candidate corresponds to a set in \rc{}. 
    \item If a set $S_i$ covers $a_j$, then voters $v_{ij}$ for $j \in [R]$ rank $c_j$ within top $\varepsilon'$ fraction. Otherwise, $v_{ij}$'s rank $c_j$ within bottom $\varepsilon'$ fraction.
    \item Independently for each voter, fill the rest of her preferences with the $m - z$ dummy candidates uniformly randomly. 
    \item  The copies of a voter only differ in the ranking of the dummy candidates. We set the number of copies to be $R = \left\lceil \frac{10 m k^2}{n \varepsilon^2} \right\rceil$. These copies are there to ensure Lemma~\ref{lem:random_construction} applies to the dummy candidates.
\end{itemize}

Clearly, the above construction has size $\mbox{poly}(1/\varepsilon,n,z,k)$. Let $\opt$ denote the optimal score on this instance. Recall that $\rand = \frac{m+1}{k+1}$. First suppose the instance of \rc{} has value $n$ (``YES'' instance) so that there are $k$ sets that cover all $n$ elements, then it is easy to check that choosing the corresponding critical candidates as the committee yields $\opt \le z < \varepsilon' \cdot \rand$. 

On the other hand, suppose the instance of \rc{} is such that any collection of sets of size $k$ only covers at most $(1-1/e + \varepsilon)n \le \frac{3}{4} n$ elements (``NO'' instance). Consider any committee $T$ and suppose $T = R \cup D$ where $R$ is a subset of critical candidates and $D$ is a subset of dummy candidates. Let $r = |R|$ and $d = |D| = k - r$. Let $n'$ be the number of elements $R$ covers in the $\rc{}$ instance. By assumption, $n - n' \ge \frac{n}{4}$ since any collection $R$ covers at most $\frac{3}{4} n$ elements. Further, since the instance is regular, $n - n' \le \frac{n}{k} r$, so that $n - n' \ge d \frac{n}{k}$.

Using Lemma~\ref{lem:random_construction}, with probability $> \frac{1}{2}$ over the choice of the ranking of the dummy candidates, the optimal score of $D$ on the $(n-n')R$ uncovered voters using the $m-z$ dummy candidates is greater than $(1-\varepsilon) \frac{m - z}{d + 1}$. Inserting the critical candidates cannot decrease this score for these voters, since the candidates in $R$ appear last in their ordering. Further, we have assumed $m = \frac{2}{\varepsilon'} k z$. Therefore, with probability $> \frac{1}{2}$, we have:
    $$ \opt > \frac{n - n'}{n} (1-\varepsilon) \frac{m - z}{d + 1} \ge \frac{n - n'}{n} \cdot \left(1 - \frac{\varepsilon'}{4}\right) \cdot \frac{m + 1}{d + 1}$$

We now split the analysis into two cases:

\begin{enumerate}
    \item Suppose $d + 1 \le \frac{k+1}{4}$. Since $n - n' \ge \frac{n}{4}$, we have 
    \[
    \opt > \frac{1}{4} \cdot \left(1 - \frac{\varepsilon'}{4}\right) \cdot \frac{m + 1}{d + 1} \ge \left(1 - \frac{\varepsilon'}{4}\right) \cdot \frac{m + 1}{k + 1} \ge  (1-\varepsilon') \rand.
    \]
    \item Suppose $d + 1 \ge \frac{k+1}{4}$. Since $n - n' \ge \frac{d}{k} n$, and since $d,k = \omega(1)$, we have:
    \[
    \opt > \frac{d}{k} \cdot \left(1 - \frac{\varepsilon'}{4}\right) \cdot \frac{m + 1}{d+1} = \frac{d}{d+1}\frac{k+1}{k} \left(1 - \frac{\varepsilon'}{4}\right) \cdot \frac{m + 1}{k+1} \ge (1-\varepsilon') \rand.
    \]
\end{enumerate}

Therefore, our construction ensures that with probability $> \frac{1}{2}$, we have $\opt \ge (1-\varepsilon') \rand$ if the original \rc{} instance has value at most $\frac{3}{4} n$.

Now suppose there is a polynomial time algorithm that can distinguish between instances with $\opt \le \varepsilon' \rand$ and $\opt \ge (1-\varepsilon') \rand$. Then, feeding the output of the above construction to this algorithm implies a $\crp$ algorithm for the decision version of \rc, which by Theorem~\ref{lem:cited_hardness} implies $\np \subseteq \crp$. Since $\rp \subseteq \np$, this implies $\rp \subseteq \crp$, so that $\zpp = \rp \cap \crp = \rp$. Since $\zpp$ is symmetric with respect to ``YES'' and ``NO'' instances, this implies $\zpp = \crp$, so that $\zpp = \np$. 

We now show how to set $\varepsilon$. For the first part of the theorem, we set $\varepsilon = \frac{1}{10} \left( \frac{k}{m} \right)^{1-\delta}$. This can be achieved by choosing $m$ such that $\left(\frac{m}{k}\right)^{\delta} = 20 k z$. Note that this ensures $m = \mbox{poly}(k,z)$ when $\delta$ is a constant, so that the construction runs in polynomial time. For this setting, we have $\varepsilon' \cdot \frac{m+1}{k+1} \le \left( \frac{m+1}{k+1} \right)^{\delta}$, while  $\varepsilon' = \left( \frac{k}{m} \right)^{1-\delta} = \left( \frac{1}{20kz} \right)^{\frac{1-\delta}{\delta}} = o(1)$, completing the proof.

For the second part of the theorem, we set $m = 20 k^{1+\alpha} z$, and $\varepsilon = \frac{1}{10} \frac{1}{k^{\alpha}}$. Again, we have $m = \mbox{poly}(k,z)$, and $\varepsilon' = o(1)$, completing the proof.
\end{proof}

Theorem~\ref{thm:hardness} now implies the following easy corollaries.
\begin{corollary}
Unless $\np = \zpp$, there is no $k^{\alpha}$-approximation to the $1$-Borda score for any constant $\alpha > 0$. Similarly, there is no $\left( \frac{m+1}{k+1} \right)^{1-\delta}$-approximation for any constant $\delta \in (0,1)$.
\end{corollary}

The next corollary adapts the hardness proof to the maximization version of the problem.

\begin{corollary}
For the maximization version of $1$-Borda, there is no polynomial time $\left(1 - \frac{1 - \varepsilon}{k+1} \right)$-approximation for constant $\varepsilon \in (0,1/2)$ unless $\np = \zpp$.
\end{corollary}
\begin{proof}
Set $\varepsilon > 0$ to be a small constant in the proof of Theorem~\ref{thm:main_lb}. Then, in the ``NO'' instance, the maximization score is at most $(m+1) \left(1 - \frac{1 - \varepsilon}{k+1} \right)$, while for the ``YES'' instance, the score is at least $(m+1) \left( 1 - \frac{\varepsilon}{k+1} \right)$. For $\varepsilon \in (0,1/2)$, the approximation factor achievable is therefore at most $\left( 1 - \frac{1-2\varepsilon}{k+1} \right)$, completing the proof.
\end{proof}

%By assumption, $n - n' \ge \max\left(n - r \frac{n}{k}, \frac{n}{4} \right) = \max\left(d \frac{n}{k}, \frac{n}{4} \right)$. For the remaining voters, the candidates in $R$ appear last in their ranking. 



%We prove the following: Assume there is an algorithm $\alg$ which runs in expected polynomial time, and outputs a committee with $1$-Borda score at most $(1 - \varepsilon) \cdot \rand$ whenever $\opt < \varepsilon \cdot \rand$. We show there will be an algorithm approximating \rc{} within a factor of $\frac{3}{4}$, and thus $\np = \zpp$.


%Since the instance of \rc{} has a full cover of size $k$, we know $\opt < \varepsilon' \cdot \rand$. Therefore, \alg{} will output a committee $T$ with $1$-Borda score at most $(1 - \varepsilon) \cdot \rand$. Suppose $T = R \cup D$ where $R$ is a subset of critical candidates and $D$ is a subset of dummy candidates. Let $r = |R|$ and $d = |D|$. Let $n'$ be the number of elements $R$ covers in the $\rc{}$ instance.
%\begin{itemize}
%    \item If $n' \geq \frac{3}{4} \cdot n$, then report it as a $\frac{3}{4}$-approximation of \rc{}.
%    \item If $n' < \frac{3}{4} \cdot n$ if the lemma realizes to be true, $\alg \geq \frac{n - n'}{n} \cdot \left((1 - \varepsilon') \cdot \frac{m + 1}{d + 1} - \varepsilon'm\right)$. Cannot be the case.
%    \item If $n' < \frac{3}{4} \cdot n$ if the lemma realizes to be false, rerun.
%\end{itemize}
%\kn{unfinished, todo for me}

\subsection{An Optimal Deterministic Algorithm}
Given the lower bound and the hardness result, an immediate question is whether there is a deterministic rule to achieve the benchmark \rand{}. We answer in the affirmative: The \b{} algorithm~\cite{Banzhaf,dubeyS,Heuristics} can be viewed as a derandomization of \rand{}: Instead of randomly picking a candidate at each iteration, it picks the candidate that gives the best expected performance if the rest of the committee is randomly constructed. It is shown in~\cite{Heuristics} that this algorithm runs in polynomial time. The following theorem implies Theorem~\ref{thm:main_banzhaf}.

\begin{theorem}
$\b \leq \rand$.
\label{thm:banzhaf_better_than_rand}
\end{theorem}
\begin{proof}
Recall that \b{} builds sets $\varnothing = T_0 \subsetneq T_1 \subsetneq \cdots \subsetneq T_k$, where at step $j$, \b{} picks $c_j \in \C \setminus T_{j - 1}$ such that:
\begin{equation}
    \label{eq:b_opt}
c_j = \mbox{argmin}_{c \in \C \setminus T_{j - 1} } \ \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_{j - 1} \cup \{c\}}} r_{\V}(S).
\end{equation}
We now use induction to show that for any $j \in \{0, 1, \ldots, k\}$,
\[
\frac{1}{\binom{m - j}{k - j}} \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_j}} r_{\V}(S) \leq \rand,
\]
which is clearly true when $j = 0$, and gives the desired result $\b \leq \rand$ when $j = k$.

For the inductive step, assume it holds for some $j - 1$. Now in the $j^{\text{th}}$ iteration, we have the following inequalities that complete the proof. Here, the first step follows since \b{} picks $c_j$ in step $j$. The second step follows since \b{} solves Eq~(\ref{eq:b_opt}), so that the score from adding $c_j$ beats the average score of adding one of the $m-j+1$ candidates in $C \setminus T_{j-1}$.  The final equality follows since $\binom{m - j + 1}{k - j + 1} = \frac{m-j+1}{k-j+1} \binom{m - j}{k - j}$, and by observing that for any $S \supseteq T_{j-1}$, there are $k-j+1$ choices of $c \in S \setminus T_{j-1}$. 
\begin{align*}
\frac{1}{\binom{m - j}{k - j}} \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_j}} r_{\V}(S) &= \frac{1}{\binom{m - j}{k - j}} \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_{j - 1} \cup \{c_j\}}} r_{\V}(S)\\
\leq &\frac{1}{\binom{m - j}{k - j}} \cdot \left(\frac{1}{m-j+1} \sum_{c \in C \setminus T_{j - 1}} \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_{j - 1} \cup \{c\}}} r_{\V}(S) \right)\\
= &  \frac{1}{\binom{m - j}{k - j}} \cdot \frac{k - j + 1}{m - j + 1} \cdot \left(\frac{1}{k-j+1} \sum_{c \in C \setminus T_{j - 1}} \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_{j - 1} \cup \{c\}}} r_{\V}(S) \right)\\
= &\frac{1}{\binom{m - j + 1}{k - j + 1}} \sum_{\substack{S \subseteq \C: |S| = k \\ S \supseteq T_{j - 1}}} r_{\V}(S) \leq \rand. \qedhere
\end{align*}
\end{proof}

To complete the proof of Theorem~\ref{thm:main_banzhaf}, for the maximization objective, \b{} achieves a value at least $(m+1) \left(1 -  \frac{1}{k+1} \right)$. Since the maximum possible value is $m$, this implies a $\left(1 - \frac{1}{k+1}\right)$-approximation.
