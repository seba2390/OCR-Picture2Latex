\section{Expected Ranks in a Random Committee}
\label{app:folklore}
The following is a well-known proof of the statement that if we pick a random size-$k$ subset of $\C$, the $t^{\text{th}}$ smallest rank is $t \cdot \frac{m + 1}{k + 1}$ in expectation. Mark $m + 1$ points on a circle. Pick a subset of $k + 1$ points uniformly at random, and then choose one point $P$ of these $k + 1$ as the cut-off point uniformly at random. Starting from $P$ and going clockwise, mark the next point as the candidate with rank $1$, and the point after that as the candidate with rank $2$, and so on, until the last point which is marked as the candidate with rank $m$. The picked subset comprises $P$ and a uniformly random size-$k$ subset of $\C$. By symmetry, the expected clockwise distance going from the $t^{\text{th}}$-smallest ranked chosen candidate to the $(t + 1)^{\text{st}}$ is the same for every $t \in \{0, 1, \ldots, k\}$, if we view $P$ as simultaneously the $0^{\text{th}}$ and the $(k + 1)^{\text{st}}$ smallest. Since these $k + 1$ distances sum to $m + 1$, all of them should be $\frac{m + 1}{k + 1}$.

\section{Analysis of \g{} for $s$-Borda: Proof of Theorem~\ref{thm:s_borda}}
\label{app:sborda}
For simplicity, we define:
\[
X_k = r_{\V}(T_{k}),\qquad Y_k = \sum_{c \in \C \setminus T_k}\left(r_{\V}(T_k) - r_{\V}(T_k \cup \{c\})\right).
\]
%where $X_k$ is the $s$-Borda score after the $k^{\text{th}}$ iteration and $Y_k$ is the total potential decrease in score after the $k^{\text{th}}$ iteration.

Additionally, let $\rho_{v, k, j} $ denote the score $r_v(c_j(v))$ of the $j^{\text{th}}$-ranked candidate $c_j(v)$ for voter $v$ in the set $T_k$, and let $R_{k} = \frac{1}{n}\sum_{v \in \V} \rho_{v, k, s}$ denote the average score of these candidates across all voters. Note that by definition:
\begin{equation}
\label{eq:xk}
    X_k = r_{\V}(T_k) = \frac{1}{n} \sum_{v \in \V} \sum_{j=1}^s \rho_{v,k,j}
\end{equation}

We first present the following lemma, which is an analog to Lemma~\ref{lem:minimum_improvement}.

\begin{lemma}
\label{lem:sborda1}
For $k \geq s$, we have $X_k - X_{k + 1} \geq \frac{R_{k}^2 - (2s + 1)R_{k} + 2X_k}{2(m - k)}.$
\end{lemma}
\begin{proof}
We observe that:
\begin{align*}
Y_k + (sR_{k} - X_k) &= \sum_{c \in \C \setminus T_k} \left(r_{\V}(T_k) - r_{\V}(T_k \cup \{c\})\right) + \frac{1}{n}\sum_{v \in \V}\sum_{j = 1}^s (\rho_{v, k, s} - \rho_{v, k, j})\\
&= \frac{1}{n}\sum_{v \in \V}\sum_{i = 1}^{\rho_{v, k, s} - 1} i\\
&= \frac{\sum_{v \in V} \rho_{v, k, s}(\rho_{v, k, s} - 1)}{2n}.
\end{align*}
Here, the first equality follows from Eq~(\ref{eq:xk}). For the second equality, observe that any candidate $c \in \C \setminus T_k$ whose $r_v(c) < \rho_{v,k,s}$ contributes $\frac{1}{n} \left(\rho_{v,k,s} - r_v(c) \right)$ to the quantity $\left(r_{\V}(T_k) - r_{\V}(T_k \cup \{c\})\right)$. Therefore, the RHS of the first equality is summing, for each voter $v$, the quantity  $\frac{1}{n} \left(\rho_{v,k,s} - r_v(c) \right)$ over all $c \in \C$ whose $r_v(c) < \rho_{v,k,s}$. This yields the second equality by a change of variables.

By Cauchy-Schwarz inequality, we have:
\[
\frac{\sum_{v \in \V} \rho_{v, k, s}^2}{n} \geq \left(\frac{1}{n}\sum_{v\in \V} \rho_{v, k, s} \right)^2 = R_{k}^2.
\]
Therefore,
\[
Y_k - X_k + sR_{k} = \frac{\sum_{v \in V} \rho_{v, k, s}(\rho_{v, k, s} - 1)}{2n} \geq \frac{1}{2}R_{k}^2 - \frac{1}{2}R_{k},
\]
which is equivalent to $Y_k \geq \frac{1}{2}R_{k}^2 - \frac{2s + 1}{2}R_{k} + X_k.$

Since the candidate chosen by \g{} is at least as good as the average, we have:
\[
X_{k} - X_{k + 1} \geq \frac{Y_k}{m - k}\geq \frac{R_{k}^2 - (2s + 1)R_{k} + 2X_k}{2(m - k)}. \qedhere
\]
\end{proof}

We now present a simple relationship between $X_k$ and $R_{k}$.

\begin{lemma}
\label{lem:sborda2}
For $k \geq s$, $R_{k} \geq \frac{X_k}{s}.$
\end{lemma}
\begin{proof}
This inequality follows directly from the definition of $\rho_{v, k, s}$: since this is defined as the rank of the $s^{\text{th}}$-ranked candidate among the already-chosen ones for voter $v$, its contribution to the score must be greater than or equal to the average of the top $s$ among the chosen candidates for voter $v$. Taking sum over all voters gives this inequality.
\end{proof}

\paragraph{Completing the proof of Theorem~\ref{thm:s_borda}}
To complete the proof, we apply induction on $k$ to prove this theorem. We note that \g{} gives the optimal solution after $s$ iterations and therefore, the induction starts with $k = s$. Suppose the claim holds true for some $k - 1 \geq s$. We prove that this claim also holds true for $k$. By induction hypothesis, we have:
\[
X_{k - 1} \leq 2s^2\cdot \frac{m + 1}{k},
\]
and as in the proof for Theorem~\ref{thm:ub_greedy_1}, we only need to consider the following case:
\[
2s^2 \cdot \frac{m + 1}{k + 1} \leq X_{k - 1} \leq 2s^2 \cdot \frac{m + 1}{k}.
\]
Otherwise the induction clearly holds.

By Lemma~\ref{lem:sborda1}, we have:
\begin{align*}
X_k &\leq X_{k - 1} - \frac{R_{k - 1}^2 - (2s + 1)R_{k - 1} + 2X_{k - 1}}{2(m - k + 1)}\\ &= X_{k - 1} - \frac{X_{k - 1}}{m - k + 1} - \frac{1}{2(m - k + 1)}(R_{k - 1}^2 - (2s + 1)R_{k - 1}).
\end{align*}

Notice that the $R_{k - 1}^2 - (2s + 1)R_{k - 1}$ is a quadratic function in $R_{k - 1}$, which is monotonically increasing for $R_{k - 1} \geq \frac{2s + 1}{2}$. Since $R_{k - 1} \geq \frac{X_{k - 1}}{s} \geq 2s\cdot \frac{m + 1}{k + 1} > \frac{2s + 1}{2}$, we know $R_{k - 1}^2 - (2s + 1)R_{k - 1}$ is at least its value when $R_{k - 1} = \frac{X_{k - 1}}{s}$. Thus, we have:
\begin{align*}
X_k &\leq X_{k - 1} - \frac{X_{k - 1}}{m - k + 1} - \frac{1}{2(m - k + 1)}\left(\left(\frac{X_{k - 1}}{s}\right)^2 - (2s + 1)\frac{X_{k - 1}}{s}\right) \\
&= -\frac{1}{2(m - k + 1)s^2}X_{k - 1}^2 + \left(1 + \frac{1}{2(m - k + 1)s}\right)X_{k - 1}\\
&=-\frac{1}{2(m + 1)s^2}X_{k - 1}^2 + X_{k - 1} - \left(\frac{kX_{k - 1}}{2(m - k + 1)(m + 1)s^2} - \frac{1}{2(m - k + 1)s}\right)X_{k - 1}\\
&\leq -\frac{1}{2(m + 1)s^2}X_{k - 1}^2 + X_{k - 1}.
\end{align*}

Similar to the proof of Theorem~\ref{thm:ub_greedy_1}, notice that the right hand side is quadratic in $X_{k - 1}$ and thus monotonically increasing for $X_{k - 1} \leq (m + 1)s^2$. Since $X_{k - 1} \leq 2s^2\cdot \frac{m + 1}{k} \leq (m + 1)s^2$, the right hand side reaches its maximum at $2s^2\cdot \frac{m + 1}{k}$. Therefore,
\[
X_k \leq -\frac{1}{2(m + 1)s^2}\left(2s^2\cdot \frac{m + 1}{k}\right)^2 + 2s^2\cdot \frac{m + 1}{k} \leq 2s^2\cdot \frac{m + 1}{k + 1},
\]
concluding our induction.


