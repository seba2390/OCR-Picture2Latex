\section{The $s$-Borda Score}
In this section, we extend our analysis of the greedy algorithms to $s$-Borda score, and show how to significantly improve on the \g{} and \b{} rules via linear programming. 

Recall that $\rand = \frac{m+1}{k+1}$ and choosing a random committee of size $k$ yields expected score $\frac{s(s + 1)}{2} \cdot \rand$. As a derandomization, \b{} has score at most this value similar to Theorem~\ref{thm:banzhaf_better_than_rand}. Let $\opt$ denote the best possible $s$-Borda score. Considering the instance with one voter for each permutation of candidates as its preference ordering, we have the following proposition:

\begin{proposition}
For any $s$, $m$ and $k$, there exists instances where $\opt =\frac{s(s + 1)}{2} \cdot \rand$.
\end{proposition}

We first consider a natural extension of \g{} in the $1$-Borda case. In Appendix~\ref{app:sborda}, we show that it achieves an $s$-Borda score at most $2s^2\cdot \rand$ (Theorem~\ref{thm:s_borda}), which is within a factor of $\frac{4s}{s + 1}$ of the \b{} rule. We then show that this bound cannot be improved even when $\opt$ is small. However, unlike the $1$-Borda case, there is no fundamental barrier to an improved algorithm when $\opt$ is small, and we present such an algorithm in Section~\ref{sec:lp}.

\subsection{The \g{} Algorithm} 
\label{sec:sborda_greedy}
\label{sec:sborda_g_ub}
\label{sec:sborda_g_lb}
The \g{} algorithm follows exactly the same procedure as for $1$-Borda, except that we now compute the score based on $s$-Borda. We present an upper bound for \g{} in the following theorem. Since the proof is very similar to the $s=1$ case, we present it in Appendix~\ref{app:sborda}.

\begin{theorem}[Proved in Appendix~\ref{app:sborda}]
\label{thm:s_borda}
$\g{} \leq 2s^2\cdot \rand$.
\end{theorem}

\paragraph{Lower Bound for Small $\opt$.} 
In general, $\opt = \Omega(s^2) \cdot \rand$, in which case the analysis of greedy is tight to within a constant factor. The question we now ask is: Does \g{} always perform better when $\opt$ is small? We answer this in the negative.

\begin{theorem}
\label{thm:sbordalb}
There exists an instance where $\opt = O(s^2) = o(1) \cdot \rand$, while the score of \g{} is $\Omega(s^2)\cdot \rand$.
\end{theorem}
To prove this lower bound, we use the following instance.
%We now present an example in which \opt{} = $O(s^2) = o(1) \cdot \rand$, while the score of \g{} is still $\Omega(s^2)\cdot \rand$. %This example points to such general cases in which \g{} performs badly while \opt{} is small. We now present an example in which the $s$-Borda score of \opt{} is $O(s^2) = o(1)\cdot \rand$, while the score of \g{} is $\Omega(s^2)\cdot \rand$. 
%This shows that \g{} can perform as bad as random even when \opt{} is small and thus motivates the improved guarantee in Section~\ref{sec:lp}.
\begin{example}
%Similar as before, we consider this example through an election graph. 
Let $m = \omega(k)$, $k = \omega(s)$, and $n = \frac{k}{s}(m - k)!$. There are $k$ ``critical'' candidates $c_1, c_2, \ldots, c_k$, while the remaining $m - k$ are ``dummy'' candidates. Candidate $c_{i(k/s) + j}$ is the $(i + 1)^{\text{st}}$ choice of the $j^{\text{th}}$ $\frac{k}{s}$ voters, $\forall i \in \{0, 1, \ldots, s - 1\}, j \in \{1, 2, \ldots, \frac{k}{s}\}$. Aside from the first $s$ rows, the critical candidates lie at the very bottom. For each group of $\frac{k}{s}$ voters, we fill the rest of the preferences with all permutations of the dummy candidates. This example is illustrated in Figure~\ref{figure:greedy_bad}. %We can make the number of voters $\mbox{poly}(m)$ by sampling permutations; see remark at the end of Section~\ref{sec:greedy_lb}.
\label{example:greedy_bad}
\end{example}

\input{greedy_bad.tex}

In this instance, \opt{} is clearly $O(s^2)$ by choosing all the critical candidates. We now show that \g{} achieves its worst-case bound even on this instance.

\begin{proposition}
In Example~\ref{example:greedy_bad}, $\g{} = \Omega(s^2)\cdot \rand$.
\end{proposition}
\begin{proof}
For the first $s$ iterations, \g{} chooses dummy candidates: as $m \gg k$, choosing a critical candidate adds $\frac{s - 1}{s}(m + 1)$ to the score, while choosing a dummy candidate adds only $\frac{1}{2}(m + 1)$.

Then, we show that, for the first $\frac{k}{2}$ iterations, \g{} chooses dummy candidates. Assume at $(j - 1)^{\text{th}}$ iteration, where $s \leq j - 1 < \frac{k}{2}$, we have chosen $j - 1$ dummy candidates, and the set of candidates is $T_{j - 1}$. Then, we have:
\[
r_{\V}(T_{j - 1}) - r_{\V}(T_{j - 1} \cup \{c_\mathrm{critical}\}) \leq \frac{s}{k}\cdot \frac{s}{j}(m + 1) = \frac{s^2}{kj}(m + 1),
\]
and
\[
r_{\V}(T_{j - 1}) - r_{\V}(T_{j - 1} \cup \{c_\mathrm{dummy}\}) = \frac{s(s + 1)}{2j}(m + 1) - \frac{s(s + 1)}{2(j + 1)}(m + 1) = \frac{s(s + 1)}{2j(j + 1)}(m + 1),
\]
where $c_\mathrm{critical}$ is some critical candidate and $c_\mathrm{dummy}$ is some dummy candidate. This is because if we choose a critical candidate, then for $\frac{s}{k}$ fraction of the voters, the bottom-ranked dummy candidate will be dropped, while the critical candidate will be added. Since we have chosen $j - 1$ dummy candidates, the bottom-ranked dummy candidate has average rank $\frac{s}{j}(m + 1)$. In other words, for $\frac{s}{k}$ fraction of the voters, we drop a candidate at rank $\frac{s}{j}(m + 1)$ and gain a candidate at the top, while for the other voters, the top $s$ candidates remain unchanged. If we choose a dummy candidate instead, the average score goes from $\frac{s(s + 1)}{2j}(m + 1)$ to $\frac{s(s + 1)}{2(j + 1)}(m + 1)$.

For $j \leq \frac{k}{2}$, we have:
$$\frac{s(s + 1)}{2j(j + 1)}(m + 1) > \frac{s^2}{kj}(m + 1),$$
and thus \g{} chooses a dummy candidate in the $j^{\text{th}}$ iteration as well. Thus, by inductive principle, \g{} chooses dummy candidates for at least $\frac{k}{2}$ iterations.

However, this implies that we can choose at most $\frac{k}{2}$ critical candidates. Suppose for the $i^{\text{th}}$ $\frac{s}{k}$ voters, there are $x_i$ critical candidates among the top $s$ candidates. We have:
$$\sum_{i = 1}^{k/s}x_i \leq \frac{k}{2}.$$

Let $T_k$ denote the final set of candidates. As we choose at most $\frac{k}{2}$ critical candidates, at least $\frac{k}{2}$ candidates must be chosen, and we derive a lower bound for $r_\V(T_k)$ based on this. We have:
\begin{align*}
r_{\V}(T_k) &\geq \frac{s}{k}\left(\sum_{i = 1}^{k/s}\sum_{j = 1}^{s - x_i}j\right)\cdot \rand \geq \frac{s}{2k}\left(\sum_{i = 1}^{k/s}(s - x_i)^2\right) \cdot \rand \\
& \geq \frac{s}{2k} \frac{\left(\sum_{i = 1}^{k/s}(s - x_i)\right)^2}{k/s}\cdot \rand \geq \frac{s^2}{8}\cdot \rand.
\end{align*}

Recall that given $n$ voters whose preference structures include all permutations of the $m$ candidates, when we choose $k$ candidates out of them, the average contribution of the $i^{\text{th}}$-ranked candidates for each voter to $r_\V(T_k)$ is $i\cdot \rand$. The first inequality is by applying the above fact on each set of $\frac{s}{k}$ voters whose preference structures include all permutations. The third inequality is by Cauchy-Schwarz inequality. The last inequality is because $\sum_{i = 1}^{k/s}x_i \leq \frac{k}{2}$. Thus, we can conclude that $r_{\V}(T_k) = \Omega(s^2)\cdot \rand$.
\end{proof}


This shows that \g{} can perform as bad as random even when \opt{} is small and thus motivates the improved guarantee in Section~\ref{sec:lp}.

