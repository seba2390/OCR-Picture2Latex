\section{Analysis of \g{} for $1$-Borda}
\label{sec:greedy}
In this section, we analyze the performance of \g{}, evaluated with respect to the benchmark \rand{}. Throughout this section, we only consider the $1$-Borda score, i.e., $s = 1$. We first show an upper bound that $\g \leq 2 \cdot \rand$, and then present an almost-matching lower-bound instance where $\g > 1.962 \cdot \rand$.

\subsection{Upper Bound}
\label{sec:greedy_ub}
Now we show $\g \leq 2 \cdot \rand$ as an upper bound. We first present the following lemma, which gives a lower bound on the improvement at each iteration.
\begin{lemma}
Let $T_t$ and $T_{t + 1}$ be the set of candidates produced by \g{} in the $t^{\text{th}}$ and $(t + 1)^{\text{st}}$ iterations, and $r_{\V}(T_t)$, $r_{\V}(T_{t + 1})$ be their respective score. We have:
\[
r_{\V}(T_t) - r_{\V}(T_{t + 1}) \geq \frac{\sum_{v \in \V} r_v(T_t)(r_v(T_t) - 1)}{2n(m - t)}.
\]
\label{lem:minimum_improvement}
\end{lemma}
\begin{proof}
For a candidate $c \notin T_t$, define $\Delta_c := r_{\V}(T_t) - r_{\V}(T_t \cup \{c\})$, i.e., the current marginal contribution of $c$ to the $1$-Borda score. Taking the sum of $\Delta_c$ over $c \notin T_t$:
\[
\sum_{c \in \C \setminus T_t} \Delta_c = \frac{1}{n} \sum_{v \in \V} \sum_{j = 1}^{r_v(T_t) - 1} j = \frac{\sum_{v \in \V} r_v(T_t)(r_v(T_t) - 1)}{2n}.
\]

\g{} chooses $c^* = \argmax_c \Delta_c$ at the $(t + 1)^{\text{st}}$ iteration, giving us
\[
r_{\V}(T_t) - r_{\V}(T_{t + 1}) = \Delta_{c^*} \geq \frac{1}{m - t} \sum_{c \in \C \setminus T_t} \Delta_c = \frac{\sum_{v \in \V} r_v(T_t)(r_v(T_t) - 1)}{2n(m - t)}. \qedhere
\]
\end{proof}

Now we prove our upper bound of $2$.
\begin{theorem}
$\g \leq 2 \cdot \rand$.
\label{thm:ub_greedy_1}
\end{theorem}
\begin{proof}
We prove by induction. As the base case where $k = 1$, $\g \leq m < m + 1 =  2 \cdot \rand$.
Now suppose that the claim holds for some $k - 1$ and we will prove that it also holds for $k$. By induction hypothesis, we have:
\[
r_{\V}(T_{k - 1}) \leq 2 \cdot \frac{m + 1}{k}.
\]

If $r_{\V}(T_{k - 1}) \leq 2 \cdot \frac{m + 1}{k + 1}$, then $r_{\V}(T_k) \leq r_{\V}(T_{k - 1}) \leq 2 \cdot \frac{m + 1}{k + 1}$ finishes the proof. Thus, we only need to consider the following case:
\[
2 \cdot \frac{m + 1}{k + 1} < r_{\V}(T_{k - 1}) \leq 2 \cdot \frac{m + 1}{k}.
\]

We now have the following, where the first inequality is by Lemma~\ref{lem:minimum_improvement} and second by Cauchy-Schwarz inequality:
\begin{align*}
r_{\V}(T_{k - 1}) - r_{\V}(T_k) &\geq \frac{\sum_{v \in \V} r_v(T_{k - 1})(r_v(T_{k - 1}) - 1)}{2n(m - k + 1)} \\
&\geq \frac{\frac{1}{n}(\sum_{v \in \V}r_v(T_{k - 1}))^2 - \sum_{v \in \V}r_v(T_{k - 1})}{2n(m - k + 1)} \\
&= \frac{(\sum_{v \in \V}r_v(T_{k - 1}))^2}{2n^2(m + 1)} \cdot \frac{m + 1}{m - k + 1} \cdot \frac{\sum_{v \in \V}(r_v(T_{k - 1}) - 1)}{\sum_{v \in \V}r_v(T_{k - 1})}.
\end{align*}

Since $r_{\V}(T_{k - 1}) \geq 2 \cdot \frac{m + 1}{k + 1}$ by assumption, we have:
\begin{align*}
\frac{m + 1}{m - k + 1} \cdot \frac{\sum_{v \in \V}(r_v(T_{k - 1}) - 1)}{\sum_{v \in \V}r_v(T_{k - 1})} &\geq \frac{m + 1}{m - k + 1} \cdot \frac{2 \cdot \frac{m + 1}{k + 1} - 1}{2 \cdot \frac{m + 1}{k + 1}}\\
&= \frac{2(m + 1) - k - 1}{2(m + 1) - 2k} \geq 1.
\end{align*}
Combining the previous two inequalities, we therefore have:
\[
r_{\V}(T_{k - 1}) - r_{\V}(T_k) \geq \frac{(\sum_{v \in \V}r_v(T_{k - 1}))^2}{2n^2(m + 1)} = \frac{r_{\V}^2(T_{k - 1})}{2(m + 1)},
\]
which is equivalent to:
\[
r_{\V}(T_k) \leq - \frac{1}{2(m + 1)} r_{\V}^2(T_{k - 1}) + r_{\V}(T_{k - 1}).
\]

Notice that the right hand side is a quadratic function in $r_{\V}(T_{k - 1})$, which is monotonically increasing for $r_{\V}(T_{k - 1}) \leq m + 1$. Since $r_{\V}(T_{k - 1}) \leq 2 \cdot \frac{m + 1}{k} \leq m + 1$, the right hand side reaches its maximum at $2 \cdot \frac{m + 1}{k}$. Thus, we have:
\[
r_{\V}(T_k) \leq - \frac{1}{2(m + 1)}\cdot \left(\frac{2(m + 1)}{k}\right)^2 + \frac{2(m + 1)}{k} \leq \frac{2(m + 1)}{k + 1},
\]
which concludes our induction.
\end{proof}

\paragraph{Proof of Theorem~\ref{thm:greedymax}.} For the maximization version, the above result implies \g{} achieves score at least $(m+1) \cdot \left(1- \frac{2}{k+1} \right)$. Since the maximum possible score is $m$, this implies that \g{} is a $\left( 1 - \frac{2}{k+1} \right)$-approximation.

\subsection{Lower Bound}
\label{sec:greedy_lb}
Now we complement our result with a lower-bound example for \g.

\begin{theorem}
\label{thm:greedy_lb}
There exists an instance in which $r_{\V}(T_k) > 1.962\cdot \rand$.
\end{theorem}
\input{greedy_lb}

\iffalse

\paragraph{Construction.} In the sequel, we will prove the above theorem. In the instance we construct, $m$, $n$, and $k$ are all sufficiently large. For convenience of illustration, we scale down the ranks by a factor of $m$: now the ranks are $\frac{1}{m}, \frac{2}{m}, \ldots, \frac{m - 1}{m}, 1$. As $m \to \infty$, $\frac{1}{m} \to 0$, so the set of ranking $\{\frac{1}{m}, \frac{2}{m}, \ldots, 1\}$ will become dense in $[0, 1]$, and thus we regard the ranking as being continuous from $0$ to $1$. Our goal becomes to construct an instance in which \g{} gives $r_{\V}(T_k) > 1.962 \cdot \frac{1}{k + 1}$.

Fix an integer $N$, and take a small-enough constant $a \in (0, 1)$, and define $f(t) = a\varphi^{\frac{t}{N}}$, where $\varphi$ denotes the golden ratio $\frac{\sqrt{5} - 1}{2} \approx 0.618$. 

We divide the set of candidates into two types -- {\em critical} and {\em dummy}. The former set has size $k$, and the latter has size $m-k$. Our proof will show that \g{} will choose the critical candidates in a fixed order, and will not choose dummy candidates. 

The critical candidates are present in $\ell$ ``layers'' as shown in the red spiral in Fig.~\ref{fig:cylinder}, where $\ell$ is sufficiently large. This figure shows the ranks of the critical candidates in the voters' profiles. In layer $i$, there are $k_i$ candidates each appearing $n/k_i$ times. We will compute $k_i$ below. Let $S_i$ denote the set of $k_i$ critical candidates in layer $i$. Let $Q$ denote the set of all permutations of the $(m-k)$ dummy candidates. The set of voters is $[N] \times S_1 \times \cdots \times S_{\ell} \times Q$. For voter $\langle t, c_1, c_2, \ldots, c_{\ell}, \pi \rangle$, the critical candidate in layer $i$ is $c_i \in S_i$, and its rank is $f(t + (i-1)N)$. The remaining candidates from $S_1 \cup \cdots \cup S_{\ell}$ have the lowest possible rank $1$ for this voter. The dummy candidates appear in the ranking in the remaining slots according to the permutation $\pi$.

In this instance, $n = N (m-k)! \prod_{i=1}^{\ell} k_i$, and $k = \sum_{i=1}^{\ell} k_i$. (See Remark at the end of the proof for how to make the construction polynomial size in $m$.) For analytic convenience, take the limit $N \rightarrow \infty$ and assume $[N]$ is a continuum $[0,1]$. Therefore, for a voter with first coordinate $x \in [0,1]$, a candidate in layer $i$ appears with rank $g_i(x) = a\varphi^{x + (i-1)}$. Note that in this instance, the candidates in each layer are symmetric from the perspective of \g{}, and so are the dummy candidates.


%\input{cylinder}

%\paragraph{Continuous Interpretation.} The above construction has the following continuous interpretation as a distribution over ranks. 

%\begin{itemize}
%    \item  Assume the voters lie on a continuum in $[0,1]$ and so do the ranks, with $0$ being the lowest rank. As mentioned above, this will be a good approximation for large $m,n$.
%    \item The rank of voter $x \in [0,1]$ for its critical candidate in layer $i \in \{1,2, \ldots, \ell\}$ is $g_i(x) = a\varphi^{x - (i-1)}$, where $a$ is a small constant. 
%    \item Independently of other layers, for layer $i$ and for each voter $x \in [0,1]$, one of the $k_i$ critical candidates chosen uniformly at random appears at rank $g_i(x)$, and the remaining $k_i-1$ critical candidates appear at rank $1$.
%    \item Independently of the critical candidates and other voters, for each voter, the order of dummy candidates is a uniformly random permutation over them. 
%\end{itemize}

%This yields a distribution over ranks over the set of voters. We obtain the discrete version by treating this distribution as a collection of samples, and making the final voter set the the union of the sets of voters, one set from each sample.  %As $\gamma \rightarrow \infty$, the behavior of \g{} on this discrete version will approximate its behavior on the continuous version. In the sequel, we analyze the continuous version.

%To interpret this continuous version, if a candidate appears with mass $y$ at a particular rank for a voter, in the underlying instance, there are copies of this voter and this candidate appears at this rank for a $y$ fraction of those voters. Therefore, if a set of candidates appears at this rank with total mass of $1$, we split these candidates in the appropriate fractions among  the underlying voter copies. 


\paragraph{Computing the $k_i$.} We first assume that \g{} chooses critical candidates in increasing order of layers. We will justify this assumption later. Recall $k_i$ is the number of candidates in the $i^{\text{th}}$ layer. Let $K_i = \sum_{j \le i} k_j$ be the total number of chosen candidates after we have chosen the $i^{\text{th}}$ layer. Let $T_{K_i}$ denote the set of chosen candidates after we have chosen the $i^{\text{th}}$ layer. We will now compute $k_i$ in order to make \g{} prefer critical to dummy candidates.

%In the analysis below, we will take the limit as $m \rightarrow \infty$ and divide the ranks throughout by $m$. Therefore, we wish to show that \g{} achieves $r_{\V}(T_k) > 1.962 \cdot \frac{1}{k + 1}$.

To simplify notation, denote $X = \int_{x = 0}^1 a\varphi^x\d x$ and $Y = \int_{x = 0}^1 a^2\varphi^{2x}\d x$. Computing these explicitly:
$$X = \frac{a}{\ln\varphi}(\varphi - 1), \qquad Y = \frac{a^2}{2\ln\varphi}(\varphi^2 - 1) = X^2 \frac{(\varphi + 1)\ln\varphi}{2(\varphi - 1)}.$$

Using this notation, the score of candidate set $T_{K_i}$ is given by
\begin{equation}
    \label{eq:improve}
    \sum_{v \in \V}r_v(T_{K_i})= \int_{x = 0}^1 g_i(x) \d x = \varphi^{i - 1}\int_{x = 0}^1 a\varphi^x\d x = \varphi^{i - 1} X.
    \end{equation}

The decrease in score had all the candidates in layer $i$ were chosen is therefore:
$$\sum_{c \in i^{\text{th}} \text{ layer}} [r_{\V}(T_{K_{i - 1}}) - r_{\V}(T_{K_{i - 1}}\cup \{c\})]= r_{\V}(T_{K_{i - 1}}) - r_{\V}(T_{K_i}) = (\varphi^{i - 2} - \varphi^{i - 1})X.$$

The candidates in $T_{K_i}$ are symmetric in that each of these gives the same decrease in score regardless of which other candidates in this layer have been chosen. Therefore, each critical candidate in layer $i$ gives a decrease of exactly $(\varphi^{i - 2} - \varphi^{i - 1}) \frac{X}{k_i}$.

Now consider the dummy candidates. Just after \g{} chooses $T_{K_{i-1}}$, each such candidate, if it improves the rank, appears at average rank $g_{i-1}(x)/2$ on a fraction $g_{i-1}(x)$ of voters. This upper bounds the decrease even after some candidates in $T_{K_i}$ are chosen. Therefore, the decrease in score due to a dummy candidate while \g{} is considering layer $i$ is at most: \kn{$\sum_{v \in \V}\frac{r_v^2(T_{K_{i-1}})}{2} = \int_{x = 0}^1 g_{i - 1}^2(x) \d x = \frac{\varphi^{2i-4}}{2}\int_{x = 0}^1 a^2\varphi^{2x} \d x$ ?}
\[
\sum_{v \in \V}\frac{r_v^2(T_{K_{i-1}})}{2} = \frac{\varphi^{2i-2}}{2}\int_{x = 0}^1 a^2\varphi^{2x} \d x = \frac{\varphi^{2i - 2}}{2} Y.
\]

Therefore, for \g{} to choose a critical candidate at layer $i$ over any dummy candidate, we need to have
\[
\frac{\varphi^{2i - 2}}{2} Y \le (\varphi^{i - 2} - \varphi^{i - 1}) \frac{X}{k_i}.
\]
Setting this to equality, and assuming \g{} breaks ties to favor critical candidates, this yields 
\begin{equation} \label{eq:ki}
k_i = \frac{(\varphi^{i - 2} - \varphi^{i - 1})X}{\frac{\varphi^{2i - 2}}{2}Y} = \frac{-4(\varphi - 1)^2}{X(\varphi + 1)\ln\varphi} \cdot \frac{1}{\varphi^i},
\end{equation}
and it follows that:
\begin{equation}\label{eq:k}
k = \sum_{i = 1}^\ell k_i = \frac{-4(\varphi - 1)^2}{X(\varphi + 1)\ln\varphi} \sum_{i = 1}^\ell \frac{1}{\varphi^i} = \frac{4(\varphi - 1)}{X(\varphi + 1)\ln\varphi} \left(\frac{1}{\varphi^{\ell - 1}} - \varphi \right).
\end{equation}
We assume $a$ is small enough that rounding $k_i$'s to the nearest integer does not change the analysis.

\paragraph{Order of Choosing Candidates in \g{}.} We now show that \g{} chooses the critical candidates in increasing order of layers. 

\begin{lemma}
\label{lem:greedyopt2}
\g{}  chooses the critical candidates from the first layer to the $\ell^{\text{th}}$ layer in order, and will not skip a layer in the process.
\end{lemma}
\begin{proof}
First, we notice that, when $a$ is small enough, \g{} will start with choosing the candidates in the first layer. Now, suppose that \g{} has finished choosing candidates in the $i^{\text{th}}$ layer, we show that in the next step, it will choose candidates in the $(i + 1)^{\text{st}}$ layer instead of candidates in the $(i + 2)^{\text{nd}}$ layer, and using the same argument, it will not choose candidates in layers $(i + 3)$, $(i + 4)$, etc, before layer $(i + 1)$.

Recall that we use $k_i$ to denote the number of candidates in the $i^{\text{th}}$ layer,  $K_i$ to denote the total number of chosen candidates after we have chosen the $i^{\text{th}}$ layer, $T_{K_i}$ to denote the corresponding set of chosen candidates. Let $z = \frac{Y}{2} = \frac{X^2(\varphi + 1)\ln\varphi}{4(\varphi - 1)}$. 

Assume \g{} has chosen $T_{K_i}$. By Eq~(\ref{eq:improve}) and~(\ref{eq:ki}), for any candidate in the $(i + 1)^{\text{st}}$ layer, the decrease in score after choosing it is exactly:
$$ \frac{r_{\V}(T_{K_{i}}) - r_{\V}(T_{K_{i+1}})}{k_{i+1}} = \frac{\varphi^{i - 1} - \varphi^i}{k_{i+1}} X = z\varphi^{2i}.$$

Similarly, if we choose a candidate in the $(i + 2)^{\text{nd}}$ layer instead, the worst-case decrease will be given by assuming \g{} has not chosen any candidate from $T_{K_i}$, so this decrease is at most:
$$ \frac{r_{\V}(T_{K_{i}}) - r_{\V}(T_{K_{i+2}})}{k_{i+2}} = \frac{\varphi^{i - 1} - \varphi^{i+1}}{k_{i+2}} X = z \left(\varphi^{2i+1} +  \varphi^{2i+2} \right).$$

Since $\varphi$ is the golden ratio, we have $\varphi^2 + \varphi = 1$, and thus we have:
$$z\varphi^{2i+1} + z\varphi^{2i+2} = z\varphi^{2i},$$
which implies that after \g{} has chosen the $i^{\text{th}}$ layer, choosing a candidate in the $(i + 1)^{\text{st}}$ layer provides the optimal decrease in score provided we break ties in its favor. This shows that \g{}  will choose candidates from the first layer to the $\ell^{\text{th}}$ layer in order.
\end{proof}

\paragraph{The Lower Bound.} So far we have shown that \g{} chooses critical candidates in increasing order of layers and does not choose dummy candidates. We finally put it all together and show the following bound, which completes the proof of Theorem~\ref{thm:greedy_lb}. 

\begin{lemma}
\label{lem:greedyopt1}
For $\ell$ sufficiently large, $r_{\V}(T_k) > 1.962 \cdot \frac{1}{k + 1}$.
\end{lemma}
\begin{proof}
Using Eq~(\ref{eq:improve}) and Eq~(\ref{eq:k}), we now have:
$$r_{\V}(T_k) \cdot(k + 1) > \varphi^{\ell - 1}X \cdot \frac{4(\varphi - 1)}{X(\varphi + 1)\ln\varphi}\left(\frac{1}{\varphi^{\ell - 1}} - \varphi\right) = \frac{4(\varphi - 1)}{(\varphi + 1)\ln\varphi}(1 - \varphi^{\ell}) > 1.962,$$
i.e., $r_{\V}(T_k) > 1.962 \cdot \frac{1}{k + 1}$.
\end{proof}


\paragraph{Remark} The current construction has $n$ which is exponential in $m$. However, using the same proof as Lemma~\ref{lem:random_construction}, if we sample $\mbox{poly}(m,1/\varepsilon)$ voters, the improvement generated by each candidate is approximated to within a factor of $(1+\varepsilon)$ at all steps of \g{} with high probability. This is sufficient to make \g{}  choose critical candidates in the correct order. Note that analysis of \g{} remains the same even for finite $N = \mbox{poly}(m)$, and the limiting case is just to simplify the analysis. This makes the overall construction polynomial size in the number of candidates $m$ and we omit the straightforward details.

%\begin{figure}[!htb]
%\centering
%\includegraphics[width = 0.8\textwidth]{election.jpg}
%\caption{$n!$ Copies of the Election Graph}
%\end{figure}

\fi
