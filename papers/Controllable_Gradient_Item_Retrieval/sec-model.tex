%!TEX root = main.tex
\section{Proposed CGIR Method}
\label{sec:method}
In this section, we first formally define the notation and the gradient item retrieval problem. Then we introduce the proposed framework, followed by discussions about how the proposed method can learn the disentangled item representations with semantic meanings. After that, we show that our weakly-supervised method can achieve disentangled representation with consistency and restrictiveness theoretically.


\begin{figure*}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/model.png}
\caption{Overview of our proposed \CGIR~ framework. It includes three major parts -- the left is for disentangled item representation, the right part aims at enforcing representation of attributes to be sparse, and the middle part is for aligning the disentangled item representation space and the sparse word representation space. They are trained in an end-to-end manner.}
\label{fig:model}
\end{figure*}

\subsection{Notation and Problem Formulation}
\textbf{Notation} In this problem, we are provided with a set of users $\mathcal{U}$, a set of items $\mathcal{I}$, a set of attribute strings $\mathcal{T}$, interaction data $\mathbf{X}$ between users and items, and item-attribute relation data $\mathbf{A}$ between attributes and items. Specifically, the interaction data $\mathbf{X}$ consists of the interactions between $N$ users and $M$ items. An interaction between user $u$ and item $i$ is denoted by $x_{u,i} \in \{0,1\}$, where $x_{u,i}=1$ indicates that user $u$ adopts item $i$, whereas $x_{u,i}=0$ means there is no recorded interaction between them. For convenience, we use $\mathbf{x}_{u,:}$ to represent the items adopted by user $u$ and $\mathbf{x}_{:,i}$ to denote the users who interacted with item $i$. The item-attribute relation data $\mathbf{A}$ consists of relations between $M$ items and $T$ attributes, $T = |\mathcal{T}|$. If item $i$ has attribute $t$, then $a_{i,t} = 1$, otherwise $a_{i,t} = 0$. The attribute vector of item $i$ is denoted as $\mathbf{a}_{i,:}$. Besides, the attribute difference data $\mathbf{Y}$ is composed of attribute difference vector $\mathbf{y}_{i,i'} = \mathbf{a}_{i,:} - \mathbf{a}_{i',:}$ , $\mathbf{y}_{i,i'} \in R^{T}$. Each element of the difference vector $y_{i,i'}^t \in \{-1,0,1\}$ indicates the difference between item $i$ and $i'$ on a certain attribute $t$. Triple data $\mathcal{D}$ is constructed using previously mentioned data and it is composed of $(i, \mathbf{y}_{i,i'}, i')$ triples where $i$ denotes reference item, $\mathbf{y}_{i,i'}$ denotes modification and $i'$ is the desired target item.

\textbf{Problem Definition} We define the gradient item retrieval problem as follows: \textit{based on a reference item and a modification, retrieve a sequence of items in which relevance for a certain desired attribute is in increasing or decreasing order, and relevance for other attributes remains the same}. To make it simple, we consider that a query consists of a reference item and a modification about only one attribute. Note that, if multiple attributes are required to be modified, we can apply the atomic modification several times. Mathematically, we define the query as $(i, \alpha t)$ where $i$ indicates the reference item, $\alpha \in \{1,-1\}$ is the modification action and $t$ is the desired modification attribute. Note that there is a bijection between $\alpha$ and the modification words "more" and "less". For the gradient item retrieval problem, it can be defined as: for a query $(i, \alpha t)$ and its corresponding retrieval sequence $Seq\text{-}i$, we want to maximize the probability of the sequence satisfying the constraint: $\alpha \cdot relevance(Seq\text{-}i@k,t) < \alpha \cdot relevance(Seq\text{-}i@k+1,t)$ and $ relevance(Seq\text{-}i@k,t') = relevance(Seq\text{-}i@k+1,t')$, for any other $ t' \in \mathcal{T}, t' \neq t$, where the $relevance$ function measures the relevance score between a retrieved item $Seq\text{-}i@k$ and a certain attribute.


% Need add more detail
\subsection{Proposed Framework of CGIR}
The general framework of the proposed method is shown in Figure \ref{fig:model}. It includes three major parts. 

The left part is designed based on Variational Autoencoder framework~\cite{KingmaW13VAE}, which learns a disentangled item representation from user activities. For each user $u$, we encode the interaction vector $x_{u,:}$ to the user hidden representation $\mathbf{z}_{u} \in R^{D}$. After calculating the interaction probability between user $u$ and all items $\mathbf{H} \in R^{M \times D}$, we reconstruct the interaction vector $x'_{u,:} \in R^{M}$. The reconstruction loss can be calculated between user interaction vector $x_{u,:}$ and its relevant reconstructed vector $x'_{u,:}$. The disentanglement loss is computed using the mean $\mu_{u}$ and variance $\sigma_{u}$.
%of normal distribution of user $u$'s hidden representation.\he{The previous sentence is confusing.} 
We keep the dimensionality of $\mu_{u}$ and  $\sigma_{u}$ the same as $\mathbf{z}_{u}$.

The right part aims to encode attribute strings to a space where attribute representations are sparse. In that space, each representation of an attribute string has only a few activated dimensions. Our intuition is, for each item, the information of its disentangled representation includes the information of all its attributes. Therefore, each attribute representation should only correspond to some dimensions of the disentangled representation. The input of the sparse encoder model is pre-trained word vectors. We use GloVe~\cite{Pennington14glove} as initial features for English words and pre-trained Chinese Word Vectors~\cite{Shen18weibo} as initial features for Chinese characters~(attribute data of Alishop-attribute dataset is in Chinese). If an attribute only has one word or phrase, we use the relevant sparse word representation as the attribute representation. For attributes including multiple words, a sum pooling is applied over sparse representations of words to obtain the attribute representation.

The middle part is for aligning the disentangled item representation space and the sparse attribute representation space. By leveraging the VAE framework, representations are factorized, where dimensions tend to be independent~\cite{Locatello18ChallengeDisentangle}. However, the meaning of each dimension or the composition of some dimensions remains unclear. The goal of this part is to ground the semantic meanings of attributes to dimensions of factorized item representations. 

To achieve the goal, one direct way is to leverage the item-attribute relation data $\mathbf{A}$, which adopted by some existing GAN-based methods \cite{CycleGAN2017, lee2018diverse, shen2020interpreting, jahanian2020steerability, zhou2020ganbased}. However, those methods ignore the relationship between items, which contradicted with the essence of item retrieval. Instead, we implicitly align the two space by minimizing the distance between the target item $i'$ and the modification result which computed by adding a correct modification $\mathbf{y}_{i,i'}$ on the reference item $i$. Note, overlapping is allowed between corresponding dimension sets of two attributes, because two attributes may have the same semantic primitives which are separately encoded into different dimensions of item representation. And to keep a linear relationship in the hidden space, we directly add an item representation and an attribute representation without any non-linear transformation. The coefficient $\gamma$ controls the strength of modification. In the training stage, we set $\gamma = 1$ since we only use the information of whether one item has a certain attribute or not. During the inference stage, in order to retrieve a sequence of items in a gradient manner, we change the strength coefficient $\gamma$ by increasing a fraction number at each step and keep the top one retrieved item for each step to form the retrieval sequence. The three parts are trained in an end-to-end manner.

\subsection{Weakly-Supervised Disentangled Representation Learning with Semantic Meaning}
% To achieve gradient item retrieval and align semantic meaning to dimensions of item representations, we need to achieve disentangled item representations with consistency and restrictiveness. Besides, each attribute representation should only be grounded to some dimensions of its relevant item representations, due to the information of item representation should conclude all its attribute's information.
\textbf{Weakly-Supervised Variational Auto-Encoder.} 
we leverage the VAE framework \cite{KingmaW13VAE} to  enforce item representations to be factorized. And, to involve the information of attribute data, as we stated in the previous section, we model the relation between item pairs and attributes instead of item-attribute data. Specifically, we model the joint distribution of observed variables $\mathbf{X}$ and $\mathbf{Y}$ by joint distribution $p_{\theta}(\mathbf{X}, \mathbf{Y})$ where $\theta$ denotes parameters of \CGIR~. Our generative model assumes that the observed data are generated from the following distribution:
\begin{equation}
\begin{split}
p_{\theta}(\Tilde{\mathbf{X}}, \Tilde{\mathbf{Y}}) &= \iint p_{\theta}(\mathbf{\Tilde{X},\Tilde{Y}}|\mathbf{Z,H}) p_{\theta}(\mathbf{Z,H})  \,d\mathbf{Z} \,d\mathbf{H}
\end{split}
\label{equ:joint}
\end{equation}
$\mathbf{\Tilde{X}}$ and $\mathbf{\Tilde{Y}}$ are variables sampled from a distribution parameterized by hidden variables $\mathbf{Z} \in R^{N \times D}$ and $\mathbf{H} \in R^{M \times D}$. The meanings of $\mathbf{Z}$ and  $\mathbf{H}$ are described in the previous subsection. As shown in Figure \ref{fig:pgm1}, $\mathbf{\Tilde{X},\Tilde{Y}}$ are independent when conditional on $\mathbf{Z}$ and $\mathbf{H}$. Therefore, we have, 
\begin{equation}
\begin{split}
p_{\theta}(\Tilde{\mathbf{X}}, \Tilde{\mathbf{Y}}) = \iint p_{\theta}(\mathbf{\Tilde{X}}|\mathbf{Z,H}) p_{\theta}(\mathbf{\Tilde{Y}}|\mathbf{Z,H})  \,d\mathbf{Z} \,d\mathbf{H}
\end{split}
\label{equ:joint_split}
\end{equation}
We assume interactions between users and items are independent and identically distributed ($i.i.d.$), and vectors in attribute difference data are also $i.i.d.$. Therefore, for the two terms in equation \ref{equ:joint_split}, we have $p_{\theta}(\mathbf{\Tilde{X}}|\mathbf{Z,H}) = \prod_{u,i}p_{\theta}(\Tilde{x}_{u,i}|\mathbf{z}_u, \mathbf{h}_i)$ and  $p_{\theta}(\Tilde{\mathbf{Y}}|\mathbf{Z,H}) = \prod_{i,i'}p_{\theta}(\mathbf{\Tilde{y}}_{i,i'}|\mathbf{h}_i, \mathbf{h}_{i'})$ separately. Following the paradigm of variational autoencoder (VAE) \cite{ChenB20PairwiseVAE, mazhou0Y019MacridVAE}, we introduce a variational dsitribution to alleviate computational burden of integral of equation\ref{equ:joint_split} and maximize the lower bound of $\ln p_{\theta}(\Tilde{x}_{u,i}, \mathbf{\Tilde{y}}_{i,i'}) $ by:

\begin{equation}
\begin{aligned}
\ln p_{\theta}(\Tilde{x}_{u,i}, & \mathbf{\Tilde{y}}_{i,i'}) \geq \mathbb{E}_{q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i}|{x}_{u,i}})} \big[\ln p_{\theta}(\Tilde{x}_{u,i} |\mathbf{z}_{u}, \mathbf{h}_{i})\big] \\
& - \mathcal{D}_{KL}\big(q_{\theta}(\mathbf{z}_{u}, \mathbf{h}_{i}|{x}_{u,i}) || p(\mathbf{z}_{u}, \mathbf{h}_{i}) \big) \\
&+ \mathbb{E}_{q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i}|{x}_{u,i}}), q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i'}|{x}_{u,i'}})} \big[\ln p_{\theta}(\mathbf{\Tilde{y}}_{i,i'} |\mathbf{h}_{i}, \mathbf{h}_{i'})\big]. 
\end{aligned}
\label{equ:vae_lb}
\end{equation}

The expectation $\mathbb{E}_{q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i}|x_{u,i}})}[\cdot]$ is still intractable. As shown in figure \ref{fig:pgm2}, we have $\mathbf{z}_{u} \perp \mathbf{h}_{i} | x_{u,i}$, according to the Common cause decomposition of graphical models\cite{Buntine11PGM}. Therefore, we have the following decomposition:

\begin{equation}
\begin{aligned}
q_{\theta}(\mathbf{z}_u,\mathbf{h}_i,|x_{u,i}) = q_{\theta}(\mathbf{z}_u,|x_{u,i}) q_{\theta}(\mathbf{h}_i,|x_{u,i}). \end{aligned}
\label{equ:demopose1}
\end{equation}

Instead of computing $\mathbb{E}_{q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i}|x_{u,i}})}[\cdot]$ directly, we use the Gaussian re-parameterization trick\cite{KingmaW13VAE} to solve $\mathbb{E}_{q_{\theta}(\mathbf{z}_u,|x_{u,i}) q_{\theta}(\mathbf{h}_i,|x_{u,i})}[\cdot]$.


\textbf{Factorization via Regularization.} A natural strategy to encourage factorization is to force statistical independence between dimensions. 
% so that each dimension describes an isolated factor, i.e. , to force $q_{\theta}(\mathbf{z}_u) \approx \prod_{d=1}^D q_{\theta}(\mathbf{z}_u^{(d)})$ and $q_{\theta}(\mathbf{h}_i) \approx \prod_{d=1}^D q_{\theta}(\mathbf{h}_i^{(d)})$. 
As demonstrate in the previous work \cite{Higgins17betaVAE}, if the prior satisfies factorization, penalizing the Kullback-Leibler term of equation \ref{equ:vae_lb} would encourage independence between the dimensions. In here, we choose two standard multivariate normal distributions as priors for $\mathbf{z}_u$ and $\mathbf{h}_i$. For the Kullback-Leibler divergence part of equation \ref{equ:vae_lb},  we can decompose it as:

\begin{equation}
\begin{split}
&\mathcal{D}_{KL}\big(q_{\theta}(\mathbf{z}_{u}, \mathbf{h}_{i}|x_{u,i}) || p(\mathbf{z}_{u}, \mathbf{h}_{i}) \big) \\
&= \mathcal{D}_{KL}\big(q_{\theta}(\mathbf{z}_{u}|x_{u,i}) q_{\theta}(\mathbf{h}_{i}|x_{u,i})|| p(\mathbf{z}_{u}) p ({ \mathbf{h}_{i}})\big)\\
&= \mathcal{D}_{KL}\big(q_{\theta}(\mathbf{z}_{u}|x_{u,i})||p(\mathbf{z}_{u})\big) + \mathcal{D}_{KL}\big(q_{\theta}(\mathbf{h}_{i}|x_{u,i})||p(\mathbf{h}_{i})\big) 
\end{split}
\label{equ:kl_decompose1}
\end{equation}

The two KL terms in equation \ref{equ:kl_decompose1} aim at enforcing factorization of user and item representations separately.
Due to the time-efficient requirement of recommendation system, we keep a representation table for items, instead of inferring them from interaction matrix at each time. Therefore, we only keep the first term of equation \ref{equ:kl_decompose1} in the final objective. Although this simplification has been used in the previous work\cite{mazhou0Y019MacridVAE}, we also empirically show  that this simplification can enforce item representations to be factorized in our experiments. Besides, We follow $\beta$-VAE\cite{Higgins17betaVAE} to strengthen the KL divergence by a factor of $\beta$.

\textbf{Geometric Relationship of Item Representation.} As shown in the middle part of Figure \ref{fig:model}, to implicitly align item space and attribute space, we leverage the geometric relationship between items. For a reference-target item pair, their distance will be minimized when a correct modification is added on the reference item. Based on the intuition, we define the third term of equation \ref{equ:vae_lb} as:

\begin{equation}
\begin{aligned}
&p_{\theta}(\mathbf{\Tilde{y}}_{i,i'}|\mathbf{h}_{i}, \mathbf{h}_{i'}) =\\
&\frac{q_{\theta}(\mathbf{h}_{i'}|\mathbf{x}_{:,i'}) \big(q_{\theta}(\mathbf{h}_{i}|\mathbf{x}_{:,i}) + \gamma \cdot \sum_{t \in \mathcal{T}} \Tilde{y}_{i,i'}^{t} \cdot F_{\theta}(t)\big)}{\sum_{j' \in [1,M]} q_{\theta}(\mathbf{h}_{i'}|\mathbf{x}_{:,i'})   \big(q_{\theta}(\mathbf{h}_{i}|\mathbf{x}_{:,i}) + \gamma \cdot \sum_{t \in \mathcal{T}} \Tilde{y}_{i,j'}^{t} \cdot F_{\theta}(t)\big)}
\label{equ:contrastive}
\end{aligned}
\end{equation}

In whole, $\gamma \cdot \sum_{t \in \mathcal{T}} \Tilde{y}_{i,i'}^{t} \cdot F_{\theta}(t)$ represents the modification $\mathbf{y}_{i,i'}$ scaled by a factor $\gamma$. During training stage, we set the modification strengthen coefficient $\gamma$ equals one. And during inference, $\gamma$ will be gradually changed to retrieve item in gradient manner. The $\Tilde{y}_{i,i'}^{t}$ indicates the modification direction for attribute $t$, $F_{\theta}(\cdot):R^{K} \rightarrow R^{D}$ is the sparse attribute encoder which encode the attribute $t$ to a sparse representation. The equation \ref{equ:contrastive} represents the probability of one triple $(i, \mathbf{y}_{i,i'}, i')$ in $\mathcal{D}$. To align two representation spaces, we maximize the equation \ref{equ:contrastive}.

\begin{figure}[h]
\centering
\includegraphics[width=0.35\textwidth]{figures/PGM1.png}
\caption{\textbf{The decoder model,} $p(\mathbf{X,Y}|\mathbf{Z,H})$.}
\label{fig:pgm1}
 \end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=0.20\textwidth]{figures/PGM2.png}
\caption{\textbf{The encoder model,} $p(\mathbf{Z,H}|\mathbf{X})$.}
\label{fig:pgm2}
\end{figure}


\textbf{Sparse Attribute Representation} Following our intuition that one item's attribute has less information than the whole item and should only be grounded to part of disentangled item representations, we enforce the attribute representation to be sparse before the alignment of attribute and item representation. Function $F_{\theta}(\cdot)$ is an attribute encoder which maps a attribute string to a sparse representation space. Specifically,

\begin{equation}
\begin{aligned}
F_{\theta}(t) = \sum_{w \in \mathcal{W}(t)} f_{\theta}(w)
\end{aligned}
\end{equation}

where $\mathcal{W}(t)$ represents the set of words used in attribute string $t$. Function $f_{\theta}(\cdot)$ upscale the word representation to another representation space. To enforce the word representation has only a few activated dimensions a sparse loss ($\textit{SL}$) is applied:

\begin{equation}
\begin{aligned}
\textit{SL} = \frac{1}{D} \sum_{d=1}^D &\Big( \text{max}  (\frac{1}{|\mathcal{W}|} \sum_{w \in \mathcal{W}} f^{d}_{\theta}(w) - \rho, 0) ^2 \\
&+ \frac{1}{|\mathcal{T}|} \sum_{w \in \mathcal{W}}  f^{d}_{\theta}(w) \times (1- f^{d}_{\theta}(w))  \Big).
\label{equ:sparseloss}
\end{aligned}
\end{equation}

The first term is an Average Sparsity Loss (ASL) which penalizes any deviation of the observed average activation value $f^{d}_{\theta}(w)$ from the desired average activation value $\rho$ which is usually set to a small value. The second term is a Partial Sparsity Loss (PSL) that facilitates the value of each dimension of $f_{\theta}(w)$ to be close to either 0 or 1\cite{Subramanian18SPINE}.

\textbf{Overall Objective Function} The above equations bring us to the following training objective. Parameter $\theta$ is optimized by maximizing the objective:

\begin{equation}
\begin{split}
\mathbb{E}_{ p_{data}(\mathbf{X})} & \Big[ \mathbb{E}_{q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i}|x_{u,i}})} \big[\ln p_{\theta}(\Tilde{x}_{u,i} |\mathbf{z}_{u}, \mathbf{h}_{i})\big] \\
& - \mathcal{D}_{KL}\big(q_{\theta}(\mathbf{z}_{u}|{x}_{u,i})||p(\mathbf{z}_{u})\big) \\
&+ \mathbb{E}_{q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i}|{x}_{u,i}}), q_{\theta}{(\mathbf{z}_{u}, \mathbf{h}_{i'}|{x}_{u,i'}})} \big[\ln p_{\theta}(\mathbf{\Tilde{y}}_{i,i'} |\mathbf{h}_{i}, \mathbf{h}_{i'})\big] \Big] \\
&-\frac{1}{D} \sum_{d=1}^D \Big( \text{max}  \big(\frac{1}{|\mathcal{W}|} \textstyle\sum_{w \in \mathcal{W}} f^{d}_{\theta}(w) - \rho, 0\big)^2 \\
&+ \frac{1}{|\mathcal{T}|} \textstyle\sum_{w \in \mathcal{W}}  f^{d}_{\theta}(w) \times \big(1- f^{d}_{\theta}(w) \big)  \Big).
\end{split}
\label{equ:whole_obj}
\end{equation}



\subsection{Disentanglement with Guarantee}
As demonstrated by Locatello et al.\cite{Locatello19challengedisentangle}, VAE-based unsupervised learning methods fundamentally cannot achieve disentanglement without model inductive biases. Therefore, a natural question is can our method deliver a disentanglement without the help of model inductive bias? Shu et al.\cite{shu20disentangleguarantee} gives a theoretical analysis which shows disentanglement can be achieved with guarantee under proper weak supervision. Within their analysis framework, three types of weakly supervised settings were considered, which are restricted labeling, matching pairing, and rank pairing. In our case, attributes of items are considered as hidden factors. For item $i$ and item $i'$, we construct the attribute difference vector $\mathbf{y}_{i,i'}$ by comparing them under each attribute $t$, $y_{i,i'}^t = a_{i,t} - a_{i',t}$. If attribute $t$ belongs to item $i$ but not for item $i'$, then the ranking of $i$ is higher than $i'$ and $y_{i,i'}^t$ equals $1$. Therefore, the $(i,\mathbf{y}_{i,i'},i')$ triple data can be understood as a special type of ranking-pair where the ranking is binarized and semantic meaningful. Then according to the \textit{Weak Supervision Disentanglement Theorem}\cite{shu20disentangleguarantee}, the disentangled representation learned under three types of weak supervision is distribution-matching an oracle disentangled representation in which the consistency property of hidden factors, considered by weak supervision signal, can be guaranteed. In our setting, we consider the ranking of one attribute between two items at each triple, because of the restriction $\sum_{t\in\mathcal{T}} |{y}^t_{i,i'}| = 1$. Further, empirically we have $\sum_{i,i' \in [1,N]} |{y}^t_{i,i'}| > 1, \forall t \in \mathcal{T}$ which means all attributes are considered by the weak supervision signal. Further, the consistency of all attributes can be guaranteed. By the Full Disentanglement Rule\cite{shu20disentangleguarantee}, the consistency of all factors further implies the restrictiveness property is guaranteed in disentangled representation.

\begin{equation}
\begin{split}
\bigwedge_{t \in \mathcal{T}} C(t) \iff  \bigwedge_{t \in  \mathcal{T}} D(t), \bigwedge_{t \in \mathcal{T}} D(t) \iff  \bigwedge_{t \in  \mathcal{T}} R(t)
\end{split}
\end{equation}

where $C(t)$ denotes the consistency of hidden factor $t$, $R(t)$ denotes restrictiveness of hidden factor $t$ and $D(t)$ denotes the disentanglement of hidden factor $t$. 