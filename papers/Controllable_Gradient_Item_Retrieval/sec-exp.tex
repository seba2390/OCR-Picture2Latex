%!TEX root = main.tex
\section{Experiments}

\begin{table*}[ht]
\centering
\small
\setlength{\tabcolsep}{4pt}{
\begin{tabular}{l | llll | llll | llll }
  \bottomrule
& \multicolumn{4}{c|}{\bf ALiShop-tag} & \multicolumn{4}{c|}{\bf ML-25M}& \multicolumn{4}{c}{\bf ML-20M}\\
\hline
Models &{\bf Hit@20}&{\bf Hit@50} &{\bf MRR}&{\bf MGS} &{\bf Hit@20}&{\bf Hit@50} &{\bf MRR}&{\bf MGS} &{\bf Hit@20}&{\bf Hit@50} &{\bf MRR}&{\bf MGS} \\
\hline
CBIR  & 0.0211 & 0.0409 & 0.01601 & 0.1811 & 0.2651 & 0.3328 & 0.2116 & 0.1629 & 0.3127 & 0.4638 & 0.2481 & 0.1494 \\
DSCMR & 0.0239 & 0.0591 & 0.01769 & 0.1934 & 0.2974 & 0.3471 & 0.2292 & 0.1683 & 0.3271 & 0.4810 & 0.2622 & 0.1543 \\
TIRG  & 0.0581 & 0.0831 & 0.02418 & 0.2364 & 0.4328 & 0.4801 & 0.3094 & 0.1903 & 0.4733 & 0.5497 & 0.3286 & 0.2085 \\
\hline
\CGIR & \textbf{0.0626} & \textbf{0.1019} & \textbf{0.02638} & \textbf{0.2796} & \textbf{0.4412} & \textbf{0.4891} & \textbf{0.3164} & \textbf{0.2588} & \textbf{0.4986} & \textbf{0.5572} & \textbf{0.3374} & \textbf{0.2359} \\
\CGIR~ w/o VAE & 0.0572 & 0.0810 & 0.02421 & 0.2314 & 0.4286 & 0.4731 & 0.2981 & 0.2094 & 0.4729 & 0.5334 & 0.3196  & 0.1938 \\
\CGIR~ w/o Sparse &  0.0628 &  0.1021 &  0.02641 & 0.2607 &  0.4462 &  0.4905 &  0.3188 & 0.2361 &  0.4990 &  0.5578 &  0.3375 & 0.2162\\ 
\toprule
\end{tabular}}
\vspace{-3pt}
\caption{Gradient Item Retrieval Performance Evaluation on three datasets. } 
\vspace{-13pt}
\label{tab:IR_performance}
\end{table*}


We evaluate \CGIR~ on real-world datasets with the aim to answer the following research questions (RQs): \\
\textbf{RQ 1} Does \CGIR~ achieve gradient item retrieval? \\
\textbf{RQ 2} Does \CGIR~ outperform other competitors in the item retrieval task?\\
\textbf{RQ 3} Can \CGIR~ achieve factorized item representation?\\
% \textbf{RQ 4} Can semantic meanings  be grounded to the dimensions of item representation?\\

% In this section, we first present the experimental settings, followed by experimental results to answer the above three research questions.

\begin{table}[H]
\centering
\setlength{\tabcolsep}{4pt}{
\begin{tabular}{l | lll }
 \bottomrule
% & \multicolumn{3}{c}{\bf Mean Gradient Score}\\
\hline
 &{\bf ALiShop-tag}&{\bf ML-25M} &{\bf ML-20M} \\
\hline
\# of users         & 465,573 & 160,775 & 136,677 \\
\# of items         & 1,02,746 & 38,715 & 20,660  \\
\# of interaction   & 4.4M & 12.5M & 10.0M \\
\# of tags          & 263 & 1086 & 1086 \\
\# of tagged items  & 1,02,746 & 29,133 & 13,025 \\
avg. \# of tags per item & 4.16 & 12.61 & 13.46 \\
\# of available tags in $\mathbf{Y}$ & 263 & 1086 & 1086 \\
\toprule
\end{tabular}}
\vspace{-3pt}
\caption{Attributes of datasets after preprocessing.}
\vspace{-13pt}
\label{tab:data_stat}
\end{table}

\subsection{Experimental Settings}

\textbf{Datasets} We experimented with two publicly accessible MovieLens data sets\footnote{The MovieLens data set: \url{https://grouplens.org/datasets/movielens/}} MovieLens-25M and MovieLens-20M, as well as an industrial internal dataset from Alibaba. For both MoviesLens data and Alishop dataset, we regard tags of a movie or an item as its attributes. In this section ``tag'' and ``attribute'' refer to the same thing and will be used interchangeably. For the user movie rating data, we follow MacridVAE, in which ratings are binarized by keeping ratings of four or higher and users who have watched at least five movies. For the tag data, we clean the user provided tags and keep those appeared in the official genome tag table. Additionally we collect a dataset, named AliShop-tag, from Alibaba's e-commerce platform Taobao.  All items in AliShop-tag has tags as well as titles and images. Every user in this dataset clicks at least ten items. The characteristics of the three datasets are summarized in Table\ref{tab:data_stat}. Note, to show all attributes can be considered by the modification data $\mathbf{Y}$ even we restrict $\sum_{t\in\mathcal{T}} |{y}^t_{i,i'}| = 1$, we analyze the number of available tags in $\mathcal{T}$, where a tag $t$ is called available if $\sum_{i,i' \in [1,N]} |{y}^t_{i,i'}| > 1$.\\
\textbf{Query Construction} Queries are created as following: pairs of products that have one attribute difference in their descriptions are selected as the query item and target item pairs; and the modification query is composed by a modification action word (``more'' or ``less'') and the different attribute, e.g. more floral. By this way, triple data is constructed, where the head is a reference item, the tail is a target item and the middle is the modification query. As conventional practice, we hold 20\% of triple data for testing and 80\% for training. The constructed data is close to what will be used in the real-world scenario, where possible modifications will be made offline for each item and then be prompted to a customer who browsed the item just now.


\textbf{Baselines.} We introduce a set of baselines in our experiments:\\
\textit{1. Content-based item Retrieval(CBIR):} We train a fully connected network to predict a matching score between a query (a reference item an a modification) and items. We embed item representation from its interaction history with all users. Besides, we encode the query as a concatenation of the representation of a reference item and the modification text.\\
\textit{2. Text Image Residual Gating \cite{Vo19TIRG} (TIRG):} We adapted this method for item retrieval. TIRG encodes the interaction between one item and users to the item representation. The method aims to map the representation of item and representation of text into the same space and combine them through residual connection. Then we estimate the matching score between the target item and the combination between reference item and modification.\\
\textit{3. Deep Supervised Cross-modal Retrieval \cite{Zhen19DSCMR} (DSCMR):}We adapt this cross-modal text-image retrieval method to our setting. As previously, we use the interaction history of each item as an input feature, and tag texts and text input. This method tries to find a common representation space, in which the samples from different modalities can be compared directly.\\
We also introduce two variants for ablation study to analyze the impact of different components of \CGIR~ to the performance.\\
\textit{1. \CGIR~ w/o VAE:} In this method, instead of using VAE as item representation encoder, we use interaction history as input and a fully connected network as an encoder to encode the interaction history of an item as its representation. This part is same as those baselines. For the remaining, we keep it same as original \CGIR.\\
\textit{2. \CGIR~ w/o Sparse:} We drop the partial sparsity loss and average sparsity loss as shown in \ref{equ:sparseloss}. For the other parts, we keep them same as \CGIR.\\



\begin{table*}[h!]
\centering
% \small
\setlength{\tabcolsep}{4pt}{
\begin{tabular}{l | llll | llll | llll }
  \bottomrule
& \multicolumn{4}{c|}{\bf ALiShop-tag} & \multicolumn{4}{c|}{\bf ML-25M}& \multicolumn{4}{c}{\bf ML-20M}\\
\hline
Models &{\bf MGS}&{\bf MGS-C} &{\bf MGS-R}&{\bf Ind.}  &{\bf MGS}&{\bf MGS-C} &{\bf MGS-R}&{\bf Ind.} &{\bf MGS}&{\bf MGS-C} &{\bf MGS-R}&{\bf Ind.} \\
\hline
CBIR   & 0.1811 & 0.2765 & 0.7638 & 0.7627 & 0.1629 & 0.2481 & 0.7904 & 0.6944 & 0.1494 & 0.2575 & 0.7619 & 0.6791 \\
DSCMR  & 0.1934 & 0.2919 & 0.7311 & 0.7398 & 0.1683 & 0.2634 & 0.7819 & 0.6819 & 0.1543 & 0.2763 & 0.7534 & 0.6637 \\
TIRG   & 0.2364 & 0.3566 & 0.7193 & 0.7341 & 0.1903 & 0.2899 & 0.7403 & 0.6563 & 0.2085 & 0.3059 & 0.7264 & 0.6440 \\
\hline
\CGIR & \textbf{0.2796} & \textbf{0.3874} & \textbf{0.8329} & \textbf{0.9834} & \textbf{0.2588} & \textbf{0.3371} & \textbf{0.8961} & \textbf{0.9563} & \textbf{0.2359} & \textbf{0.3516} & \textbf{0.8674} & \textbf{0.9521} \\
\CGIR~ w/o VAE & 0.2314 & 0.3230 & 0.7893 & 0.7692 & 0.2094 & 0.2917 & 0.7388 & 0.6691 & 0.1938 & 0.2972 & 0.7309 & 0.6529 \\
\CGIR~ w/o Sparse & 0.2607 & 0.3841 & 0.8114 & 0.9759 & 0.2361 & 0.3358 & 0.8755 & 0.9312 & 0.2162 & 0.3023 & 0.8448 & 0.9446 \\
% \CGIR~ w/o Both &  &  &  &  &  &  &  &  &  &  &  &  \\
\toprule
\end{tabular}}
\vspace{-3pt}
\caption{Gradient Effect. To analysis the gradient effect, we provide a more 
comprehensive analysis using different metrics.} 
\vspace{-13pt}
\label{tab:ConsistRestrict}
\end{table*}


\textbf{Evaluation Metrics.}
We use the following metrics to evaluate the performance of our proposed model. We use two commonly used evaluation criteria in our experiments to evaluate the performance of item retrieval. \textbf{Hit Rate} at K (HR@K) computed as the percentage of test queries where target item is within the top K retrieved items. \textbf{Mean Reciprocal Rank} (MRR) measures the mean of reciprocal rank of target item in the retrieved list. Besides, to qualitatively measure the gradient effect of retrieval result, we design a new metric, named \textbf{Mean Gradient Score} (MGS), to evaluate the degree of gradient for retrieved item list. We use the following equation to define the Mean Gradient Score:
\begin{equation}
\begin{aligned}
MGS & = \frac{1}{|\textit{test}|} \sum_{({i}, \alpha {t}) \in \textit{test}} \Big( Consistency\_Score(Seq\text{-}{{i}}, {t}) \\
 & \cdot \big( 1 - \frac{1}{|\mathcal{T}|} \sum_{\substack{{t}' \in \mathcal{T} \\ {t}' \neq {t}}} Restrictiveness\_Score( Seq\text{-}{i} , {t}') \big)\Big)
\end{aligned}
\label{equ:mgs}
\end{equation}
Here, $test$ is a set of testing pairs. Each testing pair includes an item ${i}$ and an desired modification $\alpha {t}$. $seq\text{-}{i}$ is an item sequence retrieved by increasing the strength coefficient $\gamma$ by 0.1 in each step. The first term in equation \ref{equ:mgs} is the consistency score of the retrieved item sequence. It measures whether the relevant score of items in sequence $Seq\text{-}{i}$ with respect to a certain attribute changes gradually. The second term is the restrictiveness score of the retrieved item sequence. It measures whether the modification on one attribute will influence the relevance between other attributes and items. 
% A good disentangled representation will keep a relative low value between item $i$ and unmodified attributes, which means, when applying a modification, the irrelevant attributes will keep the same. 
We define the $Consistency\_Score$ and $Restrictiveness\_Score$ as the following:
\begin{equation}
\begin{aligned}
Co&nsistency\_Score(seq\text{-}i,t) = \Big(\frac{1}{N-1}\sum_{k=1}^{K-1} \\ &\vmathbb{1}\big[ \alpha \cdot Relevance(seq\text{-}{i}@k, {t}) < \alpha  \cdot Relevance(seq\text{-}{i}@k+1, {t})\big] \Big)
\end{aligned}
\label{equ:mgs_c}
\end{equation}
\begin{equation}
\begin{aligned}
& Restrictiveness\_Score(seq\text{-}i,t) = 1 - \Big(\frac{1}{N-1}\sum_{k=1}^{K-1} \\ & f\big( \alpha \cdot Relevance(seq\text{-}{i}@k, {t}) < \alpha  \cdot Relevance(seq\text{-}{i}@k+1, {t}) \big)\Big)
\end{aligned}
\label{equ:mgs_r}
\end{equation}
where $N$ is the length of the retrieved sequence $seq\text{-}i$, $seq\text{-}i@k$ is the $k$-th item of the sequence. Specifically, $seq\text{-}i@k$ is the top one item retrieved by the combination of the reference item representation and the modification with scaling coefficient $\gamma = 0.1 \times n$, the retrieved sequence $seq\text{-}i$ is formed by increasing the coefficient. $Relevance(i,t)$ is to calculate the relevance score between item $i$ and tag $t$. For ``add/more'' modification on certain tag, we expect the next item in the gradient sequence to have a higher relevance score regarding tag $g$. For ``remove/less'' modification, we expect a decrease in relevance score. Function $\vmathbb{1}[\cdot]$ is an indicator function which map True and False to 1 and 0. And function $f(\cdot)$ map True and False to 1 and -1. For Restrictiveness Score, if the relevance between indicated tag and items of retrieved sequence in a random walk manner, it will converge to 1 as the length of sequence go to infinite. Note, for MovieLens dataset, a ground-truth relevance score between a movie and a tag is provided. The \textit{Relevance} function directly output the ground-truth relevance score. However, for Alishop-tag dataset, labeling relevance for each item over every attribute is impossible. Therefore, we adopt a heuristic method. For each modification coefficient $\gamma$, instead of measuring the real relevance between top-one retrieved item and a certain attribute, we use the occurrence ratio of items, which has the attribute, on top 100 as the relevance score.


\subsection{Gradient Item Retrieval Performance}
To answer the first and second research questions, we conduct gradient item retrieval on AliShop-tag, MovieLens-20M and MovieLens-25M. The result is shown in table \ref{tab:IR_performance}. 
% Note, our method can still work with those item without tag set, because we take user-item interaction into consideration.

\textbf{Item Retrieval Performance.} We observe that our approach outperforms the baselines significantly. This is likely because the user interaction is noisy. Directly using interactions as fingerprint for items will include those noise. However, our method use a VAE \cite{ChenB20PairwiseVAE} framework to extract information from user-item interaction. Interpreting from the information bottleneck view \cite{TishbyZ15DLIB}, the disentanglement loss enforces our model to forget those noisy part of data and compress those useful information. Therefore, the noisy user-item interaction is denoised by our method, which gives a high-quality item representation. We also notice that both our method and baselines have a drop on the AliShop-tag dataset. The main reason is likey because the industrial E-commercial dataset is more noisy which will influence the quality of item representation and the item set is larger which directly influences the evaluation metrics because we fix the number $N$ in our experiment. Moreover, we observed that \CGIR~ and TIRG outperfrom CBIR and DSCMR by a significant margin. The improvement is likely because both our method and TIRG use a ranking loss, whereas CBIR and DSCMR use a matching loss.



\textbf{Gradient Retrieval Performance.} To measure the gradient retrieval performance, we apply a modification on a source item representation by increasing/decreasing the strength coefficient $\gamma$ by 0.1 at each time. By analyzing the retrieved item sequences, we calculate the mean gradient score. We outperform all the other baselines methods on MGS. On AliShop-tag data, we achieve better mean gradient score. This is likely because the AliShop-tag dataset has larger number of items which can have a better coverage in the item representation space. During inference stage, less irrelevant items will be retrieved.

\textbf{Ablation Study for Gradient Item Retrieval.} We observe that without using VAE for disentangled item representation, there is a drop on both item retrieval performance and gradient retrieval performance and the impact on gradient retrieval performance is more serious. One reason is attribute-relevant information will appear at each dimension of distributional item representations. When a scaled modification is applied, more than one attributes' information will be changed. Another reason as we discuss previously, the VAE structure delivers a denoised item representation. Besides, we observe that without using sparse loss the item retrieval performance is competitive with the best \CGIR~, but the gradient item retrieval performance is affected obviously. This result is in the line with our expectation. The sparse loss will compress the semantic meaning of an attribute representation to several dimensions which avoid a modification on irrelevant attributes, however some information will be lost in the meanwhile.


\subsection{Gradient Effect Study}
In order to analyze the effect of disentanglement and answer the third research question. We provide two more detailed experiments. In the first one, we measure the consistency and restrictiveness. In the second one, we analyze the relation between independence level and mean gradient score(MGS).


\textbf{Consistency and restrictiveness}
To validate the effect of disentangled representation, we measure the consistency and restrictiveness separately. More specifically, we calculate the mean value of restrictiveness score (equation \ref{equ:mgs_r}) and consistency score (equation \ref{equ:mgs_c}). Additionally, We quantify the level of independence by calculating the Uncorrelatedness of item representations \cite{mazhou0Y019MacridVAE}. We define Uncorrelatedness as: 
\begin{equation}
\begin{split}
Ind\_level(\mathbf{H}) = 1 - \frac{1}{D(D-1)}\sum_{\substack{d_i, d_j \in [1,D] \\ d_i \neq d_j}  } \big|CorrCoef ( {\mathbf{H}_{:,d_i}, \mathbf{H}_{:,d_j}}\big ) |. 
\end{split}
\end{equation}
The function $CorrCoef()$ measures the correlation coefficient between two variables. As shown in table \ref{tab:ConsistRestrict}, we denote restrictiveness score as \textbf{MGS-R}, mean consistency score as \textbf{MGS-C} and independence level as \textbf{Ind.}. 

We observed that the independence level outperform all other methods, which indicates that our method can achieve factorized item representation. This  directly answer the research question 3. We also observed that \CGIR~ outperform other competitor on \textbf{MGS-R}. This improvement shows that \CGIR~ has less influence on irrelevant hidden factor, when one factor was changed. This main reason is likely because by applying the disentangled loss, the item representation is factorized, so different hidden factors are encoded into different dimensions of the item representation, which allows us to only modify the value a few dimensions during inference. The performance on metric independence level also supports this explanation. Besides, we notice that although both VAE structure and sparse loss can impact the consistency and restrictiveness, the VAE is more important for important for disentangled representation.


\textbf{Independence Level and Mean Gradient Score}
In order to analyze the relation between Mean Gradient Score and Independence Level achieved by our disentangled representation. We vary the hyper-parameters related with disentanglement ($\beta$ and $\rho$ for our method), and plot Figure \ref{fig:ind-mgs} the relationship between the level of independence and Mean Gradient Score. We use all item representations on all three datasets to calculate the level of independence. By improving independence of item representations, we achieve a better result on gradient retrieval. This suggests that disentanglement loss can help improve the gradient item retrieval result.  


\begin{figure}[ht]
\centering
\includegraphics[width=0.45\textwidth]{figures/ind-mgs.png}
\caption{\textbf{Independence Level vs. Mean Gradient Score}}
\label{fig:ind-mgs}
\end{figure}


\textbf{Case Study}
To illustrate the gradient effect achieved by our method, we visualize several cases as shown in Figure \ref{fig:ml_visual} and \ref{fig:ali_visual}. For MovieLens datasets, because the ground truth relevance score between moives and movie tags are given, we show the relevance score under the poster of each movie. We retrieved those movies by changing the value of modification strength coefficient $\gamma$ from 0.2 to 1.0 increasing 0.2 at each step. We only keep the top 1 movie into the gradient item retrieval list for each $\gamma$. We visualize ``more'' and ``less'' modification results in Figure \ref{fig:ml_increase} and Figure \ref{fig:ml_decrease}, respectively. For Alishop-tag dataset, we show the top 4 items retrieved by different modification strength coefficients. This is because we do not have ground-truth relevance score between tags and products. A heuristic way to measure and show the relevance is to use the number of desired products appeared in top@K as the relevance score. As shown in Figures \ref{fig:ali_sport} and \ref{fig:ali_thick}, the number of desired items in top 4 of each retrieved list is increasing.



\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=1\linewidth]{figures/ml_increase.png}
   \caption{Apply modification "more romantic" on movie Lions For Lambs (2007)}
   \label{fig:ml_increase} 
\end{subfigure}

\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=1\linewidth]{figures/ml_decrease.png}
   \caption{Apply modification "less action" on movie Alita:Battle Angel(2019)}
   \label{fig:ml_decrease}
\end{subfigure}
\caption{Apply modification on movie data to retrieve a sequence of movies in gradient manner.}
\label{fig:ml_visual}
\end{figure}




\begin{figure}[ht]
\centering
\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=1\linewidth]{figures/ali_sport.png}
   \caption{Apply modification "more sport" on a sneaker}
   \label{fig:ali_sport}
\end{subfigure}


\begin{subfigure}[b]{0.45\textwidth}
   \includegraphics[width=1\linewidth]{figures/ali_thick.png}
   \caption{Apply modification "more thick" on a light shirt}
   \label{fig:ali_thick}
\end{subfigure}
\caption{Apply modification on Alishop-tag data, a sequence of items are retrieved after changing the value of $\gamma$ each time}
\label{fig:ali_visual}
\end{figure}