%!TEX root = main.tex

\section{Primal-Dual Framework for Packing Problems}		\label{sec:packing}
%\paragraph{Packing Problem.}
%Let $\mathcal{E}$ be a set of $n$ resources 
%and let $f: \{0,1\}^{n} \rightarrow \mathbb{R}^{+}$ be an \emph{arbitrary} non-decreasing function.
%Let $x_{e} \in \{0,1\}$ be a variable indicating whether resource $e$ is selected. 
%The set of packing constraints $\sum_{e} b_{i,e} x_{e} \leq 1 ~\forall i$ (including $x_{e} \leq 1 ~\forall e$) are given in advance and 
%resources $e$ are revealed online one-by-one. At the arrival of resource $e$, one receives a prediction $x^{\pred}_{e} \in \{0,1\}$ 
%and needs to make a decision on $x_{e}$ while maintaining $\vect{x} = (x_{e})_{e \in \mathcal{E}}$ feasible to the set of constraints.
%The objective of the problem is to maximize $f(\vect{x})$. In this problem, we seek a fractional solution that 
%is consistent to the prediction and robust to the optimal offline solution. 
%
%
%
%
%\subsection{Algorithm for Fractional Packing}		\label{sec:packing-main}
%Recall that a differentiable function $F: [0,1]^{n} \rightarrow \mathbb{R}^{+}$ is $(\lambda,\mu)$-max-locally-smooth
%if for any set $S \subset \mathcal{E}$, and for every vector $\vect{x} \in [0,1]^{n}$, 
%the following inequality holds:
%$$
%\sum_{e \in S} \nabla_{e} F(\vect{x}) \geq \lambda F\bigl( \vect{1}_{S} \bigr) - \mu F\bigl( \vect{x} \bigr)
%$$
%%where $\vect{x} := \bigvee_{e \in S} \vect{x}^{e}$, meaning that $x_{e'}  = \max \{x^{e}_{e'}\}$ for any coordinate $e'$.
% 


\paragraph{Formulation.}
First, we model the packing problem as a configuration linear program. 
For the integral variant of the packing problem, the decision variable $x_{e}\in\{0,1\}$  indicates whether element $e$ is selected in the solution. A configuration is a set of elements $S \subseteq \mathcal{E}$ and could be feasible or not.  In addition to $\vect x$ the linear program contains variables $z_S\in\{0,1\}$ for every configuration $S$. 
The idea is that $z_S=1$ solely for the set $S$ containing all selected elements $e$, i.e.\ for which $x_e=1$.  In this case $S$ is feasible by the constraints imposed on $\vect x$.

The fractional variant of the packing problem is modeled with the same variables and constraints, but without the integrality constraints.  Now $x_e$ specifies the fraction with which $e$ is selected.  The $\vect z$ variables represent now a distribution on configurations $S$ and have to be consistent with $\vect{x}$ in the following sense. When $S$ is selected with probability $z_S$, then $e$ belongs to the selected set $S$ with probability $x_e$. Unlike for the integral linear program, $\vect z$ is not unique for given $\vect x$.  In our algorithm we chose a particular vector $\vect z$. Note that the support of $\vect z$ might contain non-feasible configurations.

We consider the following linear program and its  dual.  

\begin{minipage}[t]{0.45\textwidth}
\begin{align*}b
\max  \sum_{S} &f(\vect{1}_{S}) z_{S} \\
\sum_{e} b_{i,e}  \cdot x_{e}  &\leq 1 & &  \forall i & (\alpha_i)\\
\sum_{S: e \in S} z_{S}  &= x_{e} 	& & \forall e & (\beta_e)\\
\sum_{S} z_{S} &= 1 & & & (\gamma) \\
x_{e} , z_{S} &\geq 0 & & \forall e,S\\
\end{align*}
\end{minipage}
\quad
\begin{minipage}[t]{0.5\textwidth}
\begin{align*}
\min \sum_{i} \alpha_{i} &+ \gamma \\
\sum_{i} b_{i,e}  \cdot \alpha_{i} &\geq \beta_{e}  & &  \forall e & (x_e)\\
\gamma + \sum_{e \in S} \beta_{e} &\geq f(\vect{1}_{S})  & & \forall S & (z_S) \\
\alpha_{i} &\geq 0 & & \forall i 
\end{align*}
\end{minipage}

In the primal, $(\alpha_i)$ are the packing constraints of the given problem.  Constraints $(\beta_e)$ force the aforementioned  relation between $\vect x$ and $\vect z$.  Constraint $(\gamma)$ ensures that $\vect z$ represents a distribution.
Note that the primal constraints $(\gamma)$ and $(\beta_e)$, imply the box constraints $x_{e}  \leq 1 ~\forall e$. 
 
\paragraph{Algorithm.} 
Assume that function $F(\cdot)$ is $(\lambda, \mu)$-locally smooth.
Let $d$ be the maximal number of positive entries in a row, i.e., $d = \max_{i} |\{b_{ie}: b_{ie} > 0\}|$.  
Let $\rho$ the maximum divergence between positive row coefficients, i.e.\ $\rho = \max_{i} \max_{e,e': b_{ie' > 0}} b_{ie} / b_{ie'}$.   
The algorithm is given a prediction $\vect{x}^\pred$, i.e.\ with every arriving element $e$, it receives the values $x^\pred_{ie}$ for each resource $i$.  It uses this prediction to specify coefficients $\overline{\vect b}$, which are scaled from $\vect b$ in a specific manner.  The maximum divergence $\overline \rho$ is defined similarly as $\rho$ with $\overline{\vect b}$ replacing $\vect b$, i.e., $\overline{\rho} = \max_{i} \max_{e,e': \overline{b}_{ie' > 0}} \overline{b}_{i,e} / \overline{b}_{ie'}$.

The algorithm maintains a primal solution $\vect y\in[0,1]^{\cal E}$, computed with the primal-dual method with respect to the coefficients $\overline {\vect b}$. Its decision for element $e$ either follows the predicted solution $x^\pred_e$, or it follows $y_{e}$ in case infeasibility of the predicted solution $\vect x^\pred$ has been detected.

The value of $\overline{b}_{i,e}$ depends on coefficient $b_{i,e}$ and 
the prediction $x^\pred_e$. Specifically, $\overline{b}_{i,e} = b_{i,e}$ if
$x^{\pred}_{e} = 1$ and the predictive solution is \emph{not} feasible; $\overline{b}_{i,e} = \frac{1}{\eta} b_{i,e}$ otherwise. In both cases, packing constraints using $\overline{\vect b}$ are stronger than they would be with coefficients $\vect b$.

Intuitively, if we do not trust the prediction at all, i.e.\ $\eta=1$, then 
$\overline{b}_{i,e} = b_{i,e}$ and therefore $x_{e}, y_{e}$ would get a value proportional to the one returned by a primal-dual algorithm
in the classic setting.
Inversely, if we trust the prediction (i.e., $\eta$ is close to 0) and the prediction is feasible, then 
$\overline{b}_{i,e} = b_{i,e}$ when $x^{\pred}_{e} = 1$ and $\overline{b}_{i,e} = b_{i,e}/\eta$ when 
$x^{\pred}_{e} = 0$. Therefore, the modified constraint
$\sum_{e'} \overline{b}_{i,e'} y_{e'} \leq 1$ will likely prevent $y_{e}$, for $e$ such that $x^{\pred}_{e} = 0$, 
from getting a large value. Hence, $y_{e}$ for $e$ such that $x^{\pred}_{e} = 1$ could get a large value.
In the end of each iteration, we set the output solution $x_{e}$ roughly by scaling a factor $\frac{1}{1+\eta}$ to  $y_{e}$ or $x^{\pred}_{e}$
(depending on cases)
in order to maintain the feasibility and the consistency to the prediction.


The construction of $\vect{y}$ follows the scheme in \cite{Thang20:Online-Primal-Dual}. 
We recall the definition of the divergence factor $\overline{\rho} = \max_{i} \max_{e,e': \overline{b}_{ie' > 0}} \overline{b}_{i,e} / \overline{b}_{ie'}$.
So in particular, $\overline{\rho} \leq \rho/\eta$. 
Recall that the gradient in direction $e$ is $\nabla_{e} F(\vect{y}) = \partial F(\vect{y})/\partial y_{e}$.
By convention, when an element $e$ is not released, $\nabla_{e} F(\vect{y}) = 0$.  
While $\nabla_{e} F(\vect{y}) > 0$ --- i.e., increasing $y_{e}$ improves the objective value ---
and $\sum_{i} \overline{b}_{i,e}  \alpha_{e} \leq \frac{1}{\lambda} \nabla_{e} F(\vect{y})$, the primal variable $y_{e} $
and dual variables $\alpha_{i}$'s are increased by appropriate rates. We will argue in the analysis that 
the primal and dual solutions returned by the algorithm are feasible. 

Recall that by definition of the multilinear extension, 
%\marginpar{Better avoid game theory notation}
$$
\nabla_{e} F(\vect{y})
= F((\vect{y}_{-e}, 1)) - F((\vect{y}_{-e},0))
= \mathbb{E}_{R} \bigl[ f\bigl(\vect{1}_{R \cup \{e\}}\bigr) - f\bigl(\vect{1}_{R}\bigr) \bigr]
$$
where $(\vect{y}_{-e}, 1)$ denotes a vector which is identical to $\vect{y}$ on every coordinate different to $e$ and 
the value at coordinate $e$ is 1.  The vector $(\vect{y}_{-e}, 0)$ is defined similarly. 
The expectation is taken over random subset $R \subseteq \mathcal{E} \setminus \{e\}$ such that $e'$ is included with probability $y_{e'}$.
Therefore, during the iteration of the while loop with respect to element $e$, only $y_{e} $ is modified and $y_{e'} $ remains fixed for all $e' \neq e$, 
as a consequence $\nabla_{e} F(\vect{y})$ is constant during the iteration.
Moreover,  for every $e$, $F(\vect{y})$ and $\nabla_{e} F(\vect{y})$ 
can be efficiently approximated up to any required precision \cite{Vondrak10:Polyhedral-techniques}.

One important aspect of the primal-dual algorithm presented below, is that it works only with primal variables $\vect x$ and dual variables $\alpha$, and uses the multilinear extension $F$ instead of $f$.  However for the analysis, we will later show how to extend these solutions with variables $\vect z$ and $\vect \beta$, $\gamma$ both to show feasibility of the constructed solution and to analyze consistency and robustness of the algorithm.  

\begin{algorithm}[ht]
\begin{algorithmic}[1]  
\STATE All primal and dual variables are initially set to 0. 
\STATE Let $\vect{y}\in[0,1]^{\cal E}$ be such that $y_{e} = 0 ~\forall e$. 
\STATE At every step, always maintain $z_{S} = \prod_{e \in S} x_{e}  \prod_{e \notin S} (1 - x_{e} )$.
\FOR{each arrival of a new element $e$} 
	\IF{$x^{\pred}_{e} = 1$ \OR the predictive solution $\vect x^\pred$ is infeasible} 
	\STATE{ set $\overline{b}_{i,e} = b_{i,e}$} 
	\ELSE \STATE{ set $\overline{b}_{i,e} = b_{i,e}/\eta$}
	\ENDIF
	\WHILE{$\sum_{i} \overline{b}_{i,e}  \alpha_{i} \leq \frac{1}{\lambda} \nabla_{e} F(\vect{y})$ \AND $\nabla_{e} F(\vect{y}) > 0$}
%		\STATE During the while loop, always maintain $\beta_{e} \gets \frac{1}{\lambda} \nabla_{e} F(\vect{y})$. The value 
%			of $\beta_{e}$ will be fixed after the for loop related to resource $e$. 
		\STATE Some of the primal and dual variables are increased continuously as follows, where $\tau$ is the time during this process.
		% Let $\tau$ be the time in the execution of the algorithm. The dual variables evolve during the execution of the algorithm as follows. 
		% \marginpar{But $y_e$ is primal}
		\STATE Increase $y_{e} $ at a rate such that $ \frac{dy_{e}}{d\tau} \gets \frac{1}{\nabla_{e} F(\vect{y}) \cdot \ln(1+ d\overline{\rho}  )}$.	
				\label{algo-packing:x}
% 		\FOR{$i$ such that $\overline{b}_{i,e}  > 0$}	
% 			\STATE Increase $\alpha_{i}$ at a rate such that
% %				\begin{align*}
% 					$
% 					\frac{d \alpha_{i}}{d \tau}	\gets \frac{\overline{b}_{i,e}  \cdot \alpha_i}{\nabla_{e} F(\vect{y})}  + \frac{1}{d \lambda}
% 					$
% %				\end{align*}
% 					\label{algo-packing:alpha}
% 		\ENDFOR
	\ENDWHILE 
	\IF{$x^{\pred}_{e} = 1$ \AND the predictive solution is still feasible} 
		\STATE set $x_{e}  \gets \frac{1}{1+\eta} x^{\pred}_{e} = \frac{1}{1+\eta}$ 
	\ELSE\STATE set $x_{e}  \gets  \frac{1}{1+\eta} y_{e}$
	\ENDIF
\ENDFOR
\end{algorithmic}
\caption{Algorithm for Packing Problem.}
	\label{algo:packing}
\end{algorithm}

Note that once the prediction becomes infeasible, the algorithm works with the given $b$ coefficients and outputs the solution computed by 
$y_e$ scaled by $1/(1+\eta)$.

\paragraph{Primal variables.}
The vector $\vect x$ is completed by $\vect z$ to form a complete solution to
the primal linear program, by setting $z_{S} = \prod_{e \in S} x_{e}  \prod_
{e \notin S} (1 - x_{e} )$. 


\paragraph{Dual variables.} 
Variables $\alpha_{i}$'s  are constructed in the algorithm. 
%Let $\vect{x} = (x_{e'})_{e'}$ and 
%Let $\vect{y} = (y_{e'})_{e'}$ be the current vectors of the algorithm and 
%Let $\vect{y}^{e}$ be the vector $\vect{y}$ just after the while loop with respect to resource $e$.
Define $\gamma = \frac{\mu}{\lambda} F(\vect{y})$ and 
$\beta_{e} = \frac{1}{\lambda} \cdot \nabla_{e} F(\vect{y})$.
%During the while loop with respect to resource $e$, by the observation above (i.e., $\nabla_{e} F(\vect{y})$ is constant during the iteration), 
%we have $\beta_{e} = \frac{1}{\lambda} \cdot \nabla_{e} F(\vect{y})$. 


The following lemma provides a lower bound on the $\alpha$ variables.
%Remark that the monotonicity of the gradient is crucial in the analysis of \cite{AzarBuchbinder16:Online-Algorithms}, in particular 
%to prove the bounds on $x$-variables. However, by our approach the gradient monotonicity  
%is not needed. 


\begin{restatable}{lemma}{BoundAlpha}
\label{lem:bound-alpha}
At any moment during the iteration related to element $e$,  
for every constraint $i$
it always holds that 
%$$
%\alpha_{i}	\geq  \frac{\beta_{e}}{\max_{e'}  \overline{b}_{i,e'}  \cdot d} 
%		\left[ \exp\biggl( \ln \bigl(1+ d\overline{\rho} \bigr) 
%				\cdot \sum_{e'} \overline{b}_{i,e'}  \cdot y_{e'}  \biggr) - 1 \right].
\begin{align}
\alpha_{i}	&\geq  \frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d \lambda} 
		\left[ \exp\biggl( \ln \bigl(1+ d\overline{\rho} \bigr) 
				\cdot \sum_{e'} \overline{b}_{i,e'}  \cdot y_{e'}  \biggr) - 1 \right]. \label{eq:inv_alpha}
\end{align}
\end{restatable}
\begin{proof}
Fix a constraint $i$. We prove the claimed inequality by induction.
In the very beginning of the algorithm, when no elements are released yet, the inequality holds since both sides are 0.  
Consider any moment $\tau$ during the loop corresponding to an arriving element $e$.
Assume that the inequality holds at the beginning of the iteration step. We will show that the inequality still holds after the step.

As $F$ is a multilinear extension, by its very definition, $F$ is linear in $y_{e}$. Hence 
$\nabla_{e} F(\vect{y})$ is independent of $y_{e} $. 
Moreover, during the loop corresponding to resource $e$, only $y_{e} $ is modified, leaving  $y_{e'} $ unchanged for all $e' \neq e$.  
As as a result, 
$d \nabla_{e} F(\vect{y})/d \tau = 0$. Therefore, the rate at time $\tau$ of the the right-hand-side of Inequality~\eqref{eq:inv_alpha} is:

\begin{align*}
&\frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d \lambda } \cdot \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \overline{b}_{i,e}  \cdot
		 \frac{d y_{e} }{d \tau} \cdot \exp \biggl( \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \sum_{e'} \overline{b}_{i,e'}  \cdot y_{e'}  \biggr) \\
%
&\leq  
\frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d \lambda} \cdot \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \overline{b}_{i,e}  \cdot
		 \frac{1}{\nabla_{e} F(\vect{y}) \cdot \ln(1+ d\overline{\rho} )}
		 \cdot \biggl( \frac{\max_{e'} \overline{b}_{i,e'}  \cdot d\lambda \cdot \alpha_{i}}{ \nabla_{e} F(\vect{y}) } + 1 \biggr) \\
%
&\leq \frac{\overline{b}_{i,e}  \cdot \alpha_{i}}{\nabla_{e} F(\vect{y})}  + \frac{1}{d \lambda} = \frac{d\alpha_{i}}{d \tau},
\end{align*}
%
where in the first inequality we use the induction hypothesis and the increasing rate of $y_{e}$.
So at any time during the iteration related to $e$, the increasing rate of the left-hand side is always larger than that of the right-hand side. 
Hence, the lemma follows.
\end{proof}

%\begin{proof}
%Before the release of resource $e$, $\beta_{e} = 0$ and the inequality trivially holds. 
%It is sufficient to show the lemma inequality for any moment $\tau$ after the release of resource $e$. 
%By the algorithm, $\beta_{e}$ is set up to $\frac{1}{\lambda} \nabla_{e} F(\vect{y})$ at the very beginning 
%of the loop corresponding to $e$ and 
%then $\beta_{e}$ will never be strictly increased, i.e., $\partial \beta_{e}/\partial \tau \leq 0$. 
%%
%%We will 
%%
%%Now consider the arrival of $e$. we will prove that the follow inequality holds 
%%for any moment $\tau$ during the loop corresponding to resource $e$ (note that $\beta_{e} = \frac{1}{\lambda} \nabla_{e} F(\vect{y})$ 
%%at time $\tau$). 
%%$$
%%\alpha_{i}	\geq  \frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d \lambda} 
%%		\left[ \exp\biggl( \ln \bigl(1+ d\overline{\rho} \bigr) 
%%				\cdot \sum_{e'} \overline{b}_{i,e'}  \cdot y_{e'}  \biggr) - 1 \right].
%%$$
%%
%%Recall that $F$ is a linear extension, so by its definition, $F$ is linear w.r.t any fixed $y_{e} $; hence 
%%$\nabla_{e} F(\vect{y})$ is independent of $y_{e} $. 
%%During the loop corresponding to resource $e$, only $y_{e} $ is modified whereas other $y_{e'} $'s for $e' \neq e$ remain unchanged.  
%%As $\nabla_{e} F(\vect{y})$ is independent of $y_{e} $ and other $y_{e'} $'s remain unchanged, 
%%it holds that $\partial \nabla_{e} F(\vect{y})/\partial \tau = 0$.
%Therefore, the derivative of the right hand side of the lemma inequality according to $\tau$ is at most
%\begin{align*}
%&\frac{\beta_{e}}{\max_{e'}  \overline{b}_{i,e'}  \cdot d } \cdot \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \overline{b}_{i,e}  \cdot
%		 \frac{\partial y_{e} }{\partial \tau} \cdot \exp \biggl( \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \sum_{e'} \overline{b}_{i,e'}  \cdot y_{e'}  \biggr) \\
%%
%\leq&  
%\frac{\beta_{e}}{\max_{e'}  \overline{b}_{i,e'}  \cdot d} \cdot \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \overline{b}_{i,e}  \cdot
%		 \frac{1}{\lambda \beta_{e} \cdot \ln(1+ d\overline{\rho} )}
%		 \cdot \biggl( \frac{\max_{e'} \overline{b}_{i,e'}  \cdot d \cdot \alpha_{i}}{ \beta_{e}} + 1 \biggr) \\
%%
%\leq& \frac{\overline{b}_{i,e}  \cdot \alpha_{i}}{\lambda \beta_{e}}  + \frac{1}{d \lambda} = \frac{\partial \alpha_{i}}{\partial \tau},
%\end{align*}
%%\begin{align*}
%%&\frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d \lambda } \cdot \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \overline{b}_{i,e}  \cdot
%%		 \frac{\partial y_{e} }{\partial \tau} \cdot \exp \biggl( \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \sum_{e'} \overline{b}_{i,e'}  \cdot y_{e'}  \biggr) \\
%%%
%%&\leq  
%%\frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d \lambda} \cdot \ln \bigl(1+ d\overline{\rho} \bigr) \cdot \overline{b}_{i,e}  \cdot
%%		 \frac{1}{\nabla_{e} F(\vect{y}) \cdot \ln(1+ d\overline{\rho} )}
%%		 \cdot \biggl( \frac{\max_{e'} \overline{b}_{i,e'}  \cdot d\lambda \cdot \alpha_{i}}{ \nabla_{e} F(\vect{y}) } + 1 \biggr) \\
%%%
%%&\leq \frac{\overline{b}_{i,e}  \cdot \alpha_{i}}{\nabla_{e} F(\vect{y})}  + \frac{1}{d \lambda} = \frac{\partial \alpha_{i}}{\partial \tau},
%%\end{align*}
%%
%where in the first inequality we use the induction hypothesis and the increasing rate of $y_{e}$.
%So the rate in the left-hand side is always larger than that in the right-hand side. 
%Hence, the lemma follows.
%\end{proof}


\begin{lemma}		\label{lem:packing-primal-feasible}
The primal variables constructed by the algorithm are feasible. 
\end{lemma}
%
\begin{proof}
%We prove the primal feasibility. 
First observe that if during the execution of the algorithm in the iteration related to some element $e$,  we have
$\sum_{e'} \overline{b}_{i,e'}  y_{e'}  > 1$ for some constraint $i$ then by Lemma~\ref{lem:bound-alpha},
$$
\alpha_{i} 
>  \frac{\nabla_{e} F(\vect{y})}{\max_{e'}  \overline{b}_{i,e'}  \cdot d\lambda } 
		\left[ \exp\biggl( \ln \bigl(1+ d\overline{\rho} \bigr)  \biggr) - 1 \right]
= \frac{\overline{\rho} \cdot \nabla_{e} F(\vect{y})}{\lambda \max_{e'}  \overline{b}_{i,e'} }
\geq \frac{\nabla_{e} F(\vect{y})}{\lambda \overline{b}_{i,e} }
$$
Therefore, $\sum_{i} \overline{b}_{i,e}  \alpha_{i} > \frac{1}{\lambda} \nabla_{e} F(\vect{y})$ and hence the algorithm would have 
stopped increasing $y_{e} $ at some earlier point. 
Therefore, every constraint $\sum_{e'} \overline{b}_{i,e'}  y_{e'}  \leq 1$ is always maintained during the execution of the algorithm. 
%(Note that by definition $\overline{b}_{i,e'} \geq b_{i,e'} \geq 0$ thus it also holds that $\sum_{i} b_{i,e'}  y_{e'}  \leq 1$.)

Secondly, we show primal feasibility (even when the prediction oracle provides an infeasible solution).
If the prediction oracle provides a feasible solution, we set $S_1=\{e: x^\pred_e=1\}$ and $S_2=\emptyset$.
Otherwise, let $e^{*}$ be the the first element for which the prediction oracle provides an infeasible solution. 
Let $S_{1}$ be the set of all resources $e$ such that $x^{\pred}_{e} = 1$ and $e$ is released before $e^{*}$.
Further let
$S_{2} = \{e: x^{\pred}_{e} = 1\} \setminus S_{1}$.
For every constraint $i$, 
\begin{align*}
\sum_{e} b_{i,e} x_{e} 
&= \sum_{e: x^{\pred}_{e} = 1} b_{i,e} x_{e} +  \sum_{e: x^{\pred}_{e} = 0} b_{i,e} x_{e} \\
%
&= \sum_{e \in S_{1}}  b_{i,e} x_{e} +  \sum_{e \in S_{2}} b_{i,e} x_{e}  +   \sum_{e: x^{\pred}_{e} = 0} b_{i,e} x_{e} \\
%
&\leq \frac{1}{1+\eta} \cdot 1
+ \eta \cdot \sum_{e \in S_{2}} \overline{b}_{i,e} x_{e} 
+ \eta \cdot \sum_{e: x^{\pred}_{e} = 0} \overline{b}_{i,e} x_{e} \\
%
&= \frac{1}{1+\eta}  
+ \frac{\eta}{1+\eta}  \sum_{e \in S_{2}} \overline{b}_{i,e} y_{e} 
+ \frac{\eta}{1+\eta}  \sum_{e: x^{\pred}_{e} = 0} \overline{b}_{i,e} y_{e} \\
%
&\leq \frac{1}{1+\eta}
+ \frac{\eta}{1+\eta}  \sum_{e} \overline{b}_{i,e} y_{e} \\
%
&\leq \frac{1}{1+\eta}
+ \frac{\eta}{1+\eta}  \cdot 1 = 1.
\end{align*}
%
The first inequality is due to: (1) the feasibility of the prediction restricted on $S_{1}$, i.e., 
$\sum_{e \in S_{1}} b_{i,e} x^{\pred}_{e} \leq 1$; and (2) 
$x_{e} = \frac{1}{1 + \eta} = \frac{1}{1 + \eta} x^{\pred}_{e}$
for $e \in S_{1}$; and (3) the definitions of $\overline{b}_{i,e}$ in Algorithm~\ref{algo:packing}.
The third equality follows by the algorithm: 
$x_{e} = \frac{1}{1 + \eta} \cdot y_{e}$
for $e \notin S_{1}$. 
The last inequality holds since $\sum_{e} \overline{b}_{i,e} y_{e} \leq 1$ by the observation made at the beginning of the lemma.
Hence, $\sum_{e} b_{i,e} x_{e} \leq 1$ for every constraint $i$.

Besides, by definition of $z_{S} = \prod_{e \in S} x_{e} \prod_{e \notin S} (1 - x_{e})$
where $0 \leq x_{e} \leq 1$ for all $e$, the identity $\sum_{S} z_{S} = 1$ always holds. 
In fact, if one chooses an element $e$ with probability $x_{e}$ then $z_{S}$ is the 
probability that the set of selected elements is $S$. So the total probability $\sum_{S} z_{S}$ must be 1.
Similarly, 
$\sum_{S: e \in S} z_{S} = x_{e} \sum_{S' \subset E \setminus \{e\}} \prod_{e' \in S'} x_{e'} \prod_{e' \notin S'} (1 - x_{e'}) = x_{e}$
since $\sum_{S' \subset E \setminus \{e\}} \prod_{e' \in S'} x_{e'} \prod_{e' \notin S'} (1 - x_{e'}) = 1$ (by the same argument). 

Therefore, the solution $(\vect{x}, \vect{z})$ is primal feasible.
\end{proof}

\begin{lemma}
The dual variables defined as above are feasible. 
\end{lemma}
%
\begin{proof}
The first dual constraint $\sum_{i} b_{i,e} \alpha_{i} \geq \beta_{e}$ is satisfied by the while loop condition of the algorithm
and the definition of $\beta_{e}$.
The second dual constraint $\gamma + \sum_{e \in S} \beta_{e} \geq f(\vect{1}_{S})$ reads
\begin{align*}
	\frac{1}{\lambda} \sum_{e \in S} \nabla_{e} F(\vect{y}) + \frac{\mu}{\lambda} F(\vect{y}) \geq F(\vect{1}_{S}), 
\end{align*}
which is, by arranging terms, exactly the $(\lambda, \mu)$-max-local smoothness of $F$. 
(Recall that $F(\vect{1}_{S}) = f(\vect{1}_{S})$.)
Hence, the lemma follows.
\end{proof}

%We are now ready to prove the main theorem. 

%%
%\begin{theorem}	\label{thm:packing}
%Assume that the multilinear extension is $(\lambda, \mu)$-max-locally-smooth.
%Then, the algorithm is $\Omega\bigl( \frac{\lambda}{2\ln(1+ d\rho/\eta ) + \mu} \bigr)$-competitive
%where ...
%\end{theorem}

\Packing*
%
\begin{proof}
\paragraph{Robustness.}
First, we bound the increases of $F(\vect{y})$ and of the dual objective value --- which we denote by $D$ --- at any time $\tau$ in the execution of 
Algorithm~\ref{algo:packing}.
The derivative of $F(\vect{y})$ with respect to $\tau$ is:
%
\begin{align*} %	\label{eq:packing-primal}
\nabla_{e} F(\vect{y}) \cdot \frac{d y_{e} }{ d \tau}
= \nabla_{e} F(\vect{y}) \cdot\frac{1}{\nabla_{e} F(\vect{y}) \cdot \ln(1+ d\overline{\rho})}
= \frac{1}{\ln(1+ d\overline{\rho})}
\end{align*}
%
Besides, the rate of the dual at time $\tau$ is:
\begin{align*}
\frac{d D}{d \tau} 
&=  \sum_{i} \frac{d \alpha_{i}}{d \tau} + \frac{d \gamma}{d \tau}  
= \sum_{i: \overline{b}_{i,e}  > 0} \biggl( \frac{\overline{b}_{i,e}  \cdot \alpha_{i}}{\nabla_{e} F(\vect{y})}  + \frac{1}{d\lambda} \biggr) 
		+ \frac{\mu}{\lambda} \frac{d F(\vect{y})}{d \tau}  \\
%
&= \sum_{i: \overline{b}_{i,e}  > 0} \frac{\overline{b}_{i,e}  \cdot \alpha_{i}}{\nabla_{e} F(\vect{y})}  +  \sum_{i: \overline{b}_{i,e}  > 0} \frac{1}{d\lambda}
		+ \frac{\mu}{\lambda} \cdot \frac{1}{\ln(1+ d\overline{\rho})} \\
%
&\leq \frac{2}{\lambda} + \frac{\mu}{\lambda \cdot \ln(1+ d\overline{\rho})}
=  \frac{2\ln(1+ d\overline{\rho}) + \mu}{\lambda \cdot \ln(1+ d\overline{\rho})},
\end{align*}
where the inequality holds since during the algorithm
$\sum_{i} \overline{b}_{i,e}  \cdot \alpha_{i} \leq  \frac{1}{\lambda} \nabla_{e} F(\vect{y})$.
Hence, the ratio between $F(\vect{y})$ and the dual $D$ is at least $\frac{\lambda}{2\ln(1+ d\overline{\rho}) + \mu}$.

Besides, $x_{e} = \frac{1}{1 + \eta} \geq \frac{1}{1 + \eta} y_{e}$ if $x^{\pred}_{e} = 1$ and the predictive solution is still feasible; and
 $x_{e} = \frac{1}{1 + \eta} y_{e}$ otherwise. Therefore, 
 $\vect{x} \geq \frac{\vect{y}}{1 + \eta} $ and so 
 $F(\vect{x}) \geq F\bigl(\frac{\vect{y}}{1 + \eta}\bigr)$ by 
 monotonicity\footnote{Note that this is the only step in the analysis we use the monotonicity of $f$, which implies the monotonicity of $F$.} 
 of $F$. 
 Hence the robustness is at least 
$$
\frac{F(\vect{x})}{F(\vect{y})} \cdot \frac{\lambda}{2\ln(1+ d\overline{\rho}) + \mu}
\geq \min_{\vect{0} \leq \vect{u} \leq \vect{1}} \frac{F(\frac{1}{1 + \eta} \vect{u})}{F(\vect{u})} \cdot \frac{\lambda}{2\ln(1+ d\rho/\eta ) + \mu} 
$$ 
where the latter is due to $\overline{\rho} \leq \rho/\eta$.

\paragraph{Consistency.} By our algorithm, for every element $e$, if $x^{\pred}_{e} = 1$ (and the prediction 
oracle provides a feasible solution) 
then $x_{e} = \frac{1}{1+\eta}$. Hence, the consistency of the algorithm 
$F(\vect{x})/F(\vect{x}^{\pred}) \geq F(\frac{\vect{x}^{\pred}}{1 + \eta})/F(\vect{x}^{\pred}) \geq r(\eta)$.
\end{proof}

%Note that the competitive ratio is 
%the same up to a constant factor as the performance guarantee for maximizing a linear function
%under packing constraints. Specifically, if function $f$ is linear then the smooth parameters are
%$\lambda = \mu = 1$.
%
%\paragraph{Remark.} One can define $\overline{b}_{e} = b_{e}/(\frac{1}{1+\eta})$ instead of 
%$\overline{b}_{e} = b_{e} (1 + \eta)$ but be careful when $\eta = 1$. One can assume that 
%$\eta \in (0,1)$.
\subsection{Applications}

\subsubsection{Applications to linear functions}
When the objective $f$ can be expressed as a monotone linear functions, its multilinear extension $F$ 
is $(1,1)$-locally-smooth. Moreover, $r(\eta) = 1/(1+\eta)$. Consequently, 
Algorithm \ref{algo:packing} provides a $1/(1 + \eta)$-consistent
and $O\bigl(1/\ln(1+ d \rho/\eta)\bigr)$-robust fractional solution for the
online linear packing problem.


\subsubsection{Applications to online submodular maximization}	\label{sec:sub-max}
Consider the online problem of maximizing a monotone submodular function subject to packing constraints. 
A set-function $f: 2^{\mathcal{E}} \rightarrow \mathbb{R}+$ is \emph{submodular} if
$f(S \cup e) - f(S) \geq f(T \cup e) - f(T)$ for all $S \subset T \subseteq \mathcal{E}$. 
Let $F$ be the multilinear extension of a monotone submodular function $f$. Function $F$
admits several useful properties: (i) if $f$ is monotone then so is $F$; (ii) $F$ is concave in any
positive direction, i.e., $\nabla F(\vect{x}) \geq \nabla F(\vect{y})$ for all $\vect{x} \leq \vect{y}$
($\vect{x} \leq \vect{y}$ means $x_{e} \leq y_{e} ~\forall e$). 

\begin{lemma}	\label{lem:sub-max-locally-smooth}
Let $f$ be an arbitrary monotone submodular function. Then, its multilinear extension 
$F$ is (1,1)-locally-smooth.
\end{lemma}
%
\begin{proof}
As $F$ is the linear extension of a submodular function, 
$\nabla_{e} F(\vect{x}) = \mathbb{E}_{R}\bigl[ f\bigl(\vect{1}_{R \cup \{e\}}\bigr) - f\bigl(\vect{1}_{R}\bigr) \bigr]$
where $R$ is a random subset of $\mathcal{E} \setminus \{e\}$ such that $e'$ is included with probability $x_{e'} $.
For any subset $S = \{e_{1}, \ldots, e_{\ell}\}$, we have
\begin{align*}
F(\vect{x}) + \sum_{e \in S} \nabla_{e} F(\vect{x}) 
&= F(\vect{x}) + \sum_{e \in S} \mathbb{E}_{R} \bigl[ f\bigl(\vect{1}_{R \cup \{e\}}\bigr) - f\bigl(\vect{1}_{R}\bigr) \bigr] \\
%
&= \mathbb{E}_{R} \biggl[ f(\vect{1}_{R}) + \sum_{e \in S} \bigl[ f\bigl(\vect{1}_{R \cup \{e\}}\bigr) - f\bigl(\vect{1}_{R}\bigr) \bigr] \biggl] \\
%
&\geq \mathbb{E}_{R} \biggl[ f(\vect{1}_{R}) + \sum_{i=1}^{\ell} \bigl[ f(\vect{1}_{R \cup \{e_{1}, \ldots, e_{i}\}}) - 
								f(\vect{1}_{R \cup \{e_{1}, \ldots, e_{i-1}\}}) \bigr] \biggr] \\
%
&= \mathbb{E}_{R} \bigl[ f(\vect{1}_{R \cup S})  \bigl]  \geq \mathbb{E}_{R} \bigl[ f(\vect{1}_{S})  \bigl] \\
%
&= F\bigl( \vect{1}_{S} \bigr)
\end{align*}
the first inequality is due to the submodularity $f$ and the second one due to its monotonicity.
The lemma follows.
\end{proof}



%\begin{lemma}
%Algorithm \ref{algo:packing} yields a  $O(\ln r(\mathcal{M}))$-competitive fractional solution where $r$ is the rank of 
%matroid $\mathcal{M}$.
%\end{lemma}

%The previous lemma and Theorem \ref{thm:packing} lead to the following result. 

\begin{proposition}	\label{prop:max-submodular}
For any $0 < \eta \leq 1$, Algorithm \ref{algo:packing} gives a $(1 - \eta)$-consistent
and $O\bigl(1/\ln(1+ d \rho/\eta)\bigr)$-robust fractional solution to
the problem of online submodular maximization under packing constraints.
\end{proposition}
%
\begin{proof}
We first bound $r(\eta) = \min_{\vect{0} \leq \vect{u} \leq \vect{1}} F(\frac{1}{1 + \eta} \vect{u})/F(\vect{u})$. 
By the non-negativity and the concavity in positive direction of $F$, for any set $S \subseteq \mathcal{E}$, 
%
\begin{align}
F\biggl(\frac{\vect{u}}{1 + \eta}  \biggr) 
&\geq F(\vect{u}) - \biggl \langle \nabla F\biggl(\frac{\vect{u}}{1 + \eta} \biggr), \vect{u} - \frac{\vect{u}}{1 + \eta} \biggr \rangle
\label{eq:max-sub-1}
\\
F\biggl(\frac{\vect{u}}{1 + \eta} \biggr) 
 &\geq F(\vect{0}) + \biggl \langle \nabla F\biggl(\frac{\vect{u}}{1 + \eta} \biggr), \frac{\vect{u}}{1 + \eta}\biggr \rangle
 \geq  \biggl \langle \nabla F\biggl(\frac{\vect{u}}{1 + \eta} \biggr), \frac{\vect{u}}{1 + \eta} \biggr \rangle.
\label{eq:max-sub-2}
\end{align}
Therefore,
\begin{align}
\frac{F\biggl(\frac{\vect{u}}{1 + \eta}\biggr)}{F(\vect{u})}
&\geq 1 - \frac{\eta}{1 + \eta} \cdot \frac{\biggl \langle \nabla F\biggl(\frac{\vect{u}}{1 + \eta}\biggr), \vect{u} \biggr \rangle}{F(\vect{u})} 	\tag{by (\ref{eq:max-sub-1})}\\
%
&\geq 
1 - \frac{\eta}{1 + \eta} \cdot \frac{\biggl \langle \nabla F\biggl(\frac{\vect{u}}{1 + \eta}\biggr), \vect{u} \biggr \rangle}{F\biggl(\frac{\vect{u}}{1 + \eta}\biggr)}  	\tag{by monotonicity of $F$}\\
%
&\geq 
1 - \frac{\eta}{1 + \eta} \cdot \frac{\biggl \langle \nabla F\biggl( \frac{\vect{u}}{1 + \eta} \biggr), \vect{u} \biggr \rangle}{\biggl \langle \nabla F\biggl(\frac{\vect{u}}{1 + \eta}\biggr), \frac{\vect{u}}{1 + \eta}\biggr \rangle}
	\tag{by (\ref{eq:max-sub-2})} \\
&= 1 - \eta. 	\notag
\end{align}
So, $r(\eta) \geq 1 - \eta$. Therefore, the proposition holds by Theorem~\ref{thm:packing} and by the (1,1)-locally-smoothness of $F$ (Lemma~\ref{lem:sub-max-locally-smooth}).
\end{proof}

One can derive online randomized algorithms for the integral variants of these problems by rounding the fractional solutions.
For example, using the online contention resolution rounding schemes \cite{FeldmanSvensson16:Online-contention}, 
one can obtain randomized algorithms for several specific constraint polytopes, for example, knapsack polytopes, 
matching polytopes and matroid polytopes. 



