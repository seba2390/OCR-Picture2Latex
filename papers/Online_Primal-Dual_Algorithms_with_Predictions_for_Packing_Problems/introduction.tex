%!TEX root = main.tex

\section{Introduction}


%% short online algorithm

Online computation \cite{BorodinEl-Yaniv05:Online-computation} is a
well-established field in theoretical computer science. In online
computation, inputs are released one-by-one in form of a request sequence.
Requests arrive over time and each request must be answered by the algorithm
with an irrevocable action, without any information about future requests to
come. These actions define a solution to the problem at hand, and the
algorithm wants to optimize its objective value. Usually this value is
normalized by the optimum, which could have been computed if all the requests
were known in beforehand.  The resulting \emph{competitive ratio} measures
the price of not knowing the future requests.

%% machine learning as a way to go beyond the worst case
Several models have been studied, which relax some of the assumptions in the standard online computation model, see  \cite{Roughgarden19:Beyond-worst-case,Roughgarden20:Beyond-the-Worst-Case} for a recent survey.

%% recent trends
One of the models is motivated by spectacular advances in machine learning (ML). 
In particular, the capability of ML methods in predicting patterns of future requests would provide useful information to be exploited by online 
algorithms. A general framework for incorporating ML predictions into algorithm design in order to achieve a better performance than the worst-case one is formally introduced by \cite{LykourisVassilvtiskii18:Competitive-caching}. 
This is rapidly followed by work studying online algorithms with predictions \cite{MitzenmacherVassilvitskii20:Beyond-the-Worst-Case} 
in a large spectrum of problems 
such as scheduling \cite{LattanziLavastida20:Online-scheduling,Mitzenmacher20:Scheduling-with}, 
caching (paging) \cite{LykourisVassilvtiskii18:Competitive-caching,Rohatgi20:Near-optimal-bounds,AntoniadisCoester20:Online-metric}, 
ski rental \cite{GollapudiPanigrahi19:Online-algorithms,KumarPurohit18:Improving-online,AngelopoulosDurr20:Online-Computation}, etc.  In contrast to online computation with advice (see \cite{Komm16:Introduction-to-Online} for a survey), the aforementioned models handle predictions in a careful manner, guaranteeing good performance even if the predictions is completely wrong.
More precisely, on the one hand, if the predictions are accurate, one expects to achieve a good performance 
as in the offline setting where the input are completely given in advance. On the other hand, if the predictions are misleading, one still has to maintain a competitive solution as in the online setting where no predictive information is available.  


%% unified methods/primal-dual 
This issue has been addressed (for example the aforementioned work) 
by subtly incorporating predictions and exploiting specific problems' structures.
The primal-dual method is an elegant and powerful technique for the design of algorithms \cite{WilliamsonShmoys11:The-design-of-approximation}, 
especially for online algorithms \cite{BuchbinderNaor09:Online-primal-dual}.
In the objective of unifying previous ad-hoc approaches,  
\cite{BamasMaggiori20:The-Primal-Dual-method} presented a primal-dual 
approach to design 
online algorithms with predictions for covering problems with a linear objective function. In this paper, we build on  previous work and  
provide a primal-dual method with predictions for packing problems with non-linear objectives;
following a research direction and answering some open questions mentioned  in \cite{BamasMaggiori20:The-Primal-Dual-method}.


%The first explicite primal-dual approach in online algorithms
%is given by Buchbinder and Naor [34] who presented general algorithms based on
%the multiplicative weights update method for covering/packing problems. 
%Several uses of the primal-dual method and competitive algorithms for online algorithms then followed; for example the generalized caching problem [17, 16], the ad-auction maximization [35] and others.  



%We measure the performance of an algorithm by the \emph{competitive ratio}. Specifically, 
%an algorithm is $r$-competitive if for any instance, the ratio between the cost of the algorithm and that of 
%an optimal solution is at most $r$.  
 
 
 
\subsection{Problem and Model}  	\label{sec:model}
 
\paragraph{Packing problem} In the packing problem studied in this paper we are given of a sequence of $n$ elements ${\cal E}$, and $m$ unit capacity resources, numbered from $1$ to $m$.  Each element $e\in\cal E$ has a profit $w_e$ and a resource requirement $b_{i,e}\geq 0$ for each resource $i$. The goal is to select a set of elements of maximum total profit, such that no resource capacity is exceeded.  Formally the goal is to chose a vector $\vect{x} \in\{0,1\}^{|\cal E|}$ maximizing $\sum_{e\in\cal E} w_e x_e$ such that for every resource $i\in\{1,\ldots,m\}$ we have $\sum_{e\in\cal E} b_{i,e} x_e \leq 1$.  

\paragraph{Fractional variant}
In the fractional variant of this problem, the integrality constraint on $x$ is removed, and the goal is to optimize over vectors $x\in[0,1]^{|\cal E|}$.
By $I$ we denote the instance of a packing problem, which consists of $\langle {\cal E}, (w_e)_{e\in \cal E}, m, (b_{i,e})_{1\leq i\leq m, e\in\cal E}\rangle$.

\paragraph{Non-linear variant} We consider a generalization of this problem, where the objective is not to maximize the sum $\sum_{e\in\cal E} w_e x_e$, but $f(\vect{x})$ for a given function $f:\{0,1\}^{|\cal E|} \rightarrow {\mathbb R}^+$.  It is assumed that $f$ is monotone, in the sense that $f(\textbf{x})\leq f(\textbf{y})$, whenever $\textbf{y}$ equals $\textbf{x}$ except for a single element $e$ with $x_e=0$ and $y_e=1$. Describing $f$ explicitly needs space $O(2^{|{\cal E}|})$, hence it is assumed that $f$ is given as a black-box oracle.  For the fractional variant, the multilinear extension $F:[0,1]^{|\cal E|} \rightarrow {\mathbb R}^+$ is used instead of $f$, which is defined as
\[
    F(\vect{x}) = \sum_{S\subseteq \cal E} \prod_{e\in S} x_e \prod_{e\not\in S}(1-x_e) f(\vect{1}_S),
\]  
where $\vect{1}_S$ is the characteristic vector of the set $S$. 
Alternatively, $F(\vect{x}) = \mathbb{E} \bigl[ f(\vect{1}_{T})\bigr]$ where $T$ is a random set 
such that every element $e$ appears independently in $T$ with probability $x_{e}$. 
Note that $F(\vect{1}_{S}) = f(\vect{1}_{S})$ for every $S \subseteq \mathcal{E}$ --- $F$ is truly an extension of $f$ --- and if $f$ is monotone then so is $F$, in the sense that $F(\vect x)\leq F(\vect y)$ for all vectors $\vect x,\vect y\in[0,1]^{|\cal E|}$ such that $x_e \leq y_e$ for all elements $e$.

\paragraph{Online packing problem.} In the paper, we consider the model presented in \cite{BamasMaggiori20:The-Primal-Dual-method} 
(with several definitions rooted in \cite{LykourisVassilvtiskii18:Competitive-caching,KumarPurohit18:Improving-online}). In the online packing problem, elements $e\in\cal E$ arrive online. When element $e\in\cal E$ arrives, the algorithm learns its resource requirements $b_{i,e}$. At the arrival of $e$ only $x_e$ can be set by the algorithm, and cannot be changed later on.  Only $m$, the number of resources, are initially known to the algorithm.  The algorithm does not know $\cal E$ initially, not even its cardinality.  Formally an online algorithm $\cal A$ receives the elements of $\cal E$ in arbitrary order, and upon reception of element $e\in\cal E$ needs to choose a value for $x_e$.  At any moment, the vector $\vect x$ must satisfy all $m$ resource capacity constraints of the packing problem.  

We denote by ${\cal A}(I)$ the objective value of the solution produced by $A$ on an instance $I$.  A prediction is an integral value $x^{\pred}_{e}\in\{0,1\}$, which is given to the algorithm with every element $e$.  Confidence in the prediction is described by a parameter $\eta \in (0,1]$. For  convenience we choose larger $\eta$ value to represent smaller confidence.  Similarly to ${\cal A}(I)$, we denote by ${\cal P}(I)$ the objective value of the vector $\vect{x}^{\pred}$ produced by the prediction, and set ${\cal P}(I)=0$ whenever $\vect{x}^{\pred}$ does not satisfy all constraints. Finally we denote by ${\cal O}(I)$ the optimal fractional solution to the packing problem, which could have been computed if the whole instance $I$ were available from the beginning.


Algorithm $\mathcal{A}$ is 
 $c(\eta)$-\emph{consistent} and  $r(\eta)$-\emph{robust} if for every instance $I$, 
 \begin{align*}
 	\mathcal{A}(I) 	\geq 	\max\{c(\eta) \cdot \mathcal{P}(I), r(\eta) \cdot \mathcal{O}(I) \}.
 \end{align*}
Ideally we would like that $c(\eta)$ tends to $1$, when $\eta$ approaches $0$, meaning that with high confidence, the algorithm performs at least as good as the prediction. Additionally we would like that $r(\eta)$ tends to the same competitive ratio as in the standard online setting (without provided predictions), when $\eta$ approaches $1$.
  
\paragraph{Objective function}
At any moment in time, the algorithm maintains a vector $\vect{x}$ over the arrived elements. Conceptually this vector is padded with zeros to create a vector belonging to $[0,1]^{|\cal E|}$, which for  convenience we also denote by $\vect x$.  %We assume that the algorithm can compute the derivative $\nabla_{e} F(\vect{x})$ in constant time.
  
 %%%%%************************************
 %%%%%************************************
\subsection{Approach and Contribution}  
  
We use the primal-dual method in our approach.  
At a high level, it formulates a given problem as a (primal) linear program.  There is a corresponding dual linear program, which plays an important role in the method.  By weak duality, a solution to the dual bounds the optimal solution to the primal.  Every arrival of an element in the online problem, translates in the arrival of a variable in the primal linear program, and a corresponding constraint in the dual linear program. The algorithm updates the fractional solutions to both the primal and 
dual in order to maintain their feasibility. Then, the competitive ratio of the algorithm is established 
by showing that every time an update of primal and dual solutions is made, the increase of the primal 
can be bounded by that of the dual up to some desired factor.   
 
Note that, similar to \cite{BamasMaggiori20:The-Primal-Dual-method}, we focus on constructing fractional solution 
which is the main step in the primal-dual method. In order to derive algorithms for specific problems, online rounding schemes 
are needed and in many problems, such schemes already exist. We will provide references for the rounding schemes
tailor designed for different applications, but do not develop them in this paper.

We follow the primal-dual framework presented in \cite{Thang20:Online-Primal-Dual} to design competitive algorithms for fractional non-linear packing problems.
Given a function $f: \{0,1\}^{n} \rightarrow \mathbb{R}^{+}$, its \emph{multilinear extension} $F: [0,1]^{n} \rightarrow \mathbb{R}^{+}$ 
is defined as $F(\vect{x}) := \sum_{S \subseteq \mathcal{E}} \prod_{e \in S} x_{e} \prod_{e \notin S} (1 - x_{e}) \cdot f(\vect{1}_{S})$
where $\vect{1}_{S}$ is the characteristic vector of $S$ (i.e., the $e^{\textnormal{th}}$-component of 
$\vect{1}_{S}$ equals $1$ if $e \in S$ and equals 0 otherwise). 

The performance of an algorithm will depend on the following measures on the objective function.  

\begin{definition}[\cite{Thang20:Online-Primal-Dual}]	\label{def:max-local-smooth}
A differentiable function $F: [0,1]^{|\cal E|} \rightarrow \mathbb{R}^{+}$ is \emph{$(\lambda,\mu)$-locally-smooth} for some parameters $\lambda,\mu\geq 0$
if for any set $S \subseteq \mathcal{E}$, and for every vector $\vect{x} \in [0,1]^{|\cal E|}$, 
the following inequality holds:
$$
\langle \nabla F(\vect{x}), \vect{1}_{S} \rangle = 
\sum_{e \in S} \nabla_{e} F(\vect{x}) \geq \lambda F\bigl( \vect{1}_{S} \bigr) - \mu F\bigl( \vect{x} \bigr).
$$
where $\nabla_{e} F(\vect{x})$ denotes $\partial F(\vect{x})/\partial x_{e}$.
\end{definition}

Note that the $(\lambda,\mu)$-smoothness notion here differs from the usual notion of smoothness of functions in
convex optimization. The former, introduced in \cite{Thang20:Online-Primal-Dual}, is related to the definition of 
smooth games \cite{Roughgarden15:Intrinsic-Robustness}   
in the context of algorithmic game theory. The parameters $(\lambda,\mu)$ somehow describe how far 
the function is from being linear. For example a non-decreasing linear function is $(1,1)$-locally-smooth.  Another example is the multilinear extension of monotone submodular functions, which is $(1,2)$-locally-smooth \cite{Thang20:Online-Primal-Dual}.

The dual of a linear packing problem is a linear covering problem and vice-versa.  However this breaks when considering non-linear objective functions.  The usual trick is to linearize the model, by introducing auxiliary variables and work with what is called a configuration linear program. Now, how are we going to exploit the prediction? 
In our approach, we incorporate the predictive information in the primal-dual approach by modifying the coefficients of the constraints.  This in turn influences the increase of the primal variables and hence the solution produced by the algorithm.

This approach guarantees that:  
(i) if the confidence on the prediction is high ($\eta$ is close to 0) then value of variables corresponding to the elements selected by the prediction would get a large value; (ii) and inversely, if the confidence on the prediction is low ($\eta$ is close to 1) then
the variables would be constructed as in the standard online primal-dual method. 
The construction follows the multiplicative weights update based on the gradient of the multilinear extension \cite{Thang20:Online-Primal-Dual},
which generalizes the multiplicative update in \cite{BuchbinderNaor09:The-Design-of-Competitive,AzarBuchbinder16:Online-Algorithms}.  
Using the concept of local-smoothness, we show the feasibility of our primal/dual solutions (even when the prediction provides an infeasible one)  and also analyze the performance of the algorithm, as a function of locally-smoothness and confidence parameters.


\begin{restatable}{theorem}{Packing}
\label{thm:packing}
Let $F$ be the multilinear extension of the non-decreasing objective function $f$. Denote the \emph{row sparsity}
$d := \max_{i} |\{b_{ie}: b_{ie} > 0\}|$ and the \emph{divergence}
$\rho := \max_{i} \max_{e,e': b_{ie' > 0}} b_{ie} / b_{ie'}$.  
Assume that $F$ is $(\lambda, \mu)$-locally-smooth 
for some parameters $\lambda > 0$ and $\mu$. 
Then, for every $0 < \eta \leq 1$, 
there exists a $r(\eta)$-consistent and $r(\eta) \cdot \bigl( \frac{\lambda}{2\ln(1+ d\rho/\eta) + \mu} \bigr)$-robust algorithm
for the fractional packing problem where $r(\eta) = \min_{\vect{0} \leq \vect{u} \leq \vect{1}} F(\frac{\vect{u}}{1 + \eta} )/F(\vect{u})$.
\end{restatable}


In the following, we describe the applications of our framework to different classes of problems.
\begin{itemize}
\item \textbf{Linear objectives.} 
Numerous combinatorial problems can be formalized by linear programs with packing constraints
to which our framework applies.  
When  $f$ is  a monotone linear function, the smooth parameters of $F$ are
$\lambda = 1$ and $\mu = 1$. In this case, our algorithm guarantees the consistency $C(\eta) = \frac{1}{1 + \eta}$
and the robustness $R(\eta) =  \frac{\Omega(1)}{2(1 + \eta)\ln(1+ d\rho/\eta)}$. Note that in this case, the multilinear extension $F$ is rather easy to compute, as it satisfies $F(\vect x) = \sum_{e\in\cal E} x_e f(\vect 1_{\{e\}})$.
\item
\textbf{Submodular objectives.}
Submodular maximization constitutes a major research agenda and has been widely studied 
in optimization, machine learning \cite{Bachothers13:Learning-with,KrauseGolovin14:Submodular-Function,BianLevy17:Non-monotone-Continuous} 
and algorithm design \cite{FeldmanNaor11:A-unified-continuous,BuchbinderFeldman15:A-tight-linear}.
Using our framework, we derive an algorithm that yields a $(1 - \eta)$-consistent
and $\Omega \bigl( \frac{1}{2\ln(1+ d\rho/\eta)} \bigr)$-robust fractional solution for online submodular maximization 
with packing constraints. This performance is similar to that of linear functions up to constant factors. 
Note that using the online contention resolution rounding schemes \cite{FeldmanSvensson16:Online-contention},
generalizing the offline counterpart \cite{ChekuriVondrak14:Submodular-function}, 
one can obtain randomized algorithms for several specific constraint polytopes such as knapsack polytopes, 
matching polytopes and matroid polytopes. 
\end{itemize}

\paragraph{Ad-auctions problem.} 
An interesting particular packing problem is the ad-auctions revenue maximization.  
Informally, one needs to allocate ads in an online manner to advertisers with their budget constraints 
in order to maximize the total revenue from allocated ads. Any algorithm improving the ad allocation even by a small fraction 
would have a non-negligible impact. Building on the salient ideas of our framework, we give 
a $(1-\eta)$-consistent and $(1 - \eta e^{-\eta})$-robust algorithm for this problem. 
% This is the optimal trade-off between the consistency and robustness, because 
% any $(1 - \eta)$-consistent algorithm must have robustness at most $(1 - \eta e^{-\eta})$.  
 
 
\subsection{Related work}
The primal-dual method is a powerful tool for online computation.

A primal-dual framework for linear programs with packing/covering constraints was given in \cite{BuchbinderNaor09:The-Design-of-Competitive}.
Their method unifies several previous potential-function-based analyses and give a 
principled approach to design and analyze algorithms for problems with linear relaxations. 
A framework for covering/packing problems with 
convex/concave objectives whose gradients are monotone was provided by \cite{AzarBuchbinder16:Online-Algorithms}. Subsequently, 
\cite{Thang20:Online-Primal-Dual} presented algorithms dealing with non-convex functions 
and established the competitive ratio as a function of the smoothness
parameters of the objective function. The smoothness notion introduced in \cite{Thang20:Online-Primal-Dual} has rooted in smooth games 
defined by \cite{Roughgarden15:Intrinsic-Robustness} in the context of algorithmic game theory.

% Online algorithms with predictions
The domain of algorithms with predictions \cite{MitzenmacherVassilvitskii20:Beyond-the-Worst-Case}, 
or learning augmented algorithms, has recently emerged
and rapidly grown at the intersection of (discrete) algorithm design and machine learning (ML). The idea is to incorporate learning predictions,
together with ML techniques into algorithm design, in order to achieve performance guarantees beyond 
the worst-case analysis and provide specifically adapted solutions to different problems. 
Interesting results have been shown over a large spectrum of problems such as
 scheduling \cite{LattanziLavastida20:Online-scheduling,Mitzenmacher20:Scheduling-with}, 
 caching (paging) \cite{LykourisVassilvtiskii18:Competitive-caching,Rohatgi20:Near-optimal-bounds,AntoniadisCoester20:Online-metric}, 
 ski rental \cite{GollapudiPanigrahi19:Online-algorithms,KumarPurohit18:Improving-online,AngelopoulosDurr20:Online-Computation}, 
 counting sketches \cite{HsuIndyk19:Learning-Based-Frequency}, 
 bloom filters \cite{KraskaBeutel18:The-case-for-learned,Mitzenmacher18:A-model-for-learned}, etc. 
 Recently, \citet{BamasMaggiori20:The-Primal-Dual-method} have proposed a primal-dual approach to design online algorithms with predictions for linear problems with covering constraints. In this paper, by combining their ideas and the ideas from 
 \cite{BuchbinderNaor09:The-Design-of-Competitive,AzarBuchbinder16:Online-Algorithms,Thang20:Online-Primal-Dual},
 we present a primal-dual framework for more general problems with non-linear objectives and 
 packing/covering constraints (motivated by a question mentioned in \cite{BamasMaggiori20:The-Primal-Dual-method}). 
 
 % Matching
 Online matching and ad-auctions have been widely studied (see \cite{Mehta13:Online-Matching} and references therein). 
 Motivated by Internet advertising applications, 
 several works have considered the ad-auctions problem in various settings where forecast/prediction is available/learnable. 
 \citet{EsfandiariKorula18:Allocation-with} considered a model in which the input is stochastic and a forecast for the future items is given.       
The accuracy of the forecast is intuitively measured by the fraction of the value of an optimal solution that can be obtained from the stochastic input.
They provide algorithms with provable bounds in this setting. \citet{SchildVee19:Semi-Online-Bipartite} introduced 
the semi-online model in which unknown future has a predicted part and an 
adversarial part. They gave algorithms with competitive ratios depending on the fraction of the adversarial part (in the input). Closely related to our work is the model by \citet{MahdianNazerzadeh12:Online-Optimization}
in which given two algorithms, one needs to design a (new) algorithm which is robust to both given algorithms. They derived an algorithm for ad-auctions problems
that achieves a fraction of the maximum revenue of the given algorithms. The main difference to ours model is that their algorithm is \emph{not} robust in case a given algorithm provides infeasible solutions (which would happen for the prediction) whereas our algorithm is.  
 
 
  


%TODO: add references to online matching with prediction