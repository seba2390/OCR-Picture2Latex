
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% import packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[pdftex]{graphicx}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{multicol}

% algorithm
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\SetKwInput{KwInitialize}{Initialize}

\usepackage{bm}
\usepackage{bbm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[nameinlink]{cleveref}

% hyperlink color (citations)
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=myblue,
	citecolor=myblue,
}

\usepackage[rightcaption]{sidecap}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% new commands

% color
%\definecolor{myblue}{RGB}{0,0, 191}
%\definecolor{myblue}{RGB}{0,0, 177}
\definecolor{myblue}{RGB}{0,0, 152}

% theorems
\usepackage{amsthm}
\usepackage{thmtools, thm-restate}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{definition}{Definition}[section]
\renewcommand{\qedsymbol}{$\blacksquare$}

\crefname{assumption}{assumption}{assumptions}
\Crefname{assumption}{Assumption}{Assumptions}
\crefname{definition}{definition}{definitions}
\Crefname{definition}{Definition}{Definitions}
\crefname{proposition}{proposition}{propositions}
\Crefname{proposition}{Proposition}{Propositions}
\crefname{lemma}{lemma}{lemmas}
\Crefname{lemma}{Lemma}{Lemmas}
\crefname{theorem}{theorem}{theorems}
\Crefname{theorem}{Theorem}{Theorems}

\newenvironment{proofsketch}{%
  \renewcommand{\proofname}{Proof sketch}\proof}{\endproof}

%variables
\newcommand{\R}{\mathbb{R}} %real numbers
\newcommand{\N}{\mathbb{N}} %natural numbers
\newcommand{\C}{\mathbb{C}} %complex numbers
\newcommand{\E}{\mathbb{E}} %expectation

\newcommand{\W}{{\bm W}}
\newcommand{\A}{{\bm A}}
\newcommand{\D}{{\bm D}}
\newcommand{\Z}{{\bm Z}}
\newcommand{\Xx}{{\bm X}}
\newcommand{\G}{{\bm G}}
\newcommand{\eye}{{\bm I}}
\newcommand{\J}{{\bm J}}
\newcommand{\boldbeta}{{\bm \beta}}

\newcommand{\x}{{\bm x}}
\newcommand{\w}{{\bm w}}
\newcommand{\z}{{\bm z}}
\newcommand{\g}{{\bm g}}
\newcommand{\vvec}{{\bm v}}
\newcommand{\zero}{{\bm 0}}

\newcommand{\Loss}{\mathcal{L}}
\newcommand{\loss}{\ell}
\newcommand{\risk}{\mathcal{R}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\prox}{\mathcal{P}}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\usepackage[numbers]{natbib}

\usepackage{url}


\title{Stable and Interpretable Unrolled Dictionary Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \tmlrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{\name Bahareh Tolooshams \email btolooshams@seas.harvard.edu \\
       \name Demba Ba \email demba@seas.harvard.edu \\
       \addr School of Engineering and Applied Sciences\\
       Harvard University
       }

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\def\month{MM}  % Insert correct month for camera-ready version
%\def\year{YYYY} % Insert correct year for camera-ready version
%\def\openreview{\url{https://openreview.net/forum?id=TODO}} % Insert correct link to OpenReview for camera-ready version

\def\month{08}  % Insert correct month for camera-ready version
\def\year{2022} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=e3S0Bl2RO8}} % Insert correct link to OpenReview for camera-ready version





\begin{document}

\maketitle
% \vspace{-4mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The dictionary learning problem, representing data as a combination of a few atoms, has long stood as a popular method for learning representations in statistics and signal processing. The most popular dictionary learning algorithm alternates between sparse coding and dictionary update steps, and a rich literature has studied its theoretical convergence. The success of dictionary learning relies on access to a ``good'' initial estimate of the dictionary and the ability of the sparse coding step to provide an unbiased estimate of the code. The growing popularity of unrolled sparse coding networks has led to the empirical finding that backpropagation through such networks performs dictionary learning. We offer the theoretical analysis of these empirical results through PUDLE, a Provable Unrolled Dictionary LEarning method. We provide conditions on the network initialization and data distribution sufficient to recover and preserve the support of the latent code. Additionally, we address two challenges; first, the vanilla unrolled sparse coding computes a biased code estimate, and second, gradients during backpropagated learning can become unstable. We show approaches to reduce the bias of the code estimate in the forward pass, and that of the dictionary estimate in the backward pass. We propose strategies to resolve the learning instability by tuning network parameters and modifying the loss function. Overall, we highlight the impact of loss, unrolling, and backpropagation on convergence. We complement our findings through synthetic and image denoising experiments. Finally, we demonstrate PUDLE's interpretability, a driving factor in designing deep networks based on iterative optimizations, by building a mathematical relation between network weights, its output, and the training set.
\end{abstract}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
This paper\footnote{Source code is available at \url{https://github.com/btolooshams/stable-interpretable-unrolled-dl}} considers the dictionary learning problem, namely representing data $\x \in \X \subset \R^m$ as linear combinations of a few atoms from a dictionary $\D \in \Dcal \subset \R^{m \times p}$. Given $\x$ and $\D$, the problem of recovering the sparse (few non-zero elements) coefficients $\z \in \R^p$ is referred to as sparse coding, and can be solved through the lasso~\citep{tibshirani1996lasso} (also known as basis pursuit~\citep{chen2001atomic}):
\begin{equation}\label{eq:lasso}
\begin{array}{c}
\loss_{\x}(\D) \coloneqq \min_{\z \in \R^p}\ \Loss_{\x}(\z, \D) + h(\z)
\end{array}
\end{equation}
where $\Loss_{\x}(\z, \D) = \frac{1}{2} \| \x - \D \z \|_2^2$, and $h(\z) = \lambda \| \z \|_1$. Specifically, the problem aims to recover a dictionary $\D^*$ that generates the data, i.e.,
\begin{equation}\label{eq:gen}
\begin{array}{c}
\x =  \D^{\ast} \z^{\ast}
\end{array}
\end{equation}
where $\z^{\ast}$ is sparse. Olshausen and Field~\citep{olshausen1997sparse} introduced~\eqref{eq:gen} in computational neuroscience as a model for how early layers of the visual cortex process natural images. Sparse coding has been widely studied and utilized in the statistics~\citep{hastie2015statistical} and signal processing communities~\citep{elad2010sparse}. A few practical examples are denoising~\citep{elad2016denoise}, super-resolution~\citep{yang2010superres}, text processing~\citep{jenatton2011proximal}, and classification~\citep{mairal2009supdl}, where it enables the extraction of sparse high-dimensional features representing data. Moreover, sparse modelling is ubiquitous in many other fields such as seismic signal processing~\citep{filho2018seismic}, radar sensing for target detections~\citep{bajwa2011radar}, and astrophysics for image reconstruction from interferometric data~\citep{akiyama2017imaging}. Furthermore, \citet{cleary2017trans, cleary2021compressed} use this model to learn a dictionary consisting of gene modules for efficient imaging transcriptomics.

Sparse coding has been utilized to construct neural architectures through approaches such as sparse energy-based models~\citep{ranzato2007sparsenergy, ranzato2008sparsedbn} or recurrent sparsifying encoders~\citep{gregor2010lista}. The latter has initiated a growing literature on constructing interpretable deep networks based on an approach referred to as algorithm unrolling~\citep{hershey2014unfold, monga2019algorithm}. Deep unrolled neural networks have gained popularity as inference maps in recent years due to their computational efficiency and their performance in various domains such as image denoising~\citep{simon2019rethinking, tolooshams2020tnnls, tolooshams2020icml}, super-resolution~\citep{wang2015supersparse}, medical imaging~\citep{solomon2020deepPCA}, deblurring~\citep{schuler2016deblur, li2020deblur}, radar sensing~\citep{tolooshams2021unfolding}, and speech processing~\citep{hershey2014unfold}.
%
%%%%%%%%%%%%%%%%%%
%%%
\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/network-v5.png}
	\caption{Provable unrolled dictionary learning (PUDLE): Unrolled network architecture with dictionary $\D$.}
	\vspace{-4mm}
	\label{fig:network}
	\vspace{-2mm}
\end{figure}
%%%
%

Prior to the advent of unrolled networks, gradient-based dictionary learning relied on analytic gradients computed from the lasso given the sparse code. With unrolled networks, automatic differentiation~\citep{baydin2018automatic}, referred to as backpropagation~\citep{lecun2012efficient} in the reverse-mode, gained attention for parameter estimation~\citep{tolooshams2018mlsp}. The automatic gradient is obtained by backpropagation through the algorithm used to estimate the code. Automatic differentiation in reverse and forward-mode~\citep{franceschi2017forward} is used in other areas, e.g., hyperparameter selection~\citep{feurer2019hyperparameter}, and in a more relevant context, in the seminal work of LISTA~\citep{gregor2010lista}. Other works demonstrated empirically the convergence of $\ell_1$-based dictionary learning by backpropagation through unrolled networks~\citep{tolooshams2020tnnls}. Given finite computational power,~\citet{tolooshams2020tnnls} convert sparse coding into an encoder by unrolling $T$ iterations of ISTA~\citep{daubehies2004ista, blumensath2008ista}, and attach to it a linear decoder for reconstructing. Unrolled networks obtained in this manner suffer from two important limitations.

First, the sparse coding step in the forward pass computes a biased estimate of the code. This results, in turn, in a biased estimate of the backward gradient and, hence, a degradation of dictionary recovery performance. Second, as studied recently~\citep{malezieux2022understanding}, inaccuracies in the early iterations of the unrolled network make backpropagation  unstable. We address both of these shortcomings in this paper. Moreover, while~\citet{malezieux2022understanding} analyze the gradient computed by backpropagation through unrolled sparse-coding networks, there is no known theoretical analysis of how weight updates using this gradient impact the recovery of a ground-truth code $\z^{\ast}$, nor of their convergence to a ground-truth dictionary $\D^{\ast}$.

This paper proposes a Provable Unrolled Dictionary LEarning (PUDLE) (\Cref{fig:network}). We aim to recover $\D^{\ast}$ by training the network using backpropagation with a learning rate of $\eta$. Three different choices affect the gradient: the number of unrolled iterations, the loss, and whether one backpropagates through the decoder only or through both the encoder and decoder. We highlight the impact of such choices on the convergence of the training algorithm. Backpropagation through the decoder results in the analytic gradient $\g_t^{\text{dec}}$ using the code estimate $\z_t$. The gradients $\g_t^{\text{ae-lasso}}$ and $\g_t^{\text{ae-ls}}$ are computed by backpropagation through the autoencoder using the lasso and least-squares objectives, respectively (\Cref{algo:unfolded}). We compare the gradients with the classical gradient-based alternating-minimization algorithm for dictionary learning~\citep{chatterji2017alternating} (i.e., cycling between sparse coding and dictionary update steps using the analytic gradient $\hat \g$ (\Cref{algo:altmin})), and provide a theoretical analysis of gradient-based recovery of the dictionary $\D^{\ast}$. We provide sufficient conditions under which the gradient computation, hence the learning, is stable. Additionally, we show how using the reconstruction loss with backpropagation not only does not suffer from backpropagated instability but also ameliorates the propagation of the forward pass bias into the gradient estimate from the backward pass. Finally, we demonstrate the interpretability of the unrolled network. Our contributions are:
%
% \subsection{Contributions}
%
\begin{itemize}[noitemsep, topsep=0pt, leftmargin=12pt]
\item {\bf Unrolled sparse coding}\quad Unlike prior work~\citep{malezieux2022understanding} that studies the ability of sparse coding to recover the \emph{solution of the lasso~\eqref{eq:lasso}} given the current estimate of the dictionary (we call this local estimation), we study unrolled sparse coding for recovery of the \emph{true generating code in~\eqref{eq:gen}} (we call this global estimation). We provide sufficient conditions on the network and data distributions such that the forward pass recovers (\Cref{thm:supprec}) and preserves (\Cref{thm:supppres}) the correct code support. Assuming support identification, we show the linear convergence of the code estimated through the unrolled iterations to the solution of the lasso (\Cref{thm:fwdz}). We provide an explicit code expression at unrolled layer $t$ and its error with respect the ground-truth code $\z^{\ast}$; we highlight the biased estimate of the code when the forward pass strictly solves lasso (\Cref{thm:fwdzerrorvariable,thm:fwdzerrorfixed}). Moreover, in a more general scenario, we show that the error in the code estimate is upper bounded by two terms, i.e., one associated with the dictionary error and the other to the bias of the estimate of code amplitude, due to $\ell_1$-based optimization (\Cref{thm:fwdzglobal}). The latter highlights that vanilla lasso ($\ell_1$-based) sparse coding computes a biased estimate of codes, and below we discuss strategies to either alleviate this bias in the forward pass or mitigate its propagation into the backward pass for dictionary learning.
%
\item {\bf Mitigation of coding bias propagation into dictionary learning}\quad We study gradient estimation for dictionary learning in PUDLE. We decompose the upper bound on the gradient errors compared to the gradient direction to recover $\D^{\ast}$ into terms involving the current dictionary error, the bias of the code estimate, and the lasso loss used to compute the gradient. We show that using only the reconstruction loss while backpropagating (i.e., $\g_t^{\text{ae-ls}}$) results in the vanishing of the upper bound due to the usage of lasso loss. This means that given fixed $\lambda$, $\g_t^{\text{ae-ls}}$ ameliorates the propagation of the forward pass bias into the backward pass. Specifically, we show that $\g_t^{\text{ae-ls}}$ is a better estimator of the direction to recover $\D^{\ast}$ than $\g_t^{\text{dec}}$ and $\g_t^{\text{ae-lasso}}$. Hence, weight updates using $\g_t^{\text{ae-ls}}$ converges to a closer neighbourhood of $\D^{\ast}$ (\Cref{thm:globalgradient}). In a supervised image denoising task, we show that the advantage of $\g_t^{\text{ae-ls}}$ goes beyond dictionary learning; $\g_t^{\text{ae-ls}}$ results in better image denoising compared to $\g_t^{\text{dec}}$. Furthermore, our network outperforms the sparse coding scheme in NOODL, a state-of-the-art online dictionary learning algorithm~\citep{rambhatla2018noodl} (\Cref{tab:psnr}). Moreover, we show that the bias in the estimate of $\D^{\ast}$ vanishes as $\lambda_t = \lambda \nu^t$ (with $0 <\nu < 1$) decays within the forward unrolled layers (\Cref{fig:baslines_full}). This strategy, supported by \Cref{thm:fwdzerrorvariable}, results in an unbiased estimate of the code $\z^{\ast}$ and recovery of $\D^{\ast}$ (\Cref{thm:dictvariabledec}).
%
\item {\bf Stability of unrolled learning}\quad Our approach to resolve the instability issue of backpropagation in unrolled networks is two-fold. First, we show that under proper dictionary initialization, the instability of the gradient $\g_t^{\text{ae-lasso}}$ computation, studied by~\citet{malezieux2022understanding}, as $T$ increases is resolved. We give a condition under which the code support is identified and recovered after one iteration and, hence, gradient computation stays stable. Second, in the absence of support identification in early iterations, we propose to use the gradient $\g_t^{\text{ae-ls}}$ which resolves the stability issue introduced by lasso loss in the backward pass. We highlight this stability through image denoising training without gradient explosion (\Cref{fig:psnr}).
%
\item {\bf Interpretable sparse codes and dictionary}\quad Prior work has discussed algorithm unrolling for designing interpretable deep architectures based on optimization models~\citep{monga2019algorithm}, or interpretability of sparse representations in dictionary learning models~\citep{kim2010intersparsecodingvision}. However, there is no known work to mathematically characterize the interpretability of unrolled network architectures. In this regard, first, we construct a mathematical relation between learned weights (dictionary) at gradient convergence and the training data (\Cref{thm:interp}). Second, we relate the inferred representation/reconstruction of test examples to the training data. We highlight several interpretable features of the unrolled dictionary learning network. Specifically, we perform analysis that provide insights into questions such as {\it why am I learning a particular feature in the dictionary?} or {\it from what part of the training set or an image I am learning that feature?} (\Cref{fig:mnist_01234_learn_image_cont_to_dict}). Moreover, we provide an explanation of the relation between the new test image denoised/reconstructed through the network and the training dataset. The model provides insights on {\it how training images are used to reconstruct a new test image} (\Cref{fig:mnist_01234_learn_interpolate_gz}) or {\it how the test image picks up training images that have a similar representation to itself to reconstruct} (\Cref{fig:mnist_01234_learn_interpolate_code_sim}). 
\end{itemize}
%
%%%%%%%%%%%%%%%%%%
\section{Related Works} 
%
There is vast literature on the theoretical convergence of dictionary learning. \citet{spielman2021sparsedict} proposed a factorization method to recover the dictionary in the undercomplete setting (i.e., $p \leq m$). \citet{barak2015dlsumofsq} proposed to solve dictionary learning via sum-of-squares semidefinite program. K-SVD~\citep{aharon2006ksvd} and MOD~\citep{engan1999mod} are popular greedy approaches. Alternating-minimization-based methods have been used extensively in theory and practice~\citep{Jain2013lowrank, agarwal2014overdl, arora2014overdl}.

Recent work has incorporated gradient-based updates into alternating minimization~\citep{chatterji2017alternating, arora2015sparsecoding, rambhatla2018noodl}. \citet{chatterji2017alternating} provided a finite sample analysis and convergence guarantees when updating the dictionary using the analytic gradient. \citet{arora2015sparsecoding} proposed neurally plausible sparse coding approaches with analytic gradients. Another work focused on online dictionary learning~\citep{mairal2009onlinedl} with an unbiased gradient updates~\citep{rambhatla2018noodl}.~\citet{arora2015sparsecoding} discussed methods to reduce the bias of dictionary estimate, and~\citet{rambhatla2018noodl} showed how to reduce bias in code and dictionary estimates. A common feature in the above-mentioned work is the use of analytic gradients, i.e., explicitly designing gradient updates independent of the sparse coding step and not utilizing automatic gradients with deep learning optimizers. A theoretical analysis of backpropagation for dictionary learning exists only for shallow autoencoders~\citep{rangamani2018sparseae,nguyen2019dynamics}.

The theoretical analysis of unrolled neural networks has mainly analyzed the convergence speed of variants of LISTA~\citep{gregor2010lista}, where the focus is on sparse coding (i.e., the encoder) not dictionary learning~\citep{sperchmann2012learnstruc, xin2016maxsparse, moreau2017trainsparsefactor, gires2018tradeoffconvacc, chen2018unfoldista, liu2019alista, ablin2019stepsize}. \citet{moreau2017trainsparsefactor} showed that upon successful factorization of the Gram matrix of the dictionary within layers, the network achieves accelerated convergence. \citet{gires2018tradeoffconvacc} examined the tradeoffs between reconstruction accuracy and convergence speed of LISTA. Moreover, \citet{chen2018unfoldista} studied the learning dynamics of the weights and biases of unrolled-ISTA and proved that it achieves linear convergence. Follow-up works investigated the dynamics of step size in a recursive sparse coding encoder~\citep{liu2019alista, ablin2019stepsize}. \citet{ablin2019stepsize} minimized the lasso through backpropagation but still assumed the knowledge of the dictionary at the decoder.

\citet{ablin2020super} compared analytic and automatic gradient estimators of min-min optimizations with smooth and differentiable functions. Moreover,~\citet{malezieux2022understanding} studied the stability of gradient approximation in the early regime of unrolling for dictionary learning. Unlike our work, where we evaluate the gradients for model recovery,~\citet{ablin2020super} and~\citet{malezieux2022understanding} studied the asymptotic gradient errors locally in each step of an alternating minimization and did not provide errors concerning $\z^{\ast}$ or $\D^{\ast}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}\label{sec:prelim}
%
Given $n$ independent samples, dictionary learning aims to minimize the empirical risk, i.e.,
\begin{equation}\label{eq:erm}
\begin{array}{c}
\min_{\D \in \Dcal}\ \risk_n(\D)\quad \text{with}\quad  \risk_n(\D) \triangleq \frac{1}{n} \sum_{i=1}^n \loss_{\x^i}(\D)
\end{array}
\end{equation}
where $\lim_{n \to \infty} \risk_n(\D) = \E_{\x \in \X}\ [\loss_{\x}(\D)]\ \text{a.s.}$ To prevent scaling ambiguity between the code $\z$ and dictionary $\D$, it is common to constrain the norm of the dictionary columns. Hence, we define the set of feasible solutions for the dictionary as $\Dcal \triangleq \{ \D \in \R^{m \times p}\ \text{s.t.}\ \forall j\in \{1,2,\ldots,p\},\ \| \D_j \|_2^2 \leq 1 \}$. We can project estimates of $\D$ onto the feasible set by performing $\D_j \leftarrow \nicefrac{1}{\max (\| \D_j \|_2, 1)} \D_j$, either at every update or at the end of training. We assume certain properties on the data, specifically its domain (\Cref{assum:domain}), energy (\Cref{assum:boundx}), code distribution (\Cref{assum:distz}), and generating dictionary (\Cref{assum:d}).
%
%%%%%%% assumptions
\begin{assumption}[Domain signals]\label{assum:domain}
$\X$ and $\Dcal$ are both compact convex sets.
\end{assumption}
%
\begin{assumption}[Bounded signals]\label{assum:boundx}
$\exists\ M > 0\ \text{s.t.}\ \| \x \|_2 < M\  \forall \x \in \X$.
\end{assumption}
%
\begin{assumption}[Code distribution]\label{assum:distz}
The code $\z^{\ast}$ is at most $s$-sparse with the support $S^{\ast} = \text{supp}(\z^{\ast})$. Each element in $S^{\ast}$ is chosen from the set $[1, p]$, uniformly at random without replacement. $p_i = P(i \in S^{\ast}) = \Theta(s/p)$, and $p_{ij} = P(i,j \in S^{\ast}) = \Theta(s^2/p^2)$. Given the support, $\z_S^{\ast}$ is i.i.d, has symmetric probability distribution density function, $\E[\z_{(i)}^{\ast} \mid i \in S^{\ast}] = 0$ and $\E[\z_{(S)}^{\ast} \z_{(S^{\ast})}^{\ast \text{T}} \mid S^{\ast}] = \eye$. Moreover, the non-zero entries of the code are sub-Gaussian and lower bounded, i.e., for $i \in S^{\ast}$, $| \z_{(i)}^{\ast} | \geq C_{\min}$ where $0 < C_{\min} \leq 1$.
\end{assumption}
%%
\begin{assumption}[Generating dictionary]\label{assum:d}
$\D^{\ast}$ is $\mu$-incoherent (see \Cref{def:mu}) where $\mu = \mathcal{O}(\log{(m)})$. $\D^{\ast}$ is unit-norm columns matrix ($\| \D_i^{\ast} \|_2 = 1$), $\| \D^{\ast} \|_2 = \mathcal{O}(\sqrt{p/m})$, and $p = \mathcal{O}(m)$.
\end{assumption}
%
To achieve model recovery using gradient descent, we assume an appropriate dictionary initialization, i.e.,
\begin{assumption}[Dictionary closeness]\label{assum:closeness}
The initial dictionary $\D^{(0)}$ is $(\delta_{0}, 2)$-close to $\D^{\ast}$  (see \Cref{def:closeness}). The dictionary closeness at every update is denoted by $\| \D_j^{(l)} - \D_j^{\ast} \|_2 \leq \delta_l\ \forall j$. Furthermore, $\delta_l = \mathcal{O}^{\ast}(1 / \log{p})$.
\end{assumption}

\citet{arora2015sparsecoding} proposed a dictionary initialization method offering $(\delta, 2)$-close to ${\bm D}^{\ast}$ for $\delta = O^{\ast}(1 / \log m)$. The method is based on pairwise reweighting of samples $\{\x^i\}_{i=1}^n$ from the generative model~\eqref{eq:gen}, and does not require access to $\D^{\ast}$. In addition,  \citet{rambhatla2018noodl} utilize dictionary closeness assumptions and such dictionary initialization for their theoretical analysis. Moreover, \citet{Agarwal2017dictcluster} proposed a clustering approach to find a close initial estimate of the dictionary.

Given the $\mu$-incoherence of $\D^{\ast}$ (\Cref{assum:d}) and $\delta_l$-closeness of the dictionary, $\D^{(l)}$ is $\mu_l$-incoherent, i.e.,
%
\begin{restatable}[$\mu_l$-incoherent]{lemma}{mul}\label{lemma:mul}
$\D^{(l)}$ is $\mu_l$-incoherent where $\mu_l = \mu + 2 \sqrt{m} \delta_l$.
\end{restatable}

The recurrent encoder and decoder, which perform the computations shown in~\Cref{algo:unfolded}, use the loss $\Loss$ and proximal operator $\prox_{ b}(v) \triangleq \text{sign}(v) \max( | v | - b, 0)$ for the $\ell_1$ norm $h \colon \R^p \to \R$. The encoder implements ISTA~\citep{daubehies2004ista, blumensath2008ista} with step size $\alpha$, assumed to be less than $\nicefrac{1}{\sigma_{\text{max}}^2(\D)}$. With infinite encoder unrolling, the encoder's output is the solution to the lasso~\eqref{eq:lasso}, following the optimality condition (\Cref{lemma:kktlasso}) where we denote $f_{\x}(\z, \D) \triangleq \Loss_{\x}(\z, \D) + h(\z)$. One immediate observation is that $\lambda \geq \|\D^{\text{T}} \x \|_{\infty} \Leftrightarrow \{\zero\} \in \argmin f_{\x}(\z, \D)$. We assume $\lambda < \|\D^{\text{T}} \x \|_{\infty}$. We specify in~\Cref{thm:supprec} and ~\Cref{thm:supppres} the conditions on $\lambda$ at every encoder iteration to ensure support recovery and its preservation through the encoder. In case of a constant $\lambda$ across encoder iterations while using $\D^{\ast}$ as the dictionary (i.e., sparse coding using $\ell_1$ norm), the network recovers a biased code $\hat \z^{\ast}$. We denote this amplitude error in the code by $\hat \delta^{\ast} \triangleq \| \hat \z^{\ast} - \z^{\ast} \|_2$ which is small and goes to zero with $\lambda$ decaying through the encoder.

In addition, we assume the solution to~\eqref{eq:lasso} is unique; sufficient conditions for uniqueness in the overcomplete case (i.e., $p > m$) are extensively studied in the literature~\citep{wainwright2009sharp, candes2009l1, tibshirani2012lassounique}. \citet{tibshirani2012lassounique} discussed that the solution is unique with probability one if entries of $\D$ are drawn from a continuous probability distribution~\citep{tibshirani2012lassounique} (\Cref{assum:unique}). This assumption implies that $\D^{\text{T}}_S \D_S$ is full-rank. We argue that as long as the data $\x \in \X$ are sampled from a continuous distribution, this assumption holds for the entire learning process. The preservation of this property is guaranteed at all iterations of the alternating minimization proposed in~\citep{agarwal2014overdl}. Moreover, this assumption has been previously considered in analyses of unrolled sparse coding networks~\citep{ablin2019stepsize, malezieux2022understanding} and can be extended to $\ell_1$-based optimization problems~\citep{ tibshirani2012lassounique, rosset2004boosting}. 
%
\begin{assumption}[Lasso uniqueness]\label{assum:unique}
The entries of the dictionary $\D$ are continuously distributed. Hence, the minimizer of~\eqref{eq:lasso} is unique, i.e., $\hat \z = \argmin f_{\x}(\z, \D)$ with probability one.
\end{assumption}
%
\Cref{lemma:fixedpoint} states the fixed-point property of the encoder recursion~\citep{parikh2014proximal}. Given the definitions for {\it Lipschitz} and {\it Lipschitz differentiable} functions, (\Cref{def:lip,def:lipdiff}), the loss $\Loss$ and function $h$ satisfy following {\it Lipschitz} properties.
%
\begin{lemma}[Fixed-point property of lasso]\label{lemma:fixedpoint}
Given~\Cref{assum:unique}, we have $\zero \in \nabla_1 \Loss_{\x}(\hat \z, \D) + \partial h(\hat \z)$. The minimizer is a fixed-point of the mapping, i.e., $\hat \z = \prox_{\alpha \lambda}(\hat \z - \alpha \nabla_1 \Loss_{\x}(\hat \z, \D)) = \Phi(\hat \z)$~\citep{parikh2014proximal}.
\end{lemma}
%
\begin{lemma}[Lipschitz differentiable least squares]\label{lemma:lipdiffloss}
Given $\Loss_{\x}(\z, \D) = \frac{1}{2} \| \x - \D \z \|_2^2$, $\Dcal$, and~\Cref{assum:boundx}, the loss is Lipschitz differentiable. Let $L_1$ and $L_2$ denote the Lipschitz constants of the first derivatives $\nabla_1\Loss_{\x}(\z, \D)$ and $\nabla_2\Loss_{\x}(\z, \D)$, $L_{11}$ and $L_{21}$ the Lipschitz constants of the second derivatives $\nabla_{11}^2\Loss_{\x}(\z, \D)$ and $\nabla_{21}^2\Loss_{\x}(\z, \D)$, all w.r.t $\z$. Let $\nabla_1\Loss_{\x}(\z, \D)$ be $L_{1D}$-Lipschitz w.r.t $\D$, and we denote the Lipschitz constant of $\nabla_{11}\Loss_{\x}(\z, \D)$ and $\nabla_{21}\Loss_{\x}(\z, \D)$ w.r.t to $\D$ by $L_{11D}$ and $L_{21D}$, respectively.
\end{lemma}
%
\begin{lemma}[Lipschitz proximal]\label{lemma:lipprox}
Given $h(\z) = \lambda \| \z \|_1$, its proximal operator has bounded sub-derivative, i.e., $\| \partial \prox_{\lambda}(\z) \|_2 \leq c_{\text{prox}}$.
\end{lemma}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Unrolled Dictionary Learning}\label{sec:main}
%
The gradients defined in PUDLE (\Cref{algo:unfolded}) can be compared against the local direction at each update of classical alternating-minimization (\Cref{algo:altmin}). Assuming there are infinite samples, i.e.,
%
\begin{equation}\label{eq:glocal}
\begin{array}{c}
\text{Best local direction}:\quad \hat \g\ \triangleq\ \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n \nabla_2 \Loss_{\x^i}(\hat \z^{i}, \D) = \E_{\x \in \X}\ [ \nabla_{2} \Loss_{\x}(\hat \z, \D)]
\end{array}
\end{equation}
%
where $\hat \z = \argmin_{\z \in \R^p} \Loss_{\x}(\z, \D) + h(\z)$. Additionally, to assess the estimators for model recovery, hence dictionary learning, we compare them against gradient pointing towards $\D^{\ast}$, namely
\begin{equation}\label{eq:gglobal}
\begin{array}{l}
\text{Desired global gradient for $\D^{\ast}$}:\ \ \g^{\ast}\ \triangleq\ \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n \nabla_2 \Loss_{\x^i}(\z^{i\ast}, \D) = \E_{\x \in \X}\ [ \nabla_2 \Loss_{\x}(\z^{\ast}, \D)].
\end{array}
\end{equation}
To see why the above is the desired direction, $(\z^{\ast}, \D^{\ast})$ is a critical point of the loss $\Loss$ which reaches zero for data following the model~\eqref{eq:gen}. Hence, to reach $\D^{\ast} \in \argmin_{\D \in \Dcal} \E_{\x \in \X}[\Loss_{\x}(\z^{\ast}, \D)]$, we move towards the direction minimizing the loss in expectation. Specifically, using the gradient $\nabla_2 \Loss_{\x}(\z^{\ast}, \D) = - (\x - \D \z^{\ast}) \z^{\ast \text{T}} = (\D - \D^{\ast}) \z^{\ast} \z^{\ast \text{T}}$ as a descent direction, we move from $\D$ toward $\D^{\ast}$ modulo the code presence matrix $\z^{\ast} \z^{\ast \text{T}}$. Given these directions, we analyze the error of the gradients $\g_t^{\text{dec}}$, $\g_t^{\text{ae-lasso}}$, and $\g_t^{\text{ae-ls}}$ assuming infinite samples. In local analysis, we compare the code and gradient estimates to the lasso optimization in each update of the alternating minimization. In global analysis, we evaluate the performance in recovery of the ground-truth code $\z^{\ast}$ and the dictionary $\D^{\ast}$. In this regard, we first study the forward pass.
%
%
%%%%%
\begin{algorithm}[t]
\SetAlgoLined
\KwInitialize{Samples $\{\x^i\}_{i=1}^n \in \X$, initial dictionary $\D^{(0)}$}
{\bf Repeat:} $l = 0,1,\ldots, \text{number of epochs}$\\
 \quad {\bf Sparse coding step}:\quad  $\z^{i(l)} = \argmin_{\z} \Loss_{\x^i}(\z, \D^{(l)}) + h(\z)$,\quad (for $i \in [1,n])$\\
 \quad {\bf Dictionary update}:\quad  $\D^{(l+1)} = \D^{(l)} - \eta \hat \g^{(l)}\quad \text{where}\quad \hat \g^{(l)}\ \triangleq\ \frac{1}{n} \sum_{i=1}^n \nabla_{2} \Loss_{\x^i}(\z^{i(l)}, \D^{(l)})$
 \caption{Classical alternating-minimization-based dictionary learning using lasso~\eqref{eq:lasso}.}
 \label{algo:altmin}
\end{algorithm}
%%%%%
\begin{algorithm}[t]
\SetAlgoLined
\KwInitialize{Samples $\{\x^i\}_{i=1}^n \in \X$, initial dictionary $\D^{(0)}$, and $\z_0 = \zero$.}
{\bf Repeat:} $l = 0,1,\ldots, \text{number of epochs}$\\
\quad {\bf Forward pass}: (for $i \in [1,n]$)
\vspace{-2mm}
\begin{equation}\label{eq:encdec}
\begin{array}{l}
\text{Encoder:}\quad \z_{t+1}^{i(l)} = \Phi(\z_{t}^{i(l)}, \D^{(l)}) = \prox_{\alpha \lambda}(\z_{t}^{i(l)} - \alpha \nabla_1 \Loss_{\x^i}(\z_t^{i(l)}, \D^{(l)}))\ \text{(repeat for $T$)}\\
\text{Decoder:}\qquad \hat \x^{i(l)} = \D^{(l)} \z_T^{i(l)}
\end{array}
\end{equation}
%
\vspace{-2mm}
\quad {\bf Backward pass}: $\D^{(l+1)} = \D^{(l)} - \eta \g_T^{(l)}\quad \text{where}\ \g_T^{(l)}\ \text{is either of}$
  \begin{equation}
  \begin{array}{l}
 \g_{T}^{(l)\ \text{dec}}\ \triangleq\  \frac{1}{n} \sum_{i=1}^n  \nabla_2 \Loss_{\x^i}(\z_{T}^{i(l)}, \D^{(l)})\\
 \g_{T}^{(l)\ \text{ae-lasso}}\ \triangleq\  \frac{1}{n} \sum_{i=1}^n \nabla_2 \Loss_{\x^i}(\z_{T}^{i(l)}, \D^{(l)}) + \J_T^{i(l)+} \left(\nabla_1 \Loss_{\x^i}(\z_{T}^{i(l)}, \D^{(l)}) + \partial h(\z_{T}^{i(l)})\right)\\
 \g_{T}^{(l)\ \text{ae-ls}}\ \triangleq\  \frac{1}{n} \sum_{i=1}^n \nabla_2 \Loss_{\x^i}(\z_{T}^{i(l)}, \D^{(l)}) + \J_T^{i(l)+} \nabla_1 \Loss_{\x^i}(\z_{T}^{i(l)}, \D^{(l)})
  \end{array}
  \end{equation}
  See \Cref{def:jacobian} for $\J_T^{+}$.
 \caption{PUDLE: Provable unrolled dictionary learning framework.}
 \label{algo:unfolded}
\end{algorithm}
%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Forward pass}\label{sec:forward}
%
We show convergence results in the forward pass for $\z$ and the Jacobian, i.e.,
%
\begin{definition}[Code Jacobian]\label{def:jacobian}
Given $\D$, the Jacobian of $\z_t$ is defined as $\J_t \triangleq \frac{\partial \z_t}{\partial \D}$ with adjoint $\J_t^+$.
\end{definition}
%
The forward pass analyses give upper bounds on the error between $\z_t$ and $\hat \z$ and the error between $\J_t$ and $\hat \J$ as a function of unrolled iterations $t$. We define $\hat \J$ as following: considering the function $\z \rightarrow \Loss_{\x}(\z, \D) + h(\z)$, $\hat \z(\D)$ is its minimizer and $\hat \J = \frac{\partial \hat \z(\D)}{\partial \D}$. We will require these errors in \Cref{sec:backward}, where we analyze the gradient estimation errors. Similar to~\citep{chatterji2017alternating}, the error associated with $\g_t^{\text{dec}}$ depends on the code convergence. Unlike $\g_t^{\text{dec}}$, the convergence of backpropagation with gradient estimates $\g_t^{\text{ae-lasso}}$ and $\g_t^{\text{ae-ls}}$ relies on the convergence properties of the code \emph{and} the Jacobian~\citep{ablin2020super}. Forward-pass theories are based on studies by \citet{gilbert1992automatic} on the convergence of variables and their derivatives in an iterative process governed by a smooth operator~\citep{gilbert1992automatic}. Moreover, \citet{hale2007fixed} studied the convergence analysis of fixed point iterations for $\ell_1$ regularized optimization problems~\citep{hale2007fixed}.
%
\paragraph{Support recovery and preservation} We re-state a result from~\citep{hale2007fixed} on support selection.
%
\begin{proposition}[Finite-iteration support selection]\label{prop:supp}
Given ~\Cref{assum:unique}, let $\hat \z = \argmin f_{\x}(\z, \D)$ with $S \triangleq \text{supp}(\hat \z)$. There exists a $B>0$ such that $ \text{supp}(\z_t) = S, \forall t > B$.
\end{proposition}
%
This means the unrolled encoder identifies the support in finite iterations. Support recovery in finite iterations has been studied in the literature for LISTA~\citep{chen2018unfoldista}, Step-LISTA~\citep{ablin2019stepsize}, and shallow autoencoders~\citep{arora2015sparsecoding, rangamani2018sparseae, nguyen2019dynamics, tolooshams2020icml}. We show that under proper initialization of the dictionary, the encoder achieves linear convergence. \citet{arora2015sparsecoding} discussed some appropriate initialization which is used by \citet{rambhatla2018noodl}. Given initial closeness $\delta_0$, the encoder selects and recovers the correct signed support of the code with high probability in one iteration $B=1$ (\Cref{thm:supprec}), and the iterations preserve the correct support (\Cref{thm:supppres}). In spite of slow convergence of ISTA~\cite{liang2014local}, support recovery after one iteration in unrolled networks is studied in the literature~\citep{arora2015sparsecoding,rambhatla2018noodl,chen2018unfoldista,nguyen2019dynamics}.
%
\begin{restatable}[Forward pass support recovery]{theorem}{fwdsupprec}\label{thm:supprec}
Given \Cref{assum:distz,assum:d}, suppose $\D^{(l)}$ is $\delta_l = \mathcal{O}^{\ast}(1 / \sqrt{\log{p}})$ close to $\D^{\ast}$. If $s = \mathcal{O}^{\ast}(\sqrt{m} / \mu \log{m})$, and $\mu = \mathcal{O}(\log{m})$, then with probability of at least $1 - \epsilon^{(l)}_{\text{supp-rec}}$, the choice of $\lambda_0 = C_{\min} / 4$ recovers the support of the code $\z^{\ast}$ in one encoder iteration, i.e., $\text{sign}(\mathcal{S}_{\alpha \lambda_0}(\alpha \D^{(l)\text{T}} \x) = \text{sign}(\z^{\ast})$, where $\epsilon^{(l)}_{\text{supp-rec}} = 2 p\exp{(- \frac{C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}$.
\end{restatable}
%
%
\begin{restatable}[Forward pass support preservation]{theorem}{fwdsupppres}\label{thm:supppres}
Given \Cref{assum:distz,assum:d}, suppose $\D^{(l)}$ is $\delta_l = \mathcal{O}^{\ast}(1 / \log{p})$ close to $\D^{\ast}$. If $s = \mathcal{O}^{\ast}(\sqrt{m} / \mu \log{m})$, $\mu = \mathcal{O}(\log{m})$, and the regularizer and step size are chosen such that $\lambda_t^{(l)} = \frac{\mu_l}{\sqrt{m}} \| \z^{\ast} - \z_t \|_1 + a_{\gamma} = {\bm \Omega}(\frac{s \log{m}}{\sqrt{m}})$ and 
$\alpha^{(l)} \leq 1 - \frac{2\lambda_t^{(l)} - (1 - \frac{\delta_l^2}{2}) C_{\min}}{\lambda_{t-1}^{(l)}}$, then with probability of at least $1 - \epsilon^{(l)}_{\text{supp-pres}}$, the support, recovered at the first iteration, is preserved through the encoder iterations. We have $a_{\gamma} = \mathcal{O}(\sqrt{s\delta_l})$ and $\epsilon^{(l)}_{\text{supp-pres}} \coloneqq \epsilon^{(l)}_{\text{supp-rec}} + \epsilon^{(l)}_{\gamma} = 2 p \exp{(\frac{-C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}+ 2 s \exp{(\frac{-1}{\mathcal{O}(\delta_l)})}$.
\end{restatable}
%
The support preservation conditions on $\lambda_t$ and $\alpha$ introduce two insights. First, with an increase of $t$, the code error decrease, hence the lower bound on $\lambda_t$. Second, the decay of $\lambda_t$ as the encoder unrolls increases the upper bound on $\alpha$. Hence, we suggest a decaying strategy in values of $\lambda_t$ as $t$ increases.

The utilization of knowledge of the code error, as we do in \Cref{thm:supppres}, to set the proper thresholding/bias/regularization parameters ($\lambda$) constitutes a fairly standard practice. Below we discuss similar results in the literature. For the preservation of correct signed-support in a sparse coding network, \citet{rambhatla2018noodl} provided a proper thresholding value at every iteration as a function of the $\ell_1$-norm of the code error with respect to a ground-truth code; they additionally demonstrated an upper bound on the estimate of the code coefficients as a function of dictionary closeness. Moreover, \citet{nguyen2019dynamics} used information on the range of ground-truth code to choose proper biases in their neural network to guarantee support recovery. \citet{chen2018unfoldista} similarly provided an upper bound on the bias of their unrolled sparse coding network at every layer as a function of $\ell_2$-norm error between the code estimate at the layer and the ground-truth code. Overall, the error between a code estimate and the ground-truth code appearing in the lower bound on $\lambda_t^{(l)}$ can further simplified into terms related to terms such as the dictionary closeness $\delta_l$, code sparsity. For example, \citet{chatterji2017alternating}, for their particular sparse coding algorithm, provided $\ell_{\infty}$-norm upper bound as a function of terms such as code sparsity, data dimensionality, code range, and dictionary error.
%
\paragraph{Code convergence and error}  Given the support recovery and its preservation, the encoder convergence studied in~\citep{malezieux2022understanding} can achieve linear convergences after its first iteration. We re-state this result on the rate of convergence of the encoder in~\Cref{thm:fwdz}. We drop the superscript $(l)$ to simplify the notation.
%
%%%
% \vspace{-4mm}
\begin{wrapfigure}[14]{r}{0.35\textwidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/initAp02_zT_zhat_err.pdf}
	\caption{Code convergence (\Cref{thm:fwdz}). As the network unrolls, $\z_t$ converges to $\hat \z$, the solution of lasso.}
	\label{fig:forwardpass}
\end{wrapfigure}
%%%
%
%
%%%%%
%
\begin{restatable}[Local forward pass code convergence]{theorem}{fwdz}\label{thm:fwdz}
Given the encoder $\z_{t+1} = \Phi(\z_{t}, \D)$,~\Cref{assum:unique},~\Cref{lemma:lipmapping,lemma:strongconvexloss,lemma:fixedpoint}, then $\exists\ \rho <1, B > 0\ \text{s.t.}\ \| \z_{t} - \hat \z \|_2  \leq \mathcal{O}(\rho^{t})\ \forall t > B$, where $\hat \z$ is the unique minimizer of lasso~\eqref{eq:lasso}. Furthermore, given~\Cref{thm:supprec} and \Cref{thm:supppres}, $B = 1$.
\end{restatable}
%
\Cref{thm:fwdz} shows that in PUDLE, $\z_t$ converges to $\hat \z$ at a linear rate eventually after a certain number of unrolling (\Cref{fig:forwardpass}). The local linear convergence of ISTA and FISTA~\citep{beck2009fast} (with global rates of $\mathcal{O}(\nicefrac{1}{t})$ and $\mathcal{O}(\nicefrac{1}{t^2})$) in the neighbourhood of a fixed-point is studied in~\citep{tao2016local}. The speed of convergence depends on when support selection happens (\Cref{prop:supp})~\citep{bredies2008linear,zhang2017new, liang2014local}. We showed in \Cref{thm:supprec} and \Cref{thm:supppres} that under mild assumptions, the support is selected and recovered after one encoder iteration. In addition to local convergence, we focus on recovery of $\z^{\ast}$ and show error on the unrolled code coefficients $\z_{t, (j)}^{(l)}$ with respect to ground-truth $\z_{(j)}^{\ast}$ as $t$ increases. In~\Cref{thm:fwdzerrorvariable}, we consider the case where $\lambda_t$ at layer $t$ is set to according to \Cref{thm:supppres}; the bias decreases as the code error decreases among the layers and dictionary updates. We provide an upper bound on the coefficients errors as a function of code sparsity, dictionary error, and an unrolling error $e_{t,j}^{(l)\text{unroll}}$. The unrolling error goes to zero for appropriately large $t$. Moreover, \Cref{thm:fwdzerrorfixed} studies the case where the bias is fixed across the layers. In this scenario, we observe an additional term of $\lambda^{\text{fixed}}$ in the upper bounds on the code coefficients error; this term shows that the code error when we strictly perform $\ell_1$-norm based sparse coding does not go to zero. We refer to this error as an amplitude bias estimate error.
%
%
\begin{restatable}[Global forward pass code error with variable $\lambda_t$]{theorem}{fwdzerrorvariable}\label{thm:fwdzerrorvariable}
Given \Cref{assum:distz,assum:d}, suppose $\D^{(l)}$ is $\mu_l$-incoherent and $\delta_l = \mathcal{O}^{\ast}(1 / \log{p})$ close to $\D^{\ast}$. If $s = \mathcal{O}^{\ast}(\sqrt{m} / \mu \log{m})$, $\mu = \mathcal{O}(\log{m})$, and the regularizer and step size are chosen such that $\lambda_t^{(l)} = \frac{\mu_l}{\sqrt{m}} \| \z^{\ast} - \z_t \|_1 + a_{\gamma} = {\bm \Omega}(\frac{s \log{m}}{\sqrt{m}})$ and 
$\alpha^{(l)} \leq 1 - \frac{2\lambda_t^{(l)} - (1 - \frac{\delta_l^2}{2}) C_{\min}}{\lambda_{t-1}^{(l)}}$, then with probability of at least $1 - \epsilon^{(l)}_{\text{supp-pres}}$, for $j\in \text{supp}(\z^{\ast})$, the code coefficient error is
\begin{equation}
|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | \leq  \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} + e_{t,j}^{(l)\text{unroll}})
\end{equation}
and
\begin{equation}
\z_{T, (j)} = \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}
\end{equation}
where $e_{t,j}^{(l)\text{unroll}} \coloneqq 2(s-1)t \alpha \frac{\mu_l}{\sqrt{m}} \max_i | \z_{0,(i)}^{(l)} - \z_{(i)}^{\ast} | \delta_{\alpha,t-1} + | \z_{0,(j)}^{(l)} - \z_{(j)}^{\ast} | \delta_{\alpha,t}$, $\delta_{\alpha, t} \coloneqq (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^t$, $| \zeta_{T,j}^{(l)} | = \mathcal{O}(a_{\gamma})$ with $a_{\gamma} = \mathcal{O}(\sqrt{s\delta_l})$, $\beta_j^{(l)} = \langle\D_j^{\ast} - \D_j^{(l)}, \D_j^{\ast}\rangle \leq \frac{\delta_l^2}{2}$ and  $\epsilon^{(l)}_{\text{supp-pres}} \coloneqq \epsilon^{(l)}_{\text{supp-rec}} + \epsilon^{(l)}_{\gamma} = 2 p \exp{(\frac{-C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}+ 2 s \exp{(\frac{-1}{\mathcal{O}(\delta_l)})}$. With appropriately large $t$, $|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | = \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2})$.
\end{restatable}
%
\begin{restatable}[Global forward pass code error with fixed $\lambda_t$]{theorem}{fwdzerrorfixed}\label{thm:fwdzerrorfixed}
Given \Cref{assum:distz,assum:d}, suppose $\D^{(l)}$ is $\mu_l$-incoherent and $\delta_l = \mathcal{O}^{\ast}(1 / \log{p})$ close to $\D^{\ast}$. If $s = \mathcal{O}^{\ast}(\sqrt{m} / \mu \log{m})$, $\mu = \mathcal{O}(\log{m})$, and the regularizer and step size are chosen such that $\lambda_t^{(l)} = \lambda^{\text{fixed}} = \frac{\mu_l}{\sqrt{m}} \| \z^{\ast} - \z_0 \|_1 + a_{\gamma} = {\bm \Omega}(\frac{s \log{m}}{\sqrt{m}})$ and 
$\alpha^{(l)} \leq 1 - \frac{2\lambda_t^{(l)} - (1 - \frac{\delta_l^2}{2}) C_{\min}}{\lambda_{t-1}^{(l)}}$, then with probability of at least $1 - \epsilon^{(l)}_{\text{supp-pres}}$, for $j\in \text{supp}(\z^{\ast})$, the code coefficient error is
\begin{equation}
|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | \leq  \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} + e_{t,j}^{(l)\text{unroll,fixed}} + \lambda^{\text{fixed}})
\end{equation}
and
\begin{equation}
\z_{T, (j)} = \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}
\end{equation}
where $e_{t,j}^{(l)\text{unroll, fixed}} \coloneqq (s-1)t \alpha \frac{\mu_l}{\sqrt{m}} \max_i | \z_{0,(i)}^{(l)} - \z_{(i)}^{\ast} | \delta_{\alpha,t-1}^{\text{fixed}} + | \z_{0,(j)}^{(l)} - \z_{(j)}^{\ast} | \delta_{\alpha,t}^{\text{fixed}}$, $\delta_{\alpha, t}^{\text{fixed}} \coloneqq (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^t$, $| \zeta_{T,j}^{(l)} | = \mathcal{O}(a_{\gamma} + \lambda^{\text{fixed}})$ with $a_{\gamma} = \mathcal{O}(\sqrt{s\delta_l})$, $\beta_j^{(l)} = \langle\D_j^{\ast} - \D_j^{(l)}, \D_j^{\ast}\rangle \leq \frac{\delta_l^2}{2}$, and $\epsilon^{(l)}_{\text{supp-pres}} \coloneqq \epsilon^{(l)}_{\text{supp-rec}} + \epsilon^{(l)}_{\gamma} = 2 p \exp{(\frac{-C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}+ 2 s \exp{(\frac{-1}{\mathcal{O}(\delta_l)})}$. With appropriately large $t$, $|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | = \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} + \lambda^{\text{fixed}})$.
\end{restatable}
%

Aside from code estimation where the network parameters (e.g., regularization and step size) are finely tuned according to support recovery and preservation conditions (\Cref{thm:supprec,thm:supppres}), we provide a general upper bound on the error between the converged code and $\z^{\ast}$; the bound can be decomposed into two terms of the dictionary error and the biased amplitude estimate of the code.


%
\begin{restatable}[Global forward pass code error]{theorem}{fwdzglobal}\label{thm:fwdzglobal}
Let $\hat \z$ be the fixed-point of the encoder with iterations $\z_{t+1} = \Phi(\z_{t}, \D)$. Given~\Cref{assum:unique},~\Cref{lemma:lipmapping,lemma:strongconvexloss,lemma:fixedpoint}, we have $
\| \hat \z - \z^{\ast} \|_2  \leq \mathcal{O}(\| \D - \D^{\ast} \|_2 + \hat \delta^{\ast})$, where $\hat \delta^{\ast} = \| \hat \z^{\ast} - \z^{\ast} \|_2$, $\hat \z$ is the unique minimizer of lasso~\eqref{eq:lasso} given the dictionary $\D$,  $\hat \z^{\ast}$ is the unique minimizer of lasso~\eqref{eq:lasso} given the dictionary $\D^{\ast}$, and $\z^{\ast}$ is the ground-truth code.
\end{restatable}
%
This general decomposition is to emphasize that aside from the current estimate of the dictionary, the code error is a function of the forward pass algorithm used to solve the sparse coding problem. Specifically, the upper bound states that at the best scenario where there is access to the generating dictionary $\D^{\ast}$, the forward pass solving lasso with fixed $\lambda$ still gives a biased amplitude estimate of $\z^{\ast}$. Overall, the assumptions to get this bound are mild; the bound is valid independent of successful support recovery or data distribution. With incorporation of data distribution and conditions stated in \Cref{thm:supprec} and \Cref{thm:supppres}, the upper bound $\hat \delta^{\ast}$ can be replaced with terms involving $\lambda$, and reaches at zero as $\lambda$ decays across forward iterations.
%
\paragraph{Jacobian convergence and error}  Following properties similar to those used in \Cref{thm:fwdz}, and assuming $\J_t$ is bounded (\Cref{assum:boundj}), we show in~\Cref{thm:fwdj} that, as the PUDLE unrolls, the code Jacobian $\J_t$ converges to $\hat \J$, the Jacobian of the solution of the lasso. The convergence of the Jacobian of proximal gradient descent is also studied in~\citep{bertrand2021implicit} for hyperparameter selection through implicit differentiation~\citep{bengio2000gradient}, where the Jacobian is taken w.r.t to the hyperparameter $\lambda$ as opposed to $\D$.
%
\begin{assumption}[Bounded Jacobian]\label{assum:boundj}
The Jacobian is bounded, i.e., $\exists\ M_J >0,\ \text{s.t.}\ \| \J_t \|_2 \leq M_J\ \forall t$.
\end{assumption}
%
\begin{restatable}[Local forward pass Jacobian convergence]{theorem}{fwdj}\label{thm:fwdj}
Given the recursion $\z_{t+1} = \Phi(\z_t, \D)$, and $\hat \z$ the unique minimizer of lasso with Jacobian $\hat \J$, then $\exists\ \rho <1, B > 0\ \text{s.t.}\ \| \J_{t} - \hat \J \|_2 \leq \mathcal{O}(t \rho^{t})\ \forall t > B$. Furthermore, given~\Cref{thm:supprec} and \Cref{thm:supppres}, $B = 1$.
\end{restatable}
%
The forward pass code and Jacobian convergences \emph{after} support selection is similar to the results from~\citep{malezieux2022understanding}. The highlights of our finding are that the order of upper bound convergences can be achieved from the first iteration of the encoder. In other words, we specify, in \Cref{thm:supprec} and \Cref{thm:supppres}, the dictionary and data conditions such that the support can be recovered with $B=1$. This resolves the instability issue discussed by~\citet{malezieux2022understanding} in computation of the gradient $\g_t^{\text{ae-lasso}}$ outside of the support. Finally, we show that the global Jacobian error is in the order of dictionary error.
%
\begin{restatable}[Global forward pass Jacobian error]{theorem}{fwdjglobal}\label{thm:fwdjglobal}
Let $\hat \z$ be the fixed-point of the encoder with iterations $\z_{t+1} = \Phi(\z_{t}, \D)$. Given~\Cref{assum:unique},~\Cref{lemma:lipmapping,lemma:strongconvexloss,lemma:fixedpoint}, we have $\| \hat \J - \J^{\ast} \|_2  \leq \mathcal{O}(\| \D - \D^{\ast} \|_2 + \hat \delta_J^{\ast})$, where $\hat \delta_J^{\ast} \coloneqq \| \hat \J^{\ast} - \J^{\ast} \|_2$, and $\hat \J$, $\hat \J^{\ast}$ and $\J^{\ast}$ are Jacobians corresponding to $\hat \z$, $\hat \z^{\ast}$ and $\z^{\ast}$.
\end{restatable}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%
\begin{wrapfigure}[14]{r}{0.38\textwidth}
\vspace{-5mm}
	\includegraphics[width=0.999\linewidth]{figures/initAp02_g_ghat_err.pdf}
	\caption{Convergence rate of gradients (\Cref{thm:localgradient}).}
% 	\caption{Convergence rate of gradients (\Cref{thm:localgradient}). $\g_t^{\text{dec}}$ converges to $\hat \g$ at a linear rate after certain $t$, $\g_t^{\text{ae-lasso}}$ converges faster, and  $\g_t^{\text{ae-ls}}$ is its biased estimator.}
	\label{fig:localgradient}
\end{wrapfigure}
%%%
%
\subsection{Backward pass}\label{sec:backward}
We show two results for local gradient $\hat \g$ and global gradient $\g^{\ast}$ convergence. The goal is not to provide a finite sample analysis but to emphasize the relative differences between the gradients in~\Cref{algo:unfolded}. The impact of gradient error for parameter estimation in the convex setting has been studied by~\citet{devolder2013first} indicating that the convergence to the parameter's neighbourhood is dictated by the gradient error~\citep{devolder2013first, devolder2014first}. As dictionary learning is a bi-convex problem, findings of \citet{devolder2013first} hold as well for better estimation of the local dictionary at every step of alternating minimization. Moreover, \citet{arora2015sparsecoding}, provided a detailed analysis of sparse coding and various gradient estimations for dictionary learning, showing that by computing a more accurate gradient at every step of the alternating minimization scheme, the dictionary estimates converge to a closer neighbourhood of $\D^{\ast}$. Overall, the intuition is that the size of the gradient error dictates the size of the neighbourhood of the dictionary within which one can guarantee convergence. We argue that the method with lower gradient error recovers the dictionary better.
%

%%%%%%%%%%%%%%%
\paragraph{Local gradient estimations} We highlight the effect of finite unrolling on the gradient for parameter estimation~\citep{ablin2020super}. \Cref{thm:localgradient} shows the convergence rate of gradients to $\hat \g$, determining the similarity of PUDLE and \Cref{algo:altmin}.
%
\begin{restatable}[Local convergence of gradients]{theorem}{localgrad}\label{thm:localgradient}
Given the forward pass convergence results (\Cref{thm:fwdz,thm:fwdj}), $\exists\ \rho <1, B > 0$ such that $\forall t > B$, the errors of gradients defined in \Cref{algo:unfolded} w.r.t $\hat \g$~\eqref{eq:glocal} satisfy
\begin{equation}\label{eq:localconv}
\begin{aligned}
\| \g_t^{\text{dec}} - \hat \g \|_2 &\leq \mathcal{O}(\rho^t)\\ 
\| \g_t^{\text{ae-lasso}}  - \hat \g \|_2 &\leq \mathcal{O}(t \rho^{2t})\\
\| \g_t^{\text{ae-ls}}  - \hat \g \|_2 &\leq \mathcal{O}(t \rho^{2t} + M_J \lambda \sqrt{s}).
\end{aligned}
\end{equation}
%
Moreover, the order of upper bounds is tight (see~\Cref{lemma:localtightbound}). 
\end{restatable}
%
%%%%
First, upper bounds on the errors related to $\g_t^{\text{dec}}$ and $\g_t^{\text{ae-lasso}}$ go to zero as $t$ increases. Hence, both gradients converge to $\hat \g$. This means that asymptotically as $t$ increases, training PUDLE with $\g_t^{\text{dec}}$ and $\g_t^{\text{ae-lasso}}$ is equivalent to classical alternating-minimization (\Cref{algo:altmin}). Second, as $t$ increases, $\g_t^{\text{ae-lasso}}$ has faster convergence than $\g_t^{\text{dec}}$. Lastly, $\g_t^{\text{ae-ls}}$ is a biased estimator of $\hat \g$ (\Cref{fig:localgradient}). The convergence results on the error $\| \g_t^{\text{ae-lasso}}  - \hat \g \|_2$ is previously studied by \citet{malezieux2022understanding}.

Given the above convergence results, one may conclude that $\g_t^{\text{ae-lasso}}$ should be used for dictionary recovery. However, we show next that for dictionary recovery, the gradient $\g_t^{\text{ae-lasso}}$, used by~\citet{malezieux2022understanding}, is indeed a biased estimator of the global gradient $\g^{\ast}$ for recovery of $\D^{\ast}$. We decrease this bias by replacing $\g_t^{\text{ae-lasso}}$ with $\g_t^{\text{ae-ls}}$ and show that $\g_t^{\text{ae-ls}}$ results in a better recovery of $\D^{\ast}$ than $\g_t^{\text{ae-lasso}}$.
%
%
\paragraph{Global gradient estimations} \Cref{thm:globalgradient} shows the global gradient errors w.r.t $\g^{\ast}$ from \eqref{eq:gglobal}. We omit the gradient $\g_t^{\text{dec}}$, as it is asymptotically equivalent to  $\g_t^{\text{ae-lasso}}$. We study the errors in the limit to unrolling, i.e., as $t \rightarrow \infty$. This determines which PUDLE gradients recover $\D^{\ast}$ better~\citep{devolder2013first, devolder2014first}.
%
\begin{restatable}[Global error of gradients]{theorem}{globalgrad}\label{thm:globalgradient}
Given the convergence results from the forward pass, (\Cref{thm:fwdzglobal,thm:fwdjglobal}), the errors of gradients defined in \Cref{algo:unfolded} w.r.t global direction $\g^{\ast}$(defined in \eqref{eq:gglobal}) satisfy
\begin{equation}\label{eq:glocalconv}
\begin{aligned}
%\| \g_{\infty}^{\text{dec}} - \g^{\ast} \|_2 &\leq \mathcal{O}(\|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast})\\
% \| \g_{\infty}^{\text{ae-lasso}}  - \g^{\ast} \|_2 &\leq  \mathcal{O}(\|  \D - \D^{\ast} \|_2^2 + \|  \D - \D^{\ast} \|_2 + \|  \D - \D^{\ast} \|_2 \hat \delta^{\ast} + \hat \delta^{\ast} +  \hat \delta^{\ast 2} + M_J \lambda \sqrt{s})\\
% \| \g_{\infty}^{\text{ae-ls}}  -  \g^{\ast} \|_2 &\leq \mathcal{O}(\|  \D - \D^{\ast} \|_2^2 + \|  \D - \D^{\ast} \|_2 + \|  \D - \D^{\ast} \|_2 \hat \delta^{\ast} + \hat \delta^{\ast} +  \hat \delta^{\ast 2}).
\| \g_{\infty}^{\text{ae-lasso}}  - \g^{\ast} \|_2 &\leq  \mathcal{O}(\|  \D - \D^{\ast} \|_2^2 + \|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast} + \hat \delta_J^{\ast} + M_J \lambda \sqrt{s})\\
\| \g_{\infty}^{\text{ae-ls}}  -  \g^{\ast} \|_2 &\leq \mathcal{O}(\|  \D - \D^{\ast} \|_2^2 + \|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast} + \hat \delta_J^{\ast}).
\end{aligned}
\end{equation}
\end{restatable}
%
%%%%%
%
Several factors affect the order of upper bounds: the current estimate of the dictionary, code amplitude-bias error due to $\ell_1$ norm, and the usage of $\ell_1$ norm in the loss used for backpropagation. To study the bias in the gradient computation, let consider the scenario where $\D = \D^{\ast}$. We denote those gradients by superscript $\D^{\ast}$. If the gradients are not biased, then the upper bounds should goes to zero. The gradient errors are
\begin{equation}
% \begin{aligned}
\| \g_{\infty}^{\text{ae-lasso}, \D^{\ast}}  - \g^{\ast} \|_2 \leq  \mathcal{O}(\hat \delta^{\ast} + \hat \delta_J^{\ast} + M_J \lambda \sqrt{s})
\qquad \text{and}\qquad
\| \g_{\infty}^{\text{ae-ls}, \D^{\ast}}  -  \g^{\ast} \|_2 \leq \mathcal{O}(\hat \delta^{\ast} + \hat \delta_J^{\ast}).
% \end{aligned}
\end{equation}
For $\g_{\infty}^{\text{ae-ls}, \D^{\ast}}$, the radius of the error ball is only a function of the amplitude error of the code estimated through lasso compare to the ground-truth code $\z^{\ast}$. However, the error ball for the gradient $\g_{\infty}^{\text{ae-lasso}, \D^{\ast}}$ includes an additional term concerning the usage of lasso loss containing the regularization term $\lambda$. This implies that the $\D^{\ast}$ neighbourhood at which the gradient $\g_{\infty}^{\text{ae-ls}, \D^{\ast}}$ is guaranteed to converge to is smaller than of the $\g_{\infty}^{\text{ae-lasso}, \D^{\ast}}$ (\Cref{fig:dstar}). Implications of such gradient estimation are seen in dictionary learning where $\g_{\infty}^{\text{ae-ls}}$ recovers $\D^{\ast}$ better (\Cref{fig:dl_25,fig:dl_100}). In \Cref{fig:dl_25}, the encoder unrolls for $T=25$, hence the phenomenon of implicit acceleration is seen in faster and better dictionary learning performance of $\g_{\infty}^{\text{ae-lasso}}$ than $\g_{\infty}^{\text{dec}}$. In \Cref{fig:dl_100} where $T=100$, similar performance of $\g_{\infty}^{\text{dec}}$ and $\g_{\infty}^{\text{ae-lasso}}$ illustrates their asymptotic equivalence as $t \to \infty$ (See Appendix for additional noisy dictionary learning experiments where the measurements $\x$ are corrupted with zero-mean Gaussian noise such that the Signal-to-Noise-Ratio is approximately $12$ SNR; in this setting, the aforementioned comparative analysis still holds.)
%
%%%
\begin{figure}[h]
	\centering
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/initAp02_g_gstar_err}
	  \caption{Convergence for $\g^{\ast}$.}
  	\label{fig:dstar}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T25_D_Dstar_err}
	  \caption{Learning ($T=25$).}
  	\label{fig:dl_25}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_D_Dstar_err}
	  \caption{Learning ($T=100$).}
  	\label{fig:dl_100}
	\end{subfigure}
	\caption{Results for PUDLE's global convergence (\Cref{thm:globalgradient}) and dictionary learning.}
	\label{fig:globalgradient}
	\vspace{-4mm}
\end{figure}
%%%
%

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Towards unbiased estimation} As long as $\lambda$ is fixed within PUDLE, all defined gradients remain biased estimators of $\g^{\ast}$, due to the biased estimate of the code $\z^{\ast}$ through $\ell_1$ norm. This bias exists while dictionary learning is performed strictly using lasso through~\Cref{algo:altmin}. Given the conditions on the regularizer in~\Cref{thm:supppres} which we discussed in~\Cref{sec:forward} and the derived upper bounds in~\Cref{thm:globalgradient}, we suggest the decaying of $\lambda$ across the encoder to reduce the gradient biases and improve dictionary learning. Next, we prove in \Cref{thm:dictvariabledec} that PUDLE converges to $\D^{\ast}$ if $\lambda$ decays across the layers $t$ according to~\Cref{thm:fwdzerrorvariable}. Moreover, \Cref{thm:dictfixededec} proves that if $\lambda$ stays fixed according to~\Cref{thm:fwdzerrorfixed}, then PUDLE only guarantees to converge to a close neighbourhood of the dictionary. In these analyses, we focus on $\g_T^{\text{dec}}$. Furthermore, we show in~\Cref{sec:exp} that by decaying $\lambda$ at each unrolled layer, the gradient bias vanishes, and we recover $\D^{\ast}$.
%
%%%%%%%%%%%%%%%
\paragraph{Dictionary learning} Given the network parameters set by \Cref{thm:fwdzerrorvariable}, \Cref{thm:dictvariabledec} proves that using $\g_T^{\text{dec}}$, PUDLE recovers the dictionary; the dictionary error contracts at every update. Moreover, \Cref{thm:dictfixededec} proves that as long as $\lambda$ stays fixed across the unrolled layers, PUDLE guarantees to converge to only $\D^{\ast}$ neighbourhood characterized by the regularization parameter $\lambda$. These analyses requires for $\D^{(l)}$ to maintain a closeness to $\D^{\ast}$ which we provide a proof for in \Cref{lemma:maintaincloseness}. Hence, the dictionary closeness assumption (\Cref{assum:closeness}) stays valid.
%
\begin{restatable}[Dictionary learning with variable $\lambda_t$]{theorem}{dictvariabledec}\label{thm:dictvariabledec}
Given \Cref{assum:distz,assum:d}, suppose $\D^{(l)}$ is $\mu_l$-incoherent and $(\delta_l,2)$-close to $\D^{\ast}$ with $\delta_l = \mathcal{O}^{\ast}(1 / \log{p})$. If $s = \mathcal{O}(\sqrt{m})$, $\mu = \mathcal{O}(\log{m})$, learning rate is $\eta = \mathcal{O}(\frac{p}{s (1 - \delta_l^2/2)})$, and the regularizer and step size are set according to \Cref{thm:fwdzerrorvariable}, then for any dictionary update $l$ using $\g_T^{\text{dec}}$, with probability of at least $1 - \epsilon^{(l)}_{\text{supp-pres}}$, 
\begin{equation}
    \| \D_j^{(l+1)} - \D_j^{\ast} \|_2^2 \leq (1 - \psi)   \| \D_j^{(l)} - \D_j^{\ast} \|_2^2 
\end{equation}
where $\epsilon^{(l)}_{\text{supp-pres}} \coloneqq \epsilon^{(l)}_{\text{supp-rec}} + \epsilon^{(l)}_{\gamma} = 2 p \exp{(\frac{-C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}+ 2 s \exp{(\frac{-1}{\mathcal{O}(\delta_l)})}$.
\end{restatable}
%
\begin{restatable}[Dictionary learning with fixed $\lambda_t$]{theorem}{dictfixededec}\label{thm:dictfixededec}
Given \Cref{assum:distz,assum:d}, suppose $\D^{(l)}$ is $\mu_l$-incoherent and $(\delta_l,2)$-close to $\D^{\ast}$ with $\delta_l = \mathcal{O}^{\ast}(1 / \log{p})$. If $s = \mathcal{O}(\sqrt{m})$, $\mu = \mathcal{O}(\log{m})$, learning rate is $\eta = \mathcal{O}(\frac{p}{s (1 - \delta_l^2/2)})$, and the regularizer $\lambda^{\text{fixed}}$ and step size are set according to \Cref{thm:fwdzerrorfixed}, then for any dictionary update $l$ using $\g_T^{\text{dec}}$, with probability of at least $1 - \epsilon^{(l)}_{\text{supp-pres}}$, 
\begin{equation}
    \| \D_j^{(l+1)} - \D_j^{\ast} \|_2^2 \leq (1 - \psi)   \| \D_j^{(l)} - \D_j^{\ast} \|_2^2 + \epsilon_{\lambda}^{(l)}
\end{equation}
where $\epsilon_{\lambda}^{(l)} \coloneqq \eta \frac{2p}{s(1 - \beta_j^{(l)})} \lambda^{\text{fixed}2}$, $\epsilon^{(l)}_{\text{supp-pres}} \coloneqq \epsilon^{(l)}_{\text{supp-rec}} + \epsilon^{(l)}_{\gamma} = 2 p \exp{(\frac{-C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}+ 2 s \exp{(\frac{-1}{\mathcal{O}(\delta_l)})}$.
\end{restatable}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Experiments}\label{sec:exp}
%
\paragraph{Dictionary learning} We focus on the performance of the best-performing gradient estimator $\g_t^{\text{ae-ls}}$, and compare it with NOODL~\citep{rambhatla2018noodl}, a state-of-the-art online dictionary learning algorithm, and SPORCO~\citep{wohlberg2017sporco}, an alternating-minimization dictionary learning algorithm that uses lasso. NOODL, which uses iterative hard-thresholding (HT) for sparse coding and a gradient update employing the code's sign, has linear convergence upon proper initialization~\citep{rambhatla2018noodl}. We note that the results from $\g_t^{\text{ae-lasso}}$ are not shown, as the gradient computation was unstable~\citep{malezieux2022understanding}. We emphasize that our proposed gradient $\g_t^{\text{ae-ls}}$ does not suffer such instability. We train:
%
%
\begin{wrapfigure}[14]{r}{0.41\textwidth}
    % \vspace{-5mm}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/s20_D_Dstar_err}
	\caption{Dictionary convergences.}
% 	\caption{Dictionary convergence of $\g_t^{\text{ae-ls}}$, NOODL~\citep{rambhatla2018noodl} and SPORCO~\citep{wohlberg2017sporco}.}
	\label{fig:baslines}
\end{wrapfigure}
% 
 \begin{itemize}[noitemsep, topsep=0pt, leftmargin=12pt]
 \item $\g_t^{\text{ae-ls}}$: $\lambda$ is fixed across iterations.
 \item $\g_t^{\text{ae-ls, decay}}$ : $\lambda$ decays (i.e., $\lambda_t = \lambda \nu^t$, with $0 <\nu < 1$) where $\nu$ decreases as training progresses.
 \item $\g_t^{\text{ae-ls, HT}}$ : $\prox_{\alpha \lambda}(v)$ is replaced with $\text{HT}_{b}(v) \triangleq v \bm{1}_{|v| \geq b}$.
 \end{itemize}
With $\text{HT}$, the sparse coding step reduces to that from NOODL. In this case, we highlight the difference between the gradient update of our method (backpropagation) with NOODL. We focus on convergence, as $\eta$ across methods is not comparable.

\Cref{fig:baslines} shows the convergence of $\D\!\in\!\R^{1000 \times 1500}$ to $\D^{\ast}$ when the code is $20$-sparse (for other sparsity levels and details see~\Cref{exp_app}). A biased estimate of the code amplitudes results in convergence only to a neighbourhood of the dictionary~\citep{rambhatla2018noodl}. This is observed in the convergence of $\g_t^{\text{ae-ls}}$ and SPORCO (final error is shown). The convergence of $\g_t^{\text{ae-ls}}$ to a closer neighbourhood than SPORCO supports~\Cref{thm:globalgradient}. Moreover, with decaying $\lambda$, the code bias vanishes, hence $\g_t^{\text{ae-ls, decay}}$ and $\g_t^{\text{ae-ls, HT}}$ converges to $\D^{\ast}$ similar to NOODL.

%
%%%
\begin{figure}[h]
	\centering
	%%%%%
	\begin{subfigure}[t]{0.45\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T15_lamp16_test_psnr_tmlr_1}
	  \caption{$\g_t^{\text{ae-lasso}}$ training is not stable.}
  	\label{fig:psnr_1}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.45\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T15_lamp16_test_psnr_tmlr_2.pdf}
	  \caption{$\g_t^{\text{ae-ls}}$ improvement is smoother than  $\g_t^{\text{dec}}$.}
  	\label{fig:psnr_2}
	\end{subfigure}
	\caption{Networks behaviour (test PSNR) during training as a function of epochs.}
	\label{fig:psnr}
	\vspace{-2mm}
\end{figure}
%%%
%

\paragraph{Image denoising} To further highlight the advantage of $\g_t^{\text{ae-ls}}$ over the other gradients, we compare them in a supervised task of image denoising. In addition to $\g_t^{\text{ae-ls}}$, $\g_t^{\text{ae-lasso}}$, and $\g_t^{\text{dec}}$, we consider $\g_t^{\text{ae-ls, HT}}$ where the proximal operator is replaced with HT. This is to compare with sparse coding scheme of NOODL. We do not compare against NOODL's dictionary update, as this computation for two-dimensional convolutions is not straightforward. Prior works have shown that variants of PUDLE either rival or outperform state-of-the-art architectures~\citep{simon2019rethinking,tolooshams2020icml}. Thus, we focus on a comparative analysis of the gradients. We trained on $432$ and tested on $68$ images from BSD~\citep{martin2001bsd}. BSD dataset is a popular training dataset for denoising~\citep{zhang2017beyond, simon2019rethinking, mohan2019robust}. We used a convolutional dictionary and corrupted images with zero-mean Gaussian noise of standard deviation of $25$ (see~\Cref{exp_app} for details). We initialized the dictionary filters by standard Normal distribution; this is to follow the norm in the deep learning literature and to demonstrate the practicality and usefulness of PUDLE in the absence of an initialization method. We evaluate the denoising performance of soft-thresholding using $\lambda$ and HT with $b$ in peak signal-to-noise-ratio (PSNR).

First, we highlight the stability of $\g_t^{\text{ae-ls}}$ against $\g_t^{\text{ae-lasso}}$; \Cref{fig:psnr_1} shows the network dynamics in terms of test PSNR as a function of epochs when $\lambda = 0.16$ for $\g_t^{\text{ae-ls}}$, $\g_t^{\text{ae-lasso}}$, $\g_t^{\text{dec}}$ and $b=0.05$ for $\g_t^{\text{ae-ls, HT}}$.  We observed that $\g_t^{\text{ae-ls}}$ uses full backpropagation and stays stable. However, the training with $\g_t^{\text{ae-lasso}}$ is not stable and unstable to perform denoising where the noisy PSNR is approximately $20$ dB~\citep{malezieux2022understanding}. Second, \Cref{fig:psnr_2} shows that compared to $\g_t^{\text{dec}}$, the backpropagated gradients result in a smoother improvement during training. Moreover, \Cref{tab:psnr} shows that the advantage of $\g_t^{\text{ae-ls}}$ over $\g_t^{\text{dec}}$ is not limited to dictionary learning and is seen in denoising. We have excluded the results for $\g_t^{\text{ae-lasso}}$ from \Cref{tab:psnr} as the network failed to denoise (see \Cref{fig:psnr_1}). Additionally, the superior performance of $\g_t^{\text{ae-ls}}$ compared to $\g_t^{\text{ae-ls, HT}}$ highlights the benefits of PUDLE (i.e., $\ell_1$-based unrolling) against HT used in NOODL.
 %
 \begin{table}[!h]
% \begin{wraptable}[11]{r}{0.48\linewidth}
% \vspace{-4mm}
 \caption{Denoising of BSD68. Reported numbers are mean (std) PSNR given three independent trials.}
  \label{tab:psnr}
\begin{center}
\begin{tabular}{llllll}
\multicolumn{1}{c}{\bf METHOD} & &\multicolumn{4}{c}{\bf PSNR [dB]}\\
\midrule
 & $\lambda$ &  0.08 & 0.12 & 0.16 & 0.2   \\
  \cmidrule(r){2-6}
$\g_t^{\text{dec}}$ & &  24.21 (0.12) &  24.93 (0.14) &  25.25 (0.06) &  24.88 (0.00)  \\
 $\g_t^{\text{ae-ls}}$  & & 24.79 (0.03) &  25.43 (0.03) &  {\bf 25.63} (0.04) &  25.46 (0.05) \\
\midrule
  & $b$ &  0.02 & 0.05 & 0.08 & 0.1   \\
  \cmidrule(r){2-6}
  $\g_t^{\text{ae-ls, HT}}$  &  &  22.92 (0.07) &  25.26 (0.1) &  24.76 (0.06) &  23.94 (0.13)  \\
\end{tabular}
\end{center}
\end{table}
% \end{wraptable}
%
% \in \{0.08, 0.12, 0.16, 0.2\}$
% \in \{0.02, 0.05, 0.08, 0.1 \}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Interpretable Sparse Codes and Dictionary}
%
One motivation behind using algorithm unrolling to design deep architectures is interpretability~\citep{monga2019algorithm}; they argue that the designed networks are interpretable as they capture domain knowledge via an optimization model. For example, \citet{tolooshams2020tnnls} takes advantage of the interpretability of learned weights in an unrolled dictionary learning network to solve spike sorting, an unsupervised source separation problem in computational neuroscience. Moreover,~\citet{kim2010intersparsecodingvision} uses sparse coding to learn interpretable representations of human motions. However, none of the existing methods in the literature provide interpretability results that open the black-box network through building a mathematical relation between the learned dictionary, training data, and test representation/reconstruction. This section analyzes the interpretability of the unrolled sparse coding method in this context. We note that such mathematical relation and interpretability results also hold for dictionary learning. However, it is missing in the literature, irrespective of whether one uses an unrolling network. We provide the following theorem.
%
%
\begin{wrapfigure}[16]{r}{0.35\linewidth}
%   \vspace{2mm}
  \centering
  \includegraphics[width=0.999\linewidth]{figures/mnist_01234_dictionary_sorted.png}
  \caption{Fraction of dictionary atoms learned from $\{0, 1, 2, 3, 4\}$ MNIST.}
  \label{fig:mnist_dict}
  \end{wrapfigure}
%
% \vspace{-4mm}
\begin{restatable}[Interpretable unrolled network]{theorem}{interp}\label{thm:interp}
Consider the dictionary learning optimization of the form $\min_{\Z, \D}\ \frac{1}{2} \| \Xx - \D \Z \|_{F}^2 + \lambda \| \Z \|_1 + \nicefrac{\omega}{2} \| \D \|_F^2$, where $\Xx = [\x^1, \x^2, \ldots, \x^n] \in \R^{m \times n}$ and $\Z = [\z^1, \z^2, \ldots, \z^n] \in \R^{p \times n}$. Let $\tilde \Z$ be the given converged sparse codes, then stationary points of the problem w.r.t the network weights (dictionary) follows $\tilde \D = \Xx \G^{-1} \tilde \Z^{\text{T}}$, where we denote $\G \coloneqq (\tilde \Z^{\text{T}} \tilde \Z + \omega \eye)$la.
\end{restatable}
%
\paragraph{The dictionary interpolates the training data} Given \Cref{thm:interp}, each learned atom interpolates the training data, i.e.,
%
%% from mnist_01234_p500_layers15_lamp7_step1_lamlossp7_2021_12_01_11_49_41
%
\vspace{-2mm}
\begin{equation}\label{eq:dict_inter}
	\tilde \D_j =  \Xx (\G^{-1}\w_j) = \sum_{k=1}^n (\G^{-1}\w_j)_k \x^k
\end{equation}
where $\w_j = [\tilde \z^{1}_j, \tilde \z^{2}_j, \ldots, \tilde \z^{n}_j]^{\text{T}} \in \R^n$ is a vector containing the training code activity for dictionary atom $j$. Specifically, the importance of training image $\x^k$ in learning dictionary atom $j$ is captured by the term $(\G^{-1}\w_j)_k$. This proves the dictionary lives in the spans of the training set. Given the small number of atoms compared to the training size,~\eqref{eq:dict_inter} shows that the dictionary summarizes the training examples. We trained the network on digits of $\{0, 1, 2, 3, 4\}$ MNIST (\Cref{fig:mnist_dict} shows a fraction of the most used learned atoms). \Cref{fig:mnist_01234_learn_image_cont_to_dict} visualizes dictionary atoms along with training images with the highest contribution (green) and the lowest contribution (red). In addition, we used~\eqref{eq:dict_inter} on the partial training data to reconstruct learned atoms (shown as Estimate). Next, we interpret the relation between a new data to the training data using representer point selection, similar to~\citep{yeh2018representer}.

\begin{figure}[h]
    \centering
  \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_image_contribution_on_dict_67.pdf}
  \caption{$0$ looking like atom.}
  \end{subfigure}
  %%
  \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_image_contribution_on_dict_336.pdf}
   \caption{$4$ looking like atom.}
  \end{subfigure}
  %%
  \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_image_contribution_on_dict_371.pdf}
    \caption{$3$ looking like atom.}
  \end{subfigure}
  %%
   \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_image_contribution_on_dict_432.pdf}
    \caption{$1$ looking like atom.}
  \end{subfigure}
  \caption{Training image contributions to learning the dictionary.}
  \label{fig:mnist_01234_learn_image_cont_to_dict}
  \vspace{-3mm}
\end{figure}
%
%%%%%%%%%%%%
%
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_similar_training_examples_based_on_beta_testimage787_nohist.pdf}
  \caption{$0$ test image.}
  \end{subfigure}
  %%
  \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_similar_training_examples_based_on_beta_testimage1253_nohist.pdf}
  \caption{$1$ test image.}
  \end{subfigure}
  %%
  \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_similar_training_examples_based_on_beta_testimage3463_nohist.pdf}
  \caption{$3$ test image.}
  \end{subfigure}
  %%
   \begin{subfigure}[b]{0.245\textwidth}
  \centering
  \includegraphics[width=0.99\linewidth]{figures/01234_similar_training_examples_based_on_beta_testimage4201_nohist.pdf}
  \caption{$4$ test image.}
  \end{subfigure}
  \caption{Interpolation of training data to reconstruct a new image. Contribution of training images are shown in green (high contribution) and red (low contribution). $\boldbeta^j$ is normalized over the used examples.}
  \label{fig:mnist_01234_learn_interpolate_gz}
 \vspace{-3mm}
\end{figure}
%
\paragraph{Relation between new test image and training data} For representation of a new data, we observe that the reconstruction of a new example $\x^j$ is a linear combination of all the training examples, i.e.,
\vspace{-3mm}
\begin{equation}\label{eq:newdata_data}
\hat \x^j = \tilde \D \hat \z^{j} = \Xx \boldbeta^j = \sum_{k=1}^n \boldbeta^j_k \x^k
\end{equation}
%
where $\hat \x^j$ denotes reconstruction, $\hat \z^j$ is the code estimate, $\boldbeta^j = \G^{-1} \tilde \Z^{\text{T}} \hat \z^{j} \in \R^{n}$, and $\boldbeta^j_k = \sum_{a=1}^n \G^{-1}_{ka} \langle \tilde \z^a, \hat \z^j\rangle$. We observe that the contribution of image $k$ into the reconstruction of the test image is a function of $\boldbeta^j_k$, and the energy of $\boldbeta^j_k$ itself depends on the whole training set, and $\G^{-1}$. \eqref{eq:newdata_data} shows how each image is reconstructed as interpolation of the training images. \Cref{fig:mnist_01234_learn_interpolate_gz} shows this results, where images with high (green) $\boldbeta^j_k$ contribution are similar to the test image and those with low (red) $\boldbeta^j_k$ contribution are different. In addition, we can evaluate the overall quality of the reconstruction by looking into $\boldbeta_k^j$ in~\eqref{eq:newdata_data}. For example, we observed that for test MNIST, unnormalized $\boldbeta_k^j$ corresponding to high contributing training images is above $1$. However, for resized-CIFAR, unnormalized $\boldbeta_k^j$ of high contributing training images are often half or an order of magnitude lower than the MNIST case. This informs us of a bad representation/reconstruction of CIFAR image by the trained network. From another perspective, we can write the new image as
%
\begin{equation}\label{eq:code_sim_sum}
    \x^j = \tilde \D \hat \z^{j} =  \sum_{k=1}^n (\Xx \G^{-1})_k \langle \tilde \z^{k}, \hat \z^j \rangle
\end{equation}
%
%
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=0.48\linewidth]{figures/01234_similar_training_examples_based_on_code_testimage1746.pdf}
   \includegraphics[width=0.48\linewidth]{figures/01234_similar_training_examples_based_on_code_testimage1746hist.pdf}
  \caption{Digit $1$ test image.}
  \vspace*{-2mm}
  \end{subfigure}
  %%
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
  \includegraphics[width=0.48\linewidth]{figures/01234_similar_training_examples_based_on_code_testimage3043.pdf}
   \includegraphics[width=0.48\linewidth]{figures/01234_similar_training_examples_based_on_code_testimage3043hist.pdf}
  \caption{Digit $2$ test image.}
  \vspace*{-2mm}
  \end{subfigure}
  %%
  \caption{Contribution of images with code similarity into reconstruction of a new test image along with the histograms of the similarity of the test code to training codes from each class.}
  \label{fig:mnist_01234_learn_interpolate_code_sim}
  \vspace{-4mm}
\end{figure}
%
i.e., the contribution of each training image for reconstruction is a function of their code similarity to the new image and properties of the Gram matrix of training set code similarities. Specifically, the relation rules the contribution of transformed image $k$ (i.e., $(\Xx \G^{-1})_k$) into reconstruction of the test image as a function of its code similarity $\langle \tilde \z^{k}, \hat \z^j \rangle$. In other words,~\eqref{eq:code_sim_sum} shows that training images with the highest code similarity to the representation of the new image have the highest contribution to its reconstruction. This interpretation is demonstrated in~\Cref{fig:mnist_01234_learn_interpolate_code_sim}. The training images with the highest code similarity (green) and the lowest similarity (red) are shown. In addition, the figure demonstrates the histogram of the code similarity between the test image and the training set, grouped by their class digit. For example, for digit $1$ test image, its code similarity to train images from class $1$ are bimodal. This corresponds to $1$ digits that are tilted to the left (low similarity) and right (high similarity). Moreover, for digit $2$ test image, we observe that the histogram of images corresponding to digit $2$ are shifted the most to the right (highest similarity) than the other classes.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
%
This paper studied dictionary learning and analyzed the dynamics of unrolled sparse coding networks through a provable unrolled dictionary learning (PUDLE) framework. First, we provided a theoretical analysis of the forward pass for code recovery and dictionary learning. We discussed the bias introduced by $\ell_1$-based sparse coding in the forward pass, and how this affects the dictionary estimate in the backward pass. Second, we showed strategies to mitigate the propagation of this code bias into the backward pass; this is achieved by modification of the training loss function. We demonstrated that this bias could be further reduced and eliminated by decaying the regularization parameter within the unrolled layers. Additionally, we provided sufficient conditions on the data distribution and network to guarantee stability of backpropagated gradient computations. In the absence of such conditions, we proposed a modification to the loss function that resolves the gradient explosion and allows stable learning. In an image denoising task, we showed PUDLE outperforms the NOODL sparse coding scheme~\citep{rambhatla2018noodl}. Motivated by interpretability as a popular feature for unrolled networks, we derived a mathematical relation between the network weights (dictionary) and the training set. We proved that the network weights live in the span of the training set, and constructed a relation between predictions of new input examples and the training set. The latter allows the user to extract images from the training set that are similar/dissimilar to the input image in representation/reconstruction.

%\subsubsection*{Broader Impact Statement}
%In this optional section, TMLR encourages authors to discuss possible repercussions of their work,
%notably any potential negative impact that a user of this research should be aware of. 
%Authors should consult the TMLR Ethics Guidelines available on the TMLR website
%for guidance on how to approach this subject.

%\subsubsection*{Author Contributions}
%If you'd like to, you may include a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors. Only add
% of this information once your submission is accepted and deanonymized. 
%
%\subsubsection*{Acknowledgments}
%Use unnumbered third-level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.
%Only add this information once your submission is accepted and deanonymized. 

\bibliographystyle{tmlr}
\bibliography{tmlr}

% \newpage
\appendix
\section{Appendix - proofs}
\label{appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%
%
\subsection{Notation}
%
Bold-lower-case and upper-case letters refer to vectors ${\bm d}$ and matrices ${\D}$. We use ${\bm d}_{(j)}$ to denote the $j^{th}$ element of the vector ${\bm d}$, and ${\D}_j$ is the $j^{th}$ column of the matrix ${\D}$. We denote the code estimate at unrolled layer $t$ by $\z_t$. $\lambda > 0$ is the regularization (sparsity-enforcing) parameter. $\sigma_{\text{max}}{({\D})}$ is the maximum singular value of ${\D}$. When taking the derivatives or norms w.r.t the matrix $\D$, we assume that $\D$ is vectorized. $\nabla_1 \Loss_{\x}(\z, \D)$ and $\nabla_2 \Loss_{\x}(\z, \D)$ are the first derivatives of the loss w.r.t $\z$ and $\D$, respectively. $\nabla_{11}^2 \Loss_{\x}(\z, \D)$ is the second derivative of the loss w.r.t $\z$. $\nabla_{21}^2 \Loss_{\x}(\z, \D)$ is the derivative of $\nabla_{1} \Loss_{\x}(\z, \D)$ w.r.t $\D$. The support of $\z$ is $\text{supp}(\z) \triangleq \{j \colon \z_{(j)} \neq 0\}$.
%
%
\subsection{Basic definitions and Lemmas}
%
We list four definitions used throughout the paper below.
%
\begin{definition}[$\mu$-incoherence]\label{def:mu}
$\D$ is $\mu$-incoherent, i.e., for every pair $(i,j)$ of columns, $| \langle \D_i, \D_j \rangle | \leq \mu / \sqrt{m}$.
\end{definition}
%
%
\begin{definition}[$(\delta, \kappa)$-closeness]\label{def:closeness}
Dictionary $\D$ is $\delta$-close to $\D^{\ast}$, i.e., there is a permutation $\pi$ and sign flip operator $u$ such that $\forall i\ \| u(i) \D_{\pi(i)} - \D_i^{\ast} \|_2 \leq \delta$. Additionally, $\| \D - \D^{\ast} \|_2 \leq \kappa \| \D^{\ast} \|_2$.
\end{definition}
%
\begin{definition}[Lipschitz function]\label{def:lip}
A function $f \colon \R^m \to \R^p$ is L-Lipschitz w.r.t a norm $\| \cdot \|$ if $\ \exists\ L > 0\ \text{s.t.}\ \| f(a) - f(b) \| \leq L \| a - b \|\ \forall a, b \in \R^m$.
\end{definition}
%
\begin{definition}[Lipschitz differentiable function]\label{def:lipdiff}
A twice differentiable function $f \colon \R^m \to \R^p$ is L-Lipschitz differentiable w.r.t a norm $\| \cdot \|$ iff $\ \exists\ L > 0\ \text{s.t.}\ \| \nabla^2 f(a) \| \leq L\ \forall a \in \R^m$.
\end{definition}
%
\begin{definition}[Strong convexity]\label{def:strongconvex}
A twice differentiable function $f \colon \R^m \to \R^p$ is strongly convex if $\ \exists\ \mu > 0\ \text{s.t.}\ \nabla^2 f(a) \succeq \mu \eye$.
\end{definition}
%
\begin{definition}[Norm of subgradient]\label{def:normsubgrad}
For norms involving subgradents, we define $\| \partial h(\z) \| \coloneqq \max_{{\bm v} \in \partial h(\z)}  \| {\bm v} \|$.
\end{definition}
%
In the proof of the theorems, we use the strong convexity of the reconstruction loss after support selection and the bounded property of the Lipschitz mapping stated below.
%
\begin{lemma}[Strong convexity of reconstruction loss]\label{lemma:strongconvexloss}
Given the support selection (\Cref{prop:supp}), $\D_{S}^{\text{T}} \D_S$ is full-rank. Thus, $\forall t > B, \Loss_{\x}(\z_{t}, \D) = \Loss_{\x}(\z_{t,S}, \D_S)$ is strongly convex (\Cref{def:strongconvex}) in $\z$.
\end{lemma}
%
\begin{lemma}[Lipschitz mapping]\label{lemma:lipmapping}
 Given the recursion $\z_{t+1} = \Phi(\z_t) = \prox_{\alpha \lambda}(\z_t - \alpha \nabla_1 \Loss_{\x}(\z_t, \D))$, from~\Cref{lemma:strongconvexloss}, there exist $B >0$ such that loss $\Loss_{\x}(\z_t, \D)$ is $\mu$-strongly convex $\forall t > B$. Hence, using~\Cref{lemma:lipprox},
\begin{equation}
\| \nabla_1 \Phi(\z_t, \D) \|_2 = \| (\eye - \alpha \nabla_{11}^2 \Loss_{\x}(\z_t, \D)) \partial \prox_{\alpha \lambda}(\z_t) \|_2 \leq \rho
\end{equation}
where $\rho \triangleq  c_{\text{prox}} (1-\alpha\mu) < 1$.
\end{lemma}
%
One key term, used in the proofs, is that $\zero \in \nabla_1 \Loss_{\x}(\hat \z, \D) + \partial h(\hat \z)$ which is followed by the lasso optimality, i.e.,
%
\begin{lemma}[Lasso optimality]\label{lemma:kktlasso}
Lasso Karush-Kuhn-Tucker (KKT) optimality conditions are
\begin{equation}\label{eq:kkt}
\hat \z \in \argmin_{\z \in \R^p} f_{\x}(\z, \D) \Leftrightarrow \D^{\text{T}}(\x - \D \hat \z) \in \lambda \partial \| \hat \z \|_1, \text{and}\ \partial | \hat \z_{(j)} | =
\begin{cases} 
\{ \text{sign}(\hat \z_{(j)}\} &\text{if}\ \hat \z_{(j)} \neq 0\\
[-1, 1] &\text{if}\ \hat \z_{(j)} = 0
\end{cases}
,\forall j \in \{1,2,\ldots,p\}.
\end{equation}
\end{lemma}
%%%%%%%%
%
\subsection{Forward pass proof details}\label{sec:fwdproof}
%
Given the $\mu$-incoherence of $\D^{\ast}$, and current dictionary closeness of $\delta_l$, we re-state \Cref{lemma:mul} and proof it below. It shows that the current dictionary is $\mu_l$-close to $\D^{\ast}$.
%
\mul*
%
\begin{proof} 
\begin{equation}
\begin{aligned}
\langle \D_i^{(l)}, \D_j^{(l)} \rangle  &= \langle \D_i^{\ast}, \D_j^{\ast} \rangle - \langle \D_i^{\ast} - \D_i^{(l)}, \D_j^{\ast} \rangle - \langle \D_i^{(l)}, \D_j^{\ast} -\D_j^{(l)} \rangle\\
| \langle \D_i^{(l)}, \D_j^{(l)} \rangle |  &\leq \mu / \sqrt{m} +  \| \D_i^{\ast} + \D_i^{(l)}\|_2 \| \D_j^{\ast} \|_2 + \|\D_i^{(l)}\|_2 \| \D_j^{\ast} -\D_j^{(l)} \|_2 \leq \mu / \sqrt{m} + 2 \delta_l
\end{aligned}
\end{equation}
\end{proof}
We re-state and proof the forward pass support recovery (\Cref{thm:supprec}). This shows that given proper initialization and under mild conditions, the support of the true code $\z^{\ast}$ is recovered with high probability in one iteration of the encoder.
%
\fwdsupprec*
%
\begin{proof} 
The code estimate after one iteration is $\z_{1} = \prox_{\alpha \lambda}(\alpha \D^{(l)\text{T}} \x) = \text{sign}(\D^{(l)\text{T}} \x) \text{ReLU}(\alpha (| \D^{(l)\text{T}} \D^{\ast} \z^{\ast}| - \lambda_0))$. We focus on the positive entries. The analysis for negative entries is similar. Writting the relation for $i$-th entry,
\begin{equation}
\begin{aligned}
\z_{1,(i)} &=  \text{sign}(\D^{(l)\text{T}} \x) \text{ReLU}(\alpha (\sum_{j \in S^{\ast}} \langle \D^{(l)}_i, \D^{\ast}_j \rangle \z^{\ast}_{(j)} - \lambda_0))\\
&= \text{ReLU}(\alpha (\langle \D^{(l)}_i, \D^{\ast}_i \rangle  \z^{\ast}_{(i)} + \sum_{j \in S^{\ast \backslash \{i\}}} \langle \D^{(l)}_i,  \D^{\ast}_j \rangle  \z^{\ast}_{(j)}  - \lambda_0))\\
\end{aligned}
\end{equation}
We focus on the term inside ReLU and discard $\alpha$, shared by all terms. We shows that under proper choice of $\lambda_0$, $\langle \D^{(l)}_i, \D^{\ast}_i \rangle  \z^{\ast}_{(i)}$ is greater than $\lambda_0$ and $\vvec_i = \sum_{j \in S^{\ast \backslash \{i\}}} \langle \D^{(l)}_i, \D^{\ast}_j \rangle  \z^{\ast}_{(j)}$ is small with respect to $\lambda_0$, hence getting cancelled by ReLU. The small value of $\vvec_i$, compared to $\langle \D^{(l)}_i, \D^{\ast}_i \rangle  \z^{\ast}_{(i)}$, results in $\text{sign}(\D^{(l)\text{T}} \x)$ be equal to the $\text{sign}(\D^{(l)\text{T}} \D^{\ast} \z^{\ast})$ which is equal to the sign of $\z^{\ast}$.

Given the current dictionary distance $\| \D^{(l)}_i - \D_i^{\ast} \|_2 \leq \delta_l$, we can find a lower bound on $\langle \D^{(l)}_i, \D^{\ast}_i \rangle  \z^{\ast}_{(i)}$ as follows
\begin{equation}
\begin{aligned}
\langle \D_i^{(l)}, \D_i^{\ast} \rangle &=  \frac{1}{2} (\| \D_i^{\ast} \|_2^2 + \| \D_i^{(l)} \|_2^2 -\| \D_i^{(l)} - \D_i^{\ast} \|_2^2) = 1 - \frac{1}{2} \| \D_i^{(l)} - \D_i^{\ast} \|_2^2\\
| \langle \D_i^{(l)}, \D_i^{\ast} \rangle | &\geq 1 - \delta_l^2 / 2
\end{aligned} 
\end{equation}
Hence, for $i \in S^{\ast}$
\begin{equation}
| \langle \D^{(l)}_i , \D^{\ast}_i \rangle  \z^{\ast}_{(i)} | \geq (1 - \delta_l^2 / 2) C_{\min}
\end{equation}
otherwise, it is $0$. Given, $var(\z^{\ast}_{(i)}) = 1$ for $i \in S^{\ast}$, we find an upper bound on the variance $\vvec_i$ of as follows
\begin{equation}
\begin{aligned}
var(\vvec_i) &= \sum_{j \in S^{\ast \backslash \{i\}}} \langle \D^{(l)}_i, \D^{\ast}_j \rangle^2
=  \sum_{j \in S^{\ast \backslash \{i\}}} (\langle \D^{\ast}_i, \D^{\ast}_j \rangle + \langle \D^{(l)}_i - \D^{\ast}_i, \D^{\ast}_j \rangle )^2\\
&\leq \sum_{j \in S^{\ast \backslash \{i\}}} 2(\langle \D^{\ast}_i, \D^{\ast}_j \rangle^2 + \langle \D^{(l)}_i - \D^{\ast}_i, \D^{\ast}_j \rangle^2)
\leq \sum_{j \in S^{\ast \backslash \{i\}}} (2\mu^2/m) + 2\| (\D^{(l)}_i - \D^{\ast}_i)^{\text{T}} \D^{\ast}_{S^{\backslash \{i\}}} \|_2^2\\
&\leq (2s\mu^2/m) + 2\| (\D^{(l)}_i - \D^{\ast}_i)\|_2^2  \| \D^{\ast}_{S^{\ast \backslash \{i\}}} \|_2^2
\leq 2(s\mu^2/m + 4\delta_l^2) = \mathcal{O}^{\ast}(\delta_l^2)
\end{aligned}
\end{equation}
where we used the Gershgorin Circle Theorem for the bound $\| \D^{\ast}_{S^{\backslash \{i\}}} \|_2 \leq 2$. With the sub-Gaussian assumption on the coefficients $\z^{\ast}$, we get the following using Chernoff bound concerning $\vvec_i$.
\begin{equation}
P( | \vvec_i | \geq \frac{C_{\min}}{4}) \leq 2 \exp{(- \frac{C_{\min}^2}{4s\mu^2/m + 16\delta_l^2})} = 2 \exp{(- \frac{C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}
\end{equation}
Taking a union bound over all indices $i \in [1, p]$ will result in
\begin{equation}
P( \max_{i} | \vvec_i | \geq \frac{C_{\min}}{4}) \leq 2 p\exp{(- \frac{C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})} \coloneqq \epsilon^{(l)}_{\text{supp-rec}}
\end{equation}
Hence, we can set $\lambda_0 = C_{\min} / 2$.
\end{proof}
%
We re-state and prove the forward pass support preservation (\Cref{thm:supppres}).
%
\fwdsupppres*
%
\begin{proof}
Given current dictionary $\D^{(l)}$, in each iteration of the forward pass, we have $\z_{t+1} = \prox_{\alpha \lambda}(\z_t + \alpha \D^{\text{T}} (\D^{\ast} \z^{\ast} - \D \z_t)$. We focus on the entires that are non-negative. Then procedure for negative code entries is similar. We follow similar steps as in \citep{rambhatla2018noodl}. We get
\begin{equation}\label{eq:code_itr}
\begin{aligned}
\z_{t+1, (j)} &=  \text{ReLU}((\eye - \alpha \D^{(l)\text{T}}\D^{(l)})_{(j,:)} \z_t + \alpha (\D^{(l)\text{T}} \D^{\ast} )_{(j,:)} \z^{\ast} - \alpha \lambda_{t,j}^{(l)})\\
&=  \text{ReLU}((\eye - \alpha \D^{(l)\text{T}}\D^{(l)})_{(j,:)} \z_t + \alpha ((\D^{(l)} - \D^{\ast})^{\text{T}}\D^{\ast})_{(j,:)} \z^{\ast} +  \alpha (\D^{\ast \text{T}} \D^{\ast})_{(j,:)} \z^{\ast} - \alpha \lambda_{t,j}^{(l)})\\
&=  \text{ReLU}((1 - \alpha) \z_{t, (j)} - \alpha \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle \z_{t, (i)} + \alpha \langle(\D_j^{(l)} - \D_j^{\ast}), \D_j^{\ast}\rangle \z^{\ast}_{(j)}\\
&+ \alpha \sum_{i \neq j}\langle\D_j^{(l)} - \D_j^{\ast}, \D_i^{\ast}\rangle \z^{\ast}_{(i)} +  \alpha \z^{\ast}_{(j)} + \alpha \sum_{i \neq j} \langle \D_j^{\ast}, \D_i^{\ast}\rangle \z^{\ast}_{(i)} - \alpha \lambda_{t,j}^{(l)})\\
&=  \text{ReLU}((1 - \alpha) \z_{t, (j)} + \alpha (1 - \beta_j^{(l)}) \z^{\ast}_{(j)} + \alpha \eta_{t,j}^{(l)} - \alpha \lambda_{t,j}^{(l)})\\
\end{aligned}
\end{equation}
where $\beta_j^{(l)} = \langle\D_j^{\ast} - \D_j^{(l)}, \D_j^{\ast}\rangle$, and $\eta_{t,j}^{(l)} = - \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle \z_{t, (i)} + (\langle\D_j^{(l)} - \D_j^{\ast}, \D_i^{\ast}\rangle + \langle \D_j^{\ast}, \D_i^{\ast}\rangle) \z^{\ast}_{(i)}$. With $\| \D_j^{(l)} - \D_j^{\ast} \|_2 \leq \delta_l$, $\beta_j^{(l)}$ can be bounded as follows
%
\begin{equation}\label{eq:betabound}
\beta_j^{(l)} = \langle\D_j^{\ast} - \D_j^{(l)}, \D_j^{\ast}\rangle \leq \delta_l^2 / 2
\end{equation}
where we used the relation $ \|\D_j^{(l)} - \D_j^{\ast} \|_2^2 = 2 (1 - \langle \D_j^{(l)}, \D_j^{\ast} \rangle)$. We re-write $\eta_{t,j}^{(l)}$ below
%
\begin{equation}
\begin{aligned}
\eta_{t,j}^{(l)} &= - \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle \z_{t, (i)} + (\langle\D_j^{(l)} - \D_j^{\ast}, \D_i^{\ast}\rangle + \langle \D_j^{\ast}, \D_i^{\ast}\rangle) \z^{\ast}_{(i)}\\
&= - \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle \z_{t, (i)} + \sum_{i \neq j} (\langle\D_j^{(l)} - \D_j^{\ast}, \D_i^{\ast}\rangle + \langle \D_j^{\ast}, \D_i^{\ast}\rangle) \z^{\ast}_{(i)} + \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle \z^{\ast}_{(i)} - \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle \z^{\ast}_{(i)}\\
&= \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t, (i)}) + \sum_{i \neq j} (\langle\D_j^{(l)} - \D_j^{\ast}, \D_i^{\ast}\rangle + \langle \D_j^{\ast}, \D_i^{\ast} \rangle - \langle \D_j^{(l)}, \D_i^{(l)} \rangle) \z^{\ast}_{(i)} \\
&= \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t, (i)}) + \sum_{i \neq j} \langle\D_j^{(l)}, \D_i^{\ast} - \D_i^{(l)}\rangle \z^{\ast}_{(i)} \\
&= \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t, (i)}) + \gamma_j^{(l)} \\
\end{aligned}
\end{equation}
where $\gamma_j^{(l)} = \sum_{i \neq j} \langle\D_j^{(l)} , \D_i^{\ast} - \D_i^{(l)}\rangle \z^{\ast}_{(i)}$. Given the sub-Gaussian entries of the code $\z^{\ast}$, we provide a bound on the variance of $\gamma_j^{(l)}$ below:
\begin{equation}
var(\gamma_j^{(l)}) = \sum_{i \neq j} \langle\D_j^{(l)} , \D_i^{\ast} - \D_i^{(l)}\rangle^2 \leq s \delta_l^2
\end{equation}
Now, using Chernoff bound on the sub-Gaussian code entries, we get
\begin{equation}
P( | \gamma_j^{(l)} | > a) \leq 2 \exp{(\frac{- a^2}{2s \delta_l^2})}
\end{equation}
To bound all the terms in the support, for $j \in S^{\ast}$, we have
\begin{equation}\label{eq:agamma}
P( \max | \gamma_j^{(l)} | > a_{\gamma}) \leq \epsilon_{\gamma}^{(l)}
\end{equation}
where $\epsilon_{\gamma}^{(l)} = 2 s\exp{(\frac{- a_{\gamma}^2}{2s \delta_l^2})}$. Let $a_{\gamma} = \mathcal{O}(\sqrt{s \delta_l})$, then $\epsilon_{\gamma}^{(l)} = 2 s\exp{(\frac{- 1}{\mathcal{O}(\delta_l)})}$. The above analysis states that with probability of at least $1 - \epsilon_{\gamma}^{(l)}$, $| \gamma_j^{(l)}| \leq a_{\gamma} = \mathcal{O}(\sqrt{s \delta_l})$. Next, we write the recursion for when the support is identified (see \Cref{thm:supprec}). For the code at iteration $T$, we have
\begin{equation}\label{eq:code_T}
\begin{aligned}
\z_{T, (j)} &=  (1 - \alpha)^T \z_{0, (j)} + \z^{\ast}_{(j)}  \sum_{t=1}^T \alpha(1 - \beta_j^{(l)})(1 - \alpha)^{T - t}   + \sum_{t=1}^T \alpha (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)}) (1 - \alpha)^{T-t}\\
&= (1 - \alpha)^T \z_{0, (j)} + \z^{\ast}_{(j)} (1 - \beta_j^{(l)})(1 - (1 - \alpha)^T)  + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)}) (1 - \alpha)^{T-t}\\
&=  \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + (1 - \alpha)^T (\z_{0, (j)} - \z^{\ast}_{(j)} (1 - \beta_j^{(l)}))  + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)}) (1 - \alpha)^{T-t}\\
&= \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}\\
\end{aligned}
\end{equation}
where $\zeta _{T,j}^{(l)}= (1 - \alpha)^T (\z_{0, (j)} - \z^{\ast}_{(j)} (1 - \beta_j^{(l)}))  + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)}) (1 - \alpha)^{T-t}$. With the support correctly identified at iteration $t-1$, we show that the support is preserved at iteration $t$. With $\| \z^{\ast} - \z_t \|_1 = \mathcal{O}(s)$, for each $j \in S^{\ast}$, we have
%
\begin{equation}
\begin{aligned}
\eta_{t,j}^{(l)} &= \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t, (i)}) + \gamma_j^{(l)} 
\leq \frac{\mu_l}{\sqrt{m}} \| \z^{\ast} - \z_t \|_1 + a_{\gamma} = \mathcal{O}(\frac{s\log{m} }{\sqrt{m}})
\end{aligned}
\end{equation}
%
We make sure the regularizer is chosen such that
%
\begin{equation}
\lambda_t \geq \frac{\mu_l}{\sqrt{m}} \| \z^{\ast} - \z_t \|_1 + a_{\gamma}
\end{equation}
%
We see that the larger the code error and coherence between the columns of the current dictionary, the larger $\lambda_t$ should be. This is to suppress the noise component in the code recursion and make sure no false support is introduced. Furthermore, we want $\lambda_t$ to be lower than half of the signal component, i.e.,
%
\begin{equation}
\begin{aligned}
\alpha \lambda_t &\leq \frac{1 - \alpha}{2} \z_{t, (j)} + \frac{\alpha}{2} (1 - \beta_j^{(l)}) \z^{\ast}_{(j)}, \forall j \in S^{\ast}\\
\alpha \lambda_t &\leq \frac{1 - \alpha}{2} \z_t^{\min} + \frac{\alpha}{2} (1 - \frac{\delta_l^2}{2}) C_{\min}
\end{aligned}
\end{equation}
where $\z_t^{\min} = \min_{j} \z_{t,(j)}$. We further shrink the upper bound, given the code from previous iteration $t-1$ (i.e., $\alpha \lambda_{t-1} \leq \z_t^{\min}$). Hence, we want the regularizer to follow
%
\begin{equation}
\begin{aligned}
\alpha \lambda_t &\leq \frac{1 - \alpha}{2} \alpha \lambda_{t-1}  + \frac{\alpha}{2} (1 - \frac{\delta_l^2}{2}) C_{\min}\\
\lambda_t &\leq \frac{1 - \alpha}{2} \lambda_{t-1}  + \frac{1}{2} (1 - \frac{\delta_l^2}{2}) C_{\min}
\end{aligned}
\end{equation}
This condition is to make sure the identified supports are not killed in the recursion. We use the condition to set the step size $\alpha$. We get
\begin{equation}
\alpha \leq 1 - \frac{2\lambda_t - (1 - \frac{\delta_l^2}{2}) C_{\min}}{\lambda_{t-1}}
\end{equation}
Hence, $\lambda_t = {\bm \Omega}(\frac{s \log{m}}{\sqrt{m}})$ and $\alpha$ should be chosen sufficiently small such that the condition above is met. We denote $\epsilon^{(l)}_{\text{supp-pres}} \coloneqq \epsilon^{(l)}_{\text{supp-rec}} + \epsilon^{(l)}_{\gamma} = 2 p \exp{(\frac{-C_{\min}^2}{\mathcal{O}^{\ast}(\delta_l^2)})}+ 2 s \exp{(\frac{-1}{\mathcal{O}(\delta_l)})}$. Hence, with probability of at least $1 - \epsilon^{(l)}_{\text{supp-pres}}$, the support, recovered at the first iteration, is preserved through the encoder iterations.
\end{proof}

%
\Cref{thm:supprec} and \Cref{thm:supppres} allow to achieve linear convergence in the forward pass right after the first encoder iteration, i.e., $B=1$. With support recovery at first iteration and its preservation, we now re-state the forward pass code convergence (\Cref{thm:fwdz}).
%
\fwdz*
%%%%%
\begin{proof} Given the support selection at iteration $B$, from~\Cref{lemma:strongconvexloss}, we have $\nabla_{11}^2 \Loss_{\x}(\z_t, \D) \succeq \mu \eye$ restricted to the support for $t > B$. Then, from \Cref{lemma:lipmapping}, we get
\begin{equation*}
\| \nabla_1 \Phi(\z_t, \D) \|_2 = \| (\eye - \alpha \nabla_{11}^2 \Loss_{\x}(\z_t, \D)) \partial \prox_{\alpha \lambda}(\z_t) \|_2 \leq \rho
\end{equation*}
where $\rho \triangleq  c_{\text{prox}} (1-\alpha\mu) < 1$. Hence, using fixed-point property (\Cref{lemma:fixedpoint})
\begin{equation*}
\exists\ B > 0,\ \text{s.t.}\ \| \z_{t+1} - \hat \z \|_2 = \| \Phi(\z_t) - \Phi(\hat \z) \|_2 \leq \rho \| \z_t - \hat \z \|_2\ \forall t > B
\end{equation*}
where $\hat \z = \argmin f_{\x}(\z, \D)$. Unrolling the recursion,
\begin{equation*}
\| \z_{t} - \hat \z \|_2 \leq \rho^{t-B} \| \z_B - \hat \z \|_2.
\end{equation*}
\end{proof}
%
%
\fwdzerrorvariable*
%
\begin{proof}
We define $\tilde \eta_{t,j}^{(l)} \coloneqq \sum_{i \neq j} | \langle \D_j^{(l)}, \D_i^{(l)} \rangle | | \z^{\ast}_{(i)} - \z_{t, (i)} | + \gamma_j^{(l)}$ and upper bound it as
\begin{equation}\label{eq:tildeeta}
\tilde \eta_{t,j}^{(l)} \leq \frac{\mu_l}{\sqrt{m}} \sum_{i \neq j} E_{t,i} + \gamma_j^{(l)}
\end{equation}
where $E_{t,i} \coloneqq |\z_{(i)}^{\ast}  - \z_{t,(i)}|$. Given~\eqref{eq:kkt}, we re-write the code recursion
\begin{equation}
\begin{aligned}
\z_{t+1, (j)} &= \prox_{\alpha \lambda_{t,j}^{(l)}}((1 - \alpha) \z_{t, (j)} + \alpha (1 - \beta_j^{(l)}) \z^{\ast}_{(j)} + \alpha \eta_{t,j}^{(l)})\\
&\in (1 - \alpha) \z_{t, (j)} + \alpha (1 - \beta_j^{(l)}) \z^{\ast}_{(j)} + \alpha (\eta_{t,j}^{(l)} - \lambda_{t,j}^{(l)} \partial | \z_{t+1, (j)} |)\\
E_{t+1, j} = | \z_{t+1, (j)} - \z_{(j)}^{\ast} | &\leq (1 - \alpha) E_{t, j} + \alpha \beta_j^{(l)} | \z^{\ast}_{(j)} | + \alpha (\tilde \eta_{t,j}^{(l)} + \lambda_{t,j}^{(l)} )
\end{aligned}
\end{equation}
Opening up the recursion, we get
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq E_{0,j} \prod_{q=0}^t (1 - \alpha) + \beta_j^{(l)} | \z^{\ast}_{(j)} | \sum_{a=1}^{t+1} \alpha \prod_{q=a}^{t+1} (1 - \alpha) + \sum_{a=1}^{t+1} \alpha (\tilde \eta_{a-1,j}^{(l)} + \lambda_{a-1,j}^{(l)} ) \prod_{q=a}^{t+1} (1 - \alpha)
\end{aligned}
\end{equation}
where we define $ \prod_{q=a}^a (1 - \alpha) = 1$. Using the upper bound from~\eqref{eq:tildeeta} and $\lambda_{t,j}^{(l)}$ from \Cref{thm:supppres}, we get
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq v_{t+1, j} + 2 \frac{\mu_l}{\sqrt{m}} \alpha \sum_{a=1}^{t+1} \sum_{i \neq j} E_{a-1, i} \prod_{q=a}^{t+1} (1 - \alpha)
\end{aligned}
\end{equation}
where $v_{t+1, j} \coloneqq E_{0, j} \prod_{q=0}^t (1 - \alpha) + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{a=1}^{t+1} \alpha \prod_{q=a}^{t+1} (1 - \alpha)$. We now derive the general upper bound on $E_{t+1, j}$ as follows
\begin{equation}
\begin{aligned}
E_{1, i_1} &\leq v_{1, i_1} + 2\alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_2 \neq i_1} E_{0, i_2}
\end{aligned}
\end{equation}
For $E_{2, i_1}$, we have
\begin{equation}
\begin{aligned}
E_{2, i_1} &\leq v_{2, i_1} + 2 \alpha \frac{\mu_l}{\sqrt{m}} (\sum_{i_2 \neq i_1} E_{1, i_2} + \sum_{i_2 \neq i_1} E_{0, i_2} (1 - \alpha))
\end{aligned}
\end{equation}
Substituting $E_{1, i_1}$,
\begin{equation}
\begin{aligned}
E_{2, i_1} &\leq v_{2, i_1} + 2 \alpha \frac{\mu_l}{\sqrt{m}} (\sum_{i_2 \neq i_1}  (v_{1, i_2} + 2\alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_3 \neq i_2} E_{0, i_3}) + \sum_{i_2 \neq i_1} E_{0, i_2} (1 - \alpha))
\end{aligned}
\end{equation}
For $E_{3, i_1}$, we have
\begin{equation}
\begin{aligned}
E_{3, i_1} &\leq v_{3, i_1} + 2 \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^3 \sum_{i_2 \neq i_1} E_{a-1,i_2} (1-\alpha)^{3-a}
\end{aligned}
\end{equation}
Unrolling the recursion,
\begin{equation}
\begin{aligned}
E_{3, i_1} &\leq v_{3, i_1} + 2 \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_2 \neq i_1} v_{2,i_2} + 2 \alpha \frac{\mu_l}{\sqrt{m}} \left((1-\alpha) \sum_{i_2 \neq i_1}  v_{1,i_2} + 2 \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_3 \neq i_2} v_{1,i_3}\right)\\
&+ 2 \alpha \frac{\mu_l}{\sqrt{m}} \left( (1 - \alpha)^2 \sum_{i_2 \neq i_1} E_{0,i_2} + 2 (1-\alpha) (2\alpha \frac{\mu_l}{\sqrt{m}}) \sum_{i_3 \neq i_2, i_1}  E_{0,i_3} + (2\alpha \frac{\mu_l}{\sqrt{m}})^2 \sum_{i_4 \neq i_3, i_2, i_1} E_{0,i_4} \right)
\end{aligned}
\end{equation}
Given above, we can write up the relation as
\begin{equation}
\begin{aligned}
E_{3, i_1} &\leq v_{3, i_1} + 2 \alpha \frac{\mu_l}{\sqrt{m}} \left( (s-1) v_{2,i} + v_{1,i} ((1-\alpha) (s-1)  + 2 \alpha \frac{\mu_l}{\sqrt{m}} (s-2)) \right)\\
&+ 2 \alpha \frac{\mu_l}{\sqrt{m}}  E_{0,i} \left( (s-1)(1 - \alpha)^2 + 2 (1-\alpha) (2\alpha \frac{\mu_l}{\sqrt{m}}) (s-2) + (2\alpha \frac{\mu_l}{\sqrt{m}})^2 (s-3)\right)\\
&\leq v_{3, i_1} + (s-1) 2 \alpha \frac{\mu_l}{\sqrt{m}} \left(  v_{2,i} + v_{1,i} ((1-\alpha) + 2 \alpha \frac{\mu_l}{\sqrt{m}}) \right)\\
&+ 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} \left( (1 - \alpha)^2 + 2 (1-\alpha) (2\alpha \frac{\mu_l}{\sqrt{m}}) + (2\alpha \frac{\mu_l}{\sqrt{m}})^2 \right)\\
\end{aligned}
\end{equation}
Following similar steps for $E_{4, i_1}$, we get, 
\begin{equation}
\begin{aligned}
E_{4, i_1} &\leq v_{4, i_1} + (s-1) 2 \alpha \frac{\mu_l}{\sqrt{m}} \left( v_{3,i} + v_{2,i} (1-\alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}}) + v_{1,i} ((1-\alpha)^2 + 2 (1-\alpha)(2 \alpha \frac{\mu_l}{\sqrt{m}}) + (2 \alpha \frac{\mu_l}{\sqrt{m}})^2) \right)\\
&+ 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} \left( (1 - \alpha)^3 + 3 (1-\alpha) (2\alpha \frac{\mu_l}{\sqrt{m}})^2 + 3 (1-\alpha)^2 (2\alpha \frac{\mu_l}{\sqrt{m}}) + (2\alpha \frac{\mu_l}{\sqrt{m}})^3 \right)\\
\end{aligned}
\end{equation}
This leads to the term
\begin{equation}
\begin{aligned}
E_{4, i_1} &\leq v_{4, i_1} + (s-1) 2 \alpha \frac{\mu_l}{\sqrt{m}} \left( v_{3,i} + v_{2,i} (1-\alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^1 + v_{1,i} (1-\alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^2\right)\\
&+ 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} (1 - \alpha + 2\alpha \frac{\mu_l}{\sqrt{m}})^3\\
\end{aligned}
\end{equation}
Hence, the general term for code error at $t$ layer is
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq v_{t+1, j} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^t
\end{aligned}
\end{equation}
where for $j$ in the support, we define the upper bounds $v_{a, j} \leq v_{a, \text{max}}$ and $E_{0, j} \leq E_{0, \text{max}}$. Next, we define $(1 - \alpha)^t \leq \delta_{\alpha, t} \coloneqq (1 -\alpha + 2\alpha \frac{\mu_l}{\sqrt{m}})^t$, and use it to find an upper bound on the expression $\sum_{a=1}^{t} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a}$. We have
\begin{equation}
v_{t, j} = E_{0, j} (1 - \alpha)^t + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{k=1}^{t} \alpha (1 - \alpha)^{t-k+1}
\end{equation}
We bound the expression
\begin{equation}
\begin{aligned}
\sum_{a=1}^{t} v_{a, j} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} &\leq \sum_{a=1}^t  (E_{0, j} (1 - \alpha)^a + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k+1}) (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a}\\
&\leq E_{0, j} t \delta_{\alpha, t} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{a=1}^t (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k+1}
\end{aligned}
\end{equation} 
Using sum of geometric series, we write $\sum_{a=1}^t (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} = \frac{1 - (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^t}{\alpha - 2 \alpha \frac{\mu_l}{\sqrt{m}}} \leq \frac{1}{\alpha  (1 - 2 \frac{\mu_l}{\sqrt{m}})}$. Hence, using \eqref{eq:betabound} and \eqref{eq:agamma}, with probability of at least $1 - \epsilon_{\gamma}^{(l)}$, we have
\begin{equation}
\sum_{a=1}^{t} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} \leq E_{0,\text{max}} t \delta_{\alpha,t} + \frac{1}{\alpha  (1 - 2 \frac{\mu_l}{\sqrt{m}})} (\frac{\delta_l^2}{2} | \z_{\text{max}}^{\ast}| + 2 a_{\gamma})
\end{equation}
Hence, we bound the code error on the coefficients as following
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq v_{t+1, j} + 2 (s-1) \frac{\mu_l}{\sqrt{m}} (\frac{1}{ (1 - 2 \frac{\mu_l}{\sqrt{m}})} (\frac{\delta_l^2}{2} | \z_{\text{max}}^{\ast}| + 2 a_{\gamma}))+ 2 (t+1)(s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} \delta_{\alpha,t}
\end{aligned}
\end{equation}
Next, we further simplify the first term
\begin{equation}
v_{t+1, j} = E_{0, j} (1 - \alpha)^{t+1} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{k=1}^{t+1} \alpha (1 - \alpha)^{t-k+1}
\leq E_{0, j} \delta_{\alpha,t+1} + \frac{\delta_l^2}{2} | \z^{\ast}_{\text{max}} | + 2 a_{\gamma}
\end{equation}
Substituting the above upper bound into the upper bound for $E_{t+1,j}$, we get
\begin{equation}
E_{t+1, j} \leq E_{0, j} \delta_{\alpha,t+1} + (1 + 2 (s-1) \kappa_l) (\frac{\delta_l^2}{2} | \z_{\text{max}}^{\ast}| + 2 a_{\gamma}) + 2 (t+1)(s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} \delta_{\alpha,t}
\end{equation}
where $\kappa_l \coloneqq \frac{\mu_l}{\sqrt{m}} (\frac{1}{ (1 - 2 \frac{\mu_l}{\sqrt{m}})}$. Given $s = \mathcal{O}^{\ast}(\sqrt{m}/\mu \log{m})$, we have $(s-1) \kappa_l < 1$. Hence, with probability of at least $1 - \epsilon_{\gamma}^{(l)}$, we have
\begin{equation}
\begin{aligned}
E_{t, j} &\leq  \mathcal{O}(a_{\gamma}) + 2(s-1)t \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} \delta_{\alpha,t-1} + E_{0, j} \delta_{\alpha,t}\\
|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | &\leq  \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} + e_{t,j}^{(l)\text{unroll}})
\end{aligned}
\end{equation}
where $e_{t,j}^{(l)\text{unroll}} \coloneqq 2(s-1)t \alpha \frac{\mu_l}{\sqrt{m}} \max_i | \z_{0,(i)}^{(l)} - \z_{(i)}^{\ast} | \delta_{\alpha,t-1} + | \z_{0,(j)}^{(l)} - \z_{(j)}^{\ast} | \delta_{\alpha,t}$. With appropriately large unrolled layer $t$, $e_{t,j}^{(l)\text{unroll}} \approx 0$. Hence, for the code error on non-zero coefficients, we get
\begin{equation}
|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | = \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2})
\end{equation}
for large enough $t$. Now, we try to prove the relation for $\z_{T, (j)}$. For shrinkage, we re-write \eqref{eq:code_T}
\begin{equation}
\begin{aligned}
\z_{T, (j)} \in \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}
\end{aligned}
\end{equation}
where $\zeta _{T,j}^{(l)} = \kappa_{T,j}^{(l)} + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)}  \partial | \z_{t, (j)} |) (1 - \alpha)^{T-t}$ and $\kappa_{T,j}^{(l)} \coloneqq (1 - \alpha)^T (\z_{0, (j)} - \z^{\ast}_{(j)} (1 - \beta_j^{(l)}))$. $\kappa_{T,j}^{(l)}$ decays very fast as $T$ increases. Hence, we bound the second term. We substitute $\eta_{t,j}^{(l)} = \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t, (i)}) + \gamma_j^{(l)}$ in $\zeta_{T,j}^{(l)}$.
\begin{equation}
\begin{aligned}
\zeta _{T,j}^{(l)} &\in \kappa_{T,j}^{(l)} + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)} \partial | \z_{t, (j)} |) (1 - \alpha)^{T-t}\\
&\in \kappa_{T,j}^{(l)} + \sum_{t=1}^T \alpha  (\sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t-1, (i)}) + 2\gamma_j^{(l)} - \lambda_{t-1,j}^{(l)} \partial | \z_{t, (j)} |) (1 - \alpha)^{T-t}\\
&\leq \kappa_{T,j}^{(l)} + 2\gamma_j^{(l)} \sum_{t=1}^T \alpha (1 - \alpha)^{T-t}  + 2 \alpha \frac{\mu_l}{\sqrt{m}}\sum_{t=1}^T \sum_{i \neq j} E_{t-1,j} (1 - \alpha)^{T-t}\\
&\leq \kappa_{T,j}^{(l)} + 2\gamma_j^{(l)} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}}\sum_{t=1}^T  E_{t-1,j} (1 - \alpha)^{T-t}\\
\end{aligned}
\end{equation}
Given above, we find an upper bound on $E_{t-1,j} (1 - \alpha)^{T-t}$ below. From analysis in \Cref{thm:fwdzerrorvariable}, we have
\begin{equation}
\begin{aligned}
E_{t-1, j} &\leq v_{t-1, j} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
&+ 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t-2}\\
E_{t-1, j} (1 - \alpha)^{T-t} &\leq v_{t-1, j} (1 - \alpha)^{T-t} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} (1 - \alpha)^{T-t}\\
&+ 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t-2} (1 - \alpha)^{T-t}\\
\end{aligned}
\end{equation}
Re-write the first term,
\begin{equation}
\begin{aligned}
v_{t-1, j} (1 - \alpha)^{T-t} &= (E_{0, j} (1 - \alpha)^{t-1} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{t-k-1}) (1 - \alpha)^{T-t}\\
&= E_{0, j} (1 - \alpha)^{T-1} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + 2 \gamma_j^{(l)}) \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1}
\end{aligned}
\end{equation}
Similarly,
\begin{equation}
\begin{aligned}
v_{a, \text{max}} &= E_{0, \text{max}} (1 - \alpha)^{a} + (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k}
\end{aligned}
\end{equation}
We write
\begin{equation}
\begin{aligned}
\sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} &= \sum_{a=1}^{t-2} E_{0, \text{max}} (1 - \alpha)^{a} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
&+ \sum_{a=1}^{t-2} (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
&\leq \sum_{a=1}^{t-2} E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - 2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \sum_{a=1}^{t-2}  (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
\end{aligned}
\end{equation}
Hence,
\begin{equation}
\begin{aligned}
(1 - \alpha)^{T-t} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} &\leq \sum_{a=1}^{t-2} E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - 2} (1 - \alpha)^{T-t}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \sum_{a=1}^{t-2}  (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} (1 - \alpha)^{T-t}\\
&\leq (t-2) E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{T - 2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \frac{(1 - \alpha)^{T-t}}{\alpha (1 - 2\frac{\mu_l}{\sqrt{m}})}
\end{aligned}
\end{equation}
Combining all terms, we get
\begin{equation}\label{eq:Ealpha}
\begin{aligned}
E_{t-1, j} (1 - \alpha)^{T-t} &\leq v_{t-1, j} (1 - \alpha)^{T-t} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} (1 - \alpha)^{T-t}\\
&+ 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + 2 \alpha \frac{\mu_l}{\sqrt{m}})^{t-2} (1 - \alpha)^{T-t}\\
&\leq E_{0, j} (1 - \alpha)^{T-1} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \left( \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + 2 (s-1) \kappa_l (1 - \alpha)^{T-t} \right)\\
&\leq (1 + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \left( \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + 2 (s-1) \kappa_l (1 - \alpha)^{T-t} \right)\\
\end{aligned}
\end{equation}
where $\kappa_l = \frac{\frac{\mu_l}{\sqrt{m}}}{ (1 - 2\frac{\mu_l}{\sqrt{m}})}$. Moreover, we bound
\begin{equation}
\sum_{t=1}^T \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} = \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \frac{1 - (1-\alpha)^{t-1}}{\alpha}
\leq \sum_{t=1}^T (1 - \alpha)^{T-t} \leq \frac{1}{\alpha}
\end{equation}
Finally, we are ready to write the bound for $\zeta_{T,j}^{(l)}$
\begin{equation}
\begin{aligned}
| \zeta _{T,j}^{(l)} | &\leq \kappa_{T,j}^{(l)} + 2| \gamma_j^{(l)} | + 2 (s-1) \frac{\mu_l}{\sqrt{m}} (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) ( 1 + 2 (s-1) \kappa_l)\\
&+ \sum_{t=1}^T 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} ((1 + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2})\\
\end{aligned}
\end{equation}
Given $| \gamma_j^{(l)} | = a_{\gamma}$ with probability of $1 - \epsilon_{\gamma}^{(l)}$ where $a_{\gamma} = \sqrt{s \delta_l}$ and $s = \mathcal{O}^{\ast}(\sqrt{m} / \mu \log{m})$, we will have
\begin{equation}
    | \zeta_{T,j}^{(l)} | \leq \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2})
\end{equation}

\end{proof}
%
%
\fwdzerrorfixed*
%
\begin{proof}
We denote the regularization used in all layers $\lambda_1^j = \lambda_1^j = \cdots = \lambda_1^p = \lambda^{\text{fixed}}$. We assume that there exists such $\lambda^{\text{fixed}}$ that meets the lower bounds of regularization and also allow to pick an $\alpha >0$ according to \Cref{thm:supppres}. We define $\tilde \eta_{t,j}^{(l)} \coloneqq \sum_{i \neq j} | \langle \D_j^{(l)}, \D_i^{(l)} \rangle | | \z^{\ast}_{(i)} - \z_{t, (i)} | + \gamma_j^{(l)}$ and upper bound it as
\begin{equation}\label{eq:tildeeta_2}
\tilde \eta_{t,j}^{(l)} \leq \frac{\mu_l}{\sqrt{m}} \sum_{i \neq j} E_{t,i} + \gamma_j^{(l)}
\end{equation}
where $E_{t,i} \coloneqq |\z_{(i)}^{\ast}  - \z_{t,(i)}|$. Given~\eqref{eq:kkt}, we re-write the code recursion
\begin{equation}
\begin{aligned}
\z_{t+1, (j)} &= \prox_{\alpha \lambda_{t,j}^{(l)}}((1 - \alpha) \z_{t, (j)} + \alpha (1 - \beta_j^{(l)}) \z^{\ast}_{(j)} + \alpha \eta_{t,j}^{(l)})\\
&\in (1 - \alpha) \z_{t, (j)} + \alpha (1 - \beta_j^{(l)}) \z^{\ast}_{(j)} + \alpha (\eta_{t,j}^{(l)} - \lambda_{t,j}^{(l)} \partial | \z_{t+1, (j)} |)\\
E_{t+1, j} = | \z_{t+1, (j)} - \z_{(j)}^{\ast} | &\leq (1 - \alpha) E_{t, j} + \alpha \beta_j^{(l)} | \z^{\ast}_{(j)} | + \alpha (\tilde \eta_{t,j}^{(l)} + \lambda_{t,j}^{(l)} )
\end{aligned}
\end{equation}
Opening up the recursion, we get
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq E_{0,j} \prod_{q=0}^t (1 - \alpha) + \beta_j^{(l)} | \z^{\ast}_{(j)} | \sum_{a=1}^{t+1} \alpha \prod_{q=a}^{t+1} (1 - \alpha) + \sum_{a=1}^{t+1} \alpha (\tilde \eta_{a-1,j}^{(l)} + \lambda_{a-1,j}^{(l)} ) \prod_{q=a}^{t+1} (1 - \alpha)
\end{aligned}
\end{equation}
where we define $ \prod_{q=a}^a (1 - \alpha) = 1$. Using the upper bounds from~\eqref{eq:tildeeta_2} and $\lambda_{t,j}^{(l)}$ from \Cref{thm:supppres}, we get
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq v_{t+1, j} + r_{t+1, j} + \sum_{a=1}^{t+1} \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i \neq j} E_{a-1, i} \prod_{q=a}^{t+1} (1 - \alpha)
\end{aligned}
\end{equation}
where $v_{t+1, j} \coloneqq E_{0, j} \prod_{q=0}^t (1 - \alpha) + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + \gamma_j^{(l)}) \sum_{a=1}^{t+1} \alpha \prod_{q=a}^{t+1} (1 - \alpha)$ and $r_{t+1, j} \coloneqq \sum_{a=1}^{t+1} \alpha \lambda_{a-1, j}^{(l)} \prod_{q=a}^{t+1} (1 - \alpha)$. Following similar steps in~\Cref{thm:fwdzerrorvariable}, we now derive the general upper bound on $E_{t+1, j}$ as follows
\begin{equation}
\begin{aligned}
E_{1, i_1} &\leq v_{1, i_1} + r_{1, i_1} + \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_2 \neq i_1} E_{0, i_2}
\end{aligned}
\end{equation}
For $E_{2, i_1}$, we have
\begin{equation}
\begin{aligned}
E_{2, i_1} &\leq v_{2, i_1} + r_{2, i_1} + \alpha \frac{\mu_l}{\sqrt{m}} (\sum_{i_2 \neq i_1} E_{1, i_2} + \sum_{i_2 \neq i_1} E_{0, i_2} (1 - \alpha))
\end{aligned}
\end{equation}
Substituting $E_{1, i_1}$,
\begin{equation}
\begin{aligned}
E_{2, i_1} &\leq v_{2, i_1} +  r_{2, i_1} + \alpha \frac{\mu_l}{\sqrt{m}} (\sum_{i_2 \neq i_1}  (v_{1, i_2} +  r_{1, i_2} + \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_3 \neq i_2} E_{0, i_3}) + \sum_{i_2 \neq i_1} E_{0, i_2} (1 - \alpha))
\end{aligned}
\end{equation}
For $E_{3, i_1}$, we have
\begin{equation}
\begin{aligned}
E_{3, i_1} &\leq v_{3, i_1} + r_{3, i_1} + \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^3 \sum_{i_2 \neq i_1} E_{a-1,i_2} (1-\alpha)^{3-a}
\end{aligned}
\end{equation}
Unrolling the recursion,
\begin{equation}
\begin{aligned}
E_{3, i_1} &\leq v_{3, i_1} + r_{3, i_1} + \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_2 \neq i_1} (v_{2,i_2} + r_{2, i_2}) + \alpha \frac{\mu_l}{\sqrt{m}} \left((1-\alpha) \sum_{i_2 \neq i_1}  (v_{1,i_2} + r_{1, i_2}) + \alpha \frac{\mu_l}{\sqrt{m}} \sum_{i_3 \neq i_2} (v_{1,i_3} + r_{1, i_3}) \right)\\
&+ \alpha \frac{\mu_l}{\sqrt{m}} \left( (1 - \alpha)^2 \sum_{i_2 \neq i_1} E_{0,i_2} + 2 (1-\alpha) (\alpha \frac{\mu_l}{\sqrt{m}}) \sum_{i_3 \neq i_2, i_1}  E_{0,i_3} + (\alpha \frac{\mu_l}{\sqrt{m}})^2 \sum_{i_4 \neq i_3, i_2, i_1} E_{0,i_4} \right)
\end{aligned}
\end{equation}
Given above, we can write up the relation as
\begin{equation}
\begin{aligned}
E_{3, i_1} &\leq v_{3, i_1} + r_{3, i_1} + \alpha \frac{\mu_l}{\sqrt{m}} \left( (s-1) (v_{2,i} + r_{2, i}) + (v_{1,i} + r_{1, i}) ((1-\alpha) (s-1)  + \alpha \frac{\mu_l}{\sqrt{m}} (s-2)) \right)\\
&+ \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} \left( (s-1)(1 - \alpha)^2 + 2 (1-\alpha) (\alpha \frac{\mu_l}{\sqrt{m}}) (s-2) + (\alpha \frac{\mu_l}{\sqrt{m}})^2 (s-3)\right)\\
&\leq v_{3, i_1} + r_{3, i_1} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \left( v_{2,i} + r_{2, i} + (v_{1,i} + r_{1, i}) ((1-\alpha) + \alpha \frac{\mu_l}{\sqrt{m}}) \right)\\
&+ (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} \left( (1 - \alpha)^2 + 2 (1-\alpha) (\alpha \frac{\mu_l}{\sqrt{m}}) + (\alpha \frac{\mu_l}{\sqrt{m}})^2 \right)\\
\end{aligned}
\end{equation}
We denote $u_{t+1, i} \coloneqq v_{t+1, i} + r_{t+1, i}$ and following similar steps for $E_{4, i_1}$, we get, 
\begin{equation}
\begin{aligned}
E_{4, i_1} &\leq u_{4, i_1} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \left( u_{3,i} + u_{2,i} (1-\alpha + \alpha \frac{\mu_l}{\sqrt{m}}) + u_{1,i} ((1-\alpha)^2 + 2 (1-\alpha)(\alpha \frac{\mu_l}{\sqrt{m}}) + (\alpha \frac{\mu_l}{\sqrt{m}})^2) \right)\\
&+ (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} \left( (1 - \alpha)^3 + 3 (1-\alpha) (\alpha \frac{\mu_l}{\sqrt{m}})^2 + 3 (1-\alpha)^2 (\alpha \frac{\mu_l}{\sqrt{m}}) + (\alpha \frac{\mu_l}{\sqrt{m}})^3 \right)\\
\end{aligned}
\end{equation}
This leads to the term
\begin{equation}
\begin{aligned}
E_{4, i_1} &\leq u_{4, i_1} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \left( u_{3,i} + u_{2,i} (1-\alpha + \alpha \frac{\mu_l}{\sqrt{m}})^1 + u_{1,i} (1-\alpha + \alpha \frac{\mu_l}{\sqrt{m}})^2\right)\\
&+ (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0,i} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^3\\
\end{aligned}
\end{equation}
Hence, the general term for code error at $t$ layer is
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq u_{t+1, j} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t} (v_{a, \text{max}}+r_{a, \text{max}}) (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^t
\end{aligned}
\end{equation}
where for $j$ in the support, we define the upper bounds $v_{a, j} \leq v_{a, \text{max}}$, $r_{a, j} \leq r_{a, \text{max}}$, and $E_{0, j} \leq E_{0, \text{max}}$. Next, we define $(1 - \alpha)^t \leq \delta_{\alpha, t}^{\text{fixed}} \coloneqq (1 -\alpha + \alpha \frac{\mu_l}{\sqrt{m}})^t$, and use it to find an upper bound on the two expressions $\sum_{a=1}^{t} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a}$ and $\sum_{a=1}^{t} r_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a}$. The following bound can be achieved similar to the steps in~\Cref{thm:fwdzerrorvariable}
\begin{equation}
\sum_{a=1}^{t} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} \leq E_{0,\text{max}} t \delta_{\alpha,t}^{\text{fixed}} + \frac{1}{\alpha  (1 - \frac{\mu_l}{\sqrt{m}})} (\frac{\delta_l^2}{2} | \z_{\text{max}}^{\ast}| + a_{\gamma})
\end{equation}
Hence, we focus on $\sum_{a=1}^{t} r_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a}$ next. First, we rewrite
\begin{equation}
r_{t+1, j} = \sum_{a=1}^{t+1} \alpha \lambda_{a-1, j}^{(l)} \prod_{q=a}^{t+1} (1 - \alpha)
\end{equation} 
We replace all $\lambda_{t,j}^{(l)}$ with a fixed one $\lambda^{\text{fixed}}$, and write
\begin{equation}
\begin{aligned}
\sum_{a=1}^{t} r_{a, j} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} &\leq \sum_{a=1}^t \lambda^{\text{fixed}} \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k+1}) (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a}\\
&\leq \lambda^{\text{fixed}} \sum_{a=1}^t (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k+1}
\end{aligned}
\end{equation} 
Using sum of geometric series, we write $\sum_{a=1}^t (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} = \frac{1 - (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^t}{\alpha - \alpha \frac{\mu_l}{\sqrt{m}}} \leq \frac{1}{\alpha  (1 - \frac{\mu_l}{\sqrt{m}})}$. Hence, we get
\begin{equation}
\sum_{a=1}^{t} r_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a} \leq \lambda^{\text{fixed}} \frac{1}{\alpha  (1 - \frac{\mu_l}{\sqrt{m}})} 
\end{equation}
Hence, we bound the code error on the coefficients as following
\begin{equation}
\begin{aligned}
E_{t+1, j} &\leq u_{t+1, j} + (s-1) \frac{\mu_l}{\sqrt{m}} ( \frac{1}{(1 - \frac{\mu_l}{\sqrt{m}})} (\frac{\delta_l^2}{2} | \z_{\text{max}}^{\ast}| + a_{\gamma} + \lambda^{\text{fixed}})) + (t+1) (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} \delta_{\alpha,t}^{\text{fixed}}
\end{aligned}
\end{equation}
Next, we further simplify the first term. From before, we have
\begin{equation}
v_{t+1, j} = E_{0, j} (1 - \alpha)^{t+1} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + \gamma_j^{(l)}) \sum_{k=1}^{t+1} \alpha (1 - \alpha)^{t-k+1}
\leq E_{0, j} \delta_{\alpha,t+1}^{\text{fixed}} + \frac{\delta_l^2}{2} | \z^{\ast}_{\text{max}} | + a_{\gamma}
\end{equation}
and
\begin{equation}
\begin{aligned}
r_{t+1, j} = \alpha \lambda^{\text{fixed}} \sum_{k=1}^{t+1} (1 - \alpha)^{t-k+1} \leq \lambda^{\text{fixed}}
\end{aligned}
\end{equation}
Substituting the above upper bound into the upper bound for $E_{t+1,j}$, we get
\begin{equation}
E_{t+1, j} \leq E_{0, j} \delta_{\alpha,t+1}^{\text{fixed}} + (1 + (s-1) \kappa_l^{\text{fixed}}) (\frac{\delta_l^2}{2} | \z_{\text{max}}^{\ast}| + a_{\gamma} + \lambda^{\text{fixed}}) + (t+1)(s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} \delta_{\alpha,t}^{\text{fixed}}
\end{equation}
where $\kappa_l^{\text{fixed}} \coloneqq \frac{\mu_l}{\sqrt{m}} (\frac{1}{ (1 - \frac{\mu_l}{\sqrt{m}})}$. Given $s = \mathcal{O}^{\ast}(\sqrt{m}/\mu \log{m})$, we have $(s-1) \kappa_l^{\text{fixed}} < 1$. Hence, with probability of at least $1 - \epsilon_{\gamma}^{(l)}$, we have
\begin{equation}
\begin{aligned}
E_{t, j} &\leq  \mathcal{O}(a_{\gamma} + \lambda^{\text{fixed}}) + (s-1)t \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} \delta_{\alpha,t-1}^{\text{fixed}} + E_{0, j} \delta_{\alpha,t}^{\text{fixed}}\\
|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | &\leq  \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} + \lambda^{\text{fixed}} + e_{t,j}^{(l)\text{unroll, fixed}})
\end{aligned}
\end{equation}
where $e_{t,j}^{(l)\text{unroll, fixed}} \coloneqq (s-1)t \alpha \frac{\mu_l}{\sqrt{m}} \max_i | \z_{0,(i)}^{(l)} - \z_{(i)}^{\ast} | \delta_{\alpha,t-1}^{\text{fixed}} + | \z_{0,(j)}^{(l)} - \z_{(j)}^{\ast} | \delta_{\alpha,t}^{\text{fixed}}$. With appropriately large unrolled layer $t$, $e_{t,j}^{(l)\text{unroll, fixed}} \approx 0$. Hence, for the code error on non-zero coefficients, we get
\begin{equation}
|\z_{t,(j)}^{(l)} - \z_{(j)}^{\ast} | = \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} +  \lambda^{\text{fixed}})
\end{equation}
for large enough $t$. Now, we provide the relation for $\z_{T, (j)}$. We re-write~\eqref{eq:code_T}
\begin{equation}
\begin{aligned}
\z_{T, (j)} \in \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}
\end{aligned}
\end{equation}
where $\zeta _{T,j}^{(l)} = \kappa_{T,j}^{(l)} + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)}  \partial | \z_{t, (j)} |) (1 - \alpha)^{T-t}$ and $\kappa_{T,j}^{(l)} \coloneqq (1 - \alpha)^T (\z_{0, (j)} - \z^{\ast}_{(j)} (1 - \beta_j^{(l)}))$. $\kappa_{T,j}^{(l)}$ decays very fast as $T$ increases. Hence, we bound the second term. We substitute $\eta_{t,j}^{(l)} = \sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t, (i)}) + \gamma_j^{(l)}$ in $\zeta_{T,j}^{(l)}$.
\begin{equation}
\begin{aligned}
\zeta _{T,j}^{(l)} &\in \kappa_{T,j}^{(l)} + \sum_{t=1}^T \alpha  (\eta_{t-1,j}^{(l)} - \lambda_{t-1,j}^{(l)} \partial | \z_{t, (j)} |) (1 - \alpha)^{T-t}\\
&\in \kappa_{T,j}^{(l)} + \sum_{t=1}^T \alpha  (\sum_{i \neq j} \langle \D_j^{(l)}, \D_i^{(l)} \rangle (\z^{\ast}_{(i)} - \z_{t-1, (i)}) + \gamma_j^{(l)} - \lambda_{t-1,j}^{(l)} \partial | \z_{t, (j)} |) (1 - \alpha)^{T-t}\\
&\leq \kappa_{T,j}^{(l)} + (\gamma_j^{(l)} + \lambda^{\text{fixed}}) \sum_{t=1}^T \alpha (1 - \alpha)^{T-t}  + \alpha \frac{\mu_l}{\sqrt{m}}\sum_{t=1}^T \sum_{i \neq j} E_{t-1,j} (1 - \alpha)^{T-t}\\
&\leq \kappa_{T,j}^{(l)} + \gamma_j^{(l)} + \lambda^{\text{fixed}} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}}\sum_{t=1}^T  E_{t-1,j} (1 - \alpha)^{T-t}\\
\end{aligned}
\end{equation}
Given above, we find an upper bound on $E_{t-1,j} (1 - \alpha)^{T-t}$ below. From analysis in \Cref{thm:fwdzerrorvariable}, we have
\begin{equation}
\begin{aligned}
E_{t-1, j} &\leq v_{t-1, j} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
&+ (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t-2}\\
E_{t-1, j} (1 - \alpha)^{T-t} &\leq v_{t-1, j} (1 - \alpha)^{T-t} + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} (1 - \alpha)^{T-t}\\
&+ (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t-2} (1 - \alpha)^{T-t}\\
\end{aligned}
\end{equation}
Re-write the first term,
\begin{equation}
\begin{aligned}
v_{t-1, j} (1 - \alpha)^{T-t} &= (E_{0, j} (1 - \alpha)^{t-1} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + \gamma_j^{(l)} + \lambda^{\text{fixed}}) \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{t-k-1}) (1 - \alpha)^{T-t}\\
&= E_{0, j} (1 - \alpha)^{T-1} + (\beta_j^{(l)} | \z^{\ast}_{(j)} | + \gamma_j^{(l)} + \lambda^{\text{fixed}}) \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1}
\end{aligned}
\end{equation}
Similarly,
\begin{equation}
\begin{aligned}
v_{a, \text{max}} &= E_{0, \text{max}} (1 - \alpha)^{a} + (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k}
\end{aligned}
\end{equation}
We write
\begin{equation}
\begin{aligned}
\sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} &= \sum_{a=1}^{t-2} E_{0, \text{max}} (1 - \alpha)^{a} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
&+ \sum_{a=1}^{t-2} (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \sum_{k=1}^{a} \alpha (1 - \alpha)^{a-k} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
&\leq \sum_{a=1}^{t-2} E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - 2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \sum_{a=1}^{t-2}  (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2}\\
\end{aligned}
\end{equation}
Hence,
\begin{equation}
\begin{aligned}
(1 - \alpha)^{T-t} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} &\leq \sum_{a=1}^{t-2} E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - 2} (1 - \alpha)^{T-t}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \sum_{a=1}^{t-2}  (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} (1 - \alpha)^{T-t}\\
&\leq (t-2) E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{T - 2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \frac{(1 - \alpha)^{T-t}}{\alpha (1 - \frac{\mu_l}{\sqrt{m}})}
\end{aligned}
\end{equation}
Combining all terms, we get
\begin{equation}\label{eq:Ealpha_2}
\begin{aligned}
E_{t-1, j} (1 - \alpha)^{T-t} &\leq v_{t-1, j} (1 - \alpha)^{T-t} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} \sum_{a=1}^{t-2} v_{a, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t - a - 2} (1 - \alpha)^{T-t}\\
&+ (s-1) \alpha \frac{\mu_l}{\sqrt{m}} E_{0, \text{max}} (1 - \alpha + \alpha \frac{\mu_l}{\sqrt{m}})^{t-2} (1 - \alpha)^{T-t}\\
&\leq E_{0, j} (1 - \alpha)^{T-1} + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \left( \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + (s-1) \kappa_l (1 - \alpha)^{T-t} \right)\\
&\leq (1 + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) \left( \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + (s-1) \kappa_l (1 - \alpha)^{T-t} \right)\\
\end{aligned}
\end{equation}
where $\kappa_l^{\text{fixed}} = \frac{\frac{\mu_l}{\sqrt{m}}}{ (1 - \frac{\mu_l}{\sqrt{m}})}$. Moreover, we bound
\begin{equation}
\sum_{t=1}^T \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} = \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \frac{1 - (1-\alpha)^{t-1}}{\alpha}
\leq \sum_{t=1}^T (1 - \alpha)^{T-t} \leq \frac{1}{\alpha}
\end{equation}
Finally, we are ready to write the bound for $\zeta_{T,j}^{(l)}$
\begin{equation}
\begin{aligned}
| \zeta _{T,j}^{(l)} | &\leq \kappa_{T,j}^{(l)} + | \gamma_j^{(l)} | + \lambda^{\text{fixed}} + (s-1) \frac{\mu_l}{\sqrt{m}} (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + \gamma_{\text{max}}^{(l)} + \lambda^{\text{fixed}}) ( 1 + (s-1) \kappa_l^{\text{fixed}})\\
&+ \sum_{t=1}^T (s-1) \alpha \frac{\mu_l}{\sqrt{m}} ((1 + (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2})\\
\end{aligned}
\end{equation}
Given $| \gamma_j^{(l)} | = a_{\gamma}$ with probability of $1 - \epsilon_{\gamma}^{(l)}$ where $a_{\gamma} = \sqrt{s \delta_l}$ and $s = \mathcal{O}^{\ast}(\sqrt{m} / \mu \log{m})$, we will have
\begin{equation}
    | \zeta_{T,j}^{(l)} | \leq \mathcal{O}(\sqrt{s \| \D_j^{(l)} - \D_j^{\ast} \|_2} + \lambda^{\text{fixed}})
\end{equation}
\end{proof}
%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%

We now re-state the forward pass Jacobian (\Cref{thm:fwdj}) convergence.
%
\fwdj*
%
\begin{proof}
Differentiating the recursion,
\begin{equation*}
\J_{t+1} = \nabla_1 \Phi(\z_t, \D)^{\text{T}} \J_t + \nabla_2 \Phi(\z_t, \D)^{\text{T}}.
\end{equation*}
Similarly, 
\begin{equation*}
\hat \J = \nabla_1 \Phi(\hat \z, \D)^{\text{T}} \hat \J + \nabla_2 \Phi(\hat \z, \D)^{\text{T}}
\end{equation*}
where $\hat \z$ is a minimizer of lasso and fixed-point of the mapping (see \Cref{lemma:fixedpoint}). Subtract the terms
\begin{equation*}
\J_{t+1} - \hat \J =  \nabla_1 \Phi(\z_t, \D)^{\text{T}} (\J_t - \hat \J) + (\nabla_1 \Phi(\z_t, \D) - \nabla_1 \Phi(\hat \z, \D))^{\text{T}} \hat \J +  (\nabla_2 \Phi(\z_t, \D) - \nabla_2 \Phi(\hat \z, \D))^{\text{T}}
\end{equation*}
Given the Lipschitz properties of $\Loss$ and $h$, we can further get the upper bounds on $\| \nabla_1 \Phi({\bm a}, \D) - \nabla_1 \Phi({\bm b}, \D) \|_2 \leq L_{\Phi_1} \| {\bm b} - {\bm a} \|_2$ and $\| \nabla_2 \Phi({\bm a}, \D) - \nabla_2 \Phi({\bm b}, \D) \|_2 \leq L_{\Phi_2} \| {\bm b} - {\bm a} \|_2$. Hence, with upper bound on the norm of Jacobian (\Cref{assum:boundj}), there exists $B>0$ such that $\forall t > B$
\begin{equation*}
\begin{aligned}
\| \J_{t+1} - \hat \J \|_2 &\leq  \| \nabla_1 \Phi(\z_t, \D) \|_2 \| \J_t - \hat \J \|_2 + \| \nabla_1 \Phi(\z_t, \D) - \nabla_1 \Phi(\hat \z, \D) \|_2 \| \hat \J \|_2\\
&+  \| \nabla_2 \Phi(\z_t, \D) - \nabla_2 \Phi(\hat \z, \D) \|_2\\
& \leq \rho \| \J_t - \hat \J \|_2 + c \| \z_t - \hat \z \|_2
\end{aligned}
\end{equation*}
where $c \triangleq M_J L_{\Phi_1} + L_{\Phi_2}$. Hence,
\begin{equation*}
\| \J_{t+1} - \hat \J \|_2  \leq \rho \| \J_{t} - \hat \J \|_2 + \mathcal{O}(\rho^t).
\end{equation*}
Unrolling the recursion,
\begin{equation*}
\| \J_{t+1} - \hat \J \|_2  \leq \mathcal{O}((t+1)\rho^t).
\end{equation*}
\end{proof}
%
%
\fwdzglobal*
%
\begin{proof}
We first find the error between $\hat \z$ and $\hat \z^{\ast}$ which is the unique minimizer of lasso~\eqref{eq:lasso} given the true dictionary $\D^{\ast}$. Using fixed-point property (\Cref{lemma:fixedpoint}), we get
\begin{equation}
\| \hat \z - \hat \z^{\ast} \|_2 = \| \Phi(\hat \z, \D) - \Phi(\hat \z^{\ast}, \D^{\ast}) \|_2 \leq  \| \Phi(\hat \z, \D) - \Phi(\hat \z^{\ast}, \D) \|_2 +  \| \Phi(\hat \z^{\ast}, \D) - \Phi(\hat \z^{\ast}, \D^{\ast}) \|_2
\end{equation}
Using the $\mu$-strongly convexity of $\Loss_{\x}(\z_t, \D)$ on the support, and $L_{21}$ Lipschitz constants of $\nabla_{21}^2\Loss_{\x}(\z, \D)$, we upper bound the term as follows:
\begin{equation}
\| \hat \z - \hat \z^{\ast} \|_2 \leq \rho \| \hat \z - \hat \z^{\ast} \|_2 + \alpha L_{21} c_{\text{prox}} \| \D - \D^{\ast} \|_2
\end{equation}
Where $\rho \triangleq  c_{\text{prox}} (1-\alpha\mu) < 1$. Denote $q \triangleq \frac{\alpha c_{\text{prox}} L_{21}}{1 - \rho}$ which can be made to be small with proper choice of step size $\alpha$.
\begin{equation}
\| \hat \z - \hat \z^{\ast} \|_2 \leq q \| \D - \D^{\ast} \|_2
\end{equation}
Hence, we get the following code error
\begin{equation}
\| \hat \z - \z^{\ast} \|_2 \leq \| \hat \z - \hat \z^{\ast} \|_2 + \| \hat \z^{\ast} - \z^{\ast} \|_2 \leq q \| \D - \D^{\ast} \|_2 + \hat \delta^{\ast}  \leq \mathcal{O}(\| \D - \D^{\ast} \|_2 + \hat \delta^{\ast})
\end{equation}
\end{proof}
%
\fwdjglobal*
%
\begin{proof}
First, we define $\J^{\ast}$. For $\z^{\ast}$, we define the mapping function $\z \rightarrow \Loss_{\x}(\z, \D)$, where $\z^{\ast}(\D)$ is its minimizer evaluated at $\D^{\ast}$, i.e., $\nabla_{1} \Loss_{\x}(\z^{\ast}, \D^{\ast}) = \D^{\ast\text{T}} ({\bm D}^{\ast} \z^{\ast} - \x) = 0$ given the generative model ($\x = \D^{\ast} \z^{\ast}$). Hence, we define the Jacobian $\J^{\ast} = \frac{\partial \z^{\ast}(\D)}{\partial \D}\rvert_{\D = \D^{\ast}}$. From implicit function theorem, we get
$$\J^{\ast+}  \nabla_{11}^2 \Loss_{\x}(\z^{\ast}, \D^{\ast}) + \nabla_{21}^2 \Loss_{\x}(\z^{\ast}, \D^{\ast}) = \zero$$
which is later used in the global backward pass analysis. Alternatively, if $\nabla_{11}^2 \Loss_{\x}(\z^{\ast}, \D^{\ast})$ is invertible, then we can compute $\J^{\ast+}$ as follows:
$$\J^{\ast+} = - \nabla_{21}^2 \Loss_{\x}(\z^{\ast}, \D^{\ast}) \nabla_{11}^2 \Loss_{\x}(\z^{\ast}, \D^{\ast})^{-1}$$
The Jacobian w.r.t row $i$ of the dictionary is
$$
\J^{\ast}_{(i,:)} = - (\D^{\ast\text{T}}_{S^{\ast}} \D^{\ast }_{S^{\ast}})^{-1} (\D^{\ast}_{i, :} \z^{\ast\text{T}} + (\D^{\ast \text{T}}_{i, :} \z^{\ast} - \x_i) {\bm I}_p)_{S^{\ast}}
$$
on the support $S^{\ast}$ of $\z^{\ast}$. Outside of the support, it is zero. Now, given the recursion $\z_{t+1} = \Phi(\z_t, \D)$, we differentiate the recursion,
\begin{equation*}
\J_{t+1} = \nabla_1 \Phi(\z_t, \D)^{\text{T}} \J_t + \nabla_2 \Phi(\z_t, \D)^{\text{T}}.
\end{equation*}
Hence, we have
\begin{equation*}
\hat \J = \nabla_1 \Phi(\hat \z, \D)^{\text{T}} \hat \J + \nabla_2 \Phi(\hat \z, \D)^{\text{T}}
\end{equation*}
\begin{equation*}
\hat \J^{\ast} = \nabla_1 \Phi(\hat \z^{\ast}, \D^{\ast})^{\text{T}} \hat \J^{\ast} + \nabla_2 \Phi(\hat \z^{\ast}, \D^{\ast})^{\text{T}}
\end{equation*}
where ${\hat \J^{\ast}}$ is the Jacobian of $\hat \z^{\ast}$. Then, following similar step to~\Cref{thm:fwdj}, we can write
\begin{equation*}
\hat \J - \hat \J^{\ast} =  \nabla_1 \Phi(\hat \z, \D)^{\text{T}} (\hat \J - \hat \J^{\ast}) + (\nabla_1 \Phi(\hat \z, \D) - \nabla_1 \Phi(\hat \z^{\ast}, \D^{\ast}))^{\text{T}} \hat \J^{\ast} +  (\nabla_2 \Phi(\hat \z, \D) - \nabla_2 \Phi(\hat \z^{\ast}, \D^{\ast}))^{\text{T}}
\end{equation*}
With respect to $\D$, we denote the Lipschitz constants of $\nabla_1 \Phi(\hat \z, \D)$ and $\nabla_2 \Phi(\hat \z, \D)$ with $L_{\Phi_{1D}}$ and $L_{\Phi_{2D}}$, respectively. Then,
\begin{equation*}
\| \hat \J - \hat \J^{\ast} \|_2 \leq \rho \| \hat \J - \hat \J^{\ast} \|_2 + c \| \hat \z - \hat \z^{\ast} \|_2 + c_{D} \| \hat \D - \D^{\ast} \|_2
\end{equation*}
where $c \triangleq M_J L_{\Phi_1} + L_{\Phi_2}$ and $c_D \triangleq M_J L_{\Phi_{1D}} + L_{\Phi_{2D}}$. Given the global forward pass code error, we get
\begin{equation}
\| \hat \J - \hat \J^{\ast} \|_2 \leq q_z \| \hat \z - \hat \z^{\ast} \|_2 + q_D \| \D - \D^{\ast} \|_2 \leq (q_D + q_z q) \| \D - \D^{\ast} \|_2
\end{equation}
where $q_z \triangleq \frac{c}{1 - \rho}$, $q_D \triangleq \frac{c_D}{1 - \rho}$. Hence, we get
\begin{equation}
\| \hat \J - \J^{\ast} \|_2 \leq \| \hat \J - \hat \J^{\ast} \|_2 + \| \hat \J^{\ast} - \J^{\ast} \|_2 \leq \mathcal{O}(\| \D - \D^{\ast} \|_2 + \hat \delta_J^{\ast})
\end{equation}
where we denote $\hat \delta_J^{\ast} \coloneqq \| \hat \J^{\ast} - \J^{\ast} \|_2$
\end{proof}
%


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%
%
\subsection{Local backward pass proof details}
%
In each update of the dictionary, we bound the gradient approximations as function of unrolling $t$ (\Cref{thm:localgradient}). This shows that $\g_t^{\text{ae-lasso}}$ converges faster than $\g_t^{\text{dec}}$ and $\g_t^{\text{ae-ls}}$, and the latter is a biased estimator of $\hat \g$. This is followed by \Cref{thm:localgradient} showing the order magnitude of the bounds is indeed tight.
%
\localgrad*
%
\begin{proof}
For $\g_t^{\text{dec}}$, with the infinite fresh samples, we have $\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n \nabla_2 \Loss_{\x^i}(\z_t^{i}, \D)= \E_{\x \in \X}\ [\nabla_2 \Loss_{\x}(\z_t, \D)] \ \text{a.s.}$ Based on~\Cref{lemma:lipdiffloss}, we get
\begin{equation}
\begin{aligned}
\| \g_t^{\text{dec}} - \hat \g \|_2 &= \| \E_{\x \in \X}\ [\nabla_2 \Loss_{\x}(\z_t, \D)] - \E_{\x \in \X}\ [\nabla_2 \Loss_{\x}(\hat \z, \D)] \|_2\\
&\leq  \E_{\x \in \X}\ [ \| \nabla_2 \Loss_{\x}(\z_t, \D) - \nabla_2 \Loss_{\x}(\hat \z, \D) \|_2 ]
\leq  \E_{\x \in \X}\ [ L_2 \| \z_t - \hat \z \|_2 ] \leq \mathcal{O}(\rho^t).
\end{aligned}
\end{equation}
%
Similarly, for $\g_t^{\text{ae-lasso}}$ and $\g_t^{\text{ae-ls}}$, we replace the sample mean for gradient computations with expectation in their limit. We re-write the gradient estimation error as following
\begin{equation}
\begin{aligned}
\g_t^{\text{ae-lasso}} - \hat \g  &= \E_{\x \in \X}\ [Q(\hat \z, \J_t) (\z_t - \hat \z)] + \E_{\x \in \X}\ [Q_t^{21}(\hat \z)] + \E_{\x \in \X}\ [\J_t Q_t^{\text{lasso-}11}(\hat \z)]\\
\g_t^{\text{ae-ls}} - \hat \g  &= \E_{\x \in \X}\ [Q(\hat \z, \J_t) (\z_t - \hat \z)] + \E_{\x \in \X}\ [Q_t^{21}(\hat \z)] + \E_{\x \in \X}\ [\J_t Q_t^{\text{ls-}11}(\hat \z)]
\end{aligned}
\end{equation}
where
\begin{equation}
\begin{aligned}
Q_t^{21}(\z) &\triangleq \nabla_2 \Loss_{\x}(\z_t, \D) -  \nabla_2 \Loss_{\x}(\z, \D) - \nabla_{21}^2\Loss_{\x}(\z, \D) (\z_t - \z)\\
Q_t^{\text{lasso-}11}(\z) &\triangleq \nabla_1 \Loss_{\x}(\z_t, \D) + \partial h(\z_t) - \nabla_{11}^2\Loss_{\x}(\z, \D) (\z_t - \z)\\
Q_t^{\text{ls-}11}(\z) &\triangleq \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_{11}^2\Loss_{\x}(\z, \D) (\z_t - \z)\\
Q(\z, \J) &\triangleq \J^+ \nabla_{11}^2\Loss_{\x}(\z, \D) + \nabla_{21}^2\Loss_{\x}(\z, \D).
\end{aligned}
\end{equation}
We provide bounds on the above in \Cref{lemma:qlocal}. Hence, it suffices to bound the terms on the {\it r.h.s} as follows:
\begin{equation}
\| \g_t^{\text{ae-lasso}} - \hat \g  \|_2 \leq \E_{\x \in \X}\ [ L_1 \| \J_t - \hat \J \|_2 \| \z_t - \hat \z \|_2 + (\nicefrac{L_{21}}{2}) \| \z_t - \hat \z \|_2^2 + M_J (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 ].
\end{equation}
Using the convergence errors from the forward pass (\Cref{thm:fwdz,thm:fwdj}),
 \begin{equation}
\begin{aligned}
\| \g_t^{\text{ae-lasso}} - \hat \g  \|_2 &\leq L_1 \mathcal{O}(t \rho^{2t}) + \left(\nicefrac{L_{21}}{2} + M_J (\nicefrac{L_{11}}{2})\right) \mathcal{O}(\rho^{2t}) = \mathcal{O}(t \rho^{2t}).
\end{aligned}
\end{equation}
Similarly,
\begin{equation}
\| \g_t^{\text{ae-ls}} - \hat \g  \|_2 \leq \E_{\x \in \X}\ [ L_1 \| \J_t - \hat \J \|_2 \| \z_t - \hat \z \|_2 + (\nicefrac{L_{21}}{2}) \| \z_t - \hat \z \|_2^2 + M_J ((\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 + \| \partial h(\hat \z) \|_2)].
\end{equation}
Using the convergence errors from the forward pass (\Cref{thm:fwdz,thm:fwdj}),
 \begin{equation}
\begin{aligned}
\| \g_t^{\text{ae-ls}} - \hat \g  \|_2 &\leq L_1 \mathcal{O}(t \rho^{2t}) + \left(\nicefrac{L_{21}}{2} + \nicefrac{M_J L_{11}}{2}\right) \mathcal{O}(\rho^{2t}) + M_J \| \partial h(\hat \z) \|_2 = \mathcal{O}(t \rho^{2t} + M_J \lambda \sqrt{s}).
\end{aligned}
\end{equation}
\end{proof}
%
\begin{restatable}[Tight local bound]{lemma}{localtightbound}\label{lemma:localtightbound}
The order magnitude of the upper bounds in~\Cref{thm:localgradient} is tight.
\end{restatable}
%
\begin{proof}
It is sufficient to show that there exist an example such that its forward pass code and Jacobian convergences are $\mathcal{O}(\rho^t)$ and $\mathcal{O}(t \rho^t)$, respectively. The following example confirms this. Without loss of generality, let $\z^{\ast}$ be $1$-sparse and  non-negative, $\D = \D^{\ast}$ and $\D_{j} = \zero$ for $j \neq i$. The loss function is $\frac{1}{2} \| \D_i^{\ast} \z^{\ast}_{(i)} - \D_i \z_{(i)} \|_2^2 + \lambda | \z_{(i)} |$. Given the support recovery after first iteration, the encoder forward pass implements $\z_{t+1, (i)} = \z_{t, (i)} - \alpha (\D^{\text{T}}_i(\D_i \z_{t, (i)} - \D^{\ast}_i \z^{\ast}_{(i)}) + \lambda) = (1- \alpha) \z_{t, (i)} + \alpha (\z^{\ast}_{(i)} - \lambda)$. Hence, the forward pass convergences are
\begin{equation}
\begin{aligned}
\z_{t, (i)} &= (1 - \alpha)^t \z_0 + \sum_{k=1}^t \alpha (1 - \alpha)^{t-k} (\z^{\ast}_{(i)} - \lambda) =  (1 - \alpha)^t \z_0 +( 1 - (1 - \alpha)^{t}) (\z^{\ast}_{(i)} - \lambda)\\
\z_{t, (i)}  - \hat \z_{(i)} &= \rho^{t} (\z_0 - \z^{\ast}_{(i)} + \lambda) = \mathcal{O}(\rho^t)
\end{aligned}
\end{equation}
and
\begin{equation}
\begin{aligned}
\J_{t, (i)} &= \J_{t-1, (i)} - \alpha (\J_{t-1, (i)} + 2 \D_i \z_{t, (i)}  - \D^{\ast}_i \z^{\ast}_{(i)}) = \rho \J_{t-1, (i)} + \mathcal{O}(\rho^t) + \hat \J_{(i)}\\
\J_{t, (i)}  - \hat \J_{(i)} &= \rho^{t} \J_{0, (i)} + \sum_{k=1}^t \mathcal{O}(\rho^t) = \mathcal{O}(t \rho^t)
\end{aligned}
\end{equation}
where $\rho = 1 - \alpha$, $\hat \z_{(i)} = \z^{\ast}_{(i)} - \lambda$, and $\hat \J_{(i)} = \alpha (2 \D_i \hat \z_{(i)} - \D_i^{\ast} \z^{\ast}_{(i)})$
\end{proof}
%
%%
\begin{restatable}[Local bounds]{lemma}{qlocal}\label{lemma:qlocal}
From local gradient errors in \Cref{thm:localgradient}, the following are satisfied
\begin{equation}
\begin{aligned}
\| Q_t^{21}(\hat \z) \|_2 &\leq (\nicefrac{L_{21}}{2}) \| \z_t - \hat \z \|_2^2,\\
\| Q(\hat \z, \J_t) \|_2 &\leq L_1 \| \J_t - \hat \J \|_2,
\end{aligned}
\begin{aligned}
\quad \quad \| Q_t^{\text{lasso-}11}(\hat \z) \|_2 &\leq (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2\\
\| Q_t^{\text{ls-}11}(\hat \z) \|_2 &\leq (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 + \| \partial h(\hat \z) \|_2.
\end{aligned}
\end{equation}
\end{restatable}
%
\begin{proof}
For $Q_t^{21}(\hat \z)$, given convexity of $\nabla_1 \Loss_{\x}(\z, \D)$ and its domain (\Cref{assum:domain}) and \Cref{lemma:lipdiffloss}, we achieve the quadratic upper bound. For $Q_t^{\text{lasso-}11}(\hat \z)$, we add and subtract $\nabla_1\Loss_{\x}(\hat \z, \D)$, and then use quadratic upper bound. At line four, given~\Cref{lemma:kktlasso}, we use $\zero \in \nabla_1 \Loss_{\x}(\hat \z, \D) + \partial h(\hat \z)$ and assume that $\z_t$ recovers the sign entries of $\hat \z$.
\begin{equation}
\begin{aligned}
\| Q_t^{\text{lasso-}11} \|_2 &= \| \nabla_1 \Loss_{\x}(\z_t, \D) + \partial h(\z_t) - \nabla_{11}^2\Loss_{\x}(\hat \z, \D) (\z_t - \hat \z) \|\\
&= \| \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_1\Loss_{\x}(\hat \z, \D)  + \nabla_1\Loss_{\x}(\hat \z, \D) + \partial h(\z_t) - \nabla_{11}^2\Loss_{\x}(\hat \z, \D) (\z_t - \hat \z) \|\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 + \| \partial h(\z_t) + \nabla_1\Loss_{\x}(\hat \z, \D) \|_2\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 + \| \partial h(\z_t) - \partial h(\hat \z)  \|_2
\leq  (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2.
\end{aligned}
\end{equation}
%
Similarly,
\begin{equation}
\begin{aligned}
\| Q_t^{\text{ls-}11} \|_2 &= \| \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_{11}^2\Loss_{\x}(\hat \z, \D) (\z_t - \hat \z) \|\\
&= \| \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_1\Loss_{\x}(\hat \z, \D)  + \nabla_1\Loss_{\x}(\hat \z, \D) - \nabla_{11}^2\Loss_{\x}(\hat \z, \D) (\z_t - \hat \z) \|\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 + \| \nabla_1\Loss_{\x}(\hat \z, \D) \|_2
\leq (\nicefrac{L_{11}}{2}) \| \z_t - \hat \z \|_2^2 + \| \partial h(\hat \z) \|_2.
\end{aligned}
\end{equation}
%
For $Q(\hat \z, \J_t)$, from implicit function theorem, $Q(\hat \z, \hat \J) = \zero$ under the support $S$ of $\hat \z$ that is identified by $\z_t$. To prove this, consider the minimizer $\hat \z(\D)$. We have $\zero \in \nabla_1 f(\hat {\bm z}, {\bm D})$, hence, we get $\zero \in \hat {\J}({\D}) \nabla_{11}^2 f(\hat \z, \D) + \nabla_{21}^2 f(\hat \z, \D)$. Given the support recovery, the relation $\hat \J(\D) (\nabla_{11}^2 \Loss_{\x}(\hat \z, \D) \odot \mathbf{1}_{S^{\ast}}) + \nabla_{21}^2 \Loss_{\x}(\hat \z, \D) \odot \mathbf{1}_{S^{\ast}} = \zero$ also holds which is equivalent to $Q(\hat \z, \hat \J)$ under the support. To show this, given the recursion ${\z}_{t+1} = \Phi(\z_t, \D)$, we differentiate it and get $\J_{t+1} = \nabla_1 \Phi(\z_t, \D) \J_t + \nabla_2 \Phi(\z_t, \D)$. Given the support recovery and fixed-point property, we can write
\begin{equation}
\begin{aligned}
\hat \J &= \mathbf{1}_{S} \odot (\hat \J - \alpha \nabla_{11}^2 \Loss_{\x}(\hat \z, \D)^{\text{T}} \hat \J) + \mathbf{1}_{S} \odot (- \alpha \nabla_{21}^2 \Loss_{\x}(\hat \z, \D)^{\text{T}})\\
%
\hat \J - \mathbf{1}_{S} \odot \hat \J &=  - \hat \J \alpha \nabla_{11}^2 \Loss_{\x}(\hat \z, \D)^{\text{T}} \odot \mathbf{1}_{S}  - \alpha \nabla_{21}^2 \Loss_{\x}(\hat \z, \D)^{\text{T}} \odot \mathbf{1}_{S}\\
%
\zero &= \hat \J^{+} (\nabla_{11}^2 \mathcal{L}(\hat \z, \D) \odot \mathbf{1}_{S}) +  \nabla_{21}^2 \Loss_{\x}(\hat \z, \D) \odot \mathbf{1}_{S}
\end{aligned}
\end{equation}
If the term $(\nabla_{11}^2 \mathcal{L}(\z_t, \D) \odot \mathbf{1}_{S})$ is invertible, then we can write
\begin{equation}
\hat \J^{+} = - \nabla_{21}^2 \Loss_{\x}(\hat \z, \D) \odot \mathbf{1}_{S} (\nabla_{11}^2 \mathcal{L}(\hat \z, \D) \odot \mathbf{1}_{S})^{-1}
\end{equation}
For the Jacobian corresponding to row $i$ of the dictionary, we get
\begin{equation}
\hat \J_{(i,:)} = - (\D_S^{\text{T}} \D_S)^{-1} (\D_{i, :} \hat \z^{\text{T}} + (\D_{i, :}^{\text{T}} \hat \z - \x_i) \eye_p)_S
\end{equation}
on the support. Outside of the support $S$, the Jacobian is zero.
%
%
This proof is similarly provided by~\citet{malezieux2022understanding}. Hence, we can use $\nabla_{21}^2\Loss_{\x}(\hat \z, \D) = - \hat \J^{+} \nabla_{11}^2\Loss_{\x}(\hat \z, \D)$ under the support $S$ in the following
\begin{equation}
\begin{aligned}
\| Q(\hat \z, \J_t) \|_2 &= \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\hat \z, \D) + \nabla_{21}^2\Loss_{\x}(\hat \z, \D) \|_2
= \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\hat \z, \D) - \hat \J^{+} \nabla_{11}^2\Loss_{\x}(\hat \z, \D) \|_2\\
&\leq \| (\J_t - \hat \J)^{+} \nabla_{11}^2\Loss_{\x}(\hat \z, \D) \|_2 \leq L_1 \| \J_t - \hat \J \|_2.
\end{aligned}
\end{equation}
\end{proof}
%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%
%
\subsection{Global backward pass proof details}
%
%
We re-state and proof \Cref{thm:globalgradient} as follows:
%
\globalgrad*
%
\begin{proof}
For $\g_t^{\text{dec}}$, we compute the gradient in their limit assuming infinite fresh samples $\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n \nabla_2 \Loss_{\x^i}(\z_t^{i}, \D)= \E_{\x \in \X}\ [\nabla_2 \Loss_{\x}(\z_t, \D)] \ \text{a.s.}$. Similar to \Cref{thm:localgradient}, we re-write the errors of gradients $\g_t^{\text{ae-lasso}}$ and $\g_t^{\text{ae-ls}}$ as following
%Based on~\Cref{lemma:lipdiffloss},
%\begin{equation}
%\begin{aligned}
%\| \g_t^{\text{dec}} - \g^{\ast} \|_2 &= \| \E_{\x \in \X}\ [\nabla_2 \Loss_{\x}(\z_t, \D)] - \E_{\x \in \X}\ [\nabla_2 \Loss_{\x}(\z^{\ast}, \D)] \|_2
%\leq  \E_{\x \in \X}\ [ \| \nabla_2 \Loss_{\x}(\z_t, \D) - \nabla_2 \Loss_{\x}(\z^{\ast}, \D) \|_2 ]\\
%&\leq  \E_{\x \in \X}\ [ L_2 \| \z_t - \z^{\ast} \|_2 ]
%\leq \E_{\x \in \X}\ [ L_2 (\| \z_t - \hat \z \|_2 + \| \hat \z - \hat \z^{\ast} \|_2 + \| \hat \z^{\ast} - \z^{\ast} \|_2)]\\
%&\leq \mathcal{O}(\rho^t + \|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast}).
%\end{aligned}
%\end{equation}
%Hence,
%\begin{equation}
%\| \g_{\infty}^{\text{dec}} - \g^{\ast} \|_2 \leq \mathcal{O}(\|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast}).
%\end{equation}
%%
%
\begin{equation}
\begin{aligned}
\g_t^{\text{ae-lasso}} - \g^{\ast}  &= \E_{\x \in \X}\ [Q(\z^{\ast}, \J_t) (\z_t - \z^{\ast})] + \E_{\x \in \X}\ [Q_t^{21}(\z^{\ast})] + \E_{\x \in \X}\ [\J_t Q_t^{\text{lasso-}11}(\z^{\ast})]\\
\g_t^{\text{ae-ls}} - \g^{\ast}  &= \E_{\x \in \X}\ [Q(\z^{\ast}, \J_t) (\z_t - \z^{\ast})] + \E_{\x \in \X}\ [Q_t^{21}(\z^{\ast})] + \E_{\x \in \X}\ [\J_t Q_t^{\text{ls-}11}(\z^{\ast})].
\end{aligned}
\end{equation}
where $Q_t^{21}(\z)$, $Q_t^{\text{lasso-}11}(\z)$, $Q_t^{\text{ls-}11}(\z)$, and $Q(\z, \J)$ are defined as in \Cref{thm:localgradient}. Given \Cref{assum:boundj} and \Cref{lemma:qstartilde}, we find an upper bound on the {\it r.h.s} of the gradient errors as follows:

\begin{equation}
\begin{aligned}
\| \g_t^{\text{ae-lasso}} - \g^{\ast}  \|_2 &\leq \E_{\x \in \X}\ [ (L_1 \| \J_t - \J^{\ast} \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2) \| \z_t - \z^{\ast} \|_2 + (\nicefrac{L_{21}}{2}) \| \z_t - \z^{\ast} \|_2^2]\\
& + \E_{\x \in \X}\ [ M_J (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + M_J \| \partial h(\z_t) \|_2 + L_{1D} \| \D - \D^{\ast}\|_2]\\
&\leq \E_{\x \in \X}\ [ L_1(\| \J_t - \hat \J \|_2 +  \| \hat \J - \J^{\ast} \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2) (\| \z_t - \hat \z \|_2 + \| \hat \z - \z^{\ast} \|_2)]\\
& + \E_{\x \in \X}\ [(\nicefrac{L_{21}}{2}) (\| \z_t - \hat \z \|_2^2 + \| \hat \z - \z^{\ast} \|_2^2) + L_{1D} \| \D - \D^{\ast}\|_2]\\
& + \E_{\x \in \X}\ [ M_J (\nicefrac{L_{11}}{2}) (\| \z_t - \hat \z \|_2^2 +  \| \hat \z - \z^{\ast} \|_2^2 ) + M_J \| \partial h(\z_t) \|_2]
\end{aligned}
\end{equation}
Similarly,
\begin{equation}
\begin{aligned}
\| \g_t^{\text{ae-ls}} - \g^{\ast}   \|_2 &\leq \E_{\x \in \X}\ [ (L_1 \| \J_t - \J^{\ast}  \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2 )\| \z_t - \z^{\ast}  \|_2 + (\nicefrac{L_{21}}{2}) \| \z_t - \z^{\ast}  \|_2^2]\\
& + \E_{\x \in \X}\ [ M_J (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast}  \|_2^2 + L_{1D} \| \D - \D^{\ast} \|_2]\\
&\leq \E_{\x \in \X}\ [ L_1(\| \J_t - \hat \J \|_2 +  \| \hat \J - \J^{\ast} \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2) (\| \z_t - \hat \z \|_2 + \| \hat \z - \z^{\ast}  \|_2)]\\
& + \E_{\x \in \X}\ [(\nicefrac{L_{21}}{2}) (\| \z_t - \hat \z \|_2^2 + \| \hat \z - \z^{\ast}  \|_2^2)]\\
& + \E_{\x \in \X}\ [ M_J (\nicefrac{L_{11}}{2}) (\| \z_t - \hat \z \|_2^2 +  \| \hat \z - \z^{\ast}  \|_2^2) + L_{1D} \| \D - \D^{\ast} \|_2].
\end{aligned}
\end{equation}
Using the convergence errors from the forward pass (\Cref{thm:fwdz,thm:fwdj}),
%
\begin{equation}
\begin{aligned}
\| \g_t^{\text{ae-lasso}} - \g^{\ast}  \|_2 &\leq L_1 \mathcal{O}(t \rho^{2t} + ( \|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast})  t\rho^{t} + \rho^t (\|  \D - \D^{\ast} \|_2 + \hat \delta_J^{\ast})\\
&+ L_1 \mathcal{O}(\|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast}) (\|  \D - \D^{\ast} \|_2 + \hat \delta_J^{\ast}))\\
&+ M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2 (\rho^{t} + (\|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast}))\\
&+ \left(\nicefrac{L_{21}}{2} + \nicefrac{M_JL_{11}}{2}\right) \mathcal{O}(\rho^{t} +  \|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast}) + \mathcal{O}(\| \D - \D^{\ast} \|_2) + M_J \| \partial h(\z_t) \|_2)
\end{aligned}
\end{equation}
Hence,
\begin{equation}
\begin{aligned}
\| \g_{\infty}^{\text{ae-lasso}} - \g^{\ast}  \|_2 &\leq \mathcal{O}((\|  \D - \D^{\ast} \|_2 + \hat \delta_J^{\ast}) (\|  \D - \D^{\ast} \|_2 + \hat \delta^{\ast} +1) + M_J \lambda \sqrt{s})\\
&= \mathcal{O}(\|  \D - \D^{\ast} \|_2^2 + \|  \D - \D^{\ast} \|_2 + \|  \D - \D^{\ast} \|_2 (\hat \delta^{\ast} + \hat \delta_J^{\ast}) + \hat \delta^{\ast} + \hat \delta_J^{\ast} + M_J \lambda \sqrt{s})\\
\end{aligned}
\end{equation}
Similarly,
 \begin{equation}
\begin{aligned}
\| \g_{\infty}^{\text{ae-ls}} - \g^{\ast}  \|_2 &\leq \mathcal{O}(\|  \D - \D^{\ast} \|_2^2 + \|  \D - \D^{\ast} \|_2 + \|  \D - \D^{\ast} \|_2 (\hat \delta^{\ast} + \hat \delta_J^{\ast}) + \hat \delta^{\ast} + \hat \delta_J^{\ast})
\end{aligned}
\end{equation}
%
\end{proof}
%
%
\begin{restatable}[Global bounds]{lemma}{qstartilde}\label{lemma:qstartilde}
From global gradient errors in \Cref{thm:globalgradient}, the following are satisfied
\begin{equation}\label{eq:qtilde}
\begin{aligned}
\| Q_t^{21}(\z^{\ast}) \|_2 &\leq (\nicefrac{L_{21}}{2}) \| \z_t - \z^{\ast} \|_2^2\\
\| Q_t^{\text{lasso-}11}(\z^{\ast}) \|_2 &\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + L_{1D} \| \D - \D^{\ast} \|_2 + \| \partial h(\z_t) \|_2\\
\| Q_t^{\text{ls-}11}(\z^{\ast}) \|_2 &\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 +  L_{1D} \| \D - \D^{\ast} \|_2\\
\| Q(\z^{\ast}, \J_t) \|_2 &\leq L_1 \| \J_t - \J^{\ast} \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2.
\end{aligned}
\end{equation}
\end{restatable}
%
\begin{proof}
For $Q_t^{21}(\z^{\ast})$, we achieve the quadratic bound using convexity of $\nabla_1 \Loss_{\x}(\z, \D)$ and its domain (\Cref{assum:domain}) and \Cref{lemma:lipdiffloss}. For $Q_t^{\text{lasso-}11}(\z^{\ast})$, we add and subtract $\nabla_1\Loss_{\x}(\z^{\ast}, \D)$, and use quadratic upper bound similar to \Cref{lemma:qlocal}. At line four, we use $\zero \in \nabla_1 \Loss_{\x}(\z^{\ast}, \D^{\ast})$ (\Cref{lemma:kktlasso}) and assume that $\z_t$ recovers the sign entries of $\z^{\ast}$ (see \Cref{thm:supprec} and \Cref{thm:supppres}).
%
\begin{equation}
\begin{aligned}
&\| Q_t^{\text{lasso-}11}(\z^{\ast}) \|_2 = \| \nabla_1 \Loss_{\x}(\z_t, \D) + \partial h(\z_t) - \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) (\z_t - \z^{\ast}) \|_2\\
&= \| \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_1\Loss_{\x}(\z^{\ast}, \D)  + \nabla_1\Loss_{\x}(\z^{\ast}, \D) + \partial h(\z_t) - \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) (\z_t - \z^{\ast}) \|_2\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + \| \partial h(\z_t) + \nabla_1\Loss_{\x}(\z^{\ast}, \D) \|_2\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + \| \partial h(\z_t) + \nabla_1\Loss_{\x}(\z^{\ast}, \D) - \nabla_1\Loss_{\x}(\z^{\ast}, \D^{\ast})\|_2\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + L_{1D} \| \D - \D^{\ast} \|_2 + \| \partial h(\z_t) \|_2.
\end{aligned}
\end{equation}
%
Similarly,
\begin{equation}
\begin{aligned}
\| Q_t^{\text{ls-}11}(\z^{\ast}) \|_2 &= \| \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) (\z_t - \z^{\ast}) \|_2\\
&= \| \nabla_1 \Loss_{\x}(\z_t, \D) - \nabla_1\Loss_{\x}(\z^{\ast}, \D)  + \nabla_1\Loss_{\x}(\z^{\ast}, \D) - \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) (\z_t - \z^{\ast}) \|_2\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + \| \nabla_1\Loss_{\x}(\z^{\ast}, \D) \|_2
\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + \| \nabla_1\Loss_{\x}(\z^{\ast}, \D) - \nabla_1\Loss_{\x}(\z^{\ast}, \D^{\ast})\|_2\\
&\leq (\nicefrac{L_{11}}{2}) \| \z_t - \z^{\ast} \|_2^2 + L_{1D} \| \D - \D^{\ast} \|_2.
\end{aligned}
\end{equation}
%
For $Q(\z^{\ast}, \J_t)$, from implicit function theorem, $Q(\z^{\ast}, \J^{\ast}) = 0$ for $\D$ evaluated at $\D^{\ast}$. Hence, we can use $\nabla_{21}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) = - \J^{\ast+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast})$ in the following
%
\begin{equation}
\begin{aligned}
\| Q(\z^{\ast}, \J_t) \|_2 &= \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) + \nabla_{21}^2\Loss_{\x}(\z^{\ast}, \D) - \nabla_{21}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) + \nabla_{21}^2\Loss_{\x}(\z^{\ast}, \D^{\ast})\|_2\\
&= \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) - \J^{\ast +} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) \|_2 + \| \nabla_{21}^2\Loss_{\x}(\z^{\ast}, \D) - \nabla_{21}^2\Loss_{\x}(\z^{\ast}, \D^{\ast})  \|_2\\
&= \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) - \J_t^{+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) \|_2\\
&+ \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) - \J^{\ast+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) \|_2 + L_{21D} \| \D - \D^{\ast}  \|_2\\
&= M_J L_{11D} \| \D - \D^{\ast}  \|_2 + \| \J_t^{+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) - \J^{\ast+} \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D^{\ast}) \|_2 + L_{21D} \| \D - \D^{\ast} \|_2\\
&\leq \| (\J_t^{+} - \J^{\ast+}) \nabla_{11}^2\Loss_{\x}(\z^{\ast}, \D) \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2\\
&\leq L_1 \| \J_t - \J^{\ast} \|_2 + M_J L_{11D} L_{21D} \| \D - \D^{\ast} \|_2.
\end{aligned}
\end{equation}
\end{proof}
%
\dictvariabledec*
%
\begin{proof}
In this proof, we study $g_{T,j}^{\text{dec}}$, and for ease of notation we drop the superscript.
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} = \E[\mathbf{1}_{\z_{T,(j)}\neq 0} \z_{T,(j)} (\D^{(l)} \z_{T} - \x)] =  \E[\mathbf{1}_{\z_{(j)}^{\ast}\neq 0} \z_{T,(j)} (\D^{(l)} \z_{T} - \x)] + \gamma
\end{aligned}
\end{equation}
where $\gamma =  \E[(\mathbf{1}_{\z_{T,(j)}\neq 0} - \mathbf{1}_{\z_{(j)}^{\ast}\neq 0}) \z_{T,(j)} (\D^{(l)} \z_{T} - \x)]$. We have the event $\mathbf{1}_{\z_{T,(j)}\neq 0} - \mathbf{1}_{\z_{(j)}^{\ast}\neq 0} = 0$ happening with probability of $1 - \epsilon^{(l)}_{\text{supp-pres}}$, and $\epsilon^{(l)}_{\text{supp-pres}}$ decreases with decrease in $\delta_l$. Hence, $\gamma$ gets smaller. We write
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} &= \E[\mathbf{1}_{\z_{(j)}^{\ast}\neq 0} \z_{T,(j)} (\D^{(l)} \z_{T} - \x)] + \gamma
\end{aligned}
\end{equation}
where $B_S^{(l)}$ is an diagonal matrix with $\beta_j^{(l)}$ for $j \in S$ as entries. For $j \notin S$, $\mathbf{1}_{\z_{(j)}^{\ast}\neq 0}=0$, which results in $ \g_{T, j}^{(l)}=0$. Hence, we only focus on $j \in S$ where $\mathbf{1}_{\z_{(j)}^{\ast}\neq 0}=1$. We condition on the support and re-write the gradient as
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} &= \E[\z_{T,(j)} (\D^{(l)} \z_{T} - \x)] + \gamma\\
    &= \E[ \E[ \z_{T,(j)} [\D_S^{(l)} (\eye - B_S^{(l)}) \z_{(S)}^{\ast} + \D_S^{(l)} \zeta_{T,S}^{(l)} - \D_S^{\ast} \z_{(S)}^{\ast}] \mid S] + \gamma\\
    &= \E[\D_S^{(l)} (\eye - B_S^{(l)}) \E [\z_{T,(j)}  \z_{(S)}^{\ast} \mid S]] - \E[\D_S^{\ast} E[\z_{T,(j)} \z_{(S)}^{\ast} \mid S]] + \E[\D_S^{(l)} \E[\z_{T,(j)} \zeta_{T,S}^{(l)} ] \mid S] + \gamma\\
    &= \E[\D_S^{(l)} (\eye - B_S^{(l)}) \E [( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)})  \z_{(S)}^{\ast} \mid S]] - \E[\D_S^{\ast} E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \z_{(S)}^{\ast} \mid S]]\\
    &+ \E[\D_S^{(l)} \E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \zeta_{T,S}^{(l)} ] \mid S] + \gamma\\
    &= \E[\D_j^{(l)} (1 - \beta_j^{(l)})^2]
    - \E[\D_j^{\ast} (1 - \beta_j^{(l)})]+ \gamma\\
    &+ \E[\D_S^{(l)} (\eye - B_S^{(l)}) E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] - \E[\D_S^{\ast} E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] + \E[\D_S^{(l)} \E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \zeta_{T,S}^{(l)} \mid S]]
\end{aligned}
\end{equation}
where in the last line, we use the fact that $\E[\z_{(j)}^{\ast} \mid j \in S] = 0$ and $\E[\z_{(S)}^{\ast} \z_{(S)}^{\ast \text{T}} \mid S] = \eye$. Computing the expectation, we get
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} &= p_j \D_j^{(l)} (1 - \beta_j^{(l)})^2
    - p_j \D_j^{\ast} (1 - \beta_j^{(l)})
    + U_{T, j}^{(l)} + \gamma = p_j (1 - \beta_j^{(l)}) \left((1 - \beta_j^{(l)}) \D_j^{(l)} 
    - \D_j^{\ast}\right) + U_{T, j}^{(l)} + \gamma
\end{aligned}
\end{equation}
where $U_{T, j}^{(l)} = \E[\D_S^{(l)} (\eye - B_S^{(l)}) E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] - \E[\D_S^{\ast} E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] + \E[\D_S^{(l)} \E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \zeta_{T,S}^{(l)} \mid S]]$. Given this gradient, we now find a bound on $U_{T, j}^{(l)}$.
\begin{equation}
\begin{aligned}
    U_{T, j}^{(l)} = \E[\D_S^{(l)} (\eye - B_S^{(l)}) E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] - \E[\D_S^{\ast} \E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] + \E[\D_S^{(l)} \E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \zeta_{T,S}^{(l)} \mid S]]
\end{aligned}
\end{equation}
First, we bound $\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]]$ as following.
\begin{equation}
\begin{aligned}
\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] &\leq \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \E[ 2\gamma_j^{(l)} \z_{(i)}^{\ast}\mid S] + \frac{\mu_l}{\sqrt{m}} \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \sum_{k \neq j} \E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \z_{(i)}^{\ast} \mid S ]\\
&+ \frac{\mu_l}{\sqrt{m}} \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \sum_{k \neq j} \E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \text{sign}(\z_{t, (i)}) \z_{(i)}^{\ast} \mid S ] + \tilde \kappa_{T,j}^{(l)}
\end{aligned}
\end{equation}
where $\tilde \kappa_{T,j}^{(l)} = \E[\z_{(i)}^{\ast} \kappa_{T,j}^{(l)}]$. Similar to $\kappa_{T,j}^{(l)}$, $\tilde \kappa_{T,j}^{(l)}$ decay very fast as $T$ increases. Hence, we bound the other terms. We have
\begin{equation}
\E[ \gamma_j^{(l)} \z_{(i)}^{\ast}\mid S] \begin{cases} 
\leq \delta_l &\text{if}\ j \neq i\\
= 0 &\text{if}\ j = i
\end{cases}
,
\end{equation}
\begin{equation}
\E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \z_{(i)}^{\ast} \mid S ]
\begin{cases} 
\leq E_{t-1,k} &\text{if}\ k = i\\
= 0 &\text{if}\ k \neq i
\end{cases}
,
\end{equation}
and
\begin{equation}
\E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \text{sign}(\z_{t, (k)}) \z_{(i)}^{\ast} \mid S ] \begin{cases} 
\leq E_{t-1,i} &\text{if}\ k = i\\
= 0 &\text{if}\ k \neq i
\end{cases}
\end{equation}
Hence,
\begin{equation}
\sum_{k\neq j} \E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \z_{(i)}^{\ast} \mid S ] \begin{cases} 
\leq E_{t-1,i} &\text{if}\ j \neq i\\
= 0 &\text{if}\ j = i
\end{cases}
\end{equation}
and
\begin{equation}
\sum_{k\neq j} \E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \text{sign}(\z_{t, (k)}) \z_{(i)}^{\ast} \mid S ] \leq \begin{cases} 
E_{t-1,i} &\text{if}\ j \neq i\\
0 &\text{if}\ j = i
\end{cases}
\end{equation}
Hence, for $j \neq i$, we can write
\begin{equation}
\begin{aligned}
\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] &\leq 2\delta_l + 2\frac{\mu_l}{\sqrt{m}} \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} E_{t-1,i} + \tilde \kappa_{T,j}^{(l)}
\end{aligned}
\end{equation}
where from \eqref{eq:Ealpha_2}, we have
\begin{equation}
\begin{aligned}
E_{t-1, i} (1 - \alpha)^{T-t} &\leq (1 + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \left( \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + 2 (s-1) \kappa_l (1 - \alpha)^{T-t} \right)
\end{aligned}
\end{equation}
Hence, given the sparsity level, the term below is bounded by $a_{\gamma}$ with probability of $1 - \epsilon_{\gamma}^{(l)}$.
\begin{equation}
\begin{aligned}
\sum_{t=1}^T E_{t-1, i} (1 - \alpha)^{T-t} &\leq \sum_{t=1}^T (1 + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \left( \sum_{t=1}^T \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + \sum_{t=1}^T 2 (s-1) \kappa_l (1 - \alpha)^{T-t} \right)\\
&\leq (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 a_{\gamma}^{(l)}) (1 + s \kappa_l) = \mathcal{O}(a_{\gamma}^{(l)})
\end{aligned}
\end{equation}
Finally, we get
\begin{equation}
\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] \leq
\begin{cases} 
2 \delta_l + \frac{\mu_l}{\sqrt{m}} \mathcal{O}(a_{\gamma}^{(l)}) + \tilde \kappa_{T,j}^{(l)} &\text{if}\ j \neq i\\
\tilde \kappa_{T,j}^{(l)} &\text{if}\ j = i
\end{cases}
\end{equation}
For appropriately large $T$, $\kappa_{T,j}^{(l)}$ can be make small; Hence, in this case, we get
\begin{equation}
\| U_{T,j}^{(l)} \|_2 \leq \mathcal{O}(\sqrt{p} p_{ij} \delta_l \| \D^{(l)}\|_2)
\end{equation}
%
Now, we can re-write the gradient as
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} &= p_j (1 - \beta_j^{(l)}) (\D_j^{(l)}
    - \D_j^{\ast}) + p_j (- \beta_j^{(l)} \D_j^{(l)} + \frac{1}{p_j} U_{T, j}^{(l)} + \frac{1}{p_j}\gamma) = \tau (\D_j^{(l)}
    - \D_j^{\ast}) + \theta
\end{aligned}
\end{equation}
where $\tau = p_j (1 - \beta_j^{(l)})$, and $\theta = p_j (- \beta_j^{(l)} \D_j^{(l)} + \frac{1}{p_j} U_{T, j}^{(l)} + \frac{1}{p_j}\gamma)$. We can bound the norm of $\theta$ as follows:
\begin{equation}
\begin{aligned}
  \| \theta \|_2 \leq p_j \beta_j^{(l)} \| \D_j^{(l)}\|_2  + \| U_{T, j}^{(l)} \|_2 + \gamma
\end{aligned}
\end{equation}
Given $\| \D_j^{(l)} \|_2 = 1$, and $\beta_j^{(l)} =  \langle \D_j^{\ast} - \D_j^{(l)}, \D_j^{\ast} \rangle = \frac{1}{2} \| \D_j^{(l)} - \D_j^{\ast} \|_2^2$, we modify the upper bound
\begin{equation}
  \| \theta \|_2 \leq \frac{1}{2} p_j \| \D_j^{(l)} - \D_j^{\ast} \|_2^2  + \mathcal{O}(\sqrt{p} p_{ij} \delta_l \| \D^{(l)}\|_2) + \gamma
\end{equation}
We assume a dictionary closeness during training, i.e., $\| \D^{(l)} - \D^{\ast} \|_2 \leq 2 \| \D^{\ast} \|_2$, which we prove in \Cref{lemma:maintaincloseness}. Given this closeness, we have
\begin{equation}
  \| \D^{(l)} \|_2 \leq \| \D^{(l)} - \D^{\ast} \|_2 + \| \D^{\ast} \|_2 = \mathcal{O}(\sqrt{\frac{p}{m}})
\end{equation}
Moreover, with $\gamma$ dropping with $\delta_l$, and for $s =\mathcal{O}(\sqrt{m})$, it is reduced to
\begin{equation}
  \| \theta \|_2 \leq p_j \| \D_j^{(l)} - \D_j^{\ast} \|_2
\end{equation}
We get
\begin{equation}
\begin{aligned}
  \| \g_{T, j}^{(l)} \|_2 &\leq p_j (1 - \beta_j^{(l)}) \| \D_j^{(l)}
    - \D_j^{\ast} \|_2 + p_j \| \D_j^{(l)}
    - \D_j^{\ast} \|_2\\
  \| \g_{T, j}^{(l)} \|_2^2 &\leq p_j^2 (2 - \beta_j^{(l)})^2 \| \D_j^{(l)}
    - \D_j^{\ast} \|_2^2
\end{aligned}
\end{equation}
Using this bound, we can find a lower bound on the correlation between the gradient direction and the desired direction as follows
\begin{equation}
\begin{aligned}
  \| \g_{T, j}^{(l)} \|_2^2 &= (p_j (1 - \beta_j^{(l)}))^2 \| \D_j^{(l)}
    - \D_j^{\ast} \|_2^2 + \|\theta\|_2^2 + 2 p_j (1 - \beta_j^{(l)}) \langle \theta, \D_j^{(l)}
    - \D_j^{\ast} \rangle\\
2 \langle \theta, \D_j^{(l)}
    - \D_j^{\ast} \rangle &= - p_j (1 - \beta_j^{(l)}) \| \D_j^{(l)} 
    - \D_j^{\ast} \|_2^2 + \frac{1}{p_j (1 - \beta_j^{(l)})} \| \g_{T, j}^{(l)} \|_2^2 - \frac{1}{p_j (1 - \beta_j^{(l)})} \|\theta\|_2^2\\
2 \langle \g_{T, j}^{(l)}, \D_j^{(l)}
    - \D_j^{\ast} \rangle &= p_j (1 - \beta_j^{(l)}) \| \D_j^{(l)} 
    - \D_j^{\ast} \|_2^2 + \frac{1}{p_j (1 - \beta_j^{(l)})} \| \g_{T, j}^{(l)} \|_2^2 - \frac{1}{p_j (1 - \beta_j^{(l)})} \|\theta\|_2^2\\
& \geq (p_j (1 - \beta_j^{(l)}) - p_j \frac{1}{1 - \beta_j^{(l)}}) \| \D_j^{(l)} 
    - \D_j^{\ast} \|_2^2 + \frac{1}{p_j (1 - \beta_j^{(l)})} \| \g_{T, j}^{(l)} \|_2^2
\end{aligned}
\end{equation}
Hence, using the descent property of Theorem 6 from~\citep{arora2015sparsecoding}, setting the learning rate to $\eta = \max_j \frac{1}{p_j (1 - \beta_j^{(l)})}$, and $\psi = \eta (p_j (1 - \beta_j^{(l)}) - p_j \frac{1}{1 - \beta_j^{(l)}}) \leq 1 - \frac{1}{(1 - \beta_j^{(l)})^2}$
\begin{equation}
    \| \D_j^{(l+1)} - \D_j^{\ast} \|_2^2 \leq (1 - \psi)   \| \D_j^{(l)} - \D_j^{\ast} \|_2^2 
\end{equation}
\end{proof}
%
\dictfixededec*
%
\begin{proof}
Following steps similar to \Cref{thm:dictvariabledec}, we write the gradient as
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} &= p_j \D_j^{(l)} (1 - \beta_j^{(l)})^2
    - p_j \D_j^{\ast} (1 - \beta_j^{(l)})
    + U_{T, j}^{(l)} + \gamma = p_j (1 - \beta_j^{(l)}) \left((1 - \beta_j^{(l)}) \D_j^{(l)} 
    - \D_j^{\ast}\right) + U_{T, j}^{(l)} + \gamma
\end{aligned}
\end{equation}
where $U_{T, j}^{(l)} = \E[\D_S^{(l)} (\eye - B_S^{(l)}) E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] - \E[\D_S^{\ast} E[\z_{(S)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] + \E[\D_S^{(l)} \E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \zeta_{T,S}^{(l)} \mid S]]$. Given this gradient, we now find a bound on $U_{T, j}^{(l)}$. First, we bound $\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]]$ as following.
\begin{equation}
\begin{aligned}
\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] &\leq \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \E[ \gamma_j^{(l)} \z_{(i)}^{\ast}\mid S] + \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \E[ \lambda^{\text{fixed}} \text{sign}(\z_{t-1, (j)}) \z_{(i)}^{\ast}\mid S]\\
&+ \frac{\mu_l}{\sqrt{m}} \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} \sum_{k \neq j} \E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \z_{(i)}^{\ast} \mid S ] + \tilde \kappa_{T,j}^{(l)}\\
\end{aligned}
\end{equation}
where we set all $\lambda_{t-1,j}^{(l)} = \lambda^{\text{fixed}}$ and $\tilde \kappa_{T,j}^{(l)} = \E[\z_{(i)}^{\ast} \kappa_{T,j}^{(l)}]$. Similar to $\kappa_{T,j}^{(l)}$, $\tilde \kappa_{T,j}^{(l)}$ decay very fast as $T$ increases. Hence, we bound the other terms. We have
\begin{equation}
\E[ \gamma_j^{(l)} \z_{(i)}^{\ast}\mid S] \begin{cases} 
\leq \delta_l &\text{if}\ j \neq i\\
= 0 &\text{if}\ j = i
\end{cases}
,
\end{equation}
\begin{equation}
\E[ \lambda^{\text{fixed}} \text{sign}(\z_{t-1, (j)}) \z_{(i)}^{\ast}\mid S] \begin{cases} 
= \lambda^{\text{fixed}} &\text{if}\ j = i\\
= 0 &\text{if}\ j \neq i
\end{cases}
,
\end{equation}
\begin{equation}
\E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \z_{(i)}^{\ast} \mid S ]
\begin{cases} 
\leq E_{t-1,k} &\text{if}\ k = i\\
= 0 &\text{if}\ k \neq i
\end{cases}
\end{equation}
Hence,
\begin{equation}
\sum_{k\neq j} \E[ E_{t-1,k} \text{sign}(\z_{(k)}^{\ast} - \z_{t-1,(k)}) \z_{(i)}^{\ast} \mid S ] \begin{cases} 
\leq E_{t-1,i} &\text{if}\ j \neq i\\
= 0 &\text{if}\ j = i
\end{cases}
\end{equation}
Hence, for $j \neq i$, we can write
\begin{equation}
\begin{aligned}
\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] &\leq \delta_l + \frac{\mu_l}{\sqrt{m}} \sum_{t=1}^T \alpha (1 - \alpha)^{T-t} (E_{0,i} + E_{t-1,i}) + \tilde \kappa_{T,j}^{(l)}
\end{aligned}
\end{equation}
We have
\begin{equation}
\begin{aligned}
E_{t-1, i} (1 - \alpha)^{T-t} &\leq (1 + 2(s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \left( \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + 2 (s-1) \kappa_l (1 - \alpha)^{T-t} \right)
\end{aligned}
\end{equation}
Hence, given the sparsity level, the term below is bounded by $a_{\gamma}$ with probability of $1 - \epsilon_{\gamma}^{(l)}$.
\begin{equation}
\begin{aligned}
\sum_{t=1}^T E_{t-1, i} (1 - \alpha)^{T-t} &\leq \sum_{t=1}^T (1 + 2 (s-1) \alpha \frac{\mu_l}{\sqrt{m}} (t-1)) E_{0, \text{max}} \delta_{\alpha, T-2}\\
&+ (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 \gamma_{\text{max}}^{(l)}) \left( \sum_{t=1}^T \sum_{k=1}^{t-1} \alpha (1 - \alpha)^{T-k-1} + \sum_{t=1}^T 2 (s-1) \kappa_l (1 - \alpha)^{T-t} \right)\\
&\leq (\beta_{\text{max}}^{(l)} | \z^{\ast}_{\text{max}} | + 2 a_{\gamma}^{(l)}) (1 + s \kappa_l) = \mathcal{O}(a_{\gamma}^{(l)})
\end{aligned}
\end{equation}
Finally, we get
\begin{equation}
\E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]] \leq
\begin{cases} 
\delta_l + \frac{\mu_l}{\sqrt{m}} \mathcal{O}(a_{\gamma}^{(l)}) + \tilde \kappa_{T,j}^{(l)} &\text{if}\ j \neq i\\
\lambda^{\text{fixed}} + \tilde \kappa_{T,j}^{(l)} &\text{if}\ j = i
\end{cases}
\end{equation}
For appropriately large $T$, $\tilde \kappa_{T,j}^{(l)}$ can be make small; Hence, in this case, we get
\begin{equation}
\| U_{T,j}^{(l)} \|_2 \leq \mathcal{O}(\sqrt{p} p_{ij} \delta_l \| \D^{(l)}\|_2 + p_j \lambda^{\text{fixed}})
\end{equation}
%
Now, we can re-write the gradient as
\begin{equation}
\begin{aligned}
    \g_{T, j}^{(l)} &= p_j (1 - \beta_j^{(l)}) (\D_j^{(l)}
    - \D_j^{\ast}) + p_j (- \beta_j^{(l)} \D_j^{(l)} + \frac{1}{p_j} U_{T, j}^{(l)} + \frac{1}{p_j}\gamma) = \tau (\D_j^{(l)}
    - \D_j^{\ast}) + \theta
\end{aligned}
\end{equation}
where $\tau = p_j (1 - \beta_j^{(l)})$, and $\theta = p_j (- \beta_j^{(l)} \D_j^{(l)} + \frac{1}{p_j} U_{T, j}^{(l)} + \frac{1}{p_j}\gamma)$. We can bound the norm of $\theta$ as follows:
\begin{equation}
\begin{aligned}
  \| \theta \|_2 \leq p_j \beta_j^{(l)} \| \D_j^{(l)}\|_2  + \| U_{T, j}^{(l)} \|_2 + \gamma
\end{aligned}
\end{equation}
Given $\| \D_j^{(l)} \|_2 = 1$, and $\beta_j^{(l)} =  \langle \D_j^{\ast} - \D_j^{(l)}, \D_j^{\ast} \rangle = \frac{1}{2} \| \D_j^{(l)} - \D_j^{\ast} \|_2^2$, we modify the upper bound
\begin{equation}
  \| \theta \|_2 \leq \frac{1}{2} p_j \| \D_j^{(l)} - \D_j^{\ast} \|_2^2  + \mathcal{O}(\sqrt{p} p_{ij} \delta_l \| \D^{(l)}\|_2 + p_j \lambda^{\text{fixed}}) + \gamma
\end{equation}
We assume a dictionary closeness during training, i.e., $\| \D^{(l)} - \D^{\ast} \|_2 \leq 2 \| \D^{\ast} \|_2$, which we prove in \Cref{lemma:maintaincloseness}. Given this closeness, we have
\begin{equation}
  \| \D^{(l)} \|_2 \leq \| \D^{(l)} - \D^{\ast} \|_2 + \| \D^{\ast} \|_2 = \mathcal{O}(\sqrt{\frac{p}{m}})
\end{equation}
Moreover, with $\gamma$ dropping with $\delta_l$, and for $s =\mathcal{O}(\sqrt{m})$, it is reduced to
\begin{equation}
  \| \theta \|_2 \leq p_j (\| \D_j^{(l)} - \D_j^{\ast} \|_2 + \lambda^{\text{fixed}})
\end{equation}
We get
\begin{equation}
\begin{aligned}
  \| \g_{T, j}^{(l)} \|_2 &\leq p_j (1 - \beta_j^{(l)}) \| \D_j^{(l)}
    - \D_j^{\ast} \|_2 + p_j (\| \D_j^{(l)}
    - \D_j^{\ast} \|_2 + \lambda^{\text{fixed}})\\
  \| \g_{T, j}^{(l)} \|_2^2 &\leq 2p_j^2 (2 - \beta_j^{(l)})^2 \| \D_j^{(l)}
    - \D_j^{\ast} \|_2^2 + 2p_j^2 \lambda^{\text{fixed}2}
\end{aligned}
\end{equation}
Using this bound, we can find a lower bound on the correlation between the gradient direction and the desired direction as follows
\begin{equation}
\begin{aligned}
  \| \g_{T, j}^{(l)} \|_2^2 &= (p_j (1 - \beta_j^{(l)}))^2 \| \D_j^{(l)}
    - \D_j^{\ast} \|_2^2 + \|\theta\|_2^2 + 2 p_j (1 - \beta_j^{(l)}) \langle \theta, \D_j^{(l)}
    - \D_j^{\ast} \rangle\\
2 \langle \theta, \D_j^{(l)}
    - \D_j^{\ast} \rangle &= - p_j (1 - \beta_j^{(l)}) \| \D_j^{(l)} 
    - \D_j^{\ast} \|_2^2 + \frac{1}{p_j (1 - \beta_j^{(l)})} \| \g_{T, j}^{(l)} \|_2^2 - \frac{1}{p_j (1 - \beta_j^{(l)})} \|\theta\|_2^2\\
2 \langle \g_{T, j}^{(l)}, \D_j^{(l)}
    - \D_j^{\ast} \rangle &= p_j (1 - \beta_j^{(l)}) \| \D_j^{(l)} 
    - \D_j^{\ast} \|_2^2 + \frac{1}{p_j (1 - \beta_j^{(l)})} \| \g_{T, j}^{(l)} \|_2^2 - \frac{1}{p_j (1 - \beta_j^{(l)})} \|\theta\|_2^2\\
& \geq (p_j (1 - \beta_j^{(l)}) - 2 p_j \frac{1}{1 - \beta_j^{(l)}}) \| \D_j^{(l)} 
    - \D_j^{\ast} \|_2^2 + \frac{1}{p_j (1 - \beta_j^{(l)})} \| \g_{T, j}^{(l)} \|_2^2 - \frac{2p_j}{(1 - \beta_j^{(l)})} \lambda^{\text{fixed}2}
\end{aligned}
\end{equation}
Hence, using the descent property of Theorem 6 from~\citep{arora2015sparsecoding}, setting the learning rate to $\eta = \max_j \frac{1}{p_j (1 - \beta_j^{(l)})}$, and $\psi = \eta (p_j (1 - \beta_j^{(l)}) - 2p_j \frac{1}{1 - \beta_j^{(l)}})$
\begin{equation}
    \| \D_j^{(l+1)} - \D_j^{\ast} \|_2^2 \leq (1 - \psi)   \| \D_j^{(l)} - \D_j^{\ast} \|_2^2 + \epsilon_{\lambda}
\end{equation}
where $\epsilon_{\lambda} \coloneqq \eta \frac{2p_j}{(1 - \beta_j^{(l)})} \lambda^{\text{fixed}2}$
\end{proof}
%
% 
\begin{restatable}[Dictionary maintains closeness]{lemma}{maintaincloseness}\label{lemma:maintaincloseness}
Suppose $\D^{(l)}$ has $(\delta_l, 2)$-closeness to $\D^{\ast}$ where $\delta_l = \mathcal{O}^{\ast}(1/\log{m})$, then with probability of $1 - \epsilon_{\text{supp-pres}}^{(l)}$, we have $\| \D^{(l+1)} - \D^{\ast} \|_2 \leq 2 \| \D^{\ast} \|_2$ when using $\g_{T}^{\text{dec}}$ and the network parameters set by \Cref{thm:fwdzerrorvariable}.
\end{restatable}
%
\begin{proof}
Given the dictionary update
\begin{equation}
    \D_j^{(l+1)} - \D_j^{\ast} =  \D_j^{(l)} - \D_j^{\ast} - \eta \g_{T,j}^{(l)} 
\end{equation}
Then, with probability at least $1 - \epsilon_{\text{supp-pres}}^{(l)}$, we have the gradient
\begin{equation}
    \g_{T, j}^{(l)} = p_j (1 - \beta_j^{(l)}) (\D_j^{(l)}
    - \D_j^{\ast}) + p_j (- \beta_j^{(l)} \D_j^{(l)} + \frac{1}{p_j} U_{T, j}^{(l)} + \frac{1}{p_j}\gamma)
\end{equation}
which we substitute in the dictionary update as below
\begin{equation}
    \D_j^{(l+1)} - \D_j^{\ast} =  (1 - \eta (p_j (1 - \beta_j^{(l)}))) (\D_j^{(l)} - \D_j^{\ast}) + \eta p_j \beta_j^{(l)} \D_j^{(l)} - \eta U_{T, j}^{(l)} - \eta \gamma)
\end{equation}
writing the update in matrix form
\begin{equation}
    \D^{(l+1)} - \D^{\ast} = (\D^{(l)} - \D^{\ast}) \text{diag}(1 - \eta (p_j (1 - \beta_j^{(l)}))) + \eta  \D^{(l)} \text{diag}(p_j \beta_j^{(l)}) - \eta \D^{(l)} F + \eta \D^{\ast} H - \eta \gamma)
\end{equation}
where $F_{(ij)} = p_{ij} \E[(1 - \beta_i^{(l)}) \z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S] + p_{ij} \E[( \z^{\ast}_{(j)} (1 - \beta_j^{(l)}) + \zeta_{T,j}^{(l)}) \zeta_{T,i}^{(l)} \mid S]$, and  $H_{(ij)} = p_{ij} \E[\z_{(i)}^{\ast} \zeta_{T,j}^{(l)} \mid S]$. Given, the bound $\| U_{T,j}^{(l)} \|_2 \leq \mathcal{O}(\sqrt{p} p_{ij} \delta_l \| \D^{(l)}\|_2)$ from before, we get $\| F \|_F \leq \mathcal{O}(p p_{ij} \delta_l)$ and $\| H \|_F \leq \mathcal{O}(p p_{ij} \delta_l)$. Hence,
\begin{equation}
\| \D^{(l)} F + \D^{\ast} H \|_2 \leq \|\D^{(l)} \|_2 \| F \|_F + \| \D^{\ast} \|_2 \| H \|_F = \mathcal{O}(p p_{ij} \delta_l \| \D^{\ast} \|_2) = \mathcal{O}(\frac{s^2}{p \log{m}}) \| \D^{\ast} \|_2
\end{equation}
Using the maintained closeness at update $l$, we bound the terms in the dictionary update one by one below
\begin{equation}
    \| (\D^{(l)} - \D^{\ast}) \text{diag}(1 - \eta (p_j (1 - \beta_j^{(l)}))) \| \leq (1 - \min_j \eta p_j (1 - \beta_j^{(l)})) \| \D^{(l)} - \D^{\ast} \|_2 \leq 2 (1 - \Omega(\eta s/p)) \| \D^{\ast} \|_2
\end{equation}
\begin{equation}
    \| \D^{(l)} \text{diag}(p_j \beta_j^{(l)}) \|_2 \leq \max_j p_j \frac{\delta_l^2}{2} \| \D^{(l)} - \D^{\ast} + \D^{\ast} \|_2 \leq o(s/p) \| \D^{\ast} \|_2
\end{equation}
Given the bounds above, the dictionary update can be bounded as following
\begin{equation}
  \| \D^{(l+1)} - \D^{\ast} \|_2 \leq 2 (1 - \Omega(\eta s/p)) \| \D^{\ast} \|_2 + o(\eta s/p) \| \D^{\ast} \|_2 + \mathcal{O}(\frac{\eta s^2}{p \log{m}}) \| \D^{\ast} \|_2 + \eta \gamma \leq 2 \| \D^{\ast} \|_2
\end{equation}
\end{proof}
%
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interpretability}
%
\interp*
%
\begin{proof}
For all stationary points, the objective gradient is $\zero$ with respect to the dictionary, i.e.,
\begin{equation}
\zero = (\Xx - \tilde \D \tilde \Z) \tilde \Z^{\text{T}} + \omega \tilde \D
\end{equation}
where $\tilde \D$ is the learned dictionary at convergence. Re-aranging the terms, we get
\begin{equation}
\tilde \D = \Xx \tilde \Z^{\text{T}} (\tilde \Z \tilde \Z^{\text{T}} + \omega \eye)^{-1}
\end{equation}
Using the relation $\A^{\text{T}} (\A \A^{\text{T}} + \omega \eye)^{-1} = (\A^{\text{T}} \A + \omega \eye)^{-1} \A^{\text{T}}$, we can re-write the solution as
\begin{equation}
\tilde \D = \Xx \G^{-1} \tilde \Z^{\text{T}}
\end{equation}
where we denote $\G \coloneqq (\tilde \Z^{\text{T}} \tilde \Z + \omega \eye)$.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix - future works and limitations}\label{lim}
%
\paragraph{Beyond dictionary learning} Our results are founded on three main properties: Lipschitz differentiability of the loss, proximal gradient descent, and strong convexity in finite-iteration. The findings can be applied to other min-min optimization problems, e.g., ridge regression and logistic regression, following such properties. For example, our analysis generalizes to the unrolled network in~\citep{tolooshams2020icml} for learning dictionaries using data from the natural exponential family. In this case, the least-squares loss is replaced with negative log-likelihood, and the dictionary models the data expectation.
%
\paragraph{Limitations} Finite-iteration support selection (\Cref{prop:supp})~\citep{hale2007fixed} and strong convexity may seem stringent going beyond dictionary learning. \citet{ablin2020super} discuss generalization of local gradient convergence by relaxing strong convexity to the $p$-\L ojasiewicz property~\citep{ablin2020super,attouch2009convergence}. We considered the noiseless setting and conjecture that the relative comparison of the gradients in the presence of noise still holds, where the upper bounds will involve an additional noise term. We focused on infinite sample convergence to highlight the relative differences between the gradients. We leave for future work the derivation of finite-sample bounds, a step similar to~\citep{chatterji2017alternating, arora2015sparsecoding}.
%

%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix - details of experiments}\label{exp_app}
%
PUDLE is developed using PyTorch~\citep{paszke2017automatic}. We used one GeForce GTX 1080 Ti GPU.
%
\subsection{Numerical experiments for theories}
%
%
%%%
\begin{wrapfigure}[12]{r}{0.33\textwidth}
    \vspace{-6mm}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/code_visualization_given_init_dict_estimate.pdf}
    \caption{Example of code estimates with the initialized dictionary.}
	\label{fig:code_init}
% 	\vspace{-4mm}
\end{wrapfigure}
%
\paragraph{Dataset} We generated $n\!=\!10{,}000$ samples following~\eqref{eq:gen}. We sampled $\D^{\ast}\!\in\!\R^{50 \times 100}$ from zero-mean Gaussian distribution, and normalized the columns. The codes are $5$-sparse with their support uniformly chosen at random and their amplitudes are sampled from $\text{Uniform}(1,2)$.
%
\paragraph{Training} We let $T=200$, $\lambda = 0.2$, and $\alpha = 0.2$. The dictionary is initialized to $\D = \D^{\ast} + \tau_B {\bm B}$ with ${\bm B} \sim \mathcal{N}(\zero, \frac{1}{m} \eye)$. For~\Cref{fig:forwardpass,fig:localgradient,fig:dstar}, we set $\tau_B \approx \nicefrac{0.55}{\log m}$. \Cref{fig:code_init} shows the sparse code estimates from one example given this initialized dictionary; this is to highlight that a) the initial dictionary is not very close to the ground-truth dictionary, and b) our algorithm is able to successfully perform dictionary learning and recover the support by the end of training in spite of a failed exact recovery of the support.

For~\Cref{fig:dl_25,fig:dl_100}, we chose much larger noise level, $\tau_B \approx \nicefrac{2.8}{\log m}$. The network is trained for $600$ epochs with full-batch gradient descent using Adam optimizer~\citep{kingma2014adam} with learning rate of $10^{-3}$ and $\epsilon = 10^{-8}$. The learned dictionary is evaluated based on the error $\nicefrac{\| \D - \D^{\ast} \|_2}{\| \D^{\ast} \|_2}$. The results and conclusion were consistent across various realizations of the dataset and across various optimizers. Hence, in the main paper, the figures visualize results of one realization.
%
\paragraph{Noisy measurements} We repeated the dictionary learning experiments shown in \Cref{fig:dl_25,fig:dl_100} where the measurements $\x$ are corrupted by zero-mean Gaussian noise such that the SNR is approximately $12$ dB. Accordingly, we set $\lambda = 0.3$. \Cref{fig:dl_noisy} shows the results for both noisy and noiseless scenarios.

%
%%%
\begin{figure}[h]
	\centering
	%%%%%
	\begin{subfigure}[t]{0.24\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T25_add_noise_to_data_p12_D_Dstar_err_noise_p12.pdf}
	  \caption{Noisy ($T=25$).}
  	\label{fig:dl_25_noisy}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.24\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T25_D_Dstar_err}
	  \caption{Noiseless ($T=25$).}
  	\label{fig:dl_25_a}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.24\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_add_noise_to_data_p12_D_Dstar_err_noise_p12.pdf}
	  \caption{Noisy ($T=100$).}
  	\label{fig:dl_100_noisy}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.24\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_D_Dstar_err}
	  \caption{Noiseless ($T=100$).}
  	\label{fig:dl_100_a}
	\end{subfigure}
	%%%%%
    \caption{Dictionary learning in noisy and noiseless scenarios.}
	\label{fig:dl_noisy}
	\vspace{-4mm}
\end{figure}
%
\paragraph{Stochastic Dictionary Learning} In addition to the full-batch gradient descent results in the main paper, we repeated the experiments in \Cref{fig:dl_100} using batch size of $4, 16$ and $64$. We observed (\Cref{fig:dl_batch}) that in all scenarios PUDLE is able to learn a good estimate of the ground-truth dictionary, and $\g_t^{\text{ae-ls}}$ is superior to the other two. We note that for lower batch-size, there will be more gradient updates in one epoch, hence, the algorithm converges in lower number of epochs.
%%%
\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_batchsize4_D_Dstar_err.pdf}
	  \caption{Batch size $4$ ($T=25$).}
  	\label{fig:dl_100_batch4}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_batchsize16_D_Dstar_err.pdf}
	  \caption{Batch size $16$ ($T=100$).}
  	\label{fig:dl_100_batch16}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_batchsize64_D_Dstar_err}
	  \caption{Batch size $64$ ($T=100$).}
  	\label{fig:dl_100_batch64}
	\end{subfigure}
	%%%%%
    \caption{Dictionary learning using various batch sizes.}
	\label{fig:dl_batch}
	\vspace{-4mm}
\end{figure}
%
\paragraph{Effect of learning rate} We performed the experiments in \Cref{fig:dl_100} for various learning rate of $10^{-4}, 10^{-3}$, and $10^{-2}$. This shows the robustness of the gradient-based dictionary learning against learning rate. \Cref{fig:dl_lr} demonstrates such results where PUDLE successfully converges to the neighbourhood of the ground-truth dictionary; Regardless of the learning rate, $\g_t^{\text{ae-ls}}$ converges to a closer neighbourhood than the other two gradients. Overall, smaller the learning rate, more epochs is needed to reach convergence.
%%%
\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_lr0p01_D_Dstar_err.pdf}
	  \caption{Learning rate $10^{-2}$.}
  	\label{fig:dl_100_lr2}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_D_Dstar_err.pdf}
	  \caption{Learning rate $10^{-3}$.}
  	\label{fig:dl_100_lr3}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.28\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/T100_lr0p0001_D_Dstar_err.pdf}
	  \caption{Learning rate $10^{-4}$.}
  	\label{fig:dl_100_lr4}
	\end{subfigure}
	%%%%%
    \caption{Dictionary learning for various learning rates when $T=100$.}
	\label{fig:dl_lr}
	\vspace{-7mm}
\end{figure}
%
\paragraph{Dictionary initialization} We conducted similar experiments to \Cref{fig:dl_25,fig:dl_100}. We let $n=10{,}000$, $m=100$, and $p=100$. We generated an orthogonal $\D^{\ast}$. The sparse codes $\z^{\ast}$ are $5$-sparse and their amplitudes are drawn from sub-Gaussian $\mathcal{N}(0, 1)$. We set $\lambda=0.05$, and $\alpha = 0.2$. We used the pairwise method proposed by~\citet{arora2015sparsecoding} to initialize the dictionary. This close initialization resulted in a dictionary that provides support recovery prior training. \Cref{fig:dl_initarora} shows successful dictionary learning where $\g_t^{\text{ae-ls}}$ converges to a closer neighbourhood of $\D^{\ast}$ than the other two gradients. We used linear sum assignment optimization (i.e., \texttt{scipy.optimize.linear\_sum\_assignment}) to find the correct column permutations before computing the dictionary distance error.
%
%%%
\begin{figure}[h]
	\centering
	\begin{subfigure}[t]{0.42\linewidth}
	\centering
	\includegraphics[width=0.99\linewidth]{figures/T25_subgaussian_tmlr_init_from_arora_D_Dstar_err.pdf}
	  \caption{$T=25$.}
  	\label{fig:dl_25_initarora}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.42\linewidth}
	\centering
	\includegraphics[width=0.99\linewidth]{figures/T100_subgaussian_tmlr_init_from_arora_D_Dstar_err.pdf}
	  \caption{$T=100$.}
  	\label{fig:dl_100_initarora}
	\end{subfigure}
	%%%%%
    \caption{Dictionary learning when $\D$ is initialized using the pairwise method~\citet{arora2015sparsecoding}.}
	\label{fig:dl_initarora}
% 	\vspace{-4mm}
\end{figure}
%
\subsection{Dictionary learning}
%
\paragraph{Dataset} We generated $n\!=\!50{,}000$ samples following~\eqref{eq:gen}. We let $m\!=\!1000$ and $p\!=\!1500$, and sample $\D^{\ast}$ from zero-mean Gaussian distribution, and then normalized the columns. The sparse codes $\z^i$ are $10$, $20$, $40$-sparse, where their supports are chosen uniformly at random and amplitudes are sampled from $\text{Uniform}(1,2)$.
%
\paragraph{Training} The dictionary is initialized to $\D = \D^{\ast} + \tau_B {\bm B}$ with ${\bm B} \sim \mathcal{N}(\zero, \frac{1}{m} \eye)$ where $\tau_B \approx \nicefrac{1}{\log m}$. We let $\lambda = 0.2$, and $\alpha = 0.2$, and $T=100$. The network is trained for $1{,}000$ iterative updates with batch-size of $50$ using Adam~\citep{kingma2014adam} with learning rate of $10^{-3}$ and $\epsilon = 10^{-3}$.  For decay method, $\nu$ is decreased in value by $0.005$ every $100$ update iterations. Each filter is normalized after every update. The learned dictionary is evaluated based on the relative error $\nicefrac{\| \D - \D^{\ast} \|_2}{\| \D^{\ast} \|_2}$.
%
\begin{figure}[t]
	\centering
	%%%%%
	\begin{subfigure}[t]{0.325\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/s10_D_Dstar_err}
	  \caption{$10$-sparse code.}
  	\label{fig:s10}
	\end{subfigure}
	%
	\begin{subfigure}[t]{0.325\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/s20_D_Dstar_err}
 	 \caption{$20$-sparse code.}
  	\label{fig:s20}
	\end{subfigure}
	%%%%%
	\begin{subfigure}[t]{0.325\linewidth}
	\centering
	\includegraphics[width=0.999\linewidth]{figures/s40_D_Dstar_err}
	  \caption{$40$-sparse code.}
  	\label{fig:s40}
	\end{subfigure}
	\caption{Dictionary learning convergence using $\g_t^{\text{ae-ls}}$ compared to NOODL and SPORCO.}
	\label{fig:baslines_full}
	\vspace{-4mm}
\end{figure}
%
\subsection{Image denoising}
%
\paragraph{Training} We trained PUDLE where the dictionary is convolutional with $64$ filters of size $9 \times 9$ and strides of $4$. The encoder unrolls for $T=15$, and the step size is set to $\alpha = 0.1$. Unlike the theoretical analysis where full-batch gradient descent is studied, the network is trained stochastically with Adam optimizer~\citep{kingma2014adam} with a learning rate of $10^{-4}$ and $\epsilon = 10^{-3}$ for $250$ epochs. At every training iteration, a random $129 \times 129$ patch is cropped and a zero-mean Gaussian noise with a standard deviation of $25$ is added. We utilize random horizontal and vertical flip for augmentation. We report results in terms of the peak signal-to-noise ratio (PSNR). The standard deviation of the test PSNR across multiple noise realizations was lower than $0.02$ dB for all the methods. Hence, we only reported the mean PSNR of the test set.
%
\subsection{Interpretable sparse coding and dictionary learning}
%
We focused on digits of $\{0,1,2,3,4\}$ of MNIST. We set $T=15$, $\lambda=0.7$, and $\alpha = 1$. The dictionary dimensions are $m = 784$ and $p = 500$. We trained the network for $200$ epochs using Adam optimizer with a learning rate of $10^{-4}$ and batch size of $32$. For construction of $\G$, $\omega$ is set to $0.001$. For \Cref{fig:mnist_01234_learn_image_cont_to_dict}, we computed the image contributions using $6{,}000$ randomly chosen training images. The Gram matrix used in \Cref{fig:mnist_01234_learn_interpolate_gz}, is constructed by $6{,}000$ training examples, and the reconstruction is from the $200$ most contributed training images.

\end{document}
