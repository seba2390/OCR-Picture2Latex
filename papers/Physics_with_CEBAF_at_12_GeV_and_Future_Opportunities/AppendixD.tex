\section{Computation for NP} 
\label{sec:appendixd}
Similar to other scientific disciplines, Nuclear Physics is on the threshold of a new paradigm of discovery with the establishment of new experimental facilities, advances in theory, and expanded access to computational resources, and application of data science techniques.  

In this appendix, we cover the importance of data curation for analysis, particularly for AI/ML techniques, Streaming Readout and Data Science.  Improvements needed in computing infrastructure and services are covered, as we complete the transition of the JLab Scientific Computing Model to seamlessly utilize distributed computing including HPC centers and a possible national data infrastructure.  This will be accomplished by the use of common and community tools for scaling and interoperability. We also note the necessity of developing workflows to enable theory-experiment integration to support some of the scientific activities outlined in this document.  


%Technical advances:\\ \\
\subsection{Data Curation}
The JLab experimental and operational data sets are invaluable and must be curated to enable new research in data science techniques. To produce robust and reproducible data analytics for production systems (detector, accelerator, etc.), curated data set and a model repository is a requirement.  Preparing a collection of benchmark data sets with detailed Datasheets would allow JLab researchers to study new techniques to improve scientific discovery. Similarly, developing a model repository with detailed provenance would allow us to ensure reproducibility, models reuse, and define ``golden model''.  All production or published research must be captured in a online repository to track changes in the source code during software development on all essential efforts. The repositories should have unit tests and continuous integration with the ability to easily rollback to a previous build. In addition, the computing infrastructure should be linked to automate the builds of new containers

%%FAIR data??

\subsection{Streaming Readout}

Within the experimental computing program, recent efforts have been focused on the need to adapt the computing model to better accommodate to changes in data acquisition (DAQ)/readout schemes and advanced analytics techniques. A streaming architecture will simplify the readout schemes from the standard “event building” as well as provide more flexibility both to accommodate detector systems that have large disparities in readout times, and to accommodate more complicated geometries for detectors. Streaming readout architectures are in advanced prototypes for 12 GeV experiments and are integral to the design of SOLID. The streaming architecture for DAQ will have implications for offline processing with the possibility to move some types of processing closer to the detectors by running, for example, AI/ML algorithms in FPGAs.


\subsection{Infrastructure}
The full scientific process is a meta-workflow that connects individual data processing workflows from theory and experiment.
In order to support the growing computing needs for the JLab science mission, infrastructures that support integrated workflows and data production will need to be developed to augment or replace existing functionality to provide maximal flexibility and scalability. In order to process data for the different Halls in a timely manner, ideally, the data would be automatically transferred to onsite resources, HTC/HPC cluster(s) and/or to remote resources for processing and analysis. The computing systems must be designed to handle data volumes at scale and the ability to dynamically integrate a variety of geographically distributed computing resources. This infrastructure based on common tools has been developed by the high energy physics community and is in use for multiple experiments.
The infrastructure could be design to seamlessly integrate with EIC workflows and future workflows on High Performance Data Facility (HPDF).

Virtualization can address the different software and OS requirements from the experimental Hall.  For example, Docker containers can be used to build new instances of the development and production software and environment stack. The production containers would then be converted to Singularity images and available to be used across all computing platforms (laptops, HPC  and HTC cluster, GPU farms, etc.). 



\subsection{Computing and services}
The heart of this distributed computing model will be the virtualized service cluster which provides all the central services for local and geographically distributed computing infrastructure. This computing infrastructure would be developed using containerized components and orchestration. We propose using an open-source container orchestration system, such as  Kubernetes, to automate application deployment, scaling, and management. %We choose to back our cluster with the containerd container runtime, which forms the core runtime engine of the popular Docker tool. Some of the key motivations for using containers and container orchestration was the ability to quickly reinstall machines and workloads. 
Additionally, it enables high availability by easily and dynamically moving containers off problematic nodes to healthy ones. A tool like Kubernetes allows for compute resource management by dynamically adding and/or removing services as required by the current computing load. The use of containers allows to decouple the build, test, and deploy workflows enabling more reliable deployment. They also provide a clean solution for conflicting dependencies when running numerous services on the same node. 
%Kubernetes enables a light weight layer to maintain multiple nodes and many containers.
In additional to traditional computing workflows, the data science effort would also benefit from a virtual/modular workflow that can expand as the compute requirements grows. As such, the data science effort can also leverage the aforementioned tools and expand by using Kubeflow.
%\subsubsection{Workflows and automation}
%\subsubsection{Reproducibility and re-usability}
%\subsubsection{Hardware}
%CPU, GPU, TPU, FPGAs, and Neuromorphic Computing

\subsection{Data Science - Methods, Algorithms, and Applications}
%New advancements in machine learning and artificial intelligence could provide a better accelerator and detector operational efficient. The use of machine learning techniques to identify anomalies in an accelerator and detector systems has show promising results [expand and provide references]. Augmenting these techniques with uncertainty quantification would provide a robust and actionable techniques with a quantifiable level of confidence.      
%The use of Bayesian optimization, evolutionary algorithms, and reinforcement learning has shown promising results for design and control optimization problem [expand and \cite{}].
%Physics informed machine learning can provide enhance techniques to leverage the power of deep learning while still incorporating know physics equations [expand and \cite{}]. 
%Online ML model on FPGA can be use to improve data efficiency and targeted "trigger" [expand and \cite{}].\\ \\
The use of machine learning (ML) and artificial intelligence (AI) has significantly grown in the past 5 years and new techniques are continuously being develop to address scientific challenges. In order to provide solutions for the NP community, JLab will work on addressing the basic research needs defined in the DOE workshops and town-halls. 
Specifically, JLab will focus on developing and integrating new capabilities in ML-based uncertainty quantification techniques. The inclusion of UQ into the models is critical for study in anomaly detection, particle identification, etc. Augmenting these studies with UQ would provide robust and actionable techniques with a quantifiable level of confidence. In addition, an area of growth and potential large operational efficiency improvements lie in advancements in safe and robust optimization research for design and control. The use of Bayesian optimization, evolutionary algorithms, and reinforcement learning has shown promising results for the optimization problem. Another unique area of research that JLab can lead is in the use of domain aware ML for NP. Developing a multi-disciplinary team of experimentalists, theorists, and data scientists can yield new solutions in which the solution would leverage known physics equations and fold them into the ML model to provide a reliable solution requiring fewer data points.

The recent work in the experimental Halls, specifically Halls B and D, have shown promising results in areas such as anomaly detection and reconstruction. 
Hall B has made several advancements in AI/ML to improve the performance of the tracking system and triggering systems. 
Additionally, there are some ML efforts with strong overlap between the experimentalist and the theorists. 
The majority of these machine learning tools are well motivated and developed using manual and sequential techniques, however, are limited to a select few Halls. 
In order to improve on these results, the use of leading edge techniques in ML should be applied along with the inclusion of uncertainty quantification and physics informed models (when applicable). 
Unfortunately, these offline analyses are typically slow and include a lot of manual intervention.
In fact, the majority of DOE efforts in AI/ML are restricted to one-directional manual data flow from the physical object to an offline analysis, as shown in Figure \ref{DigitalTwin}.
\begin{figure}[tbh]
  \centering
  \includegraphics[width=1.0\textwidth]{Figures/DigitalTwin.png}
  \caption{Schematic diagram of digital twin~\cite{DT9103025}.}
  \label{DigitalTwin}
\end{figure}
Although these studies can yield great results, they lack, in their implementation, the ability to perform these tasks continuously in near real-time.
Continuous evaluation of the analytical results and model predictions is required in order to determine if the model is still valid as the system evolves.
In order to elevate these efforts, a common infrastructure is needed that should include at its core a fully realized digital twin model. 
A digital twin is a bi-directional data exchange between a physical object and its digital representation, commonly known as a digital object. 
The physical and digital objects share a common data repository, ensuring that the data used by either is identical. With a fully realized digital twin, in-situ analysis can be conducted using the digital object without impacting the physical object, for example:
\begin{enumerate}
    \item Statistical and causal analysis
    \item Anomaly detection and classification
    \item Forecasting component fatigue and failures
    \item Continuous calibration and optimization
\end{enumerate}
%Figure \ref{DigitalTwinAndAna}, illustrate a high level workflow that include the ingest of new data from the physical object to the digital object. Various %traditional and machine learning based analytic are them performed to provide continuous information.
%\begin{figure}[tbh]
%  \centering
%  \includegraphics[width=1.0\textwidth]{Figures/DTandAna.png}
%  \caption{Schematic diagram of digital twin with various analytical tools.}
%  \label{DigitalTwinAndAna}
%\end{figure}
Several of the recently funded Scientific User Facility projects are developing analytical tools used to monitor, diagnose, and optimize these facilities.
However, they are currently not addressing the issue regarding long term usability of these tools.
Jefferson Lab is in a good position to incorporate these techniques into the digital twin framework, given the upcoming experiments and computing facilities.
Needless to say, the digital twin framework can be extended to include other key systems within the complex (accelerator, facilities, etc.)
As such, the digital twin framework could eventually accommodate all critical systems that require real time monitoring and analysis/optimization.
The framework would need to be modular in order to easily include new analytical tools as new techniques are developed.
In order to support a digital twin framework, a computing infrastructure will be required to efficiently move the data from the experimental Halls to the digital twin framework for near real-time analytic. 
In fact, this framework could provide an ideal use case for the anticipated High Performance Data Facility hosted at Jefferson lab.

%\subsection{Theory}
%For Nuclear Physics in our energy regime, the underlying theory, Quantum Chromodynamic is not analytically solvable and must be approached computationally.  Through a dedicated program of work spanning 50 years, significant progress has been made in solving QCD on the lattice through advances in the theoretical formulation, algorithmic development and computational hardware, in architecture and scale.  The Exascale Computing Project completes in 2024 (reference), delivering two open science Exaflop computers, enabling more realistic simulations of physical process. As part of the ECP applications area and on-going investment in the SciDAC, Lattice codes are well positioned to use the ECP machines for gauge generation, and to use local resources for analysis in support of the 12 GeV science program in structure and spectroscopy.  However, the scale and precision of the upcoming calculations opens up a data management challenge for the Lattice Community as the projected data sizes and need to move data from the Leadership Class computers to local resources is comparable to experimental data management requirements.  In addition, LQCD results are serving as a data source in global fits.  This also emphasizes the need for data management frameworks to enable the combination of results from different sources in a user friendly environment that delivers verifiable and reproducible results.         

%Quantum Computing:



% Place to capture/include some material on computation requirements
% Number of proposal related to Global fitting and ML to extract the structure information
% Text to pull together all we AI/ML activities ... talk with the Hall leaders and Jianwei
% Marco from Hall B (Marco)