\section{Implementation}
%--- Flow chart for basic call flow

%--- Algorithm to prioritize app updates and update process

% The algorithm to prioritize application's model updates would consider
% 1) System Specific parameters: CPU utilization, tput, storage and bandwidth
% 2) Application Specific parameters: Accuracy, Latency, execution time, progress towards end goal, Number of currently processing jobs
\begin{figure}
    \centering
    \includegraphics[width=8cm]{figures/algo1.png}
    \caption{Greedy Algorithm}
    \label{fig:greedy}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{figures/algo2.png}
    \caption{DSOC Algorithm}
    \label{fig:dsoc}
\end{figure}



% We will use the below type of prioritization...
% Overall 0>prior_val>0.3 --- Green
% 0.31 - 0.6 --- Yellow
% 0.61 - 0.85 ---- Blue
% >0.85 --- Red
% Idea --- 1. Use System parameters and 2. Application paramaters
% * Set priority for System parameter and application parameter sum(alphai) =1

% Individual weights for System specific parameters
% * CPu utilization, tput, memory, storage a1,a2,a3 etc
% System performance update() left to individual System


% Individual weights for the application specific parameters
% * Accuracy, individual progress (expected vs current progress) at time-i, latency, 
% Application performance meterics update() left to the individual application

%***** AM I NOT thinking about the whole system performance?????
% ***** Make a claim that If individual apps add up their performance accordingly by making model updates considering system specific parameters and application specific parameters, we should be able to improve the overall system performance.


%%%% Important noteeee ----- Think about simulation, instead of actual deployment on the field....
%*** These new things deployed on field would lead to devastation if things go wrong. Simulation would be the best to check. How do we guarantee the performance we get out of simulation on field? *****
We can leverage the existing Docker swarm functionality for our implementation. Swarm managers control all the nodes and they can use several strategies to run containers efficiently. It can be 1. "emptiest node" technique - which fills the least utilized machine with containers 2. "global"- which ensures each machines gets exactly one instance of the specified machines. These strategies help in load balancing, scaling and fault management~\cite{swarm_docker}.
There are two methods to implement coordination among groups of applications working towards a specific goal. Greedy approach is one method where every application is eager to increase it's accuracy and performance. Whenever there's a newer model or updated code available which improves accuracy and performance, the application tries to perform an update. Figure~\ref{fig:greedy} explains how a greedy approach for patching works. We maintain an update Queue which stores all the model and code updates of applications. M$_{ij}$ is a code/model update for application-j running on node-i. Calculate 'K' updates which can be performed such that overall system performance doesn't degrade. Till the update Queue is non-empty, choose an update M$_{ij}$ and check if the node-i is unconstrained. If it's unconstrained, assign a worker to update the application-j on node-i with M${ij}$. If the node-i was constrained, delay the M$_{ij}$ update and proceed by choosing the next model in update Queue.

The second approach is the DSOC approach (Figure~\ref{fig:dsoc}) where the "Coalescer" handles all the updates, evaluating the priority of update requests. The system specific parameters like throughput, memory, cpu utilization, bandwidth and application specific parameters like accuracy improvement, execution time, latency are carefully considered before updating an existing model/code fragment.

Refer to Figure~\ref{fig:assprior} to understand how priority is assigned to model/code update. The updates are prioritized after considering all system specific and application specific parameters. c1, c2 are the parameters used to indicate the weight to be given to SP(system specific parameters) and AP(Application specific parameters) such that $0\le{c1} \le {c2} \le {1}$ and c1 + c2 = 1. 
System specific weights for CPU utilization, memory, storage and throughput are stored in sWeight. Application specific weights for accuracy, progress, latency and execution time are stored in aWeight. Using these, the system performance and application performance of running application\-j on node\-i would be calculated. Using these metrics, we would be able to calculate pVal which combines both application and system performance into single metric. The applications which need their updates immediately would be classified as green(priority one), next prioritized updates would be yellow (priority two), updates with least priority would be blue (priority three) and classifiers which need not be updated are red. Green, yellow, blue and red are the coloring scheme maintained by coalescer in order to assign priority to an application's model update (refer to Figure~\ref{fig:assprior}).
In the DSOC approach, if individual applications are consistent and make efficient progress towards end goal by carefully considering model updates, we can state that the overall system performance and progress would be prolific.

\begin{figure}
    \centering
    \includegraphics[width=8.5cm]{figures/assig1.png}
    \caption{assign\_priority calculation}
    \label{fig:assprior}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{figures/implement1.pdf}
    \caption{Accuracy improvement against task completion}
    \label{fig:implement2pred}
\end{figure}

In Figure~\ref{fig:implement2pred}, we try to carefully predict the trade-off between accuracy improvement and closeness towards end-goal. Closeness towards end-goal is percentage of task completed like 20\%, 40\%, 60\% and so on. In Greedy approach, the accuracy constantly increases and we reach faster towards the ends goal. We reach the end goal with slightly better accuracy using greedy approach using a lot of resources and performing many updates. On the other hand if we choose DSOC approach, there would be slight improvement in overall accuracy as we progress towards end-goal and there would be slower progress towards the end goal, but it uses less resources and performs fewer updates. In DSOC approach, we reach the end goal with lesser updates and slightly lesser accuracy compared to greedy model.  
%Planning to remove this graph... SO commented..
%\begin{figure}
%    \centering
%    \includegraphics[width=8cm]{figures/implement_1_pred.pdf}
%    \caption{Throughput versus \# of applications}
%    \label{fig:implement1pred}
%\end{figure}


%We need to understand how throughput varies as we scale the number of applications used.  From Figure~\ref{fig:implement1pred}, for greedy approach as we scale the number of applications the throughput increases. As more and more applications are added, many models need to be updated for accuracy improvement. Whenever there's a newer model which gives slight improvement in accuracy, Coalescer updates the application's model. This causes a sharp elevation in throughput. In our proposed xxx approach, not all the models are updated. The coalescer judiciously considers performance metrics and updates only few models with highest priority. When newer models are available which improves an application's accuracy, Coalescer considers numerous factors and prioritizes models to be  updated. Then Coalescer selects a few highest priority models which need to be immediately updated. There would be little increase in throughput as compared to the greedy approach.  




