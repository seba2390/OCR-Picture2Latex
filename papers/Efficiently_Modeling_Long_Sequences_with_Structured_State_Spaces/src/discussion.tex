
\section{Discussion}
\label{sec:discussion}


\paragraph{Related Work.}
Our work is most closely related to a line of work originally motivated by a particular biologically-inspired SSM, which led to mathematical models for addressing LRDs. %
\citet{voelker2019dynamical,voelker2019legendre} derived a non-trainable SSM motivated from approximating a neuromorphic spiking model, and \citet{chilkuri2021parallelizing} showed that it could be sped up at train time with a convolutional view.
\citet{gu2020hippo} extended this special case to a general continuous-time function approximation framework with several more special cases of \( \bm{A} \) matrices designed for long-range dependencies.
However, instead of using a true SSM, all of these works fixed a choice of \( \bm{A} \) and built RNNs around it.
Most recently, \citet{gu2021lssl} used the full \eqref{eq:1} explicitly as a deep SSM model, exploring new conceptual views of SSMs, as well as allowing \( \bm{A} \)  to be trained.
As mentioned in \cref{sec:intro}, their method used a naive instantiation of SSMs that suffered from an additional factor of \( N \) in memory and \( N^2 \) in computation.

Beyond this work, our technical contributions (\cref{sec:s4}) on the \methodabbrv{} parameterization and algorithms are applicable to a broader family of SSMs including these investigated in prior works,
and our techniques for working with these models may be of independent interest.


\paragraph{Implementation.}

The computational core of \methodabbrv's training algorithm is the Cauchy kernel discussed in \cref{sec:s4-overview,sec:s4-efficiency,sec:s4-convolution-proof}.
As described in \cref{sec:s4-convolution-proof} \cref{prop:cauchy},
there are many algorithms for it with differing computational complexities and sophistication.
Our current implementation of \methodabbrv{} actually uses the naive \( O(NL) \) algorithm
which is easily parallelized on GPUs and has more easily accessible libraries allowing it to be implemented;
we leverage the \texttt{pykeops} library for memory-efficient kernel operations.
However, this library is a much more general library that may not be optimized for the Cauchy kernels used here,
and we believe that a dedicated CUDA implementation can be more efficient.
Additionally, as discussed in this work, there are asymptotically faster and numerically stable algorithms for the Cauchy kernel (\cref{prop:cauchy}).
However, these algorithms are currently not implemented for GPUs due to a lack of previous applications that require them.
We believe that more efficient implementations of these self-contained computational kernels are possible,
and that \methodabbrv{} (and SSMs at large) may have significant room for further improvements in efficiency.


\paragraph{Limitations and Future Directions.}
In this work, we show that \methodabbrv{} can address a wide variety of data effectively.
However, it may not necessarily be the most suitable model for all types of data.
For example, \cref{tab:wt103} still found a gap compared to Transformers for language modeling.
An interesting future direction is exploring combinations of S4 with other sequence models to complement their strengths.
We are excited about other directions, including continuing to explore the benefits of S4 on audio data (e.g. pre-training or generation settings),
and generalizing HiPPO and S4 to higher-dimensional data for image and video applications.
