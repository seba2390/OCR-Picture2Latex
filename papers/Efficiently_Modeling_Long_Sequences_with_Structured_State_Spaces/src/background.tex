
\section{Background: State Spaces}
\label{sec:background}

\cref{sec:ss-continuous,sec:ss-memory,sec:ss-convolution,sec:ss-recurrent} describe the four properties of SSMs in \cref{fig:properties}:
the classic continuous-time representation, addressing LRDs with the HiPPO framework, the discrete-time recurrent representation, and the parallelizable convolution representation.
In particular, \cref{sec:ss-convolution} introduces the SSM convolution kernel $\bm{\overline{K}}$,
which is
the focus of our theoretical contributions in \cref{sec:s4}.

\subsection{State Space Models: A Continuous-time Latent State Model}
\label{sec:ss-continuous}

The state space model is defined by the simple equation \eqref{eq:1}.
It maps a 1-D input signal $u(t)$ %
to an \( N \)-D latent state \( x(t) \)
before projecting to a 1-D output signal \( y(t) \).
\begin{equation}
  \label{eq:1}
  \begin{aligned}
    x'(t) &= \bm{A}x(t) + \bm{B}u(t) \\
    y(t) &= \bm{C}x(t) + \bm{D}u(t)
  \end{aligned}
\end{equation}
SSMs are broadly used in many scientific disciplines
and related to latent state models such as Hidden Markov Models (HMM).
Our goal is to simply use the SSM as a black-box representation in a deep sequence model,
where $\bm{A}, \bm{B}, \bm{C}, \bm{D}$ are parameters learned by gradient descent.
For the remainder of this paper, we will omit the parameter \( \bm{D} \) for exposition (or equivalently, assume \( \bm{D} = 0 \)) because the term \( \bm{D}u \) can be viewed as a skip connection and is easy to compute.

\subsection{Addressing Long-Range Dependencies with HiPPO}
\label{sec:ss-memory}



Prior work found that the basic SSM \eqref{eq:1} actually performs very poorly in practice.
Intuitively, one explanation is that linear first-order ODEs solve to an exponential function,
and thus may suffer from gradients scaling exponentially in the sequence length (i.e., the vanishing/exploding gradients problem~\citep{pascanu2013difficulty}).
To address this problem, the LSSL leveraged the HiPPO theory of continuous-time memorization \citep{gu2020hippo}.
HiPPO specifies a class of certain matrices \( \bm{A} \in \mathbbm{R}^{N \times N} \) that when incorporated into \eqref{eq:1}, allows the state \( x(t) \) to memorize the history of the input \( u(t) \).
The most important matrix in this class is defined by equation \eqref{eq:hippo},
which we will call the HiPPO matrix. %
For example, the LSSL found that simply modifying an SSM from a random matrix \( \bm{A} \) to equation \eqref{eq:hippo} improved its performance on the sequential MNIST benchmark from \( 60\% \) to \( 98\% \).
\begin{equation}
  \label{eq:hippo}
  (\text{\textbf{HiPPO Matrix}})
  \qquad
  \bm{A}_{nk}
  =
  -
  \begin{cases}
    (2n+1)^{1/2}(2k+1)^{1/2} & \mbox{if } n > k \\
    n+1 & \mbox{if } n = k \\
    0 & \mbox{if } n < k
  \end{cases}
  .
\end{equation}

\subsection{Discrete-time SSM: The Recurrent Representation}
\label{sec:ss-recurrent}

To be applied on a discrete input sequence \( (u_0, u_1, \dots) \) instead of continuous function \( u(t) \),
\eqref{eq:1} must be discretized by a \textbf{step size} \( \dt \) that represents the resolution of the input.
Conceptually, the inputs \( u_k \) can be viewed as sampling an implicit underlying continuous signal \( u(t) \), where \( u_k = u(k \dt) \).




To discretize the continuous-time SSM, we follow prior work in using the bilinear method~\citep{tustin1947method}, which converts the state matrix \( \bm{A} \) into an approximation \( \bm{\overline{A}} \) .
The discrete SSM is
\begin{equation}
  \label{eq:2}
\begin{aligned}
  x_{k} &= \bm{\overline{A}} x_{k-1} + \bm{\overline{B}} u_k &
  \bm{\overline{A}} &= (\bm{I} - \dt/2 \cdot \bm{A})^{-1}(\bm{I} + \dt/2 \cdot \bm{A}) &
  \\
  y_k &= \bm{\overline{C}} x_k &
  \bm{\overline{B}} &= (\bm{I} - \dt/2 \cdot \bm{A})^{-1} \dt \bm{B} &
  \bm{\overline{C}} &= \bm{C}
  .
\end{aligned}
\end{equation}
Equation \eqref{eq:2} is now a \emph{sequence-to-sequence} map \( u_k \mapsto y_k \) instead of function-to-function.
Moreover the state equation is now a recurrence in \( x_k \),
allowing the discrete SSM to be computed like an RNN.
Concretely, \( x_k \in \mathbbm{R}^N \) can be viewed as a \emph{hidden state} with transition matrix \( \bm{\overline{A}} \).

Notationally, throughout this paper we use \( \bm{\overline{A}}, \bm{\overline{B}}, \dots \) to denote discretized SSM matrices defined by \eqref{eq:2}. %
Note that these matrices are a function of both \( \bm{A} \) as well as a step size \( \dt \); we suppress this dependence for notational convenience when it is clear.


\subsection{Training SSMs: The Convolutional Representation}
\label{sec:ss-convolution}

The recurrent SSM \eqref{eq:2} is not practical for training on modern hardware due to its sequentiality.
Instead,
there is a well-known connection between linear time-invariant (LTI) SSMs such as \eqref{eq:1} and continuous convolutions.
Correspondingly, \eqref{eq:2} can actually be written as a discrete convolution.

For simplicity let the initial state be \( x_{-1} = 0 \).
Then unrolling \eqref{eq:2} explicitly yields %
\begin{align*}
  x_0 &= \bm{\overline{B}} u_0 &
  x_1 &= \bm{\overline{A}} \bm{\overline{B}} u_0 + \bm{\overline{B}} u_1 &
  x_2 &= \bm{\overline{A}}^2 \bm{\overline{B}} u_0 + \bm{\overline{A}} \bm{\overline{B}} u_1 + \bm{\overline{B}} u_2 & \dots
  \\
  y_0 &= \bm{\overline{C}} \bm{\overline{B}} u_0 &
  y_1 &= \bm{\overline{C}} \bm{\overline{A}} \bm{\overline{B}} u_0 + \bm{\overline{C}} \bm{\overline{B}} u_1 &
  y_2 &= \bm{\overline{C}} \bm{\overline{A}}^2 \bm{\overline{B}} u_0 + \bm{\overline{C}} \bm{\overline{A}} \bm{\overline{B}} u_1 + \bm{\overline{C}} \bm{\overline{B}} u_2
  & \dots
\end{align*}
This can be vectorized into a convolution \eqref{eq:convolution} with an explicit formula for the convolution kernel \eqref{eq:krylov}.
\begin{equation}
  \label{eq:convolution}
  \begin{split}
    y_k &= \bm{\overline{C}} \bm{\overline{A}}^k \bm{\overline{B}} u_0 + \bm{\overline{C}} \bm{\overline{A}}^{k-1} \bm{\overline{B}} u_1 + \dots + \bm{\overline{C}} \bm{\overline{A}} \bm{\overline{B}} u_{k-1} + \bm{\overline{C}}\bm{\overline{B}} u_k
    \\
    y &= \bm{\overline{K}} \ast u %
    .
  \end{split}
\end{equation}
\begin{equation}%
  \label{eq:krylov}
  \bm{\overline{K}} \in \mathbb{R}^L :=
  \mathcal{K}_L(\bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) := \left(\bm{\overline{C}} \bm{\overline{A}}^i \bm{\overline{B}}\right)_{i \in [L]} = (\bm{\overline{C}}\bm{\overline{B}}, \bm{\overline{C}}\bm{\overline{A}}\bm{\overline{B}}, \dots, \bm{\overline{C}}\bm{\overline{A}}^{L-1}\bm{\overline{B}})
  .
\end{equation}
\normalsize
In other words, equation \eqref{eq:convolution} is a single (non-circular) convolution and can be computed very efficiently with FFTs, \emph{provided} that \( \bm{\overline{K}} \) is known. %
However, computing \( \bm{\overline{K}} \) in \eqref{eq:krylov} is non-trivial and is the focus of our technical contributions in \cref{sec:s4}.
We call \( \bm{\overline{K}} \) the \textbf{SSM convolution kernel} or filter.
