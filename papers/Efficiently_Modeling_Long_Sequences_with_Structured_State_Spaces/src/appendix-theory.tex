\section{Numerical Instability of LSSL}
\label{sec:lssl-instability}


This section proves the claims made in \cref{sec:s4-motivation} about prior work.
We first derive the explicit diagonalization of the HiPPO matrix, confirming its instability because of exponentially large entries.
We then discuss the proposed theoretically fast algorithm from \citep{gu2021lssl} (Theorem 2) and show that it also involves exponentially large terms and thus cannot be implemented.

\subsection{HiPPO Diagonalization}

\begin{proof}[Proof of \cref{lmm:hippo-diagonalization}]%
  The HiPPO matrix \eqref{eq:hippo} is equal, up to sign and conjugation by a diagonal matrix, to
  \begin{align*}
    \bm{A} &=
    \begin{bmatrix}
      1 \\
      -1 & 2 \\
      1 & -3 & 3 \\
      -1 & 3 & -5 & 4 \\
      1 & -3 & 5 & -7 & 5 \\
      -1 & 3 & -5 & 7 & -9 & 6 \\
      1 & -3 & 5 & -7 & 9 & -11 & 7 \\
      -1 & 3 & -5 & 7 & -9 & 11 & -13 & 8 \\
      \vdots & & & & & & & & \ddots \\
    \end{bmatrix}
    \\
    \bm{A}_{nk} &=
    \begin{cases}%
      (-1)^{n-k} (2k+1) & n > k \\
      k+1 & n=k \\
      0 & n<k
    \end{cases}
    .
  \end{align*}
  Our goal is to show that this \( \bm{A} \) is diagonalized by the matrix
  \begin{align*}
    \bm{V} = \binom{i+j}{i-j}_{ij} =
    \begin{bmatrix}
      1 \\
      1 & 1 \\
      1 & 3 & 1 \\
      1 & 6 & 5 & 1 \\
      1 & 10 & 15 & 7 & 1 \\
      1 & 15 & 35 & 28 & 9 & 1 \\
      \vdots & & & & & & \ddots \\
    \end{bmatrix}
    ,
  \end{align*}
  or in other words that columns of this matrix are eigenvectors of \( \bm{A} \).

  Concretely, we will show that the \( j \)-th column of this matrix \( \bm{v}^{(j)} \) with elements
  \begin{align*}
    \bm{v}^{(j)}_i =
    \begin{cases}%
      0 & i < j \\
      \binom{i+j}{i-j} = \binom{i+j}{2j} & i \ge j
    \end{cases}
  \end{align*}
  is an eigenvector with eigenvalue \( j+1 \).
  In other words we must show that for all indices \( k \in [N] \),
  \begin{equation}
    \label{eq:diagonalization-proof}
    (\bm{A}\bm{v}^{(j)})_k = \sum_i \bm{A}_{ki} \bm{v}^{(j)}_{i} = (j+1) \bm{v}^{(j)}_k.
  \end{equation}

  If \( k < j \), then for all \( i \) inside the sum, either \( k < i \) or \( i < j \).
  In the first case \( \bm{A}_{ki} = 0 \) and in the second case \( \bm{v}^{(j)}_i = 0 \),
  so both sides of equation \eqref{eq:diagonalization-proof} are equal to \( 0 \).

  It remains to show the case \( k \ge j \), which proceeds by induction on \( k \).
  Expanding equation \eqref{eq:diagonalization-proof} using the formula for \( \bm{A} \) yields
  \begin{align*}
    (\bm{A}\bm{v})^{(j)}_k = \sum_i \bm{A}_{ki} \bm{v}^{(j)}_{i}
    = \sum_{i=j}^{k-1} (-1)^{k-i} (2i+1) \binom{i+j}{2j} + (k+1) \binom{k+j}{2j}.
  \end{align*}

  In the base case \( k = j \), the sum disappears and we are left with \( (\bm{A}\bm{v}^{(j)})_j = (j+1) \binom{2j}{2j} = (j+1) \bm{v}^{(j)}_j \), as desired.

  Otherwise, the sum for \( (\bm{A}\bm{v})^{(j)}_{k} \) is the same as the sum for \( (\bm{A}\bm{v})^{(j)}_{k-1} \) but with sign reversed and a few edge terms.
  The result follows from applying the inductive hypothesis and algebraic simplification:
  \begin{align*}
    (\bm{A}\bm{v})^{(j)}_{k}
    &= -(\bm{A}\bm{v})^{(j)}_{k-1} - (2k-1) \binom{k-1+j}{2j} + k\binom{k-1+j}{2j} + (k+1)\binom{k+j}{2j}
    \\&= -(j+1)\binom{k-1+j}{2j} - (k-1)\binom{k-1+j}{2j} + (k+1)\binom{k+j}{2j}
    \\&= -(j+k)\binom{k-1+j}{2j} + (k+1)\binom{k+j}{2j}
    \\&= -(j+k)\frac{(k-1+j)!}{(k-1-j)!(2j)!} + (k+1)\binom{k+j}{2j}
    \\&= -\frac{(k+j)!}{(k-1-j)!(2j)!} + (k+1)\binom{k+j}{2j}
    \\&= -(k-j)\frac{(k+j)!}{(k-j)!(2j)!} + (k+1)\binom{k+j}{2j}
    \\&= (j-k)(k+1)\binom{k+j}{2j} + (k+1)\binom{k+j}{2j}
    \\&= (j+1)\bm{v}^{(j)}_k
    .
  \end{align*}

\end{proof}

\subsection{Fast but Unstable LSSL Algorithm}

Instead of diagonalization,
\citet[Theorem 2]{gu2021lssl} proposed a sophisticated fast algorithm to compute
\begin{align*}
  K_L(\bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) = (\bm{\overline{C}}\bm{\overline{B}}, \bm{\overline{C}}\bm{\overline{A}}\bm{\overline{B}}, \dots, \bm{\overline{C}}\bm{\overline{A}}^{L-1}\bm{\overline{B}}).
\end{align*}
This algorithm runs in \( O(N\log^2 N + L\log L) \) operations and \( O(N+L) \) space.
However, we now show that this algorithm is also numerically unstable.

There are several reasons for the instability of this algorithm, but most directly we can pinpoint a particular intermediate quantity that they use.
\begin{definition}%
  The fast LSSL algorithm computes coefficients of \( p(x) \), the characteristic polynomial of \( A \), as an intermediate computation.
  Additionally, it computes the coefficients of its inverse, \( p(x)^{-1} \pmod{x^L} \).
\end{definition}

We now claim that this quantity is numerically unfeasible.
We narrow down to the case when \( \bm{\overline{A}} = \bm{I} \) is the identity matrix.
Note that this case is actually in some sense the most typical case:
when discretizing the continuous-time SSM to discrete-time by a step-size \( \dt \),
the discretized transition matrix \( \bm{\overline{A}} \) is brought closer to the identity.
For example, with the Euler discretization \( \bm{\overline{A}} = \bm{I} + \dt \bm{A} \),
we have \( \bm{\overline{A}} \to \bm{I} \) as the step size \( \dt \to 0 \).

\begin{lemma}%
  When \( \bm{\overline{A}} = \bm{I} \), the fast LSSL algorithm requires computing terms exponentially large in \( N \).
\end{lemma}
\begin{proof}%
  The characteristic polynomial of \( \bm{I} \) is
  \begin{align*}
    p(x) =
    \mathsf{det}\left| \bm{I} - x\bm{I} \right|
    = (1-x)^N.
  \end{align*}
  These coefficients have size up to \( \binom{N}{\frac{N}{2}} \approx \frac{2^N}{\sqrt{\pi N/2}} \).

  The inverse of \( p(x) \) has even larger coefficients.
  It can be calculated in closed form by the generalized binomial formula:
  \begin{align*}
    (1-x)^{-N} = \sum_{k=0}^{\infty} \binom{N+k-1}{k}x^k.
  \end{align*}
  Taking this \( \pmod{x^L} \), the largest coefficient is
  \begin{align*}
    \binom{N+L-2}{L-1} = \binom{N+L-2}{N-1} = \frac{(L-1)(L-2)\dots(L-N+1)}{(N-1)!}.
  \end{align*}
  When \( L=N-1 \) this is
  \begin{align*}
    \binom{2(N-1)}{N-1} \approx \frac{2^{2N}}{\sqrt{\pi N}}
  \end{align*}
  already larger than the coefficients of \( (1-x)^N \), and only increases as \( L \) grows.
\end{proof}

\section{\methodabbrv{} Algorithm Details}
\label{sec:s4-details}

This section proves the results of \cref{sec:s4-efficiency}, providing complete details of our efficient algorithms for \methodabbrv{}.

\cref{sec:s4-nplr-proof,sec:s4-recurrence-proof,sec:s4-convolution-proof}
prove \cref{thm:hippo-nplr,thm:s4-recurrence,thm:s4-convolution}
respectively.

\subsection{NPLR Representations of HiPPO Matrices}
\label{sec:s4-nplr-proof}

We first prove \cref{thm:hippo-nplr},
showing that all HiPPO matrices for continuous-time memory fall under the \methodabbrv{} normal plus low-rank (NPLR) representation.

\begin{proof}[Proof of \cref{thm:hippo-nplr}]%
  We consider each of the three cases HiPPO-LagT, HiPPO-LegT, and HiPPO-LegS separately.
  Note that the primary HiPPO matrix defined in this work (equation \eqref{eq:hippo}) is the HiPPO-LegT matrix.

  \textbf{HiPPO-LagT.}
  The HiPPO-LagT matrix is simply
  \begin{align*}
    \bm{A}_{nk} &=
    \begin{cases}%
      0            & n < k \\
      -\frac{1}{2} & n=k   \\
      -1           & n > k \\
    \end{cases}
    \\
    \bm{A} &=
    -
    \begin{bmatrix}
      \frac{1}{2} &             &             &            & \dots \\
      1           & \frac{1}{2} &             &             \\
      1           & 1           & \frac{1}{2} &             \\
      1           & 1           & 1           & \frac{1}{2} \\
      \vdots      &             &             &              & \ddots \\
    \end{bmatrix}
    .
  \end{align*}
  Adding the matrix of all \( \frac{1}{2} \), which is rank 1, yields
  \begin{align*}
    -
    \begin{bmatrix}
      & -\frac{1}{2} & -\frac{1}{2} & -\frac{1}{2} \\
      \frac{1}{2} &              & -\frac{1}{2} & -\frac{1}{2} \\
      \frac{1}{2} & \frac{1}{2}  &              & -\frac{1}{2} \\
      \frac{1}{2} & \frac{1}{2}  & \frac{1}{2}  &              \\
    \end{bmatrix}
    .
  \end{align*}
  This matrix is now skew-symmetric.
  Skew-symmetric matrices are a particular case of normal matrices
  with pure-imaginary eigenvalues.

  \citet{gu2020hippo} also consider a case of HiPPO corresponding to the generalized Laguerre polynomials that generalizes
  the above HiPPO-LagT case.
  In this case, the matrix \( \bm{A} \) (up to conjugation by a diagonal matrix) ends up being close to the above matrix,
  but with a different element on the diagonal.
  After adding the rank-1 correction, it becomes the above skew-symmetric matrix plus a multiple of the identity.
  Thus after diagonalization by the same matrix as in the LagT case, it is still reduced to diagonal plus low-rank (DPLR) form,
  where the diagonal is now pure imaginary plus a real constant.

  \textbf{HiPPO-LegS.}
  We restate the formula from equation \eqref{eq:hippo} for convenience.
  \begin{align*}
    \bm{A}_{nk}
    =
    -
    \begin{cases}
      (2n+1)^{1/2}(2k+1)^{1/2} & \mbox{if } n > k \\
      n+1                      & \mbox{if } n = k \\
      0                        & \mbox{if } n < k
    \end{cases}
    .
  \end{align*}
  Adding \( \frac{1}{2}(2n+1)^{1/2}(2k+1)^{1/2} \) to the whole matrix gives
  \begin{align*}
    -
    \begin{cases}
      \frac{1}{2} (2n+1)^{1/2}(2k+1)^{1/2}  & \mbox{if } n > k \\
      \frac{1}{2}                           & \mbox{if } n = k \\
      -\frac{1}{2} (2n+1)^{1/2}(2k+1)^{1/2} & \mbox{if } n < k \\
    \end{cases}
  \end{align*}

  Note that this matrix is not skew-symmetric,
  but is \( \frac{1}{2}\bm{I} + \bm{S} \) where \( \bm{S} \) is a skew-symmetric matrix.
  This is diagonalizable by the same unitary matrix that diagonalizes \( \bm{S} \).

  \textbf{HiPPO-LegT.}

  Up to the diagonal scaling,
  the LegT matrix is
  \begin{align*}
    \bm{A} =
    -
    \begin{bmatrix}
      1      & -1 & 1  & -1  & \dots \\
      1      & 1  & -1 & 1  \\
      1      & 1  & 1  & -1 \\
      1      & 1  & 1  & 1  \\
      \vdots &    &    &     & \ddots
    \end{bmatrix}
    .
  \end{align*}
  By adding \( -1 \) to this matrix and then the matrix
  \begin{align*}
    \begin{bmatrix}
      &  &   &  &  \\
      2 &  & 2 &  &  \\
      &  &   &  &  \\
      2 &  & 2 &  &  \\
    \end{bmatrix}
  \end{align*}
  the matrix becomes
  \begin{align*}
    \begin{bmatrix}
      & -2 &   & -2 &  \\
      2 &    &   &    &  \\
      &    &   & -2 &  \\
      2 &    & 2 &    &  \\
    \end{bmatrix}
  \end{align*}
  which is skew-symmetric.
  In fact, this matrix is the inverse of the Chebyshev Jacobi.

  An alternative way to see this is as follows.
  The LegT matrix is the inverse of the matrix
  \begin{align*}
    \begin{bmatrix}
      -1 & 1  &    & 0  \\
      -1 &    & 1  &    \\
      & -1 &    & 1  \\
      &    & -1 & -1 \\
    \end{bmatrix}
  \end{align*}
  This can obviously be converted to a skew-symmetric matrix by adding a rank 2 term.
  The inverses of these matrices are also rank-2 differences from each other by the Woodbury identity.

  A final form is
  \begin{align*}
    \begin{bmatrix}
      -1 & 1 & -1 & 1 \\
      -1 & -1 & 1 & -1 \\
      -1 & -1 & -1 & 1 \\
      -1 & -1 & -1 & -1 \\
    \end{bmatrix}
    +
    \begin{bmatrix}
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
      1 & 0 & 1 & 0 \\
      0 & 1 & 0 & 1 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 1 & 0 & 1 \\
      -1 & 0 & 1 & 0 \\
      0 & -1 & 0 & 1 \\
      -1 & 0 & -1 & 0 \\
    \end{bmatrix}
  \end{align*}
  This has the advantage that the rank-2 correction is symmetric (like the others),
  but the normal skew-symmetric matrix is now \( 2 \)-quasiseparable instead of \( 1 \)-quasiseparable.

\end{proof}

\subsection{Computing the \methodabbrv{} Recurrent View}
\label{sec:s4-recurrence-proof}

We prove \cref{thm:s4-recurrence} showing the efficiency of the \methodabbrv{} parameterization for computing one step of the recurrent representation (\cref{sec:ss-recurrent}).

Recall that without loss of generality, we can assume that the state matrix \( \bm{A} = \bm{\Lambda} - \bm{P}\bm{Q}^* \) is diagonal plus low-rank (DPLR), potentially over \( \mathbbm{C} \).
Our goal in this section is to explicitly write out a closed form for the discretized matrix \( \bm{\overline{A}} \).

Recall from equation \eqref{eq:2} that
\begin{align*}
  \bm{\overline{A}} &= (\bm{I} - \dt/2 \cdot \bm{A})^{-1}(\bm{I} + \dt/2 \cdot \bm{A}) \\
  \bm{\overline{B}} &= (\bm{I} - \dt/2 \cdot \bm{A})^{-1} \dt \bm{B}
  .
\end{align*}



We first simplify both terms in the definition of \( \bm{\overline{A}} \) independently.

\textbf{Forward discretization.}
The first term is essentially the Euler discretization motivated in \cref{sec:ss-recurrent}.
\begin{align*}
  \bm{I} + \frac{\dt}{2} \bm{A}
  &= \bm{I} + \frac{\dt}{2} (\bm{\Lambda} - \bm{P} \bm{Q}^*)
  \\&= \frac{\dt}{2} \left[ \frac{2}{\dt}\bm{I} + (\bm{\Lambda} - \bm{P} \bm{Q}^*) \right]
  \\&= \frac{\dt}{2} \bm{A_0}
\end{align*}
where \( \bm{A_0} \) is defined as the term in the final brackets.

\textbf{Backward discretization.}
The second term is known as the Backward Euler's method.
Although this inverse term is normally difficult to deal with,
in the DPLR case we can simplify it using Woodbury's Identity (\cref{prop:woodbury}).
\begin{align*}
  \left( \bm{I} - \frac{\dt}{2} \bm{A} \right)^{-1}
  &=
  \left( \bm{I} - \frac{\dt}{2} (\bm{\Lambda} - \bm{P} \bm{Q}^*) \right)^{-1}
  \\&=
  \frac{2}{\dt} \left[ \frac{2}{\dt} - \bm{\Lambda} + \bm{P} \bm{Q}^* \right]^{-1}
  \\&=
  \frac{2}{\dt} \left[ \bm{D} - \bm{D} \bm{P} \left( \bm{I} + \bm{Q}^* \bm{D} \bm{P} \right)^{-1} \bm{Q}^* \bm{D} \right]
  \\&= \frac{2}{\dt} \bm{A_1}
\end{align*}
where \( \bm{D} = \left( \frac{2}{\dt}-\bm{\Lambda} \right)^{-1} \)
and \( \bm{A_1} \) is defined as the term in the final brackets.
Note that
\( \left( 1 + \bm{Q}^* \bm{D} \bm{P} \right) \)
is actually a scalar in the case when the low-rank term has rank \( 1 \).


\textbf{\methodabbrv{} Recurrence.}
Finally, the full bilinear discretization can be rewritten in terms of these matrices as
\begin{align*}
  \bm{\overline{A}} &= \bm{A_1} \bm{A_0} \\
  \bm{\overline{B}} &= \frac{2}{\dt} \bm{A_1} \dt \bm{B} = 2 \bm{A_1} \bm{B}
  .
\end{align*}
The discrete-time SSM \eqref{eq:2} becomes
\begin{align*}
  x_{k} &= \bm{\overline{A}} x_{k-1} + \bm{\overline{B}} u_k \\
  &= \bm{A_1} \bm{A_0} x_{k-1} + 2 \bm{A_1} \bm{B} u_k \\
  y_k &= \bm{C} x_k
  .
\end{align*}
Note that \( \bm{A_0}, \bm{A_1} \) are accessed only through matrix-vector multiplications.
Since they are both DPLR, they have \( O(N) \) matrix-vector multiplication,
showing \cref{thm:s4-recurrence}.


\subsection{Computing the Convolutional View}
\label{sec:s4-convolution-proof}

The most involved part of using SSMs efficiently is computing \( \bm{\overline{K}} \).
This algorithm was sketched in \cref{sec:s4-overview} and is the main motivation for the \methodabbrv{} parameterization.
In this section, we define the necessary intermediate quantities and prove the main technical result. %


The algorithm for \cref{thm:s4-convolution} falls in roughly three stages, leading to \cref{alg:s4-convolution}.
Assuming \( \bm{A} \) has been conjugated into diagonal plus low-rank form, we successively simplify the problem of computing \( \bm{\overline{K}} \)
by applying the techniques outlined in \cref{sec:s4-overview}.

\begin{remark}
  \textbf{We note that for the remainder of this section}, we transpose \( \bm{C} \) to be a column vector of shape \( \mathbbm{C}^{N} \) or \( \mathbbm{C}^{N \times 1} \) instead of matrix or row vector \( \mathbbm{C}^{1 \times N} \) as in \eqref{eq:1}.
  In other words the SSM is
  \begin{equation}
    \begin{aligned}
      x'(t) &= \bm{A}x(t) + \bm{B}u(t) \\
      y(t) &= \bm{C}^* x(t) + \bm{D}u(t)
      .
    \end{aligned}
  \end{equation}
  This convention is made so that \( \bm{C} \) has the same shape as \( \bm{B}, \bm{P}, \bm{Q} \) and simplifies the implementation of S4.
\end{remark}

\paragraph{Reduction 0: Diagonalization}
By \cref{lmm:conjugation}, we can switch the representation by conjugating with any unitary matrix.
For the remainder of this section, we can assume that \( \bm{A} \) is (complex) diagonal plus low-rank (DPLR).

Note that unlike diagonal matrices, a DPLR matrix does not lend itself to efficient computation of \( \bm{\overline{K}} \).
The reason is that \( \bm{\overline{K}} \) computes terms \( \bm{\overline{C}}^* \bm{\overline{A}}^i \bm{\overline{B}} \) which involve powers of the matrix \( \bm{\overline{A}} \).
These are trivially computable when \( \bm{\overline{A}} \) is diagonal, but is no longer possible for even simple modifications to diagonal matrices such as DPLR.

\paragraph{Reduction 1: SSM Generating Function}

To address the problem of computing powers of \( \bm{\overline{A}} \), we introduce another technique.
Instead of computing the SSM convolution filter \( \bm{\overline{K}} \) directly,
we introduce a generating function on its coefficients and compute evaluations of it.

\begin{definition}[SSM Generating Function]%
  \label{def:generating-function}
  We define the following quantities:
  \begin{itemize}%
    \item The \emph{SSM convolution function} is \( \mathcal{K}(\bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) = (\bm{\overline{C}}^*\bm{\overline{B}}, \bm{\overline{C}}^*\bm{\overline{A}}\bm{\overline{B}}, \dots) \)
      and the (truncated) SSM filter of length \( L \)
      \begin{equation}
        \mathcal{K}_L(\bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) = (\bm{\overline{C}}^*\bm{\overline{B}}, \bm{\overline{C}}^*\bm{\overline{A}}\bm{\overline{B}}, \dots, \bm{\overline{C}}^*\bm{\overline{A}}^{L-1}\bm{\overline{B}}) \in \mathbbm{R}^L
      \end{equation}
    \item The \emph{SSM generating function} at node \( z \) is
      \begin{equation}
        \label{eq:generating-function}
        \hat{\mathcal{K}}(z; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \in \mathbbm{C} := \sum_{i=0}^\infty \bm{\overline{C}}^* \bm{\overline{A}}^i \bm{\overline{B}} z^i
        = \bm{\overline{C}}^* (\bm{I} - \bm{\overline{A}} z)^{-1} \bm{\overline{B}}
      \end{equation}
      and the \emph{truncated SSM generating function} at node \( z \) is
      \begin{equation}
        \hat{\mathcal{K}}_L(z; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}})^* \in \mathbbm{C} := \sum_{i=0}^{L-1} \bm{\overline{C}}^* \bm{\overline{A}}^i \bm{\overline{B}} z^i
        = \bm{\overline{C}}^* (\bm{I} - \bm{\overline{A}}^L z^L) (\bm{I} - \bm{\overline{A}} z)^{-1} \bm{\overline{B}}
      \end{equation}
    \item The truncated SSM generating function at nodes \( \Omega \in \mathbbm{C}^M \) is
      \begin{equation}
        \hat{\mathcal{K}}_L(\Omega; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \in \mathbbm{C}^M := \left( \hat{\mathcal{K}}_L(\omega_k; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \right)_{k \in [M]}
      \end{equation}

  \end{itemize}
\end{definition}

Intuitively, the generating function essentially converts the SSM convolution filter from the time domain to frequency domain.
Importantly, it preserves the same information, and the desired SSM convolution filter can be recovered from evaluations of its generating function.
\begin{lemma}%
  The SSM function \( \mathcal{K}_L(\bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \) can be computed from the SSM generating function \( \hat{\mathcal{K}}_L(\Omega; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \)
  at the roots of unity \( \Omega = \{ \exp(-2\pi i \frac{k}{L} : k \in [L] \} \)
  stably in \( O(L \log L) \) operations.
\end{lemma}
\begin{proof}%
  For convenience define
  \begin{align*}
    \bm{\overline{K}} &= \mathcal{K}_L(\bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \\
    \bm{\hat{K}} &=  \hat{\mathcal{K}}_L(\Omega; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \\
    \bm{\hat{K}}(z) &=  \hat{\mathcal{K}}_L(z; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}})
    .
  \end{align*}
  Note that
  \begin{align*}
    \bm{\hat{K}}_j = \sum_{k=0}^{L-1} \bm{\overline{K}}_k \exp\left(-2\pi i \frac{jk}{L}\right).
  \end{align*}
  Note that this is exactly the same as the Discrete Fourier Transform (DFT):
  \begin{align*}
    \bm{\hat{K}} = \mathcal{F}_L \bm{K}.
  \end{align*}
  Therefore \( \bm{K} \) can be recovered from \( \bm{\hat{K}} \) with a single inverse DFT,
  which requires \( O(L \log L) \) operations with the Fast Fourier Transform (FFT) algorithm.
\end{proof}


\paragraph{Reduction 2: Woodbury Correction}

The primary motivation of \cref{def:generating-function} is that it turns \emph{powers} of \( \bm{\overline{A}} \) into a single \emph{inverse} of \( \bm{\overline{A}} \) (equation \eqref{eq:generating-function}).
While DPLR matrices cannot be powered efficiently due to the low-rank term, they can be inverted efficiently by the well-known Woodbury identity.

\begin{proposition}[Binomial Inverse Theorem or Woodbury matrix identity~\cite{woodbury1950,golub2013matrix}]
  \label{prop:woodbury}
  Over a commutative ring $\mathcal{R}$, let $\bm{A} \in \mathcal{R}^{N \times N}$ and $\bm{U},\bm{V} \in \mathcal{R}^{N \times p}$. Suppose $\bm{A}$ and $\bm{A}+\bm{U}\bm{V}^*$ are invertible. Then $\bm{I}_p + \bm{V}^*\bm{A}^{-1}\bm{U} \in \mathcal{R}^{p \times p}$ is invertible and
  \begin{align*}
    (\bm{A} + \bm{U}\bm{V}^*)^{-1} = \bm{A}^{-1} - \bm{A}^{-1}\bm{U}(\bm{I}_p + \bm{V}^*\bm{A}^{-1}\bm{U})^{-1}\bm{V}^*\bm{A}^{-1}
  \end{align*}
\end{proposition}

With this identity, we can convert the SSM generating function on a DPLR matrix \( \bm{A} \) into one on just its diagonal component.

\begin{lemma}%
  \label{lmm:resolvent-woodbury}
  Let \( \bm{A} = \bm{\Lambda} - \bm{P} \bm{Q}^* \) be a diagonal plus low-rank representation.
  Then for any root of unity \( z \in \Omega \), the truncated generating function satisfies
  \begin{align*}
    \bm{\hat{K}}(z) &=
    \frac{2}{1+z}\left[ \bm{\tilde{C}}^* \bm{R}(z) \bm{B} - \bm{\tilde{C}}^* \bm{R}(z) \bm{P} \left( 1 + \bm{Q}^* \bm{R}(z) \bm{P} \right)^{-1} \bm{Q}^* \bm{R}(z) \bm{B} \right]
    \\
    \bm{\tilde{C}} &= (\bm{I}-\bm{\overline{A}}^L)^* \bm{C}
    \\
    \bm{R}(z; \bm{\Lambda}) &= \left(\frac{2}{\dt} \frac{1-z}{1+z} - \bm{\Lambda}\right)^{-1}
    .
  \end{align*}
\end{lemma}
%
\begin{proof}%
  Directly expanding \cref{def:generating-function} yields
  \begin{align*}
    \mathcal{K}_L(z; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}})
    &=
    \bm{\overline{C}}^* \bm{\overline{B}} + \bm{\overline{C}}^* \bm{\overline{A}} \bm{\overline{B}} z + \dots + \bm{\overline{C}}^* \bm{\overline{A}}^{L-1} \bm{\overline{B}} z^{L-1}
    \\&=
    \bm{\overline{C}}^* \left(\bm{I}-\bm{\overline{A}}^L\right) \left(\bm{I} - \bm{\overline{A}} z\right)^{-1} \bm{\overline{B}}
    \\&=
    \bm{\tilde{C}}^* \left(\bm{I} - \bm{\overline{A}} z\right)^{-1} \bm{\overline{B}}
  \end{align*}
  where \(  \bm{\tilde{C}}^* = \bm{C}^* \left(\bm{I}-\bm{\overline{A}}^L\right) \).

  We can now explicitly expand the discretized SSM matrices \( \bm{\overline{A}} \) and \( \bm{\overline{B}} \) back in terms of the original SSM parameters \( \bm{A} \) and \( \bm{B} \).
  \cref{lmm:bilinear-resolvent} provides an explicit formula, which allows further simplifying
  \begin{align*}
    \bm{\tilde{C}}^* \left(\bm{I} - \bm{\overline{A}} z\right)^{-1} \bm{\overline{B}}
    &= \frac{2}{1+z} \bm{\tilde{C}}^* \left(\frac{2}{\Delta} \frac{1-z}{1+z} - \bm{A}\right)^{-1}  \bm{B}
    \\&=
    \frac{2}{1+z} \bm{\tilde{C}}^* \left(\frac{2}{\Delta} \frac{1-z}{1+z} - \bm{\Lambda} + \bm{P}\bm{Q}^* \right)^{-1} \bm{B}
    \\&=
    \frac{2}{1+z}\left[ \bm{\tilde{C}}^* \bm{R}(z) \bm{B} - \bm{\tilde{C}}^* \bm{R}(z) \bm{P} \left( 1 + \bm{Q}^* \bm{R}(z) \bm{P} \right)^{-1} \bm{Q}^* \bm{R}(z) \bm{B} \right]
    .
  \end{align*}
  The last line applies the Woodbury Identity (\cref{prop:woodbury}) where \( \bm{R}(z) = \left(\frac{2}{\Delta} \frac{1-z}{1+z} - \bm{\Lambda}\right)^{-1} \).
\end{proof}


The previous proof used the following self-contained result to back out the original SSM matrices from the discretization.
\begin{lemma}%
  \label{lmm:bilinear-resolvent}
  Let \( \bm{\overline{A}}, \bm{\overline{B}} \) be the SSM matrices \( \bm{A}, \bm{B} \) discretized by the bilinear discretization with step size \( \dt \). Then
  \begin{align*}
    \bm{C}^*\left(\bm{I} - \bm{\overline{A}z} \right)^{-1} \bm{\overline{B}}
    =
    \frac{2\Delta}{1+z} \bm{C}^* \left[ {2 \frac{1-z}{1+z}} - \Delta \bm{A} \right]^{-1} \bm{B}
  \end{align*}
\end{lemma}
\begin{proof}%
  Recall that the bilinear discretization that we use (equation \eqref{eq:2}) is
  \begin{align*}
    \bm{\overline{A}}
    &=
    \left(\bm{I} - \frac{\Delta}{2} \bm{A}\right)^{-1} \left(\bm{I} + \frac{\Delta}{2} \bm{A}\right)
    \\
    \bm{\overline{B}} &= \left(\bm{I} - \frac{\Delta}{2} \bm{A}\right)^{-1} \Delta \bm{B}
  \end{align*}
  The result is proved algebraic manipulations.
  \begin{align*}
    \bm{C}^*\left(\bm{I} - \bm{\overline{A}} z\right)^{-1} \bm{\overline{B}}
    &=
    \bm{C}^* \left[ \left(\bm{I} - \frac{\Delta}{2} \bm{A}\right)^{-1}\left(\bm{I} - \frac{\Delta}{2} \bm{A}\right)  - \left(\bm{I} - \frac{\Delta}{2} \bm{A}\right)^{-1} \left(\bm{I} + \frac{\Delta}{2} \bm{A}\right) z \right]^{-1} \bm{\overline{B}}
    \\&=
    \bm{C}^* \left[ \left(\bm{I} - \frac{\Delta}{2} \bm{A}\right) - \left(\bm{I} + \frac{\Delta}{2} \bm{A}\right) z \right]^{-1} \left(\bm{I} - \frac{\Delta}{2} \bm{A}\right) \bm{\overline{B}}
    \\&=
    \bm{C}^* \left[ \bm{I}(1 - z) - \frac{\Delta}{2} \bm{A} (1+z) \right]^{-1} \Delta\bm{B}
    \\&=
    \frac{\Delta}{1-z} \bm{C}^* \left[ \bm{I} - \frac{\Delta \bm{A}}{2 \frac{1-z}{1+z}} \right]^{-1} \bm{B}
    \\&=
    \frac{2\Delta}{1+z} \bm{C}^* \left[ {2 \frac{1-z}{1+z}} \bm{I} - \Delta \bm{A} \right]^{-1} \bm{B}
  \end{align*}
\end{proof}

Note that in the \methodabbrv{} parameterization, instead of constantly computing \( \bm{\tilde{C}} = \left(\bm{I} - \bm{\overline{A}}^L\right)^* \bm{C} \),
we can simply reparameterize our parameters to learn \( \bm{\tilde{C}} \) directly instead of \( \bm{C} \),
saving a minor computation cost and simplifying the algorithm.

\paragraph{Reduction 3: Cauchy Kernel}
We have reduced the original problem of computing \( \bm{\overline{K}} \) to the problem of computing the SSM generating function \( \hat{\mathcal{K}}_L(\Omega; \bm{\overline{A}}, \bm{\overline{B}}, \bm{\overline{C}}) \)
in the case that \( \bm{\overline{A}} \) is a diagonal matrix.
We show that this is exactly the same as a Cauchy kernel, which is a well-studied problem with fast and stable numerical algorithms.

\begin{definition}%
  \label{def:cauchy}
  A \textbf{Cauchy matrix} or kernel on nodes \( \Omega = (\omega_i) \in \mathbbm{C}^M \) and \( \Lambda = (\lambda_j) \in \mathbbm{C}^N \) is
  \begin{align*}
    \bm{M} \in \mathbbm{C}^{M \times N} &= \bm{M}(\Omega, \Lambda) = (\bm{M}_{ij})_{i \in [M], j \in [N]} \qquad
    \bm{M}_{ij} = \frac{1}{\omega_i - \lambda_j}
    .
  \end{align*}
  The computation time of a Cauchy matrix-vector product of size \( M \times N \) is denoted by \( \mathcal{C}(M, N) \).
\end{definition}

Computing with Cauchy matrices is an extremely well-studied problem in numerical analysis,
with both fast arithmetic algorithms and fast numerical algorithms based on the famous Fast Multipole Method (FMM)
\citep{pan2001structured,pan2015transformations,pan2017fast}.
\begin{proposition}[Cauchy]%
  \label{prop:cauchy}
  A Cauchy kernel requires \( O(M+N) \) space, and operation count
  \begin{align*}
    \mathcal{C}(M, N) =
    \begin{cases}%
      O\left( MN \right)  & \text{naively} \\
      O\left( (M+N) \log^2(M+N) \right) & \text{in exact arithmetic} \\
      O\left( (M+N) \log(M+N) \log \frac{1}{\varepsilon} \right) & \text{numerically to precision \( \varepsilon \)}
      .
    \end{cases}
  \end{align*}
\end{proposition}

\begin{corollary}%
  Evaluating \( \bm{Q}^* \bm{R}(\Omega; \Lambda) \bm{P} \) (defined in \cref{lmm:resolvent-woodbury}) for any set of nodes \( \Omega \in \mathbbm{C}^L \), diagonal matrix \( \Lambda \), and vectors \( \bm{P}, \bm{Q} \) can be computed in \( \mathcal{C}(L,N) \) operations and \( O(L+N) \) space, where \( \mathcal{C}(L,N) = \tilde{O}(L+N) \) is the cost of a Cauchy matrix-vector multiplication.
\end{corollary}
\begin{proof}%
  For any fixed \( \omega \in \Omega \), we want to compute \( \sum_{j} \frac{q_j^* p_j}{\omega - \lambda_j} \). Computing this over all \( \omega_i \) is therefore exactly a Cauchy matrix-vector multiplication.
\end{proof}

This completes the proof of \cref{thm:s4-convolution}.
In \cref{alg:s4-convolution},
note that the work is dominated by Step \ref{step:cauchy},
which has a constant number of calls to a black-box Cauchy kernel, with complexity given by \cref{prop:cauchy}.

