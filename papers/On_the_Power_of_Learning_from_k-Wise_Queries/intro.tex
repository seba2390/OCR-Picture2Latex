\section{Introduction}

In this paper, we consider several well-studied models of learning from i.i.d.~samples that restrict the algorithm's access to samples to evaluation of functions of an individual sample. The primary model of interest is the statistical query model introduced by Kearns \cite{kearns1998efficient} as a restriction of Valiant's PAC learning model \cite{valiant1984theory}. The SQ model allows the learning algorithm to access the data only via \emph{statistical queries}, which are estimates of the expectation of any function of labeled examples with respect to the input distribution $D$. More precisely, if the domain of the functions is $Z$, then a statistical query is specified by a function $\phi:Z \times \{\pm 1\} \to [-1,1]$ and by a tolerance parameter $\tau$. Given $\phi$ and $\tau$, the statistical query oracle returns a value $v$ which satisfies $|v - \Ex_{(z,b) \sim D}[\phi(z,b)]| \le \tau$.

The SQ model is known to be closely-related to several other models and concepts: linear statistical functionals \cite{wasserman2013all}, learning with a distance oracle \cite{Ben-DavidIK90}, approximate counting (or linear) queries extensively studied in differential privacy (e.g., \cite{DinurN03,BlumDMN05,DworkMNS06,roth2010interactive}), local differential privacy \cite{kasiviswanathan2011can}, evolvability \cite{valiant2009evolvability,feldman2008evolvability}, and algorithms that extract a small amount of information from each sample \cite{Ben-DavidD98,FeldmanGRVX:12,FeldmanPV:13,SteinhardtVW16}. This allows to easily extend the discussion in the context of the SQ model to these related models and we will formally state several such corollaries.% for the local differential privacy and low-communication models.

Most standard algorithmic approaches used in learning theory are known to be implementable using SQs (e.g., \cite{blum1998polynomial,DunaganV04,BlumDMN05,chu2007map,FeldmanPV:13,BalcanF15,FeldmanGV:15}) leading to numerous  theoretical (e.g., \cite{BalcanBFM12,de2015learning,dwork2015preserving}) and practical (e.g., \cite{chu2007map,roy2010airavat,sujeeth2011optiml,dwork2015generalization}) applications. SQ algorithms have also been recently studied outside the context of learning theory \cite{FeldmanGRVX:12,FeldmanPV:13,FeldmanGV:15}. In this case we denote the domain of data samples by $X$.

Another reason for the study of SQ algorithms is that it is possible to prove information-theoretic lower bounds on the complexity of any SQ algorithm that solves a given problem. Given that a large number of algorithmic approaches to problems defined over data sampled i.i.d.~from some distribution can be implemented using statistical queries, this provides a strong and unconditional evidence of the problem's hardness. For a number of central problems in learning theory and complexity theory, unconditional lower bounds for SQ algorithms are known that closely match the known {\em computational} complexity upper bounds for those problems (\eg \cite{BlumFJ+:94, FeldmanGRVX:12,FeldmanPV:13,DachmanFTWW:15,DiakonikolasKS:16}). %Moreover, in most cases these are the only known lower bounds that exclude a wide range of algorithmic approaches.


A natural strengthening of the SQ model (and other related models) is to allow function over $k$-tuples of samples instead of a single sample. That is, for a $k$-ary query function $\phi:X^k \to [-1,1]$, the algorithm can obtain an estimate of $\Ex_{x_1,\ldots, x_k \sim D}[\phi(x_1,\ldots,x_k)]$. It can be seen as interpolating between the power of algorithms that can see all the samples at once and those that process a single sample at a time. While most algorithms can be implemented using standard unary queries, some algorithms are known to require such more powerful queries. The most well-known example is Gaussian elimination over $\mathbb{F}_2^n$ that is used for learning parity functions. Standard hardness amplification techniques rely on mapping examples of a function $f(z)$ to examples of a function $g(f(z_1),\ldots,f(z_k))$ (for example \cite{BonehLipton:93,FeldmanLS:11colt}). Implementing such reduction requires $k$-wise queries and, consequently, to obtain a lower bound for solving an amplified problem with unary queries one needs a lower bound against solving the original problem with $k$-wise queries. A simple example of 2-wise statistical query is collision probability $\Pr_{x_1,x_2 \sim D} [x_1=x_2]$ that is used in several distribution property testing algorithms.

\subsection{Previous work}
Blum, Kalai and Wasserman \cite{blum2003noise} introduced and studied the power of $k$-wise SQs in the context of weak {\em distribution-specific} PAC learning: that is the learning algorithm observes pairs $(z,b)$, where $z$ is chosen randomly from some fixed and known distribution $P$ over $Z$ and $b=f(z)$ for some unknown function $f$ from a class of functions $\cal C$. They showed that if a class of functions $\cal C$ can be learned with error $1/2-\lambda$ relative to distribution $P$ using $q$ $k$-wise SQs of tolerance $\tau$ then it can be learned with error $\max\{1/2 - \lambda, 1/2 -\tau/2^k \}$ using $O(q \cdot 2^k)$ unary SQs of tolerance $\tau/2^k$.

More recently, Steinhardt \etal \cite{SteinhardtVW16} considered $k$-wise queries in the $b$-bit sampling model in which for any query function $\phi:X^k \to \zo^{b}$ an algorithm get the value $\phi(x_1,\ldots,x_k)$ for $x_1,\ldots,x_k$ drawn randomly and independently from $D$ (it is referred to as one-way communication model in their work). They give a general technique for proving lower bounds on the number of such queries that are required to solve a given problem.

\subsection{Our results}
In this work, we study the relationship between the power of $k$-wise queries and unary queries for arbitrary problems in which the input is determined by some unknown input distribution $D$ that belongs a (known) family of distributions $\D$ over domain $X$. %Namely, problems in which the queries of an algorithm are answered relative to some unknown distribution $D \in \D$.

\paragraph{Separation for distribution-independent learning:}
We first demonstrate that for distribution-independent PAC learning $(k+1)$-wise queries are exponentially stronger than $k$-wise queries. We say that the $k$-wise SQ complexity of a certain problem is $m$ if $m$ is the smallest such that there exists an algorithm that solves the problem using $m$ $k$-wise SQs of tolerance $1/m$.
\begin{theorem}\label{thm:k_wise_sep} (Informal)
For every positive integer $k$ and any prime number $p$, there is a concept class $\calC$ of Boolean functions defined over a domain of size $p^{k+1}$ such that the $(k+1)$-wise SQ complexity of distribution-independent PAC learning $\calC$ with is $O_k(\log{p})$ whereas the $k$-wise SQ complexity of distribution-independent PAC learning of $\calC$ is $\Omega_k(p^{1/4})$.
\end{theorem}

The class of functions we use consists of all indicator functions of $k$-dimensional affine subspaces of $\F_p^{k+1}$. Our lower bound is a generalization of the lower bound for unary SQs in \cite{Feldman:16sqd} (that corresponds to $k=1$ case of the lower bound). A simple but important observation that allows us to easily adapt the techniques from earlier works on SQs to the $k$-wise case is that a $k$-wise SQ for an input distribution $D \in \D$ are equivalent to unary SQ for a product distribution $D^k$.

The upper bound relies on the ability to find the affine subspace given $k+1$ positively labeled and linearly independent points in $\F_p^{k+1}$.  Unfortunately, for general distributions the probability of observing such a set of points can be arbitrarily small. Nevertheless, we argue that there will exist a unique lower-dimensional affine subspace that contains enough probability mass of all the positive points in this case. This upper bound essentially implies that given $k$-wise queries one can solve problems that require Gaussian elimination over a system of $k$ equations.

\paragraph{Reduction for flat $\D$:}\label{subsec:intro_flat}
The separation in Theorem~\ref{thm:k_wise_sep} relies on using an unrestricted class of distributions $\D$. We now prove that if $\D$ is ``flat" relative to some ``central" distribution $\bar{D}$ then one can upper bound the power of $k$-wise queries in terms of unary queries.

%It turns out that if we restrict to the special class of \emph{flat} distributions -- defined next -- then there is an unary SQ algorithm that simulates any $k$-wise SQ algorithm with a relatively small overhead.

\begin{defn}[Flat class of distributions]
Let $\calD$ be a set of distributions over $X$, and $\bar{D}$ a distribution over $X$. For $\gamma \geq 1$ we say that $\calD$ is $\gamma$-flat if there exists some distribution $\bar{D}$ over $X$ such that for all $D \in \mathcal{D}$ and all measurable subsets $E \subseteq X$, we have that $\Pr_{x\sim D}[x \in E] \le \gamma \cdot \Pr_{x\sim \bar{D}}[x \in E]$.
\end{defn}

We now state our upper bound for flat classes of distributions, where we use $\STAT_D^{(k)}(\tau)$ to refer to the oracle that answers $k$-wise SQs for $D$ with tolerance $\tau$.
\begin{theorem}\label{thm:flat}
Let $\gamma \geq 1$, $\tau > 0$ and $k$ be any positive integer. Let $X$ be a domain and $\calD$ a $\gamma$-flat class of distributions over $X$. There exists a randomized algorithm that given any $\delta > 0$ and a $k$-ary function $\phi: X^k \to [-1,1]$ estimates $D^k[\phi]$ within $\tau$  for every (unknown) $D \in \calD$ with success probability at least $1-\delta$ using $$\tilde{O}\bigg( \frac{\gamma^{k-1} \cdot k^3}{\tau^3} \cdot \log (1/\delta)\bigg)$$
queries to $\STAT_D^{(1)}(\tau/(6 \cdot k))$.\end{theorem}

To prove this result, we use a recent general characterization of SQ complexity \cite{Feldman:16sqd}. This characterization reduces the problem of estimating $D^k[\phi]$ to the problem of distinguishing between $D^k$ and $D_1^k$ for every $D \in \D$ and some fixed $D_1$. We show that when solving this problem, any $k$-wise query can be replaced by a randomly chosen set of unary queries. Finding these queries requires drawing samples from $D^{k-1}$. As we do not know $D$, we use $\bar{D}$ instead incurring the $\gamma^{k-1}$ overhead in sampling. In Section \ref{sec:pf_flat} we show that weaker notions of ``flatness" based on different notions of divergence between distributions can also be used in this reduction.

It is easy to see that, when PAC learning $\calC$ with respect to a fixed distribution $P$ over $Z$, the set of input distributions is 2-flat (relative to the distribution that is equal to $P$ on $Z$ and gives equal weight $1/2$ to each label). Therefore, our result generalizes the results in \cite{blum2003noise}. More importantly, the tolerance in our upper bound scales linearly with $k$ rather than exponentially (namely, $\tau/2^k)$.

This result can be used to obtain lower bounds against $k$-wise SQs algorithms from lower bounds against unary SQ algorithms. In particular, it can be used to rule out reductions that require looking at $k$ points of the original problem instance to obtain each point of the new problem instance. As an application, we obtain exponential lower bounds for solving constraint stochastic satisfaction problems and DNF learning by $k$-wise SQ algorithm with $k=n^{1-\alpha}$ for any constant $\alpha > 0$ from lower bounds for CSPs given in \cite{FeldmanPV:13}.
We state the result for learning DNF here. Definitions and the lower bound for CSPs can be found in Section \ref{sec:lower-bounds}.
\begin{theorem}\label{thm:dnf-k-wise-intro}
For any constant $\alpha >0$ (independent of $n$), there exists a constant $\beta>0$ such that
 any  algorithm that learns DNF formulas of size $n$ with error $<1/2 - n^{- \beta \log n}$ and success probability at least $2/3$ requires at least $2^{n^{1-\alpha}}$ calls to $\STAT^{(n^{1-\alpha})}_D(n^{- \beta \log n})$.
\end{theorem}
This lower bound is based on a simple and direct reduction from solving the stochastic CSP that arises in Goldreich's proposed PRG \cite{goldreich2000candidate} to learning DNF that is of independent interest (see Lemma \ref{lem:reduce-dnf}). For comparison, the standard SQ lower bound for learning polynomial size DNF \cite{BlumFJ+:94} relies on hardness of learning parities of size $\log n$ over the uniform distribution. Yet, parities of size $\log n$ can be easily learned from $(\log^2 n)$-wise statistical queries (since solving a system of $\log^2 n$ linear equations will uniquely identify a $\log n$-sparse parity function). Hence our lower bound holds against qualitatively stronger algorithms.  Our lower bound is also exponential in the number of queries whereas the known argument implies only a quasipolynomial lower bound\footnote{We remark that an exponential lower bound on the number of queries has not been previously stated even for unary SQs. The unary version can be derived from known results as explained in Section \ref{sec:lower-bounds}.}.

\paragraph{Reduction for low-communication queries:}
Finally, we point out that $k$-wise queries that require little information about each of the inputs can also be simulated using unary queries. This result is a simple corollary of the recent work of Steinhardt \etal \cite{SteinhardtVW16} who show that any computation that extracts at most $b$ bits from each of the samples (not necessarily at once) can be simulated using unary SQs.

\begin{theorem}\label{thm:sq_and_cc}
Let $\phi:X^k \to \{\pm 1\}$ be a function, and assume that $\phi$ has $k$-party public-coin randomized communication complexity of $b$ bits per party with success probability $2/3$. Then, there exists a randomized algorithm that, with probability at least $1-\delta$, estimates $\Ex_{x \sim D^k}[\phi(x)]$ within $\tau$ using $O(b \cdot k \cdot \log(1/\delta)/\tau^2)$ queries to $\STAT^{(1)}_D(\tau')$ for some $\tau' = \tau^{O(b)}/k$.
\end{theorem}

As a simple application of Theorem~\ref{thm:sq_and_cc}, we show a unary SQ algorithm that estimates the collision probability of an unknown distribution $D$ within $\tau$ using $1/\tau^2$ queries $\STAT^{(1)}_{D}(\tau^{O(1)})$. The details appear in Section~\ref{sec:sq_cc}.

\paragraph{Corollaries for related models:}
Our separation result and reductions imply similar results for $k$-wise versions of two well-studied learning models: local differential privacy and the $b$-bit sampling model. % \emph{learning with restricted focus of attention}.

Local differentially private algorithms \cite{kasiviswanathan2011can} (also referred to as randomized response) are differentially private algorithms in which each sample goes through a differentially private transformation chosen by the analyst. This model is the focus of recent privacy preserving industrial applications by Google \cite{ErlingssonPK14} and Apple. We define a $k$-wise version of this model in which analyst's differentially private transformations are applied to $k$-tuples of samples. This model interpolates naturally between the usual (or global) differential privacy and the local model.

Kasiviswanathan \etal\cite{kasiviswanathan2011can} showed that a concept class is learnable by a local differentially private algorithm if and only if it is learnable in the SQ model. Hence up to polynomial factors the models are equivalent (naturally, such polynomial factors are important for applications but here we focus only on the high-level relationships between the models). This result also implies that $k$-local differentially private algorithms (formally defined in Section~\ref{subsec:local_DP}) are equivalent to $k$-wise SQ algorithms (up to a polynomial blow-up in the complexity). Theorem~\ref{thm:k_wise_sep} then implies an exponential separation between $k$-wise and $(k+1)$-wise local differentially private algorithms (see Corollary~\ref{cor:local_DP} for details). It can be seen as a substantial strengthening of a separation between the local model and the global one also given in \cite{kasiviswanathan2011can}. The reductions in Theorem \ref{thm:flat} and Theorem \ref{thm:sq_and_cc} imply two approaches for simulating  $k$-local differentially private algorithms using 1-local algorithms.

The SQ model is also known to be equivalent (up to a factor polynomial in $2^b$) to the $b$-bit sampling model introduced by Ben-David and Dichterman \cite{Ben-DavidD98} and studied more recently in \cite{FeldmanGRVX:12,FeldmanPV:13,ZhangDJW13,SteinhardtD15,SteinhardtVW16}. Lower bounds for the $k$-wise version of this model are given in \cite{ZhangDJW13,SteinhardtVW16}.
Our results can be easily translated to this model as well. We provide additional details in Section~\ref{sec:apps}.
