\subsection{Applications to solving CSPs and learning DNF}
\label{sec:lower-bounds}
We now give some examples of the application of our reduction to obtain lower bounds against $k$-wise SQ algorithms. Our applications for stochastic constraint satisfaction problems (CSPs) and DNF learning. We start with the definition of a stochastic CSP with a {\em planted solution} which is a pseudo-random generator based on Goldreich's proposed one-way function \cite{goldreich2000candidate}.
\begin{definition}
Let $t \in \mathbb{N}$ and $P: \{\pm 1\}^t \to \{\pm 1\}$ be a fixed predicate. We are given access to samples from a distribution $P_{\sigma}$, corresponding to a (``planted'') assignment $\sigma \in \{\pm 1 \}^n$. A sample from this distribution is a uniform-random  $t$-tuple $(i_1, \dots, i_t)$ of distinct variable indices along with the value $P(\sigma_{i_1}, \dots, \sigma_{i_t})$. The goal is to recover the assignment $\sigma$ when given $m$ independent samples from $P_{\sigma}$. A (potentially) easier problem is to distinguish any such planted distribution from the distribution $U_t$ in which the value is an independent uniform-random coin flip (instead of $P(\sigma_{i_1}, \dots, \sigma_{i_t})$).
\end{definition}
We say that a predicate $P: \{\pm 1\}^t \to \{\pm 1\}$ has complexity $r$ if $r$ is the degree of the lowest-degree non-zero Fourier coefficient of $P$. It can be as large as $t$ (for the parity function).
A lower bound on the (unary) SQ complexity of solving such CSPs was shown by \cite{FeldmanPV:13} (their result is for the stronger $\VSTAT$ oracle but here we state the version for the $\STAT$ oracle).
\begin{theorem}[\cite{FeldmanPV:13}]\label{thm:FPV_csp}
Let $t, q \in \mathbb{N}$ and $P: \{\pm 1\}^t \to \{\pm 1\}$ be a fixed predicate of complexity $r$. Then for any $q >0$, any  algorithm that, given access to a distribution $D \in \{P_\sigma\ |\  \sigma  \in \{\pm 1 \}^n\} \cup \{U_t\}$  decides correctly whether $D = P_\sigma$ or $D=U_t$ with probability at least $2/3$ needs $q/2^{O(t)}$ queries to $\STAT^{(1)}_D\left(\left(\frac{\log q}{n}\right)^{r/2}\right)$.
\end{theorem}

The set of input distributions in this problem is $2$-flat relative to $U_t$ and it is one-to-many decision problem. Hence Theorem \ref{thm:flat-decision-reduction} implies\footnote{We can also get essentially the same result by applying the simulation of a $k$-wise SQ using unary SQs from Theorem \ref{thm:flat}.} the following lower bound for $k$-wise SQ algorithms.
\begin{theorem}\label{thm:csp-k-wise}
Let $t \in \mathbb{N}$ and $P: \{\pm 1\}^t \to \{\pm 1\}$ be a fixed predicate of complexity $r$. Then for any $\alpha >0$, any  algorithm that, given access to a distribution $D \in \{P_\sigma\ |\  \sigma  \in \{\pm 1 \}^n\} \cup \{U_t\}$  decides correctly whether $D = P_\sigma$ or $D=U_t$ with probability at least $2/3$ needs $2^{n^{1-\alpha} -O(t)}$ queries to $\STAT^{(n^{1-\alpha})}_D\left((2/n^{\alpha})^{r/2}  \cdot n^{1-\alpha}/4\right)$.
\end{theorem}
\begin{proof}
Let $\A$ be a $k$-wise SQ algorithm using $q'$ queries to $\STAT^{(n^{1-\alpha})}_D\left((2/n^{\alpha})^{r/2} \cdot n^{1-\alpha}/6\right)$ which solves the problem with success probability $2/3$.
We let $k=n^{1-\alpha}$ and apply Theorem \ref{thm:flat-decision-reduction} to obtain an algorithm that uses unary SQs and solves the problem with success probability $2/3$. This algorithm uses $q_0 = q' \cdot 2^{n^{1-\alpha}} \cdot n^{O(r)}$ queries to $\STAT^{(1)}_D\left((2/n^{\alpha})^{r/2}\right)$. Now choosing $q = 2^{2n^{1-\alpha}}$ we get that $\left(\frac{\log q}{n}\right)^{r/2} \leq (2/n^{\alpha})^{r/2}$. This means that $q_0 \geq q/2^{O(t)} = 2^{2n^{1-\alpha}- O(t)}$.
Hence $q' = 2^{2n^{1-\alpha}-O(t) - n^{1-\alpha}- O(r)} = 2^{n^{1-\alpha} -O(t)}$.
\end{proof}
Similar lower bounds can be obtained for other problems considered in \cite{FeldmanPV:13}, namely, planted satisfiability and $t$-SAT refutation.

To obtain a lower bound for learning DNF formulas we can use a simple reduction from the Goldreich's PRG defined above to learning DNF formulas of polynomial size. It is based on ideas implicit in the reduction from $t$-SAT refutation to DNF learning from \cite{DanielyS16}.
\begin{lemma}\label{lem:reduce-dnf}
$P: \{\pm 1\}^t \to \{\pm 1\}$ be a fixed predicate. There exists a mapping $M$ from $t$-tuples of indices in $[n]$ to $\zo^{tn}$ such that for every $\sigma \in \{\pm 1 \}^n$ there exists a DNF formula $f_\sigma$ of size $2^t$ satisfying $P(\sigma_{i_1}, \dots, \sigma_{i_t}) = f_\sigma(M(i_1, \dots, i_t))$.
\end{lemma}
\begin{proof}
The mapping $M$ maps $(i_1, \dots, i_t)$ to the concatenation of the indicator vectors of each of the indices. Namely, for $j \in [t]$ and $\ell \in [n]$, $M(i_1, \dots, i_t)_{j,\ell} = 1$ if and only if $i_j = \ell$, where we use the double index $j,\ell$ to refer to element $n (j-1) + \ell$ of the vector. Let $v_{j,\ell}$ denote the variable with the index $j,\ell$. Let $\sigma$ be any assignment and we denote by $z_j^\sigma$ the $j$-th variable of our predicate $P$ when the assignment is equal to $\sigma$. We first observe that $z_j^\sigma \equiv \bigwedge_{\ell \in [n], \sigma_\ell = 0} \bar{v}_{j,\ell}$. This is true since, by definition, the value of the $j$-th variable of our predicate is $\sigma_{i_j}$. This value is $1$ if and only if $i_j \not\in \{\ell \in [n]\ | \ \sigma_\ell = 0\}$. This is equivalent to $v_{j,\ell}$ being equal to $0$ for all $\ell \in [n]$ such that  $\sigma_\ell = 0$. Analogously, $\bar{z}^\sigma_j \equiv \bigwedge_{\ell \in [n], \sigma_\ell = 1} \bar{v}_{j,\ell}$. This implies that any conjunction of variables $z_1^\sigma,\bar{z}^\sigma_1,\ldots,z^\sigma_t,\bar{z}^\sigma_t$ can be expressed as a conjunction over variables $\bar{v}_{j,\ell}$. Any predicate $P$ can be expressed as a disjunction of at most $2^t$ conjunctions and hence there exists a DNF formula $f_\sigma$ of size at most $2^t$ whose value on $M(i_1, \dots, i_t)$ is equal to $P(\sigma_{i_1}, \dots, \sigma_{i_t})$
\end{proof}

This reduction implies that by converting a sample $((i_1, \dots, i_t),b)$ to a sample $(M(i_1, \dots, i_t),b)$ we can transform the Goldreich's PRG problem into a problem in which our goal is to distinguish examples of some DNF formula $f_\sigma$ from randomly labeled examples. Naturally, an algorithm that can learn DNF formulas can output a hypothesis which predicts the label (with some non-trivial accuracy), whereas such hypothesis cannot exist for predicting random labels. Hence known SQ lower bounds on planted CSPs \cite{FeldmanPV:13} immediately imply lower bounds for learning DNF. Further, by applying Lemma \ref{lem:reduce-dnf} together with Thm.~\ref{thm:csp-k-wise} for $t=r=\log n$ we obtain the first lower bounds for learning DNF against $n^{1-\alpha}$-wise SQ algorithms.
\begin{theorem}\label{thm:dnf-k-wise}
For any constant (independent of $n$) $\alpha >0$, there exists a constant $\beta>0$ such that
 any  algorithm that PAC learns DNF formulas of size $n$ with error $<1/2 - n^{- \beta \log n}$ and success probability at least $2/3$ needs at least $2^{n^{1-\alpha}}$ queries to $\STAT^{(n^{1-\alpha})}_D(n^{- \beta \log n})$.
\end{theorem}
We remark that this is a lower bound for PAC learning polynomial size DNF formulas with respect to some fixed (albeit non-uniform) distribution over $\zo^n$. The approach for relating $k$-wise SQ complexity to unary SQ complexity given in \cite{blum2003noise} applies to this setting. Yet, in their proof the tolerance needed for the unary SQ algorithm is $\tau/2^k$ and therefore it would not give a non-trivial lower bounds beyond $k=O(\log n)$.

%Our reduction produces a $\zo^{2tn}$
