\section{Reduction for flat distributions}\label{sec:pf_flat}
\newcommand{\dci}{{\kappa_1}}
To prove Theorem~\ref{thm:flat} we use the characterization of the SQ complexity of the problem of estimating $D^k[\phi]$ for $D\in \D$ using a notion of statistical dimension from \cite{Feldman:16sqd}. Specifically, we use the characterization of the complexity of solving this problem using unary SQs and also the generalization of this characterization that characterizes the complexity of solving a problem using $k$-wise SQs. The latter is equal to 1 (since a single $k$-wise SQ suffices to estimate $D^k[\phi]$). Hence the $k$-wise statistical dimension is also equal to 1. We then upper bound the unary statistical dimension by the $k$-wise statistical dimension. The characterization then implies that an upper bound on the unary statistical dimension gives an upper bound on the SQ complexity of estimating $D^k[\phi]$.

We also give a slightly different way to define flatness that makes it easier to extend our results to other notions of divergence.
\begin{defn}
Let $\calD$ be a set of distributions over $X$. Define
$$R_\infty(\D) \doteq \inf_{\bar D \in S^X} \sup_{D\in \D} \Div_\infty(D\|\bar D), $$
where $S^X$ denotes the set of all probability distributions over $X$ and $$\Div_\infty(D\|\bar D) \doteq \sup_{y\in X} \ln \frac{\Pr_{x\sim D}[x=y]}{\Pr_{x\sim \bar D}[x=y]}$$ denotes the max-divergence. We say that $\calD$ is $\gamma$-flat if $R_\infty(\D) \leq \ln \gamma$.
\end{defn}

For simplicity, we will start by relating the $k$-wise SQ complexity to unary SQ complexity for decision problems. The statistical dimension for this type of problems is substantially simpler than for the general problems but is sufficient to demonstrate the reduction. We then build on the results for decision problems to obtain the proof of Theorem~\ref{thm:flat}.

\subsection{Decision problems}
The $k$-wise generalization of the statistical dimension for decision problems from \cite{Feldman:16sqd} is defined as follows.
\begin{defn}
Let $k$ be any positive integer. Consider a set of distributions $\calD$ and a reference distribution $D_0$ over $X$. Let $\mu$ be a probability measure over $\calD$ and let $\tau > 0$. The $k$-wise maximum covered $\mu$-fraction is defined as
$$ \kappa_1\text{-}\fract^{(k)}(\mu,D_0,\tau) \doteq \sup_{\phi: X^k \to [-1,+1]} \bigg\{ \Pr_{D \sim \mu}[|D^k[\phi]-D_0^k[\phi]| > \tau] \bigg\}. $$
\end{defn}
\begin{defn}[$k$-wise randomized statistical dimension of decision problems]\label{def:rdm_sd}
Let $k$ be any positive integer. For any set of distributions $\cald$, a reference distribution $D_0$ over $X$ and $\tau > 0$, we define
$$ \RSD_{\kappa_1}^{(k)}(\calB(\calD,D_0), \tau) \doteq \sup_{\mu \in S^{\calD}} ( \kappa_1\text{-}\fract^{(k)}(\mu,D_0,\tau))^{-1}, $$
where $S^\D$ denotes the set of all probability distributions over $\D$.
\end{defn}

As shown in \cite{Feldman:16sqd}, $\RSD$ tightly characterizes the randomized statistical query complexity of solving the problem using $k$-wise queries. As observed before, the $k$-wise versions below are implied by the unary version in \cite{Feldman:16sqd} simply by defining the domain to be $X' \doteq X^k$ and the set of input distributions to be $\D' \doteq \{D^k \ |\  D \in \D\}$.

\begin{theorem}[\cite{Feldman:16sqd}]\label{thm:random-algorithm2queries}
Let $\calB(\D,D_0)$ be a decision problem, $\tau > 0, \delta \in (0,1/2)$, $k \in \mathbb{N}$ and $d=\RSD^{(k)}_\dci(\calB(\D,D_0),\tau)$. Then there exists a randomized algorithm that solves $\calB(\D,D_0)$ with success probability $\geq 1-\delta$ using $d \cdot \ln(1/\delta)$ queries to $\STAT^{(k)}_D(\tau/2)$. Conversely, any algorithm that solves $\calB(\D,D_0)$ with success probability $\geq 1-\delta$ requires at least $d \cdot (1-2\delta)$ queries to $\STAT^{(k)}_D(\tau)$.
\end{theorem}

We will also need the following dual formulation of the statistical dimension given in Theorem~\ref{def:rdm_sd}.
\begin{lem}[\cite{Feldman:16sqd}]\label{fa:rcvr}
Let $k$ be any positive integer. For any set of distributions $\cald$, a reference distribution $D_0$ over $X$ and $\tau > 0$,  the statistical dimension $\RSD_{\kappa_1}^{(k)}(\calB(\calD,D_0), \tau)$ is equal to the smallest $d$ for which there exists a distribution $\calP$ over functions from $X^k$ to $[-1,+1]$ such that for every $D \in \calD$,
$$ \Pr_{\phi \sim \calP}[|D^k[\phi]-D_0^k[\phi]| > \tau] \geq \frac{1}{d}.$$
\end{lem}

We can now state the relationship between $\RSD_{\kappa_1}^{(k)}$ and $\RSD_{\kappa_1}^{(1)}$ for any $\gamma$-flat $\D$.
\begin{lem}\label{lem:k-wise-flat-decision}
Let $\gamma \geq 1$, $\tau > 0$ and $k \in \mathbb{N}$. Let $X$ be a domain, $\calD$ be a $\gamma$-flat class of distributions over $X$ and $D_0$ be any distribution over $X$. Then
$$\RSD_{\kappa_1}^{(1)}(\calB(\calD,D_0),\tau/(2k))  \leq \frac{4k \cdot \gamma^{k-1}}{\tau} \cdot \RSD_{\kappa_1}^{(k)}(\calB(\calD,D_0),\tau).$$
\end{lem}
\begin{proof}
Let $d \doteq \RSD_{\kappa_1}^{(k)}(\calB(\calD,D_0),\tau)$. Fact~\ref{fa:rcvr} implies the existence of a distribution $\calP$ over $k$-wise functions such that for every $D \in \calD$,
$$\Pr_{\phi \sim \calP}[|D^k[\phi]-D_0^k[\phi]| > \tau] \geq \frac{1}{d}.$$
We now fix $D$ and let $\phi$ be such that $|D^k[\phi]-D_0^k[\phi]| > \tau$. 

By the standard hybrid argument,  
\begin{equation}\label{eq:good_j}
\E_{j \sim [k]} \left[\left|D^{j} D_0^{k-j}[\phi]-D^{j-1}D_0^{k-j+1}[\phi]\right|\right] > \frac{\tau}{k} ,
\end{equation}
where $j \sim [k]$ denotes a random and uniform choice of $j$ from $[k]$.
This implies that
\begin{equation*}\label{eq:pull_out}
\E_{j \sim [k]} \Ex_{x_{< j} \sim D^{j-1}} \Ex_{x_{> j} \sim D_0^{k-j}} \bigg[\bigg|D[\phi(x_{<j}, \cdot, x_{> j})] - D_0[\phi(x_{<j}, \cdot, x_{> j})]\bigg|\bigg] > \frac{\tau}{k}.
\end{equation*}
By an averaging argument (and using the fact that $\phi$ takes values between $-1$ and $+1$), we get that with probability at least $\tau/(4 \cdot k)$ over the choice of $j\sim [k]$, $x_{< j} \sim D^{j-1}$ and $x_{> j} \sim D_0^{k-j}$, we have that
\begin{equation*}\label{eq:after_whp_switch}
\bigg|D[\phi(x_{<j}, \cdot, x_{> j})] - D_0[\phi(x_{<j}, \cdot, x_{> j})]\bigg| > \frac{\tau}{2 \cdot k}.
\end{equation*}

Since $\calD$ is a $\gamma$-flat class of distributions, there exists a (fixed) distribution $\bar{D}$ over $X$ such that for every measurable event $E \subset X$, $\Pr_{x\sim D}[x \in E] \le \gamma \cdot \Pr_{x\sim \bar{D}}[x \in E]$. Thus, we can replace the unknown input distribution $D$ by the distribution $\bar{D}$ and get that, with probability at least $\tau/(4 \cdot k \cdot \gamma^{k-1})$ over the choice of $j\sim [k]$, $x_{< j} \sim \bar{D}^{j-1}$ and $x_{> j} \sim D_0^{k-j}$, we have
\begin{equation}\label{eq:after_flat}
\bigg|D[\phi(x_{<j}, \cdot, x_{> j})] - D_0[\phi(x_{<j}, \cdot, x_{> j})]\bigg| > \frac{\tau}{2 \cdot k}.
\end{equation}
We now consider the following distribution $\mathcal{P'}$ over unary SQ functions (i.e., over $[-1,+1]^X$): Independently sample $\phi$ from $\calP$, $j$ uniformly from $[k]$, $x_{< j} \sim \bar{D}^{j-1}$ and $x_{> j} \sim D_0^{k-j}$, and output the (unary) function $\phi'(x) = \phi(x_{<j}, x, x_{> j})$. Then, for every $D\in \D$, we have that with probability at least $\frac{1}{d }\cdot \frac{\tau}{4k} \cdot \frac{1}{\gamma^{k-1}}$ over the choice of $\phi'$ from $\calP'$, we have that $|D[\phi'] - D_0[\phi']| > \tau/(2 \cdot k)$. Thus, by Fact~\ref{fa:rcvr} $$\RSD_{\kappa_1}^{(1)}\left(\calB(\calD,D_0),\frac{\tau}{2 \cdot k}\right) \le \frac{4 d \cdot \gamma^{k-1} \cdot k}{\tau}.$$
\end{proof}

Lemma \ref{lem:k-wise-flat-decision} together with the characterization in Theorem \ref{thm:random-algorithm2queries} imply the following upper bound on the SQ complexity of a decision problem in terms of its $k$-wise SQ complexity.
\begin{theorem}\label{thm:flat-decision-reduction}
Let $\gamma \geq 1$, $\tau > 0$ and $k \in \mathbb{N}$. Let $X$ be a domain, $\calD$ be a $\gamma$-flat class of distributions over $X$ and $D_0$ be any distribution over $X$. If there exists an algorithm that, with probability at least $2/3$ solves $\calB(\D,D_0)$ using $t$ queries to $\STAT^{(k)}_D(\tau)$, then for every $\delta>0$, there exists an algorithm that, with probability at least $1-\delta$ solves $\calB(\D,D_0)$ using $t \cdot 12k \cdot \gamma^{k-1} \cdot \ln(1/\delta) /\tau$ queries to $\STAT^{(1)}_D(\tau/(4k))$.
\end{theorem}

\subsection{General problems}
We now define the general class of problems over sets of distributions and a notion of statistical dimension for these types of problems.
\begin{defn}[Search problems]
A search problem $\calZ$ over a class $\calD$ of distributions and a set $\calF$ of solutions is a mapping $\calZ: \calD \to 2^{\calF} \setminus \{\emptyset\}$, where $2^{\calF}$ denotes the set of all subsets of $\calF$. Specifically, for every distribution $D \in \calD$, $\calZ(D) \subseteq \calF$ is the (non-empty) set of valid solutions for $D$. For a solution $f \in \calF$, we denote by $\calZ_f$ the set of all distributions for which $f$ is a valid solution.
\end{defn}

\begin{defn}[Statistical dimension for search problems \cite{Feldman:16sqd}]\label{def:search_SD}
For $\tau > 0$, $k \in \mathbb{N}$, a domain $X$ and a search problem $\calZ$ over a class of distributions $\calD$ over $X$ and a set of solutions $\calF$, we define the \emph{$k$-wise statistical dimension} with $\kappa_1$-discrimination $\tau$ of $\calZ$ as
\begin{equation*}
\SD^{(k)}_{\kappa_1}(\calZ,\tau) \doteq \sup_{D_0 \in S^X} \inf_{f \in \calF} \RSD^{(k)}_{\kappa_1}(\calB(\calD \setminus \calZ_f, D_0), \tau),
\end{equation*}
where $S^X$ denotes the set of all probability distributions over $X$.
\end{defn}

Lemma~\ref{lem:search_lb} lower-bounds the deterministic $k$-wise SQ complexity of a search problem in terms of its ($k$-wise) statistical dimension.
\begin{theorem}[\cite{Feldman:16sqd}]\label{lem:search_lb}
Let $\calZ$ be a search problem, $\tau > 0$ and $k \in \mathbb{N}$. The deterministic $k$-wise SQ complexity of solving $\calZ$ with access to $\STAT^{(k)}(\tau)$ is at least $\SD^{(k)}_{\kappa_1}(\calZ,\tau)$.
\end{theorem}

The following theorem from \cite{Feldman:16sqd} gives an upper bound on the SQ complexity of a search problem in terms of its statistical dimension. It relies on the multiplicative weights update method to reconstruct the unknown distribution sufficiently well for solving the problem. The use of this algorithm introduces dependence on $\KL$-radius of $\D$. Namely, we define
$$\KLR(\D) \doteq \inf_{\bar D \in S^X } \sup_{D\in \D} \KL(D\|\bar D), $$
where $\KL(\cdot \| \cdot)$ denotes the KL-divergence.
\begin{theorem}
[\cite{Feldman:16sqd}]\label{lem:search_ub}
Let $\calZ$ be a search problem, $\tau, \delta > 0$ and $k \in \mathbb{N}$. There is a randomized $k$-wise SQ algorithm that solves $\calZ$ with success probability $1-\delta$ using
$$ O\bigg(\SD^{(k)}_{\kappa_1}(\calZ,\tau) \cdot \frac{\KLR(\calD)}{\tau^2} \cdot \log\bigg( \frac{\KLR(\calD)}{\tau \cdot \delta}\bigg) \bigg)$$
queries to $\STAT^{(k)}(\tau/3)$.
\end{theorem}

Note that $\KL$-divergence between two distributions is upper-bounded (and is usually much smaller) than the max-divergence we used in the definition of $\gamma$-flatness. Specifically, if $\D$ is $\gamma$-flat  then $\KLR(\D) \leq \ln \gamma$.
We are now ready to prove Theorem~\ref{thm:flat} which we restate here for convenience.
\begin{reptheorem}{thm:flat}[restated]
Let $\gamma \geq 1$, $\tau > 0$ and $k$ be any positive integer. Let $X$ be a domain and $\calD$ be a $\gamma$-flat class of distributions over $X$. There exists a randomized algorithm that given any $\delta > 0$ and a $k$-ary function $\phi: X^k \to [-1,1]$, estimates $D^k[\phi]$ within $\tau$  for every (unknown) $D \in \calD$ with success probability at least $1-\delta$ using $$\tilde{O}\bigg( \frac{\gamma^{k-1} \cdot k^3}{\tau^3} \cdot \log (1/\delta)\bigg)$$
queries to $\STAT_D^{(1)}(\tau/(6 \cdot k))$.\end{reptheorem}
\begin{proof}
We first observe that the task of estimating $D^k[\phi]$ up to additive $\tau$ can be viewed as a search problem $\calZ$ over the set $\calD$ of distributions and over the class $\calF$ of solutions that corresponds to the interval $[-1,+1]$. Next, observe that one can easily estimate $D^k[\phi]$ up to additive $\tau$ using a single query to $\STAT^{(k)}_D(\tau)$. Lemma~\ref{lem:search_lb} implies that $\SD^{(k)}_{\kappa_1}(\calZ,\tau) = 1$. By Definition~\ref{def:search_SD}, for every $D_1 \in S^X$, there exists $f \in \calF$, such that $\RSD^{(k)}_{\kappa_1}(\calB(\calD \setminus \calZ_f, D_1), \tau) = 1$. By Lemma \ref{lem:k-wise-flat-decision},
 $$\RSD_{\kappa_1}^{(1)}\left(\calB(\calD \setminus \calZ_f, D_1),\frac{\tau}{2 \cdot k}\right) \le \frac{4 \cdot \gamma^{k-1} \cdot k}{\tau}.$$

Thus, Fact~\ref{fa:rcvr} and Definition~\ref{def:search_SD} imply that
$$ \SD_{\kappa_1}^{(1)}(\calZ,\frac{\tau}{2 \cdot k}) \le \frac{4 \cdot \gamma^{k-1} \cdot k}{\tau}.$$
Applying Lemma~\ref{lem:search_ub}, we conclude that there exists a randomized unary SQ algorithm that solves $\calZ$ with probability at least $1-\delta$ using at most $$O\bigg(\gamma^{k-1} \cdot k^3 \cdot \frac{\KLR(\mathcal{D})}{\tau^3} \cdot \log\bigg( \frac{k \cdot \KLR(\calD)}{\tau \cdot \delta}\bigg)\bigg)$$
queries to $\STAT^{(1)}(\tau/(6 \cdot k))$. This -- along with the fact that $\KLR(\calD) \le \ln(\gamma)$ whenever $\calD$ is a $\gamma$-flat set of distributions -- concludes the proof of Theorem~\ref{thm:flat}.
\end{proof}

\paragraph{Other divergences:} While the max-divergence that we used for measuring flatness suffices for the applications we give in this paper (and is relatively simple), it might be too conservative in other problems. For example, such divergence is infinite even for two Gaussian distributions with the same standard deviation but different means. A simple way to obtain a more robust version of our reduction is to use approximate max-divergence. For $\delta \in [0,1)$ it is defined as: $$\Div_\infty^\delta(D\|\bar D) \doteq \ln \sup_{E \subseteq X} \frac{\Pr_{x\sim D}[x\in E]-\delta}{\Pr_{x\sim \bar D}[x\in E]} .$$ Note that $\Div_\infty^0(D\|\bar D) = \Div_\infty(D\|\bar D)$. Similarly, we can define a radius of $\D$ in this divergence $$R_\infty^\delta(\D) \doteq \inf_{\bar D \in S^X} \sup_{D\in \D} \Div_\infty^\delta(D\|\bar D) .$$

Now, it is easy to see that, if $\Div_\infty^\delta(D\|\bar D) \leq r$ then $\Div_\infty^{k\delta}(D^k\|\bar D^k) \leq kr$. This means that if in the proof of Lemma \ref{lem:k-wise-flat-decision} we use the condition
$R_\infty^{\tau/(8k^2)}(\D) \leq \ln \gamma$ instead of $\gamma$-flatness then we will obtain that the event in Equation \eqref{eq:after_flat} holds with probability at least $$ \left( \frac{\tau}{4k} - (k-1) \cdot  \frac{\tau}{8k^2}\right) / \gamma^{k-1} \geq \frac{\tau}{\gamma^{k-1} \cdot 8k}$$ over the same random choices.

This implies the following generalization of Theorem \ref{thm:flat}.
\begin{theorem}\label{thm:flat-approx}
Let $\tau > 0$ and $k$ be any positive integer. Let $\calD$ be a class of distributions over a domain $X$ and $\gamma = \exp(R_\infty^{\tau/(8k^2)}(\D))$. There exists a randomized algorithm that given any $\delta > 0$ and a $k$-ary function $\phi: X^k \to [-1,1]$, estimates $D^k[\phi]$ within $\tau$  for every (unknown) $D \in \calD$ with success probability at least $1-\delta$ using $$\tilde{O}\bigg( \frac{\gamma^{k-1} \cdot k^3 \cdot \KLR(\calD)}{\tau^3} \cdot \log (1/\delta)\bigg)$$
queries to $\STAT_D^{(1)}(\tau/(6 \cdot k))$.\end{theorem}

An alternative approach is to use Renyi divergence of order $\alpha > 1$ defined as follows:
 $$\Div_\alpha(D\|\bar D) \doteq \frac{1}{1-\alpha} \cdot \ln \left (\E_{y \sim D} \left[\left(\frac{\Pr_{x\sim D}[x=y]}{\Pr_{x\sim \bar D}[x=y]}\right)^{\alpha-1}\right] \right).$$ The corresponding radius is defined as $$R_\alpha(\D) \doteq \inf_{\bar D \in S^X} \sup_{D\in \D} \Div_\alpha(D\|\bar D) .$$

To use it in our application we need the standard property of the Renyi divergence for product distributions $\Div_\alpha(D^k\|\bar D^k) = k \cdot \Div_\alpha(D\|\bar D)$ and also the following simple lemma from \cite[Lemma 1]{MansourMR09}:
\begin{lem}
For $\alpha > 1$, any two distributions $D,\bar D$ over $X$ and an event $E\subseteq X$:
$$\Pr_{x\sim D}[x\in E] \leq \left(\exp(\Div_\alpha(D\|\bar D) ) \cdot \Pr_{x\sim \bar D}[x\in E]\right)^{\frac{\alpha-1}{\alpha}}. $$
\end{lem}
We will need the inverted version of this lemma:
$$\Pr_{x\sim \bar D}[x\in E] \geq \frac{\left(\Pr_{x\sim D}[x\in E]\right)^{\frac{\alpha}{\alpha-1}}}{\exp(\Div_\alpha(D\|\bar D) )}. $$
Applying this in the proof of Lemma \ref{lem:k-wise-flat-decision} for $\gamma = \exp(R_\alpha(\D))$, we obtain that the event in Equation \eqref{eq:after_flat} holds with probability at least $$ \left(\frac{\tau}{4k}\right)^{\frac{\alpha}{\alpha-1}}/ \gamma^{k-1}. $$
This gives the following generalization of Theorem \ref{thm:flat}.
\begin{theorem}\label{thm:flat-approx}
Let $\tau > 0,\alpha >1$ and $k$ be any positive integer. Let $\calD$ be a class of distributions over a domain $X$ and $\gamma = \exp(R_\alpha(\D))$. There exists a randomized algorithm that given any $\delta > 0$ and a $k$-ary function $\phi: X^k \to [-1,1]$, estimates $D^k[\phi]$ within $\tau$  for every (unknown) $D \in \calD$ with success probability at least $1-\delta$ using $$\tilde{O}\bigg( \gamma^{k-1} \cdot \left( \frac{k}{\tau}\right)^{2+\frac{\alpha}{\alpha-1}} \cdot \log (1/\delta)\bigg)$$
queries to $\STAT_D^{(1)}(\tau/(6 \cdot k))$.\end{theorem}



\input{lower-bounds} 