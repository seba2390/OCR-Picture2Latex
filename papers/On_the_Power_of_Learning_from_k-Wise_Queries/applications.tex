\section{Corollaries for other models}
\label{sec:apps}

\subsection{$k$-local differential privacy}\label{subsec:local_DP}
We start by formally defining the $k$-wise version of the \emph{local differentially privacy} model from \cite{kasiviswanathan2011can}.

\begin{defn}[$k$-local randomizer]
A $k$-local $\eps$-differentially private (DP) randomizer is a randomized map $R: X^k \to W$ such that for all $u, u' \in X^k$ and all $w \in W$, we have that $\Pr[R(u) = w] \le e^{\epsilon} \cdot \Pr[R(u') = w]$ where the probabilities are taken over the coins of $R$.
\end{defn}

The following definition gives a $k$-wise generalization of the local randomizer (LR) oracle which was used in \cite{kasiviswanathan2011can}.

\begin{defn}[$k$-local Randomizer Oracle]
Let $z = (z_1,\dots,z_n) \in X^n$ be a database. A $k$-LR oracle $\LR_z(\cdot,\cdot)$ gets a $k$-tuple of indices $\bar i \in [n]^k$ and a $k$-local $\eps$-DP randomizer as inputs, and outputs an element $w \in W$ which is sampled from the distribution $R(z_{i_1},\ldots,z_{i_k})$.
\end{defn}

We are now ready to give the definition of $k$-local differential privacy.
\begin{defn}[$k$-local differentially private algorithm]
A $k$-local $\epsilon$-differentially private algorithm is an algorithm that accesses a database $z \in X^n$ via a $k$-LR oracle $\LR_z$ with the restriction that for all $i \in [n]$, if $\LR_z(\bar i_1,R_1), \dots, \LR_z(\bar i_t,R_t)$ are the algorithm's invocations of $\LR_z$ on $k$-tuples of indices that include index $i$, where for each $j \in [t]$ $R_j$ is a $k$-local $\epsilon_j$-DP randomizer, then $\epsilon_1 + \dots + \epsilon_t \le \epsilon$.
\end{defn}

The following two theorems -- which follow from Theorem 5.7 and Lemma 5.8 of \cite{kasiviswanathan2011can} -- show that $k$-local differentially private algorithms are equivalent (up to polynomial factors) to $k$-wise statistical query algorithms.

\begin{theorem}\label{thm:local_DP_first}
Let $\calA_{SQ}$ be a $k$-wise SQ algorithm that makes at most $t$ queries to $\STAT^{(k)}_D(\tau)$. Then, for every $\beta>0$, there exists a $k$-local $\epsilon$-DP algorithm $\calA_{DP}$ such that if the database $z$ has $n \geq n_0=O(k \cdot t \cdot \log(t/\beta)/(\epsilon^2 \cdot \tau^2))$ entries sampled i.i.d.~from the distribution $D$, then $\calA_{DP}$ makes $n_0/k$ queries and the total variation between $\calA_{DP}$'s and $\calA_{SQ}$'s output distributions is at most $\beta$.
\end{theorem}

\begin{theorem}\label{thm:local_DP_sec}
Let $z \in X^n$ be a database with entries drawn i.i.d.~from a distribution $D$. For every $k$-local  $\epsilon$-DP algorithm $\calA_{DP}$ making $t$ queries to $\LR_z$ and $\beta >0$, there exists a $k$-wise statistical query algorithm $\calA_{SQ}$ that in expectation makes $O(t \cdot e^{\epsilon})$ queries to $\STAT^{(k)}_D(\tau)$ for $\tau = \Theta(\beta/(e^{2\epsilon} \cdot t))$ such that the total variation between $\calA_{SQ}$'s and $\calA_{DP}$'s output distributions is at most $\beta$.
\end{theorem}

By combining Theorem~\ref{thm:k_wise_sep}, Theorem~\ref{thm:local_DP_first} and Theorem~\ref{thm:local_DP_sec} we then obtain the following corollary.
\begin{corollary}
\label{cor:local_DP}
For every positive integer $k$ and any prime number $p$, there is a concept class $\calC$ of Boolean functions defined over a domain of size $p^{k+1}$ for which there exists a $(k+1)$-local 1-DP distribution-independent PAC learning algorithm using a database consisting of $\widetilde{O}_k(\log{p})$ i.i.d.~samples, whereas any $k$-local 1-DP distribution-independent PAC learning algorithm requires $\Omega_k(p^{1/4})$ samples.
\end{corollary}

The reduction in Theorem \ref{thm:flat} then implies that for $\gamma$-flat classes of distributions a $k$-local DP algorithm can be simulated by a $1$-local DP algorithm with an overhead that is linear in $\gamma^{k-1}$ and polynomial in other parameters.
\begin{theorem}\label{thm:flat-dp}
Let $\gamma \geq 1$,  $k$ be any positive integer. Let $X$ be a domain and $\calD$ a $\gamma$-flat class of distributions over $X$. Let $z \in X^n$ be a database with entries drawn i.i.d. from a distribution $D \in \calD$.  For every $k$-local $\epsilon$-DP algorithm $\calA$ making $t$ queries to a $k$-LR oracle $\LR_z$ and $\beta>0$, there exists a $1$-local $\eps$-DP algorithm $\calB$ such that if $n \geq n_0=
%\gamma^{k-1} \cdot \poly\left(\frac{t \cdot k \cdot e^\eps}{\beta \cdot \eps}\right)$
\tilde{O}\bigg(\frac{\gamma^{k-1}\cdot t^6 \cdot k^6\cdot e^{11\eps} }{\beta^3\eps^2} \bigg)$
%\tilde O(k \cdot t \cdot \log(t/\beta) \cdot b^2/(\epsilon^2 \cdot \tau^2))$
then  for every $D\in \D$, $\calB$ makes $n_0/k$ queries to $1$-LR oracle $\LR'_z$ and the total variation distance between $\calB$'s and $\calA$'s output distributions is at most $\beta$.
\end{theorem}
The reduction from Theorem \ref{thm:sq_and_cc} can be translated to this model analogously.

\subsection{$k$-wise $b$-bit sampling model}\label{subsec:RFA}
For an integer $b>0$, a $b$-bit sampling oracle $\COMM_D(b)$ is defined as follows: Given any function $\phi: X \to \zo^b$, $\COMM_D(b)$  returns $\phi(x)$ for $x$ drawn randomly and independently from $D$, where $D$ is the unknown input distribution. This oracle was first studied by Ben-David and Dichterman \cite{Ben-DavidD98} as a {\em weak Restricted Focus of Attention} model. They showed that algorithms in this model can be simulated efficiently using statistical queries and vice versa. Lower bounds against algorithms that use such an oracle have been studied in \cite{FeldmanGRVX:12,FeldmanPV:13}. More recently, motivated by communication constraints in distributed systems, the sample complexity of several basic problems in statistical estimation has been studied in this and related models \cite{ZhangDJW13,SteinhardtD15,SteinhardtVW16}. These works also study the natural $k$-wise generalization of this model. Specifically, $\COMM^{(k)}_D(b)$ is the oracle that given any function $\phi: X^k \to \zo^b$, returns $\phi(x)$ for $x$ drawn randomly and independently from $D^k$.

The following two theorems -- which follow from Theorem 5.2 in \cite{Ben-DavidD98} and Proposition 3 in \cite{SteinhardtVW16} (that strengthens a similar result in \cite{Ben-DavidD98}) -- show that $k$-wise algorithms in the $b$-bit sampling model are equivalent (up to polynomial and $2^b$ factors) to $k$-wise statistical query algorithms.

\begin{theorem}\label{thm:RFA_first}
Let $\calA_{SQ}$ be a $k$-wise SQ algorithm that makes at most $t$ Boolean queries to $\STAT^{(k)}_D(\tau)$. Then, for every $\beta>0$, there exists a $k$-wise $1$-bit sampling algorithm $\calA_{1\text{-bit}}$ that uses $O(\frac{t}{\tau^2} \cdot \log(t/\beta))$ queries to  $\COMM^{(k)}_D(b)$ and the total variation distance between $\calA_{SQ}$'s and $\calA_{1\text{-bit}}$'s output distributions is at most $\beta$.
\end{theorem}

\begin{theorem}\label{thm:RFA_sec}
Let $\calA_{b \text{-bit}}$ be a $k$-wise $b$-bit sampling algorithm that makes at most $t$ queries to $\COMM^{(k)}_D(b)$. Then, for every $\beta>0$, there exists a $k$-wise SQ algorithm $\calA_{SQ}$ that makes $2bt$ queries to $\STAT^{(k)}_D(\beta/(2^{b+1} t))$ and the total variation distance between $\calA_{SQ}$'s and $\calA_{b\text{-bit}}$'s output distributions is at most $\beta$.
\end{theorem}

Feldman \etal \cite{FeldmanGRVX:12} give a tighter correspondence between the $\COMM$ oracle and the slightly stronger $\VSTAT$ oracle. Their simulations can be extended to the $k$-wise case in a similar way.% instead of $\STAT$. This simulation was extended to $\COMM_D(b)$ in \cite{FeldmanPV:13} at the expense of factor $2^b$ blow-up in the SQ complexity.


The following corollary now follows by combining Theorem~\ref{thm:k_wise_sep}, Theorem~\ref{thm:RFA_first} and Theorem~\ref{thm:RFA_sec}.

\begin{corollary}\label{cor:RFA-separation}
Let $b = O(1)$. For every positive integer $k$ and any prime number $p$, there is a concept class $\calC$ of Boolean functions defined over a domain of size $p^{k+1}$ for which there exists a $(k+1)$-wise $b$-bit sampling distribution-independent PAC learning algorithm making $\widetilde{O}_k(\log{p})$ queries, whereas any $k$-wise $b$-bit sampling distribution-independent PAC learning algorithm requires $\widetilde{\Omega}_k(p^{1/{12}})$ queries.
\end{corollary}

The reduction in Theorem \ref{thm:flat} then implies that for $\gamma$-flat classes of distributions a $k$-wise $1$-bit sampling algorithm can be simulated by a $1$-wise $1$-bit sampling algorithm.
\begin{theorem}\label{thm:flat-1-bit}
Let $\gamma \geq 1$,  $k$ be any positive integer. Let $X$ be a domain and $\calD$ a $\gamma$-flat class of distributions over $X$. For every algorithm $\calA$ making $t$ queries to $\COMM^{(k)}_D(1)$ and every $\beta > 0$, there exists a 1-bit sampling algorithm $\calB$ that for every $D\in \D$, uses $\tilde{O}\bigg(\frac{\gamma^{k-1}\cdot t^6 \cdot k^5 }{\beta^3} \bigg)$ queries to $\COMM_D(1)$ and the total variation distance between $\calB$'s and $\calA$'s output distributions is at most $\beta$.
\end{theorem}

