\section{Separation of $(k+1)$-wise from $k$-wise queries}\label{sec:pf_sep}
\newcommand{\ind}{\mathbbm{1}}

We start by describing the concept class $\calC$ that we use to prove Theorem~\ref{thm:k_wise_sep}. Let $\ell$ and $k$ be positive integers with $\ell \geq k+1$. The domain will be $\mathbb{F}_p^{\ell}$. For every $a = (a_1,\dots, a_{\ell}) \in \mathbb{F}_p^{\ell}$, we consider the hyperplane
$$ \Hyp_a \doteq \{ z = (z_1,\dots,z_{\ell}) \in \mathbb{F}_p^{\ell}: z_{\ell} = a_1 z_1 + \dots + a_{\ell-1} z_{\ell-1} + a_{\ell}\}.$$
We then define the Boolean-valued function $f_a: \mathbb{F}_p^{\ell} \to \{\pm 1\}$ to be the indicator function of the subset $\Hyp_a \subseteq \mathbb{F}_p^{\ell}$, i.e., for every $z \in \mathbb{F}_p^{\ell}$,

\[
 f_a(z) = \begin{dcases*}
        +1  & if $z \in \Hyp_a$,\\
        -1 & otherwise.
        \end{dcases*}
\]

Then, we will consider the concept classes $\calC_{\ell} \doteq \{f_a: a \in \mathbb{F}_p^{\ell}\}$. We denote $\calC \doteq \calC_{k+1}$. We start by stating our upper bound on the $(k+1)$-wise SQ complexity of the distribution-independent PAC learning of $\calC_{k+1}$.

\begin{lem}[$(k+1)$-wise upper bound]\label{le:k_ub}
Let $p$ be a prime number and $k$ be a positive integer. There exists a distribution-independent PAC learning algorithm for $\calC_{k+1}$ that makes at most $t \cdot \log(1/\eps) $ queries to $\STAT^{(k+1)}(\eps/t)$, for some $t = O_k(\log{p})$.
\end{lem}

We next state our lower bound on the $k$-wise SQ complexity of the same tasks considered in Lemma~\ref{le:k_ub}.

\begin{lem}[$k$-wise lower bound]\label{le:k_plus_1_lb}
 Let $p$ be a prime number and $\ell$, $k$ be positive integers with $\ell \geq k+1$ and $k = O(p)$. There exists $t =  \Omega\big(p^{(\ell-k)/4}\big)$ such that any distribution-independent PAC learning alogrithm for $\calC_{\ell}$ with error at most $1/2-2/t$ that is given access to $\STAT^{(k)}(1/t)$ needs at least $t$ queries.
\end{lem}

Note that  Lemma~\ref{le:k_ub} and Lemma~\ref{le:k_plus_1_lb} imply Theorem~\ref{thm:k_wise_sep}.

\subsection{Upper bound}

%Note that it is enough to prove Lemma~\ref{le:k_ub} in the case where $\ell = k+1$. This is because in the case of $\ell > k+1$, we can choose a subset $S \subseteq [\ell]$ of coordinates of size $\ell - k - 1$, and consider all $p^{\ell-k-1}$ settings of the variables $\{a_i: i \in S\}$. For each such setting, we can run the learning algorithm for the case where $\ell = k+1$. This results in a list of at most $p^{\ell-k-1}$ candidate hypotheses, one of which is guaranteed to be close to the true concept. To find this good hypothesis, we then estimate the empirical error for each candidate hypothesis using unary SQs, and output a hypothesis with small enough error. Henceforth, we assume that $\ell = k+1$ and prove Lemma~\ref{le:k_ub}.

\paragraph{Notation}
We first introduce some notation that will be useful in the description of our algorithm. For any matrix $M$ with entries in the finite field $\mathbb{F}_p$, we denote by $\rk(M)$ the rank of $M$ over $\mathbb{F}_p$. Let $(a_1,\dots,a_{k+1}) \in \mathbb{F}_p^{k+1}$ be the unknown vector that defines $f_a$ and $P$ be the unknown distribution over tuples $(z_1, \dots, z_{k+1}) \in \mathbb{F}_p^{k+1}$. 

Note that $\Hyp_a$ is an affine subspace of $\mathbb{F}_p^{k+1}$. To simplify our treatment of affine subspaces, we embed the points of $\mathbb{F}_p^{k+1}$ into $\mathbb{F}_p^{k+2}$ by mapping each $z \in \mathbb{F}_p^{k+1}$ to $(z,1)$. This embedding maps every affine subspace $V$ of $\mathbb{F}_p^{k+1}$ to a linear subspace $W$ of $\mathbb{F}_p^{k+2}$, namely the span of the image of $V$ under our embedding. Note that this mapping is one-to-one and allows us to easily recover $V$ from $W$ as $V = \{z \in \mathbb{F}_p^{k+1} \ | \ (z,1) \in W\}$.  Hence given $k+1$ examples
$$\big((z_{1,1}, \dots, z_{1,k+1}),b_1\big),\big((z_{2,1}, \dots, z_{2,k+1}),b_2\big), \dots, \big((z_{k+1,1}, \dots, z_{k+1,k+1}),b_{k+1}\big)$$  we define the matrix:
\begin{equation}\label{eq:Z_mat_def}
Z \doteq
\begin{bmatrix}
z_{1,1}       & z_{1,2} &  \cdot & z_{1,k+1} & 1 \\
z_{2,1}       & z_{2,2} & \cdot & z_{2,k+1} & 1 \\
\cdot       & \cdot & \cdot & \cdot & \cdot \\
\cdot       & \cdot & \cdot & \cdot & \cdot \\
z_{k+1,1}       & z_{k+1,2} & \cdot & z_{k+1,k+1} & 1
\end{bmatrix}.
\end{equation}
For $\ell \in [k+1]$ we also denote by $Z_\ell$ the matrix that consists of the top $\ell$ rows of $Z$.  Further, for a $(k+1)$-wise query function $\phi\big((z_1,b_1),\ldots,(z_{k+1},b_{k+1})  \big)$, we use $Z$ to refer to the matrix obtained from the inputs to the function.

Let $Q$ be the distribution defined by sampling a random example $\big((z_{1}, \dots, z_{k+1}),b\big)$, conditioning on the event that $b=1$ and outputting $(z_{1}, \dots, z_{k+1},1)$. Note that if the examples from which $Z$ is built are positively labeled i.i.d. examples then each row of $Z$ is sampled i.i.d. from $Q$ and hence $Z_\ell$ is distributed according to $Q^\ell$.
%Let $Q$ be the distribution over $\mathbb{F}_p^k$ obtained by sampling $(z_1, \dots, z_{k+1}) \sim P$, conditioning on the event that $z_{k+1} = a_1 z_1 + \dots + a_k z_k +a_{k+1}$, and outputting $(z_1, \dots, z_k)$. Equivalently, we can define the distribution $Q$ as sampling a random example, conditioning on the event that its label is $1$, and restricting to the first $k$ coordinates.  We will
We denote by $1^{k+1}$ the all $+1$'s vector of length $k+1$.

\paragraph{Learning algorithm}
We start by explaining the main ideas behind the algorithm. On a high level, in order to be able to use $(k+1)$-wise SQs to learn the unknown subspace, we need to make sure that there exists an affine subspace that contains most of the probability mass of the positively-labeled points and
that is spanned by $k+1$ random positively-labeled points with noticeable probability. Here, the probability is with respect to the unknown distribution over labeled examples. Thus, for positively labeled tuples $(z_{1,1}, \dots, z_{1,k+1})$, $(z_{2,1}, \dots, z_{2,k+1})$, $\dots$, $(z_{k+1,1}, \dots, z_{k+1,k+1})$, we  consider the $(k+1) \times (k+2)$ matrix $Z$ defined in \Cref{eq:Z_mat_def}. If $W$ is the row-span of $Z$, then the desired (unknown) affine subspace is the set $V$ of all points $(z_1, \dots,z_{k+1})$ such that $(z_1, \dots,z_{k+1}, 1) \in W$.
	
	
	If the (unknown) distribution over labeled examples is such that with noticeable probability, $k+1$ random positively-labeled points form a full-rank linear system (i.e., the matrix $Z$ has full-rank with noticeable probability conditioned on $(b_1,\dots,b_{k+1}) = 1^{k+1}$), we can use $(k+1)$-wise SQs to find, one bit at a time, the $(k+1)$-dimensional row-span $W$ of $Z$, and we can then output the set $V$ of all points $(z_1, \dots,z_{k+1})$ such that $(z_1, \dots,z_{k+1}, 1) \in W$ as the desired affine subspace (below, we refer to this step as the Recovery Procedure).
		
	 We now turn to the (more challenging) case where the system is not full-rank with noticeable probability (i.e., the matrix $Z$ is rank-deficient with high probability conditioned on $(b_1,\dots,b_{k+1}) = 1^{k+1}$). Then, the system has rank at most $i$ with high probability, for some $i < k+1$. There is a large number of possible $i$-dimensional subspaces and therefore it is no longer clear that there exists a single $i$-dimensional subspace that contains most of the mass of the positively-labeled points. However, we demonstrate that for every $i$, if the rank of $Z$ is at most $i$ with sufficiently high probability, then there exists a \emph{fixed} subspace $W$ of dimension at most $i$ that contains a large fraction of the probability under the row-distribution of $Z$ (it turns out that if this subspace has rank equal to $i$, then it should be \emph{unique}). We can then use $(k+1)$-wise SQs to output the affine subspace $V$ consisting of all points $(z_1,\dots,z_{k+1})$ such that $(z_1,\dots,z_{k+1},1) \in W$ (via the Recovery Procedure).


 %To ensure this, we prove (in \Cref{le:ex_sub}) that
%Note that in principle this can be due to the distribution over individual rows of $Z$ having small positive mass on \emph{several} subspaces each of dimension at most $i$. But in order to recover a meaningful subspace using statistical queries, we need the rows of $Z$ to lie inside a \emph{fixed} $i$-dimensional subspace with high probability.
	%Note that such a subspace corresponds to a subset of columns of $Z$ that are linearly independent with high probability. \textcolor{red}{Once we find this subspace}, we can set the redundant coordinates of $(a_1,\dots,a_{k+1})$ to $0$'s and solve for the remaining coordinates using the same ``recovery'' procedure as before: namely, we invert the lower-dimensional full-rank system and output the bits one at a time using $(k+1)$-wise SQs.
	The general description of the algorithm is given in Algorithm~\ref{alg:k_wise_SQ}, and the Recovery Procedure (allowing the reconstruction of the affine subspace $V$) is separately described in \Cref{alg:recovery}. We denote the indicator function of event $E$ by $\ind(E)$. Note that the statistical query corresponding to the event $\ind(E)$ gives an estimate of the probability of $E$.
	%\vnote{Matrix $Z$ is defined only under this condition anyway. There is some confusion here between inputs to a query and (conditioned) random variables used for analysis. It would be more clear to use separate symbols for them.}
\begin{algorithm}[H]
\caption{$(k+1)$-wise SQ Algorithm}
\label{alg:k_wise_SQ}
{\bf Inputs.} $k \in \mathbb{N}$, error probability $\epsilon > 0$.\\
{\bf Output.} Function $f:\mathbb{F}_p^{k+1} \to \{\pm 1 \}$.
\begin{algorithmic}[1]
\State Set tolerance of each SQ to $\tau = (\epsilon/2^{c\cdot(k+2)})^{(k+1)^{k+3}}$, where $c>0$ is a large enough absolute constant.
\State Define the threshold $\tau_i = 2^{c \cdot (k+2-i)} \cdot k \cdot \tau^{1/(k+1)^{k+2-i}}$ for every $i \in [k+1]$.
\State Ask the SQ $\phi(z,b) \doteq \ind(b=1)$ and let $w$ be the response.
  \If{$w \le \epsilon -\tau$}
    \State\label{st:early_term} Output the all $-1$'s function.
  \EndIf
% \State For $(z_1,\dots,z_{k+1}) \in (\mathbb{F}_p^{k+1})^{k+1}$ and $(b_1,\dots,b_{k+1}) \in \{\pm 1\}^{k+1}$, let:
\State Let $\widetilde{\phi}\big((z_1,b_1),\ldots,(z_{k+1},b_{k+1}) \big) \doteq \ind((b_1,\dots,b_{k+1}) = 1^{k+1})$.
\State\label{st:v_resp} Ask the SQ $\widetilde{\phi}$ and let $v$ be the response.
\For{$i = k+1$ down to $1$}
       % \State For $(z_1,\dots,z_{k+1}) \in (\mathbb{F}_p^{k+1})^{k+1}$ and $(b_1,\dots,b_{k+1}) \in \{\pm 1\}^{k+1}$, let:
\State Let $\phi_i\big((z_1,b_1),\ldots,(z_{k+1},b_{k+1})  \big) \doteq \ind((b_1,\dots,b_{k+1}) = 1^{k+1}\mbox{ and }\rk(Z) = i$).
	\State Ask the SQ $\phi_i$ and let $v_i$ be the response.
  \If{$v_i/v \geq \tau_i$}
    %\State\label{st:LI_col} \textcolor{red}{Using $(k+1)$-wise SQs, find subset $S \subseteq [k+1]$ of $i$ linearly independent columns of $Z$.}
    %\State\label{st:lin_comb} Using $(k+1)$-wise SQs, $\forall t \in [k]$, write $z_t$ as a linear comb. of $\{z_s: s \in S \setminus \{k+1\}\} \cup \{1^{k+1}\}$.
    %\State Let $V \subseteq \mathbb{F}_p^{k+1}$ be the resulting subspace.
	\State Run Recovery Algorithm on input $(i,v_i)$ and let $\widehat{V}$ be the subspace of $\mathbb{F}_p^{k+1}$ it outputs.
	      		\State Define function $f:\mathbb{F}_p^{k+1} \to \{-1,1\}$ by:
	\State $f(z_1,\dots,z_{k+1}) = +1$ if $(z_1,\dots,z_{k+1}) \in \widehat{V}$.
	\State $f(z_1,\dots,z_{k+1}) = -1$ otherwise.
	\State Return $f$.
  \EndIf
      \EndFor

\end{algorithmic}
\end{algorithm}
%\vnote{In this description $Z$ and $(z_{1,k+1}, z_{2,k+1}, \dots, z_{k+1,k+1})$ are inputs to the procedure. They are not and are only defined inside a query function. So the query is defined as: $\phi_{j,d}\big((z_1,b_1),\ldots,(z_{k+1},b_{k+1})  \big)$ is equal to 1 if $(b_1,\dots,b_{k+1}) = 1^{k+1}$  and $\rk(Z) = i$ and bit $j$ of the description of the solution $a$ to the system $Ma=\bar{z}_{k+1}$ is equal to $d$. Here $M$ is the matrix that is formed from $z_1,...,z_{k+1}$ in the same as you do for $Z$ (I do not use the same name to keep the random variable and its distribution separate from specific instantiations).}
%\vnote{Another small comment is that there is no need to separately compute $S$ and $a$. There is a unique subspace and its entire description is short.}
\begin{algorithm}[H]
	\caption{Recovery Procedure}
	\label{alg:recovery}
	{\bf Input.} Integer $i \in [k+1]$.\\
	{\bf Output.} Subspace $\widehat{V}$ of $\mathbb{F}_p^{k+1}$ of dimension $i$.
	\begin{algorithmic}[1]
		%\State \textcolor{red}{Let $i = |S|$.}
		 %\State\label{st:solve} \textcolor{red}{Using $i$-wise SQs, solve the system $\{z_{k+1,\ell} = \sum_{v \in S \cap [k]} a_v z_{v,\ell} + a_{k+1}: \ell \in [i]\}$ for $a \in \mathbb{F}_p^i$.}
         \State Let $m_i = (k+2) \cdot i \cdot \lceil \log p \rceil$
		 \For{each bit $j \leq m_i$}% in the binary representation of output subspace $W$}
		 \State Define event $E_j(Z) = \ind(\text{bit } j \text{ of row span of } Z \text{ is } 1)$.
		 \State Let $\phi_{i,j} \big((z_1,b_1),\ldots,(z_{k+1},b_{k+1})  \big) \doteq \ind( E_j(Z) \mbox{ and } (b_1,\dots,b_{k+1}) = 1^{k+1}\mbox{ and }\rk(Z) = i$).
		 \State Ask the SQ $\phi_{i,j}$ and let $u_{i,j}$ be the response.
		
		    \If{$u_{i,j}/v_i \geq (9/10)$}
		    \State Set bit $j$ in binary representation of $\widehat{W}$ to $1$.
		    \Else
		    \State Set bit $j$ in binary representation of $\widehat{W}$ to $0$.
		    \EndIf
		  \EndFor
		  \State Let $\widehat{V}$ be the set all points $(z_1,\dots,z_{k+1})$ such that $(z_1,\dots,z_{k+1},1) \in \widehat{W}$.
	\end{algorithmic}
\end{algorithm}


%\texttt{<do stuff>}
%\textcolor{red}{We point out that in Step~\ref{st:LI_col} of Algorithm~\ref{alg:k_wise_SQ}, the subset $S$ can be taken to be the first subset of $i$ linearly independent columns of $Z$ that includes the last column, in some canonical ordering of the subsets of columns of $Z$. Moreover, Step~\ref{st:LI_col} can be implemented using $i \cdot \lceil \log_2(k+1) \rceil$ many $(k+1)$-wise SQs by asking queries of the form ``$(b_1,\dots,b_{k+1})=1^{k+1}$ and $\rk[Z] = i$ and the $r$th bit of the $m$th element of $S$ is $1$'' (for some $r \in [\lceil \log_2(k+1) \rceil]$ and $m \in [i]$). Furthermore, Step~\ref{st:solve} of \Cref{alg:recovery} (the recovery procedure) can be implemented using $i \cdot \lceil \log_2(p) \rceil$ many \textcolor{red}{$i$-wise} SQs by asking queries of the form ``$(b_1,\dots,b_{k+1})=1^{k+1}$ and $\rk[Z] = i$ and the $r$th bit in the binary expansion of $a_v$ is $1$ where $a$ is the solution to the system  $\{z_{k+1,\ell} = \sum_{v \in S \cap [k]} a_v z_{v,\ell} + a_{k+1}: \ell \in [i]\}$'' (for some $r \in [\lceil \log_2(p) \rceil]$ and $v \in [i]$).}

% Similarly, Step~\ref{st:lin_comb} can be implemented using $k \cdot i \cdot \lceil \log_2(p) \rceil$ many $(k+1)$-wise SQs.

\paragraph{Analysis}
We now turn to the analysis of Algorithm~\ref{alg:k_wise_SQ} and the proof of Lemma~\ref{le:k_ub}. We will need the following lemma, which shows that if the rank of $Z$ is at most $i$ with high probability, then there is a \emph{fixed} subspace of dimension at most $i$ containing most of the probability mass under the row-distribution of $Z$.
%\vnote{It would be useful to explain how this Lemma is related to the high-level description.}
\begin{lem}\label{le:ex_sub}
Let $i \in [k+1]$. If $\Pr_{Q^{k+1}}[\rk(Z) \le i] \geq 1-\xi$, then there exists a subspace $W$ of $\mathbb{F}_p^{k+2}$ of dimension at most $i$ such that $\Pr_{z \sim Q}[z \notin W] \le \xi^{1/k}$.
\end{lem}

\begin{remark}
We point out that the exponential dependence on $1/k$ in the probability upper bound in Lemma~\ref{le:ex_sub} is tight. To see this, let $p = 2$, and $\{e_1, \dots , e_k\}$ be the standard basis in $\mathbb{F}_2^k$. Consider the base distribution $P$ on $\mathbb{F}_2^k$ that puts probability mass $1-\alpha$ on $e_1$, and probability mass $\alpha/(k-1)$ on each of $e_2$, $e_3$, $\dots$, $e_k$. Then, a Chernoff bound implies that if we draw $k$ i.i.d. samples from $P$, then the dimension of their span is at most $2 \cdot \alpha \cdot k$ with probability at least $1 - \exp(-k)$. On the other hand, for any subspace $W$ of $\mathbb{F}_2^k$ of dimension $2 \cdot \alpha \cdot k$, the probability that a random sample from $P$ lies inside $W$ is only $1- \Theta(\alpha)$.
\end{remark}

To prove Lemma~\ref{le:ex_sub}, we will use the following proposition.
\begin{proposition}\label{prop:ind}
Let $\ell \in [k+1]$, $i \in [\ell-1]$ and $\eta >0$. If $\Pr_{Q^{\ell}}[\rk(Z_{\ell}) \le i] \geq 1-\eta$, then for every $\nu \in (0,1]$, either there exists a subspace $W$ of $\mathbb{F}_p^{k+2}$ of dimension $i$ such that $\Pr_{z \sim Q}[z \notin W] \le \nu$ or $\Pr_{Q^i}[\rk(Z_{i}) \le i-1] \geq 1-\eta/\nu$.
\end{proposition}

\begin{proof}
Let $p \doteq \Pr_{Q^i}[\rk(Z_{i}) \le i-1]$. For every (fixed) matrix $A_i \in \mathbb{F}_p^{i \times (k+2)}$, define
$$\mu(A_i) \doteq \Pr_{Q^\ell}[\rk(Z_{\ell}) \le i ~ | ~ Z_{i} = A_i].$$
Then,
\begin{align*}
\Pr_{Q^{\ell}}[\rk(Z_{\ell}) \le i] &= p+(1-p)\cdot \Pr_{Q^{\ell}}[\rk(Z_{\ell}) \le i ~ | ~ \rk(Z_{i}) = i]\\
&= p+(1-p)\cdot \Ex_{ Q^i}\bigg[\mu(Z_i) \bigg| ~ \rk(Z_{i}) = i \bigg].
\end{align*}
%\vnote{I think the last line can be very confusing. What is the internal probability over? I think you could clear it up by first defining $\mu(A) \doteq \Pr_{Q^\ell}[\rk(Z_{\ell}) \le i ~ | ~ Z_{i} = A]$ and then using $\Ex_{ Q^i}\bigg[\mu(Z_i) \bigg| ~ \rk(Z_{i}) = i \bigg]$ (the expectation is just over $Q^i$ since you conditioning on $\rk(Z_{i}) = i$ explicitly). }
Since $\Pr_{Q^{\ell}}[\rk(Z_{\ell}) \le i] \geq 1-\eta$, we have that
$$  \Ex_{ Q^i}\bigg[\mu(Z_i) \bigg| ~ \rk(Z_{i}) = i \bigg] \geq 1 - \eta/(1-p). $$
Hence, there exists a setting $A_i \in \mathbb{F}_p^{i \times (k+2)}$ of $Z_{i}$  such that $\rk(A_{i}) = i$ and
$$\Pr[\rk(Z_{\ell}) \le i ~ | ~ Z_{i} = A_{i}] \geq 1 - \eta/(1-p).$$
We let $W$ be the $\mathbb{F}_p$-span of the rows of $A_{i}$. Note that the dimension of $W$ is equal to $i$ and that $\Pr_{z \sim Q}[z \notin W] \le \eta/(1-p)$. Thus, we conclude that for every $\nu \in (0,1]$, either $p \geq 1-\eta/\nu$ or $\Pr_{z \sim Q}[z \notin W] \le \nu$, as desired.
\end{proof}

We now complete the proof of Lemma~\ref{le:ex_sub}.
\begin{proof}[Proof of Lemma~\ref{le:ex_sub}]
Starting with $\ell = k+1$ and $\eta = \xi$, we inductively apply Proposition~\ref{prop:ind} with $\nu = \xi^{1/k}$ until we either get the desired subspace $W$ or we get to the case where $i=1$. In this case, we have that $\Pr_{Q^{\ell}}[\rk(Z_{\ell}) \le 1] \geq 1-\xi^{1/k}$ for $\ell \geq 2$. Since the last column of $Z_{\ell}$ is the all $1$'s vector, we conclude that there exists $z^* \in \mathbb{F}_p^{k+1}$ such that $\Pr_{z \sim Q}[z \neq (z^*,1)] \le \xi^{1/k}$. We can then set our subspace $W$ to be the $\mathbb{F}_p$-span of the vector $(z^*,1)$.
\end{proof}

For the proof of Lemma~\ref{le:k_ub} we will also need the following lemma, which states sufficient conditions under which the Recovery Procedure (\Cref{alg:recovery}) succeeds.
\begin{lem}\label{le:recovery}
	Let $i \in [k+1]$. Assume that in \Cref{alg:k_wise_SQ}, $v > \epsilon^{k+1}/2$ and $v_i/v \geq \tau_i$. If there exists a subspace $W$ of $\mathbb{F}_p^{k+2}$ of dimension equal to $i$ such that
	\begin{equation}\label{eq:lemma_W_assumption}
	\Pr_{z \sim Q}[z \notin W] < \frac{\tau_i} {4 \cdot (k+1)},
	\end{equation}
	then the affine subspace $\widehat{V}$ output by \Cref{alg:recovery} (i.e., the Recovery Procedure) consists of all points $(z_1,\dots,z_{k+1})$ such that $(z_1,\dots,z_{k+1},1) \in W$.
\end{lem}

We note that \Cref{le:recovery} would still hold under quantitatively weaker assumptions on $v$, $v_i/v$ and $\Pr_{z \sim Q}[z \notin W]$ in \Cref{eq:lemma_W_assumption}. In order to keep the expressions simple, we however choose to state the above version which will be sufficient to prove \Cref{le:k_ub}. The proof of \Cref{le:recovery} appears in \Cref{subsec:pf_rec_lem}. We are now ready to complete the proof of Lemma~\ref{le:k_ub}.

\begin{proof}[Proof of Lemma~\ref{le:k_ub}]
If Algorithm~\ref{alg:k_wise_SQ} terminates at Step~\ref{st:early_term}, then the error of the output hypothesis is at most $\epsilon$, as desired. Henceforth, we assume that Algorithm~\ref{alg:k_wise_SQ} does not terminate at Step~\ref{st:early_term}. Then, we have that $\Pr[b = 1] > \epsilon$, and hence $\Pr[(b_1,\dots,b_{k+1}) = 1^{k+1}] > \epsilon^{k+1}$. Thus, the value $v$ obtained in Step~\ref{st:v_resp} of Algorithm~\ref{alg:k_wise_SQ} satisfies $v > \epsilon^{k+1} - \tau \geq \epsilon^{k+1}/2$, where the last inequality follows from the setting of $\tau$. Let $i^*$ be the first (i.e., largest) value of $i \in  [k+1]$ for which $v_i/v \geq \tau_i$. To prove that such an $i^*$ exists, we proceed by contradiction, and assume that for all $i \in [k+1]$, it is the case that $v_i/v < \tau_i$. Note that $Z$ has an all $1$'s column, so it has rank at least $1$. Moreover, it has rank at most $k+1$. Therefore, we have that
\begin{align*}
1 &= \Pr[1 \le \rk(Z) \le k+1 ~ |~ (b_1,\dots,b_{k+1})=1^{k+1}]\\
&= \displaystyle\sum\limits_{i=1}^{k+1} \Pr[\rk(Z) = i ~ | ~ (b_1,\dots,b_{k+1})=1^{k+1}]\\
&\le \displaystyle\sum\limits_{i=1}^{k+1} \frac{v_i + \tau}{v - \tau}\\
&\le 2 \cdot \displaystyle\sum\limits_{i=1}^{k+1} \frac{v_i + \tau}{v}\\
&\le 2 \cdot \displaystyle\sum\limits_{i=1}^{k+1} (\frac{v_i}{v}  + \frac{2\tau}{\epsilon^{k+1}})\\
&< 2 \cdot \displaystyle\sum\limits_{i=1}^{k+1} \tau_i + 4 \cdot (k+1) \cdot \frac{\tau}{\epsilon^{k+1}}.
\end{align*}
Using the fact that $\tau_i$ is monotonically non-increasing in $i$ and the settings of $\tau_1$ and $\tau$, the last inequality gives
\begin{align*}
1 &\le 2 \cdot (k+1) \cdot \tau_1 + 4 \cdot (k+1) \cdot \frac{\tau}{\epsilon^{k+1}} < 1,
\end{align*}
a contradiction.

We now fix $i^*$ as above. We have that
\begin{align*}
\Pr[\rk(Z) \le i^* ~ | ~ (b_1,\dots,b_{k+1})=1^{k+1}] &= 1 - \displaystyle\sum\limits_{i = i^*+1}^{k+1} \Pr[\rk(Z) = i ~ | ~ (b_1,\dots,b_{k+1})=1^{k+1}]\\
&\geq 1 - \displaystyle\sum\limits_{i = i^*+1}^{k+1} \frac{v_i + \tau}{v-\tau}\\
&\geq 1 - 2 \cdot \displaystyle\sum\limits_{i = i^*+1}^{k+1} (\frac{v_i}{v}  + \frac{2\tau}{\epsilon^{k+1}})\\
&> 1 - 2 \cdot \displaystyle\sum\limits_{i = i^*+1}^{k+1} (\tau_i + 2 \cdot \frac{\tau}{\epsilon^{k+1}})\\
& \geq 1 - 4 \cdot \displaystyle\sum\limits_{i = i^*+1}^{k+1} \tau_i\\
&\geq 1 - 4 \cdot k\cdot \tau_{i^*+1}.
%&= 1 - \textcolor{red}{2^{6\cdot (k-i^*)}\cdot k^2 \cdot  \tau^{1/k^{k-i^*}}}
\end{align*}
By Lemma~\ref{le:ex_sub}, there exists a subspace $W$ of $\mathbb{F}_p^{k+2}$ of dimension at most $i^*$ such that
\begin{equation}\label{eq:notin_W}
\Pr_{z \sim Q}[z \notin W] \le (4 \cdot k)^{1/k} \cdot \tau_{i^*+1}^{1/k}.
\end{equation}

\begin{proposition}\label{prop:failure}
	For every $i \in [k]$, we have that $(k+1) \cdot (4 \cdot k)^{1/k} \cdot \tau_{i+1}^{1/k} \le \tau_{i }/4$.
\end{proposition}
We note that \Cref{prop:failure} follows immediately from the definitions of $\tau_{i}$ and $\tau$ (and by letting $c$ by a sufficiently large positive absolute constant). Moreover, \Cref{prop:failure} (applied with $i = i^*$) along with \Cref{eq:notin_W} imply that $\Pr_{z \sim Q}[z \notin W]$ is at most $\tau_{i*}/(4(k+1))$.

By a union bound, we get that with probability at least
\begin{equation}\label{eq:alg_succ_prob}
1- (k+1) \cdot \Pr_{z \sim Q}[z \notin W] \geq 1 - \frac{\tau_{i^*}}{4},
\end{equation}
all the rows of $Z$ belong to $W$.

Since $v_{i*}/v \geq \tau_{i*}$, we also have that:
\begin{align}\label{eq:cond_lb_i_star}
\Pr[\rk(Z) = i^* ~ | ~ (b_1,\dots,b_{k+1})=1^{k+1}] &\geq \frac{v_{i*}-\tau}{v + \tau}\nonumber\\
&\geq \frac{1}{2} \cdot \frac{(v_{i*}-\tau)}{v}\nonumber\\
&\geq \frac{1}{2} \cdot (\tau_{i^*} - \frac{2 \cdot \tau}{\epsilon^{k+1}})\nonumber\\
& \geq \frac{\tau_{i^*}}{3}
\end{align}
Combining \Cref{eq:alg_succ_prob} and \Cref{eq:cond_lb_i_star}, we get that the rank of $W$ is \emph{equal to} $i^*$.

Let $V$ be the affine subspace consisting of all points $(z_1,\dots,z_{k+1})$ such that $(z_1,\dots,z_{k+1},1) \in W$. By \Cref{le:recovery}, we get that \Cref{alg:recovery} (and hence \Cref{alg:k_wise_SQ}) correctly recovers the affine subspace $V$.

We note that the function $f$ output by Algorithm~\ref{alg:k_wise_SQ} is the $\pm 1$ indicator of a subspace of the true hyperplane $\Hyp_a$. To see this, note that $f$ is the $\pm 1$ indicator function of the subspace $V$, and by Equations~(\ref{eq:notin_W}) and (\ref{eq:cond_lb_i_star}), we have that with probability at least $\tau_{i*}/12$ over $Z \sim Q^{k+1}$, all the columns of $Z$ belong to $W$ and $\rk(Z) = i^*$. Since the dimension of $W$ is equal to $i^*$ and since we are conditioning on $(b_1,\dots,b_{k+1})=1^{k+1}$, this implies that the correct label of all the points in $V$ is $+1$. Hence, $f$ only possibly errs on positively-labeled points (by wrongly giving them the label $-1$). Moreover, Algorithm~\ref{alg:k_wise_SQ} ensures that the output function $f$ gives the label $+1$ to every $(z_1,\dots,z_{k+1}) \in \mathbb{F}_p^{k+1}$ for which $(z_1,\dots,z_{k+1},1) \in W$. Therefore, the function $f$ that is output by Algorithm~\ref{alg:k_wise_SQ} (when it does not terminate at Step~\ref{st:early_term}) has error at most the right hand side of (\ref{eq:notin_W}). So to upper-bound the error probability, it suffices for us to verify that the right-hand side of (\ref{eq:notin_W}) is at most $\epsilon$. This is obtained by applying the next proposition with $i = i^*+1$.
\begin{proposition}\label{prop:tau_i_pow_1_ov_k}
	For every $i \in [k+1]$, we have that $(4 \cdot k)^{1/k} \cdot \tau_{i}^{1/k} \le \epsilon^{k} $.
\end{proposition}

The proof of \Cref{prop:tau_i_pow_1_ov_k} follows immediately from the definitions of $\tau_{i}$ and $\tau$ and by letting $c$ be a sufficiently large positive absolute constant.

The number of queries performed by the $(k+1)$-wise algorithm is at most $O(k^2 \cdot \log{p})$, and their tolerance is $\tau \geq (\epsilon/2^{c\cdot(k+2)})^{(k+1)^{k+3}}$, where $c$ is a positive absolute constant. Finally, we remark that the dependence of the SQ complexity of the above algorithm on the error parameter $\eps$ is $\eps^{-k^{O(k)}}$. It can be improved to a linear dependence on $1/\eps$ by learning with error $1/3$ and then using boosting in the standard way (boosting in the SQ model works essentially as in the regular PAC model \cite{aslam1993general}).
\end{proof}

\subsection{Lower bound}
Our proof of lower bound is a generalization of the lower bound in \cite{Feldman:16sqd} (for $\ell=2$ and $k=1$). It relies on a notion of {\em combined randomized statistical dimension} (``combined" refers to the fact that it examines a single parameter that lower bounds both the number of queries and the inverse of the tolerance).
In order to apply this approach we need to extend it to $k$-wise queries. This extension follows immediately from a simple observation. If we define the domain to be $X' \doteq X^k$ and the input distribution to be $D' \doteq D^k$ then asking a $k$-wise query $\phi:X^k \to [-1,1]$ to $\STAT^{(k)}_D(\tau)$ is equivalent to asking a unary query $\phi: X' \to [-1,1]$ to $\STAT^{(k)}_{D'}(\tau)$. Using this observation we define the $k$-wise versions of the notions from \cite{Feldman:16sqd} and give their properties that are needed for the proof of Lemma~\ref{le:k_plus_1_lb}.

\subsubsection{Preliminaries}
Combined randomized statistical dimension is based on the following notion of average discrimination.
\begin{defn}[$k$-wise average $\kappa_1$-discrimination]\label{def:kappa_1_disc}
Let $k$ be any positive integer. Let $\mu$ be a probability measure over distributions over $X$ and $D_0$ be a reference distribution over $X$. Then,
$$ \bar{\kappa}_1^{(k)}(\mu,D_0) \doteq \sup_{\phi: X^k \to [-1,+1]} \bigg\{ \Ex_{D \sim \mu}[|D^k[\phi]-D_0^k[\phi]|] \bigg\}. $$
\end{defn}

We denote the problem of PAC learning a concept class $\calC$ of Boolean functions up to error $\epsilon$ by $\mathcal{L}_{PAC}(\calC,\epsilon)$. Let $Z$ be the domain of the Boolean functions in $\calC$. For any distribution $D_0$ over labeled examples (i.e., over $Z \times \{\pm 1\}$), we define the Bayes error rate of $D_0$ to be
\begin{equation*}
\err(D_0) = \displaystyle\sum\limits_{z \in Z} \min\{D_0(z,1) , D_0(z,-1)\} = \min_{h: Z \to \{\pm1 \}} \Pr_{(z,b) \sim D_0}[h(z) \neq b].
\end{equation*}

%The following statistical dimension -- defined for decision problems -- will be useful when proving Lemma~\ref{le:k_plus_1_lb}.
\begin{defn}[$k$-wise combined randomized statistical dimension]\label{def:csdr}
Let $k$ be any positive integer. Let $\calD$ be a set of distributions and $D_0$ a reference distribution over $X$. The $k$-wise combined randomized statistical dimension of the decision problem $\calB(\calD,D_0)$ is then defined as
$$ \CSDR_{\bar{\kappa}_1}^{(k)}(\calB(\calD,D_0)) \doteq \sup_{\mu \in S^{\calD}} (\bar{\kappa}_1^{(k)}(\mu,D_0))^{-1}, $$
where $S^\D$ denotes the set of all probability distributions over $\D$.

Further, for any concept class $\calC$ of Boolean functions over a domain $Z$, and for any $\epsilon > 0$, the $k$-wise combined randomized statistical dimension of $\mathcal{L}_{PAC}(\calC,\epsilon)$ is defined as
\begin{equation*}
\CSDR_{\bar{\kappa}_1}^{(k)}(\mathcal{L}_{PAC}(\calC,\epsilon)) \doteq \sup_{D_0 \in S^{Z \times \{\pm 1\}}: \err(D_0) > \epsilon} \CSDR_{\bar{\kappa}_1}^{(k)}(\calB(\calD_{\calC},D_0)),
\end{equation*}
where $\calD_{\calC} \doteq \{ P^f: P \in S^{Z}, f \in \calC\}$ with $P^f$ denoting the distribution on labeled examples $(x,f(x))$ with $x \sim P$.
\end{defn}

The next theorem lower bounds the randomized $k$-wise SQ complexity of PAC learning a concept class in terms of its $k$-wise combined randomized statistical dimension.% (introduced in Definition~\ref{def:csdr}).

\begin{theorem}[\cite{Feldman:16sqd}]\label{thm:sq_RSD}
Let $\calC$ be a concept class of Boolean functions over a domain $Z$, $k$ be a positive integer and $\epsilon, \delta > 0$. Let $d \doteq \CSDR_{\bar{\kappa}_1}^{(k)}(\mathcal{L}_{PAC}(\calC,\epsilon))$. Then, the randomized $k$-wise SQ complexity of solving $\mathcal{L}_{PAC}(\calC,\epsilon - 1/\sqrt{d})$ with access to $\STAT^{(k)}(1/\sqrt{d})$ and success probability $1-\delta$ is at least $(1-\delta) \cdot \sqrt{d} - 1$.
\end{theorem}


To lower bound the statistical dimension we will use the following ``average correlation'' parameter introduced in \cite{FeldmanGRVX:12}.
\begin{defn}[$k$-wise average correlation]\label{def:rho}
Let $k$ be any positive integer. Let $\calD$ be a set of distributions and $D_0$ a reference distribution over $X$. Assume that the support of every distribution $D \in \calD$ is a subset of the support of $D_0$. Then, for every $x \in X^k$, define $\hat{D}(x) \doteq \frac{D^k(x)}{D_0^k(x)} - 1$. Then, the $k$-wise average correlation is defined as
$$ \rho^{(k)}(\calD,D_0) \doteq \frac{1}{|\calD|^2} \cdot \displaystyle\sum\limits_{D, D' \in \calD} | D_0^k[\hat{D} \cdot \hat{D}']|. $$
\end{defn}

Lemma~\ref{lem:ub_rho} relates the average correlation to the average discrimination (from Definition~\ref{def:kappa_1_disc}).
\begin{lem}[\cite{Feldman:16sqd}]\label{lem:ub_rho}
Let $k$ be any positive integer. Let $\calD$ be a set of distributions and $D_0$ a reference distribution over $X$. Let $\mu$ be the uniform distribution over $\calD$. Then,
$$ \bar{\kappa}_1^{(k)}(\mu,D_0) \le 4 \cdot \sqrt{\rho^{(k)}(\calD,D_0)}. $$
\end{lem}



\subsubsection{Proof of Lemma~\ref{le:k_plus_1_lb}}\label{subsec:sep_lb}

Denote $X \doteq \mathbb{F}_p^{\ell} \times \{\pm 1\}$. Let $\calD$ be the set of all distributions over $X^k$ that are obtained by sampling from any given distribution over $(\mathbb{F}_p^{\ell})^k$ and labeling the $k$ samples according to any given hyperplane indicator function $f_a$. Let $D_0$ be the uniform distribution over $X^k$. We now show that $\CSDR_{\bar{\kappa}_1}(\calB(\calD,D_0)) = \Omega\big(p^{(\ell-k)/2}\big)$. By definition,
$$ \CSDR_{\bar{\kappa}_1}(\calB(\calD,D_0)) \doteq \sup_{\mu \in S^{\calD}} (\bar{\kappa}_1(\mu,D_0))^{-1}. $$
We now choose the distribution $\mu$. For $a \in \mathbb{F}_p^{\ell}$, we define $P_a$ to be the distribution over $\mathbb{F}_p^{\ell}$ that has density $\alpha = 1/(2 (p^{\ell}-p^{\ell-1}))$ on each of the $p^{\ell}-p^{\ell-1}$ points outside $\Hyp_a$, and density $\beta = 1/p^{\ell-1}-\alpha p +\alpha = 1/(2p^{\ell-1})$ on each of the $p^{\ell-1}$ points inside $\Hyp_a$. We then define $D_a$ to be the distribution obtained by sampling $k$ i.i.d.~random examples of $\Hyp_a$, the marginal of each over $\mathbb{F}_p^{\ell}$ being $P_a$. Let $\calD' \doteq \{D_a ~ | ~ a \in \mathbb{F}_p^{\ell}\}$, and let $\mu$ be the uniform distribution over $\calD'$. By Lemma~\ref{lem:ub_rho}, we have that $\bar{\kappa}_1(\mu,D_0) \le 4 \cdot \sqrt{\rho(\calD,D_0)}$, so it is enough to upper bound $\rho(\calD,D_0)$.

We first note that for $a, a' \in \mathbb{F}_p^{\ell}$, we have
\begin{align*}
D_0[\hat{D}_a \cdot \hat{D}_{a'}] &= \Ex_{(z,b) \sim D_0} [ \hat{D}_a(z,b) \cdot \hat{D}_{a'}(z,b)]\\
&= \Ex_{(z,b) \sim D_0} \bigg[ \bigg(\frac{D_a(z,b)}{D_0(z,b)}-1 \bigg) \cdot \bigg(\frac{D_{a'}(z,b)}{D_0(z,b)}-1\bigg)\bigg]\\
&= \Ex_{(z,b) \sim D_0} \bigg[ \frac{D_a(z,b) \cdot D_{a'}(z,b)}{D_0^2(z,b)} - \frac{D_a(z,b)}{D_0(z,b)} - \frac{D_{a'}(z,b)}{D_0(z,b)} +1\bigg]\\
&= \Ex_{(z,b) \sim D_0} \bigg[ \frac{D_a(z,b) \cdot D_{a'}(z,b)}{D_0^2(z,b)}\bigg] - 2 \cdot \Ex_{(z,b) \sim D_0} \bigg[\frac{D_a(z,b)}{D_0(z,b)}\bigg] +1\\
&= 2^{2k} \cdot p^{2 k \ell} \cdot \Ex_{(z,b) \sim D_0}[D_a(z,b) \cdot D_{a'}(z,b)] - 2^{k+1} \cdot p^{k \ell} \cdot \Ex_{(z,b) \sim D_0}[D_a(z,b)] + 1
\end{align*}

We now compute each of the two expectations that appear in the last equation above.

\begin{proposition}\label{prop:single_ex}
For every $a \in \mathbb{F}_p^{\ell}$,
$$ \Ex_{(z,b) \sim D_0}[D_a(z,b)] = \frac{1}{2^k} \cdot \bigg(\frac{1}{p} \cdot \beta + \bigg(1-\frac{1}{p}\bigg) \cdot \alpha\bigg)^k = \frac{1}{2^k \cdot p^{k\cdot \ell}}.$$
\end{proposition}

The proof of Proposition~\ref{prop:single_ex} appears in the appendix.

\begin{proposition}\label{prop:pair_ex}
For every $a, a' \in \mathbb{F}_p^{\ell}$,
\[
\Ex_{(z,b) \sim D_0}[D_a(z,b) \cdot D_{a'}(z,b)] = \begin{dcases*}
	& $\frac{1}{2^k} \cdot (\frac{1}{p} \cdot \beta^2 + (1-\frac{1}{p}) \cdot \alpha^2)^k$ if $\Hyp_a = \Hyp_{a'}$,\\
	& $\frac{1}{2^k}\cdot (\alpha^2 \cdot (1-\frac{2}{p}))^k$ if $\Hyp_a \cap \Hyp_{a'} = \emptyset$,\\
	& $\frac{1}{2^k} \cdot ( \frac{\beta^2}{p^2} +\alpha^2 \cdot (1-\frac{2}{p}+\frac{1}{p^2}))^k$ otherwise.
        \end{dcases*}
\]
\end{proposition}

The proof of Proposition~\ref{prop:pair_ex} appears in the appendix. Using Proposition~\ref{prop:single_ex} and Proposition~\ref{prop:pair_ex}, we now compute $D_0[\hat{D}_a \cdot \hat{D}_{a'}]$.

\begin{proposition}\label{prop:D_0}
For every $a, a' \in \mathbb{F}_p^{\ell}$,
\[
D_0[\hat{D}_a \cdot \hat{D}_{a'}] = \begin{dcases*}
	& $(p+1-\frac{1}{p-1})^k - 1$ if $\Hyp_a = \Hyp_{a'}$,\\
	& $\frac{1}{2^k} \cdot \frac{(1-\frac{2}{p})^k}{(1-\frac{1}{p})^{2k}}-1$ if $\Hyp_a \cap \Hyp_{a'} = \emptyset$,\\
	& $0$ otherwise.
        \end{dcases*}
\]
\end{proposition}

The proof of Proposition~\ref{prop:D_0} appears in the appendix. When computing $\rho(\calD,D_0)$, we will also use the following simple proposition.
\begin{proposition}\label{prop:pairs_hyp}
\begin{enumerate}
\item The number of pairs $(a,a') \in (\mathbb{F}_p^{\ell})^2$ such that $\Hyp_a = \Hyp_{a'}$ is equal to $p^{\ell}$.
\item The number of pairs $(a,a') \in (\mathbb{F}_p^{\ell})^2$ such that $\Hyp_a$ and $\Hyp_{a'}$ are distinct and parallel is equal to $p^{\ell}\cdot(p-1)$.
\item The number of pairs $(a,a') \in (\mathbb{F}_p^{\ell})^2$ such that $\Hyp_a$ and $\Hyp_{a'}$ are distinct and intersecting is equal to $p^{2\cdot \ell}-p^{\ell+1}$.
\end{enumerate}
\end{proposition}

Using Proposition~\ref{prop:D_0} and Proposition~\ref{prop:pairs_hyp}, we are now ready to compute $\rho(\calD,D_0)$ as follows
\begin{align*}
\rho(\calD,D_0) &\le \frac{1}{p^{2\cdot \ell}} \cdot \bigg[ p^{\ell} \cdot (p+1-\frac{1}{p-1})^k +p^{\ell} \cdot (p-1) + p^{2\cdot \ell} \cdot 0 \bigg]\\
&\le O\bigg(\frac{1}{p^{\ell-k}}\bigg) + \frac{1}{p^{\ell-1}}\\
&= O\bigg(\frac{1}{p^{\ell-k}}\bigg),
\end{align*}
where we used above the assumption that $k = O(p)$. We deduce that $\bar{\kappa}_1(\mu,D_0)  = O\bigg(1/p^{(\ell-k)/2}\bigg)$, and hence $\CSDR_{\bar{\kappa}_1}(\calB(\calD,D_0)) = \Omega\bigg(p^{(\ell-k)/2}\bigg)$. This lower bound on $\CSDR_{\bar{\kappa}_1}(\calB(\calD,D_0))$, along with Definition~\ref{def:csdr}, Theorem~\ref{thm:sq_RSD} and the fact that $D_0$ has Bayes error rate equal to $1/2$, imply Lemma~\ref{le:k_plus_1_lb}.



\iffalse
In order to prove \Cref{le:recovery}, we will need the following straightforward propositions.

\begin{proposition}\label{prop:frac_lb}
	For any positive real numbers $N$, $D$ and $\tau$ such that $\tau = o(N)$ and $\tau = o(D)$, we have that
	\begin{equation*}
	\frac{N-\tau}{D+\tau} \geq \frac{N}{D} \cdot (1-o(1)).
	\end{equation*}
\end{proposition}

\begin{proposition}\label{prop:frac_ub}
	For any positive real numbers $N$, $D$ and $\tau$ such that $\tau = o(D)$, we have that
	\begin{equation*}
	\frac{N+\tau}{D-\tau} \le \frac{N}{D} \cdot (1+o(1)) + o(1).
	\end{equation*}	
\end{proposition}

\begin{proposition}\label{prop:tau_i_ub}
	For every $i \in [k+1]$, we have that $\tau_i = o_c(1)$.
\end{proposition}
\begin{proposition}\label{prop:bd_tau_v_tau_i}
	If $v > \epsilon^{k+1}/2$, then for every $i \in [k+1]$, we have that
	\begin{equation*}
	\tau = o_c \bigg( (v \cdot \tau_i - \tau ) \cdot (1-\tau_i/4)\bigg).
	\end{equation*}
\end{proposition}

The proof of \Cref{prop:bd_tau_v_tau_i} follows from the definitions of $\tau$ and $\tau_i$ in \Cref{alg:k_wise_SQ}.
\fi
