% \vspace{-10pt}
\subsection{PMLM Experiments}\label{subsec:pmlm_res}
\input{table/pmlm_res} 

\textbf{PM on Naive.}~~ We further pre-train RoBERTa on the \texttt{Naive} dataset with our pragmatic masking strategy (PM) and compare to a model trained on the same dataset with random masking (RM). As Table~\ref{tab:pmlm_res} shows, PM-N outperforms RM-N with an average improvement of $0.69$ macro $F_1$ points across the $15$ tasks. %\footnote{RM does not improve over the baseline RoBERTa (not further pre-trained) as Table~\ref{tab:pmlm_res} shows.}
 We also observe that PM-N improves over RM-N in $12$ out of the $15$ tasks, thus reflecting the effectiveness of our PM strategy even when working with a dataset such as \texttt{Naive} where it is not guaranteed (although likely) that a tweet has hashtags and/or emojis. Moreover, RM-N outperforms RM-NR on eight tasks with improvement of $0.12$ average $F_1$. This indicates that pragmatic cues (i.e., emoji and hashtags) are essential for learning social media data. 


%Rd-PMLM outperforms Rd-RoBERTa with 0.87 average macro $F_1$ and achieves improvements on 12 tasks out of 15 tasks. Rd-PMLM also obtains the best performance on Hate\textsubscript{David}, Humor\textsubscript{Meaney}, and Senti\textsubscript{Rosen} across all the settings. This indicates that LM can learn the social media language more efficiently by pragmatic masking. The further pre-training with random token masking is hard to learn domain specific knowledge. 

\noindent\textbf{PM of Hashtags.}~~ To study the effect of PM on the controlled setting where we guarantee each sample has at least one hashtag \textit{anywhere}, we further pre-train RoBERTa on the \texttt{Hashtag\_any} dataset with PM (PM-HA in Table~\ref{tab:pmlm_res}) and compare to a model further pre-trained on the same dataset with the RM (RM-HA). As Table~\ref{tab:pmlm_res} shows, PM-HA does not improve over RM-HA. Rather, PM-HA results are marginally lower than those of RM-HA. We suspect that the degradation is due to confusions when a hashtag is used as a word of a sentence. Thus, we investigate the effectiveness of hashtag location.


%The averaged macro $F_1$ of HA-PM is marginally lower than the HA-RM but HA-PM achieve best performance on Hate\textsubscript{Waseem} across all the settings. 

\noindent\textit{\textbf{Effect of Hashtag Location.}}~~ Previous studies~\cite{ren2016context, abdul-2017-emonet} use hashtags as a proxy to label data with social meaning concepts, indicating that hashtags occuring at the end of posts are reliable cues. Hence, we further pre-train RoBERTa on the \texttt{Hashtag\_end} dataset with PM and RM, respectively. As Table~\ref{tab:pmlm_res} shows, PM exploiting hashtags in the end (PM-HE) outperforms random masking (RM-HE) with an average improvement of $1.08$ $F_1$ across the $15$ tasks. It is noteworthy that PM-HE shows improvements over RM-HE in the majority of tasks ($12$ tasks), and both of them outperform the baselines (1) and (3). Compared to RM-HA and PM-HA, the results demonstrate the utility of end-location hashtags on training a LM.
%also indicate that tweets used hashtags at the end enhance the domain-adaptation of learning social media data. }

%on this We refer to these two pre-trained models as HE-PM and HE-RM. Both HE-PM and HE-RM not only outperform the baseline model, but HE-PM also outperforms HE-RM with the average improvement of $1.10$ macro $F_1$ across $15$ tasks. HE-PM shows improvements over HE-RM in $12$ tasks. 

\noindent \textbf{PM of Emojis.}~~ Again, in order to study the impact of PM of emojis under a controlled condition where we guarantee each sample has at least one emoji, we further pre-train RoBERTa on the \texttt{Emoji\_any} dataset with PM and RM, respectively. As Table~\ref{tab:pmlm_res} shows, both methods result in sizable improvements on most of tasks. PM-EA outperforms the random masking method (RM-EA) (macro $F_1$ =$0.38$ improvement) and also exceeds the baseline (1), (2), and (3) with $1.52$, $0.20$, and $1.50$ average $F_1$, respectively. PM-EA thus obtains the best overall performance (macro $F_1$ = $77.30$) and also achieves the best performance on Crisis\textsubscript{Oltea-14}, two irony detection tasks, Offense\textsubscript{Zamp}, and Sarc\textsubscript{Ptacek} across all settings of our PM. 
This indicates that emojis carry important knowledge for social meaning tasks and demonstrates the effectiveness of our PM mechanism to distill and transfer this knowledge to diverse tasks. %via can help the LM learn the knowledge more effectively.   

\noindent \textit{\textbf{Effect of Emoji Location.}}~~ We analyze whether learning is sensitive to emoji location: we further pre-train RoBERTa on \texttt{Emoji\_end} dataset with PM and RM and refer to these two models as PM-EE and RM-EE, respectively. Both models perform better than our baselines (1) and (3), and PM-EE achieves the best performance on four datasets across all settings of our PM. Unlike the case of hashtags, the location of the masked emoji is not sensitive for the learning.  

Overall, results show the effectiveness of our PMLM method in improving the self-supervised LM. All models trained with PM on emoji data obtain better performance than those pre-trained on hashtag data. It suggests that emoji cues are somewhat more helpful than hashtag cues for this type of guided model pre-training in the context of social meaning tasks. This implies emojis are more relevant to many social meaning tasks than hashtags are. In other words, in addition to them being cues for social meaning, hashtags can also stand for general topical categories to which different social meaning concepts can apply (e.g., \textit{\#lunch} can be accompanied by both \textit{happy} and \textit{disgust} emotions).
