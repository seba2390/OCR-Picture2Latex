%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Praphrase Model}\label{sec:para} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to paraphrase our datasets, we fine-tune the T5\textsubscript{BASE} model~\cite{raffel2020exploring} on $4$ paraphrase datasets from PIT-2015 (tweet)~\cite{xu2015semeval},  {LanguageNet} (tweet)~\cite{lan2017continuously}, {Opusparcus} ~\cite{creutz2018open}  (video subtitle), and Quora Question Pairs (Q\&A website) \footnote{\url{https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs}}. For PIT-2015, LanguageNet and Opusparcus, we only keep sentence pairs with a semantic similarity score $\geq 70\%$. We then merge all datasets and shuffle them. Next, we split the data into Train, Dev, and Test ($80\%$, $10\%$, and $10\%$) and fine-tune T5 on the Train split for $20$ epochs with constant learning rate of $3e-4$. 