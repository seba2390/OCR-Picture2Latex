% \section{Data}
\subsection{Pre-training Data}
\noindent \textbf{TweetEnglish Dataset.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We extract $2.4$B English tweets\footnote{We select English tweets based on the Twitter language tag.} from a larger in-house dataset collected between $2014$ and $2020$. We lightly normalize tweets by removing usernames and hyperlinks and add white space between emojis to help our model identify individual emojis. We keep all the tweets, retweets, and replies but remove the `RT USER:' string in front of retweets. To ensure each tweet contains sufficient context for modeling, we filter out tweets shorter than $5$ English words (not counting the special tokens hashtag, emoji, USER, URL, and RT). We call this dataset \textbf{\texttt{TweetEng}}. Exploring the distribution of hashtags and emojis within TweetEng, we find that $18.5$\% of the tweets include at least one hashtag but no emoji, $11.5$\% have at least one emoji but no hashtag, and $2.2$\% have both at least one hashtag and at least one emoji. Investigating the hashtag and emoji location, we observe that $7.1$\% of the tweets use a hashtag as the last term, and that the last term of $6.7$\% of tweets is an emoji. We will use \texttt{TweetEng} as a general pool of data from which we derive for both our PMLM and SFT methods. 

%---------------------------------------
\textbf{PM Datasets.}\label{subsec:prag_masking:data}
%---------------------------------------
We extract five different subsets from \texttt{TweetEng} to explore the utility of our proposed PMLM method. Each of these five datasets comprises $150$M tweets as follows:
\texttt{\textbf{Naive}}. A randomly selected tweet set. Based on the distribution of hashtags and emojis in \texttt{TweetEng}, each sample in \texttt{Naive} still has some likelihood to include one or more hashtags and/or emojis. We are thus still able to perform our PM method on \texttt{Naive}. \texttt{\textbf{Naive-Remove}}. To isolate the utility of using pragmatic cues, we construct a dataset by removing all hashtags and emojis from \texttt{Naive}. \texttt{\textbf{Hashtag\_any}}. Tweets with at least one hashtag anywhere but no emojis. \texttt{\textbf{Emoji\_any}}. Tweets with at least one emoji anywhere but no hashtags. \texttt{\textbf{Hashtag\_end}}. Tweets with a hashtag as the last term but no emojis. \texttt{\textbf{Emoji\_end}}. Tweets with an emoji at the end of the tweet but no hashtags.\footnote{We perform an analysis based on two 10M random samples of tweets from Hashtag\_any and Emoji\_any, respectively. We find that on average there are 1.83 hashtags per tweet in Hashtag\_any and 1.88 emojis per tweet in Emoji\_any.}

%\subsection{Social Meaning Datasets}\label{subsec:sm_datasets}
\noindent\textbf{SFT Datasets.}\label{subsec:sft:data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We experiment with two SFT settings, one based on \textit{hashtags} (\textit{SFT-H}) and another based on \textit{emojis} (\textit{SFT-E}). For SFT-H, we utilize the \texttt{Hashtag\_end} dataset mentioned above. The dataset includes $5$M unique hashtags (all occurring at the end of tweets), but the majority of these are low frequency. We remove any hashtags occurring $< 200$ times, which gives us a set of $63K$ hashtags in $126M$ tweets. We split the tweets into Train ($80\%$), Dev ($10\%$), and Test ($10\%$). For each sample, we use the end hashtag as the sample label.\footnote{We use the last hashtag as the label if there are more than one hashtag in the end of a tweet. Different from PMLM, SFT is a multi-class single-label classification task. We plan to explore the multi-class multi-label SFT in the future.} We refer to this resulting dataset as \textbf{\texttt{Hashtag\_pred}}. For emoji SFT, we work with the \texttt{emoji\_end} dataset. Similar to SFT-H, we remove low-frequence emojis ($< 200$ times), extract the same number of tweets as \texttt{Hashtag\_pred}, and follow the same data splitting method. We acquire a total of $1,650$ unique emojis in final positions, which we assign as class labels and remove them from the original tweet body. We refer to this dataset as \textbf{\texttt{Emoji\_pred}}.

\subsection{Evaluation Benchmark}\label{sec:smpb} 
We collect $15$ datasets representing eight different social meaning tasks to evaluate our models, as follows:~\footnote{To facilitate reference, we give each dataset a name.}
 

% \begin{enumerate}[leftmargin=15pt]
\noindent\textbf{Crisis awareness.}~We use \texttt{Crisis\textsubscript{Oltea}} ~\cite{olteanu2014crisislex}, a corpus for identifying whether a tweet is related to a given disaster or not. %The dataset collects generated during six disasters, including hurricane, tornado, etc. 
    
 \noindent     \textbf{Emotion.} We utilize \texttt{Emo\textsubscript{Moham}}, introduced by~\citet{mohammad-2018-semeval}, for emotion recognition. We use the version adapted in \citet{barbieri-2020-tweeteval}.
 
\noindent\textbf{Hateful and offensive language.} We use \texttt{Hate\textsubscript{Waseem}}~\cite{waseem-2016-hateful}, \texttt{Hate\textsubscript{David}}~\cite{davidson-2017-hateoffensive}, and \texttt{Offense\textsubscript{Zamp}}~\cite{zampieri-2019-predicting}.

\noindent     \textbf{Humor.} We use the humor detection datasets \texttt{Humor\textsubscript{Potash}}~\cite{potash-2017-semeval} and \texttt{Humor\textsubscript{Meaney}}~\cite{meaney2021hahackathon}.

\noindent\textbf{Irony.} We utilize \texttt{Irony\textsubscript{Hee-A}} and  \texttt{Irony\textsubscript{Hee-B}} from \citet{van-hee2018semeval}. 
    % \begin{itemize}[leftmargin=10pt]
    %     \item \textbf{Irony\textsubscript{Hee-A}}~~ \citet{van-hee2018semeval} initially retrieve irony tweets using related hashtags (i.e.,  `\#irony', `\#sarcasm', and `not'). To enhance the quality of labeled data, they reassign the label based on human annotations. 
    %     \item \textbf{Irony\textsubscript{Hee-B}}~~ \citet{van-hee2018semeval} provide a more fine-grained annotation on the samples of Irony\textsubscript{Hee-A}. Each sample is annotated with a type of irony: (1) \textit{verbal irony by means of a polarity contrast}, (2) \textit{other types of verbal irony}, (3) \textit{situational irony}, or (4) \textit{non-ironic}.
    % \end{itemize}
  
\noindent     \textbf{Sarcasm.} %The goal in sarcasm detection is to predict whether a social media post is \textit{sarcastic} or \textit{not sarcastic}. 
We use four sarcasm datasets from \texttt{Sarc\textsubscript{Riloff}}~\cite{riloff2013sarcasm}, \texttt{Sarc\textsubscript{Ptacek}}~\cite{ptavcek2014sarcasm}, \texttt{Sarc\textsubscript{Rajad}}~\cite{rajadesingan2015sarcasm}, and \texttt{Sarc\textsubscript{Bam}}~\cite{bamman2015contextualized}. 
    % \begin{itemize}[leftmargin=10pt]
    %     \item \textbf{Sarcasm\textsubscript{Riloff}}~~ \citet{riloff2013sarcasm} create a manually annotated dataset for sarcasm detection. The sarcastic tweet are originally collected by sarcasm hashtags (`\#sarcasm' or `\#sarcastic'). 
    %     \item \textbf{Sarcasm\textsubscript{ptavcek}}~~ \citet{ptavcek2014sarcasm} introduce a distance supervised dataset for sarcasm detection. The `\#sarcasm' hashtag is used as the proxy to annotate a tweet is sarcastic or not. 
    %     \item \textbf{Sarcasm\textsubscript{Rajad}}~~ \citet{rajadesingan2015sarcasm} provide a English sarcasm detection corpus. They retrieve the sarcastic by using hashtag `\#sarcasm' and `\#not' as the distant indicator. The non-sarcastic tweets are random tweets without any seed hashtags.
    %     \item \textbf{Sarcasm\textsubscript{Bam}}~~ \citet{bamman2015contextualized} also utilize distant supervision to collect English sarcastic tweets. The sarcastic tweet uses the hashtag `\#sarcasm' or `\#sarcastic' as its last term. To investigate the sarcasm expression in the social context, they only extract tweets that are responses to another tweet.
    % \end{itemize}
 
\noindent\textbf{Sentiment.} We employ the three-way sentiment analysis dataset from \texttt{Senti\textsubscript{Rosen}}~\cite{rosenthal-2017-semeval}. 

\noindent\textbf{Stance.} We use \texttt{Stance\textsubscript{Moham}}, a stance detection dataset from \citet{mohammad-2016-semeval}. The task is to identify the position of a given tweet towards a target of interest. 

We use the Twitter API~\footnote{\url{https://developer.twitter.com/}} to crawl datasets which are available only in tweet ID form. We note that we could not download all tweets since some tweets get deleted by users or become inaccessible for some other reason. Since some datasets are old (dating back to 2013), we are only able to retrieve $73\%$ of the tweets on average (i.e., across the different datasets). %This inaccessibility of the data motivates us to paraphrase the datasets as we explain in Section~\ref{sec:para_model}. Before we paraphrase the data or use it in our various experiments, 
We normalize each tweet by replacing the user names and hyperlinks to the special tokens `USER' and `URL', respectively. %This ensures our paraphrased dataset will never have any actual usernames or hyperlinks, thereby protecting user identity. 
For datasets collected based on hashtags by original authors (i.e., \textit{distant supervision}), we also remove the seed hashtags from the original tweets. For datasets originally used in cross-validation, we acquire $80\%$ Train, $10\%$ Dev, and $10\%$ Test via random splits. For datasets that had training and test splits but not development data, we split off $10\%$ from training data into Dev. The data splits of each dataset are presented in Table~\ref{tab:gold_data}. %We now introduce our pragmatic masking method.
\input{table/dataset}
%We describe the data crawling, preparation, and splits of evaluation datasets in Section~\ref{app:sec:crawling} in Appendix. 

To test our models under the \textbf{\textit{few-shot setting}}, we conduct few-shot experiments on varying percentages of the Train set of each task (i.e., $1\%$, $5\%$, $10\%$, $20\%$ \dots $90\%$). For each of these sizes, we randomly sample three times with replacement (\textit{as we report the average of three runs in our experiments}) and evaluate each model on the original Dev and Test sets. We also evaluate our models on the \textbf{\textit{zero-shot setting}} utilizing data from Arabic: \texttt{Emo\textsubscript{Mageed}}~\cite{mageed-2020-aranet}, \texttt{Irony\textsubscript{Ghan}}~\cite{idat2019}; Italian: \texttt{Emo\textsubscript{Bian}}~\cite{bianchi2021feel} and \texttt{Hate\textsubscript{Bosco}}~\cite{bosco2018overview}; and Spanish:    \texttt{Emo\textsubscript{Moham}}~\cite{mohammad-2018-semeval} and \texttt{Hate\textsubscript{Bas}}~\cite{basile-2019-semeval}. %We describe the data crawling, preparation, and splits of evaluation datasets in Section~\ref{app:sec:crawling} in Appendix. 



  
  %\footnote{Information about individual dataset sizes is in Appendix~\ref{append:datasets}.} This data attrition motivates our paraphrase work we describe in Section~\ref{subsec:paraphrasing}. We now describe our method to paraphrase the benchmark. 
 
% \textbf{Crisis awareness.} Crisis awareness task anlyze social media communication during the crises and mass emergencies. We utilize a dataset from \citet{olteanu2014crisislex}. 
% \begin{itemize}
%  \item 
% \textbf{Crisis\textsubscript{Oltea}}~~ \citet{olteanu2014crisislex}