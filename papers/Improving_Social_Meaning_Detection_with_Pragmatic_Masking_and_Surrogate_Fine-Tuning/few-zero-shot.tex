\vspace{-3pt}
\section{Zero- and Few-Shot Learning}\label{sec:zfew}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Since our methods exploit general cues in the data for pragmatic masking and learn a broad range of social meaning concepts, we hypothesize they should be particularly effective in \textbf{\textit{few-shot learning}}. To test this hypothesis, we fine-tune our best models (i.e., PragS1 and PragS2) on varying percentages of the Train set of each task as explained in Section~\ref{sec:smpb}. Figure~\ref{fig:data_percent} shows that our two models \textit{always} achieve better average macro $F_1$ scores than each of the RoBERTa and BERTweet baselines across \textit{all} data size settings. Strikingly, our PragS1 and PragS2 outperform RoBERTa with an impressive $11.16$ and $10.55$ average macro $F_1$, respectively, when we fine-tune them on only $1\%$ of the downstream gold data. If we use only $5\%$ of gold data, our PragS1 and  PragS2 improve over the RoBERTa baseline with $5.50\%$ and $5.08$ points, respectively. This demonstrates that our proposed methods most effectively alleviate the challenge of labeled data even under the \textit{severely} few-shot setting. In addition, we observe that the domain-specific LM, BERTweet,  is outperformed by RoBERTa when labeled training data is severely scarce ($\leq 20\%$) (although it achieves superior performance when it is fine-tuned on the full dataset). \textit{These results suggest that, for the scarce data setting, it may be better to further pre-train and surrogate fine-tune an PLM than pre-train a domain-specific LM from scratch.}
%demonstrate that our proposed methods not only adequately adapting the RoBERTa to the social media domain but also preserve the advantage of its large-scale pre-training. 
We provide model performance on each downstream task and various few-shot settings in Section~\ref{sec:append_fewshot} in Appendix. 

% It also shows that \textit{our method is remarkable for few-shot learning compared to mere fine-tuning that does not exploit SFT}. We were curious how it is that this effectiveness of SFT across the different social meaning tasks would compare to multi-task learning, another approach to knowledge transfer. We describe our experiments to answer this question next.%low-resource task of social meaning. 
%\chiyu{In the few-shot setting, our model always acquires the same or better results than the baseline model exploiting only $50\%$ of the training data utilized by the baseline model. }
\input{table/data_percent}

\input{table/multi-lingual}

Our proposed methods are language agnostic, and may fare well on languages other than English. Although we do not test this claim directly in this work, we do score our English-language best models on six datasets from three other languages (\textit{\textbf{zero-shot setting}}). %Namely, we test our models on data from Arabic: \texttt{Emo\textsubscript{Mageed}}~\cite{mageed-2020-aranet}, \texttt{Irony\textsubscript{Ghan}}~\cite{idat2019}; Italian: \texttt{Emo\textsubscript{Bian}}~\cite{bianchi2021feel} and \texttt{Hate\textsubscript{Bosco}}~\cite{bosco2018overview}; and Spanish:    \texttt{Emo\textsubscript{Moham}}~\cite{mohammad-2018-semeval} and \texttt{Hate\textsubscript{Bas}}~\cite{basile-2019-semeval}. 
We fine-tune our best English model (i.e., PragS2 in Table~\ref{tab:sft_res}) on the English dataset Emo\textsubscript{Moham}, Irony\textsubscript{Hee-A}, and Hate\textsubscript{David} and, then, evaluate on the Test set of emotion, irony, and hate speech datasets from other languages, respectively. We compare these models against the English RoBERTa baseline fine-tuned on the same English data. As Table~\ref{tab:zero} shows, our models outperform the baseline in the zero-shot setting on five out of six dataset with an average improvement of $5.96$ $F_1$. These results emphasize the effectiveness of our methods even in the zero-shot setting across different languages and tasks, and motivate future work further extending our methods to other languages.

