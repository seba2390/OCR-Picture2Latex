\section{Hyper-parameters and Procedure}\label{subsec:models:hyperparameter}

\textbf{Pragmatic Masking.} For pragmatic masking, we use the Adam optimizer with a weight decay of $0.01$~\cite{loshchilov2018decoupled} and a peak learning rate of $5e-5$. The number of the epochs is five. 

\noindent \textbf{Surrogate Fine-Tuning.} For surrogate fine-tuning, we fine-tune RoBERTa on surrogate classification tasks with the same Adam optimizer but use a peak learning rate of $2e-5$. %We fine-tune on \texttt{Hashtag\_pred} task for 50 epochs and \texttt{Hashtag\_token} task for 30 epochs, respectively. 

The pre-training and surrogate fine-tuning models are trained on eight Nvidia V$100$ GPUs ($32$G each). On average the running time is $24$ hours per epoch for PMLMs, $2.5$ hours per epoch for SFT models. All the models are implemented by Huggingface Transformers~\cite{wolf-2020-transformers}.%~\footnote{Our code is available here: \url{https://anonymous.4open.science/r/fd268167-947a-482c-aca7-7b54aa6b5ed9/}.}
%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Downstream Fine-Tuning.} We evaluate the further pre-trained models with pragmatic masking and surrogate fine-tuned models on the $15$ downstream tasks in Table~\ref{tab:gold_data}. We set maximal sequence length as $60$ for $13$ text classification tasks. For Crisis\textsubscript{Oltea} and Stance\textsubscript{Moham}, we append the topic term behind the post content, separate them by [SEP] token, and set maximal sequence length to $70$, especially. For all the tasks, we pass the hidden state of [CLS] token from the last Transformer-encoder layer through a non-linear layer to predict. Cross-Entropy calculates the training loss. We then use Adam with a weight decay of $0.01$ to optimize the model and fine-tune each task for $20$ epochs with early stop ($patience = 5$ epochs). We fine-tune the peak learning rate in a set of $\{1e-5, 5e-6\}$ and batch size in a set of $\{8, 32, 64\}$. We find the learning rate of $5e-6$ performs best across all the tasks. For the downstream tasks whose Train set is smaller than $15,000$ samples, the best mini-batch size is eight. The best batch size of other larger downstream tasks is $64$. For fine-tuning BERTweet, we use the hyperparameters identified in~\citet{nguyen-etal-2020-bertweet}, i.e., a fixed learning rate of $1e-5$ and a batch size of $32$. 

%\noindent\textbf{Multi-task fine-tuning.} For Multi-task fine-tuning, we use a batch size of $64$ for all the tasks and train with $30$ epochs without early stopping. We implement the multi-task learning using hard-sharing~\cite{caruana1997multitask} model. We send the final state of the [CLS] token to the non-linear layer of the corresponding task.

We use the same hyperparameters to run three times with random seeds for all downstream fine-tuning (unless otherwise indicated). All downstream task models are fine-tuned on four Nvidia V$100$ GPUs ($32$G each). At the end of each epoch, we evaluate the model on the Dev set and identify the model that achieved the highest performance on Dev as our best model. We then test the best model on the Test set. In order to compute the model's overall performance across $15$ tasks, we use same evaluation metric (i.e., macro $F_1$) for all tasks. We report the average Test macro $F_1$ of the best model over three runs. We also average the macro $F_1$ scores across $15$ tasks to present the model's overall performance. 