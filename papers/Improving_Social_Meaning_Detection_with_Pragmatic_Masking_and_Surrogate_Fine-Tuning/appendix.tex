% \clearpage
% \appendix
\appendixpage

\numberwithin{figure}{section}
\numberwithin{table}{section}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% \section{Data Crawling and Preparation}\label{app:sec:crawling}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% We use the Twitter API~\footnote{\url{https://developer.twitter.com/}} to crawl datasets which are available only in tweet ID form. We note that we could not download all tweets since some tweets get deleted by users or become inaccessible for some other reason. Since some datasets are old (dating back to 2013), we are only able to retrieve $73\%$ of the tweets on average (i.e., across the different datasets). %This inaccessibility of the data motivates us to paraphrase the datasets as we explain in Section~\ref{sec:para_model}. Before we paraphrase the data or use it in our various experiments, 
% We normalize each tweet by replacing the user names and hyperlinks to the special tokens `USER' and `URL', respectively. %This ensures our paraphrased dataset will never have any actual usernames or hyperlinks, thereby protecting user identity. 
% For datasets collected based on hashtags by original authors (i.e., \textit{distant supervision}), we also remove the seed hashtags from the original tweets. For datasets originally used in cross-validation, we acquire $80\%$ Train, $10\%$ Dev, and $10\%$ Test via random splits. For datasets that had training and test splits but not development data, we split off $10\%$ from training data into Dev. The data splits of each dataset are presented in Table~\ref{tab:gold_data}. %We now introduce our pragmatic masking method.
% \input{table/dataset}


% \hl{Check T5 and hyper-parameter}
% \input{datasets}
% \section{Dataset}
% Table~\ref{tab:gold_data} presents the distribution of 15 social meaning datasets. 
% \input{acl-ijcnlp2021-templates/table/dataset}
\input{hyperparameter}
% \section{Summary of model performance}
% Table~\ref{tab:full_res} summarizes performance of our models across different settings. 
% \input{acl-ijcnlp2021-templates/table/full_res}
% \vspace{-5pt}
% \section{Paraphrasing Model and SMPB}


\input{few_shot_all}
\input{uniformity-tolerance}
% \input{t5_para}
% \vspace{-5pt}
% \input{table/para_data}
% \input{table/para_sample_app}

% \subsection{Paraphrase-Based Methods}
% We present the average Test macro $F_1$ over three runs in Table~\ref{tab:para_res}. As Table~\ref{tab:para_res} shows, although none of our paraphrase-based models (Para-Models) exceed the gold RoBERTa baseline, our P4 model gets very close (i.e., only $0.35\%$ less). In addition, an ensemble of our Para-Models is slightly better than the gold RoBERTa baseline ($F_1=75.83$ vs. $75.78$). %We also explore intermediate fine-tuning where we try to further fine-tune the already fine-tuned paraphrase-based model on gold downstream data, but results we acquire are still below the performance of the model fine-tuned on gold without intermediate fine-tuning on paraphrase data (average $F_1$ $76.91$ vs. $78.12$).
% \input{acl-ijcnlp2021-templates/table/para_res}




