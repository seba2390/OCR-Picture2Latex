\vspace{-5pt}
\section{Proposed Methods}\label{sec:method}
\vspace{-3pt}
% \input{table/illustration}
%%%%%%%%%%%%%%
%Now, we introduce our two approaches to improve the PLM for the social meaning tasks.% Sections~\ref{sec:prag_masking} and~\ref{sec:surrogate_ft} demonstrate the pragmatic masking and surrogate fine-tuning respectively.

\subsection{Pragmatic Masking}\label{sec:prag_masking} 
MLMs employ random masking, and so are not guided to learn any particular type of information during pre-training. Several attempts have been made to employ task-specific masking where the objective is to predict information relevant to a given downstream task. Task relevant information is usually identified based on world knowledge (e.g., a sentiment lexicon~\cite{gu-2020-train,ke-2020-sentilare}, part-of-speech (POS) tags~\cite{zhou-2020-limit}) or based on some other type of modeling such as pointwise mutual information~\cite{tian-2020-skep} with supervised data. Although task-specific masking is useful, it is desirable to identify a \textit{more general} masking strategy that \textit{does not depend on external information} that may not be available or hard to acquire (e.g., costly annotation). For example, there are no POS taggers for some languages and so methods based on POS tags would not be applicable. Motivated by the fact that random masking is intrinsically sub-optimal~\cite{ke-2020-sentilare,kawintiranon-2021-knowledge} and this particular need for a more general and dependency-free masking method, we introduce our novel pragmatic masking mechanism that is suited to a wide range of social meaning tasks. % (e.g., sarcasm detection, sentiment analysis). %We call our method \textit{pragmatic} because it exploits cues such as hashtags and emojis in interpersonal social media communication. This type of ``meaning in interaction" is the domain of linguistic \textit{pragmatics}~\cite{thomas2014meaning}. 
\input{table/tw_sample}

To illustrate, consider the tweet samples in Table~\ref{tab:tw_sample}: In example (1), the emoji ``\emojisick'' combined with the suffix ``-ing'' in ``\emojisick ing'' is a clear signal indicating the \textit{disgust} emotion. In example (2) the emoji ``\emojiupsidedown'' and the hashtag ``\#sarcasm'' communicate \textit{sarcasm}. In example (3) the combination of the emojis ``\emojidispoint'' and ``\emojianger'' accompany `hard' emotions characteristic of \textit{offensive} language. We hypothesize that by simply masking cues such as emojis and hashtags, we can bias the model to learn about different shades of social meaning expression. This masking method can be performed in a \textit{self-supervised} fashion since hashtags and emojis can be automatically identified. We call the resulting language model \textbf{\textit{pragmatically masked language model (PMLM)}}. Specifically, when we choose tokens for masking, we prioritize hashtags and emojis as Figure~\ref{fig:pm} shows.
%%%%%%%%%%%%%%%%%%%%
% We view hashtags and emojis in a tweet as proxy labels that carry meaning about the tweet itself. Hashtags have been used as proxy (\textit{distant labels}) in NLP tasks such as emotion~\cite{sintsova-2016-dystemo,abdul-2017-emonet} and sarcasm detection~\cite{riloff2013sarcasm, ptavcek2014sarcasm}. Emoji were also used to improve highly-subjective tasks such as sentiment analysis, emotion detection, and sarcasm detection~\cite{felbo-2017-using,barbieri-2018-semeval}. Inspired by previous work, we propose a pragmatic masking strategy to enhance the MLM pre-training on social media data. We call the resulting model \textbf{\textit{pragmatically masked language model (PMLM)}}. Specifically, when we choose tokens for masking, we prioritize hashtags and emojis. 
%%%%%%%%%%%%%%%%%%%%%%
The pragmatic masking strategy follows several steps:
\noindent \textbf{(1) Pragmatic token selection.} %The input tweet text is tokenize into byte-pair-encodings (BPE) by the RoBERTa tokenizer. We search the tokens that belongs to hashtag or emojis and group these tokens by individual hashtag or emoji. 
We randomly select up to $15\%$ of input sequence, giving masking \textbf{priority} to hashtags or emojis. The tokens are selected by whole word masking (i.e., whole hashtag or emoji). \noindent \textbf{(2) Regular token selection.} If the pragmatic tokens are less than $15\%$, we then randomly select regular BPE tokens to complete the percentage of masking to the $15\%$. \noindent \textbf{(3) Masking.} This is the same as the RoBERTa MLM objective where we replace $80\%$ of selected tokens with the [MASK] token, $10\%$ with random tokens, and we keep $10\%$ unchanged. 


\subsection{Surrogate Fine-tuning}\label{sec:surrogate_ft}
The current transfer learning paradigm of first pre-training then fine-tuning on particular tasks is limited by how much labeled data is available for downstream tasks. In other words, this existing set up works only given large amounts of labeled data. We propose surrogate fine-tuning where we intermediate fine-tune PLMs to predict thousands of example-level cues (i.e., hashtags occurring at the end of tweets) as Figure~\ref{fig:sft} shows. This method is inspired by previous work that exploited hashtags~\cite{riloff2013sarcasm,ptavcek2014sarcasm,rajadesingan2015sarcasm,sintsova-2016-dystemo,abdul-2017-emonet,barbieri-2018-semeval} or emojis~\cite{wood2016emoji,felbo-2017-using,wiegand-2021-exploiting} as proxy for labels in a number of social meaning tasks. %, this method of surrogate labeling has not been exploited for transfer learning. I
However, instead of identifying a small \textit{specific} set of hashtags or emojis for a \textit{single} task and using them to collect a dataset of \textit{distant} labels, we diverge from the literature in proposing to use data with \textit{any} hashtag or emoji as a surrogate labeling approach suited for \textit{any} (or at least most) social meaning task. %We then propose to use these hashtags to fine-tune pre-trained LMs to predict the identity of the hashtags themselves, rather than certain task-specific labels decided a priori. 
As explained, we refer to our method as \textbf{\textit{surrogate fine-tuning (SFT).}}