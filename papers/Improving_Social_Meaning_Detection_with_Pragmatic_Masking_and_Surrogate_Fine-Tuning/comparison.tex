\vspace{-5pt}
\subsection{Model Comparisons}\label{subsec:comparisons}
The purpose of our work is to produce representations effective across all social meaning tasks, rather than a single given task. However, we still compare our best model (i.e., PragS2) on each dataset to the SOTA of that particular dataset and the published results on a Twitter evaluation benchmark~\cite{barbieri-2020-tweeteval}. \textit{All our reported results are an average of three runs}, and we report using the same respective metric adopted by original authors on each dataset. As Table~\ref{tab:compare} shows, our model achieves the best performance on eight out of $15$ datasets. On average, our models are $0.97$ points higher than the closest baseline, i.e., BERTweet. This shows the superiority of our methods, even when compared to models trained simply with MLM with $\sim3 \times$ more data ($850$M tweets for BERTweet vs. only $276$M for our best method). We also note that some SOTA models adopt task-specific approaches and/or require task-specific resources. For example,~\citet{bamman2015contextualized} utilize Stanford sentiment analyzer to identify the sentiment polarity of each word. In addition, task-specific methods can still be combined with our proposed approaches to improve performance on individual tasks.
\input{table/compare}
%\footnote{Our best setting, X2+SFT-H, exploits two streams of data, for a total of $276$M tweets. This is still only $\sim 32$\% of the data in BERTweet.} %We discuss this further in Appendix xx. 
%As such, we set new SOTA on all tasks, therby further demonstrating the effectiveness of our proposed methods.



