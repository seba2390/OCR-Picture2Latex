\section{Experiments}\label{sec:experiment}
% \chiyu{Add a content guide}

\input{data}

\subsection{Implementation and Baselines}\label{subsec:baseline} 
%---------------------------------------
For both our experiments on PMLM (Section~\ref{subsec:pmlm_res}) and SFT (Section~\ref{subsec:sft_result}), we use the pre-trained English RoBERTa\textsubscript{Base}~\cite{liu2019roberta} model as the initial checkpoint model. We use this model, rather than a larger language model, since we run a large number of experiments and needed to be efficient with GPUs. We use the RoBERTa~\footnote{For short, we refer to the official released English RoBERTa\textsubscript{Base} as RoBERTa in the rest of the paper.} tokenizer to process each input sequence and pad or truncate the sequence to a maximal length of $64$ BPE tokens. We continue training RoBERTa with our proposed methods for five epochs with a batch size of $8,192$ and then fine-tune the further trained models on downstream datasets. We provide details about our hyper-parameters in Appendix\ref{subsec:models:hyperparameter}. Our \textbf{baseline (1)} fine-tunes original pre-trained RoBERTa on downstream datsets without any further training. %~\footnote{In this paper, further pre-training indicates that we take RoBERTa checkpoint and further train it on MLM objectives.}. 
Our \textbf{baseline (2)} fine-tunes a SOTA Transformer-based PLM for English tweets, i.e., BERTweet~\cite{nguyen-etal-2020-bertweet}, on downstream datasets. For PMLM experiments, we provide \textbf{baseline (3)}, which further pre-trains RoBERTa on \texttt{Naive-Remove} dataset with the random masking strategy and MLM objectives. We refer to this model as RM-NR. We now present our results.

%RM-NR surpasses RoBERTa\textsubscript{Base} on 10 out of 15 tasks and obtains slightly improvement on average macro F\textsubscript{1} (i.e., $0.02$).

