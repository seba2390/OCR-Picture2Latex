\section{Model Analyses}
 To better understand model behavior, we carry out both a qualitative and a quantitative analysis. For the qualitative analysis, we encode all the Dev and Test samples from one emotion downstream task using two PLMs (RoBERTa and BERTweet) and our two best models (i.e., PragS1 and PragS2)\footnote{Note that we use these representation models \textit{without} downstream fine-tuning.}. We then use the hidden state of the [CLS] token from the last Transformer encoder layer as the representation of each input. We then map these tweet representation vectors ($768$ dimensions) to a $2$-D space through t-SNE technique~\cite{van2008visualizing} and visualize the results. Comparing our models to the original RoBERTa and BERTweet, we observe that the representations from our models give sensible clustering of emotions before fine-tuning on downstream dataset. %For space limitations, we are not able to include the visualization in the paper.\footnote{With more space, we will provide the visualization in the camera ready version if the paper is accepted.}
 
 \input{table/sample_tsne}
 
Recent research~\cite{ethayarajh-2019-contextual, li-etal-2020-sentence, gao-etal-2021-simcse} has identified an anisotropy problem with the sentence embedding from PLMs, i.e., learned representations occupy a narrow cone, which significantly undermines their expressiveness. Hence, several concurrent studies~\cite{gao-etal-2021-simcse, liu2021fast} seek to improve uniformity of PLMs. However,~\citet{wang-2021-understanding} reveal a uniformity-tolerance dilemma, where excessive uniformity makes a model intolerant to semantically similar samples, thereby breaking its underlying semantic structure. Following~\citet{wang-2021-understanding}, we investigate the uniformity and tolerance of our models. The uniformity metric indicates the embedding distribution in a unit hypersphere, and the tolerance metric is the mean similarities of samples belonging to the same class. Formulas of uniformity and tolerance are defined in Section~\ref{sec:uni-tole} in appendix. We calculate these two metrics for each model using development data from our $13$ downstream datasets (excluding Crisis\textsubscript{Oltea} and Stance\textsubscript{Moham}). As Table~\ref{tab:uniformity-tolerance} shows, RoBERTa obtains a low uniformity and high tolerance score with its representations are located at a narrow cone where the cosine similarities of data points are extremely high. Results reveal that none of MLMs (i.e., pragmatic masking and random masking models) improves the spatial anisotropy. Nevertheless, surrogate fine-tuning is able to alleviate the anisotropy improving the uniformity. SFT-H achieves best uniformity (at $3.00$). Our hypothesis is that fine-tuning on our extremely fine-grained hashtag prediction task forces the model to learn a more uniform representation where hashtag classes are separable. Finally, we observe that our best model, Prag2, makes a balance between uniformity and tolerance (uniformity$=2.36$, tolerance$=0.35$). 
 \input{table/uniformity-tolerance}