% \vspace{-10pt}
\section{Conclusion}\label{sec:conclude} 
We proposed two novel methods for improving transfer learning with PLMs, pragmatic masking and surrogate fine-tuning, and demonstrated the effectiveness of these methods on a wide range of social meaning datasets. 
Our models exhibit remarkable performance in the few-shot setting and even the severely few-shot setting. Our models also establish new SOTA on eight out of fifteen datasets when compared to tailored, task-specific models with access to external resources. Our proposed methods are also language independent, and show promising performance when applied in zero-shot settings on six datasets from three different languages. In future research, we plan to further test this language independence claim. We hope our methods will inspire new work on improving language models without use of much labeled data.%~\cite{nguyen-etal-2020-bertweet}
