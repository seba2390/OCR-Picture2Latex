\vspace{-3pt}
\section{Related works}\label{sec:related_work}
\vspace{-3pt}
%#########################
\textbf{Masked Language models.} 
\citet{devlin-2019-bert} introduced BERT, a language representation model pre-trained by joint conditioning on both left and right context in all layers with the Transformer encoder~\cite{vaswani2017attention}. BERT's pre-training introduces a self-supervised learning objective, i.e., masked language modeling (MLM), to train the Transformer encoder. MLM predicts masked tokens in input sequences exploiting bi-directional context. %For pre-training, BERT randomly selects 15\% of tokens to be replaced with [MASK]. 
RoBERTa~\cite{liu2019roberta} optimizes BERT performance by removing the next sentence prediction objective and by pre-training on a larger corpus using a bigger batch size. In the last few years, several variants of LMs with different masking methods were proposed. Examples are XLNet~\cite{yang2019xlnet} and MASS~\cite{song2019mass}. %These include XLNet~\cite{yang2019xlnet}, MASS~\cite{song2019mass}, and MARGE~\cite{lewis2020pre}.% introduce MARGE to pre-train LM via paraphrasing. \citet{joshi-2020-spanbert} enhance the BERT performance by masking the continuous span of tokens rather than random tokens. 
To incorporate more domain specific knowledge into LMs, some works introduce knowledge-enabled masking strategies. For example, \citet{sun2019ernie,zhang-2019-ernie,lin-2021-entitybert} propose to mask tokens of named entities, while %ERINE-Tsinghua~\cite{zhang-2019-ernie} introduce a knowledge graph method to select masking words.
\citet{tian-2020-skep} and \citet{ke-2020-sentilare} select sentiment-related words to mask during pre-training. \citet{gu-2020-train} and \citet{kawintiranon-2021-knowledge} propose selective masking methods to mask the more important tokens for downstream tasks (e.g., sentiment analysis and stance detection). However, these masking strategies depend on external resources and/or annotations (e.g., a lexicon or labeled corpora).~\citet{corazza-2020-hybrid} investigate the utility of hybrid emoji-based masking for enhancing abusive language detection. Previous works, therefore, only focus on one or another particular task (e.g., sentiment, abusive language detection) rather than the type of broad representations we target.
%and do not investigate the utility of hashtags nor the effect of location of emojis and hashtags on the downstream tasks. %Many recent concurrent studies combine the MLM objective with contrastive learning objective to 

\textbf{Intermediate Fine-Tuning.} Although pre-trained language models (PLM) have shown significant improvements on NLP tasks, intermediate training of the PLM on one or more data-rich tasks can further improve performance on a target downstream task. Most previous work (e.g., ~\cite{wang-2019-tell,pruksachatkun-2020-intermediate,phang-2020-english,chang-2021-rethinking,poth-2021-pre}) focus on intermediate fine-tuning on a given gold-labeled dataset related to a downstream target task. Different to these works, our surrogate fine-tuning method is \textit{agnostic} to narrow downstream tasks and fine-tunes an PLM on large-scale data with tens of thousands of \textit{surrogate} labels that may be relevant to all social meaning. We now introduce our methods. % (i.e., hashtags and emojis) and, then, enhance model performance on various downstream tasks (task agnostic). 

% \textbf{Social meaning detection.}
% Several works have also been carried out to detect social meaning. These include works cited earlier in this paper. %irony and sarcasm~\cite{riloff2013sarcasm,ptavcek2014sarcasm,rajadesingan2015sarcasm,bamman2015contextualized,van-hee2018semeval}, humor~\cite{potash-2017-semeval}, sentiment~\cite{socher-2013-recursive,rosenthal-2017-semeval}, emotion~\cite{sintsova-2016-dystemo, abdul-2017-emonet,mohammad-2018-semeval}, offensive language and hate speech~\cite{waseem-2016-hateful,davidson-2017-hateoffensive, zampieri-2019-predicting}, stance~\cite{mohammad-2016-semeval}, angency~\cite{kokil-2019-happiness}, sociality~\cite{kokil-2019-happiness}, crisis awareness~\cite{olteanu2014crisislex} misinformation~\cite{cui2020coaid,zhou-2020-recovery}, author profiling~\cite{rangel2019overview,pardo2018overview,fagni2020tweepfake}, and political ideology~\cite{preotiuc-2017-beyond}. 
% A challenge that work on social meaning detection faces is replicability as a function of inaccessibility. Many tweets get deleted or become inaccessible.~\newcite{barbieri-2020-tweeteval} collect datasets from $7$ tasks and restrict each task to $50$K tweets so that they can directly share the tweets. Our work is similar in that we attempt to ease the data bottleneck, but has the advantage of not putting a cap on the data size since we can paraphrase and share arbitrarily large amounts of data. %By proposing to use paraphrased training data that can be directly shared instead of original tweets, we aim to ease this bottleneck.

%The datasets are crowed-source annotated or distant supervised (e.g., utilize the hashtag as a proxy~\cite{riloff2013sarcasm,abdul-2017-emonet}). Most of the social meaning analysis datasets utilize use posts at Twitter\footnote{\url{https://twitter.com/}} where is the largest social media platform~\cite{farzindar2015natural} around the whole world. Due to the Twitter content redistribution restriction, the developed corpora usually solely provide the tweet IDs and labels without the actual tweet contents. We needs to hydrate the tweet IDs via Twitter API. However, we usually cannot obtain all the tweets because of the inaccessibility of parts of tweets. 

