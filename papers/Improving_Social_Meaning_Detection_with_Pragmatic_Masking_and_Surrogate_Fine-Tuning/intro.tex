%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%
Masked language models (MLMs) such as BERT~\cite{devlin-2019-bert} have revolutionized natural language processing (NLP). These models exploit the idea of self-supervision where sequences of unlabeled text are masked and the model is tasked to reconstruct them. Knowledge acquired during this stage of denoising (called \textit{pre-training}) can then be transferred to downstream tasks through a second stage (called \textit{fine-tuning}). Although pre-training is general, does not require labeled data, and is task agnostic, fine-tuning is narrow, requires labeled data, and is task-specific. For a class of tasks $\Tau$, some of which we may not know in the present but which can become desirable in the future, it is unclear how we can bridge the learning objective mismatch between these two stages. In particular, how can we \textbf{(i)} make pre-training more tightly related to downstream task learning objective; and \textbf{(ii)} focus model pre-training representation on an all-encompassing range of concepts of general affinity to various downstream tasks?

\begin{figure}[t]
\centering
\begin{subfigure}[]{\linewidth}
  \centering
  \includegraphics[width=\linewidth]{table/pragmatic_masking.png}
  \caption{Pragmatic masking}\label{fig:pm}
\end{subfigure} 
\begin{subfigure}[]{\linewidth}
  \centering
  \includegraphics[width=\linewidth]{table/sft.png}
  \caption{Surrogate fine-tuning}\label{fig:sft}
\end{subfigure}
\caption{Illustration of our proposed pragmatic masking and surrogate fine-tuning methods.}
\end{figure}

%Although MLMs themselves do not require labeled data, fine-tuning is dependent on availability of labels. Although this set up of pre-train then fine-tune usually works well, it is dependent on labels for the fine-tuning stage and assumes knowledge of 
%-----------------------------------------------
% \input{table/tw_sample}
%-----------------------------------------------
We raise these questions in the context of learning a cluster of tasks to which we collectively refer as \textit{social meaning}. We loosely define social meaning as meaning emerging through human interaction such as on social media. Example social meaning tasks include emotion, irony, and sentiment detection. We propose two main solutions that we hypothesize can bring pre-training and fine-tuning closer in the context of learning social meaning: First, we propose a particular type of guided masking that prioritizes learning contexts of tokens %we suspect are 
crucially relevant to social meaning in interactive discourse. Since the type of ``meaning in interaction'' we are interested in is the domain of linguistic pragmatics~\cite{thomas2014meaning}, we will refer to our proposed masking mechanism as \textit{pragmatic masking}. We explain pragmatic masking in Section~\ref{sec:prag_masking}. 

Second, we propose an additional novel stage of fine-tuning that does not depend on gold labels but instead exploits general data cues possibly relevant to \textit{all} social meaning tasks. More precisely, we leverage sequence-level user assigned tags for \textit{intermediate} fine-tuning of pre-trained language models. In the case of Twitter, for example, hashtags naturally assigned by users at the end of posts can carry discriminative power that is by and large relevant to a wide host of tasks. Although cues such as hashtags and emojis have been previously used as surrogate lables before for one task or another, we put them to a broader use that is not focused on a particular (usually narrow) task that learns from a handful of cues. In other words, our goal is to learn extensive concepts carried by tens of thousands of cues. A model endowed with such a knowledge-base of social concepts can then be further fine-tuned on any narrower task in the ordinary way. We refer to this method as \textit{surrogate fine-tuning} (Section~\ref{sec:surrogate_ft}). Another migration from previous work is that our methods excel not only in the full-data setting but also for \textit{few-shot learning}, as we will explain below.
%Different from previous work exploiting these social cues, our methods are also effective in \textit{few-shot learning}.

% \textbf{Pragmatic Masking.} MLMs employ random masking and so are not guided to learn any specific types of information during pre-training. Several attempts have been made to employ task-specific masking where the objective is to predict information relevant to a given downstream task. Task relevant information is usually identified based on world knowledge (e.g., a sentiment lexicon~\cite{gu-2020-train,ke-2020-sentilare}, a knowledge graph~\cite{zhang-2019-ernie}, part-of-speech (POS) tags~\cite{zhou-2020-limit}) or based on some other type of modeling with supervised data (e.g., pointwise mutual information~\cite{tian-2020-skep}). 
% \input{acl-ijcnlp2021-templates/table/tw_sample}
% Although task-specific masking is useful, it is desirable to identify a \textit{more general} masking strategy that \textit{does not depend on external information}. The reason is that such information may be unavailable, or hard to acquire. For example, there are no POS taggers for some languages and so methods based on POS tags would not be applicable. Motivated by the fact that random masking is intrinsically sub-optimal and this particular need for a more general and dependency-free masking method, we introduce a novel \textit{pragmatic masking mechanism} suited to a wide range of social meaning tasks (e.g., sarcasm detection, sentiment analysis). We call our method \textit{pragmatic} because it exploits cues such as hashtags and emojis in interpersonal social media communication. This type of ``meaning in interaction" is the domain of linguistic \textit{pragmatics}~\cite{thomas2014meaning}. 
% To illustrate, consider the tweet samples in Table~\ref{tab:tw_sample}: In example (1), the emoji ``\emojisick" combined with the suffix ``-ing" in ``\emojisick ing" is a clear signal indicating the \textit{disgust} emotion. In example (2) the emoji ``\emojienutral" and the hashtag ``\#sarcasm" communicate \textit{sarcasm}. In example (3) the combination of the emojis ``\emojidispoint" and ``\emojianger" accompany `hard' emotions characteristic of \textit{offensive} language. We hypothesize that by simply masking cues such as emojis and hashtags, we can bias the model to learn about the different shades of social meaning expression. This masking method can be performed in a \textit{self-supervised} fashion since hashtags and emojis can be automatically identified. 
% \textbf{Surrogate Fine-Tuning.} In addition, we hypothesize that hashtags can be used for a particular flavor of \textit{fine-tuning} where these hashtags are used as proxy for class labels. One nuance is that we do not actually choose the hashtags to correspond to any given downstream task. Rather, we simply turn all hashtags occurring at the end of tweets in a large tweet collection into a set of class labels. A model can then be fine-tuned to predict these labels. Since the hashtags are only surrogates for a wide range of pragmatic communication functions not defined a priori, we refer to this proposed method as \textit{surrogate fine-tuning}. 
% \hl{Talk about paraphrase.}
% \input{acl-ijcnlp2021-templates/table/tw_sample}
%This randomness is countered by the practice of training these models for a very a large number of epochs.   
%\textbf{Social media are important.} Our world today is highly connected, with social media playing a key role in personal and professional communication. Many research develop corpora to analyze social meaning in social media. For privacy and ethical consideration, the social media platform, e.g., Twitter, restricts the re-distribution of the user posts. The tweets usually are shared in dehydrated format, we thus have to download the content via API. However, the deletion and inaccessibility of social media posts leads to a big challenge of replicability in social media modeling. Hence, we introduce a new social meaning benchmark where provides paraphrases of original posts.     

% \begin{tcolorbox}
% \footnotesize
% \begin{itemize}[leftmargin=0pt]
%   \item Just got chased through my house with a bowl of tuna fish. \emojisick ing.  \vspace{-7pt}
%   \item I love waiting 2 hours to see 2 min. Of a loved family members part in a dance show \emojienutral \#sarcasm \vspace{-7pt}
%   \item USER Awww \emojidispoint\emojidispoint CUPCAKES SUCK IT UP. SHE LOST \emojianger\emojianger GET OVER IT \emojianger\emojianger
% \end{itemize}
% \end{tcolorbox}
In order to evaluate our methods, we present a social meaning benchmark composed of $15$ different datasets crawled from previous research sources. We perform an extensive series of methodical experiments directly targeting our proposed methods. Our experiments set new state-of-the-art (SOTA) in the supervised setting across different datasets. Moreover, our experiments reveal a striking capacity of our models in improving downstream task performance in few-shot and severely few-shot settings (i.e., as low as $1\%$ of gold data), and even the zero-shot setting on languages other than English (i.e., as evaluated on six different datasets from three languages in Section~\ref{sec:zfew}). %Since we face challenges in acquiring the different datsets, with only $73\%$ of the data being accessible, we paraphrase the benchmark and make it available to the community to facilitate future research. We refer to our new benchmark as the \textit{Social Meaning Paraphrase Benchmark (SMPB)} (Section~\ref{sec:smpb}). Motivated by occasional data inaccessibility for social media research, we also report successful models fine-tuned exclusively on paraphrase data (Section~\ref{sec:para_model}). 




To summarize, we make the following \textbf{contributions}:
 \textbf{(1)} We propose a novel pragmatic masking strategy that makes use of social media cues akin to improving social meaning detection. \textbf{(2)} We introduce a new effective surrogate fine-tuning method suited to social meaning that exploits the same simple cues as our pragmatic masking strategy. \textbf{(3)} We report new SOTA on eight out of $15$ supervised datasets in the full-data setting. \textbf{(4)} Our methods are remarkably effective for few-shot and zero- and learning. We now review related work.%\textbf{(4)} We introduce SMPB, a new social meaning detection benchmark comprising $15$ different datasets whose accessibility is enhanced via paraphrase technology. 
 
 %\footnote{We will release all our models and data upon acceptance.} %We now introduce our social meaning datasets and benchmark.
% \end{enumerate}

%The rest of the paper is organized as follows: We introduce our benchmark in Section~\ref{sec:smpb}.  Sections~\ref{sec:prag_masking} and ~\ref{sec:sft} present our pragmatic masking and surrogate fine-tuning methods, respectively. We compare our models to SOTA in Section~\ref{sec:comparisons} and show their effectiveness in zero- and few-shot settings in Section~\ref{sec:zfew}. Section~\ref{sec:para_model} presents our experiments with paraphrase-based models. We review related work in Section~\ref{sec:related_work} and conclude in Section~\ref{sec:conclude}.
