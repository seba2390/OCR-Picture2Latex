\vspace{-15pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{SFT Experiments}\label{subsec:sft_result}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \textit{\textbf{Surrogate fine-tuning with Emojis.}}
%--------------------------
We conduct SFT using hashtags and emojis. We continue training the original RoBERTa on the \texttt{Hashtag\_pred} and \texttt{Emoji\_pred} dataset for $35$ epochs and refer to these trained models as \textbf{SFT-H} and \textbf{SFT-E}, respectively. 
%\footnote{As an aside, the surrogate fine-tuned model achieves $32.84$ accuracy and $26.21$ macro $F_1$ on the emoji prediction Test set.} 
To evaluate SFT-H and SFT-E, we further fine-tune the obtained models on $15$ task-specific datasets. 
As Table~\ref{tab:sft_res} shows, SFT-E outperforms the first baseline (i.e., RoBERTa) with $1.16$ $F_1$ scores. Comparing SFT-E and PMLM trained with the same dataset (PM-EE), we observe that the two models perform similarly ($76.94$ for SFT-E vs. $76.96$ for PM-EE). Our proposed SFT-H method is also highly effective. On average, SFT-H achieves $2.19$ and $0.87$ $F_1$ improvement over our baseline (1) and (2), respectively. SFT-H also yields sizeable improvements on datasets with smaller training samples, such Irony\textsubscript{Hee-B} (improvement of $7.84$ $F_1$) and Sarc\textsubscript{Riloff} (improvement of $6.65$ $F_1$). Comparing SFT-H to the PMLM model trained with the same dataset (i.e., PM-HE), we observe that SFT-H also outperforms PM-H with $1.38$ $F_1$. This result indicate that SFT can more effectively utilize the information from tweets with hashtags.
% achieve the best performance of $6$ tasks and an average $F_1$ of $77.43$.

\input{table/sft_res}


% %but fails to surpass SFT-H and EA+SH. 
% We also fine-tune the best hashtag-based PLM, HE-PM, on \texttt{Emoji\_pred} dataset for $35$ epochs. %\footnote{The surrogate fine-tuned model achieves $33.19$ accuracy and $26.94$ macro $F_1$ on the Test set of \textt{Emoji\_pred}. \hl{Move these footnotes to appendix?}}
% This model is denoted HE+SE in Table~\ref{tab:mtl_res}. The HE+SE achieve the best performance of $6$ tasks and an average $F_1$ of $77.43$. 
 
% \textit{\textbf{Surrogate fine-tuning with hashtags.}}
%----------------------------------------
% We also perform SFT exploiting hashtags. we use the original RoBERTa model as our initial checkpoint and surrogate fine-tune on \texttt{Hashtag\_pred} for $35$ epochs.  %\footnote{The surrogate fine-tuned model achieves $61.24$ accuracy and $38.21$ macro $F_1$ on the Test set of \textt{Hashtag\_pred}.} 

% We then continue fine-tuning obtained models on each of the $15$ downstream datasets for evaluation. As Table~\ref{tab:sft_res} shows,  



%\footnote{The surrogate fine-tuned model achieves $61.25$ accuracy and $38.14$ macro $F_1$ on the Test set of \textt{Hashtag\_pred}.}  and continue fine-tuning EA+SH on the downstream tasks. As Table~\ref{tab:mtl_res} reveals, EA+SH achieves the best averaged macro $F_1$ (i.e., $78.12$). %and outperforms the SoTA pre-trained language model, BERTweet~\cite{nguyen-etal-2020-bertweet} with $1.00$ macro $F_1$.



%Same as downstream fine-tuning models, HP-FT pass the final hidden state of [CLS] token to a classification non-linear layer. We take model trained with 35 epochs~\footnote{The model is still training for 50 epochs. The model of 35th epoch is our latest and best model so far. } that achieves $61.24$ accuracy and $38.21$ macro $F_1$ on Test set of Hashtag\_pred. 


%that the HP-FT achieve significant improvement comparing with RoBERTa model. HP-FT outperforms RoBERTa with $2.22$ average macro $F_1$ over 15 tasks. Comparing with HE-P, HP-FT more effectively utilize the hashtag data with $1.48$ improvement. HP-FT 



%This suggests that fine-tuning on hashtag prediction as intermediate task is a more useful approach to adapt pre-trained LM to social media.

%\noindent\textbf{Hashtag Identification.}~~ To investigate the utility of token-level hashtag identification as intermediate task, we fine-tune RoBERTa on \texttt{Hashtag\_token} dataset for $30$ epochs and refer the fine-tuned model to \texttt{HT-FT}. HT-FT takes the final states of all tokens from Transformer layers and passes the hidden states to a non-linear layer of classification. The best HT-FT is fine-tuned with 30 epochs, which performs $98.14$ accuracy and $88.72$ macro $F_1$. HT-FT then is evaluated on $15$ downstream tasks. However, we observe that HT-FT obtain negative performance on $12$ tasks. The average macro $F_1$ is $75.59$.
\vspace{-5pt}
\subsection{Combining PM and SFT}\label{subsec:combine}
To further improve the PMLM with SFT, we take the best hashtag-based model (i.e., PM-HE in Table~\ref{tab:pmlm_res}) and fine-tune on \texttt{Emoji\_pred} (i.e., SFT-E) for $35$ epochs. We refer to this last setting as PM-HE+SFT-E but use the easier alias \textbf{PragS1} in Table~\ref{tab:sft_res}. We observe that PragS1 outperforms both, reaching an average $F_1$ of $77.43$ vs. $75.78$ for the baseline (1) and $76.94$ for SFT-E. Similarly, we also take the best emoji-based PMLM (i.e., PM-EA in Table~\ref{tab:pmlm_res}) and fine-tune on \texttt{Hashtag\_pred} SFT (i.e., SFT-H) for $35$ epochs. This last setting is referred to as PM-EA+SFT-H, but we again use the easier alias \textbf{PragS2}. Our best result is achieved with a combination of PM with emojis and SFT on hashtags (the PragS2 condition). This last model achieves an average $F_1$ of $78.12$ and is $2.34$ and $1.02$ average points higher than baselines of RoBERTa and BERTweet, respectively. 
 
