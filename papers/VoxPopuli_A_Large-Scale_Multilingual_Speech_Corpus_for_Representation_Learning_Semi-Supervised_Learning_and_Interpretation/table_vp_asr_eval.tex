\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{cr|c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c@{\hs{1.4}}c|@{\hs{1.2}}c}
    \toprule
    & & En & De & It & Fr & Es & Pl & Ro & Hu & Nl & Cs & Sl & Fi & Hr & Sk & Avg. $\downarrow$ \\
    \midrule
    Sup. & Dev & 30.1 & 29.0 & 41.6 & 28.6 & 27.4 & 27.1 & 28.5 & 27.4 & 35.7 & 27.8 & 95.7 & 45.7 & 44.9 & 30.2 &  37.1 \\
    baseline & Test & 30.0 & 29.3 & 45.2 & 30.5 & 31.4 & 25.6 & 27.7 & 27.9 & 38.3 & 27.7 & 96.5 & 41.6 & 40.2 & 32.7 & 37.5 \\
    \midrule
    VP-10K & Dev & 15.5 & 17.2 & 19.1 & 13.9 & 8.6 & 12.8 & 8.3 & 11.5 & 18.5 & 11.1 & 20.6 & 21.1 & 15.6 & 10.4 & 14.6 \\
    + FT & Test & 16.2 & 16.2 & 21.5 & 15.4 & 11.0 & 12.5 & 9.4 & 12.0 & 19.7 & 11.8 & 26.1 & 17.1 & 14.1 & 11.1 & 15.3 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{\vp~ASR baselines and in-domain unsupervised pre-training.} We report \vp~dev and test WER for languages with $\ge$10 hours of data. Top: supervised monolingual Transformer baselines. Bottom: wav2vec 2.0 \emph{Base} model pre-trained on 10K-hour \vp~unlabeled data (23 languages) and fine-tuned on \vp~ASR data. As we can see, pre-training with in-domain unlabeled data substantially improves performance especially for low-resource languages.}
    \label{tab:vp_asr_eval}
\end{table*}