\begin{table}[t]
\centering
\small
\begin{tabular}{r|c@{\hs{1.2}}c@{\hs{1.2}}c@{\hs{1.2}}|c@{\hs{1.2}}c@{\hs{1.2}}c}
\toprule
 & \multicolumn{3}{c|}{Train Hours} & \multicolumn{3}{c}{Test WER $\downarrow$} \\
  & De & Fr & Es & De & Fr & Es \\
\midrule
Baseline$^{\dagger}$ & 1582 & 787 & 660 & 12.8 & 19.4 & 16.5 \\
\midrule
VP-50K & 314 & 364 & 203 & 17.0 & 18.8 & 11.9 \\
+ LM & (20\%) & (46\%) & (31\%) & \textbf{7.8} & \textbf{9.6} & \textbf{10.0} \\
\bottomrule
\end{tabular}
\caption{\textbf{ASR with out-of-domain unsupervised pre-training and less supervision.} We report test WER on Common Voice (CV). Top: supervised baseline trained on the combination of an extended CV train set and several other corpora (decoding with LM). Bottom: our wav2vec 2.0 \emph{Base} model pre-trained on 50K-hour \vp~data (out-of-CV-domain) and fine-tuned on the standard CV train set (a subset of the baseline's one). We optionally use 4-gram LMs trained on CV for decoding. Our model outperforms the baseline (even without LM) while using less supervised train data. $^{\dagger}$Deepspeech Polyglot.}

\label{tab:wer_scores_main_cv}
\end{table}
