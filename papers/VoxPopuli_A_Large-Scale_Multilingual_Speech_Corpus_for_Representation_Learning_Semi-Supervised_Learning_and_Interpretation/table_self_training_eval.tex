\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{r|cc|cc|cc||cc|cc|cc}
    \toprule
     & \multicolumn{2}{c|}{Fr$\rightarrow$En $\uparrow$} & \multicolumn{2}{c|}{Es$\rightarrow$En $\uparrow$} & \multicolumn{2}{c||}{De$\rightarrow$En $\uparrow$} & \multicolumn{2}{c|}{Fr $\downarrow$} & \multicolumn{2}{c|}{Es $\downarrow$} & \multicolumn{2}{c}{De $\downarrow$}  \\
     \midrule
     Train hours (EP+CV) & \multicolumn{2}{c|}{38+264} & \multicolumn{2}{c|}{32+113} & \multicolumn{2}{c||}{42+184} & \multicolumn{2}{c|}{38+264} & \multicolumn{2}{c|}{32+113} & \multicolumn{2}{c}{42+184} \\
     Test set & EP & CV & EP & CV & EP & CV & EP & CV & EP & CV & EP & CV \\
    \midrule
    (Cascaded) Baseline$^{\dagger}$ & 25.4 & 27.6 & 26.5 & 27.4 & 21.3 & 21.0 & 24.3 & 18.3 & 15.0 & 21.4 & 19.8 & 16.0 \\
    Our end-to-end baseline & 24.5 & 27.0 & 20.5 & 26.6 & 17.5 & 20.0 & 20.8 & 18.8 & 17.2 & 14.1 & 23.2 & 18.4 \\
    With 800h self-training & 26.7 & 28.6 & 22.4 & 26.8 & 18.8 & \textbf{20.1} & 19.5 & 17.3 & 15.6 & 13.7 & 21.8 & 17.5 \\
    With 3000h self-training & \textbf{27.4} & \textbf{28.9} & \textbf{22.7} & \textbf{27.3} & \textbf{19.6} & 20.0 & \textbf{19.0} & \textbf{17.0} & \textbf{15.3} & \textbf{13.2} & \textbf{21.4} & \textbf{17.3} \\
    % + Iteration 2 & \textbf{26.8} & \textbf{29.1} & \textbf{20.9} & \textbf{27.1} & \textbf{18.9} & 21.2 & \textbf{18.7} & \textbf{16.7} & 16.6 & \textbf{13.7} & \textbf{22.2} & \textbf{17.3} \\
    \midrule
    % 300h weakly labeled & 12.9 & 6.1 & 17.7 & 9.9 & 10.1 & 4.7 & \\
    400h weakly labeled & 22.9 & 10.1 & 22.2 & 10.9 & 18.0 & 8.8 & \\
    % + labeled & \textbf{29.1} & \textbf{29.1} & \textbf{25.7} & \textbf{28.5} & \textbf{22.4} & \textbf{21.4} & \\
    + labeled & \textbf{31.1} & \textbf{30.3} & \textbf{28.4} & \textbf{29.7} & \textbf{24.4} & \textbf{23.4} & \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{ST and ASR using \vp~data for self-training or weak supervision.} Left: test BLEU for ST models. Right: test WER for ASR models. We evaluate in-\vp-domain performance with EuroParl-ST (EP) and the out-of-domain performance with CoVoST 2 (CV). We combine both corpora to train our baseline and pseudo-label 3K-hour monolingual \vp~unlabeled data for self-training.
    For ST training with weak supervision, we combine EP, CV and 300h weakly labeled data from \vp. Both approaches for leveraging \vp~data improve in-domain (EP) and out-of-domain (CV) performance simultaneously. $^\dagger$~EP baselines from \citet{iranzo2020europarl} and CV baselines from \citet{wang2020covost}.}
    \label{tab:st_self_training_eval}
\end{table*}
