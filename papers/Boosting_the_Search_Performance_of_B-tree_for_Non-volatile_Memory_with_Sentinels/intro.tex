\section{Introduction}
A conventional computer employs DRAM as main memory and hard disk as storage. 
The disk provides persistency but demands a long time to load and store data~\cite{b1}. 
DRAM is volatile and cannot reach large capacity because of its density limitation~\cite{lee2009architecting, qureshi2009scalable, zhou2009durable}. 
Recently, the next-generation non-volatile memory (NVM), such as phase-change memory (PCM)~\cite{raoux2008phase}, spin-transfer torque memory (STT-RAM)~\cite{kawahara2010scalable}, and resistive RAM (RRAM)~\cite{chang2016resistance}, emerges as a promising medium that combines both DRAM's byte-addressability and disk's persistency. Hence, researchers and practitioners have proposed to place NVM on the memory bus to build {\em persistent memory}, which acts
as a new tier of computer architecture and blurs the boundary between DRAM and disk.


Multiple classic data structures have been tuned for NVM. Among them 
B+-tree is one that has been widely used for databases and file systems since the era of disks~
\cite{bayer2002organization}. 
B+-tree is a multi-level structure composed of internal and leaf nodes. 
Internal nodes (INs) are used to index lower-level INs and LNs while leaf nodes (LNs) hold actual key-value (KV) pairs. 
INs and LNs have the same structure but the pointers of INs point to the address of LNs while the pointers of LNs point to the value data that stored in the B+-tree. And INs have one more pointer than LNs which indicates the leftmost LN of the internal nodes.
 Keys are sorted in both INs and LNs. When tuning B+-tree for NVM, 
 researchers need to consider the fact that CPU or memory controller may reorder writes from
 CPU cache to NVM \cite{yang2015nv, hwang2018endurable, wang2019circ}. One way to retain a writing order is to 
employ a combination of cache line flush and memory fence~\cite{moraru2013consistent}, which, however, incurs significant performance overheads. 
With further and further understanding of B+-tree's properties, state-of-the-art 
B+-trees designed for NVM
have evolved from being with sorted nodes (e.g., CDDS-Tree~\cite{venkataraman2011consistent}) to unsorted nodes (e.g., NV-Tree, wB+-tree, and FPTree~\cite{oukid2016fptree}), and again sorted nodes (e.g., FAST-FAIR and Circ-Tree). When developing such B+-tree variants, researchers
 mainly placed their emphasis on reducing the use of cache line flush and memory fence in order to achieve a high write performance for insertion and deletion. 

Nevertheless, the search operation with a target key for the key's corresponding value 
is of paramount importance for B+-tree. 
Search is not only an independent operation, but 
insertion and deletion also rely on a preceding search 
to proceed. 
As the latest FAST-FAIR and Circ-Tree are with sorted nodes, we focus on optimizing the search operation over sorted KV pairs stored in NVM.
It is revealed that linear search is more efficient than binary search over sorted keys although the latter has a theoretically lower time complexity.
The reason is that binary search incurs more cache misses while linear search facilitates CPU's branch prediction and prefetch.
Our aim of this paper is to further reduce cache misses in searching. We find that no matter if we use binary search or linear search,
we must access and check keys that are not the target one. Although accessing those keys leads us to continue the search, it is the cause of cache misses. 
This is the observation of {\em read amplification} in searching a B+-tree node. 
We intend to reduce as much read amplification as possible.
In other words, we want to timely find the exact cache line in which the target key resides. It is impossible except using some auxiliary information.
Previously, Oukid et~al.~\cite{oukid2016fptree} proposed to use 1B hash values as fingerprints for keys. 
A key and its fingerprint have the same offset. Searching a key transforms to finding the key's fingerprint that 
indicates the location of the key. However,
due to the collisions of 1B fingerprints and the calculation cost, fingerprinting is not efficient in practice. 

We closely study a sorted B+-tree node. We find that each cache line has a range of keys and all cache lines
exhibit a monotonically increasing trend. We can choose a {\em sentinel} key in a uniform fashion, e.g., the smallest or the largest key, 
from each cache line to reflect such a trend. When searching a target key, instead of scanning cache lines, we 
go through the Sentinel Array to determine which cache line the target key stays in. 
As a result, searching a target key converts to searching the Sentinel Array and one actual cache line of KV pairs.
The Sentinel Array contains representatives of cache lines of a B+-tree node and it thus
occupies much fewer cache lines. Also, a linear scan of it facilitates CPU's prefetch and branch prediction.
Consequently, employing the Sentinel Array significantly reduces read amplification. 

We have augmented FAST-FAIR and Circ-Tree with
the idea of Sentinel Array. Experimental results show that compared to the use of Sentinel Array boosts the search performance by up to 42.6\%
and 48.4\%, respectively, for FAST-FAIR and Circ-Tree. Moreover, using Sentinel Arrays is up to 60.1\% faster than using the idea of fingerprints.
Maintaining Sentinel Arrays, however, incurs at most 4.1\% and 6.5\% more time for insertions with
FAST-FAIR and Circ-Tree, respectively.

The rest of this paper is organized as follows. The background and motivation are discussed in Section 2. Section 3 shows the detailed design and implementation of Sentinel Array. The evaluation and results are presented in Section 4. In the last, we conclude this paper and look forward to future work.
