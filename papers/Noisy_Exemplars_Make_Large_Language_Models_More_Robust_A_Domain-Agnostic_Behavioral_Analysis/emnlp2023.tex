% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{array}
\usepackage{titlesec}
\usepackage{multirow}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{4pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{10pt plus 4pt minus 2pt}{3pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{10pt plus 4pt minus 2pt}{3pt plus 2pt minus 2pt}
\titlespacing\paragraph{0pt}{4pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}

\setlength{\textfloatsep}{4pt plus 4pt minus 2pt}

\renewcommand{\baselinestretch}{0.95}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xurl}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Constructing Comprehensive Domain-Agnostic Behavioral Testing for Large Language Models}
\title{Noisy Exemplars Make Large Language Models More Robust: \\ A Domain-Agnostic Behavioral Analysis}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Hongyi Zheng \\
  New York University \\
  \texttt{hz2212@nyu.edu} \\
  \And
  Abulhair Saparov \\
  New York University \\
  \texttt{as17582@nyu.edu} \\
  }

\begin{document}

\maketitle

\begin{abstract}
Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.
\end{abstract}

\section{Introduction}

Large language models (LLMs) achieve human-like performance on many natural language processing tasks after few-shot learning due to increasing scale \cite{kaplan2020scaling}. However, they often struggle in conducting multi-hop reasoning tasks after standard prompting \cite{DBLP:journals/corr/abs-2112-11446}. Recently, multiple prompt engineering methods such as chain-of-thought prompting \cite{wei2023chainofthought}, zero-shot prompting \cite{kojima2023large} and least-to-most-prompting \cite{zhou2023leasttomost} have led to significant empirical improvements in these tasks.

Despite these signs of progress, there is an important drawback in recent studies: the datasets used in these experiments are often idealized, noise-free, and rather distinct from examples that LLMs encounter in real applications, which put the generalizability of these prompting methods into question since applications of LLMs in practice are often noisy, containing errors, redundant or irrelevant sentences, utilizing out-of-distribution vocabulary, etc. For instance, \citet{cheng-etal-2018-towards} show that very small amount of common lexical perturbations such as word replacement and deletion could result in drastic change in machine translation results.

There are few existing studies investigating the robustness of these prompting schemes through behavioral experiments on perturbed examples. Thus in our research, we create a selection of domain-agnostic tests to investigate the robustness of state-of-the-art prompting methods. Our two main goals are: (1) to compare and contrast the performance of prompting methods with respect to various perturbations, and (2) to explore empirical approaches that may improve their robustness.\footnote{Our code is open source and available at \url{https://github.com/Hiroki39/Noisy-Exemplars-Make-Large-Language-Models-More-Robust}}

\begin{figure}[t]
    \centering\captionsetup{justification=centering}
    \includegraphics[width=0.5\textwidth]{images/perturb.png}
    \caption{A simple repetition may result in LLM producing incorrect solution.}
    \label{fig:perturb}
\end{figure}

\section{Related Work}

\begin{figure*}[t]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{images/overview2.png}
    \caption{Examples for each type of perturbation test. A more detailed example is shown in \autoref{table:exemp}.}
    \label{fig:overview}
\end{figure*}

\subsection{Prompt Engineering}

Contemporary prompt engineering methods that aim to improve LLMs' reasoning performance stem from chain-of-thought prompting (\textsc{CoT}) proposed by \citet{wei2023chainofthought}, which draws inspiration from the earlier work of \citet{ling-etal-2017-program} with the key idea of augmenting standard few-shot prompting with a chain-of-thought (i.e. a description of the reasoning steps that lead to the answer). \textsc{CoT} improves LLM's performance in a wide range of reasoning tasks. Nevertheless, recent research shows that it may lead to inconsistent reasoning steps and thus worse performance under certain circumstances \cite{ye2022the}, which highlights the necessity to conduct further behavioral analyses to identify such circumstances and find ways to mitigate this issue.

\textsc{CoT} inspired a few subsequent prompting techniques. Zero-shot prompting (\textsc{0CoT}) proposed by \citet{kojima2023large} requires significantly less human engineering to generate prompts compared with the original approach. Least-to-most prompting (\textsc{LtM}) proposed by \citet{zhou2023leasttomost} decomposes complex reasoning tasks into easier subproblems to improve the model performance. Selection-Inference prompting \cite{creswell2023selectioninference}, LAMBADA \cite{kazemi2023lambada}, and tree-of-thought prompting \cite{yao2023tree} further break down the problem so that the LLM is queried for each step of the reasoning. Lastly, self-consistency prompting \cite{wang2023selfconsistency} uses sampling and aggregation techniques to diversify reasoning paths and increase the chance of deriving correct answers.

These methods have been shown to be effective in increasing model accuracy under noise-free environments. Our work, on the other hand, focuses on investigating the robustness of these methods when a variety of perturbations are present.

\subsection{Behavioral Testing}

The concept of behavioral testing (also known as black-box testing) is first proposed by \citet{536464} as an effective approach to probing large software or computer systems. \citet{ribeiro-etal-2020-beyond} brought many of these insights to the testing of NLP models and proposed \textsc{CheckList}, a comprehensive domain-agnostic methodology that embraces the benefits of challenge sets such as systematic control of test examples \cite{belinkov-glass-2019-analysis} while avoiding their drawbacks such as the fact that challenge sets are artificial and do not resemble real data.

Recent behavioral analysis of LLMs has revealed their deficiencies when handling domain-specific perturbations. LLMs are shown to be sensitive to domain-specific perturbations in reading comprehension \cite{jia-liang-2017-adversarial}, text classification \cite{gan2023sensitivity}, as well as logical reasoning tasks \cite{ye2022unreliability}. Some tests investigate LLM consistency via replacing real concept nouns with fictional ones \cite{saparov2023language} and adding semantically related yet logically irrelevant distractor sentences \cite{saparov2023testing}. While those studies mainly focus on domain-specific perturbations, or on synthetic settings, our work evaluates the prompt engineering methods' robustness against \emph{domain-agnostic} perturbations applied to more realistic data, across more levels of abstraction, which are more widespread in the practical use of LLMs.

\begin{figure*}[t]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth]{images/simple_perturb2.png}
    \caption{Perturbations on the test question vs accuracy under various combinations of datasets and prompting approaches. 95\% confidence intervals are shown.}
    \label{fig:simple_perturb}
\end{figure*}

\section{Method}\label{sec:tests}

We aim to conduct domain-agnostic analysis with tests that span multiple levels of abstraction, are easy to automate, while still closely resembling examples in real applications. Thus, we use the following four perturbation tests:

\noindent \textbf{\texttt{Typo}}. We test whether the model's output is sensitive to typing errors in its input by introducing typos. To be robust to this perturbation, LLMs cannot rely on copying problem sentences with repeated words to produce reasoning chains. Typos are introduced by randomly swapping one character with its adjacent character within a token with probability $0.1$ given that the token has more than one character and is not numeric.

\noindent \textbf{\texttt{Synonym}}. We test whether models could recognize semantically similar tokens that refer to the same object of interest by replacing some words with synonyms. To be robust to this perturbation, LLMs should not exploit lexical spurious correlations. Operationally, every noun and verb token is replaced by one of its synonyms within its WordNet \cite{miller-1992-wordnet} synsets with probability $0.2$.\footnote{higher than \textbf{\texttt{Typo}} probability to ensure the number of perturbed tokens is comparable with \textbf{\texttt{Typo}} test}

\noindent \textbf{\texttt{Repetition}}. We also test whether models are robust to relevant but redundant information by duplicating a sentence in the input. To be robust to this perturbation, the model must ignore the redundant sentence, or utilize it to complete the CoT. Operationally, we randomly choose a sentence within the problem text other than the question sentence (i.e. the last sentence), and insert a copy of it before the last sentence. This minimizes the risk of breaking coreference links \cite{jia-liang-2017-adversarial}.

\noindent \textbf{\texttt{Shortcut}}. We test whether model behavior is affected if an intermediate result is given in the question description. The LLM may take advantage of this extra information to expedite reasoning. Conversely, the LLM could also be confused, as it would serve as redundant information if the LLM first ignored this directly given intermediate result but later derived it. Operationally, we extract the first reasoning step for the problem (first hop for \textsc{CoT} and \textsc{0CoT}; first sub-problem and its corresponding answer for \textsc{LtM}) and insert it before the question sentence.

\section{Experimental Setup}

\subsection{Model and Dataset}

The GSM8K \cite{cobbe2021training} and StrategyQA \cite{geva-etal-2021-aristotle} datasets are used for all experiments. The relatively simple problem statements in these datasets facilitate the creation of adversarial examples for testing. The two datasets provide the intermediate reasoning steps required by the few-shot prompting methods. For the choice of LLMs, GPT-3.5-Turbo \cite{DBLP:journals/corr/abs-2005-14165}, LLaMA2 7B and LLaMA2 13B models are used in our experiments. All models are open source with weights available for public use.

\subsection{Prompting Methods}

We experiment with \textsc{CoT}, \textsc{0CoT}, and \textsc{LtM}. Our prompt design closely follows \citet{zhou2023leasttomost}, 
\citet{kojima2023large}, and \citet{shi2023large}. For few-shot prompting techniques (i.e. \textsc{CoT} and \textsc{LtM}), each input prompt includes either one or eight exemplar problems as well as their corresponding solution, followed by the test problem. The exemplars are created from the problems within GSM8K either with or without perturbations to allow us better understand how perturbing the exemplars would affect the performance of the LLMs in answering the test problem. For \textsc{0CoT}, the test question is directly presented without exemplars but instead followed by "A: \textit{Let's think step by step}". To facilitate answer extraction, a high-level instruction, "\textit{End your response with 'The answer is <answer>'}" is prepended to every prompt. We do not apply \textsc{LtM} prompting for StrategyQA as it does not provide high-quality decompositions for each question, which are required by \textsc{LtM} prompting.

\section{Results}

We carry out two experiments. In the first experiment, we evaluate the effect of perturbations of the test question on LLM performance under different prompting methods. In the second experiment, we perturb exemplar questions and vary the proportion of perturbed exemplars for few-shot prompting techniques to investigate whether an increasing share of perturbed exemplars would lead to better robustness against the perturbations in the test question.

\subsection{Do Perturbations in the Test Question Affect Prompting Performance?}\label{sec:simple_perturb}

\begin{figure*}[t]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{images/perturb_shots2.png}
    \caption{Number of perturbed in-context exemplars vs accuracy for GSM8K trials. Note that the total number of in-context exemplars is always 8. Dashed red lines indicate the accuracy of \textsc{0CoT} in \autoref{sec:simple_perturb}. 95\% confidence intervals are shown.}
    \label{fig:perturb_shots}
\end{figure*}

\begin{figure*}[t]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{images/perturb_shots3.png}
    \caption{Number of perturbed in-context exemplars vs accuracy for StrategyQA trials. Note that we do not apply \textsc{LTM} on StrategyQA and all bars represent \textsc{CoT} results. Dashed red lines indicate the accuracy of \textsc{0CoT} in \autoref{sec:simple_perturb}. 95\% confidence intervals are shown.}
    \label{fig:perturb_shots2}
\end{figure*}

In this experiment, we investigate the effect of perturbations on prompting methods with original unperturbed exemplars. For \textsc{CoT} and \textsc{LtM} trials, one unperturbed question and its associated reasoning process are provided as the exemplar, followed by the perturbed test question, whereas no exemplar is provided for \textsc{0CoT} trials. The results are illustrated in \autoref{fig:simple_perturb}. We draw a few conclusions from the results:

\noindent\textbf{In general, prompting methods are most susceptible to \texttt{Synonym} replacement}. When conducting the \textbf{\texttt{Synonym}} tests, the accuracy of the LLM falls below the baseline performance across trials. For instance, for GPT-3.5-Turbo on the GSM8K dataset, \textbf{\texttt{Synonym}} perturbation lowers the accuracy by $0.097 \sim 0.122$ (green bars in the top-left subplot of \autoref{fig:simple_perturb}) as compared to the baseline (blue bars), which are larger decreases as compared to the \textbf{\texttt{Typo}} ($0.034 \sim 0.044$, orange bars) and \textbf{\texttt{Repetition}} tests ($0.046 \sim 0.062$, red bars). This might result from low-frequency phrases and sentences created by the \textbf{\texttt{Synonym}} perturbation, as every token is substituted with its synonym regardless of the adjacent tokens. Many substitutions would likely create grammatically correct yet infrequent sentences and phrases (e.g. ``She eats three \textit{repasts} per day'') which might lead to worse performance.

% \noindent\textbf{Least-to-most prompting method performs poorly under shortcut perturbation}. According to the results, the performance of \textbf{LTM} dropped sharply ($-0.246$) when the \textbf{Shortcut} perturbation is applied (purple bars in Figure \ref{fig:simple_perturb}). In contrast, under \textbf{0CoT} and \textbf{CoT} prompting schemes, the LLM is able to take advantage of the extra information provided and achieves better performance ($+0.008$ and $+0.004$ respectively). Such divergence between \textbf{LTM} and other prompting methods might stem from the difference in reasoning structure between \textbf{CoT} and \textbf{LTM}: the "hint" provided in the \textbf{CoT} and \textbf{0CoT} trials are the first reasoning step which is usually a plain statement. Nevertheless, for \textbf{LTM} method, the "hint" provided involves a pair of sentences: one question sentence describing the sub-problem, followed by a statement that answers the sub-problem. Although it seems that the latter setting provides more extra information, it consists of a sub-problem question sentence which LLM sometime will confuse with the main question sentence that it is supposed to answer. That is, LLM will need to correctly recognize that the sub-problem question sentence is not supposed to be a question it needs to answer, but rather a component of the hint, which implicitly increases the difficulty level of the task. \\


% \noindent \textbf{Since \textsc{0CoT} has no few-shot exemplars, it is more robust to perturbations in the test example}. We could observe that \textsc{0CoT} performs consistently better than all other methods with non-trivial accuracy margins ranging from $0.016$ to $0.270$ across all perturbation tests. We hypothesize that perturbation-free exemplars result in the LLM adapting to the clean and errorless context, thus impairing its robustness to perturbations in test questions.

\subsection{Do Perturbations in the Exemplars Affect Few-shot Prompting Performance?}\label{sec:perturbprompt}

Following the previous experiment, a question of interest naturally arises: would perturbing the exemplars during the few-shot learning process improves the robustness of few-shot prompting methods? To answer this question, we conduct another experiment for two few-shot prompting methods \textsc{CoT} and \textsc{LtM}. In each trial, $8$ exemplars are presented to the LLM, with a proportion of them ($0$, $1$, $2$, $4$, or $8$ out of $8$) being perturbed. Then, the perturbed test question is presented to the LLM. The results are shown in \autoref{fig:perturb_shots} and \autoref{fig:perturb_shots2}. We find the following key observations:

\noindent\textbf{Increasing the proportion of perturbed exemplars improves few-shot prompting performance, except for the \texttt{Typo} perturbation.} From the results, the accuracy for both few-shot prompting methods trends upwards when the number of perturbed exemplars presented to the LLM increases aside from LLaMA2-13B on the shortcut perturbation, demonstrating the evidence that the LLM is able to adapt to the perturbations through the few-shot learning process. Take GPT-3.5-Turbo on GSM8K dataset as an example, if excluding trials with the \textbf{\texttt{Typo}} perturbation, increasing the number of perturbed exemplars from $0$ to $8$ (lightest bars versus darkest bars in the third row of \autoref{fig:perturb_shots}) results in an average of $0.035$ increase in accuracy, with the minimum increase of $0.011$ and the maximum of $0.114$. The performance of the LLM in many trials surpasses the benchmark set by the \textsc{0CoT} method in \autoref{sec:simple_perturb} with the maximum advantage of $0.048$ achieved by \textsc{CoT} after few-shot learning with all eight exemplars with the \textbf{\texttt{Synonym}} perturbation. \textbf{\texttt{Typo}} trials turn out to be the exception, where increasing the number of perturbed exemplars does not lead to an improvement in accuracy. We speculate that this results from the fact that typos are much more common in the pretraining dataset as compared to other perturbations, which offsets the benefits of few-shot learning on perturbed exemplars.

This experiment suggests that perturbing in-context exemplars may serve as a more efficient alternative to augmenting pretraining with perturbations. It is much cheaper and easier to introduce perturbed exemplars at inference time rather than in pretraining. %Compared with pretraining the LLM with the augmented dataset, introducing perturbed exemplars at the inference stage to improve model robustness is more computationally friendly and easier to implement.

\section{Discussion and Future Work}

In conclusion, through two experiments, we conducted an investigation of the robustness of state-of-the-art prompting schemes via a series of domain-agnostic perturbation tests. Our first experiment revealed the robustness of \textsc{0CoT} prompting versus few-shot prompting methods when the few-shot exemplars are unperturbed. Our second experiment demonstrated that perturbing few-shot exemplars led to notable improvements in robustness to perturbations in the test question, which is valuable in real applications. Suppose a user knows in advance that the questions of interest will be subject to some certain kind of perturbation (e.g. mathematics problems scraped from the online forums which are subject to typos and uneliminated HTML tags, or questions asked by beginner-level English speakers that may contain grammatical errors). The user can then prepend exemplars perturbed in a similar fashion to improve the robustness of the LLM on their test examples. This can easily be applied in real-world use cases as all perturbations discussed above are easily automated. % serve as an efficient approach to improve accuracy. 

There are a number of directions for future work available: the first is to explore the effect of more comprehensive perturbations, such as presenting the LLM with a problem statement containing multiple possible lines of reasoning that each lead to the correct conclusion, or adding some semantically related but logically irrelevant extra information to the problem statement \cite{shi2023large}. Another direction is to explore the influence of several other variables on the robustness of the prompting methods, such as the model size, the number of hops required to answer the problem, whether the problem is counterfactual, etc. Lastly, we observe that in some cases, though the LLM is able to produce the correct answer under perturbation, the model is more likely to produce sentences with errors as shown in \autoref{table:exemp} (e.g. ``Janet lay $16$ eggs per day''). Further studying the relationship between perturbations in the model's input and the errors in the output would be illuminating. %How perturbing the exemplars and test questions would influence the sentence quality in the model output remains an issue worth investigating.
%Thus, further work on behavioral analysis of prompting schemes are expected to produce a number of valuable results that contributes to our understanding of these increasingly capable transformer-based language models.

\section*{Limitations}
We acknowledge several limitations of our work. Firstly, due to the constraints in computational power, available time, and budget, we only experiment with a relatively small set of LLMs and datasets, and our results could benefit from experiments on an increased number of models and datasets with more diverse tasks. Secondly, some perturbations might introduce unintended side effects. For instance, in the \textbf{\texttt{Repetition}} test, adding repeated information might not always be idempotent, as repeating sentences like \textit{A man walked into the bar} might alter the correct answer to the problem. These nuances and subtleties might require more detailed manual inspection.

\section*{Reproducibility Statement}
In our experiments, a fixed random seed (42) is applied over all trials to ensure the reproducibility of perturbed questions and exemplars, and the sampling temperature is set to $0$ so that the model behavior is as deterministic as possible. The experiment output files as well as the analysis codes are available in our GitHub repository (\url{www.github.com/Hiroki39/Noisy-Exemplars-Make-Large-Language-Models-More-Robust}).

% \section*{Acknowledgements}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\onecolumn
\appendix

\section*{Appendices}
% \section{Anonymous GitHub Link}
% \label{sec:code}

\section{Test Question and Model Output Example}

\begin{center}
\small
\renewcommand{\arraystretch}{1.5}
\begin{longtable}{| p{3.8cm} | p{3.2cm} | p{3.2cm} | p{4.1cm} |}
    \hline\label{table:exemp}\multirow{2}{*}{\hspace{0.8cm}\textbf{Input Question}}  & \multicolumn{3}{p{11.2cm}|}{\textbf{\hspace{4.5cm} Model Output}} \\ \cline{2-4}
    &
    \textbf{\hspace{1.3cm}\textsc{CoT}} & \textbf{\hspace{1.3cm}\textsc{0CoT}} & \textbf{\hspace{1.5cm}\textsc{LtM}} \\ \hline
    \textbf{Original} \newline
    Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for \$2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
    &
    \newline Janet uses 3+4=7 eggs every day for breakfast and muffins. So she has 16-7=9 eggs left to sell at the farmers' market. She makes 9*2=\$18 every day at the farmers' market. The answer is 18.
    &
    \newline Janet has 16 eggs per day. She eats 3 for breakfast and uses 4 for muffins, which leaves her with 9 eggs. She sells these 9 eggs at \$2 per egg, which means she makes \$18 per day at the farmers' market. The answer is \$18.
    &
    \newline How many eggs does Janet use for breakfast and muffins every day? Janet uses 3+4=7 eggs every day. How many eggs does she have left to sell? So she has 16-7=9 eggs left to sell every day. How much money does she make selling the eggs? Selling each egg for \$2, she makes 9*2=\$18 every day at the farmers' market. The answer is 18.
    \\
    \hline
    \textbf{Typo} \newline
    Janet's ducks \textcolor{red}{aly} 16 eggs per day. She eats three for breakfast every \textcolor{red}{omrning} and bakes \textcolor{red}{muffnis} for her friends every day with four. She sells the remainder at the farmers' market daily \textcolor{red}{ofr} \$2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
    &
    \newline Janet uses 3 eggs for breakfast and 4 for muffins, so she uses 3+4=7 eggs per day. That means she has 16-7=9 eggs left to sell at the farmers' market. Selling each egg for \$2, she makes 9*2=\$18 every day at the farmers' market. The answer is 18.
    &
    \newline Jante has 16 eggs per day, she eats 3 for breakfast and uses 4 for muffins, which leaves her with 9 eggs. She sells these 9 eggs at \$2 per egg, which means she makes \$18 every day at the farmers' market. The answer is \$18.
    &
    \newline How many eggs does Janet use for breakfast every day? Janet uses 3 eggs for breakfast every day. How many eggs does Janet use for muffins every day? Janet uses 4 eggs for muffins every day. How many eggs does Janet have left to sell? Janet has 16 - 3 - 4 = 9 eggs left to sell. How much money does Janet make every day at the farmers' market? Janet makes 9 x \$2 = \$18 every day at the farmers' market. The answer is \$18. \\
    \hline
    \textbf{Synonym} \newline
    Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the \textcolor{red}{residue} at the farmers' market daily for \$2 per fresh duck egg. How much in dollars does she make every \textcolor{red}{sidereal day} at the farmers' market?
    &
    \newline Janet lays 16 eggs per day, and she eats 3 + 4 = 7 eggs per day. So, she has 16 - 7 = 9 eggs left to sell at the farmers' market. She makes 9 x \$2 = \$18 every day at the farmers' market. The answer is 18.
    &
    \newline Janet has 16 duck eggs per day. She eats 3 for breakfast and bakes muffins with 4, which leaves her with 9 eggs. She sells the remaining 9 eggs at the farmers' market for \$2 each, which means she makes \$18 per day. The answer is \$18.
    &
    \newline How many orchids does Janet have left after breakfast and baking muffins? Janet has 16 - 3 - 4 = 9 orchids left. How much money does she make from selling these orchids? She makes 9 x \$2 = \$18 from selling the orchids. How much money does Janet make every day at the farmers' market? So Janet makes \$18 every day at the farmers' market. The answer is 18.
    \\
    \hline
    \textbf{Repetition} \newline
    Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for \$2 per fresh duck egg. \textcolor{red}{Janet's ducks lay 16 eggs per day.} How much in dollars does she make every day at the farmers' market?
    &
    \newline Janet uses 3+4=7 eggs every day for breakfast and muffins. So she has 16-7=9 eggs left to sell at the farmers' market. She makes 9*2=\$18 every day at the farmers' market. The answer is 18.
    &
    \newline Janet has 16 eggs per day. She eats 3 for breakfast and uses 4 for muffins, which leaves her with 9 eggs. She sells these 9 eggs at \$2 per egg, which means she makes \$18 every day at the farmers' market. The answer is \$18.
    &
    \newline How many eggs does Janet use for breakfast and muffins every day? Janet uses 3+4=7 eggs every day. How many eggs does she have left to sell? So she has 16-7=9 eggs left to sell every day. How much money does she make selling the eggs? Selling each egg for \$2, she makes 9*2=\$18 every day at the farmers' market. The answer is 18.
    \\
    \hline
    \textbf{Shortcut} \newline
    Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for \$2 per fresh duck egg. \textcolor{red}{Janet sells 16 - 3 - 4 = 9 duck eggs a day.} How much in dollars does she make every day at the farmers' market?
    &
    \newline Janet sells 9 duck eggs a day, so she makes 9 x $2 = $18 every day at the farmers' market. The answer is \$18.
    &
    \newline Janet sells 9 duck eggs a day, so she makes 9 x \$2 = \$18 every day at the farmers' market. The answer is \$18.
    &
    \newline How many eggs does Janet sell? Janet sells 9 duck eggs a day. How much in dollars does she make every day at the farmers' market? She makes 9 x $2 = $18 every day at the farmers' market. The answer is 18.
    \\
    \hline
    \captionsetup{justification=centering}
    \caption{An example of different perturbations and corresponding model outputs. The first column illustrates the original and perturbed example test question and the next three columns show the model outputs with \textsc{CoT}, \textsc{0CoT}, and \textsc{LtM} respectively.}
\end{longtable}
\end{center}

\end{document}
