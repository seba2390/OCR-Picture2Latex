%%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
\pagebreak
\begin{center}
\textbf{\Large Supplementary Material: Flexible Bayesian Modeling for Longitudinal Binary and Ordinal Responses}
\end{center}
%%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\bibnumfmt}[1]{[S#1]}
\renewcommand{\citenumfont}[1]{S#1}
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%


\section{MCMC posterior simulation details}
\label{sec:smmcmcdetail}

Based on the joint posterior distributions derived from (\ref{eq:jointpostmodelbin}), we design the MCMC sampling algorithm for the proposed model with binary responses. This process can be achieved entirely with Gibbs updates, by iterating the following steps. For notation simplicity, we let $(\phi\mid -)$ denote the posterior full conditional distribution for parameter $\phi$. 

\begin{description}
   \item[Step 1:] For $i=1,\cdots,n$ update $\boldsymbol{\mathcal{Z}}_i$ from $N(\mathbf{m}_i,\boldsymbol{\mathcal{V}}_i)$, where $\boldsymbol{\mathcal{V}}_i=(\Omega_i+(1/\sigma_{\epsilon}^2)\mathbf{I})^{-1}$, and $\mathbf{m}_i=\boldsymbol{\mathcal{V}}_i(\boldsymbol{\lambda}_i+(1/\sigma_{\epsilon}^2)\mathbf{Z}_i)$. Here $\Omega_i$ denote the diagonal matrix of $\boldsymbol{\xi}_i$, and $\boldsymbol{\lambda}_i=(Y_{i1}-1/2,\cdots,Y_{iT_i}-1/2)^{\top}$.   
   
   \item[Step 2:] Update the PÃ³lya-Gamma random variables $\xi_{it}$ by sample from $PG(1,\mathcal{Z}_{it})$, for $i=1,\cdots,n$ and $t=1,\cdots,T_i$. 
   
   \item[Step 3:]  Update $\sigma_{\epsilon}^2$ by sample from $IG(a_{\epsilon}+\sum_{i=1}^nT_i/2,b_{\epsilon}+\sum_{i=1}^n(\boldsymbol{\mathcal{Z}}_i-\mathbf{Z}_i)^{\top}(\boldsymbol{\mathcal{Z}}_i-\mathbf{Z}_i)/2)$.
   
   \item[Step 4:] Update $\tilde{\mathbf{Z}}_i$ for $i=1,\cdots,n$,
   \begin{itemize}
   	\item In the case that all the subjects having observations on a common grid, $\mathbf{Z}_i^*$ vanishes and $\tilde{\mathbf{Z}}_i=\mathbf{Z}_i$. It has full conditional distribution $\mathbf{Z}_i\mid -\sim N(\tilde{\boldsymbol{\mu}}_i,\tilde{\mathbf{V}}_i)$, where $\tilde{\mathbf{V}}_i=((1/\sigma_{\epsilon}^2)\mathbf{I}+\boldsymbol{\Sigma}^{-1})^{-1}$, and $\tilde{\boldsymbol{\mu}}_i=\tilde{\mathbf{V}}_i((1/\sigma_{\epsilon}^2)\boldsymbol{\mathcal{Z}_i}+\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu})$.
   
   	\item  In the case that the repeated measurements for the subjects are collected on uncommon grids, we first update $\mathbf{Z}_i^*$ from $N(\boldsymbol{\mu}_i^*,\boldsymbol{V}_i^*)$, where
   	\begin{equation*}
\begin{split}
&\boldsymbol{\mu}_i^*=\mu(\boldsymbol{\tau}^*_i)+\Sigma(\boldsymbol{\tau}^*_i,\boldsymbol{\tau}_i)\Sigma(\boldsymbol{\tau}_i,\boldsymbol{\tau}_i)^{-1}(\mathbf{Z}_i-\mu(\boldsymbol{\tau}_i))=\mathbf{B}_i\mathbf{Z}_i-\mathbf{u}_i,\\
&\boldsymbol{V}_i^*=\Sigma(\boldsymbol{\tau}^*_i,\boldsymbol{\tau}^*_i)-\Sigma(\boldsymbol{\tau}^*_i,\boldsymbol{\tau}_i)\Sigma(\boldsymbol{\tau}_i,\boldsymbol{\tau}_i)^{-1}\Sigma(\boldsymbol{\tau}_i,\boldsymbol{\tau}^*_i),
\end{split}
\end{equation*}
with $\mathbf{B}_i=\Sigma(\boldsymbol{\tau}^*_i,\boldsymbol{\tau}_i)\Sigma(\boldsymbol{\tau}_i,\boldsymbol{\tau}_i)^{-1}$ and $\mathbf{u}_i=\mathbf{B}_i\mu(\boldsymbol{\tau}_i)-\mu(\boldsymbol{\tau}^*_i)$.

Then, to update $\mathbf{Z}_i$, we sample from $N(\tilde{\boldsymbol{\mu}}_i,\tilde{\mathbf{V}}_i)$, where
\begin{equation*}
\begin{split}
&\tilde{\mathbf{V}}_i=[(1/\sigma_{\epsilon}^2)\mathbf{I}+\Sigma(\boldsymbol{\tau_i},\boldsymbol{\tau_i})^{-1}+\mathbf{B}_i^T(\boldsymbol{V}_i^*)^{-1}\mathbf{B}_i]^{-1},\\
&\tilde{\boldsymbol{\mu}}_i=\tilde{\mathbf{V}}_i[(1/\sigma_{\epsilon}^2)\boldsymbol{\mathcal{Z}}_i+\Sigma(\boldsymbol{\tau_i},\boldsymbol{\tau_i})^{-1}\mu(\mathbf{\tau}_i)+\mathbf{B}_i^T(\boldsymbol{V}_i^*)^{-1}(\mathbf{u}_i+\mathbf{Z}_i^*)].
\end{split}
\end{equation*}
   \end{itemize}
 
   \item[Step 5:] Update $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ jointly by sample from $N(\boldsymbol{\mu}^*,\boldsymbol{\Sigma}/\kappa^*)$ and $IW(\nu^*,\boldsymbol{\Psi}^*)$, respectively, with
   \begin{equation*}
\begin{split}
&\boldsymbol{\mu}^*=\frac{\kappa}{\kappa+n}\boldsymbol{\mu}_0+\frac{n}{\kappa+n}\tilde{\mathbf{Z}}^m,\quad
\kappa^*=n+\kappa,\quad \nu^*=n+\nu\\
&\boldsymbol{\Psi}^*=\boldsymbol{\Psi}+S+\frac{n\kappa}{n+\kappa}(\tilde{\mathbf{Z}}^m-\boldsymbol{\mu}_0)(\tilde{\mathbf{Z}}^m-\boldsymbol{\mu}_0)^T,\quad
S=\sum^{n}_{i=1}(\tilde{\mathbf{Z}}_i-\tilde{\mathbf{Z}}^m)(\tilde{\mathbf{Z}}_i-\tilde{\mathbf{Z}}^m)^{top},
\end{split}
\end{equation*}
where $\tilde{\mathbf{Z}}^m$ denote the mean of $\{\tilde{\mathbf{Z}}_i\}_{i=1}^n$.
 
   \item[Step 6:] Update $\mu_0$ from $N(a_{\mu}^*,b_{\mu}^*)$, where $b_{\mu}^*=[\mathbf{1}^{\top}[(\nu-3)\boldsymbol{\Sigma}]^{-1}\mathbf{1}+\frac{1}{b_{\mu}}]^{-1}$, and $a_{\mu}^*=b_{\mu}^*[\mathbf{1}^{\top}[(\nu-3)\boldsymbol{\Sigma}]^{-1}\boldsymbol{\mu}+\frac{a_{\mu}}{b_{\mu}}]$.
   
   \item[Step 7:] Update $\sigma^2$ from $\text{Gamma}(a_{\sigma}+\frac{(\nu+|\boldsymbol{\tau}|-1)|\boldsymbol{\tau}|}{2},b_{\sigma}+\frac{1}{2}tr(\boldsymbol{\Psi}_{\rho}\boldsymbol{\Sigma}^{-1}))$. Here $\boldsymbol{\Psi}_{\rho}$ denotes the correlation matrix $\boldsymbol{\Psi}_{\boldsymbol{\phi}}/\sigma^2$.
   
   \item [Step 8:] Using the Griddy-Gibbs sampler by \citet{Ritter1992}, update $\rho$ from 
   \begin{equation*}
   	P(\rho=c_l\mid -)=\frac{|\boldsymbol{\Psi}_{c_l}|^{(\nu+|\boldsymbol{\tau}|-1)/2}\exp(-\frac{1}{2}tr(\boldsymbol{\Psi}_{c_l}\boldsymbol{\Sigma}^{-1}))}{\sum_{l=1}^G|\boldsymbol{\Psi}_{c_l}|^{(\nu+|\boldsymbol{\tau}|-1)/2}\exp(-\frac{1}{2}tr(\boldsymbol{\Psi}_{c_l}\boldsymbol{\Sigma}^{-1}))},
   \end{equation*}
   where $c_1,\cdots,c_G$ are grid points on a plausible region of $\rho$ and $\boldsymbol{\Psi}_{c_l}$ denotes the correlation matrix when $\rho$ taking the value $c_l$.
   
   \item [Step 9:] Using the Griddy-Gibbs sampler, update $\nu$ from
   \begin{equation*}
   	P(\nu=c_l\mid -)=\frac{N(\boldsymbol{\mu}\mid\boldsymbol{\mu}_0,(c_l-3)\boldsymbol{\Sigma})IW(\boldsymbol{\Sigma}\mid c_l+|\boldsymbol{\tau}|-1,\boldsymbol{\Psi}_{\boldsymbol{\phi}})}{\sum_{l=1}^GN(\boldsymbol{\mu}\mid\boldsymbol{\mu}_0,(c_l-3)\boldsymbol{\Sigma})IW(\boldsymbol{\Sigma}\mid c_l+|\boldsymbol{\tau}|-1,\boldsymbol{\Psi}_{\boldsymbol{\phi}})}.
   \end{equation*}
 where $c_1,\cdots,c_G$ are grid points on a plausible region of $\nu$.
 
 \end{description}

\section{Proofs}

\subsection*{Proof of Proposition \ref{prop:meancovcondsignal}}
\label{subsec:proofmeancovcond}

\begin{proof}
For the probability response curve $\mathbf{P}_{\mathbf{1}\boldsymbol{\tau}}$, we have 
\begin{equation*}
    \begin{split}
       \mathbf{P}_{\mathbf{1}\boldsymbol{\tau}}&=\int (\text{Pr}(Y_{\tau_1}= 1 \mid\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}},\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2),\cdots,\text{Pr}(Y_{\tau_T}=1 \mid\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}},\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2))^{\top}p(\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2) \, d\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}\\
       &=\int \boldsymbol{\pi}(\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}})N(\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2\mathbf{I}) \, d\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}=\text{E}(\boldsymbol{\pi}(\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}})\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2).
    \end{split}
    \label{eq:probcurvecondsignal}
\end{equation*}

 
Then, to find the diagonal and off-diagonal elements for the covariance matrix 
of $\mathbf{Y}_{\boldsymbol{\tau}}$, we use the law of total variance/covariance. 
For the diagonal elements, we can write
\begin{equation*}
    \begin{split}
        \text{Var}(Y_{\tau}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)&=\text{Var}[\text{E}(Y_{\tau}\mid \mathcal{Z}_{\boldsymbol{\tau}})\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]+\text{E}[\text{Var}(Y_{\tau}\mid \mathcal{Z}_{\boldsymbol{\tau}})\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]\\
        &=\text{Var}[\varphi(\mathcal{Z}_{\tau})\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]+\text{E}[\varphi(\mathcal{Z}_{\tau})(1-\varphi(\mathcal{Z}_{\tau}))\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]\\
        &=\text{E}[\varphi(\mathcal{Z}_{\tau})\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]-\text{E}^2[\varphi(\mathcal{Z}_{\tau})\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2].
    \end{split}
    \label{eq:varcondsignal}
\end{equation*}
Similarly, for the off-diagonal entries, we obtain
\begin{equation*}
    \begin{split}
        \text{Cov}(Y_{\tau},Y_{\tau^{\prime}}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)&=\text{Cov}[\text{E}(Y_{\tau}\mid \boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}),\text{E}(Y_{\tau^{\prime}}\mid \boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}})\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]+\text{E}[\text{Cov}(Y_{\tau},Y_{\tau^{\prime}}\mid \boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}})\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2]\\
        &=\text{Cov}[\varphi(\mathcal{Z}_{\tau}),\varphi(\mathcal{Z}_{\tau^{\prime}})\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2].
    \end{split}
    \label{eq:covcondsignal}
\end{equation*}
\end{proof}


\subsection*{Proof of Proposition \ref{prop:deltaapproxlogitnormal}}
\label{subsec:proofdeltaapprox}

\begin{proof}
To establish the result, we first prove the following lemma. 
\begin{lemma}
\label{lem:approxlogitnormal}
Consider the bivariate vector $\mathbf{Z}=(Z_1,Z_2)^{\top}$ that follows $N(\boldsymbol{\mu},\boldsymbol{\Sigma})$, where $\boldsymbol{\mu}=(\mu_1,\mu_2)^{\top}$ and 
%$\boldsymbol{\Sigma}=\begin{pmatrix} \sigma^2 & \gamma\sigma^2 \\ \gamma\sigma^2 & \sigma^2 \end{pmatrix}$.
$\boldsymbol{\Sigma}=\begin{pmatrix} \sigma_1^2 & \gamma\sigma_1\sigma_2 \\ \gamma\sigma_1\sigma_2 & \sigma_2^2 \end{pmatrix}$.
Then we have,
%\begin{equation*}
%    \begin{split}
%        & E(\varphi(Z_i))\approx \varphi(\mu_i)+\frac{\sigma^2}{2}\varphi^{\prime\prime}(\mu_i),\quad i=1,2,\\
        %& E(\varphi^2(Z_i))\approx \varphi^2(\mu_i)+\sigma^2[(\varphi^{\prime}(\mu_i))^2+\varphi(\mu_i)\varphi^{\prime\prime}(\mu_i)],\quad i=1,2,\\
%        & E(\varphi(Z_1)\varphi(Z_2))\approx \varphi(\mu_1)\varphi(\mu_2)+\frac{\sigma^2}{2}[\varphi^{\prime\prime}(\mu_1)\varphi(\mu_2)+2\gamma\varphi^{\prime}(\mu_1)\varphi^{\prime}(\mu_2)+\varphi(\mu_1)\varphi^{\prime\prime}(\mu_2)].
%    \end{split}
%   \label{eq:momentlogitnormal}
%\end{equation*}
\begin{equation*}
    \begin{split}
        & E(\varphi(Z_i))\approx \varphi(\mu_i)+\frac{\sigma_i^2}{2}\varphi^{\prime\prime}(\mu_i),\quad i=1,2,\\
        %& E(\varphi^2(Z_i))\approx \varphi^2(\mu_i)+\sigma^2[(\varphi^{\prime}(\mu_i))^2+\varphi(\mu_i)\varphi^{\prime\prime}(\mu_i)],\quad i=1,2,\\
        & E(\varphi(Z_1)\varphi(Z_2))\approx \varphi(\mu_1)\varphi(\mu_2)+\frac{1}{2}[\sigma_1^2\varphi^{\prime\prime}(\mu_1)\varphi(\mu_2)+2\gamma\sigma_1\sigma_2\varphi^{\prime}(\mu_1)\varphi^{\prime}(\mu_2)+\sigma_2^2\varphi(\mu_1)\varphi^{\prime\prime}(\mu_2)].
    \end{split}
   \label{eq:momentlogitnormal}
\end{equation*}
\end{lemma}

\begin{proof}
To show the result, we write $\mathbf{Z}=\boldsymbol{\mu}+\boldsymbol{\zeta}$, where $\boldsymbol{\zeta}\sim N(0,\boldsymbol{\Sigma})$.
By Taylor expansion around the mean, 
\begin{equation*}
    \varphi(Z_i)\approx \varphi(\mu_i)+\zeta_i\varphi^{\prime}(\mu_i)+\frac{\zeta_i^2}{2}\varphi^{\prime\prime}(\mu_i).
\label{eq:taylorlogitnomralmean}
\end{equation*}
Then taking expectation yields $\text{E}(\varphi(Z_i))\approx \varphi(\mu_i)+\frac{\sigma_i^2}{2}\varphi^{\prime\prime}(\mu_i)$, $i=1,2$.

%The expectation of $\varphi^2(Z_i)$ can be derived using the same technique,
%\begin{equation*}
%    \varphi^2(Z_i)\approx \varphi^2(\mu_i)+2\zeta_i\varphi(\mu_i)\varphi^{\prime}(\mu_i)+\zeta_i^2[(\varphi^{\prime}(\mu_i))^2+\varphi(\mu_i)\varphi^{\prime\prime}(\mu_i)].
%    \label{eq:taylorlogitnormalvar}
%\end{equation*}
%Taking expectation with respect to $\zeta_i$ again, we arrive at the result. 

As for $E(\varphi(Z_1)\varphi(Z_2))$, consider the function $f(\mathbf{Z})=\varphi(Z_1)\varphi(Z_2)$, using the bivariate version of Taylor expansion,
\begin{equation*}
f(\mathbf{Z})\approx f(\boldsymbol{\mu})+\bigtriangledown f(\boldsymbol{\mu})^{\top}\boldsymbol{\zeta}+\frac{1}{2}\boldsymbol{\zeta}^{\top}\bigtriangledown^2f(\boldsymbol{\mu})\boldsymbol{\zeta}.
\label{eq:taylorlogitnormalcov}    
\end{equation*}
Similarly, taking expectation with respect to $\boldsymbol{\zeta}$ we can obtain the result. 
\end{proof}

Turning to the proof of Proposition \ref{prop:deltaapproxlogitnormal}, 
we notice that $\mathbf{Z}_{\boldsymbol{\tau}}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}\sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})$. Marginalizing out $\mathbf{Z}_{\boldsymbol{\tau}}$, we have $\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}\mid \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2\sim N(\boldsymbol{\mu},\boldsymbol{\Sigma}+\sigma_{\epsilon}^2\mathbf{I})$.
Therefore, for any $\tau,\tau^{\prime}\in\boldsymbol{\tau}$, we have
\begin{equation*}
    \begin{pmatrix}\mathcal{Z}_{\tau} \\ \mathcal{Z}_{\tau^{\prime}} \end{pmatrix}\mid \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2 \,\sim\, N(\begin{pmatrix}
        \mu_{\tau} \\ \mu_{\tau^{\prime}}
    \end{pmatrix}, \begin{pmatrix}
        \Sigma_{\tau,\tau}+\sigma_{\epsilon}^2 & \Sigma_{\tau,\tau^{\prime}} \\ \Sigma_{\tau^{\prime},\tau} & \Sigma_{\tau^{\prime},\tau^{\prime}}+\sigma_{\epsilon}^2
    \end{pmatrix})
\end{equation*}
To establish the connection with the mean and covariance of the signal process, we write
\begin{align*}
    &\begin{pmatrix}
        \mu_{\tau} \\ \mu_{\tau^{\prime}}
    \end{pmatrix}\,=\,\begin{pmatrix}
        \text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}) \\ \text{E}(Z_{\tau^{\prime}}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})
    \end{pmatrix}\nonumber\\
    &\begin{pmatrix}
        \Sigma_{\tau,\tau}+\sigma_{\epsilon}^2 & \Sigma_{\tau,\tau^{\prime}} \\ \Sigma_{\tau^{\prime},\tau} & \Sigma_{\tau^{\prime},\tau^{\prime}}+\sigma_{\epsilon}^2
    \end{pmatrix}\,=\,
    \begin{pmatrix}
       \text{Var}(Z_{\tau}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})+\sigma_{\epsilon}^2 & \text{Cov}(Z_{\tau},Z_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma}) \\ \text{Cov}(Z_{\tau},Z_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma}) & \text{Var}(Z_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})+\sigma_{\epsilon}^2
    \end{pmatrix}
\end{align*}

Similar to the proof of Proposition \ref{prop:meancovcondsignal}, we can show
\begin{equation*}
    \begin{split}
        &\text{Pr}(Y_t=1\mid \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2)=\text{E}(\varphi(\mathcal{Z}_{\tau})\mid \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2)\\
        &\text{Cov}(\mathbf{Y}_{\tau},\mathbf{Y}_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2)=\text{Cov}[\varphi(\mathcal{Z}_{\tau}),\varphi(\mathcal{Z}_{\tau^{\prime}})\mid \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2]
    \end{split}
\end{equation*}
Applying Lemma \ref{lem:approxlogitnormal}, the desired outcome emerges as a direct consequence of algebraic simplification. 
\end{proof}



\subsection*{Proof of Proposition \ref{prop:marginalsignal}}
\label{subsec:proofmarginalsignal}

\begin{proof}
The result is proved by considering the corresponding f.d.d.s. on any finite grids $\boldsymbol{\tau}$. Let the bold letter denote the corresponding process evaluated at $\boldsymbol{\tau}$. From the model assumption mentioned in (\ref{eq:finiterepbinmodel}) 
and (\ref{eq:multigpiwppool}), we have
\begin{equation*}
    \mathbf{Z}\mid\boldsymbol{\mu},\boldsymbol{\Sigma}\,\sim\, N(\boldsymbol{\mu},\boldsymbol{\Sigma}),\,\, \boldsymbol{\mu}|\boldsymbol{\Sigma}\,\sim\, N(\mu_0\mathbf{1},(\nu-3)\boldsymbol{\Sigma}),\,\, \boldsymbol{\Sigma}\sim IW(\nu,\boldsymbol{\Psi}).
\end{equation*}
    
To obtain the marginal distribution of $\mathbf{Z}$, we have
\begin{equation*}
    p(\mathbf{Z})=\int\int p(\mathbf{Z}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})p(\boldsymbol{\mu}\mid \boldsymbol{\Sigma})p(\boldsymbol{\Sigma}) \, d\boldsymbol{\mu} \, d\boldsymbol{\Sigma}.
    \label{eq:marginalsignal}
\end{equation*}
Marginalizing over the mean vector $\boldsymbol{\mu}$, we obtain 
$\mathbf{Z}\mid\boldsymbol{\Sigma}\sim N(\mu_0\mathbf{1},(\nu-2)\boldsymbol{\Sigma})$. 
Based on that, 
\begin{equation*}
    \begin{split}
        p(\mathbf{Z})&=\int p(\mathbf{Z}\mid \boldsymbol{\Sigma})p(\boldsymbol{\Sigma}) \, d\boldsymbol{\Sigma}\\
        &\propto \int \frac{\exp\{-\frac{1}{2}\text{Tr}[(\boldsymbol{\Psi}_{\boldsymbol{\phi}}+\frac{(\mathbf{Z}-\mu_0\mathbf{1})(\mathbf{Z}-\mu_0\mathbf{1})^{\top}}{\nu-2})\boldsymbol{\Sigma}^{-1}]\}}{|\boldsymbol{\Sigma}|^{(\nu+|\boldsymbol{\tau}|+1)/2}} \, d\boldsymbol{\Sigma}\\
        &\propto [1+\frac{(\mathbf{Z}-\mu_0\mathbf{1})^{\top}\boldsymbol{\Psi}_{\boldsymbol{\phi}}^{-1}(\mathbf{Z}-\mu_0\mathbf{1})}{\nu-2}]^{-(\nu+|\boldsymbol{\tau}|)/2},
   \end{split}
   \label{eq:marginalizescalematrix}
\end{equation*}
which can be recognized as the kernel of a MVT distribution. Therefore, the result holds. 

\end{proof}




\section{Synthetic data examples}
\label{sec:simstudy}

The principal goal of analyzing longitudinal data is to estimate the mean and covariance structure of the subject's repeated measurements. We conduct simulation studies to evaluate the proposed method on fulfilling this goal. In the following, Section \ref{subsec:simmean} evaluates the
reliability of the proposed model in capturing the fluctuation of the mean structure, and Section \ref{subsec:simcov} explores the performance of the proposed model in estimating within subject covariance structure. 
%Both of them deal with dichotomous ordinal responses. 
Unless otherwise specified, the posterior analyses in this section are based on 5000 posterior samples collected every 4 iterations from a Markov chain of 30000 iterations, with the first 10000 samples being discarded. 



 
\subsection{Estimating mean structure}
\label{subsec:simmean}

Consider a generic process of generating longitudinal binary responses,
\begin{equation}
\begin{split}
&
\mathbf{Y}_i = Y_i(\boldsymbol{\tau}_{i}) \mid \mathcal{Z}_i(\boldsymbol{\tau}_{i}) 
\stackrel{ind.}{\sim} Bin(1,\eta(\mathcal{Z}_i(\boldsymbol{\tau}_{i}))), 
\quad \boldsymbol{\tau}_i=(\tau_{i1},\cdots,\tau_{iT_i}),\quad i=1,\cdots,n,\\
& 
\mathcal{Z}_i(\boldsymbol{\tau}_{i}) =\boldsymbol{\mathcal{Z}}_i
=f(\boldsymbol{\tau}_i)+\boldsymbol{\omega}_i+\boldsymbol{\epsilon}_i
\quad \boldsymbol{\epsilon}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},\sigma_{\epsilon}^2\mathbf{I}),
    \end{split}    
    \label{eq:datagensim}
\end{equation}
where $\eta(\cdot)$ is a generic link function mapping $\mathbb{R}$ to $(0,1)$, $f(\tau)$ is a signal function, and 
$\boldsymbol{\omega}_i$ is a realization from a mean zero continuous stochastic process that depicts the temporal covariance within subject. The objective is twofold. First, to estimate the subject's probability response curve, which is defined as the probability of obtaining positive response, as a function of time. Second, to estimate the true underlying signal function. 

We consider three data generating processes. The specific choice of $\eta(\cdot)$, $f(\tau)$ and $\boldsymbol{\omega}_i$ for each generating process is summarized as follows:
\begin{itemize}
\item Case 1: $\eta_1(\cdot)=\varphi(\cdot)$, where $\varphi(\cdot)$ is the expit function, $f_1(\tau)=0.3+3\sin(0.5\tau)+\cos(\tau/3)$, and $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},K_1(\boldsymbol{\tau},\boldsymbol{\tau}))$, with covariance kernel $K_1(\tau_{t},\tau_{t^{\prime}})=\exp(-|\tau_t-\tau_{t^{\prime}}|^2)$.
\item Case 2: $\eta_2(\cdot)=\Phi(\cdot)$, where $\Phi(\cdot)$ denotes the CDF of standard normal distribution, $f_2(\tau)=0.1+2\sin(0.25\tau)+\cos(0.25\tau)$, and $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} MVT(5,\mathbf{0},K_2(\boldsymbol{\tau},\boldsymbol{\tau}))$, with covariance kernel $K_2(\tau_{t},\tau_{t^{\prime}})=\frac{1}{3}\exp(-|\tau_t-\tau_{t^{\prime}}|^2)$.
\item Case 3: a mixture of Case 1 and Case 2, with equal probability of generating data from each model. 
\end{itemize}

For $n=30$ subjects, we simulate $T=31$ binary observations at time $\tau=0,\cdots,30$, following the aforementioned data generating processes. To enforce an unbalanced study design, we randomly drop out a proportion of the simulated data. We term the drop out proportion sparsity level, for which we consider $10\%$, $25\%$ and $50\%$. 

The proposed hierarchical model is applied to the data, with a weakly informative prior placed on the mean structure. We obtain posterior inference of the probability response curve and the signal process on a finer grid $\boldsymbol{\tau}^+=(0,\frac{1}{3},\frac{2}{3},\cdots,30)$. Figure \ref{fig:signalprobcurvesub} plots posterior point and interval estimates of the subject's probability response curve for a randomly selected one in each case. Despite the data generating process and the sparsity level, the model can recover the evolution of the underlying probability used in generating binary responses. We observe a shrink in the interval estimate at the set of grid points where at least one subject has observation, that is, $\boldsymbol{\tau}$. The expanding of the credible interval width at $\check{\boldsymbol{\tau}}$ reflect the lack of information at those time grids.   

\begin{figure}[t!]
\centering
\begin{subfigure}{\textwidth}
  \centering 
  % include first image
  \includegraphics[width=16cm,height=3.2cm]{SignalProbCurveSub90.png}  
  \caption{Sparsity level at $10\%$.}
  \label{subfig:signalprobcurve90}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering 
  % include first image
  \includegraphics[width=16cm,height=3.2cm]{SignalProbCurveSub75.png}  
  \caption{Sparsity level at $25\%$.}
  \label{subfig:signalprobcurve75}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
%  % include second image
  \includegraphics[width=16cm,height=3.2cm]{plot/SignalProbCurveSub50.png}  
  \caption{Sparsity level at $50\%$.}
  \label{subfig:signalprobcurve50}
\end{subfigure}

\caption{Simulation study regarding the mean structure. Inference results for the probability response curve.  In each panel, the dashed line and shaded region correspond to the posterior mean and $95\%$ credible interval estimates, the (orange) dot is the original binary data, whereas the (green) cross denotes the true probability of generating that responses.}
\label{fig:signalprobcurvesub}
\end{figure}

We further investigate the model's ability in out-of-sample prediction, by estimating the probability response curve for a new subject from the same cohort. Figure \ref{fig:signalprobcurvenew} shows the posterior point and interval estimates of $\text{Pr}(Y_{*}(\tau_{*t})=1)$, including, as a reference point, the posterior mean estimates of each subject's probability response curve $\text{Pr}(Y_{i}(\tau_{it})=1)$, $i=1,\cdots,n$. The true probability function that triggered the binary response, given as the signal transformed by the link function, is also shown in the figure. It is obtained with the simulated data with $10\%$ sparsity, while there is no major difference for the other two sparsity levels. The behavior of the probability response curve for the new subject is to be expected. It follows the overall trend depicted by the true underlying probability function, while suffers from a comparable level of measurement error with the observed subjects.  

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=4cm]{SignalProbCurveNew.png}
\caption{Simulation study regarding the mean structure. Prediction of the probability response curve for a new subject. In each panel, the dashed lines and shaded region shows the posterior mean and $95\%$ interval estimates of probability response curve for a new subject. The solid lines are the posterior mean estimates of probability response curves for the in-sample subjects. The dotted line is the true probability function for generating binary responses.}
\label{fig:signalprobcurvenew}
\end{figure}

It is also of interest to assess the model's ability in recovering the underlying continuous signal process, since the signal process describes the intrinsic behavior and is crucial to answer related scientific questions.  
In our proposed model, the signal process is modeled nonparametricly through a GP. To further emphasize the benefits of this model formulation, we compare the proposed model with its simplified backbone. The simpler model differs from the original one in modeling the mean function. Instead of modeling the mean function $\mu$ through a GP, we consider modeling it parametricly by $\mu(\tau)\equiv\mu_0$, and $\mu_0\sim N(a_{\mu},b_{\mu})$. The model's ability in capturing the signal process is summarized by the rooted mean square error (RMSE), which is defined by $\text{RMSE}^{\mathcal{M}}=\sqrt{\frac{1}{n}\sum_{i=1}^n\frac{1}{|\boldsymbol{\tau}^+|}\sum_{\tau\in\boldsymbol{\tau}^+}(\hat{Z}^{\mathcal{M}}_i(\tau)-f(\tau))^2}$. Here $\hat{Z}^{\mathcal{M}}_i(\tau)$ denote the model $\mathcal{M}$ estimated signal for subject $i$ evaluated at time $\tau$, which can be obtained at every MCMC iteration. Figure \ref{fig:signalRMSE} explores the posterior distribution of the RMSE under the proposed model and its simplified version, for different data generating process and sparsity level combinations. Despite the scenario, the proposed model shows a notably smaller RMSE. Contrasting the performance with the simpler model highlights the practical utility of including the layer of GP for the mean function in terms of effective estimation of the underlying continuous signal process.     

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=4cm]{SignalRMSE.png}
\caption{Simulation study regarding the mean structure. Box and violin plots of the posterior samples of RMSE for different data generating process and sparsity level combinations. The red box corresponds to the proposed model while the blue box is for the simplified model. }
\label{fig:signalRMSE}
\end{figure}


\subsection{Estimating covariance structure}
\label{subsec:simcov}

Since we emphasize the importance of modeling dependence in longitudinal data, we now explore how well our model works for estimating different covariance structure. Consider the data generating process in (\ref{eq:datagensim}), with expit link function and signal $f(\tau)=0.1+2\sin(0.5\tau)+\cos(0.5\tau)$. We examine a number of possible choices for generating $\boldsymbol{\omega}_i$, that imply covariance structures which would not be in the same form as the covariance kernel used in the proposed model. The primary interest is to exhibit the robustness of covariance kernel choice to different true covariance structures. We let $T_i=T$ and $\tau_{it}=\tau_t$, namely that all subjects are observed over the same time grids. For $n=100$ subjects, we generate sequences of length $T=11$ at time $\tau=0,\cdots,10$. We study the following options of generating $\boldsymbol{\omega}_i$:
         
\begin{itemize}
\item Case 1: $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},K_1(\boldsymbol{\tau},\boldsymbol{\tau}))$, with squared exponential kernel $K_1(\tau_{t},\tau_{t^{\prime}})=\exp(-|\tau_t-\tau_{t^{\prime}}|^2/(2\cdot 3^2))$. Each realized trajectory is infinitely differentiable. 
\item Case 2:  $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},K_2(\boldsymbol{\tau},\boldsymbol{\tau}))$, with exponential kernel $K_2(\tau_{t},\tau_{t^{\prime}})=\exp(-|\tau_t-\tau_{t^{\prime}}|/5)$. Each realization is effectively from a continuous-time AR(1) GP.
\item Case 3:  $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} MVT(5,\mathbf{0},K_3(\boldsymbol{\tau},\boldsymbol{\tau}))$, with compound symmetry kernel $K_3(\tau_{t},\tau_{t^{\prime}})=\mathbf{I}_{\{\tau_t=\tau_{t^{\prime}}\}}+0.4\mathbf{I}_{\{\tau_t\neq\tau_{t^{\prime}}\}}$. The covariance between two observations remains a constant, despite their distance.
\item Case 4: $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} MVT(5,\mathbf{0},K_4(\boldsymbol{\tau},\boldsymbol{\tau}))$, with kernel $K_4(\tau_{t},\tau_{t^{\prime}})=0.7K_2(\tau_{t},\tau_{t^{\prime}})+0.3K_3(\tau_{t},\tau_{t^{\prime}})$, a mixture of AR(1) and compound symmetry covariance structure.
\end{itemize}

In terms of longitudinal binary responses, the covariance structure can be elucidated in two senses, namely the covariance between the pair of binary data $(Y_i(\tau_{t}),Y_i(\tau_{t^{\prime}}))$ and between the pair of signal $(Z_i(\tau_t),Z_i(\tau_{t^{\prime}}))$. We consider the covariance structure of the signal process first. From Proposition \ref{prop:marginalsignal}, $\text{Cov}(Z_i(\tau_t),Z_i(\tau_{t^{\prime}}))=\Psi_{\boldsymbol{\phi}}(\tau_t,\tau_{t^{\prime}})$, $\forall i$, where the covariance function $\Psi_{\boldsymbol{\phi}}$ is defined in (\ref{eq:matern52covfun}). Hence, the signal covariance structure estimated from the model is also isotropic, facilitating a graphic comparison between the posterior estimate of  $\Psi_{\boldsymbol{\phi}}(\tau_d)$ versus the true covariance kernel $K(\tau_d)$, where $\tau_d=|\tau_t-\tau_{t^{\prime}}|$. The results are presented in Figure \ref{fig:covariogram}. As expected, the proposed model recovers the truth, despite the mis-specification of the covariance kernel. Comparing with the other three cases, the posterior point estimate of covariance kernel is less accurate in Case 3. This can be explained by noticing that the constant covariance in that case violates the model assumption. Nonetheless, the posterior interval still covers the truth. 

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=4cm]{Covariogram.png}
\caption{Simulation study regarding the covariance structure. Inference results for the signal covariance kernels. In each panel, the dashed line and shaded region correspond to the posterior mean and 95\% credible interval estimates, whereas the solid line denotes the true covariance kernel.}
\label{fig:covariogram}
\end{figure}

As for the covariance between the pair of binary data, we consider two measurements, the Pearson correlation coefficient and the tetrachoric correlation coefficient. For a review of the definitions and properties of these two correlation coefficients, we refer to \citet{Joakim2011}. At each MCMC iteration, we predict a new sequence of binary responses of length $T$, denoted as $\{Y^{(s)}_{i^*}(\boldsymbol{\tau}):s=1,\cdots,S\}$. Correspondingly, we also obtain samples of binary sequences from the true data generating process, denoted by $\{\hat{Y}^{(s)}_{i^*}(\boldsymbol{\tau}):s=1,\cdots,S\}$. Both sets of binary sequences form $S/n$ datasets that mimic  the original samples. From the datasets comprised by posterior predictive samples $Y^{(s)}_{i^*}(\boldsymbol{\tau})$, we obtain interval estimates of the two correlation coefficients. In addition, for $\hat{Y}^{(s)}_{i^*}(\boldsymbol{\tau})$ that are generated from the truth, we obtain point estimates, which can be viewed as the correlation coefficients from the data, accounting for the variation in the data generating process. Notice that marginally the binary process is not guaranteed to be isotropic. Hence, the correlation coefficients should be calculated for every possible pair of $(\tau_t,\tau_{t^{\prime}})\in\boldsymbol{\tau}$. The resulting point and interval estimates of both types of correlation coefficients are displayed in Figure \ref{fig:bincorrCI}. All the posterior interval estimates cover the truth, indicating that the proposed model effectively captures the binary covariance structure.  

 \begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCIsqexp}
            \caption{Case 1.}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCIou}
            \caption{Case 2.}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCIcsy}
            \caption{Case 3.}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCImix}
            \caption{Case 4.}
    \end{subfigure}
    \caption{Simulation study regarding the covariance structure. Posterior interval estimate of correlation coefficients (``box'') versus point estimate obtained from the true data generating process (``$\star$''). In each panel, the upper triangle and the lower triangle are for the Pearson and the  rachoric correlation coefficient, respectively.}
    \label{fig:bincorrCI}
\end{figure}


%\begin{figure}[t!]
%\centering
%\begin{subfigure}{\textwidth}
%  \centering 
  % include first image
%  \includegraphics[width=16cm,height=4cm]{bincorrphicover.png}  
%  \caption{Coverage of the Phi correlation coefficient.}
%  \label{subfig:bincorrphicover}
%\end{subfigure}
%\begin{subfigure}{\textwidth}
%  \centering 
%  % include first image
%  \includegraphics[width=16cm,height=4cm]{bincorrtetcover.png}  
%  \caption{Coverage of the tetrachoric correlation coefficient.}
%  \label{subfig:bincorrtetcover}
%\end{subfigure}
%\caption{Simulation study regarding the covariance structure. $\tilde{\text{CL}}$ (upper triangle) versus $\hat{\text{CL}}$ (lower triangle). The numbers on the diagonal are the time indices. In each row, the figures from the left to the right correspond to the data generated from Case 1 to 4, respectively. The cell is marked by a cross if the corresponding coverage is greater than 0.9.}
%\label{fig:bincorrcover}
%\end{figure}

The simulation studies have illustrated the benefits of our approach, that is, avoiding 
possible bias in covariance structure estimation caused by mis-specification of the covariance 
kernel for the signal process. Such benefits are led by the IWP prior placed on the covariance 
function. To emphasize this point, we consider an alternative, simplified modeling approach, with $Z_i\stackrel{i.i.d.}{\sim}GP(\mu,\Psi_{\boldsymbol{\phi}})$, $\mu\sim GP(\mu_0,\Psi_{\boldsymbol{\phi}}/\kappa)$. That is, instead of modeling the covariance function nonparametricly, we assume a covariance kernel of certain parametric form, specified by $\Psi_{\boldsymbol{\phi}}$. We consider the centralized signal process $\omega_i=Z_i-\mu$ evaluated at a finite grid $\boldsymbol{\tau}$, denoted as $\boldsymbol{\omega}_i$. Under the proposed model, $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim}MVT(\nu,\mathbf{0},\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}))$, while under the simplified model, $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},(1+\frac{1}{\kappa})\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}))$. We know the true distribution of $\boldsymbol{\omega}_i$ from the data generating process. Therefore, we can compute the 2-Wasserstein distance between the model estimated distribution of $\boldsymbol{\omega}_i$ to the truth. The usage of 2-Wasserstein distance is motivated by its straightforward interpretation: a 2-Wasserstein distance of $d$ means that coordinatewise standard deviations differ by at most $d$ \citep[Thm.~3.4]{Huggins2020}. Iterating over the posterior samples of model parameters, we obtain the distributions of 2-Wasserstein distance between the model estimated distribution of $\boldsymbol{\omega}_i$ and the truth, which is shown in Figure \ref{fig:covwassd}. Clearly, for the proposed model, the 2-Wasserstein distances are substantially small. Contrasting the performance testifies our motivation of modeling the covariance structure nonparametricly.

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=6cm]{covwassd.png}
\caption{Simulation study regarding the covariance structure. Histogram for the posterior samples of the 2-Wasserstein distance between the f.d.d.s. of the centralized signal process obtained from the proposed model (upper panel) and the simplified model (lower panel) to the truth.}
\label{fig:covwassd}
\end{figure}



\section{Additional results for data examples}
\label{sec:dataaddresults}

\subsection{Binary responses from \textit{Studentlife} study}

%We propose a prior specification strategy in Section \ref{subsec:modelapply}. 
%We suggest the default hyperprior for $\mu_0$ and $\nu$ as $\mu_0\sim N(0,100)$ 
%and $\nu\sim Unif(4,30)$. $\sigma^2$ and $\rho$ control the covariance structure.  
%Their prior hyperparameters can be determined by exploring the covariance structure 
%of the data. On the other hand, 
The hyperprior for $\sigma_{\epsilon}^2$ depends on the belief about the extent 
of the measurement error. Hence, it is useful to perform a prior sensitivity 
analysis with respect to this hyperprior, especially on the real data. 


In general, the measurement error reflects the remaining variability of the 
underlying continuous process, whose major change has been captured by the signal 
process. Consequently, it should have small probability of taking large values. 
For the analysis conducted in Section \ref{subsec:resultsrealapp},
we believe the measurement error range should be small, and we pick a 
moderate value for the error degree of freedom. Specifically, we 
take $R=0.1$ and $\upsilon=10$, and using the method described in 
Section \ref{subsec:modelapply}, obtain the hyperprior for 
$\sigma_{\epsilon}^2$ as $IG(5,0.001)$. We term it the original hyperprior.


To perform a prior sensitivity analysis, we assume an alternative hyperprior 
on $\sigma_{\epsilon}^2$. In the case of valence score, we assume a larger 
measurement error range $R=0.5$, resulting in the hyperprior 
$\sigma_{\epsilon}^2\sim IG(5,0.02)$. As for the arousal score, we assume 
the error distribution has a heavier tail, achieved by setting 
$\upsilon=6$. The hyperprior in this case is $\sigma_{\epsilon}^2\sim IG(3,0.0007)$.
We check the posterior samples for $\mu_0$, $\sigma^2$, $\rho$, and $\nu$, 
because these four parameters determine the signal process, which is the 
inference target of primary interest. Results are shown in 
Figure \ref{fig:priorsen}. The posterior distributions of the four model 
parameters are similar, suggesting that the conclusion are robust with 
respect to the hyperprior choice for the error variance.

\begin{figure}[t!]
\centering
\begin{subfigure}{\textwidth}
  \centering 
  % include first image
  \includegraphics[width=16cm,height=6cm]{valpriorsen.png}  
  \caption{Prior sensitivity analysis on the valence data.}
  \label{subfig:valpriorsen}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering 
  % include first image
  \includegraphics[width=16cm,height=6cm]{aropriorsen.png}  
  \caption{Prior sensitivity analysis on the arousal data.}
  \label{subfig:aropriorsen}
\end{subfigure}
\caption{\textit{Studentlife} data. Histogram of the posterior samples for model parameters $\mu_0$, $\sigma^2$, $\rho$, and $\nu$. The solid line depict the kernel density estimation. The dashed line corresponds to the mean and the dotted lines represent the $2.5\%$ and $97.5\%$ percentile, respectively.}
\label{fig:priorsen}
\end{figure}



\subsection{Four levels arousal score data}

Particular to the ordinal responses, we assess the time dependence through the joint probability $\text{Pr}(\mathbf{Y}_{\tau}=j,\mathbf{Y}_{\tau^{\prime}}=j^{\prime}\mid\{\mathbf{Z}_{j\boldsymbol{\tau}}\},\{\sigma^2_{\epsilon j}\})$, whose posterior inference can be obtained by evaluating (\ref{eq:jointprobmult}) with the posterior samples of model parameters. Figure \ref{fig:quadarojointprob} displays the posterior point and interval estimate for all possible pairs of the joint probabilities. It suggests that the proposed model enables flexible estimate of the time dependence among the ordinal responses. 

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=10cm]{QuadAroJointProb.png}
\caption{Four levels arousal score data. Posterior mean (dashed line) and 95\% interval estimate (shaded region) of the joint probability of the observations on the same subject made at time $\tau$ and $\tau^{\prime}$.}
\label{fig:quadarojointprob}
\end{figure}

 
