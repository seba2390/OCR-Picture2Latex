
%\section{Model for dichotomous ordinal responses}
%\section{The primary modeling approach}
%
\section{The modeling approach for binary responses}
\label{sec:binarymodel}


Here, we develop the methodology for longitudinal binary responses.
%We illustrate the principal model designs with binary longitudinal responses.
 %on multi-scale, we highlight the special case with binary responses in this section. As we will discuss in Section \ref{sec:polyordinalmodel}, with the continuation-ratio factorization, fitting the model for general ordinal responses is equivalent with fitting models for binary responses separately. Hence, discussing models for this special case is indispensable.
The data consist of repeated binary responses on $n$ subjects, 
with the observation on subject $i$ at time $\tau_{it}$ denoted by $Y_{it}$. The set of 
repeated outcomes for the $i$-th subject is collected into a $T_i$-dimensional vector 
$\mathbf{Y}_i=$ $(Y_{i1},\cdots,Y_{iT_i})^\top$. The hierarchical model construction 
is presented in Section \ref{subsec:standardmodel}. In Section \ref{subsec:modprop}, 
we discuss model properties related to our inference objectives. Bayesian inference and 
prediction is developed in Section \ref{subsec:modelapply}. In Section \ref{subsec:simstudy},
we outline the findings from simulation studies, the details of which are included in 
the Supplementary Material. Finally, to place our contribution within the literature, we 
discuss in Section \ref{subsec:literaturereview} the proposed model in the context of 
relevant Bayesian nonparametric approaches.


\subsection{Model specification}
\label{subsec:standardmodel}

We examine the data from a functional data analysis perspective, treating each observed 
data vector $\mathbf{Y}_i$ as the evaluation of trajectory $Y_i(\tau)$ on grid 
$\boldsymbol{\tau}_i=$ $(\tau_{i1},\cdots,\tau_{iT_i})^{\top}$, for $i=1,\cdots,n$. 
The $n$ trajectories are assumed to be (conditionally) independent realizations from 
a continuous-time stochastic process. The prior probability model is built on the stochastic 
process. This approach avoids strong pre-determined assumptions on the transition 
mechanism within the sequence of subject-specific responses in $\mathbf{Y}_i$, while it 
is suitable to accommodate repeated measurements regardless of their observational pattern.  


The functional data analysis view of longitudinal data dates back at least to 
\citet{Zhao2004}, %where the similarity of the objective is recognized. They suggest 
where it is suggested that functional data analysis tools, 
such as principal component analysis, can be used to capture %populational 
periodic structure in longitudinal data. Indeed, \citet{Yao2005} study functional 
principal component analysis (FPCA) for sparse longitudinal data, a method that can 
provide effective recovery of the entire individual trajectories from fragmental data. 
FPCA has been applied in finance \citep{Ingrassia2005}, biomechanics \citep{Dona2009}, 
and demographic studies \citep{Shamshoian2020}. Its extension to examine sequences 
of discrete data is studied in \citet{Hall2008}.     


Our methodology builds from a GP-based hierarchical model for continuous functional 
data \citep{Yang2016}. Regarding mean-covariance estimation, the model in \cite{Yang2016} 
can be considered as a Bayesian counterpart of \citet{Yao2005}. 
The hierarchical scheme enables a natural extension to studies with binary responses. 
We assume that, subject to measurement error, the $i$-th subject's responses, 
$Y_{it} \equiv$ $Y_i(\tau_{it})$, depend on the $i$-th trajectory of the underlying 
process, evaluated at times $\tau_{it}$, through the following model
\begin{equation*}
Y_i(\tau_{it}) \mid Z_i(\tau_{it}),\epsilon_{it} \, \stackrel{ind.}{\sim} \,
Bin(1, \varphi(Z_i(\tau_{it})+\epsilon_{it})),\quad t=1,\cdots,T_i,\quad i=1,\cdots,n,
\label{eq:bernmodel}
\end{equation*}
where $\varphi(x) = $ $\exp(x)/\{ 1 + \exp(x) \}$ denotes the expit function. 
The error terms are i.i.d. from a white noise process, that is, 
$\epsilon_{it} \mid \sigma^2_{\epsilon} \stackrel{i.i.d.}{\sim} N(0,\sigma^2_{\epsilon})$, 
and independent of the process realizations $Z_i(\cdot)$. The main building block 
for the model construction is a hierarchical GP prior for the $Z_i(\cdot)$.
In particular, given random mean function $\mu(\cdot)$ and covariance kernel 
$\Sigma(\cdot,\cdot)$, the $Z_i(\cdot)$ are i.i.d. GP realizations, denoted by 
$Z_i \mid \mu,\Sigma \stackrel{i.i.d.}{\sim} GP(\mu,\Sigma)$, for $i=1,\cdots,n$.
%The defining characteristic of our proposed model is that we take a fully 
%nonparametric treatment in modeling the mean and covariance function. 
The hierarchical GP prior model is completed with nonparametric priors for the 
mean function and covariance kernel:
\begin{equation}
\mu \mid \Sigma \sim GP(\mu_0,\Sigma/\kappa), \quad 
\Sigma\sim IWP(\nu,\Psi_{\boldsymbol{\phi}}) ,
\label{eq:gpiwpprior}
\end{equation}
where $GP(\cdot,\cdot)$ and $IWP(\cdot,\cdot)$ denote the GP and IWP prior, 
respectively. The nonparametric prior reflects the intuition that parametric forms 
will generally not be sufficiently flexible for the mean and covariance functions.


We adopt an IWP prior for the covariance kernel, defined such that, on any finite 
grid $\boldsymbol{\tau}=(\tau_1,\cdots,\tau_T)$ with $|\boldsymbol{\tau}|$ points, 
the projection $\Sigma(\boldsymbol{\tau},\boldsymbol{\tau})$ follows an inverse-Wishart 
distribution with mean 
$\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}) / (\nu-2)$, denoted by 
$IW(\nu,\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}))$.
Here, $\Psi_{\boldsymbol{\phi}}(\cdot,\cdot)$ is a non-negative definite function 
with parameters $\boldsymbol{\phi}$. Note that we use the parameterization from 
\citet{Dawid1981} for the inverse-Wishart distribution, in particular, $\nu$ is the 
shape parameter and $\nu+|\boldsymbol{\tau}|-1$ is the degrees of freedom parameter in the
more common parameterization. \citet{Yang2016} validate that this parameterization defines 
an infinite dimensional probability measure whose finite dimensional projection on grid 
$\boldsymbol{\tau}$ coincides with the inverse-Wishart distribution 
$IW(\nu,\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}))$.  


The model formulation is completed with prior specification for the hyperparameters. 
The error variance is assigned an inverse Gamma prior, 
$\sigma_{\epsilon}^2\sim IG(a_{\epsilon},b_{\epsilon})$. We focus primarily on 
stationary specifications under the prior structure in (\ref{eq:gpiwpprior}). 
In particular, we work with mean function, $\mu_0(\tau) \equiv \mu_0$, and isotropic 
covariance function, $\Psi_{\boldsymbol{\phi}}$, within the Matérn class, a widely 
used class of covariance functions \citep{Rasmussen2006}. In general, 
the Matérn covariance function is specified by a scale parameter $\sigma^2$, a range 
parameter $\rho$, and a smoothness parameter $\iota$. %Following \citet{Yang2016}, to 
To encourage %smooth estimation of the 
smoothness in the probability response curves, we set 
$\iota = 5/2$, such that the covariance kernel is given by
\begin{equation*}
\Psi_{\boldsymbol{\phi}}(\tau,\tau^{\prime}) \, = \, 
\sigma^2 \, \left( 1+\frac{\sqrt{5}|\tau-\tau^{\prime}|}{\rho}+
\frac{5|\tau-\tau^{\prime}|^2}{3\rho^2} \right) \,
\exp\left( -\frac{\sqrt{5}|\tau-\tau^{\prime}|}{\rho} \right),
    \label{eq:matern52covfun}
\end{equation*}
where $\boldsymbol{\phi}=\{\sigma^2,\rho\}$. %Note that one can incorporate nonstationary mean and covariance function by simply letting $\mu_0=\mu_0(\tau)$ and $\sigma^2=\sigma^2(\tau)$. 
For hyperparameters $\mu_0$, $\sigma^2$, $\rho$, we take the commonly used choice, 
\begin{equation*}
\mu_0\sim N(a_{\mu},b_{\mu}),\quad \sigma^2\sim \text{Gamma}(a_{\sigma},b_{\sigma}),\quad 
\rho\sim Unif(a_{\rho},b_{\rho}).
    \label{eq:hyperprior}
\end{equation*}
Finally, we set $\kappa = (\nu-3)^{-1}$, such that the continuous-time process for 
the $Z_i(\cdot)$ is a TP when $\mu$ and $\Sigma$ are marginalized out 
(see Section \ref{subsec:modprop} for details). As a consequence, parameter $\nu$ controls 
the tail heaviness of the marginal process, with smaller values of $\nu$ corresponding to 
heavier tails. We place a uniform prior on $\nu$, $\nu\sim Unif(a_{\nu},b_{\nu})$, 
with $a_{\nu}>3$ to ensure positive definiteness of $\Sigma/\kappa$. 


As discussed in \citet{Diggle1988}, the correlation of repeated measurements on the same 
subject commonly has the following patterns. First, it should decrease with respect to 
the measurements' separation in time, while remaining positive to indicate the measurements 
are from the same subject. This feature is encapsulated by the form of the covariance 
kernel $\Psi_{\boldsymbol{\phi}}$. The IWP prior elicits realizations for which this property 
holds a priori, while enabling a flexible estimate of the covariance structure with information 
from the data a posteriori. Second, measurements that are made arbitrarily close in time are 
subject to imperfect correlation, possibly caused by subsampling of each subject. This feature 
is represented by the error term in our model. Moreover, the motivation for adding the error 
term arises from the fact that measurement error is introduced in the estimation of a 
continuous-time function based on data collected at discrete time points. 


Although the probability model is formulated through stochastic process realizations, 
posterior simulation is based on the corresponding finite dimensional distributions (f.d.d.s.). 
Consequently, to write the model for the data, we need to represent the likelihood and prior 
in multivariate forms through evaluating the functions on finite grids. 
Denoting $Y_i(\boldsymbol{\tau}_i)$ by $\mathbf{Y}_i$, $Z_i(\boldsymbol{\tau}_i)$ by 
$\mathbf{Z}_i$, and $\boldsymbol{\epsilon}_i=$
$(\epsilon_{i1},\cdots,\epsilon_{iT_i})^{\top}$, the %proposed model implies that 
model for the data can be written as 
\begin{equation}
\begin{split}
&\mathbf{Y}_i\mid \mathbf{Z}_i,\boldsymbol{\epsilon}_i \, \stackrel{ind.}{\sim} \,
\prod^{T_i}_{t=1}Bin(1, \varphi(Z_{it}+\epsilon_{it})),\quad i=1,\cdots,n,\\ 
&\mathbf{Z}_i\mid \mu(\boldsymbol{\tau}_i),\Sigma(\boldsymbol{\tau}_i,\boldsymbol{\tau}_i) \,
\stackrel{ind.}{\sim} \, N(\mu(\boldsymbol{\tau}_i),\Sigma(\boldsymbol{\tau}_i,\boldsymbol{\tau}_i)),
\quad \boldsymbol{\epsilon}_i\mid \sigma_{\epsilon}^2
\, \stackrel{ind.}{\sim} \, N(\mathbf{0},\sigma_{\epsilon}^2 \, \mathbf{I}).
\end{split}
\label{eq:finiterepbinmodel}
\end{equation}
%for the observed trajectories. 
Notice that the grids $\{\boldsymbol{\tau}_i:i=1,\cdots,n\}$ are not necessarily the same
for all subjects. Therefore, the shared GP and IWP prior in (\ref{eq:gpiwpprior}) need to 
be evaluated on the pooled grid $\boldsymbol{\tau}=\cup_{i=1}^n\boldsymbol{\tau}_i$. 
If $\boldsymbol{\mu}$, $\boldsymbol{\Sigma}$, and $\boldsymbol{\Psi}_{\boldsymbol{\phi}}$ 
denote $\mu(\boldsymbol{\tau})$, $\Sigma(\boldsymbol{\tau},\boldsymbol{\tau})$, and $\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau})$, respectively, then
\begin{equation}
\boldsymbol{\mu} \mid \boldsymbol{\Sigma},\mu_{0}, \nu 
\, \sim \, N(\mu_0\mathbf{1}, (\nu-3) \boldsymbol{\Sigma}),\quad 
\boldsymbol{\Sigma} \mid \nu, \boldsymbol{\phi} 
\, \sim \, IW(\nu,\boldsymbol{\Psi}_{\boldsymbol{\phi}}).
\label{eq:multigpiwppool}
\end{equation}
The hierarchical model formulation for the data in (\ref{eq:finiterepbinmodel}) 
and (\ref{eq:multigpiwppool}) forms the basis for the posterior simulation algorithm, 
which is discussed in detail in Section \ref{subsec:modelapply}.  



\subsection{Model properties}
\label{subsec:modprop}

To fix ideas for the following discussion, we refer to $Z_i(\tau)$ as the signal process of 
the binary process $Y_i(\tau)$, and to $\mathcal{Z}_i(\tau)=Z_i(\tau)+\epsilon_i(\tau)$ as 
the latent process of $Y_i(\tau)$. Since the stochastic process is characterized by its 
f.d.d.s., we shall investigate the random vectors $\mathbf{Y}_{\boldsymbol{\tau}}=$
$Y_i(\boldsymbol{\tau})$, $\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}=$
$\mathcal{Z}_{i}(\boldsymbol{\tau})$, and $\mathbf{Z}_{\boldsymbol{\tau}}=$
$Z_{i}(\boldsymbol{\tau})$, for a generic grid vector $\boldsymbol{\tau}=$
$(\tau_1,\cdots,\tau_T)^{\top}$. We surpass the subject index $i$ because the subject 
trajectories are identically distributed. The Supplementary Material includes 
proofs for the propositions included in this section. 


Among the various inference goals in a study that involves longitudinal binary data, 
estimating the probability response curve and the covariance structure of the repeated 
measurements are the most important ones.  In Proposition \ref{prop:meancovcondsignal}, 
we derive the probability response curves and covariance matrix of the binary 
vector $\mathbf{Y}_{\boldsymbol{\tau}}$, conditional on the signal vector
$\mathbf{Z}_{\boldsymbol{\tau}}$ and error variance $\sigma^2_{\epsilon}$. 
The probability response curve can be defined generically 
as $\mathbf{P}_{\mathbf{y}\boldsymbol{\tau}} =$
$(\text{Pr}(Y_{\tau_1}=y_{\tau_1}\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2),
\cdots,\text{Pr}(Y_{\tau_T}=y_{\tau_T}\mid \mathbf{Z}_{\boldsymbol{\tau}},
\sigma_{\epsilon}^2) )^{\top}$, where $y_{\tau_t}$ is either 0 or 1. 
Without loss of generality, we focus on $\mathbf{P}_{\mathbf{1}\boldsymbol{\tau}}$.


%\footnote{
%\textcolor{blue}{We have an issue with the $\varphi$ notation for the logistic 
%function: the same letter is used for the function with univariate argument (which 
%is well defined, with the notation given in Section 2.1), and with multivariate 
%argument, which is ambiguous. Making matters worse, derivatives appear in later
%expressions for both types of functions.
%Now, in the expression for the probability response curve, the function is given by 
%$\varphi(\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}) =$
%$\prod_{t=1}^{T} \varphi(\mathcal{Z}_{t})$, right? We need to either use a different
%letter for this function (one not used elsewhere!) and say what it denotes, or just 
%write the product (the second option may be better, if the shorthand notation is not 
%needed in too many places). The corresponding revision is also needed in the SM.}
%}
%\footnote{
%\textcolor{blue}{Proposition 1 is obviously very important, but we may want to try 
%to improve the presentation around it. Is it possible to come up with a figure that
%will illustrate the covariance structure? (for this, it's fine to use the approximations).
%We also need to be more clear about which expressions are used for the posterior 
%inference results, the exact expressions with numerical integration or the delta 
%method approximations?}
%}


\begin{proposition}
\label{prop:meancovcondsignal}
The probability response curve is given by
$\mathbf{P}_{\mathbf{1}\boldsymbol{\tau}} =$
%$E(\mathbf{Y}_{\boldsymbol{\tau}}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)=$
$E(\boldsymbol{\pi}(\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}})
\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)$, 
where $\boldsymbol{\pi}(\mathbf{x})$ denotes the vector operator that applies 
the expit function to every entry of $\mathbf{x}$. Regarding the covariance matrix, 
for $\tau\in\boldsymbol{\tau}$, 
$Var(Y_{\tau}\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2) =$
$E(\varphi(\mathcal{Z}_{\tau})\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2) - E^2(\varphi(\mathcal{Z}_{\tau})\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)$, 
and for $\tau, \tau^{\prime}\in\boldsymbol{\tau}$, with $\tau^{\prime} \neq \tau$, $Cov(Y_{\tau},Y_{\tau^{\prime}}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)=$
$Cov(\varphi(\mathcal{Z}_{\tau}),\varphi(\mathcal{Z}_{\tau^{\prime}})\mid \mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)$. 
The conditional expectations in all of the above expressions are with respect to distribution,
$\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}\mid \mathbf{Z}_{\boldsymbol{\tau}},
\sigma_{\epsilon}^2 \sim N(\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2 \, \mathbf{I})$.
\end{proposition}

%\begin{remark}
%Proposition \ref{prop:meancovcondsignal} indicates a fixed mean-variance relationship for the binary variable $\mathbf{Y}_{\tau}$, which might be restrictive. This is because even though we model the signal process nonparametricly, the response distribution is assumed to be parametric. Therefore, considered as a whole, the proposed model is semiparametric. As a consequence, the model suffers the overdispersion limitation induced by the binomial distribution. 
%\end{remark}

The practical utility of Proposition \ref{prop:meancovcondsignal} lies on performing 
posterior inference for the probability response curve and the covariance structure of 
the binary process, conditioning on the signal process and the noise. With posterior 
samples of $\mathbf{Z}_{\boldsymbol{\tau}}$ and $\sigma^2_{\epsilon}$, we can 
simulate $\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}$ 
from $N(\mathbf{Z}_{\boldsymbol{\tau}},\sigma^2_{\epsilon}\mathbf{I})$ and numerically 
compute the corresponding moments in Proposition \ref{prop:meancovcondsignal}. 
The entries of $\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}$ are independent, given 
$\mathbf{Z}_{\boldsymbol{\tau}}$, and thus simulating 
$\boldsymbol{\mathcal{Z}}_{\boldsymbol{\tau}}$ is not computationally demanding, 
even when $|\boldsymbol{\tau}|$ is large. 

%Because the first two moments of a logit-normal random vector do not have an analytical form in general, it is hard to sense the shape of the probability response curve and the correlation decay in time of the binary process. %The following proposition reveals that, further marginalizing out $\mathbf{Z}_{\boldsymbol{\tau}}\mid \boldsymbol{\mu}_{\boldsymbol{\tau}},\boldsymbol{\Sigma}_{\boldsymbol{\tau},\boldsymbol{\tau}}\sim N(\boldsymbol{\mu}_{\boldsymbol{\tau}},\boldsymbol{\Sigma}_{\boldsymbol{\tau},\boldsymbol{\tau}})$, 
%establishes the connection of the mean and covariance structure between the binary process and the underlying signal process through the second order Taylor expansion approximation. 

We next establish a closer connection between the binary process and the signal process.  
Proposition \ref{prop:deltaapproxlogitnormal} reveals that the evolution of the binary process 
over time can be (approximately) expressed as a function of the expectation of the signal 
process and the total variance. Moreover, the covariance of the binary process is approximately 
the covariance of the signal process scaled by a factor related to the expectation of the signal. 


\begin{proposition}
\label{prop:deltaapproxlogitnormal}
Consider the proposed model as described in (\ref{eq:finiterepbinmodel}) and 
denote $\mu(\boldsymbol{\tau})=\boldsymbol{\mu}$, 
and $\Sigma(\boldsymbol{\tau},\boldsymbol{\tau})=\boldsymbol{\Sigma}$. Then, 
\begin{align*}
&\text{Pr}(Y_{\tau}=1\mid \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2)\approx \varphi(\text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}))+\frac{\text{Var}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})+\sigma_{\epsilon}^2}{2}\varphi^{\prime\prime}(\text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})),\,\, \forall \tau\in\boldsymbol{\tau}, \nonumber\\
&\text{Cov}(Y_{\tau},Y_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2)
\approx 
\varphi^{\prime}(\text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}))\varphi^{\prime}(\text{E}(Z_{\tau^{\prime}}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})) \, 
\text{Cov}(Z_{\tau},Z_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})\\
& -\frac{1}{4}[\text{Var}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})+\sigma_{\epsilon}^2][\text{Var}(Z_{\tau^{\prime}}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})+\sigma_{\epsilon}^2]\varphi^{\prime\prime}(\text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}))\varphi^{\prime\prime}(\text{E}(Z_{\tau^{\prime}}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})),\,\, \forall \tau,\tau^{\prime}\in\boldsymbol{\tau}.
%\label{eq:deltapproxcov}
\end{align*}
Here, $\varphi^{\prime}(x)=\frac{d\varphi(x)}{dx}=\varphi(x)[1-\varphi(x)]$ 
and $\varphi^{\prime\prime}(x)=$ $\frac{d^2\varphi(x)}{dx^2}=$ 
$\varphi(x)[1-\varphi(x)][1-2\varphi(x)]$.
\end{proposition}

%
%\footnote{
%\textcolor{blue}{
%It seems that proposition 2 needs help with the notation, and maybe even the result for the 
%covariance? The first line (expression for the probability) seems OK. 
%Now for the proof of proposition 2, you use lemma 1, which I assume is applied to 
%bivariate vector $(\mathcal{Z}_{\tau},\mathcal{Z}_{\tau^{\prime}})$. I'm doing the algebra
%to arrive at the approximation using lemma 1, and I'm getting an extra term to what is 
%given in proposition 2, specifically, $- 0.25 \sigma^{4} (\varphi^{\prime\prime}(\mu))^{2}$,
%where $\sigma^{2}$ is the total variance. Am I just messing up with the algebra?
%BTW, the result definitely needs the assumption of a constant mean function, so, 
%we probably should reiterate that assumption in the statement of proposition 2, 
%and we should write
%$\{ \varphi^{\prime}(\text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})) \}^{2}$
%instead of 
%$\varphi^{\prime}(\text{E}(Z_{\tau}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}))
%\varphi^{\prime}(\text{E}(Z_{\tau^{\prime}}\mid \boldsymbol{\mu},\boldsymbol{\Sigma}))$
%in the result for the covariance.}
%}
%


%\begin{proposition}
%\label{prop:deltaapproxlogitnormal}
%Consider the bivariate vector $\mathbf{Z}=(Z_1,Z_2)^{\top}$ that follows $N(\boldsymbol{\mu},\boldsymbol{\Sigma})$, where $\boldsymbol{\mu}=(\mu_1,\mu_2)^{\top}$ and $\boldsymbol{\Sigma}=\begin{pmatrix} \sigma^2 & \gamma\sigma^2 \\ \gamma\sigma^2 & \sigma^2 \end{pmatrix}$. Then we have,
%\begin{equation*}
%    \begin{split}
%        & E(\varphi(Z_i))\approx \varphi(\mu_i)+\frac{\sigma^2}{2}\varphi^{\prime\prime}(\mu_i),\\
%        & E(\varphi^2(Z_i))\approx \varphi^2(\mu_i)+\sigma^2[(\varphi^{\prime}(\mu_i))^2+\varphi(\mu_i)\varphi^{\prime\prime}(\mu_i)],\quad i=1,2,\\
%        & E(\varphi(Z_1)\varphi(Z_2))\approx \varphi(\mu_1)\varphi(\mu_2)+\frac{\sigma^2}{2}[\varphi^{\prime\prime}(\mu_1)\varphi(\mu_2)+2\gamma\varphi^{\prime}(\mu_1)\varphi^{\prime}(\mu_2)+\varphi(\mu_1)\varphi^{\prime\prime}(\mu_2)].
 %   \end{split}
%    \label{eq:momentlogitnormal}
%\end{equation*}
%\end{proposition}

%From \citet{Pirjol2013}, when $Z\sim N(0,\sigma^2)$, $E(\varphi(Z))=\frac{1}{2}$. Combining with Proposition \ref{prop:deltaapproxlogitnormal} yields the following corollary, which is the special case when the normal variables have mean $0$. 
%\begin{corollary}
%\label{cor:meanzerocase}
%Assume the same setting as Proposition \ref{prop:deltaapproxlogitnormal}. If $\boldsymbol{\mu}=\mathbf{0}$, then 
%\begin{equation*}
%\begin{split}
%& \text{E}(\varphi(Z_i)\mid \boldsymbol{\mu},\boldsymbol{\Sigma})=\frac{1}{2},\quad \text{Var}(\varphi(Z_i)\mid \boldsymbol{\mu},\boldsymbol{\Sigma})\approx\frac{\sigma^2}{16},\quad i=1,2,\\
%& \text{Cov}(\varphi(Z_1),\varphi(Z_2)\mid \boldsymbol{\mu},\boldsymbol{\Sigma})\approx\frac{\gamma\sigma^2}{16},\quad \text{Corr}(\varphi(Z_1),\varphi(Z_2)\mid \boldsymbol{\mu},\boldsymbol{\Sigma})\approx\gamma=\text{Corr}(Z_1,Z_2\mid \boldsymbol{\mu},\boldsymbol{\Sigma})
%\end{split}    
%\label{eq:approxmeanzerocase}
%\end{equation*}
%\end{corollary}
%The proof is straightforward from Proposition \ref{prop:deltaapproxlogitnormal}, by substituting $\boldsymbol{\mu}=\mathbf{0}$. This corollary reveals that the logit transformation of normal random variables rarely changes their correlation, given that the normal random variables have mean $0$. 


%For the proposed model, marginalizing out $\mathbf{Z}_{\boldsymbol{\tau}}$, we have $\boldsymbol{\mathcal{Z}}\mid \boldsymbol{\mu}_{\boldsymbol{\tau}},\boldsymbol{\Sigma}_{\boldsymbol{\tau},\boldsymbol{\tau}},\sigma_{\epsilon}^2\sim N(\boldsymbol{\mu}_{\boldsymbol{\tau}},\boldsymbol{\Sigma}_{\boldsymbol{\tau},\boldsymbol{\tau}}+\sigma_{\epsilon}^2\mathbf{I})$. As a direct result of Proposition \ref{prop:deltaapproxlogitnormal}, 



Our inference results are based on exact expressions, such as the ones in 
Proposition \ref{prop:meancovcondsignal}. Nonetheless, the approximate expressions 
derived in Proposition \ref{prop:deltaapproxlogitnormal} are practically useful to 
gain more insight on properties of the binary process, as well as for prior specification. 
Note that exploring properties of the binary process is not trivial due to the lack 
of general analytical forms for moments of logit-normal distributions. Hence, a 
connection with properties of the signal process is useful. For instance, if we 
specify the covariance for the signal process to decrease as a function of separation 
in time, %while always remaining positive, 
an analogous structure will hold (approximately) for the binary process. 
%
% (AK) The comment below is not relevant given the expanded formula in Proposition 2 
%
%As another example pertaining to prior specification, if we
%consider a constant mean function for the signal process a priori, the ratio of the 
%covariance for the signal process and the binary process is a constant. 


%
%The importance of Proposition \ref{prop:deltaapproxlogitnormal} is twofold. On one hand, 
%answering questions about the properties of the binary process is hard due to the nonexistence 
%of the analytical form for moments of a logit-normal random vector in general. 
%Nonetheless, the properties of the continuous signal process is well understood. 
%For example, we specify the covariance kernel for the signal process such that the covariance 
%within the vector $\mathbf{Z}_{\boldsymbol{\tau}}$ decreases as a function of separation in time, 
%while always remaining positive. However, previously we do not know whether such a property 
%is true for the sequence $\mathbf{Y}_{\boldsymbol{\tau}}$ as well. With the help of 
%Proposition \ref{prop:deltaapproxlogitnormal}, the answer is positive. On the other hand, 
%in applications we only observe the binary process, which could make specifying prior 
%hyperparameters for the signal process clueless. Proposition \ref{prop:deltaapproxlogitnormal} 
%paves the way for prior specification. For instance, consider a priori constant mean function 
%for the signal process. The ratio of the covariance for the signal process and the binary 
%process is then a known constant. Hence, the observed pattern of the binary data are directly 
%related to the signal process.  
%



%It is especially useful in prior specification, which will be discussed in detail in Section \ref{subsec:modelapply}.
%These two propositions pave the way for estimating the probability response curve 
%and the covariance structure of the binary process, conditional on the signal process. 
%As a direct result, we have
%\begin{equation*}
%\begin{split}
%    &\mathbf{P}_{\mathbf{1}\boldsymbol{\tau}}\approx \boldsymbol{\pi}(\mathbf{Z}_{\boldsymbol{\tau}})+\frac{\sigma_{\epsilon}^2}{2}\boldsymbol{\pi}^{\prime\prime}(\mathbf{Z}_{\boldsymbol{\tau}}),\quad \boldsymbol{\pi}^{\prime\prime}(\mathbf{Z}_{\boldsymbol{\tau}})=(\varphi^{\prime\prime}(Z_{\tau_1}),\cdots,\varphi^{\prime\prime}(Z_{\tau_T}))^{\top},\\
%    &\text{Cov}(\mathbf{Y}_{\tau},\mathbf{Y}_{\tau^{\prime}}\mid\mathbf{Z}_{\boldsymbol{\tau}},\sigma_{\epsilon}^2)=\frac{\sigma^2_{\epsilon}}{2}[\varphi^{\prime\prime}(Z_{\tau})\varphi(Z_{\tau^{\prime}})+\varphi(Z_{\tau})\varphi^{\prime\prime}(Z_{\tau^{\prime}})]-\varphi(Z_{\tau})\varphi(Z_{\tau^{\prime}})-\frac{\sigma^4_{\epsilon}}{4}\varphi^{\prime\prime}(Z_{\tau})\varphi^{\prime\prime}(Z_{\tau^{\prime}}),
%\end{split}    
%    \label{eq:approxestall}
%\end{equation*}
%which can provide posterior estimate of the probability response curve and covariance structure with posterior samples of $\mathbf{Z}_{\boldsymbol{\tau}}$ and $\sigma^2_{\epsilon}$. 


The previous discussion focuses on studying the f.d.d.s of the binary process 
given the signal process. Therefore, it is important to investigate 
the marginal f.d.d.s of the signal process. We show that, under the specification 
$\kappa =$ $(\nu-3)^{-1}$, the f.d.d.s. of the signal process correspond to a 
multivariate Student-t (MVT) distribution, and thus the signal process is a TP. 
We first state the definition of the MVT distribution and the TP
\citep[see, e.g.,][]{Shah2014}. Notice that we use the covariance matrix as a 
parameter for the MVT distribution, instead of the more common parameterization 
based on a scale matrix.

\begin{definition}
\label{def:defmvtandtp}
The random vector $\mathbf{Z}\in\mathbb{R}^n$ is MVT distributed, denoted 
$\mathbf{Z}\sim MVT(\nu,\boldsymbol{\mu},\boldsymbol{\Psi})$, 
%with degree of freedom parameter $\nu\in\mathbb{R}^{+}\setminus[0,2]$, %$\boldsymbol{\mu}\in\mathbb{R}^n$ and $\boldsymbol{\Psi}\in\Pi(n)$ ($\Pi(n)$ 
%is the set of real valued, $n\times n$, symmetric, positive definite matrices), 
%and write $\mathbf{Z}\sim MVT(\nu,\boldsymbol{\mu},\boldsymbol{\Psi})$, 
if it has density
\begin{equation*}
    %p(\mathbf{Z})=
\frac{\Gamma(\frac{\nu+n}{2})}{[(\nu-2)\pi]^{n/2}
\Gamma(\frac{\nu}{2})}|\boldsymbol{\Psi}|^{-1/2}
\left(
1 + \frac{(\mathbf{Z}-\boldsymbol{\mu})^T\boldsymbol{\Psi}^{-1}
(\mathbf{Z}-\boldsymbol{\mu})}{\nu-2} \right)^{-\frac{\nu+n}{2}}
\label{eq:defmvt}
\end{equation*}
where $\nu > 2$ is the degrees of freedom parameter, $\boldsymbol{\mu}\in\mathbb{R}^n$,
and $\boldsymbol{\Psi}$ is an $n\times n$ symmetric, positive definite matrix.
Under this parameterization, $E(\mathbf{Z})=\boldsymbol{\mu}$ 
and $Cov(\mathbf{Z})=\boldsymbol{\Psi}$. 

Consider a process $Z(\tau)$ formulated through mean function $\mu(\tau)$, a 
non-negative kernel function $\Psi(\tau,\tau)$, and parameter $\nu>2$, such that its 
f.d.d.s correspond to the MVT distribution with mean vector and covariance matrix induced 
by $\mu(\tau)$ and $\Psi(\tau,\tau)$, respectively. Then, $Z(\tau)$ follows a TP, denoted 
by $Z(\tau)\sim TP(\nu,\mu(\tau),\Psi(\tau,\tau))$.
%such that its f.d.d.s. 
%$\mathbf{Z}_{\boldsymbol{\tau}}\sim MVT(\nu,\boldsymbol{\mu}_{\boldsymbol{\tau}}.
%\boldsymbol{\Psi}_{\boldsymbol{\tau},\boldsymbol{\tau}})$, 
%then $Z(\tau)\sim TP(\nu,\mu(\tau),\Psi(\tau,\tau))$.  
\end{definition}


%\citet{Shah2014} established Definition \ref{def:defmvtandtp} and validated 
%that TP is a well-defined stochastic process. Accordingly, we can show the signal 
%process of the proposed model follows a TP after marginalizing its prior. 


Marginalizing over $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$
in (\ref{eq:finiterepbinmodel}) and (\ref{eq:multigpiwppool}),
the implied distribution for $\mathbf{Z}_{\boldsymbol{\tau}}$ is MVT, with degrees of 
freedom parameter $\nu$ (with $\nu > 3$ in our context), mean vector $\mu_0 \mathbf{1}$,
and covariance matrix $\boldsymbol{\Psi}_{\boldsymbol{\phi}} =$
$\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau})$. We thus obtain
the following result for the signal process.

\begin{proposition}
\label{prop:marginalsignal}
%The marginal distribution of $\mathbf{Z}_{\boldsymbol{\tau}}$ is a MVT with degree of 
%freedom $\nu$, mean $E(\mathbf{Z}_{\boldsymbol{\tau}})=\mu_0(\boldsymbol{\tau})$ and 
%covariance $Cov(\mathbf{Z}_{\boldsymbol{\tau}})=
%\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau})$. As a consequence, 
Under the model formulation in (\ref{eq:finiterepbinmodel}) and (\ref{eq:multigpiwppool}),
the signal process follows marginally a TP, that is, 
$Z\sim TP(\nu,\mu_0,\Psi_{\boldsymbol{\phi}})$.
\end{proposition}


%\begin{proof}
%The result is proved by considering the corresponding f.d.d.s. on any finite grids $\boldsymbol{\tau}$. Let the bold letter denote the corresponding process evaluated at $\boldsymbol{\tau}$. From the model assumption, 
%\begin{equation}
%    p(\mathbf{Z})=\int\int p(\mathbf{Z}\mid \boldsymbol{\mu},\boldsymbol{\Sigma})p(\boldsymbol{\mu}\mid \boldsymbol{\Sigma})p(\boldsymbol{\Sigma})d\boldsymbol{\mu}d\boldsymbol{\Sigma}
%    \label{eq:marginalsignal}
%\end{equation}
%We first notice that marginalizing over its mean $\boldsymbol{\mu}$,  $\mathbf{Z}\sim N(\boldsymbol{\mu}_0,(\nu-2)\boldsymbol{\Sigma})$. Based on that, 
%\begin{equation}
%    \begin{split}
%        p(\mathbf{Z})&=\int p(\mathbf{Z}\mid \boldsymbol{\Sigma})p(\boldsymbol{\Sigma})d\boldsymbol{\Sigma}\\
%        &\propto \int \frac{\exp\{-\frac{1}{2}\text{Tr}[(\boldsymbol{\Psi}_{\boldsymbol{\phi}}+\frac{(\mathbf{Z}-\boldsymbol{\mu}_0)(\mathbf{Z}-\boldsymbol{\mu}_0)^{\top}}{\nu-2})\boldsymbol{\Sigma}^{-1}]\}}{|\boldsymbol{\Sigma}|^{(\nu+|\boldsymbol{\tau}|+1)/2}}d\boldsymbol{\Sigma}\\
%        &\propto [1+\frac{(\mathbf{Z}-\boldsymbol{\mu}_0)^{\top}\boldsymbol{\Psi}_{\boldsymbol{\phi}}^{-1}(\mathbf{Z}-\boldsymbol{\mu}_0)}{\nu-2}]^{-(\nu+|\boldsymbol{\tau}|)/2},
%    \end{split}
%    \label{eq:marginalizescalematrix}
%\end{equation}
% which can be recognized as the kernel of a MVT distribution. Therefore, the result holds. 

%\end{proof}


Proposition \ref{prop:marginalsignal} is beneficial 
%Recognizing the marginal distribution of $Z(\tau)$ as TP benefits 
in terms of both computation and interpretation. Without a constraint on $\kappa$, as 
in \citet{Yang2016}, the marginal distribution of $\mathbf{Z}_{\boldsymbol{\tau}}$ does 
not have analytical form. Hence, for prediction at new time points, one has to sample 
from an IWP and a GP, which is computationally intensive, especially for a dense grid. 
In contrast, we can utilize the analytical form of the TP predictive distribution to 
develop a predictive inference scheme that resembles that of GP-based models
(see Section \ref{subsec:modelapply}). 
%Consequently, performing prediction is similar as under a GP. 
Moreover, the result highlights the model property that the degrees of freedom 
parameter $\nu$ controls how heavy tailed the process is. Smaller values of $\nu$ 
correspond to heavier tails. As $\nu$ gets larger, the tails resemble Gaussian tails. 
Moreover, $\nu$ controls the dependence between $Z_{\tau}$ and $Z_{\tau^{\prime}}$, 
which are jointly MVT distributed, with smaller values indicating higher dependence. 
Such interpretation of parameter $\nu$ facilitates the choice of its hyperprior. 


The local behavior of stochastic process realizations is crucial for interpolation. 
Under the longitudinal setting, continuous, or perhaps differentiable, signal process 
trajectories are typically anticipated. Evidently, the observed
data can not visually inform the smoothness of signal process realizations. Rather, 
such smoothness should be captured in the prior specification that incorporates 
information about the data generating mechanism. For weakly stationary processes, 
mean square continuity is equivalent to the covariance function being continuous at 
the origin \citep{Stein1999}. And, the process is $\iota$-times mean square differentiable 
if and only if the $2\iota$-times derivative of the covariance function at the origin 
exists and is finite. Under our model, the signal process follows a TP marginally. 
Its covariance structure is specified by the Matérn covariance function with smoothness 
parameter $\iota$. Referring to the behavior of the Matérn class of covariance functions 
at the origin, we obtain the following result for the mean square continuity and 
differentiability of the signal process.

\begin{proposition}
\label{prop:smoothsignal}
Consider the proposed model with marginal signal process 
$Z\sim TP(\nu,\mu_0,\Psi_{\boldsymbol{\phi}})$, where $\Psi_{\boldsymbol{\phi}}$ belongs to 
the Matérn family of covariance functions with smoothness parameter $\iota$. Then, 
the signal process is mean square continuous and $\lfloor\iota\rfloor$-times mean 
square differentiable.   
\end{proposition}

%Moreover, the smoothness property of the signal process facilitate the decomposition of the signal process. Consider the centered signal process $\zeta(\tau)=Z(\tau)-\mu(\tau)$ on the study period $[0,T]$. It is mean square integrable and mean square continuous. Hence, by Karhunen-Loève theorem, 
%	$\zeta(\tau)=\sum_{\ell=1}^{\infty}\sqrt{\lambda}_{\ell}\xi_{\ell}e_{\ell}(\tau)$, 
%where $\lambda_{\ell}$ and $e_{\ell}(t)$ are the eigenvalue and eigenfunction that satisfies the integral equation $\int_{\mathcal{T}}\Psi_{\boldsymbol{\phi}}(\tau,\tau^{\prime})e_{\ell}(\tau^{\prime})d\tau^{\prime}=\lambda_{\ell}e_{\ell}(\tau)$. We are particularly interested in the eigenfunctions associated with the larger eigenvalues, as they help to unfold the internal variability. We illustrate this point with a real data example in Section \ref{subsec:resultsrealapp}.  


%
%In conclusion, we have investigated properties that are helpful in model implementation. 
%Proposition \ref{prop:meancovcondsignal} and \ref{prop:deltaapproxlogitnormal} illustrate 
%how to obtain inference about the probability response curve and the covariance structure 
%of the binary process, conditioning on the signal process. Proposition \ref{prop:marginalsignal} 
%and \ref{prop:smoothsignal} further examine the distribution and smoothness property of the 
%signal process. The practical utility of these properties in prior specification and 
%posterior inference will be discussed in the following section.   
%

The results in this section study several properties that are useful in model
implementation. Indeed, the practical utility of such model properties with respect 
to prior specification and posterior inference is discussed in the next section.




%\subsection{Bayesian inference and computation}
\subsection{Prior specification and posterior inference}
\label{subsec:modelapply}

%\subsubsection{Prior specification}
%\label{subsubsec:priorspecbinmodel}

The model described in Section \ref{subsec:standardmodel} contains parameters 
$\{\sigma_{\epsilon}^2,\mu_0,\sigma^2,\rho,\nu\}$ whose prior hyperparameters need 
to be specified. We develop a default specification strategy that relies on the 
model properties explored in Section \ref{subsec:modprop}.
%to include relevant information into the prior distributions. 

First, we set the prior for $\mu_0$ such that the prior expected probability 
response curve does not favor any category, and the corresponding prior uncertainty 
bands span a significant portion of the unit interval. For instance, this can 
be achieved with prior $\mu_0\sim N(0,100)$ which yields prior expected probability
of positive response of about $1/2$ across $\tau$.
%$\text{E}(\text{Pr}(Y_{\tau}=1\mid Z_{\tau},\sigma_{\epsilon}^2))\equiv \frac{1}{2}$, $\forall \tau$. 
%In general, specifying prior for $\sigma^2$ and $\rho$ could be difficult. 
%This is because for most of the application scenarios we will not have 
%information about the variance and correlation structure of the unobserved 
%signal process, which are controlled by these parameters. 
In general, we would not expect to have available prior information about the 
variance and correlation structure of the unobserved signal process, which are 
controlled by parameters $\sigma^2$ and $\rho$. However, 
Proposition \ref{prop:deltaapproxlogitnormal} suggests an approximate relationship 
between the covariance structure of the binary process and the signal process, 
and we can thus specify the corresponding priors similarly to GP-based models. 
%Consequently, the prior hyperparameters $\{a_{\sigma},b_{\sigma},a_{\rho},b_{\rho}\}$ 
%can be determined similarly as in GP curve fitting. 
In particular, we select the uniform prior for the range parameter $\rho$ such that 
the correlation between $Z_{\tau}$ and $Z_{\tau^{\prime}}$ decreases to $0.05$ when 
the difference between $\tau$ and $\tau^{\prime}$ is within a pre-specified subset 
of the observation time window. For instance, for the data analysis in Section 
\ref{sec:realapp} where the total observation window comprises 72 days, we used a
$Unif(3,12)$ prior for $\rho$, which implies that the aforementioned correlation 
decreases to $0.05$ when the time difference ranges from 7 to 31 days. 
The hyperprior for $\nu$ is $Unif(a_{\nu},b_{\nu})$. We specify $a_{\nu}>3$ to 
reflect the constraint for $\Sigma/(\nu-3)$ to be a well-defined covariance matrix, 
and $b_{\nu}$ large enough such that the tail behavior of the marginal TP is hard 
to distinguish from that of a GP. For instance, a default choice is $a_{\nu}=4$ 
and $b_{\nu}=30$.


We follow \citet{Fong2010} to specify the prior for 
$\sigma_{\epsilon}^2\sim IG(a_{\epsilon},b_{\epsilon})$. Integrating 
out $\sigma_{\epsilon}^2$, the measurement error $\epsilon$ is marginally 
distributed as a univariate Student-t distribution with location parameter 0, scale 
parameter $b_{\epsilon}/a_{\epsilon}$, and degrees of freedom parameter $2a_{\epsilon}$. 
For a predetermined measurement error range $(-R,R)$ with degree of freedom $\upsilon$, 
we can use the relationship $\pm t_{1-(1-q)/2}^{\upsilon}\sqrt{b_{\epsilon}/a_{\epsilon}}=\pm R$ 
to obtain $a_{\epsilon}=$ $\upsilon/2$ and $b_{\epsilon}=$
$R^2\upsilon/[2(t_{1-(1-q)/2}^{\upsilon})^2]$, where $t^{\upsilon}_q$ is the 
$q$-th percentile of a Student-t distribution with $\upsilon$ degrees of freedom. 



%\subsubsection{Posterior inference}
%\label{subsubsec:postinfbinmodel}

Proceeding to posterior inference, we develop an MCMC algorithm based on 
%the multivariate representation in 
(\ref{eq:finiterepbinmodel}) and (\ref{eq:multigpiwppool}). We introduce layers of 
latent variables,  beginning with $\xi_{it}\sim PG(1,0)$ for every observation $Y_{it}$, 
where $PG(a,b)$ denotes the Pólya-Gamma distribution with shape parameter $a$ and 
tilting parameter $b$ \citep{Polson2013}. Denote the collection of Pólya-Gamma variables 
for each subject by $\boldsymbol{\xi}_i=(\xi_{i1},\cdots,\xi_{iT_i})^{\top}$. Also, 
introduce $\mathcal{Z}_{it}=Z_{it}+\epsilon_{it}$, and let $\boldsymbol{\mathcal{Z}}_i=$
$(\mathcal{Z}_{i1},\cdots,\mathcal{Z}_{iT_i})^{\top}$. Recall 
that $\boldsymbol{\tau}=\cup_{i=1}^n\boldsymbol{\tau}_i$ is the pooled grid. Denote 
the evaluations on the pooled grid by $\tilde{\mathbf{Z}}_i=Z_i(\boldsymbol{\tau})$ 
and let $\mathbf{Z}_i^*=\tilde{\mathbf{Z}}_i\setminus\mathbf{Z}_i$. 
That is, $\mathbf{Z}_i^*=Z_i(\boldsymbol{\tau}_i^*)$,
where $\boldsymbol{\tau}_i^*=\boldsymbol{\tau}\setminus\boldsymbol{\tau}_i$ is the set 
of grid points at which the $i$-th trajectory misses observations. Then, the hierarchical 
model for the data $\{Y_{it}: t=1,\cdots,T_i, i=1,\cdots,n\}$ can be expressed as 
\begin{equation*}
    \begin{split}
        &Y_{it}\mid\mathcal{Z}_{it}\stackrel{ind.}{\sim} Bin(1,\varphi(\mathcal{Z}_{it})),\,\,\xi_{it}\stackrel{i.i.d.}{\sim} PG(1,0),\,\, t=1,\cdots,T_i,\\
        &\boldsymbol{\mathcal{Z}}_i\mid \mathbf{Z}_i,\sigma_{\epsilon}^2\stackrel{ind.}{\sim} N(\mathbf{Z}_i,\sigma_{\epsilon}^2\mathbf{I}_{T_i}),\,\, \tilde{\mathbf{Z}}_i=(\mathbf{Z}_i,\mathbf{Z}_i^*)^{\top}\mid\boldsymbol{\mu},\boldsymbol{\Sigma}\stackrel{i.i.d.}{\sim}N(\boldsymbol{\mu},\boldsymbol{\Sigma}),\,\, i=1,\cdots,n,\\
        &\sigma_{\epsilon}^2\sim IG(a_{\epsilon},b_{\epsilon}),\,\,\boldsymbol{\mu}\mid \mu_0,\boldsymbol{\Sigma},\nu\sim N(\mu_0\mathbf{1},(\nu-3)\boldsymbol{\Sigma}),\,\,\mu_0\sim N(a_{\mu},b_{\mu}),\\
        & \boldsymbol{\Sigma}\mid \nu,\boldsymbol{\Psi}_{\boldsymbol{\phi}}\sim IW(\nu,\boldsymbol{\Psi}_{\boldsymbol{\phi}}),\,\,\boldsymbol{\Psi}_{\boldsymbol{\phi}}=\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}),\,\, \boldsymbol{\phi}=\{\sigma^2,\rho\},\\
        & \sigma^2\sim \text{Gamma}(a_{\sigma},b_{\sigma}),\,\,\rho\sim Unif(a_{\rho},b_{\rho}),\,\,\nu\sim Unif(a_{\nu},b_{\nu}).
    \end{split}
    \label{eq:hiermodelbin}
\end{equation*}
Hence, the joint posterior density of all model parameters can be written as
\begin{equation}
    \begin{split}
        p(\{\boldsymbol{\mathcal{Z}}_i\}_{i=1}^n,&\{\boldsymbol{\xi}_i\}_{i=1}^n,\{\tilde{\mathbf{Z}}_i\}_{i=1}^n,
        \boldsymbol{\mu},\boldsymbol{\Sigma},\sigma_{\epsilon}^2,\mu_0,\sigma^2,\rho,\nu\mid \{\mathbf{Y}_i\}_{i=1}^n)\\
        &\propto \prod_{i=1}^n\{p(\mathbf{Y}_i\mid \boldsymbol{\mathcal{Z}}_i,\boldsymbol{\xi}_i)p(\boldsymbol{\xi}_i)p(\boldsymbol{\mathcal{Z}}_i\mid \mathbf{Z}_i,\sigma_{\epsilon}^2)p(\mathbf{Z}_i^*\mid\mathbf{Z}_i,\boldsymbol{\mu},\boldsymbol{\Sigma})p(\mathbf{Z}_i\mid \boldsymbol{\mu},\boldsymbol{\Sigma})\}\\
        &\times p(\boldsymbol{\mu}\mid \mu_0,\boldsymbol{\Sigma},\nu)p(\boldsymbol{\Sigma}\mid \sigma^2,\rho,\nu)p(\sigma_{\epsilon}^2)p(\mu_0)p(\sigma^2)p(\rho)p(\nu).
    \end{split}
    \label{eq:jointpostmodelbin}
\end{equation}


The introduction of the latent variables enables a Gibbs sampling scheme with conditionally 
conjugate updates. Denote generically by $p(\boldsymbol{\theta}\mid -)$ 
the posterior full conditional for parameter $\boldsymbol{\theta}$. Notice 
that $p(\boldsymbol{\mathcal{Z}}_i,\boldsymbol{\xi}_i\mid -)\propto $
$p(\mathbf{Y}_i \mid \boldsymbol{\mathcal{Z}}_i,\boldsymbol{\xi}_i) p(\boldsymbol{\xi}_i) p(\boldsymbol{\mathcal{Z}}_i\mid \mathbf{Z}_i,\sigma_{\epsilon}^2)$, 
which matches the Bayesian logistic regression structure in \citet{Polson2013}. 
Therefore, $p(\boldsymbol{\mathcal{Z}}_i\mid -)$ and $p(\boldsymbol{\xi}_i\mid -)$ 
can be sampled directly. 
%Also seeing that $p(\sigma_{\epsilon}^2\mid -)\propto 
%\prod_{i=1}^nN(\boldsymbol{\mathcal{Z}}_i\mid %\mathbf{Z}_i,\sigma_{\epsilon}^2\mathbf{I}_{\boldsymbol{\tau}_i})
%IG(\sigma_{\epsilon}^2\mid a_{\epsilon},b_{\epsilon})$, we have 
%$\sigma_{\epsilon}^2\mid -\sim IG(a_{\epsilon}^*,b_{\epsilon}^*)$. 
Factorizing the prior of $\tilde{\mathbf{Z}}_i$ as $p(\tilde{\mathbf{Z}}_i|\boldsymbol{\mu},\boldsymbol{\Sigma})=$
$p(\mathbf{Z}_i^*\mid\mathbf{Z}_i,\boldsymbol{\mu},\boldsymbol{\Sigma})p(\mathbf{Z}_i\mid \boldsymbol{\mu},\boldsymbol{\Sigma})$, 
results in $p(\mathbf{Z}_i^*,\mathbf{Z}_i| -)\propto$
$ p(\mathbf{Z}_i^*\mid\mathbf{Z}_i,\boldsymbol{\mu},\boldsymbol{\Sigma})
p(\mathbf{Z}_i\mid \boldsymbol{\mu},\boldsymbol{\Sigma})
p(\boldsymbol{\mathcal{Z}}_i\mid \mathbf{Z}_i,\sigma_{\epsilon}^2)$. This forms yields
ready updates for $\mathbf{Z}_i^*$ and $\mathbf{Z}_i$ using GP-based predictive sampling.
All other model parameters can be sampled using standard updates. The details of the 
MCMC algorithm are given in the Supplementary Material. 


We have linked the probability response curve and covariance structure of the 
binary process $Y_i(\tau)$ to the corresponding signal process $Z_i(\tau)$. 
%To perform posterior estimation of them, we need posterior samples of the signal 
%process, which are obtained as 
To estimate the signal process, we obtain posterior samples for 
$\mathbf{Z}_i^+=Z_i(\boldsymbol{\tau}^+)$, where 
$\boldsymbol{\tau}^+\supset \boldsymbol{\tau}$ is a finer grid than the pooled grid.
Denote $\check{\boldsymbol{\tau}}=\boldsymbol{\tau}^+\setminus\boldsymbol{\tau}$ 
as the time points where none of the subjects have observations, and 
let $\check{\mathbf{Z}}_i=Z_i(\check{\boldsymbol{\tau}})$. Using the marginal TP 
result from Proposition \ref{prop:marginalsignal}, 
\begin{equation*}
    \begin{pmatrix} \tilde{\mathbf{Z}}_i \\ \check{\mathbf{Z}}_i \end{pmatrix}\sim 
    MVT\left( \nu,
    \begin{pmatrix} \boldsymbol{\mu}_{0\boldsymbol{\tau}} \\ \boldsymbol{\mu}_{0\check{\boldsymbol{\tau}}} \end{pmatrix}, \begin{pmatrix} \boldsymbol{\Psi}_{\boldsymbol{\tau},\boldsymbol{\tau}} & \boldsymbol{\Psi}_{\boldsymbol{\tau},\check{\boldsymbol{\tau}}} \\ \boldsymbol{\Psi}_{\check{\boldsymbol{\tau}},\boldsymbol{\tau}} & 
    \boldsymbol{\Psi}_{\check{\boldsymbol{\tau}},\check{\boldsymbol{\tau}}}
    \end{pmatrix} \right),
    \label{eq:jointpostpred}
\end{equation*}
where $\boldsymbol{\mu}_{0\cdot}=\mu_0\mathbf{1}_{|\cdot|}$, and 
$\boldsymbol{\Psi}_{\cdot,\cdot}$ denotes the covariance function evaluation 
$\Psi_{\boldsymbol{\phi}}(\cdot,\cdot)$. Next, based on the conditionals of the 
MVT distribution \citep{Shah2014},
\begin{equation}
    \check{\mathbf{Z}}_i\mid \tilde{\mathbf{Z}}_i \sim 
    MVT \left( \nu+|\boldsymbol{\tau}|,\check{\boldsymbol{\mu}}_{i\check{\boldsymbol{\tau}}},\frac{\nu+S_{i\boldsymbol{\tau}}-2}{\nu+|\boldsymbol{\tau}|-2}\check{\boldsymbol{\Psi}}_{\check{\boldsymbol{\tau}},\check{\boldsymbol{\tau}}}\right),
    \label{eq:condpostpred}
\end{equation}
with $\check{\boldsymbol{\mu}}_{i\check{\boldsymbol{\tau}}}=\boldsymbol{\Psi}_{\check{\boldsymbol{\tau}},\boldsymbol{\tau}}\boldsymbol{\Psi}_{\boldsymbol{\tau},\boldsymbol{\tau}}^{-1}(\tilde{\mathbf{Z}}_i- \boldsymbol{\mu}_{0\boldsymbol{\tau}})+ \boldsymbol{\mu}_{0\check{\boldsymbol{\tau}}}$, $S_{i\boldsymbol{\tau}}=(\tilde{\mathbf{Z}}_i- \boldsymbol{\mu}_{0\boldsymbol{\tau}})^{\top}\boldsymbol{\Psi}_{\boldsymbol{\tau},\boldsymbol{\tau}}^{-1}(\tilde{\mathbf{Z}}_i- \boldsymbol{\mu}_{0\boldsymbol{\tau}})$ and $\check{\boldsymbol{\Psi}}_{\check{\boldsymbol{\tau}},\check{\boldsymbol{\tau}}}=\boldsymbol{\Psi}_{\check{\boldsymbol{\tau}},\check{\boldsymbol{\tau}}}-\boldsymbol{\Psi}_{\check{\boldsymbol{\tau}},\boldsymbol{\tau}}\boldsymbol{\Psi}_{\boldsymbol{\tau},\boldsymbol{\tau}}^{-1}\boldsymbol{\Psi}_{\boldsymbol{\tau},\check{\boldsymbol{\tau}}}$. 
%In practice, for post burn-in samples $\tilde{\mathbf{Z}}_i^{(s)},
%\mu_0^{(s)},\boldsymbol{\phi}^{(s)}$ and $\nu^{(s)}$, we can obtain posterior predictive 
%samples of $\check{\mathbf{Z}}_i^{(s)}$ from MVT defined in (\ref{eq:condpostpred}). 
%Joint with $\tilde{\mathbf{Z}}_i^{(s)}$ produces posterior estimates for the signal 
%process $Z_i(\tau)$ on $\boldsymbol{\tau}^+$, which allows us to perform posterior 
%inference for quantities of interest regarding the original binary process $Y_i(\tau)$ 
%on the finer grid. 
Using (\ref{eq:condpostpred}), given each posterior sample for $\tilde{\mathbf{Z}}_i$, 
$\mu_0$, $\boldsymbol{\phi}$ and $\nu$, we can complete the posterior realization for the 
signal process over the finer grid. As discussed in Section \ref{subsec:modprop}, we can 
then obtain full posterior inference for functionals of the binary process.


The predictive distribution of the signal process also illustrates the information 
borrowed across subjects. For the $i$-th subject, the grid, $\boldsymbol{\tau}^+$,
where predictions are made can be 
partitioned as $\boldsymbol{\tau}_i\cup\boldsymbol{\tau}_i^*\cup\check{\boldsymbol{\tau}}$, 
where $\boldsymbol{\tau}_i^*=\boldsymbol{\tau}\setminus\boldsymbol{\tau}_i$ represents 
the grid points where subject $i$ does not have observations, while at least one of 
the other subjects have observations. Then, we first predict $Z_i(\boldsymbol{\tau}_i^*)$ 
conditioning on $Z_i(\boldsymbol{\tau}_i)$ by the GP predictive distribution, and next 
predict $Z_i(\check{\boldsymbol{\tau}})$ conditioning on $Z_i(\boldsymbol{\tau}_i)$ 
and $Z_i(\boldsymbol{\tau}_i^*)$ by the TP predictive distribution. 
Comparing with the GP, (\ref{eq:condpostpred}) suggests the TP is scaling the predictive 
covariance by the factor $\frac{\nu+S_{i\boldsymbol{\tau}}-2}{\nu+|\boldsymbol{\tau}|-2}$. 
Note that $S_{i\boldsymbol{\tau}}$ is distributed as the sum of squares of 
$|\boldsymbol{\tau}|$ independent $MVT_1(\nu,0,1)$ random variables and hence 
$\text{E}(S_{i\boldsymbol{\tau}})=|\boldsymbol{\tau}|$. Accordingly, if we have made 
good interpolation prediction, the predictive covariance for extrapolation of
$Z_i(\check{\boldsymbol{\tau}})$ is expected to scale down and vice versa. 
Comparing with predicting both $Z_i(\boldsymbol{\tau}_i^*)$ and 
$Z_i(\check{\boldsymbol{\tau}})$ conditioning on $Z_i(\boldsymbol{\tau}_i)$ 
through the GP predictive distribution, our model allows using information across 
subjects to adjust the individual %observed 
trajectory's credible interval.  




\subsection{Synthetic data examples}
\label{subsec:simstudy}

We assess the model by applying it to carefully designed simulation scenarios that reflect 
our main contributions. The full details are provided in the Supplementary Material. Here, 
we briefly discuss the simulation study setting and summarize the main findings.

For the two sets of simulation studies we considered, the 
longitudinal binary responses are generated from the following generic process:
\begin{equation}
    \begin{split}
    &%\mathbf{Y}_i = 
    Y_i(\boldsymbol{\tau}_{i}) \mid \mathcal{Z}_i(\boldsymbol{\tau}_{i}) \stackrel{ind.}{\sim} 
    Bin(1,\eta(\mathcal{Z}_i(\boldsymbol{\tau}_{i}))), \quad \boldsymbol{\tau}_i=(\tau_{i1},\cdots,\tau_{iT_i}),\quad i=1,\cdots,n,\\
    & \mathcal{Z}_i(\boldsymbol{\tau}_{i}) %=\boldsymbol{\mathcal{Z}}_i
    =f(\boldsymbol{\tau}_i)+\boldsymbol{\omega}_i+\boldsymbol{\epsilon}_i\quad \boldsymbol{\epsilon}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},\sigma_{\epsilon}^2\mathbf{I}),
    \end{split}   
    \label{eq:simgendata}
\end{equation}
where $\eta(\cdot)$ is a generic link function mapping $\mathbb{R}$ to $(0,1)$, $f(\tau)$ 
is a signal function, and $\boldsymbol{\omega}_i$ is a realization from a mean zero 
continuous stochastic process that depicts the temporal covariance within the $i$-th subject. 


The first set of simulation studies focuses on evaluating the effectiveness of the 
proposed model in capturing the fluctuation of the temporal trend. We consider 
different link function, signal function, and temporal covariance structure combinations, 
and we simulate unbalanced data with different sparsity levels. The results demonstrate 
that, despite the data generating process and the sparsity level, the model can 
recover not only the subject's probability response curve, but also the underlying 
continuous signal function.   


The objective of the second set of simulation studies is to explore the performance 
of the proposed model in estimating the within subject covariance structure. To this end, 
we examine a number of possible choices for generating the $\boldsymbol{\omega}_i$ 
in (\ref{eq:simgendata}), which imply covariance structures that are not of the 
same form as the covariance kernel of the model. The results reveal that the model 
can recover the true covariance between the signal variables, 
$(Z_i(\tau_{it}),Z_i(\tau_{i t^{\prime}}))$, and the binary 
responses, $(Y_i(\tau_{it}),Y_i(\tau_{i t^{\prime}}))$, thus providing empirical 
evidence for the robustness of the covariance kernel choice. 


In both cases, we examine simplified versions of the model for comparison. 
The simplified models are constructed by modeling either the mean structure or 
the covariance structure parametrically in the two sets of simulation studies, 
respectively. Demonstrating that the proposed model outperforms its parametric 
backbones, we highlight the practical utility of the nonparametric modeling 
for the mean and covariance structure.  




\subsection{Connections with existing literature}
\label{subsec:literaturereview}

% nonparametric modeling approaches

%As a Bayesian nonparametric approach to model longitudinal binary response, 
Our methodology is broadly related with certain Bayesian nonparametric methods. 
The proposed model is related to a particular class of conditional models, known as 
transition models, which induce the aging effect by allowing past values to explicitly 
affect the present observation, usually through autoregressive dynamics. 
\citet{DiLucca2013} studied a class of non-Gaussian autoregression models
%They model a sequence of continuous outcomes through a dependent DP with an additional normal 
%kernel as a prior for the regression on lagged terms in an autoregression. It can be extended 
for continuous responses, which can be extended to handle binary longitudinal outcomes by 
treating them as a discretized version of the continuous outcomes. 
%In addition, motivated by a specific application in fisheries science, 
\citet{MariaJASA2018} developed a nonparametric density regression model for ordinal 
regression relationships that evolve in discrete time. 
%Their methodology was built on the well-studied DP mixture model for cross-sectional 
%ordinal regression \citep{DeYoreoKottas2018} as the marginal at a specific time, while 
%introducing temporal dependence in the weights and atoms of the DP constructive definition. 
Compared with the proposed methodology, these models are more flexible in terms of the 
binary response distribution. However, it is demanding to handle higher than first-order
dynamics, and there is no natural way to treat missing data under a discrete time 
autoregressive framework, hindering applications for unbalanced longitudinal studies.


The proposed model is more closely related to subject-specific models, where the 
responses are assumed to be independent conditioning on subject-specific effects. 
The main approach has been to construct models for longitudinal binary responses
building from the various Bayesian nonparametric models for longitudinal continuous data, 
developed under the mixed effects framework \citep[e.g.,][]{LiLinMuller2010,Ghosh2010,Quintana2016}.
For instance, embedding a Dirichlet process mixture of normals prior as the probability model 
for the latent variables, \citet{Jara2007} and \citet{TangDuan2012} consider binary responses,
and \citet{Kunihama2019} handle mixed-scale data comprising continuous and binary responses. 
The proposed model differs in the way of treating subject-specific effects, and it arguably 
offers benefits in terms of computational efficiency. 
%Moreover, for the specific problem we considered, no covariate information is associated 
%with the longitudinal responses. The DP mixture models can be criticized for over-parameterizing.  


There is a growing trend of adopting functional data analysis tools in longitudinal data 
modeling. These methods specify observations as linear combinations of functional principal 
components (FPCs), with the FPCs represented as expansions of a pre-specified basis. 
Bayesian methods include \citet{Jiang2020} for continuous responses, and 
\citet{vanDerLinde2009} for binary and count responses. Challenges include inference 
which is sensitive to the basis choice, and a complex orthogonality constraint on the FPCs.
Recently, \citet{JamseAmy2022} proposed 
%the relaxed mutual orthogonal processes which 
an approach that can serve as foundation for generalized FPC analysis of sparse and irregular 
binary responses. Nonetheless, our model involves a more parsimonious formulation, 
including the structure with the GP and TP predictive distributions. 
%Their approach provides advantages in terms of flexibility and computational convenience. 
%Comparing with them, we use a more parsimonious model formulation 
%that achieves comparable results. <-- this is asking for trouble, since we do not have that comparison 


%The development of statistical methods for longitudinal binary data stems from models for longitudinal continuous responses. Since the most natural way to view binary data is to postulate the existence of a latent continuous variable associated with each response, such models can be extended to deal with binary data through appropriate link function. A plethora of literature have covered the topic of longitudinal continuous data analysis \citep{HedekerGibbons2006,Verbeke2009,Fitzmaurice2011}, with the majority of them under the mixed effects modeling framework. Suppose we have repeated measurements on $n$ subjects, denoted as $\mathbf{Z}_i=(Z_{i1},\cdots,Z_{iT_i})^T$, $i=1,\cdots,n$. Typically the mixed effects model assumes $\mathbf{Z}_i\sim N(\mathbf{u},\mathbf{V}_i)$, where $\mathbf{u}$ denote the common fixed effect across subjects, and $\mathbf{V}_i$ is the random subject effect that depicts the influence of subjects on their repeated observations \citep{Diggle1988}. The crucial question for the mixed effects model is the choice of fixed and random effects' structure, which has various options \citep{Lindstrom1990,Shi1996,Zhang2001}. These models have been extended to deal with dichotomous data by adopting a link function (logit, probit, etc.), resulting in the generalized mixed models (GMM). For a comprehensive review about GMM, we refer to \citet{Hedeker2008}.
%These models have been extended to deal with dichotomous and ordinal data by adopting a link function (logit, probit, etc.), and a specific representation of response probabilities (proportional odds, adjacent-categories, continuation-ratio, etc.), resulting in the generalized mixed models (GMM). For a comprehensive review about GMM, we refer to \citet{Hedeker2008}. 



%Seeking for more flexibility, one option is to assume the fixed or random effects of the latent variable have more sophisticated, nonparametric structure. Popular choices include modeling the fixed effect through smoothing techniques \citep{HeZhu2002,WuZhang2006}, applying a matrix stick-breaking process for a residual covariance structure \citep{Das2014}, capturing subject random effect by stochastic process \cite{ZhangLin1998}, putting a Dirichlet process (DP) prior \citep{LiLinMuller2010} or mixture of Pólya trees \citep{Ghosh2010} for random effect distributions, and using a combination of stochastic processes and DP mixture model for the subject random effect \citep{Quintana2016}. Admittedly, such models can be embedded in a hierarchical framework to deal with discrete responses. For instance, under the GMM framework which incorporate a DP mixture of normal prior as probability model for the latent variables,  \citet{Jara2007} and \citet{TangDuan2012} consider binary responses and \citet{Kunihama2019} handles mixed-scale data consisted by continuous and binary responses. Nonetheless, these approaches may not be applicable beyond dichotomous responses because of the computational burden. Concerning multivariate longitudinal ordinal responses, \citet{Tran2021} proposes to use a latent factor model within the GMM framework where the random effect is modeled by Ornstein-Uhlenbeck process. Albeit its good performance, the proposed model is restrictive in the sense that it depends on pre-specified covariance structure. In conclusion, these models are trading between flexibility and computational difficulty. Even though they achieved the balance for their specific purpose respectively, they are not the ideal option for the specific problem we considered. Besides, they examine the mean and covariance structure of the latent variable separately, which may neglect their dependence. 

% transition modeling framework
% Besides, specifying the smoothness parameter of the Matérn covariance kernel to satisfies $\iota=p-\frac{1}{2}$, we effectively model the signal process through a particular form of a continuous time AR$(p)$ process. Hence, the proposed model is related to other transition models. For example, 
