\section{Synthetic data examples}
\label{sec:simstudy}

The principal goal of analyzing longitudinal data is to estimate the mean and covariance structure of the subject's repeated measurements. We conduct simulation studies to evaluate the proposed method on fulfilling this goal. In the following, Section \ref{subsec:simmean} evaluates the
reliability of the proposed model in capturing the fluctuation of the mean structure, and Section \ref{subsec:simcov} explores the performance of the proposed model in estimating within subject covariance structure. 
%Both of them deal with dichotomous ordinal responses. 
Unless otherwise specified, the posterior analyses in this section are based on 5000 posterior samples collected every 4 iterations from a Markov chain of 30000 iterations, with the first 10000 samples being discarded. 

 
\subsection{Estimating mean structure}
\label{subsec:simmean}

Consider a generic process of generating longitudinal binary responses,
\begin{equation}
\begin{split}
    &\mathbf{Y}_i=Y_i(\boldsymbol{\tau})\stackrel{i.i.d.}{\sim} Bin(1,\eta(\mathcal{Z}_i(\boldsymbol{\tau}))),\quad \boldsymbol{\tau}_i=(\tau_{i1},\cdots,\tau_{iT_i}),\quad i=1,\cdots,n,\\
    & \mathcal{Z}_i(\boldsymbol{\tau})=\boldsymbol{\mathcal{Z}}_i=f(\boldsymbol{\tau}_i)+\boldsymbol{\omega}_i+\boldsymbol{\epsilon}_i\quad \boldsymbol{\epsilon}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},\sigma_{\epsilon}^2\mathbf{I}),
    \end{split}    
    \label{eq:datagensim}
\end{equation}
where $\eta(\cdot)$ is a generic link function mapping $\mathbb{R}$ to $(0,1)$, $f(\tau)$ is a signal function, and 
$\boldsymbol{\omega}_i$ is a realization from a mean zero continuous stochastic process that depicts the temporal covariance within subject. The objective is twofold. First, to estimate the subject's probability response curve, which is defined as the probability of obtaining positive response, as a function of time. Second, to estimate the true underlying signal function. 

We consider three data generating processes. The specific choice of $\eta(\cdot)$, $f(\tau)$ and $\boldsymbol{\omega}_i$ for each generating process is summarized as follows:
\begin{itemize}
\item Case 1: $\eta_1(\cdot)=\varphi(\cdot)$, where $\varphi(\cdot)$ is the expit function, $f_1(\tau)=0.3+3\sin(0.5\tau)+\cos(\tau/3)$, and $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},K_1(\boldsymbol{\tau},\boldsymbol{\tau}))$, with covariance kernel $K_1(\tau_{t},\tau_{t^{\prime}})=\exp(-|\tau_t-\tau_{t^{\prime}}|^2)$.
\item Case 2: $\eta_2(\cdot)=\Phi(\cdot)$, where $\Phi(\cdot)$ denotes the CDF of standard normal distribution, $f_2(\tau)=0.1+2\sin(0.25\tau)+\cos(0.25\tau)$, and $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} MVT(5,\mathbf{0},K_2(\boldsymbol{\tau},\boldsymbol{\tau}))$, with covariance kernel $K_2(\tau_{t},\tau_{t^{\prime}})=\frac{1}{3}\exp(-|\tau_t-\tau_{t^{\prime}}|^2)$.
\item Case 3: a mixture of Case 1 and Case 2, with equal probability of generating data from each model. 
\end{itemize}

For $n=30$ subjects, we simulate $T=31$ binary observations at time $\tau=0,\cdots,30$, following the aforementioned data generating processes. To enforce an unbalanced study design, we randomly drop out a proportion of the simulated data. We term the drop out proportion sparsity level, for which we consider $10\%$, $25\%$ and $50\%$. 

The proposed hierarchical model is applied to the data, with a weakly informative prior placed on the mean structure. We obtain posterior inference of the probability response curve and the signal process on a finer grid $\boldsymbol{\tau}^+=(0,\frac{1}{3},\frac{2}{3},\cdots,30)$. Figure \ref{fig:signalprobcurvesub} plots posterior point and interval estimates of the subject's probability response curve for a randomly selected one in each case. Despite the data generating process and the sparsity level, the model can recover the evolution of the underlying probability used in generating binary responses. We observe a shrink in the interval estimate at the set of grid points where at least one subject has observation, that is, $\boldsymbol{\tau}$. The expanding of the credible interval width at $\check{\boldsymbol{\tau}}$ reflect the lack of information at those time grids.   

\begin{figure}[t!]
\centering
\begin{subfigure}{\textwidth}
  \centering 
  % include first image
  \includegraphics[width=16cm,height=3.2cm]{SignalProbCurveSub90.png}  
  \caption{Sparsity level at $10\%$.}
  \label{subfig:signalprobcurve90}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering 
  % include first image
  \includegraphics[width=16cm,height=3.2cm]{SignalProbCurveSub75.png}  
  \caption{Sparsity level at $25\%$.}
  \label{subfig:signalprobcurve75}
\end{subfigure}
\begin{subfigure}{\textwidth}
  \centering
%  % include second image
  \includegraphics[width=16cm,height=3.2cm]{plot/SignalProbCurveSub50.png}  
  \caption{Sparsity level at $50\%$.}
  \label{subfig:signalprobcurve50}
\end{subfigure}

\caption{Simulation study regarding the mean structure. Inference results for the probability response curve.  In each panel, the dashed line and shaded region correspond to the posterior mean and $95\%$ credible interval estimates, the (orange) dot is the original binary data, whereas the (green) cross denotes the true probability of generating that responses.}
\label{fig:signalprobcurvesub}
\end{figure}

We further investigate the model's ability in out-of-sample prediction, by estimating the probability response curve for a new subject from the same cohort. Figure \ref{fig:signalprobcurvenew} shows the posterior point and interval estimates of $\text{Pr}(Y_{*}(\tau_{*t})=1)$, including, as a reference point, the posterior mean estimates of each subject's probability response curve $\text{Pr}(Y_{i}(\tau_{it})=1)$, $i=1,\cdots,n$. The true probability function that triggered the binary response, given as the signal transformed by the link function, is also shown in the figure. It is obtained with the simulated data with $10\%$ sparsity, while there is no major difference for the other two sparsity levels. The behavior of the probability response curve for the new subject is to be expected. It follows the overall trend depicted by the true underlying probability function, while suffers from a comparable level of measurement error with the observed subjects.  

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=4cm]{SignalProbCurveNew.png}
\caption{Simulation study regarding the mean structure. Prediction of the probability response curve for a new subject. In each panel, the dashed lines and shaded region shows the posterior mean and $95\%$ interval estimates of probability response curve for a new subject. The solid lines are the posterior mean estimates of probability response curves for the in-sample subjects. The dotted line is the true probability function for generating binary responses.}
\label{fig:signalprobcurvenew}
\end{figure}

It is also of interest to assess the model's ability in recovering the underlying continuous signal process, since the signal process describes the intrinsic behavior and is crucial to answer related scientific questions.  
In our proposed model, the signal process is modeled nonparametricly through a GP. To further emphasize the benefits of this model formulation, we compare the proposed model with its simplified backbone. The simpler model differs from the original one in modeling the mean function. Instead of modeling the mean function $\mu$ through a GP, we consider modeling it parametricly by $\mu(\tau)\equiv\mu_0$, and $\mu_0\sim N(a_{\mu},b_{\mu})$. The model's ability in capturing the signal process is summarized by the rooted mean square error (RMSE), which is defined by $\text{RMSE}^{\mathcal{M}}=\sqrt{\frac{1}{n}\sum_{i=1}^n\frac{1}{|\boldsymbol{\tau}^+|}\sum_{\tau\in\boldsymbol{\tau}^+}(\hat{Z}^{\mathcal{M}}_i(\tau)-f(\tau))^2}$. Here $\hat{Z}^{\mathcal{M}}_i(\tau)$ denote the model $\mathcal{M}$ estimated signal for subject $i$ evaluated at time $\tau$, which can be obtained at every MCMC iteration. Figure \ref{fig:signalRMSE} explores the posterior distribution of the RMSE under the proposed model and its simplified version, for different data generating process and sparsity level combinations. Despite the scenario, the proposed model shows a notably smaller RMSE. Contrasting the performance with the simpler model highlights the practical utility of including the layer of GP for the mean function in terms of effective estimation of the underlying continuous signal process.     

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=4cm]{SignalRMSE.png}
\caption{Simulation study regarding the mean structure. Box and violin plots of the posterior samples of RMSE for different data generating process and sparsity level combinations. The red box corresponds to the proposed model while the blue box is for the simplified model. }
\label{fig:signalRMSE}
\end{figure}


\subsection{Estimating covariance structure}
\label{subsec:simcov}

Since we emphasize the importance of modeling dependence in longitudinal data, we now explore how well our model works for estimating different covariance structure. Consider the data generating process in (\ref{eq:datagensim}), with expit link function and signal $f(\tau)=0.1+2\sin(0.5\tau)+\cos(0.5\tau)$. We examine a number of possible choices for generating $\boldsymbol{\omega}_i$, that imply covariance structures which would not be in the same form as the covariance kernel used in the proposed model. The primary interest is to exhibit the robustness of covariance kernel choice to different true covariance structures. We let $T_i=T$ and $\tau_{it}=\tau_t$, namely that all subjects are observed over the same time grids. For $n=100$ subjects, we generate sequences of length $T=11$ at time $\tau=0,\cdots,10$. We study the following options of generating $\boldsymbol{\omega}_i$:
         
\begin{itemize}
\item Case 1: $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},K_1(\boldsymbol{\tau},\boldsymbol{\tau}))$, with squared exponential kernel $K_1(\tau_{t},\tau_{t^{\prime}})=\exp(-|\tau_t-\tau_{t^{\prime}}|^2/(2\cdot 3^2))$. Each realized trajectory is infinitely differentiable. 
\item Case 2:  $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},K_2(\boldsymbol{\tau},\boldsymbol{\tau}))$, with exponential kernel $K_2(\tau_{t},\tau_{t^{\prime}})=\exp(-|\tau_t-\tau_{t^{\prime}}|/5)$. Each realization is effectively from a continuous-time AR(1) GP.
\item Case 3:  $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} MVT(5,\mathbf{0},K_3(\boldsymbol{\tau},\boldsymbol{\tau}))$, with compound symmetry kernel $K_3(\tau_{t},\tau_{t^{\prime}})=\mathbf{I}_{\{\tau_t=\tau_{t^{\prime}}\}}+0.4\mathbf{I}_{\{\tau_t\neq\tau_{t^{\prime}}\}}$. The covariance between two observations remains a constant, despite their distance.
\item Case 4: $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} MVT(5,\mathbf{0},K_4(\boldsymbol{\tau},\boldsymbol{\tau}))$, with kernel $K_4(\tau_{t},\tau_{t^{\prime}})=0.7K_2(\tau_{t},\tau_{t^{\prime}})+0.3K_3(\tau_{t},\tau_{t^{\prime}})$, a mixture of AR(1) and compound symmetry covariance structure.
\end{itemize}

In terms of longitudinal binary responses, the covariance structure can be elucidated in two senses, namely the covariance between the pair of binary data $(Y_i(\tau_{t}),Y_i(\tau_{t^{\prime}}))$ and between the pair of signal $(Z_i(\tau_t),Z_i(\tau_{t^{\prime}}))$. We consider the covariance structure of the signal process first. From Proposition \ref{prop:marginalsignal}, $\text{Cov}(Z_i(\tau_t),Z_i(\tau_{t^{\prime}}))=\Psi_{\boldsymbol{\phi}}(\tau_t,\tau_{t^{\prime}})$, $\forall i$, where the covariance function $\Psi_{\boldsymbol{\phi}}$ is defined in (\ref{eq:matern52covfun}). Hence, the signal covariance structure estimated from the model is also isotropic, facilitating a graphic comparison between the posterior estimate of  $\Psi_{\boldsymbol{\phi}}(\tau_d)$ versus the true covariance kernel $K(\tau_d)$, where $\tau_d=|\tau_t-\tau_{t^{\prime}}|$. The results are presented in Figure \ref{fig:covariogram}. As expected, the proposed model recovers the truth, despite the mis-specification of the covariance kernel. Comparing with the other three cases, the posterior point estimate of covariance kernel is less accurate in Case 3. This can be explained by noticing that the constant covariance in that case violates the model assumption. Nonetheless, the posterior interval still covers the truth. 

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=4cm]{Covariogram.png}
\caption{Simulation study regarding the covariance structure. Inference results for the signal covariance kernels. In each panel, the dashed line and shaded region correspond to the posterior mean and 95\% credible interval estimates, whereas the solid line denotes the true covariance kernel.}
\label{fig:covariogram}
\end{figure}

As for the covariance between the pair of binary data, we consider two measurements, the Pearson correlation coefficient and the tetrachoric correlation coefficient. For a review of the definitions and properties of these two correlation coefficients, we refer to \citet{Joakim2011}. At each MCMC iteration, we predict a new sequence of binary responses of length $T$, denoted as $\{Y^{(s)}_{i^*}(\boldsymbol{\tau}):s=1,\cdots,S\}$. Correspondingly, we also obtain samples of binary sequences from the true data generating process, denoted by $\{\hat{Y}^{(s)}_{i^*}(\boldsymbol{\tau}):s=1,\cdots,S\}$. Both sets of binary sequences form $S/n$ datasets that mimic  the original samples. From the datasets comprised by posterior predictive samples $Y^{(s)}_{i^*}(\boldsymbol{\tau})$, we obtain interval estimates of the two correlation coefficients. In addition, for $\hat{Y}^{(s)}_{i^*}(\boldsymbol{\tau})$ that are generated from the truth, we obtain point estimates, which can be viewed as the correlation coefficients from the data, accounting for the variation in the data generating process. Notice that marginally the binary process is not guaranteed to be isotropic. Hence, the correlation coefficients should be calculated for every possible pair of $(\tau_t,\tau_{t^{\prime}})\in\boldsymbol{\tau}$. The resulting point and interval estimates of both types of correlation coefficients are displayed in Figure \ref{fig:bincorrCI}. All the posterior interval estimates cover the truth, indicating that the proposed model effectively captures the binary covariance structure.  

 \begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCIsqexp}
            \caption{Case 1.}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCIou}
            \caption{Case 2.}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCIcsy}
            \caption{Case 3.}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
            \includegraphics[width=\textwidth,height=4cm]{bincorrCImix}
            \caption{Case 4.}
    \end{subfigure}
    \caption{Simulation study regarding the covariance structure. Posterior interval estimate of correlation coefficients (``box'') versus point estimate obtained from the true data generating process (``$\star$''). In each panel, the upper triangle and the lower triangle are for the Pearson and the  rachoric correlation coefficient, respectively.}
    \label{fig:bincorrCI}
\end{figure}


%\begin{figure}[t!]
%\centering
%\begin{subfigure}{\textwidth}
%  \centering 
  % include first image
%  \includegraphics[width=16cm,height=4cm]{bincorrphicover.png}  
%  \caption{Coverage of the Phi correlation coefficient.}
%  \label{subfig:bincorrphicover}
%\end{subfigure}
%\begin{subfigure}{\textwidth}
%  \centering 
%  % include first image
%  \includegraphics[width=16cm,height=4cm]{bincorrtetcover.png}  
%  \caption{Coverage of the tetrachoric correlation coefficient.}
%  \label{subfig:bincorrtetcover}
%\end{subfigure}
%\caption{Simulation study regarding the covariance structure. $\tilde{\text{CL}}$ (upper triangle) versus $\hat{\text{CL}}$ (lower triangle). The numbers on the diagonal are the time indices. In each row, the figures from the left to the right correspond to the data generated from Case 1 to 4, respectively. The cell is marked by a cross if the corresponding coverage is greater than 0.9.}
%\label{fig:bincorrcover}
%\end{figure}

The simulation studies have illustrated the virtue of our approach, which is avoiding possible bias in covariance structure estimation caused by mis-specification of the covariance kernel for the signal process. This virtue is led by the IWP prior putting on the covariance function. To emphasize this point, we consider an alternative, simplified modeling approach, with $Z_i\stackrel{i.i.d.}{\sim}GP(\mu,\Psi_{\boldsymbol{\phi}})$, $\mu\sim GP(\mu_0,\Psi_{\boldsymbol{\phi}}/\kappa)$. That is, instead of modeling the covariance function nonparametricly, we assume a covariance kernel of certain parametric form, specified by $\Psi_{\boldsymbol{\phi}}$. We consider the centralized signal process $\omega_i=Z_i-\mu$ evaluated at a finite grid $\boldsymbol{\tau}$, denoted as $\boldsymbol{\omega}_i$. Under the proposed model, $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim}MVT(\nu,\mathbf{0},\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}))$, while under the simplified model, $\boldsymbol{\omega}_i\stackrel{i.i.d.}{\sim} N(\mathbf{0},(1+\frac{1}{\kappa})\Psi_{\boldsymbol{\phi}}(\boldsymbol{\tau},\boldsymbol{\tau}))$. We know the true distribution of $\boldsymbol{\omega}_i$ from the data generating process. Therefore, we can compute the 2-Wasserstein distance between the model estimated distribution of $\boldsymbol{\omega}_i$ to the truth. The usage of 2-Wasserstein distance is motivated by its straightforward interpretation: a 2-Wasserstein distance of $d$ means that coordinatewise standard deviations differ by at most $d$ \citep[Thm.~3.4]{Huggins2020}. Iterating over the posterior samples of model parameters, we obtain the distributions of 2-Wasserstein distance between the model estimated distribution of $\boldsymbol{\omega}_i$ and the truth, which is shown in Figure \ref{fig:covwassd}. Clearly, for the proposed model, the 2-Wasserstein distances are substantially small. Contrasting the performance testifies our motivation of modeling the covariance structure nonparametricly.

\begin{figure}[t!]
\centering
\includegraphics[width=16cm,height=6cm]{covwassd.png}
\caption{Simulation study regarding the covariance structure. Histogram for the posterior samples of the 2-Wasserstein distance between the f.d.d.s. of the centralized signal process obtained from the proposed model (upper panel) and the simplified model (lower panel) to the truth.}
\label{fig:covwassd}
\end{figure}






%\subsection{Polychotomous ordinal responses}
%\label{subsec:simordinal}

%Simulation studies with multinomial ordinal responses. Check the performance in estimating the mean and covariance structure.  

%I plan to add two categorical covariates, one ordinal and one nominal. Consider a multinomial ordinal responses that are generated from the continuation-ratio logits model with the latent continuous variable having different structure with the model assumption. I will perform the proposed model and show the estimate of both the probability response curves and the covariance structure. May also include the analysis of error in the supplement material.



