\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lineno}
%\linenumbers
\usepackage{color}
\usepackage{tabularray}
\newcommand\at[2]{\left.#1\right|_{#2}}



\title{Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism}


%Towards Ignoring Backgrounds and Improving Generalization, a Costless DNN Visual Attention Mechanism Applicable to COVID-19 Detection

\author[1,2,*]{Pedro R.A.S. Bassi}
\author[3]{Sergio S.J. Dertkigil}
\author[1,2,*]{Andrea Cavalli}
\affil[1]{Alma Mater Studiorum - University of Bologna, Bologna, Italy}
\affil[2]{Istituto Italiano di Tecnologia, Genova, Italy}
\affil[3]{School of Medical Sciences, University of Campinas, Campinas, Brazil}
\affil[*]{e-mail: pedro.salvadorbassi2@unibo.it, andrea.cavalli@unibo.it}

\begin{abstract}
This work introduces an attention mechanism for image classifiers and the corresponding deep neural network (DNN) architecture, dubbed ISNet. During training, the ISNet uses segmentation targets to learn how to find the image's region of interest and concentrate its attention on it. The proposal is based on a novel concept, background relevance minimization in LRP explanation heatmaps. It can be applied to virtually any classification neural network architecture, without any extra computational cost at run-time. Capable of ignoring the background, the resulting single DNN can substitute the common pipeline of a segmenter followed by a classifier, being faster and lighter. After injecting synthetic bias in images' backgrounds (in diverse applications), we compare the ISNet to multiple state-of-the-art neural networks, and quantitatively demonstrate its superior capacity of minimizing the bias influence over the classifier decisions. The tasks of COVID-19 and tuberculosis detection in chest X-rays commonly employ mixed training databases, which naturally foster background bias and shortcut learning. By focusing on lungs, the ISNet reduced shortcut learning, leading to significantly superior generalization to external (out-of-distribution) test datasets. ISNet presents an accurate, fast, and light methodology to ignore backgrounds and improve generalization.

\end{abstract}


\begin{document}

\flushbottom
\maketitle

\thispagestyle{empty}

\noindent \textbf{Keywords: Attention Mechanisms; Neural Networks; Layer-wise Relevance Propagation; COVID-19 Detection; Tuberculosis Detection; Facial Attribute Estimation}

\section{Introduction}

In the last decade, deep neural networks (DNNs) have revolutionized computer vision and image classification. Counting on millions of trainable parameters, the powerful and flexible models proved capable of analyzing entire images, becoming a new standard in many different fields. However, the features contained in the image background may have a strong and detrimental effect on the classification process. Cluttered backgrounds are known to reduce classification accuracy, by making it harder for the neural network to focus on the relevant image features. But background features can also unintentionally correlate with the images' classes. In these cases, a database presents background bias. Trained in such environments, a deep neural network can learn to base its decisions not only on relevant image regions, but also on the background features. The influence of background bias over the classifier deteriorates its capacity to analyze the images' relevant features and reduces the trustworthiness of its decisions. The biased model will perform artificially well on the training dataset, and on evaluation databases that contain the same background biases. This is a common condition for test datasets that are independent and identically distributed (i.i.d.) in relation to the training data. Nevertheless, the DNN will not be adequate for a practical application, which may not portray the same background biases. This scenario favors shortcut learning\cite{ShortcutLearning}, a condition where deep neural networks learn decision rules that perform well on standard benchmarks, but poorly on real-world applications. I.e., unlike an overfitted DNN, with shortcut learning the model will perform well on an i.i.d. test dataset, but it will fail to generalize and be accurate on out-of-distribution (o.o.d.) databases. To further complicate the problem, the image features that cause shortcut learning can be difficult for a person to identify\cite{ShortcutLearning}. The objective of this study is to present a novel attention mechanism, to more efficiently deal with one of the main causes of shortcut learning: background bias.

Attention mechanisms were introduced to the field of computer vision to make neural networks focus on salient image features, and pay less attention to irrelevant aspects of the figure. Especially, the paradigm of spatial attention for classification aims to create DNNs that focus on image regions that are the most relevant for the classification task\cite{attentionSurvey}. Examples of spatial attention mechanisms are recurrent attention models (RAM)\cite{RAM}, spatial transformer networks (STN)\cite{STN} and Attention Gated Networks\cite{AGNet}. A recent review\cite{attentionSurvey} extensively covered many spatial attention mechanisms employed in computer vision. Although the strategies vary greatly, there is a common characteristic in the mechanisms employed for image classification: they rely solely on the images and their classification labels to learn where the DNN shall place its attention. I.e., the networks learn that an image region is important if paying attention to it improves classification performance. Because focusing on bias can increase training accuracy, we argue that most state-of-the-art attention mechanisms are not adequate to prevent shortcut learning.

To avoid classifier focus on background bias, attention mechanisms based on semantic segmentation are required. Image segmentation consists of semantically partitioning a figure, assigning a category to each of its pixels. The process allows a differentiation between objects, and background identification. Therefore, attention mechanisms based on semantic segmentation can force a classifier to disregard the image background. Moreover, the correlation between background features and image classes is harmless to a segmenter, since these features do not contribute to the optimization of the segmentation loss, which does not rely on the classification labels. As a general definition, we can characterize a spatial attention mechanism for a classifier as a strategy that dynamically selects where a DNN pays attention to. Accordingly, the most common segmentation-based attention mechanism is a segmentation-classification pipeline, containing 2 deep neural networks. The first DNN segments the important regions of the image, identifying the foreground. Its output is used to erase the background, creating a segmented image that the second DNN classifies. Therefore, the classifier will not have access or pay attention to background features. Other strategies involve using segmentation outputs/masks to weight classifier convolutional feature maps\cite{SSG}. In both cases, prior to the classification procedure we need to run a segmenter and obtain its output. The necessity to run two deep neural networks makes these models computationally expensive. Moreover, the heavier memory and time requirements at run-time can be a strong disadvantage for the DNNs' deployment in mobile devices.

As flexible models, DNNs are capable of performing image classification and semantic segmentation at the same time, using a single network. Multi-task learning\cite{MultiTaskOriginal} allows the simultaneous optimization of multiple loss terms, normally by minimizing their linear combination. Accordingly, it is possible to combine a classification and a segmentation loss. Past studies concluded that such multi-task models can be advantageous for both the classification and the segmentation tasks, and improve the analysis of noisy images\cite{MultiTask1}\textsuperscript{,}\cite{MultiTask2}. Moreover, the utilization of shared representations may promote similar attention foci for the two tasks\cite{MultiTask1}. However, we maintain that the utilization of this strategy does not guarantee a similar attention pattern for segmentation and classification, especially in the presence of background bias. Thus, we pose that using the multi-task DNN will not prevent shortcut learning.

This scenario reveals the need for a new attention mechanism to hinder shortcut learning, while being less computationally expensive than the traditional segmentation-based mechanisms, which always require a segmenter. In this study we propose a novel attention mechanism for classifiers. We designed it to ensure resistance to background bias. Thus, the mechanism creates a classifier that learns its attention profile from ground-truth semantic segmentation targets, which the DNN receives during training. These targets teach the classifier to implicitly find the image foreground, and to concentrate all of its attention on it. Therefore, the classifier ignores the background and the sources of bias within it.

We based our new proposal on layer-wise relevance propagation (LRP)\cite{LRP}. LRP is an explanation technique, designed to create heatmaps for deep classifiers. Heatmaps are graphics that explain the model's behavior by making explicit how each part of an input image affects the DNN output. We can create a heatmap for an input image and one class. If positive, the values in the map (i.e., relevance) indicate how strongly the classifier associated the image regions with the class. If negative, they represent areas that decreased the classifier confidence in the class (e.g., regions associated with the other possible classes). LRP is a technique created to improve DNN explainability, and, if bias is present in the figure background and affects the classification process, LRP will reveal it. 

This study introduces the concept of background relevance minimization (BRM) in LRP heatmaps. The minimization of heatmap background relevance during training automatically forces a classifier to implicitly identify the image background and diminish the attention paid to it. Because BRM teaches the classifier where to look at, it can be regarded as a novel spatial attention mechanism. To perform BRM we introduce a new DNN architecture. After learning from segmentation targets, the DNN is able to implicitly distinguish the input image foreground and background (implicitly segmenting the figure), and focus only on the foreground. Thus, we called the architecture Implicit Segmentation Neural Network, or ISNet. Note, however, that the ISNet is not a segmenter, nor does it produce segmentation outputs. It is a classifier. Nevertheless, we may examine LRP heatmaps to assess the effectiveness and precision of the 'implicit segmentation': a lack of relevance in the background indicates that the region of interest was successfully delimited.

During the training procedure, ISNet comprises a classifier and a temporary structure, which we call 'LRP block'. The novel structure shares all parameters with the classifier, and is linked to it via multiple skip connections. Its job is to perform Layer-wise Relevance Propagation through the classifier, producing heatmaps for all possible classes. Thus, unlike the segmentation branch in a multi-task learning model, the LRP block is not a standard DNN branch with independent parameters. The LRP block implements the LRP procedure as neural network layers. Therefore, standard deep learning libraries can easily perform backpropagation through the block. Consequently, we could design a loss term (named heatmap loss) that compares ground-truth segmentation targets with LRP heatmaps, and penalizes background relevance. We minimize a linear combination of the heatmap loss and a classification loss. The process teaches a classifier to classify an image paying negligible attention to its background features, even when they correlate to the figure's classes. Unlike a standard multi-task DNN, which aims to solve a segmentation and a classification task, both ISNet loss functions have the same objective: to improve classification performance. Essentially, the heatmap loss utilizes segmentation targets to restrict the classifier attention to a region of interest, aiming for improved o.o.d. classification performance.

Besides having a different objective, the ISNet also presents drastic methodological differences in relation to multi-task learning for simultaneous classification and semantic segmentation, even though it utilizes multiple loss terms. The second loss function in the multi-task network optimizes a standard DNN output, created by a dedicated neural network branch. It makes this output match segmentation targets, solving a semantic segmentation task. This optimization procedure does not restrain the model's classifier attention or explanation, as our experiments will prove. Thus, the methodology cannot be regarded as an attention mechanism, nor can it reduce classifier attention to background bias. Meanwhile, the ISNet does not have a dedicated branch for producing standard semantic segmentation masks. The LRP block is a temporary structure, without independent parameters, which uses LRP to explain the classifier's decisions with heatmaps. The second loss term for the ISNet takes as input the classifier's LRP heatmap, and the cost function minimization directly constrains the classifier's attention pattern, giving rise to a classifier spatial attention mechanism. Therefore, unlike a multi-task DNN, the ISNet training procedure optimizes classifier explanations (and attention) instead of a standard DNN segmentation output.

A major advantage of the ISNet architecture is that, after the training procedure, the LRP block can be removed or inactivated without altering the classifier behavior. It will still only focus on the regions of interest. Consequently, ISNet presents no additional computational cost at run-time relative to a common classifier. In contrast to our proposal, the accuracy of a standard classification neural network will drop significantly if it is trained with segmented images and later receives unsegmented samples during run-time. ISNet's major advantage in relation to other attention mechanisms capable of reliably ignoring background bias is that it does not require a segmenter at run-time. Therefore, it substitutes two deep neural networks by a single one, creating a more efficient model, which is much faster and has a much smaller memory requirement. This benefit is even more solid when we consider models that will be deployed in portable or less capable devices.

We can employ Background Relevance Minimization with virtually any classification neural network architecture, converting it to an ISNet. Moreover, with the removal of the LRP Block after training, the ISNet's DNN structure will be the same as the standard (unconverted) classifier's. In this work, we opted for a Densely Connected Convolutional Network\cite{DenseNet} (DenseNet) as the classifier in most tasks. Being a DNN with a small number of parameters in relation to its depth, it resonates with the efficiency focus of the ISNet design. To showcase the ISNet's versatility, we also experimented with a VGG\cite{vggOriginal} classifier.

The novel contributions of this study are: to create a classifier architecture that, without a segmenter at run-time, reliably ignores the image’s background regardless of its sources of bias, significantly surpassing the state-of-the-art DNNs' capacity to hinder shortcut learning (1); to introduce the concept of background relevance minimization in heatmaps produced by Layer-wise Relevance Propagation, which allowed the creation of a theoretically founded (Section \ref{mathBackground}) explanation-based spatial attention mechanism (2); to convert and reinterpret the process of Layer-wise Relevance Propagation as neural network layers, allowing automatic backpropagation through LRP (3).

We were not able to find other works optimizing LRP explanations to improve classifiers' attention patterns. Key reasons for our choice of optimizing LRP were the technique's superior robustness\cite{LRPRobustness}, high-definition, computational efficiency (we must generate heatmaps during training), differentiability (required for DNN optimization), and capacity to reduce explanation noise for deep classifiers\cite{LRPBook}. Theoretically, the choice is based on the LRP-$\varepsilon$'s roots on the deep Taylor decomposition framework\cite{LRPBook}. These fundamentals make LRP a viable tool to efficiently, reliably and stably optimize classifiers to be insensitive to background bias, even considering very deep networks. Please refer to Section \ref{mathBackground} for a justification of why LRP optimization produces resistance to background bias attention and shortcut learning, and for a theoretical explanation of its optimization advantages over alternative techniques, such as GradCAM\cite{GradCAM}, input gradients (saliency maps)\cite{saliency}, and Gradient*Input\cite{GradInput}. These advantages will also be empirically verified by our experiments, which compare the ISNet to networks optimizing GradCAM (GAIN\cite{GAIN} and HAM\cite{HAM}), input gradients (RRR\cite{RRR}), and Gradient*Input (ISNet Grad*Input, an ablation experiment where we substitute the ISNet's LRP heatmaps by Gradient*Input explanations).

Guided Attention Inference Networks\cite{GAIN} (GAIN) train classifiers optimizing their GradCAM\cite{GAIN} (a simpler explanation technique) heatmaps. The main objective of the DNN is to produce GradCAM heatmaps that will serve as priors for training weakly-supervised semantic segmenters (i.e., segmenters trained on datasets with little to no available ground-truth segmentation targets)\cite{SEC}. A classifier's heatmaps may serve as localization cues for training such models. Standard classifiers normally do not pay attention to all parts of the object of interest, creating heatmaps that partially highlight the image areas related to the classes. To produce more complete GradCAM explanations, GAIN classifiers are trained by minimizing a linear combination of a standard classification loss, the attention mining loss, and, optionally, the external supervision loss\cite{GAIN}. The last two functions are based on the optimization of GradCAM heatmaps, the model using the tree terms can also be called extended GAIN. The attention mining's objective is to make the DNN pay attention to all image regions correlated with the sample's classes. Meanwhile, the external supervision loss optimizes the GradCAM heatmaps to match the available ground-truth segmentation targets.

Although GAIN's main purpose is the creation of priors for weakly-supervised semantic segmentation, its authors claim that, as an additional benefit, the GAIN classifiers are more robust to background bias. Supporting this statement, they trained DNNs on databases with background bias, and showed that GAIN's o.o.d. test performance surpassed a standard classifier's, especially when using the three loss functions\cite{GAIN}. However, LRP and GradCAM are fundamentally diverse techniques: GradCAM is based on a linear combination of a DNN layer's output feature maps, while LRP back-propagates a signal through the entire DNN. We hypothesize that GradCAM contains fundamental weaknesses, which make it difficult for attention mechanisms optimizing GradCAM heatmaps to securely avoid background bias attention and shortcut learning. We analyze these weaknesses in Section \ref{GAINComparison}. As a more robust and theoretically founded explanation technique\cite{LRP}\textsuperscript{,}\cite{LRPRobustness}, LRP does not have these flaws. Therefore, it more reliably identifies background bias, making it a more trustworthy foundation for attention mechanisms that hinder shortcut learning. To support this claim, we compare the ISNet to the extended GAIN (employing segmentation targets for all training samples), and prove our proposed LRP-based attention mechanism is more reliable, and more effective in reducing attention to background bias, thus improving o.o.d. test performances. 

The Right for the Right reasons neural network (RRR)\cite{RRR} optimizes gradients of the neural network outputs with respect to its input. It minimizes background elements in the input gradients, by using a square loss. The model was shown to be highly successful in avoiding background attention, and it was originally implemented over perceptrons with two hidden-layers\cite{RRR}. However, input gradients (saliency maps) are known to be very noisy when considering deep classifier architectures\cite{LRPvsGrad}\textsuperscript{,}\cite{LRP}. Thus, here we test RRR with deep backbones (VGG-19 and DenseNet121) in multiple experiments, comparing it to the ISNet. To justify the observed empirical results, we theoretically analyze why LRP-$\varepsilon$ produces more coherent and less noisy explanations of deep networks, thus improving the ISNet optimization convergence, and leading to superior resistance to background attention and better overall accuracy (Section \ref{mathBackground}).

Besides GAIN, RRR and the ISNet Grad*Input, we also compared the ISNet to a standard segmentation-classification pipeline with 2 DNNs, comprising a U-Net\cite{unet} segmenter followed by a classifier. Moreover, we compared our proposal to a standalone classifier, without prior image segmentation. To confirm whether it is incapable of ignoring background bias, we chose an attention gated neural network (AG-Sononet\cite{AGNet}) as the third baseline. We employ it as a representation of standard attention mechanisms, which do not learn from segmentation targets. It was specifically chosen for two reasons: it is an attention mechanism for convolutional neural networks (CNNs), and the tested ISNet is also a CNN; and the mechanism was designed as an efficient substitute for the segmentation-classification pipeline commonly used to classify medical images, an architecture that we also employ as baseline. The Vision Transformer\cite{VisionTransformer} is another benchmark model, as it represents an increasingly popular choice of attention mechanism (also not considering segmentation targets). Moreover, we utilized a multi-task U-Net, trained to perform segmentation and classification simultaneously. Again, we aim to verify whether the model is susceptible to shortcut learning caused by background bias. By confirming this susceptibility, we will prove that standard multi-task learning and the proposed ISNet have fundamentally different use cases and objectives. The new model controls its classifier focus, hindering background attention and shortcut learning, but it does not produce semantic segmentation outputs. On the other hand, the multi-task DNN creates such outputs, but it does not restrain classifier attention, nor prevents shortcut learning and attention to background bias. Finally, a second GradCAM based attention mechanism, Hierarchical Attention Mining (HAM)\cite{HAM}, is considered as a baseline in Appendix \ref{CheXpertClassification}. The ISNet PyTorch and PyTorch Lightning (for simple multi-GPU training) implementations are publicly available at https://github.com/PedroRASB/ISNet, along with the implementations for the 6 state-of-the-art benchmark DNNs.

\subsection{Applications}

Currently, the most popular open databases of COVID-19 chest X-rays contain either no or few COVID-19 negative/control cases\cite{GitCovidSet}\textsuperscript{,}\cite{BrixiaSet}. Due to this limitation, to this date most studies resorted to mixed source datasets to train DNNs to differentiate between COVID-19 patients, healthy people, and patients with other diseases (e.g., non-COVID-19 pneumonia). In these datasets, different classes come from different databases assembled in distinct hospitals and cities. This scenario raises concerns that the dissimilar sources could contain different biases, which may help DNNs to classify the images according to their source dataset, rather than according to the disease symptoms. One study\cite{critic} accurately determined the source datasets after removing all lung regions from the images, proving the presence of background bias. A review\cite{ShortcutCovid} concluded that, if models are allowed to analyze the whole X-ray or a rectangular bounding box around the lungs, they tend to strongly focus on areas outside of the lungs. Thus, they fail to generalize or achieve satisfactory performance on external, o.o.d. datasets, with dissimilar sources in relation to the training images. Moreover, the review identified that the problem is a cause of shortcut learning\cite{ShortcutLearning}. A paper\cite{NatureCovidBias} utilized external datasets to evaluate traditional COVID-19 detection DNNs, whose reported results had been highly positive, and it demonstrated a strong drop in their performances. For these reasons, researchers have resorted to testing on o.o.d. datasets to understand the bias and generalization capability of DNNs trained on mixed databases. The approach shows reduced and more realistic performances in relation to the standard evaluation methodologies \cite{ShortcutCovid}\textsuperscript{,}\cite{bassi2021covid19}\textsuperscript{,}\cite{covidSegmentation}.  A common conclusion of these works is that using a segmentation-classification pipeline (segmenting the lungs before classification) improves generalization capability, reducing the bias induced by mixed training datasets\cite{bassi2021covid19}\textsuperscript{,}\cite{covidSegmentation}. The ISNet shall be able to focus only on the lungs, and we consider that the task of COVID-19 detection using the usual mixed datasets of chest X-rays will be useful to demonstrate its benefits.

We trained the ISNet on a mixed dataset based on one of the largest open COVID-19 chest X-ray databases\cite{BrixiaSet}, with the objective of distinguishing COVID-19 positive cases, normal images, and non-COVID-19 pneumonia. The two diseases have similar symptoms, making their differentiation non-trivial, and both produce signs in chest X-rays. Examples of COVID-19 signs are bilateral radiographic abnormalities, ground-glass opacity, and interstitial abnormalities\cite{clinicalCOVID}. Examples of pneumonia signals are poorly defined small centrilobular nodules, airspace consolidation, and bilateral asymmetric ground-glass opacity\cite{clinicalPneumonia}. We evaluate the optimized DNN in a cross-dataset approach, using images collected from external locations in relation to the training samples. Evaluation with an o.o.d. dataset\cite{ShortcutCovid} shall allow us to assess whether ignoring the image background reduces shortcut learning and increases generalization performance. We present more details about the databases in section \ref{dataset}.

The dataset mixing issue is not exclusive to COVID-19 detection. Instead, the technique is necessary whenever researchers need to use a classification database that does not contain all the required classes. Sometimes, such databases are the largest ones, a desirable quality for deep learning. This is the case in COVID-19 detection, and we find another example in tuberculosis (TB) detection. To the best of our knowledge, the tuberculosis X-ray dataset from TB Portals\cite{TBPortals} is the open-source database with the largest number of tuberculosis-positive X-rays to date. However, the dataset is composed of TB-positive cases only. Thus, dataset mixing is required to use the TB Portals data for training tuberculosis detection DNNs. Moreover, a recent review\cite{TBReview} showed that a large number of studies that classify tuberculosis with neural networks use dataset mixing. Furthermore, very few works evaluate DNN generalization to o.o.d. test datasets\cite{TBReview}, and a study suggested that TB classification performance may strongly drop when DNNs are tested with datasets from external sources (o.o.d.)\cite{TBBadGeneralization}. Another paper exposed strong DNN attention outside of the lungs in TB-detection with mixed databases, and advised the utilization of lung segmentation before classification\cite{TBSegmentation}. Although the World Health Organization states that chest radiography is essential for the early detection of TB, they provide no recommendation on the use of computer aided detection as of 2016\cite{who}. The reasons for this decision were the small number of studies, methodological limitations, and limited generalizability of the findings\cite{who}. Consequently, we hypothesize that the tuberculosis detection task is prone to producing background bias and shortcut learning, especially when mixed datasets are employed. For this reason, we included the application in this study. We classify X-rays as normal or tuberculosis-positive. Our training dataset mixes the TB Portals database\cite{TBPortals} with healthy X-rays from the CheXPert dataset\cite{irvin2019chexpert}. To analyze the extent of shortcut learning, we evaluate the DNNs on an i.i.d. test dataset, and on an o.o.d. database\cite{ChineseDataset1}. We analyze the TB detection task to: demonstrate that background bias and shortcut learning are not exclusive to COVID-19 detection, and to evaluate if the ISNet can diminish the problems in diverse unfavorable scenarios.

To further display the general applicability of the ISNet, and its ability to hinder background attention in diverse domains, we employed the DNN in a third classification task: facial attribute estimation. We utilized 30000 samples from the CelebA database\cite{celebA}, which is composed of natural images, in three channels, displaying people in a wide variety of poses, and containing background clutter. Using these in the wild images, the ISNet will classify facial features, while focusing only on the face region, which can assume many shapes, sizes, and locations in the figures. Facial attribute estimation is a multi-label classification problem, and it is fundamentally different from COVID-19  or TB detection in chest X-rays. Since a paper\cite{celebA} indicated that the classification of more attributes causes the classifiers to naturally focus more on the faces, we opted to classify only 3 facial attributes (rosy cheeks, high cheekbones and smiling), increasing the difficulty of learning an adequate attention profile. Past works did not necessarily search for, or found, an accuracy improvement when using face segmentation before facial attribute estimation. However, they pointed out that background features may influence classifiers\cite{FaceBias}\textsuperscript{,}\cite{FacesSmallSets}. Therefore, minimizing background attention can reduce bias and improve security, which is crucial when the classified attributes will be employed for user identification\cite{FaceBias}. In this work, we utilize the facial attribute estimation task to discover if the ISNet can precisely contain its attention inside the foreground (i.e., if its implicit image segmentation is precise) when evaluating natural images, considering a dataset where finding the images' foreground (faces) would constitute a challenging semantic segmentation problem. Thus, we will not employ a machine learning pipeline that localizes, aligns, and crops around the face before performing classification (a common approach to increase accuracy\cite{celebA}).

Before showing the aforementioned applications, our Results Section begins with a set of experiments using artificial bias. They consist in training the neural networks in natural and medical image datasets containing synthetic background bias, defined as a geometrical shape whose format (square, triangle or circle) is correlated with the image classification label. The strong artificial background bias attracts the attention of standard classifiers, making them lose focus on the image's true region of interest and causing evident shortcut learning. After training, we test the neural networks on 3 test datasets: a biased set, which contains the same geometrical shapes found in training; an unbiased set, with no geometrical shapes; and a deceiving bias dataset, which has the geometrical shapes, but the correlation between their shapes and the classification labels is changed (e.g., if the circle was correlated with class 1 in training, it will now be associated to class 2). Refer to Section \ref{dataset} for more details about the datasets.

A network resistant to background bias will produce similar accuracy in the 3 testing scenarios, demonstrating that its decisions were not affected by the geometrical shapes. On the other hand, a model paying attention to bias will show the behavior that characterizes shortcut learning\cite{ShortcutLearning}: high accuracy in the biased test, reduced performance in the standard test, and an even larger accuracy reduction in the deceiving bias test. Thus, the synthetic bias experiments allow a quantitative evaluation of the neural networks' resistance to shortcut learning caused by background bias. Moreover, LRP heatmaps should be able to clearly show attention to the geometrical shapes in the models affected by shortcut learning. We experimented with adding synthetic background bias to 3 different databases. First, the COVID-19 detection dataset, displaying X-rays, and the facial attribute estimation database, typifying natural images. These are the same datasets we also analyzed without the artificial bias (Section \ref{dataset}). Moreover, we also added the geometrical shapes to a subset of the Stanford Dogs dataset\cite{StanfordDogs}, a dog breed classification database, which has a very recognizable region of interest (foreground), the dogs. The subset consists in all images showing the breeds Pekingese, Tibetan Mastiff, and Pug. It is small, with 501 natural RGB images. Accordingly, the synthetic bias experiments evaluate the ISNet behavior in diverse dataset scales, as there are 13932 images for COVID-19 detection, and 30000 for facial attribute estimation. Stanford Dogs contains bounding-boxes, but no ground-truth segmentation targets\cite{StanfordDogs}. Thus, we converted its bounding-boxes to segmentation masks, employing a pre-trained general purpose semantic segmenter, named DeepMAC\cite{deepMAC}. Upon visual inspection, the quality of the masks was surprisingly high.

\section{Results}
The Results section presents multiple experiments, designed to test the ISNet capacity of reducing attention to background bias, and hindering the shortcut learning it tends to cause. Below, we provide a summary of the experiments and their objectives.

\begin{itemize}
    \item Synthetic background bias: addition of strong synthetic bias to the background of three diverse natural and medical image datasets. Being artificial, the bias is controllable, allowing the creation of diverse evaluation scenarios. We employ them to quantitatively compare the different neural networks' capacity of hindering the shortcut learning induced by background bias. Moreover, experiments with three diverse image domains and tasks allow us to draw more reliable conclusions, by analyzing repeating patterns in the diverse experiments' results.
    \item COVID-19 and tuberculosis detection in X-rays: two different medical datasets that have non-synthetic background bias, caused by dataset mixing. As mixing is often required in the applications, they are known to be prone to shortcut learning and unsatisfactory generalization\cite{ShortcutCovid}\textsuperscript{,}\cite{NatureCovidBias}\textsuperscript{,}\cite{who}. With these tasks, we assess the neural networks' capacity of hindering shortcut learning in two relevant real-world applications. Capable models shall be able to improve o.o.d. generalization, enhancing accuracy in X-rays drawn from external hospitals, whose radiographs were not seen during training.
    \item Facial attribute estimation: will allow the comparison of the diverse neural networks' LRP attention profiles, considering a natural RGB dataset, with challenging in-the-wild images. Moreover, it will lead to an analysis of the impact of segmentation-based attention mechanisms in an in-domain application, with standard i.i.d. testing. Thus, it will better delimit the ISNet's objective and use-case scenario.
\end{itemize}



\subsection{Synthetic Background Bias}
\label{resultsSynth}

{Table \ref{synth} presents the test average F1-Scores for all trained neural networks in the experiments with synthetic background bias. It summarizes the results for the three different datasets, Stanford Dogs subset (Pekingese, Tibetan Mastiff, and Pug classes), COVID-19 detection, and facial attribute estimation (CelebA subset\cite{celebA}, with rosy cheeks, high cheekbones and smiling categories). Columns show test performance on the biased, standard, and deceiving tests. The more similar the values across the columns, the better the model. I.e, performance drop across the columns (from left to right) represents shortcut learning. In the multi-class single-label experiments (Stanford Dogs and COVID-19 detection), scores are reported as mean and standard deviation. In facial attribute estimation (multi-label problem), they are displayed as mean and error, considering 95\% confidence. Please refer to Section \ref{statisticalMethods} for details about the statistical analysis. Moreover, Section \ref{BaselineImplementations} explains the multiple architectures employed as benchmarks, while the ISNet is summarized in section \ref{ISNet}. Training procedures and hyper-parameters are discussed in Section \ref{training}. Details about the synthetic bias and the underlying datasets can be found in Section \ref{dataset}.


\begin{table}[!h]
\centering
\small
\caption{Test macro-average F1-Scores for neural networks trained in natural datasets with synthetic background bias (geometrical shapes). Considers 3 test environments: biased, which presents the geometrical shapes; standard, without the geometrical shapes; and deceiving, where the correlation between classification labels and geometrical shapes are changed}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[412]Q[175]Q[175]Q[177]},
  row{1} = {},
  row{2} = {c},
  row{3} = {},
  row{4} = {},
  row{5} = {},
  row{6} = {},
  row{7} = {},
  row{8} = {},
  row{9} = {},
  row{10} = {},
  row{11} = {},
  row{12} = {c},
  row{14} = {},
  row{16} = {},
  row{20} = {},
  row{21} = {},
  row{22} = {c},
  row{24} = {},
  row{26} = {},
  row{30} = {},
  row{31} = {},
  cell{2}{1} = {c=4}{0.939\linewidth},
  cell{12}{1} = {c=4}{0.939\linewidth},
  cell{13}{4} = {},
  cell{15}{4} = {},
  cell{17}{4} = {},
  cell{18}{4} = {},
  cell{19}{4} = {},
  cell{22}{1} = {c=4}{0.939\linewidth},
  cell{23}{4} = {},
  cell{25}{4} = {},
  cell{27}{4} = {},
  cell{28}{4} = {},
  cell{29}{4} = {},
  hlines,
  vlines,
}
Model                                                               & {Biased \\Test maF1} & {Standard \\Test maF1} & {Deceiving Bias\\~Test maF1} \\
\textbf{Stanford Dogs with Synthetic Background Bias}               &                      &                        &                              \\
\textbf{ISNet}                                                      & 0.548 +/-0.035       & 0.553 +/-0.035         & 0.548 +/-0.035               \\
ISNet Grad*Input                                                    & 0.55 +/-0.034       & 0.545 +/-0.034         & 0.545 +/-0.034               \\
Standard Classifier                                                 & 0.926 +/-0.019       & 0.419 +/-0.034         & 0.071 +/-0.017               \\
Segmentation-classification Pipeline                                & 0.653 +/-0.033       & 0.649 +/-0.033         & 0.649 +/-0.033               \\
Multi-task U-Net                                                    & 0.522 +/-0.036       & 0.455 +/-0.036         & 0.38 +/-0.035                \\
AG-Sononet                                                          & 0.956 +/-0.015       & 0.214 +/-0.027         & 0.019 +/-0.009               \\
Extended GAIN                                                       & 0.935 +/-0.017       & 0.445 +/-0.034         & 0.1 +/-0.019                 \\
RRR                                                                 & 0.851 +/-0.025       & 0.548 +/-0.034         & 0.299 +/-0.025               \\
Vision Transformer                                                  & 0.637 +/-0.034       & 0.419 +/-0.032         & 0.399 +/-0.032               \\
\textbf{COVID-19 Detection with Synthetic Background Bias}          &                      &                        &                              \\
\textbf{ISNet}                                                      & 0.775 +/-0.008       & 0.775 +/-0.008         & 0.775 +/-0.008               \\
ISNet Grad*Input                                                    & 0.542 +/-0.01        & 0.544 +/-0.01          & 0.417 +/-0.01                \\
Standard Classifier                                                 & 0.775 +/-0.008       & 0.434 +/-0.01          & 0.195 +/-0.004               \\
Segmentation-classification Pipeline                                & 0.618 +/-0.009       & 0.619 +/-0.009         & 0.618 +/-0.009               \\
Multi-task U-Net                                                    & 0.667 +/-0.01        & 0.341 +/-0.007         & 0.156 +/-0.004               \\
AG-Sononet                                                          & 0.943 +/-0.005       & 0.386 +/-0.008         & 0.047 +/-0.003               \\
Extended GAIN                                                       & 0.41 +/-0.009        & 0.306 +/-0.006         & 0.219 +/-0.003               \\
RRR                                                                 & 0.464 +/-0.009       & 0.458 +/-0.008         & 0.426 +/-0.008               \\
Vision Transformer                                                  & 0.685 +/-0.009       & 0.496 +/-0.009         & 0.327 +/-0.009               \\
\textbf{Facial Attribute Estimation with Synthetic Background Bias} &                      &                        &                              \\
\textbf{ISNet}                                                      & 0.807 +/-0.027       & 0.807 +/-0.027         & 0.807 +/-0.027               \\
ISNet Grad*Input                                                    & 0.496 +/-0.02        & 0.499 +/-0.02          & 0.503 +/-0.021               \\
Standard Classifier                                                 & 0.974 +/-0.012       & 0.641 +/-0.054         & 0.398 +/-0.019               \\
Segmentation-classification Pipeline                                & 0.794 +/-0.031       & 0.794 +/-0.031         & 0.794 +/-0.031               \\
Multi-task U-Net                                                    & 0.985 +/-0.008       & 0.665 +/-0.129         & 0.351 +/-0.015               \\
AG-Sononet                                                          & 0.985 +/-0.009       & 0.616 +/-0.094         & 0.326 +/-0.016               \\
Extended GAIN                                                       & 0.886 +/-0.023       & 0.773 +/-0.034         & 0.633 +/-0.03                \\
RRR                                                                 & 0.794 +/-0.024       & 0.77 +/-0.032          & 0.557 +/-0.025               \\
Vision Transformer                                                  & 0.675 +/-0.023       & 0.645 +/-0.03          & 0.531 +/-0.023               
\end{tblr}
\label{synth}
\end{table}

The three datasets present remarkably diverse classification tasks, with different difficulties, domains (medical images, dogs photographs and photos of people), and dataset sizes (Section \ref{dataset}). The data augmentation policy is shared for COVID-19 detection and facial attribute estimation, consisting in random rotations, translations and flipping (Section \ref{ProcessingAgumentation}). Thus, the two applications exemplify standard data augmentation practices. Meanwhile, in Stanford Dogs we opted to utilize no data augmentation. As the augmentation procedures may totally or completely remove the synthetic background bias from the image, this choice makes the Stanford Dogs experiment a simulation of a case of extreme background bias. The ISNet is resistant to image flipping, rotations, and translations. These operations did not negatively affect the training procedure, nor did they worsen the validation error during preliminary tests with an augmented validation dataset. As the three tasks represents very diverse scenarios, the DNNs' performances vary across the experiments. However, similar patterns can be observed throughout Table \ref{synth}, and they will base our conclusions in the remaining of this subsection.

First, all datasets are highly prone to cause shortcut learning, as intended. In these experiments, shortcut learning refers to the classifiers considering the synthetic background geometrical shapes in their decisions, instead of the features in the image foreground (faces, lungs, or dogs). The higher the classifier attention to the geometrical shapes, the larger the maF1 drop when they are removed from the evaluation dataset (standard test), and when they are substituted by other geometrical shapes, which indicate the wrong classification class (deceiving bias test). The tendency for shortcut learning in the three tasks is proved by the large performance drops seen in the standard classifiers (Table \ref{synth}). This baseline model represents a common deep convolutional neural network, without any attention mechanism. In the most extreme case, Stanford Dogs, the standard classifier's maF1 falls from 0.926 +/-0.019 (geometrical shapes in the test dataset) to 0.546 +/-0.034 (no geometrical shapes), and finally to 0.071 +/-0.017 (deceiving geometrical shapes). To put this result into perspective, from the 201 evaluation samples, in the biased test the model correctly classifies 192, in the unbiased test, 89, and, in the deceiving test, 10.

Second, besides the segmentation-classification pipeline and the ISNet ablation (ISNet Grad*Input), the ISNet was the only model that was able to ignore the background bias. In the three experiments in Table \ref{synth}, the ISNet shows no reduction in maF1 scores when the synthetic background bias was removed (standard test) or substituted by deceiving bias. On the other hand, all other models show a drop in maF1 across the columns in Table \ref{synth}. Even considering the interval estimates in the table, none of the other benchmark DNNs have overlapping maF1 intervals in all three different testing scenarios. Thus, there is unambiguous evidence of bias influence over these classifiers. Conversely, the results show that the geometrical shapes do not affect the ISNet decisions, revealing an absence of the shortcut learning they tend to induce. Consequently, we observe that the proposed model successfully minimized background attention.

Moreover, the ISNet's resistance to background bias and the consequent shortcut learning is not accompanied by an accuracy loss: the proposed model has the highest average F1-Score in COVID-19 detection and facial attribute estimation (standard and deceiving bias tests in Table \ref{synth}). In the dogs classification task, its score is the highest, except for the segmentation-classification pipeline, a much larger and slower model (Section \ref{speed}). In summary, considering the testing scenarios where high performance is meaningful (standard and deceiving test), the ISNet was even able to match or surpass the large segmentation-classification pipeline maF1 scores, and it consistently surpassed all other neural networks.

As further proof of the ISNet capacity of avoiding background bias attention while retaining high accuracy, its test maF1 scores, when trained with the synthetically biased data, match or surpass a standard classifier trained in datasets without any synthetic background bias. Such model achieves 0.556 +/-0.035 maF1 in Stanford Dogs, 0.546 +/-0.01 in COVID-19 detection, and 0.802 +/-0.028 in facial attribute estimation. Meanwhile, in the three tasks, the ISNet trained with synthetic bias achieved standard test maF1 of 0.553 +/-0.035, 0.775 +/-0.008 and 0.807 +/-0.027, respectively (Table \ref{synth}). We did not find relevant natural background bias in Stanford Dogs or CelebA. Accordingly, in these two cases, the ISNet was able to match a standard classifier trained in an unbiased environment, indicating that the addition of synthetic bias could not reduce the ISNet accuracy. Meanwhile, the ISNet is also capable of hindering the shortcut learning caused by the natural background bias present in the COVID-19 dataset. This capacity explains why it strongly surpassed the standard classifier trained in the COVID-19 database without any synthetic bias. I.e., in this scenario the standard model suffers shortcut learning caused by natural background bias, which the ISNet hinders (refer to section \ref{covidResults} for details).

In Stanford Dogs, the standard classifier is a VGG-19\cite{vggOriginal}. This architecture is also used as the classification backbone for the ISNet, ISNet Grad*Input, segmentation-classification pipeline (defined as a U-Net segmenter followed by VGG-19 classifier), extended GAIN, and RRR. In the two other datasets, a DenseNet121\cite{DenseNet} substitutes the VGG-19 as the standard classifier and the backbone for the aforementioned DNNs. This design choice allows us to analyze the different architectures' behavior with a deep (19-layer VGG) and a very deep (121-layer DenseNet) backbone. The remaining benchmark DNNs (multi-task U-Net, AG-Sononet and vision transformer) utilize their original architectures in all tasks (Section \ref{BaselineImplementations}). Evidencing the versatility of the ISNet, the model was able to surpass multiple benchmark DNNs, and to be resistant to shortcut learning, both with the DenseNet121 and the VGG-19 backbones.

\subsubsection{Comparisons to the State-of-the-Art}
\label{baselineComparisons}

We now analyze the behavior of the multiple alternative benchmark models, and explain their disadvantages in relation to the ISNet, considering the quantitative results in Table \ref{synth}. First, the standard segmentation-classification pipeline (U-Net followed by VGG-19) was able to efficiently avoid attention to the synthetic background bias, and hinder shortcut learning. In COVID-19 detection and facial attribute estimation, we observe no F1-Score drop when comparing the biased dataset evaluation to the unbiased and deceiving tests. A tiny maF1 reduction can be observed when the synthetic bias is removed from the Stanford Dogs test dataset. The U-Net in the pipeline produced a test intersection over union (IoU) of 0.61 with the ground-truth segmentation targets in Stanford Dogs, 0.875 in COVID-19 detection, and 0.924 in facial attribute estimation. The worse segmentation performance in Stanford Dogs may indicate that the U-Net could not perfectly remove bias from all images in the task, explaining the small accuracy drop in Table \ref{synth}. Overall, the pipeline's resistance to shortcut learning is expected. The geometrical shapes do not affect the U-Net training procedure, their addition to the images have negligible effect over the segmenter's test performance. Indeed, when trained without the synthetic bias for lung segmentation or face segmentation, the U-Net achieved 0.893 and 0.925 IoU, respectively. The results are remarkably similar to what was observed with the artificial bias. Thus, assuming that the segmenter is able to properly erase the background bias, the classifier receives unbiased images. Like the ISNet, the pipeline's results for COVID-19 detection and facial attribute estimation with synthetic bias (Table \ref{synth}) is very similar to the model's performance when trained without the artificial shapes: 0.645 +/-0.009 maF1 in COVID-19 detection, and 0.806 +/-0.027 maF1 in facial attribute estimation. The ISNet matched the pipeline in facial attribute estimation, was surpassed in Stanford Dogs, and surpassed the pipeline in COVID-19 detection. The smaller model's unexpected and significant advantage in COVID-19 detection will be analyzed in detail in Section \ref{heatmapAnalysis}. Due to the combination of a dedicated segmenter and a classifier, two large deep neural networks, the run-time alternative pipeline was much slower and more memory consuming than the ISNet, as will be explained in Section \ref{speed}.

The ISNet Grad*Input represents an ablation experiment, where the ISNet's Layer-wise Relevance Propagation heatmaps were substituted by simpler explanations, Gradient*Input\cite{GradInput} (element-wise multiplication of the DNN input and the gradient of the logit, with relation to the input). In Stanford Dogs, the model presented a tiny maF1 drop when the geometrical shapes were removed or substituted by deceiving bias. It performed well on the task, minimizing shortcut learning and practically matching the original ISNet. However, in the two other tasks, we could not get the models' losses to properly converge. Even though we tried multiple hyper-parameter configurations (changing momentum, learning rate, and loss hyper-parameters), the ISNet Grad*Input heatmap loss always remained high and unstable. Consequently, resistance to bias was not guaranteed (Table \ref{synth}), the model's maF1 was the worst in the facial attribute estimation standard and biased tests, and it was overshadowed by the original ISNet's in COVID-19 detection. The reason for the low performance and unsatisfactory loss convergence may lie on the different classification backbones utilized in the three experiments: VGG-19 in Stanford Dogs, and DenseNet121 in the other tasks. Indeed, in preliminary tests using a DenseNet121 as backbone in Stanford Dogs, we could not make the ISNet Grad*Input heatmap loss properly converge. Input gradients are known to be highly noisy for deep models and are normally used to explain only simpler neural networks\cite{LRPvsGrad}. Therefore, noisy Gradient*Input heatmaps in the very deep DenseNet121 may prevent the proper minimization of the explanations' background attention. This is a substantial drawback of optimizing Gradient*Input instead of LRP, as very deep classifier architectures are required for state-of-the-art performance in multiple computer vision applications. For example, DenseNet121 is among the best performing models for disease classification in chest X-Rays\cite{chexnet}. As we could not obtain proper loss convergence with the deep backbone, we did not implement the ISNet Grad*Input in the remaining applications in this study. For an in-depth theoretical analysis, and comparison between the optimization of LRP and Gradient*Input, please refer to Section \ref{mathBackground}.

Instead of optimizing the multiplication of the input and the input gradient (Gradient*Input), the RRR neural network ("Right for the Right Reasons") optimizes just the input gradient\cite{RRR}. The model showed more resistance to shortcut learning than the standard classifier, with smaller maF1 reduction across the three test settings (the biased, standard, and deceiving). However, the geometrical shapes could affect the network's decisions and cause shortcut learning. Thus, in relation to the ISNet, RRR was less resistant to the effect of background bias. There are a few reasons for the ISNet surpassing RRR. First, like Gradient*Input, input gradients are nosier than the LRP heatmaps that the ISNet optimizes. Second, under the Deep Taylor Decomposition point of view, LRP-$\varepsilon$ represents a more faithful decomposition of the network output, and a more accurate representation of its behavior. Please refer to Section \ref{mathBackground} for a detailed analysis of the advantages of LRP optimization. Finally, the ISNet heatmap loss formulation is more restrictive than the RRR loss (Section \ref{loss}), better ensuring that reduced loss values correspond to the smaller influence of background features on the classifier decisions.

Although Gradient*Input explanations are similar to input gradients\cite{LRPvsGrad}, Table \ref{synth} displays noticeable differences between RRR and the ISNet Grad*Input. A main reason for the discrepancies may be the diverse loss functions that the two models utilize. The RRR heatmap loss (dubbed "right reasons"\cite{RRR}) is, in essence, a square loss comparing the input gradients' background region and zero targets. Meanwhile, the ISNet Grad*Input employs the restrictive ISNet heatmap loss. The RRR right reasons loss just requires the explanations' background components to be small (in magnitude). Conversely, the ISNet loss is relative, demanding the background components to be much smaller, in magnitude, than the foreground elements. Moreover, it has an additional term, which forces the sum of the absolute values in the heatmap foreground to stay within a pre-defined range. Therefore, the RRR loss can drop due to an overall reduction of the input gradients, affecting both their foreground and background elements. However, the relative nature of the ISNet heatmap loss makes the classifier pay progressively more attention to the foreground, in relation to the background. Meanwhile, the additional loss term keeps the heatmaps' average value stable. Empirically, we observed that, when increasing the weight of RRR's right reasons loss ($\lambda_{1}$ parameter\cite{RRR}), the loss value decreased, but this effect was not necessarily accompanied by a significant reduction in shortcut learning. Indeed, we could not get the same maF1 in the three testing scenarios (the biased, standard, and deceiving), unless $\lambda_{1}$ was set so high that the DNN essentially resorted to random guessing (we searched through $\lambda_{1}$ values from $1$ to  $10^{20}$). On the other hand, the ISNet and ISNet Grad*Input were able to avoid the accuracy drop that characterizes shortcut learning. However, the RRR's maF1 surpassed the ISNet Grad*Input in the facial attribute estimation experiment. Consequently, we hypothesize that the utilization of a less restrictive loss allowed RRR to converge more easily, considering the noisiness of both input gradients and Gradient*Input. Notice that the ISNet loss had no difficulty converging when LRP, a less noisy explanation\cite{LRPRobustness}, was optimized (original ISNet).

GradCAM\cite{GradCAM} is possibly the most popular explanation technique in computer vision, known to be simple and easily interpretable\cite{GradCAM}. The Extended GAIN\cite{GAIN} is the benchmark neural network that optimizes such explanations. As RRR, GAIN was more resistant to background bias than the baseline classifiers. However, it could not match the ISNet capacity of avoiding background attention: the geometrical shapes influenced the DNN's decisions, as shown by its accuracy drop across the columns in Table \ref{synth}. We tried increasing the weights of GAIN's attention mining and/or external supervision losses\cite{GAIN},
up to an external supervision weight (w) of 10000. Even such settings could not effectively reduce GAIN’s maF1 gap. Unlike input gradients and Gradient*Input, GradCAM is not a very noisy explanation. However, we have found that the optimization of GradCAM commonly leads to a phenomenon we named "spurious mapping". It is defined by a classifier learning to produce GradCAM explanations that display no attention to the background, minimizing GradCAM-based losses, while the model's decisions do rely on background bias, and shortcut learning is evident. I.e., spurious mapping is when the deep neural network learns to produce unreliable GradCAM heatmaps, which hide its background attention. The phenomenon was verified for the tree GAIN models in Table \ref{synth}. Please refer to Section \ref{GAINComparison} for an in-depth analysis of the issue. Examples of the spurious GradCAM heatmaps are available in Figure \ref{spurious}. The possibility of spurious mapping is intrinsic to the GradCAM formulation, which is fundamentally diverse from LRP. In summary, the propensity for spurious mapping undermines the capacity of GradCAM optimization to minimize attention to background bias, or to hinder shortcut learning, leading to it being surpassed by the ISNet in Table \ref{synth}.

The multi-task U-Net could not prevent shortcut learning in any of the experiments in Table \ref{synth}, as shown by a significant maF1 drop when comparing the biased evaluation to the standard and deceiving tests. Such as the multi-task U-Net, most multi-task models initiate as a sequence of shared layers, which later forks, creating two or more branches. One path can produce segmentation outputs while the other generates classification scores. However, a deep neural network is a very flexible model, and the shared layers can learn a representation that contains enough information to satisfactorily optimize both tasks. The model's objective is to minimize both the classification and the segmentation losses. The segmentation-exclusive path receives the shared layers' output and learns decision rules that ignore information related to the background bias, which is useless for improving segmentation performance. The classifier branch has the same input as the segmentation one, but it focuses on background bias, because it improves the minimization of the training classification loss. Even multi-task architectures that promote interactions between the segmentation and classification branches\cite{MultiTask1} allow them to process task-specific features, permitting diverse foci. We observed that the multi-task model achieved a good segmentation performance even when the images had the artificial background bias, on which the classification decisions strongly focused, as shown in Section \ref{heatmapAnalysis} heatmaps. Indeed, the presence of the geometrical shapes on the background did not influence the segmentation performance. In COVID-19 detection, the DNN trained and tested on the original dataset (without geometrical shapes) achieved 0.893 IoU, the one trained with the artificially biased dataset and tested on the original test database, 0.887, and the DNN trained and tested with the artificially biased images, 0.886. In facial attribute estimation, the IoU values in these three scenarios were 0.924, 0.926 and 0.926, respectively. Thus, we observe that the segmenter in a multi-task model can find the region of interest regardless of background bias, ignoring it. On the other hand, significant classification performance reductions when the synthetic bias was removed or substituted by deceiving bias (Table \ref{synth}) quantitatively prove that the segmenter strongly focuses on the image's backgrounds. We did not find a correlation between good segmentation and lack of background attention for classification, as the tasks' foci diverged due to background classification bias.

Not being able to effectively constrain the classifier attention to the region of interest, a multi-task model is not able to replace an ISNet for the purpose hindering background bias attention and avoiding shortcut learning. A possible strategy to use a standard multi-task model (trained for classification and semantic segmentation) to avoid shortcut learning would include running it twice. Firstly, one can run the DNN, store its segmentation output, and use it to find and remove the image background (creating a segmented image). Afterwards, one would need to rerun the DNN, now analyzing the segmented input image, and obtain the classification output. This is similar to the strategy employed in the alternative segmentation-classification pipeline, but considering that the classifier and segmenter would be parts of one multi-task model (e.g., multi-task U-Net). Since the U-Net has more parameters than the DenseNet121, this strategy shows only a small advantage in memory consumption. As the model needs to run twice, it does not present a compelling speed improvement. For these reasons, we did not find the strategy much beneficial in relation to the state-of-the-art, nor worth implementing.

One may argue that it is possible to use a multi-task model segmentation output to selectively weight the feature map produced by the DNN shared layers block, reducing background activations. This strategy would represent a time economy in relation to the previous one, as we would need to run the model shared layers just once, then sequentially run the segmentation branch (getting its output), and the classification one. However, modern DNNs have an exceptionally large receptive field in later convolutional layers. For example, the VGG-16 has a receptive field length of 212 pixels in its last convolution, while deeper models (e.g., ResNet101) can have more than 1000, effectively the whole input image\cite{ReceptiveField}. Consequently, using a segmentation mask to gate the shared layers' last feature map would not be effective to mitigate shortcut learning. Due to the large receptive field, a non-suppressed feature map element would be able to carry information from the background. Moreover, reducing this receptive field means strongly limiting the number of shared layers, or gating an earlier shared layer (making it necessary to run twice all shared layers after the gated feature map). Thus, both strategies negate the advantages of a multi-task model.

All neural networks previously discussed in this Section, except for the standard classifier, rely on semantic segmentation targets to optimize a classifier attention pattern. Conversely, the last two benchmark models, AG-Sononet and vision transformer, do not learn from semantic segmentation masks. Their attention profile is only guided by the classification loss optimization. Attention to background bias can help the DNN classify the training samples, reducing training classification loss. Without the information contained in segmentation targets, the classifier cannot effectively distinguish between background bias and foreground features, because both are correlated to classification labels. Consequently, the classifier may deem any of them, or both, relevant.

The AG-Sononet and most spatial attention mechanisms are designed to focus on relevant image features and learn to ignore irrelevant regions \cite{AGNet}. The background geometrical shapes in our experiments represents an easy to learn feature, which is correlated with the image classes. For this reason, the AG-Sononet attention mechanism learns to consider the background bias a relevant region and focuses on it. Accordingly, the model consistently showed the strongest discrepancy between results on the the biased, standard, and deceiving evaluations, for all experiments in Table \ref{synth}. Indeed, the maF1 drop was larger than what we observed for the standard classifier, showing a stronger focus on the geometrical shapes, in detriment of the foreground features (faces, lungs, or dogs). We observe that, although useful for dealing with background clutter\cite{AGNet} (irrelevant background features, unrelated to the images' classes), attention gated networks are not adequate for avoiding shortcut learning in datasets presenting background bias. Section \ref{heatmapAnalysis} will provide further evidence for this conclusion.

Finally, we analyze the vision transformer\cite{VisionTransformer}. Unlike all other trained models, it is not a convolutional neural network. It relies on the transformer, a structure that produced state-of-the-art performance in natural language processing tasks, which is built upon the concept of self-attention\cite{transformer}. Self-attention mechanisms relate diverse positions in a sequence (its input), to produce a new representation of the sequence\cite{transformer}. In relation to the standard classifier (CNN), the vision transformer showed smaller maF1 gaps when comparing the three testing scenarios, indicating less shortcut learning. The model significantly surpassed the standard classifier in all deceiving bias tests, and on the standard test for COVID-19 detection. Such results indicate a more diffuse attention profile: although the vision transformer considers the geometrical shapes in its decisions, their influence does not overpower the impact of the foreground features as much as we observed in the standard classifier. However, not learning from segmentation targets, the vision transformer cannot effectively comprehend that it should not consider the geometrical shapes. For this reason, the network's capacity of avoiding background attention and shortcut learning lagged significantly behind the ISNet's, as demonstrated by all Table \ref{synth} experiments.

\subsection{COVID-19 Detection}
\label{covidResults}
We analyzed the DNNs on the o.o.d. test dataset for COVID-19 detection, with the three classes (COVID-19, normal and non-COVID-19 pneumonia) collected from distinct localities/datasets in relation to the training database. Table \ref{performance} shows class and average precision, recall, F1-Score, specificity, and ROC-AUC for the trained DNNs. Class AUC scores are expressed as point estimates and error, considering 95\% confidence. Mean AUCs are only given as point estimates (refer to Section \ref{statisticalMethods} for the reasoning behind this decision). Other performance metrics are reported in the following format: mean +/-std, [95\% HDI]. Mean refers to the metric's mean value, according to its probability distribution, and std to its standard deviation. 95\% HDI indicates the 95\% highest density interval, defined as an interval containing 95\% of the metric's probability mass. Furthermore, any point inside the interval must have a probability density that is higher than that of any point outside. Table \ref{performance} indicates in bold text the models' balanced accuracy (i.e., average recall) and macro-averaged F1-Score (maF1). Section \ref{statisticalMethods} describes the statistical approach that we used to provide the performance metrics' interval estimates. The class ROC-AUC is calculated with the standard one-versus-rest approach, while the mean AUC is produced with a pairwise technique\cite{MulticlassAUC}, instead of a simple average of the class scores. The U-Net in the alternative segmentation-classification pipeline achieved a test intersection over union (IoU) of 0.893 segmenting the lungs, and the resulting segmentation masks seemed adequate upon visual inspection.


\begin{longtblr}[
  caption = {Performance metrics for the deep neural networks in COVID-19 detection},
  label = {performance},
]{
  width = \linewidth,
  colspec = {Q[267]Q[163]Q[163]Q[163]Q[177]},
  row{8} = {},
  row{9} = {},
  row{16} = {},
  row{17} = {},
  row{24} = {},
  row{25} = {},
  row{32} = {},
  row{33} = {},
  row{34} = {},
  row{35} = {},
  row{36} = {},
  row{37} = {},
  row{38} = {},
  row{39} = {},
  row{40} = {},
  row{41} = {},
  hlines,
  vlines,
}
Model and Metric                       & Normal                             & Pneumonia                          & COVID-19                           & {Mean\\(macro-average)}                                     \\
{\textbf{ISNet }\\\textbf{Precision}}  & {0.544 +/-0.026,\\{[}0.494,0.594]} & {0.794 +/-0.01, \\{[}0.774,0.814]} & {0.993 +/-0.002,\\{[}0.988,0.997]} & {0.777 +/-0.009,\\{[}0.759,0.795]}                          \\
{U-Net+DenseNet121\\ Precision}        & {0.446 +/-0.019,\\{[}0.408,0.483]} & {0.791 +/-0.015,\\{[}0.763,0.82]}  & {0.723 +/-0.011,\\{[}0.702,0.744]} & {0.653 +/-0.009,\\{[}0.636,0.67]}                           \\
{DenseNet121\\ Precision}              & {0.364 +/-0.02,\\{[}0.324,0.402]}  & {0.827 +/-0.018,\\{[}0.792,0.861]} & {0.649 +/-0.01,\\{[}0.629,0.67]}   & {0.614 +/-0.009,\\{[}0.594,0.631]}                          \\
{Multi-task U-Net\\ Precision}         & {0.552 +/-0.033,\\{[}0.488,0.617]} & {0.232 +/-0.02,\\{[}0.194,0.272]}  & {0.469 +/-0.01,\\{[}0.449,0.489]}  & {0.418 +/-0.013,\\{[}0.392,0.444]}                          \\
{AG-Sononet\\ Precision}               & {0.104 +/-0.013,\\{[}0.079,0.129]} & {0.665 +/-0.025,\\{[}0.616,0.715]} & {0.549 +/-0.01,\\{[}0.528,0.569]}  & {0.439 +/-0.01,\\{[}0.419,0.459]}                           \\
{Extended GAIN\\Precision}             & {0.189~+/-0.019,\\{[}0.152,0.225]} & {0.603~+/-0.016,\\{[}0.571,0.636]} & {0.642~+/-0.011,\\{[}0.62,0.664]}  & {~0.478 +/-0.009,\\{[}0.461,0.496]}                         \\
{RRR \\Precision}                      & {0.262~+/-0.015,\\{[}0.232,0.293]} & {0.728~+/-0.016,\\{[}0.697,0.758]} & {0.723~+/-0.011,\\{[}0.701,0.745]} & {0.571 +/-0.008\\{[}0.555,0.587]}                           \\
{Vision Transformer\\Precision}        & {0.268~+/-0.015,\\{[}0.239,0.297]} & {0.552~+/-0.016,\\{[}0.521,0.584]} & {0.572~+/-0.014,\\{[}0.544,0.598]} & {0.464~+/-0.009,\\{[}0.447,0.481]}                          \\
{\textbf{ISNet}\\\textbf{Recall}}      & {0.566 +/-0.026,\\{[}0.515,0.616]} & {0.933 +/-0.007,\\{[}0.919,0.946]} & {0.835 +/-0.01,\\{[}0.816,0.853]}  & {\textbf{0.778 +/-0.009,}\\\textbf{ [0.76,0.796]}}          \\
{U-Net+DenseNet121\\ Recall}           & {0.796 +/-0.021,\\{[}0.756,0.837]} & {0.466 +/-0.014,\\{[}0.439,0.494]} & {0.838 +/-0.009,\\{[}0.819,0.856]} & {\textbf{0.7 +/-0.009,}\\\textbf{ [0.683,0.717]}}           \\
{DenseNet121\\ Recall}                 & {0.57 +/-0.026,\\{[}0.518,0.618]}  & {0.294 +/-0.013,\\{[}0.27,0.32]}   & {0.916 +/-0.007,\\{[}0.902,0.93]}  & {\textbf{0.594 +/-0.01,}\\\textbf{ [0.574,0.612]}}          \\
{Multi-task U-Net\\ Recall}            & {0.338 +/-0.024,\\{[}0.29,0.386]}  & {0.08 +/-0.008,\\{[}0.066,0.095]}  & {0.776 +/-0.011,\\{[}0.755,0.797]} & {\textbf{0.398 +/-0.009,}\\\textbf{ [0.38,0.416]}}          \\
{AG-Sononet\\ Recall}                  & {0.156 +/-0.019,\\{[}0.12,0.192]}  & {0.18 +/-0.011,\\{[}0.16,0.201]}   & {0.824 +/-0.01,\\{[}0.805,0.843]}  & {\textbf{0.387 +/-0.008,}\\\textbf{ [0.371,0.402]}}         \\
{Extended GAIN\\Recall}                & {0.22~+/-0.021,\\{[}0.178,0.261]}  & {0.406~+/-0.014,\\{[}0.379,0.432]} & {0.796~+/-0.01,\\{[}0.775,0.816]}  & {\textbf{0.474~+/-0.009,}\\\textbf{[0.456,0.492]}}          \\
{RRR~\\Recall}                         & {0.574~+/-0.025,\\{[}0.524,0.624]} & {0.445~+/-0.014,\\{[}0.417,0.471]} & {0.753~+/-0.011,\\{[}0.731,0.775]} & {\textbf{0.59~+/-0.01,}\\\textbf{[0.57,0.611]}}             \\
{Vision Transformer\\Recall}           & {0.665~+/-0.024,\\{[}0.616,0.712]} & {0.415~+/-0.014,\\{[}0.388,0.442]} & {0.486~+/-0.013,\\{[}0.461,0.511]} & {\textbf{0.522~+/-0.01,}\\\textbf{[0.501,0.542]}}           \\
{\textbf{ISNet}\\\textbf{Specificity}} & {0.937 +/-0.005,\\{[}0.928,0.946]} & {0.834 +/-0.009,\\{[}0.817,0.851]} & {0.995 +/-0.002,\\{[}0.991,0.998]} & {0.922 +/-0.003,\\{[}0.916,0.928]}                          \\
{U-Net+DenseNet121\\ Specificity}      & {0.869 +/-0.006,\\{[}0.857,0.882]} & {0.915 +/-0.006,\\{[}0.903,0.928]} & {0.708 +/-0.011,\\{[}0.686,0.73]}  & {0.831 +/-0.004,\\{[}0.823,0.839]}                          \\
{DenseNet121\\ Specificity}            & {0.869 +/-0.006,\\{[}0.856,0.881]} & {0.958 +/-0.005,\\{[}0.949,0.967]} & {0.549 +/-0.012,\\{[}0.525,0.573]} & {0.792 +/-0.004,\\{[}0.784,0.8]}                            \\
{Multi-task U-Net\\ Specificity}       & {0.964 +/-0.004,\\{[}0.957,0.971]} & {0.818 +/-0.009,\\{[}0.801,0.835]} & {0.201 +/-0.01,\\{[}0.182,0.22]}   & {0.661 +/-0.004,\\{[}0.653,0.669]}                          \\
{AG-Sononet\\ Specificity}             & {0.823 +/-0.007,\\{[}0.808,0.837]} & {0.938 +/-0.006,\\{[}0.927,0.948]} & {0.384 +/-0.012,\\{[}0.361,0.408]} & {0.715 +/-0.004,\\{[}0.707,0.722]}                          \\
{Extended GAIN\\Specificity}           & {0.875~+/-0.006,\\{[}0.863,0.888]} & {0.817~+/-0.009,\\{[}0.799,0.834]} & {0.597~+/-0.012,\\{[}0.573,0.62]}  & {0.763~+/-0.004,\\{[}0.754,0.772]}                          \\
{RRR\\Specificity}                     & {0.787~+/-0.008,\\{[}0.772,0.802]} & {0.886~+/-0.007,\\{[}0.872,0.9]}   & {0.737~+/-0.011,\\{[}0.716,0.758]} & {0.803~+/-0.004,\\{[}0.795,0.812]}                          \\
{Vision Transformer\\Specificity}      & {0.761~+/-0.008,\\{[}0.745,0.776]} & {0.769~+/-0.01,\\{[}0.75,0.788]}   & {0.669~+/-0.012,\\{[}0.646,0.692]} & {0.733~+/-0.005,\\{[}0.723,0.742]}                          \\
{\textbf{ISNet}\\\textbf{F1-Score}}    & {0.555 +/-0.022,\\{[}0.512,0.597]} & {0.858 +/-0.007,\\{[}0.844,0.871]} & {0.907 +/-0.006,\\{[}0.896,0.918]} & {\textbf{0.773 +/-0.009,}\\\textbf{ [0.755,0.791]}}         \\
{U-Net+DenseNet121\\ F1-Score}         & {0.571 +/-0.018,\\{[}0.535,0.607]} & {0.586 +/-0.013,\\{[}0.561,0.611]} & {0.776 +/-0.008,\\{[}0.76,0.792]}  & {\textbf{0.645 +/-0.009,}\\\textbf{ [0.626,0.663]}}         \\
{DenseNet121\\ F1-Score}               & {0.444 +/-0.02,\\{[}0.403,0.482]}  & {0.434 +/-0.015,\\{[}0.405,0.463]} & {0.76 +/-0.008,\\{[}0.744,0.775]}  & {\textbf{0.546 +/-0.01,}\\\textbf{ [0.527,0.565]}}          \\
{Multi-task U-Net\\ F1-Score}          & {0.419 +/-0.025,\\{[}0.369,0.469]} & {0.119 +/-0.011,\\{[}0.098,0.14]}  & {0.585 +/-0.009,\\{[}0.566,0.602]} & {\textbf{0.374 +/-0.01,}\\\textbf{ [0.355,0.394]}}          \\
{AG-Sononet\\ F1-Score}                & {0.124 +/-0.015,\\{[}0.096,0.153]} & {0.284 +/-0.015,\\{[}0.255,0.312]} & {0.659 +/-0.009,\\{[}0.641,0.676]} & {\textbf{0.356 +/-0.008,}\\\textbf{ [0.34,0.372]}}          \\
{Extended GAIN\\F1-Score}              & {0.203~+/-0.019,\\{[}0.166,0.24]}  & {0.485~+/-0.013,\\{[}0.46,0.511]}  & {0.711~+/-0.009,\\{[}0.693,0.728]} & {\textbf{0.466~+/-0.009,}\\\textbf{\textbf{[0.449,0.485]}}} \\
{RRR\\F1-Score}                        & {0.36~+/-0.018,\\{[}0.325,0.394]}  & {0.552~+/-0.013,\\{[}0.526,0.577]} & {0.737~+/-0.009,\\{[}0.72,0.755]}  & {\textbf{0.55 +/-0.009,}\\\textbf{[0.532,0.568]}}           \\
{Vision Transformer\\F1-Score}         & {0.382~+/-0.017,\\{[}0.348,0.415]} & {0.474~+/-0.013,\\{[}0.448,0.499]} & {0.525~+/-0.011,\\{[}0.503,0.548]} & {\textbf{0.46~+/-0.009,}\\\textbf{[0.443,0.478]}}           \\
\textbf{ISNet AUC}                     & 0.931 +/-0.01                      & 0.962 +/-0.006                     & 0.976 +/-0.005                     & 0.952                                                       \\
U-Net+DenseNet121~AUC                  & 0.888 +/-0.019                     & 0.78 +/-0.016                      & 0.846 +/-0.013                     & 0.833                                                       \\
DenseNet121~AUC                        & 0.804 +/-0.023                     & 0.805 +/-0.015                     & 0.86 +/-0.013                      & 0.808                                                       \\
Multi-task U-Net~AUC                   & 0.721 +/-0.034                     & 0.412 +/-0.019                     & 0.487 +/-0.02                      & 0.553                                                       \\
AG-Sononet~AUC                         & 0.451 +/-0.028                     & 0.681 +/-0.019                     & 0.658 +/-0.018                     & 0.591                                                       \\
Extended GAIN~AUC                      & 0.7 +/-0.025                       & 0.756 +/-0.016                     & 0.806 +/-0.016                     & 0.724                                                       \\
RRR AUC                                & 0.782~+/-0.02                      & 0.736~+/-0.017                     & 0.835~+/-0.014                     & 0.775                                                       \\
Vision Transformer AUC                 & 0.755~+/-0.032                     & 0.645~+/-0.019                     & 0.619~+/-0.019                     & 0.683                                                       
\end{longtblr}




Comparing the test metrics macro-averages (Table \ref{performance}) for the five models containing DenseNet121s (ISNet, DenseNet121, segmentation-classification pipeline, Extended GAIN and RRR), we observe that the ISNet obtained the highest results, followed by the alternative segmentation-classification pipeline, RRR, the model without segmentation, and finally GAIN. We also observe that the 95\% highest density intervals for the ISNet and pipeline do not overlap with any other network for any average performance measurement. Consequently, the probability of the models having equivalent performances is minute. Relative to the alternative segmentation methodology, the ISNet achieved a 12.8\% increment in macro-averaged F1-Score. In relation to the DenseNet121 without segmentation, the difference rises to 22.7\%, in relation to RRR, 22.3\%, and, in relation to GAIN, we observe a 30.7\% gap. In this work, our segmentation targets were automatically generated by a U-Net, trained for lung segmentation in another study\cite{bassi2021covid19}. Therefore, we believe that the performances achieved by the ISNet and by the alternative segmentation-classification pipeline could increase even more, provided a dataset containing a large amount of chest X-rays accompanied by manually segmented lung regions.

The multi-task U-Net and the AG-Sononet, two models based on the VGG-16 architecture, presented similar and poor metrics in Table \ref{performance}, being strongly surpassed by the standalone DenseNet121. During training, the two models converged, with training and validation losses similar to the DenseNet121-based DNNs. However, their poor evaluation performances reveal a failure in generalizing to the o.o.d. test dataset. The same can be said for the vision transformer, whose performance could not surpass the standard DenseNet121.

The reported results may seem worse than other studies in the field of COVID-19 detection, which report remarkably high performances, strongly surpassing expert radiologists (e.g., F1-Scores close to 100\%). However, evidence suggests that, currently, such results are obtained when the training and test datasets come from the same distribution/sources (i.e., datasets that are independent and identically distributed)\cite{ShortcutCovid}. Moreover, studies show that these strong performances may be boosted by bias and shortcut learning, preventing the neural networks from generalizing, or achieving comparable results in the real-world\cite{ShortcutCovid}\textsuperscript{,}\cite{bassi2021covid19}\textsuperscript{,}\cite{NatureCovidBias}. Instead, the performances obtained in this study are in accordance with other works that evaluate their DNNs in external databases (i.e., out-of-distribution datasets, whose image sources are diverse in relation to the ones that generate the training samples)\cite{ShortcutCovid}\textsuperscript{,}\cite{bassi2021covid19}\textsuperscript{,}\cite{NatureCovidBias}. For example, an article\cite{NatureCovidBias} reported AUC of 0.786 +/-0.025 on an external dataset, when evaluating a DenseNet121 used for COVID-19 detection without lung segmentation. Here, the DenseNet121 obtained 0.808 AUC, which falls into their reported confidence interval. Another paper\cite{bassi2021covid19} evaluates COVID-19 detection and utilizes lung segmentation before classification with a DenseNet201. They achieved mean maF1 of 0.754, with 95\% HDI of [0.687,0.82], also evaluating their model on an external dataset. We observe that the ISNet maF1 95\% HDI, [0.755,0.791], fits inside their reported 95\% HDI. We must note that the aforementioned studies use different databases. Thus, caution is required when directly comparing the numerical results.

COVID-19 detection with mixed datasets is known to be a task prone to shortcut learning\cite{NatureCovidBias}\textsuperscript{,}\cite{ShortcutCovid}. Indeed, the results in Table \ref{performance} are consistent with our findings from the synthetic bias experiments. Previously (Table \ref{synth}), we repeatedly observed that the ISNet and the alternative segmentation-classification pipeline (U-Net+DenseNet121) were the two models with the highest resistance to background bias attention and the shortcut learning it tends to cause. Now, Table \ref{performance} shows that the two models had significantly superior o.o.d. maF1 in COVID-19 detection, with no HDI superposition with the other neural networks. Since improved o.o.d. performance is a characteristic indication of shortcut-learning reduction\cite{ShortcutLearning}, the results in the COVID-19 detection task confirm the ISNet and pipeline's superior resistance to background bias-induced shortcut learning.

The interesting finding that the ISNet could considerably surpass the alternative segmentation-classification pipeline in COVID-19 detection will be analyzed in Section \ref{heatmapAnalysis}. Regarding the other benchmark DNNs, Table \ref{performance} shows that none of their average performance scores could significantly surpass the standard classifier (DenseNet121). To overcome the standard DNN, the benchmark models should be able to efficiently hinder shortcut learning, while not losing overall accuracy in the process. The quantitative results in the table indicate a failure to achieve this goal, confirming the weaknesses previously discussed in Section \ref{baselineComparisons}.

To illustrate that it is not possible to simply train a model with segmented images, and then use it without segmentation at run-time, we tested the alternative segmentation methodology after removing the U-Net from its pipeline. That is, we simulated a DenseNet121 trained on segmented images and used to classify unsegmented ones. As expected, this resulted in a dramatic performance drop: maF1 was reduced from 0.645 +/-0.009 to 0.217 +/-0.003 (changing its 95\% HDI from [0.626,0.663] to [0.211,0.224]). The test confirms that, unlike the ISNet, the alternative segmentation-classification pipeline needs a segmenter at run-time. As a final note, table \ref{confusion} displays the confusion matrices for the implemented DNNs. 

\begin{table}[!h]
\centering
\small
\caption{Confusion matrices for the deep neural networks in COVID-19 detection}
\label{confusion}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[125]Q[85]Q[125]Q[115]Q[25]Q[125]Q[85]Q[125]Q[115]},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{0.325\linewidth},
  cell{1}{6} = {r=2}{},
  cell{1}{7} = {c=3}{0.325\linewidth},
  cell{3}{1} = {c=4}{0.45\linewidth,c},
  cell{3}{6} = {c=4}{0.45\linewidth,c},
  cell{7}{1} = {c=4}{0.45\linewidth,c},
  cell{7}{6} = {c=4}{0.45\linewidth,c},
  cell{11}{1} = {c=4}{0.45\linewidth,c},
  cell{11}{6} = {c=4}{0.45\linewidth,c},
  cell{15}{1} = {c=4}{0.45\linewidth,c},
  cell{15}{6} = {c=4}{0.45\linewidth,c},
  cell{16}{1} = {},
  cell{16}{2} = {},
  cell{16}{3} = {},
  cell{16}{4} = {},
  cell{16}{6} = {},
  cell{16}{7} = {},
  cell{16}{8} = {},
  cell{16}{9} = {},
  cell{17}{1} = {},
  cell{17}{2} = {},
  cell{17}{3} = {},
  cell{17}{4} = {},
  cell{17}{6} = {},
  cell{17}{7} = {},
  cell{17}{8} = {},
  cell{17}{9} = {},
  cell{18}{1} = {},
  cell{18}{2} = {},
  cell{18}{3} = {},
  cell{18}{4} = {},
  cell{18}{6} = {},
  cell{18}{7} = {},
  cell{18}{8} = {},
  cell{18}{9} = {},
  vlines,
  hline{1,3-19} = {1-4,6-9}{},
  hline{2} = {2-4,7-9}{},
}
True Class                   & Predicted Class &           &          &  & True Class                       & Predicted Class &           &          \\
                             & Normal          & Pneumonia & COVID-19 &  &                                  & Normal          & Pneumonia & COVID-19 \\
\textbf{ISNet}               &                 &           &          &  & \textbf{Multi-task U-Net}        &                 &           &          \\
Normal                       & 210             & 157       & 3        &  & Normal                           & 125             & 42        & 203      \\
Pneumonia                    & 81              & 1210      & 4        &  & Pneumonia                        & 62              & 103       & 1130     \\
COVID-19                     & 93              & 156       & 1266     &  & COVID-19                         & 38              & 300       & 1177     \\
\textbf{U-Net + DenseNet121} &                 &           &          &  & \textbf{Attention Gated Sononet} &                 &           &          \\
Normal                       & 296             & 9         & 65       &  & Normal                           & 57              & 2         & 311      \\
Pneumonia                    & 271             & 604       & 420      &  & Pneumonia                        & 346             & 233       & 716      \\
COVID-19                     & 95              & 149       & 1271     &  & COVID-19                         & 151             & 114       & 1250     \\
\textbf{DenseNet121}         &                 &           &          &  & \textbf{Extended GAIN}           &                 &           &          \\
Normal                       & 211             & 16        & 143      &  & Normal                           & 81              & 145       & 144      \\
Pneumonia                    & 306             & 381       & 608      &  & Pneumonia                        & 241             & 526       & 528      \\
COVID-19                     & 63              & 62        & 1390     &  & COVID-19                         & 108             & 200       & 1207     \\
\textbf{\textbf{RRR}}        &                 &           &          &  & \textbf{Vision Transformer}      &                 &           &          \\
Normal                       & 213             & 84        & 73       &  & Normal                           & 247             & 50        & 73       \\
Pneumonia                    & 355             & 576       & 364      &  & Pneumonia                        & 279             & 538       & 478      \\
COVID-19                     & 243             & 130       & 1142     &  & COVID-19                         & 393             & 385       & 737      
\end{tblr}
\end{table}

\subsection{Tuberculosis Detection}
\label{TBResults}

Table \ref{tb} reports performance metrics for tuberculosis detection, according to evaluation on the external, o.o.d. dataset. We display the metrics' mean and error, considering 95\% confidence (see section \ref{statisticalMethods} for details about the statistical analysis). On the i.i.d. test dataset all models had mean AUC over 0.9. Moreover, on the i.i.d. evaluation database the segmentation-classification pipeline achieved 0.955 +/-0.016 maF1, the ISNet 0.974 +/-0.012, the extended GAIN 0.982 +/-0.009, the vision transformer 0.926 +/-0.02, RRR 0.839 +/-0.028, and all other DNNs' mean maF1 scores surpassed 0.985. Our i.i.d. test results are in line with other studies that detected tuberculosis with DNNs, most of which report extremely high AUC and F1-Score\cite{TBReview}. We could not find any study that employed a training dataset like ours and used o.o.d. testing, as the evaluation methodology is rare in tuberculosis detection. Regarding lung segmentation performances, the multi-task U-Net had IoU of 0.941 on the o.o.d. test dataset, and the segmenter inside the segmentation-classification pipeline 0.82, according to the segmentation targets that we automatically generated (see Section \ref{dataset}). Considering the o.o.d. dataset's manually created segmentation targets\cite{ChineseDataset1}\textsuperscript{,}\cite{ShenMasks}, the two DNNs achieved 0.904 and 0.908 IoU, respectively. The produced segmentation outputs seemed adequate upon visual inspection.


% \usepackage{color}
% \usepackage{tabularray}
\begin{longtblr}[
  caption = {Performance metrics for the deep neural networks in tuberculosis detection (o.o.d. test dataset)},
  label = {tb},
]{
  width = \linewidth,
  colspec = {Q[363]Q[185]Q[185]Q[202]},
  row{8} = {},
  row{9} = {},
  row{16} = {},
  row{17} = {},
  row{24} = {},
  row{25} = {},
  row{26} = {},
  row{27} = {},
  row{28} = {},
  row{29} = {},
  row{30} = {},
  row{31} = {},
  row{32} = {},
  row{33} = {},
  hlines,
  vlines,
}
Model and Metric             & Normal          & Tuberculosis    & {Mean\\ (macro-average)} \\
\textbf{ISNet Precision}     & 0.744 +/-0.045  & 0.734 +/-0.043  & 0.739 +/-0.044           \\
U-Net+DenseNet121 Precision  & 0.63 +/-0.061   & 0.573 +/-0.043  & 0.601 +/-0.052           \\
DenseNet121 Precision        & 0.578 +/-0.055  & 0.564 +/-0.046  & 0.571 +/-0.05            \\
Multi-task U-Net Precision   & 0.515 +/-0.048  & 0.539 +/-0.053  & 0.527 +/-0.05            \\
AG-Sononet Precision         & 0.731 +/-0.06   & 0.599 +/-0.041  & 0.665 +/-0.05            \\
Extended GAIN Precision      & 0.576 +/-0.04   & 0.766 +/-0.06   & 0.671 +/-0.05            \\
RRR Precision                & 0.663 +/-0.049  & 0.664 +/-0.046  & 0.663 +/-0.048           \\
Vision Transformer Precision & 0.52 +/-0.044   & 0.56 +/-0.059   & 0.54 +/-0.052            \\
\textbf{ISNet Recall}        & 0.714 +/-0.046  & 0.762 +/-0.042  & \textbf{0.738 +/-0.044}  \\
U-Net+DenseNet121 Recall     & 0.409 +/-0.05   & 0.768 +/-0.042  & \textbf{0.589 +/-0.046}  \\
DenseNet121 Recall           & 0.479 +/-0.05   & 0.659 +/-0.047  & \textbf{0.569 +/-0.048}  \\
Multi-task U-Net Recall      & 0.586 +/-0.05   & 0.468 +/-0.05   & \textbf{0.527 +/-0.05}   \\
AG-Sononet Recall            & 0.406 +/-0.05   & 0.855 +/-0.035  & \textbf{0.631 +/-0.042}  \\
Extended GAIN Recall         & 0.883 +/-0.032  & 0.372 +/-0.048  & \textbf{0.627 +/-0.04}   \\
RRR Recall                   & 0.642 +/-0.049  & 0.685 +/-0.046  & \textbf{0.663 +/-0.048}  \\
Vision Transformer Recall    & 0.679 +/-0.047  & 0.395 +/-0.049  & \textbf{0.537 +/-0.048}  \\
\textbf{ISNet F1-Score}      & 0.729 +/-0.046  & 0.748 +/-0.043  & \textbf{0.738 +/-0.044}  \\
U-Net+DenseNet121 F1-Score   & 0.496 +/-0.056  & 0.656 +/-0.044  & \textbf{0.576 +/-0.05}   \\
DenseNet121 F1-Score         & 0.524 +/-0.052  & 0.608 +/-0.047  & \textbf{0.566 +/-0.05}   \\
Multi-task U-Net F1-Score    & 0.548 +/-0.049  & 0.501 +/-0.052  & \textbf{0.524 +/-0.05}   \\
AG-Sononet F1-Score          & 0.522 +/-0.057  & 0.704 +/-0.04   & \textbf{0.613 +/-0.048}  \\
Extended GAIN F1-Score       & 0.697 +/-0.039  & 0.501 +/-0.056  & \textbf{0.599 +/-0.048}  \\
RRR F1-Score                 & 0.652 +/-0.049  & 0.674 +/-0.046  & \textbf{0.663 +/-0.048}  \\
Vision Transformer F1-Score  & 0.589 +/-0.046  & 0.463 +/-0.054  & \textbf{0.526 +/-0.05}   \\
\textbf{\textbf{ISNet AUC}}  & 0.809 +/-0.031  & 0.809 +/-0.031  & 0.809 +/-0.031           \\
U-Net+DenseNet121 AUC        & 0.667 +/-0.039  & 0.667 +/-0.039  & 0.667 +/-0.039           \\
DenseNet121~AUC              & 0.576 +/-0.04   & 0.576 +/-0.04   & 0.576 +/-0.04            \\
Multi-task U-Net~AUC         & 0.549 +/-0.041  & 0.549 +/-0.041  & 0.549 +/-0.041           \\
AG-Sononet~AUC               & 0.717 +/-0.037  & 0.717 +/-0.037  & 0.717 +/-0.037           \\
Extended GAIN~AUC            & 0.676 +/-0.038  & 0.676 +/-0.038  & 0.676 +/-0.038           \\
RRR AUC                      & 0.728 +/- 0.036 & 0.728 +/- 0.036 & 0.728 +/- 0.036          \\
Visision Transformer AUC     & 0.558 +/-0.041  & 0.558 +/-0.041  & 0.558 +/-0.041           
\end{longtblr}

As in COVID-19 detection, the ISNet was the best performing model on the o.o.d. test dataset. Furthermore, it showed no confidence interval overlap with the other models for AUC. The alternative segmentation-classification pipeline results were not as promising as in COVID-19 detection. We applied a threshold of 0.3 to the model's U-Net outputs. This value was chosen because it maximized the segmenter's validation IoU (considering that the training and validation datasets are i.i.d.). However, a larger threshold, 0.5, would optimize the pipeline's o.o.d. evaluation result: it would achieve 0.631 +/-0.048 maF1, becoming again one of the best generalizing DNNs. In this task the segmentation-classification pipeline is sensible to changes in the U-Net output threshold, which is difficult to optimize without access to the o.o.d. test database. The same effect was not seen with the other applications in this study: they showed no significant mean maF1 benefit by optimizing the segmentation threshold beyond the value obtained with validation. Interestingly, even in TB-detection the threshold change from 0.3 to 0.5 did not improve IoU with the o.o.d. dataset's manual segmentation masks (it slightly decreased, from 0.904 to 0.896). As a final note, in preliminary tests training with unthresholded U-Net outputs, the pipeline was not able to ignore the background.

In this experiment, RRR was more promising than in COVID-19 detection, as its maF1 and AUC significantly surpassed the baseline DenseNet121. Moreover, the model showed a reduced gap between the i.i.d. and o.o.d. performance, a result consistent with a reduction in shortcut learning. However, the RRR's o.o.d. performance could not match the ISNet, possibly indicating that the network traded overall accuracy (o.o.d. and i.i.d.) for improved resistance to shortcut learning. Indeed, we needed to set the "right reasons" loss weight\cite{RRR} to a remarkably high value, $\lambda_{1}=10^{8}$, to maximize RRR's o.o.d. performance. We suspect the possible overall accuracy reduction to be caused by the noisiness of the input gradients the model optimizes\cite{LRPRobustness}, and by the fact that, in relation to LRP, input gradients represent a less accurate decomposition of the classifier outputs (refer to Section \ref{mathBackground}). These disadvantages could have undermined RRR's joint minimization of the classification loss and the "right reasons" loss.

The remaining benchmark DNNs (GAIN, vision transformer, multi-task U-Net and AG-Sononet) displayed unimpressive generalization capability, with mean maF1 scores in the 50\% to early 60\% range. Meanwhile, they had strong results on the i.i.d. test dataset. The discrepancy between performances on the i.i.d and o.o.d. test databases is evidence that the classification problem is prone to shortcut learning. As in COVID-19 detection, multi-task learning, attention gates, the extended GAIN and the vision transformer did not strongly improve generalization; the four model's maF1 95\% confidence intervals overlap with the DenseNet121's. Again, this result agrees with the findings from the synthetic bias experiments, and it corroborates with the discussion about the benchmark models' weaknesses, presented in Section \ref{baselineComparisons}.

The ISNet performance on the i.i.d. evaluation database is among the lowest, while it was the best performing model on the o.o.d. dataset. Shortcut learning is characterized by high accuracy on standard benchmarks (i.i.d. datasets), but impaired o.o.d. generalization, and poor real-world performance\cite{ShortcutLearning}. Thus, the ISNet quantitative results are coherent with a reduction in shortcut learning, without overall accuracy degradation. Therefore, TB classification serves as an additional example of a real-world application that can be heavily affected by background bias and shortcut learning, representing another notable use case for the ISNet.

\subsection{Facial Attribute Estimation}

Table \ref{FacesPerformance} shows the test performance of all DNNs in the facial attribute estimation task, analyzing three attributes (rosy cheeks, high cheekbones and smiling). The table cells report the metrics' mean and error, considering 95\% confidence. Section \ref{statisticalMethods} describes the statistical methods used to create the confidence intervals. We observe that, in this task, the models have similar performances, presenting strong overlap in the 95\% confidence intervals for macro-averaged F1-Score in Table \ref{FacesPerformance}. The U-Net in the alternative segmentation-classification pipeline achieved a test IoU of 0.925, and the face segmentation masks seemed adequate during the visual inspection. In facial attribute estimation, removing the U-Net before testing the alternative segmentation methodology causes a devastating effect, as it did in COVID-19 detection: it reduced maF1 from 0.806 +/-0.027 to 0.543 +/-0.191. 

We use the facial attribute estimation task to analyze the ISNet in an in-domain application, employing a standard i.i.d. test dataset. Forcing a model to not consider the background may help the DNN focus on important features, improving accuracy when the network finds it difficult to identify them. But it may also reduce performance on the i.i.d. test database, by making the model ignore background bias that could artificially help classification. Indeed, models affected by shortcut learning perform well on the standard i.i.d. benchmarks\cite{ShortcutLearning}\textsuperscript{,}\cite{ShortcutCovid}. The weak impact of segmentation-based attention mechanisms (e.g., ISNet and segmentation-classification pipeline) on performance scores may indicate that the CelebA\cite{celebA} dataset background features do not have a strong correlation with the images' classes, and that standard classifiers can naturally focus on the faces. This proposition is supported by the analysis of of LRP heatmaps for the task, as will be shown in Section \ref{heatmapAnalysis}. The figures will also show that the ISNet precisely focused on the photographs' region of interest (the faces), considering natural images with cluttered backgrounds and a wide variety of segmentation masks' shapes, locations, and sizes. By accurately focusing on the faces, the ISNet represents a security improvement for facial attribute estimation. Indeed, in the synthetic bias experiment, the ISNet could prevent the facial attribute estimator outputs from being influenced by non-facial features (Table \ref{synth}).


\begin{table}[!h]
\centering
\caption{Performance metrics for the deep neural networks in facial attribute estimation}
\label{FacesPerformance}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[290]Q[150]Q[179]Q[150]Q[167]},
  row{8} = {},
  row{9} = {},
  row{16} = {},
  row{17} = {},
  row{24} = {},
  row{25} = {},
  row{26} = {},
  row{27} = {},
  row{28} = {},
  row{29} = {},
  row{30} = {},
  row{31} = {},
  row{32} = {},
  row{33} = {},
  hlines,
  vlines,
}
Model and Metric             & Rosy Cheeks    & High Cheekbones & Smiling        & {Mean\\(macro-average)} \\
\textbf{ISNet Precision}     & 0.59 +/-0.048  & 0.843 +/-0.019  & 0.937 +/-0.013 & 0.79 +/-0.027           \\
U-Net+DenseNet121 Precision  & 0.615 +/-0.048 & 0.863 +/-0.018  & 0.905 +/-0.016 & 0.794 +/-0.027          \\
DenseNet121 Precision        & 0.624 +/-0.05  & 0.876 +/-0.018  & 0.926 +/-0.014 & 0.809 +/-0.027          \\
Multi-task U-Net Precision   & 0.652 +/-0.049 & 0.909 +/-0.016  & 0.937 +/-0.013 & 0.833 +/-0.026          \\
AG-Sononet Precision         & 0.545 +/-0.044 & 0.896 +/-0.017  & 0.911 +/-0.015 & 0.784 +/-0.025          \\
Extended GAIN Precision      & 0.644 +/-0.052 & 0.851 +/-0.019  & 0.902 +/-0.016 & 0.799 +/-0.029          \\
RRR Precision                & 0.59 +/-0.05   & 0.839 +/-0.019  & 0.836 +/-0.019 & 0.755 +/-0.029          \\
Vision Transformer Precision & 0.476 +/-0.046 & 0.664 +/-0.023  & 0.693 +/-0.023 & 0.611 +/-0.031          \\
\textbf{ISNet Recall}        & 0.71 +/-0.04   & 0.828 +/-0.02   & 0.876 +/-0.018 & \textbf{0.805 +/-0.027} \\
U-Net+DenseNet121 Recall     & 0.707 +/-0.045 & 0.843 +/-0.019  & 0.912 +/-0.015 & \textbf{0.821 +/-0.026} \\
DenseNet121 Recall           & 0.671 +/-0.048 & 0.822 +/-0.021  & 0.897 +/-0.016 & \textbf{0.797 +/-0.028} \\
Multi-task U-Net Recall      & 0.695 +/-0.048 & 0.792 +/-0.023  & 0.914 +/-0.015 & \textbf{0.8 +/-0.029}   \\
AG-Sononet Recall            & 0.79 +/-0.036  & 0.816 +/-0.021  & 0.926 +/-0.014 & \textbf{0.844 +/-0.024} \\
Extended GAIN Recall         & 0.617 +/-0.053 & 0.818 +/-0.021  & 0.901 +/-0.016 & \textbf{0.779 +/-0.03}  \\
RRR Recall                   & 0.65 +/-0.049  & 0.836 +/-0.02   & 0.913 +/-0.014 & \textbf{0.8 +/-0.028}   \\
Vision Transformer Recall    & 0.63 +/-0.045  & 0.801 +/-0.019  & 0.786 +/-0.021 & \textbf{0.739 +/-0.028} \\
\textbf{ISNet F1-Score}      & 0.644 +/-0.047 & 0.835 +/-0.02   & 0.905 +/-0.016 & \textbf{0.795 +/-0.028} \\
U-Net+DenseNet121 F1-Score   & 0.658 +/-0.047 & 0.853 +/-0.019  & 0.908 +/-0.016 & \textbf{0.806 +/-0.027} \\
DenseNet121 F1-Score         & 0.647 +/-0.049 & 0.848 +/-0.02   & 0.911 +/-0.015 & \textbf{0.802 +/-0.028} \\
Multi-task U-Net F1-Score    & 0.673 +/-0.049 & 0.846 +/-0.02   & 0.925 +/-0.014 & \textbf{0.815 +/-0.028} \\
AG-Sononet F1-Score          & 0.645 +/-0.043 & 0.854 +/-0.019  & 0.918 +/-0.015 & \textbf{0.806 +/-0.026} \\
Extended GAIN F1-Score       & 0.63 +/-0.053  & 0.834 +/-0.02   & 0.901 +/-0.016 & \textbf{0.788 +/-0.03}  \\
RRR F1-Score                 & 0.619 +/-0.05  & 0.837 +/-0.02   & 0.873 +/-0.017 & \textbf{0.776 +/-0.029} \\
Vision Transformer F1-Score  & 0.542 +/-0.047 & 0.726 +/-0.022  & 0.737 +/-0.022 & \textbf{0.668 +/-0.03}  \\
\textbf{\textbf{ISNet AUC}}  & 0.936 +/-0.011 & 0.922 +/-0.01   & 0.97 +/-0.005  & 0.943 +/-0.009          \\
U-Net+DenseNet121 AUC        & 0.942 +/-0.01  & 0.938 +/-0.008  & 0.979 +/-0.004 & 0.952 +/-0.008          \\
DenseNet121~AUC              & 0.945 +/-0.01  & 0.939 +/-0.008  & 0.98 +/-0.004  & 0.955 +/-0.007          \\
Multi-task U-Net~AUC         & 0.907 +/-0.018 & 0.912 +/-0.011  & 0.962 +/-0.007 & 0.927 +/-0.012          \\
AG-Sononet~AUC               & 0.945 +/-0.01  & 0.939 +/-0.008  & 0.978 +/-0.004 & 0.954 +/-0.007          \\
Extended GAIN~AUC            & 0.945 +/-0.011 & 0.939 +/-0.008  & 0.982 +/-0.004 & 0.955 +/-0.008          \\
RRR AUC                      & 0.899~+/-0.018 & 0.924~+/-0.009  & 0.951~+/-0.007 & 0.925~+/-0.011          \\
Vision Transformer AUC       & 0.898~+/-0.015 & 0.798~+/-0.016  & 0.821~+/-0.015 & 0.839~+/-0.015          
\end{tblr}
\end{table}



\subsection{Speed and Efficiency Analysis}
\label{speed}

Models were trained with an NVIDIA Tesla V100 graphics processing unit (GPU), from a HPC, or with an NVIDIA RTX 3080. The performances reported in this subsection consider the RTX 3080 and mixed precision. In facial attribute estimation an epoch considered 24183 train images and 2993 validation samples, loaded in mini-batches of 10 figures. An epoch took about 2300 s for the ISNet Grad*Input, 1300 s for the ISNet, 1500 s for GAIN, 900 s for RRR, 320 s for the alternative segmentation model, 240 s for the standalone DenseNet121, 250 s for the multi-task U-Net, 160 seconds for the vision transformer, and 60 s for the AG-Sononet. It is worth noting that the alternative pipeline also required training a U-Net, which needed 400 epochs of about 180 s. Considering this additional training procedure, the ISNet's training time was about 82\% longer than the alternative segmentation-classification pipeline's. Attention gates are not a computationally expensive methodology during both training and run-time. Meanwhile, the multi-task model requires sensible additional computation to create the segmentation outputs. It was faster than the U-Net followed by the DenseNet121, but we must consider that its classification path is based on a different architecture (VGG-16). The ISNet needs to generate one heatmap per class, while GAIN produces one GradCAM explanation for each ground-truth class in the image, uses it to mask the original sample, and classifies it again. Thus, GAIN training tends to be slower than the ISNet optimization in multi-label problems, but faster in single-label tasks. Both models have similar run-time speed, as does RRR. In the remainder of this subsection, we will mostly compare the ISNet performance to the alternative segmentation-classification pipeline's, since they are the tested methods that proved most usable for the objective of avoiding shortcut learning caused by background bias. Furthermore, both methods were based on a DenseNet121 classifier, allowing a fairer comparison.

During training, the ISNet is slower than a traditional pipeline of segmentation and classification. The model's biggest limitation may be the need to generate one heatmap per class. Treating the different heatmap calculations as a propagation of different batch elements allows parallelism, but training time and memory consumption still increases with more classes. The worst-case scenario for this problem is if memory is not sufficient for the batch approach. In this case, the heatmaps will need to be created in series, causing training time to increase linearly with the number of classes. Therefore, more efficient implementations for the LRP block are a promising path for future developments.

At run-time, the ISNet shows clear benefits because the LRP block can be removed from the model, leaving only the classifier. With mixed precision and using mini-batches of 10 images, the standard pipeline of a U-Net followed by a DenseNet121 classifies an average of 207 samples per second, while the DenseNet121-based ISNet classifies an average of 353. Utilizing the same mini-batch size, but without mixed precision, the alternative pipeline classifies 143 samples per second, and the ISNet, 298. Therefore, the ISNet is about 70\% to 108\% faster in this configuration. However, there is an even greater advantage regarding the model's size: the ISNet has about 8M parameters (the same as the DenseNet121), while the combined U-Net and DenseNet121 have 39M. Thus, the model size is reduced by a factor of almost 5. Naturally, the smaller the classifier model in relation to the segmentation network, the stronger the performance benefit provided by the ISNet architecture.

\subsection{Heatmap Analysis}
\label{heatmapAnalysis}

Besides being necessary for the calculation of the heatmap loss, the heatmaps created by our LRP block can be analyzed, improving the explainability of our models. In this section we compare heatmaps for the different DNN architectures, evaluating their attention to background bias. Please refer to Appendix \ref{radiologistAnalysis} for a comparison between ISNet X-ray heatmaps and lung lesions segmented by a radiologist. Figure \ref{triangle} shows heatmaps for the artificially biased test datasets, according to the implemented DNNs, all trained with databases containing white geometrical shapes correlated with the image's labels (in the image corners). The samples displayed here are from the biased evaluation scenario, thus containing the same background bias that was present during training. In the LRP heatmaps, red (positive relevance) indicates areas more associated with the image true class (indicated on the top of the figure), and blue (negative relevance) are regions that reduced the DNN confidence for the class (e.g., areas more associated to the other possible classes). For better visualization, we normalized the LRP maps presented in this section, making the blue and red channels range between 0 and 255. The face photograph has positive labels for the three attributes. Thus, it has three geometrical shapes, the square indicates rosy cheeks, while the circle represents high cheekbones, and the triangle smiling. Other images come from single-label datasets, thus just one geometrical shape is present, which is correlated with the class stated above the images (Figure \ref{triangle}). The image last row displays heatmaps for a standard classifier trained without the artificial bias, allowing the comparison of the ISNet attention pattern to the "natural" attention profile, which emerged without the influence of the geometrical shapes. Notice that the images' foreground (region of interest), defined in the ground-truth segmentation targets, are: the lungs, excluding the region overlapping the heart, the dogs' entire bodies, and the faces, excluding ears and hair, but including facial hair, eyes and mouth.

The vision transformer heatmaps in Figure \ref{triangle} are not created by LRP, they are generated with attention rollout, the standard methodology to visualize the model's attention\cite{VisionTransformer}. It does not differentiate positive and negative relevances, nor is it class selective. Although LRP can be used with vision transformers, the method usually requires custom model implementations, while attention rollout worked directly with the official PyTorch vision transformer code, which we utilized. Moreover, attention rollout served the purpose of showing that the model has noticeable background and bias attention.

\begin{figure}[!h]
\includegraphics[width=0.7\textwidth]{Figure2Bias.png}
\centering
\caption{Heatmaps for positive COVID-19 and Pneumonia X-rays and photographs, extracted from the synthetically biased test datasets (biased test). The triangle (background bias) indicates the classes COVID-19, smiling or Pug. The circle pneumonia, high cheekbones, and Tibetan Mastiff. The square rosy cheeks and Pekingese. Red colors in the LRP maps indicate areas the DNN associated to the image class (top row), blue colors are areas that reduced the network confidence for the classes. Ideally, heatmaps should display focus on the images' foregrounds: dogs, faces or lungs}
\label{triangle}
\end{figure}

The ISNet clearly shows no attention to the geometrical shapes. This result is confirmed by the quantitative scores in Table \ref{synth}, which demonstrate that the synthetic bias could not affect the ISNet's decisions; its removal from the test dataset, or even its substitution by confounding bias, could not reduce the ISNet maF1 score. Moreover, even comparing the ISNet heatmaps with the explanations for a standard classifier trained without synthetic bias, we observe that the ISNet could better focus on the region of interest. In the dogs classification, positive relevance (red) can be clearly seen especially in the dogs' faces. In facial attribute estimation, for the smile class we have positive attention in the mouth region. Finally, in lung disease classification, a correlation between the ISNet attention and radiological findings is clear (refer to Appendix \ref{radiologistAnalysis}).

Heatmaps for the ISNet Grad*Input show some attention over the synthetic bias. Thus, the explanations and the quantitative results in Table \ref{synth} indicate that the model's capacity to avoid background attention was surpassed by the ISNet, showcasing the advantage of LRP optimization (refer to Section \ref{mathBackground}). The segmentation-classification pipeline has no discernible attention over the synthetic bias, except for the Pug picture, where some red relevance is visible over the triangle. Indeed, only in Stanford Dogs the model had a small accuracy improvement when the bias was included in testing (Table \ref{synth}). Furthermore, the pipeline's heatmaps are less interpretable for the dogs classification task. This reflects the model's U-Net (segmenter) lower performance in the application: it has 0.61 test IoU for dog segmentation, versus 0.893 segmenting lungs, and 0.925 in face segmentation. Indeed, upon visual inspection, the dogs' segmentations tend to capture the overall animal location, but they are not true to its contours. The lower IoU can explain why some of the bias could have been fed to the dogs classifier, including part of the triangle in the pug image, and leading to the small maF1 gap in Table \ref{synth}. All the remaining benchmark DNNs' heatmaps reveal significant attention over the geometrical shapes (Figure \ref{triangle}). Thus, the LRP heatmaps explain the results in Table \ref{synth}: these model's explanations indicate that the geometrical shapes influence their decisions, and the benchmark classifiers had a significant accuracy drop when the synthetic bias was removed from the test dataset or substituted by deceiving bias.

Figure \ref{maps} presents test images and the associated heatmaps for the 3 classification problems without synthetic bias: COVID-19 detection, TB detection and facial attribute estimation. All X-rays are from the o.o.d. test databases (as they were in Figure \ref{training}). Figure \ref{maps2} displays GradCAM heatmaps for our methodology and for the extended GAIN, which is based on the optimization of GradCAM heatmaps. GradCAMs for the synthetic bias experiments will be analyzed in Section \ref{GAINComparison}, as they motivate an in-depth comparison of LRP optimization and GradCAM optimization. In the GradCAMs, the redder the area, the more attention was paid to it. GradCAM was implemented with the library PyTorch GradCAM\cite{GradCAMTorch}. High resolution versions of the X-rays and heatmaps can be found in the supplementary material (Supplementary Data 1).


\begin{figure}[!h]
     \centering
     \begin{subfigure}[b]{0.65\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Figure1.png}
         \caption{}
         \label{maps1}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.65\textwidth}
         \centering
         \includegraphics[width=\textwidth]{GCAMFigure1.png}
         \caption{}
         \label{maps2}
     \end{subfigure}
        \caption{LRP (a) and GradCAM (b) heatmaps for the deep neural networks. Image true class stated above the figures, DNN that produced the heatmap identified on the left. For LRP, red colors indicate areas that the DNN associated to the true class, blue colors are areas that decreased the network confidence for the class. For GradCAM, redder areas received more attention}
        \label{fig:three graphs}
\label{maps}
\end{figure}


In both the GradCAM and LRP heatmaps, we observe that the ISNet successfully kept the classifier attention inside the key areas (lungs/faces), with little to no background focus. LRP produced explanations of much higher resolution. Moreover, some details visible with LRP were lost with GradCAM. For example, the lesions in the left lungs of the pneumonia and COVID-19 X-rays can only be seen with LRP. Such lesions match with radiologist findings (see Appendix \ref{radiologistAnalysis}). Some areas of negative relevance (blue) in the LRP heatmaps were highlighted by GradCAM. However, LRP red regions have a stronger correlation with the radiologist's annotations (available in Supplementary Data 1). As an illustration, the right tuberculosis X-ray presents a lesion in the left lung's upper lobe, identified by the radiologist and the LRP heatmap (red), but not by GradCAM. Except for resolution and class selectivity (telling apart positive and negative relevance), the ISNet's LRP and GradCAM explanations are consistent, with GradCAM focusing on regions of high relevance in the LRP maps. Overall, the heatmap analysis shows that the ISNet capability of avoiding background attention was not matched by any other DNN.

Except for the ISNet and the model combining a segmenter and a classifier, the LRP heatmaps reveal significant background attention for all X-ray classifiers. X-ray classification with mixed databases is a task prone to shortcut learning\cite{ShortcutCovid}, as is confirmed by the amount of background attention seen in most heatmaps. The lack of background focus in the ISNet explanations confirms the model's superior capability of reducing shortcut learning, and explains its better performances on the external (o.o.d.) X-ray databases.

A very visible example of background bias can be found in the pneumonia X-ray, as markings over the right shoulder. The ISNet and the segmentation-classification pipeline are the only DNNs that did not pay any attention to it. The same can be said for an "R" in the neck area of the left TB X-ray, and an "L" in the corner of the other tuberculosis X-ray (over the shoulder). In the pneumonia sample, we observe that the multi-task U-Net and the AG-Sononet, were the models that focused the most on the bias. Accordingly, the two DNNs had the worst performances on the o.o.d. test database for pneumonia and COVID-19 detection. To create the multi-task model heatmaps, we considered LRP propagation from the classification outputs, since we aim to analyze the input features that influenced the classification decisions. Besides focusing on markings, we can observe that the X-ray classifiers (except for the ISNet and segmentation-classification pipeline) also paid attention to parts of the body outside the lungs. In the left extended GAIN LRP heatmap for tuberculosis we can even notice attention to the body contour. Naturally, such undesired sources of focus can also be seen as background bias and signs of shortcut learning.

The heatmaps lead us to the same conclusion we had when analyzing the multi-task DNN o.o.d. F1-Scores and its results in the synthetic bias experiments: a multi-task network learning segmentation and classification can show good segmentation outputs, while its classifier concentrates on background bias. The multi-task DNN heatmaps confirm that its classification decision rules strongly consider areas outside of the lungs, creating an attention pattern similar to the DenseNet121's. Therefore, multi-task classification and segmentation is ineffective for preventing focus on background bias and avoiding shortcut learning.

An attention gated DNN focuses on features that improve classification performance, and, since background bias reduces the classification training loss, the model learns to focus on it. In fact, in the Figure \ref{maps} X-ray heatmaps we observe that the AG-Sononet is the DNN that paid the most attention to background markings. It is noticeable that DNN concentrates its attention on fewer image features (in relation to a standard classifier, like the DenseNet), but the selected features can be in the background and constitute bias. In conclusion, models that do not employ an attention mechanism based on semantic segmentation cannot reliably differentiate relevant image features from background features that are correlated with the images' classes. This conclusion is also valid for the vision transformer. The model has a more diffuse attention distribution, and its o.o.d. performance scores surpassed the AG-Sononet in COVID-19 and tuberculosis detection, but it could not overcome a standard classifier (DenseNet121). Indeed, the diffuse attention profile indicates that the vision transformer's decisions are influenced by both foreground features and by background bias, explaining why the model could not hinder shortcut learning (as demonstrated in Table \ref{synth}).

The LRP heatmaps (Figures \ref{triangle} and \ref{maps}) for RRR show that the model could not effectively concentrate attention inside the regions of interest. Background bias relevance is visible, corroborating with the quantitative results in Table \ref{synth}. This finding indicates that, in our experiments, the observed low values in the networks "right reasons" loss\cite{RRR} were not associated with a true focus on the images' foregrounds. We utilized deep networks and high image resolution, which augment the noise in input gradients. We suspect the noise to be detrimental for the achievement of true foreground focus during the training process, thus the minimization of the right reason loss can mostly reflect an overall reduction in the magnitude of the classifier input gradients. Please refer to Section \ref{mathBackground} for a more profound analysis.

In COVID-19 detection, there is significant relevance alongside the lungs' borders in the alternative segmentation-classification pipeline heatmaps. This effect is even more pronounced in the model's heatmaps for tuberculosis detection. It was not noticeable in the dogs classification task, as the dogs' segmenter cannot accurately follow the animal's contours. In TB classification we see attention in the background of the pipeline's heatmaps, as exemplified in Figure \ref{maps}. Upon visual inspection, we verified that the model's segmentation outputs for the images are adequate, and the background is being properly erased. Indeed, the model presents high IoU (over 0.9) even with manually created lung segmentation masks from the o.o.d. TB database. Thus, the classifier is paying attention to the dark (erased) background, as is confirmed by the homogeneous and undetailed aspect of this attention. The classifier inside the alternative segmentation-classification pipeline receives a segmented image, where the borders between the region of interest and the background are easily identifiable, as is the lungs' placement, size and shape. Therefore, the classifier may have learned to analyze the shape of the lungs' borders and the background format, using this information to distinguish between the classes in the training dataset (even though we employed translation and rotation as data augmentation). Being unrelated to the diseases, decision rules based on the aforementioned aspects shall hinder generalization capability, being considered another form of shortcut learning. This phenomenon may explain why the alternative pipeline performance in the o.o.d. COVID-19 or TB test datasets was worse than the ISNet's, which does not show significant border or background attention. In preliminary tests we applied different thresholding techniques to the pipeline segmenter's output, allowing a range of values and thresholding only quantities outside of this range. The methodology produced blurry lung borders, but it did not improve generalization performances.

The two photographs in Figure \ref{maps} are labeled as containing the 3 considered attributes. In the facial attribute estimation task, we see that the DenseNet121 pays some attention to body features outside of the face, and to the remaining background. However, most of the model's main focus is on the face. This observation reinforces the idea that background features in the CelebA dataset are not strongly correlated with the image's classes, thus not representing much background bias, which would attract more classifier focus. Moreover, the heatmaps show that a standard classifier could identify the relevant image region. Attention gated CNNs were designed to be more resistant to background clutter\cite{AGNet}. Additionally, past studies suggest that, considering images with cluttered backgrounds, multi-task models learning classification and segmentation can achieve better performances for the two tasks\cite{MultiTask1}\textsuperscript{,}\cite{MultiTask2}. When comparing the two models with the DenseNet121, we observe that their attention is more contained inside the face regions. Thus, multi-task learning and attention gates can be useful to deal with background clutter that is not correlated with the samples' classes. The two photographs in figure \ref{maps} exemplify the wide diversity of segmentation mask sizes in the CelebA dataset. We observe that, classifying both the small and the large face, the ISNet precisely kept the model focus inside the region of interest.

\subsubsection{GAIN and Reliability Comparison Between LRP and GradCAM}
\label{GAINComparison}

In Figure \ref{maps2}, the ISNet GradCAM explanations mostly focus on parts of the region of interest (lungs/faces). This is an expected and acceptable behavior, as classifiers naturally focus more on the parts of the image foreground that they deem more relevant\cite{GAIN} (e.g., the mouth region when classifying a smile). Meanwhile, the GAIN GradCAM heatmaps successfully achieved the model's losses' main objective\cite{GAIN}: to highlight the region of interest (lungs or faces) in a more complete manner. Unlike the ISNet, GAIN's main purpose is to generate GradCAM heatmaps that will be used to train weakly supervised semantic segmenters, which benefit from heatmaps highlighting the region of interest entirely\cite{GAIN}.

Besides the standard classification loss, extended GAIN utilizes the attention mining and the external supervision losses\cite{GAIN}. The first has the objective of expanding the DNN attention to all image regions related to the classes. The second should make the GradCAM explanations match the ground-truth segmentation masks. Analyzing the model's LRP heatmaps, we observe that it focuses both on the region of interest and on the background. For example, Figure \ref{maps1} shows that the AG-Sononet learned to mostly focus on bias for X-ray classification, while GAIN pays attention to bias and to the lungs. Likewise, GAIN's LRP heatmaps for photographs (Figure \ref{maps1}) show focus on the faces and on the background. Indeed, in relation to all tested DNNs, GAIN had the largest amount of background focus in facial attribute estimation. Supporting the LRP explanations, numerical results show that GAIN had F1-Score below the standard classifier in this task (Table \ref{FacesPerformance}). Moreover, GAIN's o.o.d. test results for COVID-19 and TB detection could not significantly surpass the standard classifier (Tables \ref{performance} and \ref{tb}). Such results indicate that the model also suffered strong shortcut learning, corroborating the background focus shown in the its LRP heatmaps.

In the LRP explanations for the artificially biased images (exemplified in Figure \ref{spurious} and \ref{triangle}), we observed an analogous situation. GAIN heatmaps indicate focus on the geometrical shapes, and on the region of interest. Once more, GAIN's performance scores confirm the validity of the LRP explanations: maF1 decreased when the synthetic bias was removed from testing, or substituted by deceiving bias (Table \ref{synth}). Overall, LRP heatmaps and numerical results point that, although GAIN could reduce the concentration of attention on background bias in relation to a standard classifier, it could not effectively prevent the attention to background bias. Possibly, the observed distribution of attention across most of the image is a consequence of the attention mining loss, while the external supervision loss could not effectively hinder background focus.

\begin{figure}[!h]
\includegraphics[width=0.6\textwidth]{Figure2b.png}
\centering
\caption{LRP heatmaps and GradCAM for positive COVID-19 X-ray and photograph, both images extracted from the artificially biased test datasets. The triangle (background bias) indicates the presence of the classes COVID-19, smiling or Pug. The circle indicates pneumonia, high cheekbones and Tibetan Mastiff. Red colors in the LRP maps indicate areas the DNN associated to COVID-19 or smiling, blue colors are areas that reduced the network confidence for the classes}
\label{spurious}
\end{figure}

Contrary to LRP, GAIN GradCAM heatmaps indicate little to no background attention, as can be seen in Figure \ref{maps2}. For example, the markings in the pneumonia and tuberculosis X-rays (Figure \ref{maps}) can be seen in the GAIN LRP heatmaps, but not in GradCAM. Moreover, GAIN's GradCAM explanations show no attention over the artificial biases, again disagreeing with GAIN's LRP heatmaps, as clearly demonstrated in Figure \ref{spurious}. Therefore, unlike LRP, GAIN's GradCAM cannot explain the model's quantitative results, especially how the inclusion of the synthetic background bias improved F1-Scores, and how deceiving bias reduced it (Table \ref{synth}). Accordingly, we may conclude that the extended GAIN classifier learned to produce GradCAM heatmaps that hide background focus, inaccurately representing the DNN attention, while the classifier's decision rules consider background features.

The GAIN performance scores and LRP heatmaps evidentiate that GradCAM has fundamental flaws, which allow the creation of heatmaps that do not reflect the neural network's real attention pattern. In essence and in its most common implementation (including GAIN's), GradCAM heatmaps are based on a linear combination of the different channels (feature maps) in the output of a chosen DNN layer (normally, the last convolution)\cite{GAIN}\textsuperscript{,}\cite{GradCAM}. The combination weight of a channel is given by the average value of the gradient in the channel, which is back-propagated from a DNN output neuron until the chosen layer. Afterwards, a ReLU function is applied to the result of the linear combination, removing its negative values. In the end, the resulting low-resolution heatmap is interpolated to the original image size. Below, we analyze two shortcomings of this procedure.

First, by deleting negative values, the ReLU operation can ignore negative evidence, i.e., feature map areas that reduced the DNN confidence for the class considered in the GradCAM heatmap. Negative evidence can cause classifier bias. As an illustration, if the class COVID-19 is highly correlated with a triangle, instead of increasing the COVID-19 prediction score when the geometrical shape is present, the classifier can reduce the confidence for the other classes when it sees a triangle. For this reason, GradCAM's ReLU function can make GAIN losses less effective in hindering background attention related to negative evidence. LRP indicates both the positively and the negatively relevant input features. To avoid both types of background attention, the ISNet's heatmap loss (Section \ref{loss}) must minimize positive and negative relevance. Moreover, to fully solve the problem of negative evidence, the ISNet minimizes background relevance in the heatmaps of all possible image classes. Otherwise, negative background relevance in the losing classes can produce bias (see Section \ref{IsNetSec}).

This first GradCAM weakness could be alleviated by substituting the ReLU in GradCAM by an alternative function, such as the absolute value, and GAIN can be easily adapted to consider heatmaps for all classes in the external supervision loss. We experimented with these modifications, but had no success improving GAIN's o.o.d. generalization, and the DNN still paid attention to background bias. Such results point to a more fundamental problem in GradCAM. The technique's heatmaps are produced from a linear combination of a DNN layer's feature maps. Normally, the output from the last convolutional layer is chosen, as it contains a higher level of abstraction, better capturing the DNN behavior\cite{GradCAM}\textsuperscript{,}\cite{GAIN}. However, modern neural networks are very deep, and their latter convolutional layers have exceptionally large receptive fields, commonly covering the entire input. When layers' feature maps are interpolated to the input size, we expect their activations to align with the input features that caused such activations. This assumption is fundamentally necessary for GradCAM to produce faithful explanations, and it seems to hold for standard classifiers, and for DNNs that do not directly optimize GradCAM heatmaps. However, due to the large receptive fields, there is no guarantee that a DNN will not learn to map the input's background features to late layers' activations that align with the image foreground. For example, an activation in the corner of the first layer's feature map may cause an activation in the center of a feature map from the last convolutional layer. In such cases, GradCAM will not show background attention, but background features will be able to alter the outputs of the deep layers, influencing the classifier decisions. Thus, GradCAM heatmaps will become misleading explanations of the DNN behavior.

This spurious DNN mapping strategy does not seem common for classifiers trained to minimize a standard classification loss, nor for the ISNet, whose GradCAM explanations are consistent with LRP heatmaps (Figure \ref{maps} and \ref{triangle}). However, the GAIN classifiers may learn spurious mapping to satisfy the GAIN's GradCAM-based external supervision loss, while the hidden attention to background bias allows the DNN to better optimize the training classification loss. Possibly, this undesired solution is more easily achieved than the ideal solution (classifiers without background attention) in many classification settings.

Unlike the problems caused by the ReLU function, the possibility of spurious mapping is intrinsic to the nature of deep neural networks. Furthermore, the fundamental definition of GradCAM allows such mapping strategies to generate unreliable heatmaps. Limiting receptive field to avoid spurious mapping would restrict the DNN design, limiting its capacity and depth. Using GradCAM heatmaps created with earlier layer activations (which have smaller receptive fields) is also not ideal. Firstly, they would not effectively capture high-level semantics\cite{GCAMAbstraction}. Moreover, GradCAM-based losses (e.g., GAIN) using such heatmaps would prompt the DNN to filter out the influence of background features at an early stage in the network. To ignore the background, a DNN must learn to identify the images' foreground features, an operation that may be complex. The utilization of early GradCAMs would limit the number of layers that the DNN can use to implement this operation, restricting its complexity. In conclusion, due to GradCAM weaknesses, we advocate that GradCAM heatmaps are not reliable enough to be directly optimized with the objective of controlling classifier attention, especially when bias is present.

LRP and GradCAM create explanation heatmaps with drastically different procedures. Instead using a linear combination of one layer's feature maps, LRP backpropagates a signal through the entire neural network, capturing information from all parameters and activations to produce a high-definition heatmap, which incorporate high level semantics and precise localization\cite{LRPBook}. Consider a case of spurious mapping, where the classification was influenced by a late layer's activation aligned with the region of interest, but the activation was actually caused by a background feature in the input. In this case, due to its backpropagation rules (Section \ref{LRP}), LRP will map the late activation's relevance back to the background region in the DNN's input. Thus, LRP will reveal background attention, but GradCAM will not. Consequently, LRP-based attention mechanisms such as the ISNet are not vulnerable to a solution based on spurious mapping.

In summary, past studies have shown that LRP is more robust than GradCAM\cite{LRPRobustness}, the technique has a more consistent mathematical background\cite{LRP}\textsuperscript{,}\cite{LRPBook}, its optimization is more theoretically founded (Section \ref{mathBackground}), and it does not present GradCAM's aforementioned flaws. Accordingly, in this study's experiments, our LRP-based attention mechanism surpassed the GradCAM based GAIN, by demonstrating less background attention and improved o.o.d. test performance. Therefore, we pose that Layer-wise Relevance Propagation is more reliable than GradCAM for the construction of attention mechanisms and the reduction of shortcut learning.


\section{Discussion}

%grad input, rrr, math foundamentals

Background bias attracts the attention of a deep classifier, making it consider unreliable and non-generalizable background features in its decisions. Resulting models perform well in similarly biased environments, but lose performance when deployed, as the biasing features are not present in real-world data. In other words, background bias enforces shortcut learning\cite{ShortcutLearning}. This problem is clearly demonstrated by our experiments with synthetic background bias, where the bias removal from the testing data (or its substitution by deceiving bias) has dramatic consequences to standard classifiers' accuracy (Table \ref{synth}).

With multiple synthetic bias experiments, considering different tasks, dataset sized and classifier backbones, we systematically demonstrated the ISNet superior capacity to avoid background attention and the shortcut learning induced by background bias. Quantitative results show that only two of the multiple tested neural networks could consistently hinder background bias attention, having their decisions not affected by the synthetic bias: the ISNet and the alternative segmentation-classification pipeline. The ISNet matched or surpassed the large pipeline in two of the three synthetic bias experiments, and it overcame it in all the experiments with naturally occurring background bias (COVID-19 and TB detection). Furthermore, the proposed model is much more efficient: by replacing a U-Net followed by a DenseNet121 with an ISNet based on the same DenseNet121 classifier, we obtain a model that is about 70\% to 108\% faster at run-time, and has almost 80\% less parameters. This benefit would be even more pronounced for smaller classification neural networks.

All other state-of-the-art neural networks we tested failed to avoid shortcut learning, as they considered the synthetic bias in their decisions (standard classifier, multi-task DNN, AG-Sononet, GAIN, HAM, RRR and vision transformer). Thus, they demonstrate significant performance degradation when the synthetic bias is removed or replaced by deceiving bias (Table \ref{synth}). When analyzing the LRP heatmaps for the models, attention to the synthetic bias is apparent, unlike in the case of the ISNet.

Dataset mixing is necessary whenever researchers need to work with a classification database that does not contain all the desired classes. However, as samples from different classes are gathered from diverse sources, background characteristics specific to the sources become correlated with the classification labels. Accordingly, they correspond to background bias, fostering shortcut learning\cite{ShortcutLearning}. Dataset mixing became the standard for creating COVID-19 detection datasets, as large data sourced did not provide images for all medical conditions of interest (e.g., COVID-19, pneumonia and healthy subjects)\cite{BrixiaSet}\textsuperscript{,}\cite{BimcvSet}\textsuperscript{,}\cite{GitCovidSet}. Consequently, shortcut learning is a known issue in the task of COVID-19 detection using X-rays from mixed datasets\cite{bassiCovid}\textsuperscript{,}\cite{critic}\textsuperscript{,}\cite{NatureCovidBias}\textsuperscript{,}\cite{ShortcutCovid}. We confirmed that a standard classifier (DenseNet121 analyzing whole images) failed to properly generalize to an external, o.o.d. COVID-19 detection dataset (images from hospitals not seen in training). It achieved only 0.546 +/-0.01 macro-average F1-Score, and its heatmaps revealed strong background attention. The results are consistent with shortcut learning. Besides COVID-19 detection, we analyzed the task of tuberculosis detection with one of the largest open-source Tuberculosis X-ray datasets\cite{TBPortals}, which also requires dataset mixing. In this application, a standard classifier (DenseNet121) achieved high performances on i.i.d. evaluation datasets, weak results on o.o.d. databases, and strong background attention. Therefore, the application exemplifies another case of background bias fostering shortcut learning. 

The quantitative results in COVID-19 and TB detection confirm our findings from the synthetic bias experiments: the ISNet reduced attention to background bias and achieved the highest generalization performance in both tasks. It significantly surpassed all other neural networks, with 0.773 +/-0.009 maF1 in the external COVID-19 test dataset, and 0.738 +/-0.044 maF1 in the o.o.d. evaluation for TB classification. Like in the synthetic bias experiments, except for the segmentation-classification pipeline and the ISNet, all other neural networks displayed significant background attention in their LRP heatmaps. This finding explains why the ISNet excelled their o.o.d. test performances. As past studies suggest\cite{bassi2021covid19}\textsuperscript{,}\cite{covidSegmentation}, the utilization of lung segmentation to remove the X-ray background before classification reduced shortcut learning in COVID-19 detection. Accordingly, and corroborating with the findings from the synthetic bias experiments, the segmentation-classification pipeline strongly surpassed the other benchmark DNNs on an external COVID-19 dataset (0.645 +/-0.009 maF1), and its LRP heatmaps showed focus on the lungs. However, as best demonstrated in TB detection, the alternative pipeline can pay attention to lung borders and to the erased background. This possibly reveals decision rules that consider the region of interest shape and location, hindering generalization. This attention profile was more apparent in TB detection. Unsurprisingly, the pipeline's generalization was worse in the task (0.576 +/-0.05 maF1 with o.o.d. evaluation). The ISNet did not display this problem, and it significantly surpassed the alternative model in both TB and COVID-19 detection. 
%The only exception was the ISNet Grad*Input, an ablation experiment substituting LRP heatmaps by Gradient*Input explanations, 

An attention gated network and a vision transformer could not attenuate shortcut learning, they paid attention to sources of background bias and generalized poorly. This result suggests that attention mechanisms that do not learn from semantic segmentation targets are not useful to avoid shortcut learning. Without an external guide indicating what should be ignored, the models learn to focus on the image features with high potential to improve classification loss during training, as background bias features. Using a multi-task learning model, which performed image segmentation and classification simultaneously, was also ineffective in dealing with background bias. The model could correctly segment lungs, faces or dogs, regardless of background features. However, the classification procedure focused on the background and showed poor generalization. Therefore, in the presence of background bias, the multi-task DNN attention profiles for segmentation and classification diverged.

Besides the ISNet, two benchmark DNNs optimize explanation heatmaps to control a classifier's attention: GAIN\cite{GAIN} and RRR\cite{RRR}. However, instead of LRP, GAIN's heatmaps are built with GradCAM\cite{GradCAM}, and RRR's with input gradients\cite{saliency}. In relation to the ISNet, the models' capability of avoiding background attention is not up to par. Unlike the LRP-based ISNet, they could not avoid the artificial biases' influence on the classifier decisions and F1-Scores, as demonstrated by significant accuracy drops when the synthetic bias was removed or substituted by deceiving bias. Accordingly, the models' LRP explanation heatmaps (Figure \ref{maps1} and \ref{triangle}) indicate attention to the natural and artificial sources of background bias. Accordingly, the models' o.o.d. performances performances were strongly surpassed by the ISNet. Like GAIN, Hierarchical Attention Mining\cite{HAM}, another GradCAM-based attention mechanism, was also unable to avoid background bias attention. In an ablation experiment (the ISNet Grad*Input), we substituted the ISNet LRP heatmaps by Gradient*Input explanations, which are seen as an improvement over input gradients\cite{GradInput}. The model was tested with the three synthetic bias experiments. It displayed a small performance degradation when background bias was removed from the test data. However, the model could not converge properly when utilizing a large classifier backbone (DenseNet121), leading to small overall accuracy. In summary, we empirically demonstrate the higher reliability of LRP optimization in deep neural networks, in relation to optimizing the more common explanation methodologies of GradCAM, input gradients and Gradient*Input.


Furthermore, we justify our empirical results with an in-depth theoretical analysis (Section \ref{mathBackground}). Briefly, LRP-$\varepsilon$ has a solid mathematical background, with roots on the application of Taylor expansions at the DNN's neurons\cite{LRPBook}. These mathematical fundamentals allow the classifier resulting from LRP optimization (ISNet) to have superior bias resistance, without losing overall accuracy. Meanwhile, GradCAM optimization led to classifiers learning to produce spurious GradCAM explanations, which showed no background bias attention, while quantitative performance scores (Table \ref{synth}) proved that the models' decisions were being influenced by the background bias. LRP heatmaps revealed the attention to background bias for such models. Section \ref{GAINComparison} and Appendix \ref{CheXpertClassification} analyze these results in detail.

On the other hand, input gradients, optimized by RRR, are a powerful tool to optimize shallower classifiers, as shown by the study presenting the method\cite{RRR}. However, input gradients are commonly not seen as adequate explanations for deep neural networks, especially due to their noisiness\cite{LRPvsGrad}. We hypothesize that such characteristic hinders the optimization of RRR when considering large classifier backbones, similarly to Gradient*Input optimization. We formally demonstrate how LRP-$\varepsilon$ reduces explanation noise in comparison to input gradients and Gradient*Input (Appendix \ref{lrpVsGradInput}). Moreover, we show that, from a Deep Taylor Decomposition standpoint, LRP-$\varepsilon$ represents more contextualized and coherent explanations of the classifier behavior. In summary, LRP's theoretical fundamentals justify why its optimization leads to robust and accurate classifiers (Section \ref{mathBackground}), explaining its superior bias resistance, empirically verified in multiple and diverse experiments with background bias (Tables \ref{synth} to \ref{tb}).

We can convert virtually any classifier architecture to an ISNet, without any extra parameters or computational cost at run-time. The limitation of the ISNet is its training time. For the DenseNet121-based model, the training time was 82\% longer than the standard segmentation-classification pipeline's in facial attribute estimation. The proposed model creates heatmaps for different classes in parallel, but the length of its training time still increases with the number of classes in the classification problem. More efficient implementations may reduce this limitation. However, in its current formulation, the ISNet exchanges training time for run-time performance. But this trade-off may be very profitable, given that DNNs can be trained with powerful computers, then later deployed in less expensive or portable devices. 

The ISNet results for the task of COVID-19 detection are promising, and in accordance with studies that employed lung segmentation and evaluated their DNNs in external databases, a technique that is required to reduce the effects of background bias and shortcut learning in the reported performances\cite{ShortcutCovid}\textsuperscript{,}\cite{bassi2021covid19}\textsuperscript{,}\cite{NatureCovidBias}.  Appendix \ref{radiologistAnalysis} compared ISNet test X-ray heatmaps to X-rays with lung lesions located by a radiologist. The compassion indicated a correlation between the radiologist's analysis and the ISNet heatmaps. Our i.i.d. evaluation results in TB detection are in line with other studies\cite{TBReview}, but a strong drop in performance was observed with the o.o.d. test dataset. This poor generalization was also shown by a previous work\cite{TBBadGeneralization} and is a known concern for TB detection\cite{who}. The ISNet alleviated the problem. Without clinical tests to ensure adequate real-world results, we cannot claim diagnosis performance or point our methodology as a substitute for RT-PCR in COVID-19 detection. Moreover, our COVID-19 datasets contain data from the first semester of 2020. Classification performance may strongly diverge for present-day X-rays, as the disease's main symptoms change, especially considering vaccination and the prevalence of more recent variants of the virus. The X-ray classification tasks in this paper must be seen as demonstrations of the novel attention mechanism's efficiency, not as indications of clinical performance.

In facial attribute estimation, all tested DNNs performances metrics were similar. Moreover, the DenseNet121 background attention is much less intense in this task than in the previous two. This scenario indicates that the CelebA dataset background features are not strongly correlated with the samples' classes, portraying much less background bias in relation to the mixed COVID-19 or TB databases. Accordingly, the attention gated network and the multi-task DNN properly focused on the faces. The ISNet did not strongly improve accuracy for the in-domain problem with little background bias. When synthetic bias was added to the data, the ISNet became the best performing model, as benchmark neural networks lost performance. In summary, background relevance minimization was successful in avoiding background attention with both biomedical and natural images, the last containing background clutter and a wide variety of segmentation target configurations. Thus, it improved security in facial attribute estimation. Able to implicitly segment the region of interest and contain the classifier attention within it, the ISNet run-time model represents an efficient use of a deep neural network potential and flexibility.

The ISNet avoids background focus and shortcut learning, increasing confidence in the results achieved by a DNN. Additionally, background relevance minimization is more interpretable than standard spatial attention mechanisms: we know that it works by hindering attention outside of a region of interest, which is clearly defined by the ground-truth segmentation targets used during training. A past work qualitatively and quantitatively demonstrated that LRP explanations provide higher resolution heatmaps and more interpretable information than attention mechanisms and the corresponding attention heatmaps\cite{LRPVsAttention}. Furthermore, in relation to standard attention, LRP heatmaps can reveal additional evidence used by the classifier to make a decision\cite{LRPVsAttention}. Another study compared explanation techniques and found layer-wise relevance propagation to be among the most robust, surpassing GradCAM and Gradient*Input\cite{LRPRobustness}\textsuperscript{,}\cite{LRPvsGrad}. In this study, we empirically and theoretically demonstrate that the ISNet's optimization of LRP heatmaps lead to deep classifiers that are resistant to background bias, while retaining high accuracy, surpassing multiple state-of-the-art attention mechanisms.

\section{Methods} 
\subsection{Layer-wise Relevance Propagation}
\label{LRP}
Deep neural networks are not easy to interpret. This is because they are complex and nonlinear structures with millions of trainable parameters. Layer-wise relevance propagation\cite{LRP} (LRP) is an explanation technique tailored for deep models. It was created to solve the challenging task of providing heatmaps to interpret DNNs. Furthermore, it is one of the most robust explanation methodologies to date\cite{LRPRobustness}. Heatmaps are graphics that show how a neural network distributes its attention in the input space. For each class, a heatmap explains if an image region has a positive or negative, strong or weak influence (relevance) on the classifier confidence for that class. From a theoretical perspective, the LRP is rooted on the Deep Taylor Decomposition framework\cite{LRPBook}\textsuperscript{,}\cite{LRPZb}, which explains a classifier's decisions according to a series of local Taylor expansions, performed at each neuron. Please refer to Section \ref{mathBackground} for an exposition of LRP's fundamentals, and the theoretical justification for the ISNet's optimization of LRP heatmaps.

LRP is based on the almost conservative propagation of a quantity called relevance through the DNN layers, starting from one of the network output neurons and ending at the input layer, where the heatmap is produced. The meaning of the relevance in the heatmap is determined by the choice of the output neuron where the propagation starts. Positive values indicate that an input image pixel was associated with the class predicted by the chosen output neuron, while negative values indicate areas that reduce the classifier confidence in the class (e.g., regions that the classifier related to other classes in a multi-class, single-label problem). Furthermore, high (absolute) values of relevance show input features that were important for the classifier decision, i.e., the heatmap makes the classifier's attention explicit. 

Previous studies used LRP to understand which X-ray features were important for a DNN classifying COVID-19 patients, pneumonia patients, and healthy people\cite{bassi2021covid19}\textsuperscript{,}\cite{FirstPaperCovid}. One study used lung segmentation as a preprocessing step and found a strong correlation between lung areas with high LRP relevance for the COVID-19 class and regions where radiologists identified severe COVID-19 symptoms\cite{bassi2021covid19}. Both studies used training datasets from mixed sources and observed a concerning quantity of relevance outside of the lungs when segmentation was not used. In particular, LRP revealed that words exclusively present in the COVID-19 class strongly attracted the classifier's attention, which associated them with COVID-19. Figure \ref{seduto} is one example, containing the word ``SEDUTO'' and the letters ``DX'' in its upper corners. In the corresponding heatmap, the words' red coloration indicates their high relevance for the COVID-19 class. 

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{Seduto.png}
\centering
\caption{COVID-19 positive chest X-ray and respective LRP heatmap, where red regions show positive relevance for the COVID-19 class and blue areas show negative relevance (image parts associated with other classes, i.e., normal or pneumonia). Figure extracted from\cite{bassi2021covid19}}
\label{seduto}
\end{figure}

LRP starts from the output of the neural network. After choosing one output neuron (according to its predicted class), we can define its relevance as equal to its output value (prior to nonlinear activation), and set the relevance of all other last-layer neurons to zero. Then, LRP uses different rules to propagate the relevance through each DNN layer, one at a time, until it reaches the model's input. The choice of the set of rules influences the heatmap's interpretability and the stability of the relevance propagation\cite{LRPBook}. Here, we will briefly explain the rules and procedures that were important for the creation of the ISNet.

The most basic rule is called LRP-0. We define the k-th output of a fully-connected layer ($z_{k}$) before activation as:

\begin{equation}
z_{k}=b_{k}+\sum_{j} w_{jk}a_{j}
\end{equation}

Where $w_{jk}$ represents the layer's weight that connects its input j to output k, $b_{k}$ the output k bias parameter, and $a_{j}$ the value of its input j. LRP-0 propagates the relevance from the layer output, $R_{k}$, to its input, $R_{j}$, according to the following equation\cite{LRPBook}:

\begin{equation}
\label{LRP0}
R_{j}=\sum_{k}\frac{w_{jk}a_{j}}{z_{k}}R_{k}
\end{equation}

Analyzing the fraction above, we see that LRP-0 redistributes the relevance from the layer k-th output ($R_{k}$) to its inputs (j) according to how much they contributed to the layer's k-th activation ($z_{k}$). A second rule, LRP-$\varepsilon$, changes LRP-0 to improve the relevance propagation stability, reduce the noise in the relevance signal, and improve explanation contextualization and coherence (refer to Section \ref{mathBackground}). It adds a small constant, $\varepsilon$, to $z_{k}$. Being $sign(\cdot)$, a function that evaluates to 1 for positive or zero arguments, and to -1 otherwise, LRP-$\varepsilon$ is defined as:

\begin{equation}
\label{LRPeEquation}
R_{j}=\sum_{k}\frac{w_{jk}a_{j}}{z_{k}+sign(z_{k})\varepsilon}R_{k}
\end{equation}

LRP is a scalable technique that can be efficiently implemented. A four-step procedure exists to rapidly execute the aforementioned LRP rules in a fully connected or convolutional layer with ReLU activation function\cite{LRPBook} (see section \ref{layers}). It is possible to adopt a different rule for the DNN input layer, taking into account the nature of the input space. For images, a rule called Z$^{B}$\cite{LRPBook} considers the maximum and minimum pixel values allowed in the figures. Average and sum pooling, as linear operations, can be treated similarly to a convolutional layer. We can either handle max pooling as sum pooling, or adopt a winner-takes-all strategy, propagating all the layer output relevance to its inputs that were actually selected by the max pooling operation. Finally, batch normalization layers can be merged with neighboring convolutional or fully connected layers, defining a single equivalent layer, through which we propagate the relevance \cite{BNLRP}.

With defined rules for the most common DNN layers, LRP configures a very flexible technique, which is scalable and applicable to virtually any neural network architecture. For example, for the Keras Python library, implementations for many popular deep learning models are available at\cite{innvestigate}.

\subsection{Background Relevance Minimization and Implicit Segmentation Neural Network}
\label{IsNetSec}

Layer-wise Relevance Propagation was created to improve DNN interpretability\cite{LRP}. With the ISNet, we introduce an additional function for it by using LRP to explicitly control a DNN's attention. LRP heatmaps show the relevance of each input feature (e.g., pixels) for the DNN classification. During training, background relevance minimization penalizes undesired attention in the maps, forcing the classifier to minimize it and thus to consider only the relevant image regions. To minimize undesired LRP relevance (both positive and negative) during training, we propose a loss function comprising two terms: the traditional classification loss (which compares the supervised labels with the DNN classification output, e.g., cross-entropy) and a new loss term (which we call 'heatmap loss') that contrasts the classifier's LRP heatmaps with segmentation targets. We precisely define the ISNet loss function in Section \ref{loss}. During training, if a gold standard of segmentation targets is not available for the entire dataset, a pretrained segmenter (e.g., U-Net) can create the segmentation labels.

The training-time ISNet is defined by two modules: a classifier (e.g., DenseNet\cite{DenseNet}, ResNet\cite{ResNet}, VGG\cite{VGG}) and an LRP block. The LRP block is a DNN created as a mirror image of the classifier. Its layers perform LRP propagation rules, using shared weights and skip connections with the equivalent classifier layers to propagate relevance through them. Thus, the LRP block produces LRP heatmaps. The advantage of defining the LRP relevance propagation process itself as a neural network is that the optimizers in standard deep learning libraries can automatically backpropagate the heatmap loss through the LRP block. Therefore, we avoid the task of manually defining LRP backpropagation rules, which could require changes in the standard optimization algorithms, making the ISNet much more complicated to implement and less practical. All parameters in the LRP block are shared with the classifier. It therefore generates the same heatmaps as those created with traditional LRP. Moreover, due to parameter sharing, after backpropagation of the heatmap loss through the LRP block, the optimization of the block's parameters is automatically reflected in the classifier. Section \ref{layers} defines all the types of LRP block layers that we created (alternatively, LRP layers), which are enough to perform LRP in a modern, efficient, and complex classifier: a Densely Connected Convolutional Network\cite{DenseNet}. 

For ISNets to work properly, we need to produce one heatmap for each possible class, starting the LRP relevance propagation at the output neuron that classifies it. We cannot minimize unwanted LRP relevance for a single class (e.g., the winning one). Imagine that we have a bias in the image background, associated with class C, and we minimize only the unwanted LRP relevance for class C. In this case, the classifier can negatively associate all other classes with the bias, using it to lower their outputs, making the class C output neuron the winning one. This negative association is expressed as negative relevance in the other classes' heatmaps. Consequently, the penalization of positive and negative unwanted relevance in all maps is a solution to the problem. Fortunately, using the LRP block to perform relevance propagation provides a simple and efficient strategy for creating multiple heatmaps. They are treated as different samples inside a mini-batch, being processed in parallel, and the relevance propagations for different classes do not interact with each other. 

Figure \ref{SimpleISNet} presents a simple ISNet example, whose classifier comprises two convolutional layers, L1 and L2, followed by a linear layer, L3. $\bm{x}_{i}$ indicates the input of classifier layer Li. With LRPi being the layer in the LRP block responsible for performing the relevance propagation through Li, its output, $\bm{R}_{i-1}$, is the relevance at the input of layer Li. $\bm{y'}$ is the classifier output for one of the classes (with all other outputs set to zero), which will be used to generate the heatmap associated with it. To perform the propagation, LRPi shares parameters with Li.

\begin{figure}[h]
    \includegraphics[width=0.5\textwidth]{ISNetSimple.png}
    \centering
    \caption{Representation of an ISNet for a classifier containing 3 layers. The classifier is on the left, and the equivalent LRP block on the right}
    \label{SimpleISNet}
\end{figure}

After training the ISNet, using the classifier and LRP block and simultaneously minimizing the respective loss functions, the LRP block can be removed. During run-time, the classifier will continue to automatically pay attention only to the images' region of interest. As an additional benefit, the LRP heatmaps may be used before removal for their original function, that is, to interpret the classifier's decisions. Moreover, we can use them to check for background attention, assessing the precision of the learned implicit foreground segmentation. 

Except for the dog breed classification task, we used a DenseNet121 as the LRP classifier, because it is a modern and very deep model (121 layers) with strong potential in the field of lung disease classification\cite{chexnet}. Moreover, the efficiency of Dense Neural Networks makes them good candidates for becoming ISNet classifiers. Even though the DenseNet121 has more than 100 layers, it only uses around 8M trainable parameters, much fewer than a VGG with 16 layers (138M parameters) and even fewer than a U-Net (about 31M parameters). Due to its definition, an ISNet can be created with any classification neural network that is analyzable by LRP and that has enough power to successfully accomplish the implicit foreground segmentation. To showcase this versatility, the dog breed classification task considers a VGG-19 classifier backbone instead of the DenseNet121. As LRP can be applied to virtually any DNN and already has implementations for most modern classifiers\cite{innvestigate} (even for recurrent networks), our approach has strong flexibility.



\subsection{ISNet Loss Function}
\label{loss}


The minimization of unwanted LRP relevance is fundamental for the ISNet, so we begin by explaining the loss function that penalizes it. Two terms comprise the ISNet loss, $L_{IS}$, a classification loss, $L_{C}$, which penalizes the classifier's classification outputs in relation to labels, and a term that quantifies the amount of unwanted attention present in the heatmaps, created by the LRP Block. We call this term 'heatmap loss' or $L_{LRP}$.

\begin{equation}
L_{IS}=(1-P).L_{C}+P.L_{LRP}, \mbox{ where } 0 \le P \le 1
\end{equation}

The hyper-parameter $P$ in the above equation balances the influence of the two losses in the gradient and parameters update. It must be valued between 0 and 1, with larger values increasing the strength of the heatmap loss. If P is too small, the network will not minimize $L_{LRP}$ effectively, and background attention will be high. Training datasets with stronger biases may require higher P values. However, if P is too high, training classification accuracy will decrease very slowly. To tune P, we advise a search through different values (e.g., 0.3, 0.5, and 0.7), as the ideal choice may change for different models and tasks. Nevertheless, fine adjustment of the parameter does not seem to strongly affect model performance. With adequate values of P, the two losses will converge.

$L_{C}$ is a standard loss function for classification. Here, we used cross-entropy or binary cross-entropy (for a single-label or multi-label task, respectively), averaged across the mini-batch samples. We will now explain the calculation of the second term, $L_{LRP}$. First, some definitions are needed: we denote as $\bm{H}$ a tensor containing the heatmaps for all classes, created for a single input image or for a mini-batch of B images (in this paper we use bold letters to indicate tensors). For a multi-class classification problem with K classes and input images of shape (C,Y,X), where C is the number of channels, Y the image height, and X its width, $\bm{H}$ assumes the following shape: (B,K,C,Y,X).The second dimension (class) indicates at which output neuron the relevance propagation began. So in the k-th heatmap, the positive relevance is associated with the class k. A single LRP heatmap (for image b and class k), $\bm{H}_{bk}$, has the same shape as an input image, (C,Y,X). The mini-batch and classes dimensions were merged inside the LRP block, but we separate them before loss calculation. $\bm{M}$ is a tensor containing the segmentation targets (masks) for each figure in the mini-batch. A mask separates the image foreground and background. It is defined as a single-channel image with shape (Y,X), valued 1 in the regions of interest and 0 in areas of undesired attention. Having B masks, one for each mini-batch image, we just repeat them in the channels and classes dimensions for $\bm{M}$ to match the shape of $\bm{H}$. We denote the mask for a single image as $\bm{M}_{bk}$. It is possible to use different masks for different classes in the image, or to repeat the same mask for all classes.

The heatmap loss is composed of two terms, the first is the heatmap background loss, $L_{1}$, and it penalizes background attention. Its calculation starts by the computing of the element-wise absolute value of the heatmaps, $abs(\bm{H}_{bk})$, because the training procedure must minimize both the positive and the negative undesired relevance. Afterwards, the absolute maps are normalized, as all their elements are divided by the mean value inside their region of interest (RoI). To compute this mean, we perform an element-wise product ($\odot$) between the absolute map and the image's segmentation target, $\bm{M}_{bk}$, sum the elements of the resulting tensor, $Sum(\cdot)$, and divide the summation result by the number of elements in the region of interest, $Sum(\bm{M}_{bk})$. In all divisions we add a small $e$ value ($10^{-10}$) to the denominator, enforcing the relationship $0/0=0$, and avoiding indeterminate results. We call the normalized absolute heatmaps $\bm{H'}_{bk}$. If an image region produced small relevances (less than 1) in its $\bm{H'}_{bk}$ maps, the attention paid to the area was necessarily smaller than the average attention paid to the region of interest. Notice that our normalization procedure does not shift the zero value in the heatmaps. Thus, minimizing relevance in $\bm{H'}_{bk}$ also minimizes it in the original LRP heatmaps.

\begin{equation}
\bm{H'}_{bk}=\frac{abs(\bm{H}_{bk})}{[Sum(abs(\bm{H}_{bk}) \odot \bm{M}_{bk})/(Sum(\bm{M}_{bk})+e)] +e}
\label{eqabs}
\end{equation}

$\bm{H'}$ is a tensor containing all normalized absolute heatmaps $\bm{H'}_{bk}$, with the same shape as $\bm{H}$. The next step in the $L_{1}$ loss calculation is zeroing the relevance inside the regions of interest in the $\bm{H'}_{bk}$ heatmaps. This is because background relevance minimization should not affect the classifier's ability to analyze these regions. With $\bm{1}$ being a tensor whose elements are 1, with the same shape as $\bm{H}$ and $\bm{M}$, we create the inverted masks tensor, $\bm{1}-\bm{M}$, whose values are 1 in the region of undesired attention and 0 in the important image areas. Thus, an element-wise multiplication of $\bm{H'}$ with the inverted masks generates a new tensor, $\bm{UH'}$, containing heatmaps that show only the undesired relevance:

\begin{equation}
\bm{UH'}=(\bm{1}-\bm{M}) \odot \bm{H'}=[UH'_{bkcyx}]
\end{equation}


Afterwards, we apply a Global Weighted Ranked Pooling\cite{GWRP} (GWRP) operation to $\bm{UH'}$. GWRP is a weighted arithmetic mean of $\bm{UH'}$ in the two spatial dimensions (y and x), outputting a tensor $\bm{R}=[r_{bkc}]$, which has the remaining three dimensions (batch, class, and channel). GWRP begins by ranking the elements in each (Y,X) matrix inside the tensor $\bm{UH'}$ in descending order, creating vectors $\bm{V_{bkc}}$, whose elements we will call $V_{i}^{bkc}$, $i=\{1,2,...,X.Y\}$. Naturally, we will have one vector for each batch element (b), class (k), and channel (c). Each vector is reduced to a scalar, which we will call $r_{bkc}$, by taking the weighted arithmetic mean of the its elements. The weights in the mean decrease exponentially, such that high relevance elements in the heatmap's background strongly increase $r_{bkc}$. A constant scalar, $d$, is the hyper-parameter controlling the rate of the exponential decay. Notice that, when $d=0$, GWRP is the same as max-pooling, and, when $d=1$, it is the same as average pooling. Training is more stable with larger d, we suggest testing values like 0.9, 0.996 and 1. The parameter has limited effect on accuracy; thus, it can be set to 1 to simplify hyper-parameter tuning. However, higher d values may require higher P values to avoid background attention. The purpose of using GWRP is to allow a strong penalization of minute high attention regions in the image background.

\begin{equation}
r_{bkc}=\frac{\sum_{i}d^{i}.V_{i}^{bkc}}{\sum_{i}d^{i}}, \mbox{ where } 0 \le d \le 1
\end{equation}

The scalars $r_{bkc}$ are real positive values, which measure of the background relevance in channel c of the heatmap for the mini-batch image b, starting the LRP relevance propagation at class k.

Cross-entropy is the most common error function for image segmentation. We want the classifier to distinguish the image foreground and background, to ignore the latter. Therefore, cross-entropy seemed a sensible choice for the $r_{bkc}$ penalization. As the cross-entropy's target for the undesired relevance, the natural choice is zero, which represents a model not considering background features during classification. Under this specific condition, the cross-entropy function, $CE(\cdot)$, for a scalar, $x$, can be expressed as:

\begin{equation}
CE(x)=-ln(1-x)
\end{equation}

Which evaluates to 0 when $x=0$, and to infinity when $x=1$. Consequently, we must choose a function, $f(\cdot)$, to map $r_{bkc}$ to the interval $[0,1[$ before applying cross-entropy. Some design requirements for $f(\cdot)$, considering positive arguments ($r_{bkc}\geq 0$), are: being monotonically increasing, differentiable, limited between 0 and 1, and to map 0 to 0. The sigmoid function, a common activation function for image segmentation, cannot be used, since it evaluates to 0.5 for a null input. After practical tests with some candidates, we decided to use the following function:

\begin{equation}
f(x)=\frac{x}{x+E}
\end{equation}

Where $E$ is a hyper-parameter controlling the function's slope and how fast it saturates to 1. We set this value to 1 in all experiments, as the ISNet does not appear to be overly sensitive to the parameter. Smaller values lead to a stronger penalization of background attention, and earlier saturation of the loss function, which may decelerate the beginning of the optimization process.

Now, we can calculate $CE(f(r_{bkc}))$, the cross-entropy between $f(r_{bkc})$ and 0, and average it across the channels (c), classes (k) and batch (b) dimensions, resulting in the scalar heatmap background loss, $L_{1}$. Considering B mini-batch images, K classes, and C image channels, we have:

\begin{gather}
CE(f(r_{bkc}))=-ln(1-\frac{r_{bkc}}{r_{bkc}+E})\\
L_{1}=\frac{1}{B.K.C}\sum_{b=1}^{B}\sum_{k=1}^{K}\sum_{c=1}^{C}CE(f(r_{bkc}))
\end{gather}

The loss function monotonically grows with the undesired relevance in a heatmap, $r_{bkc}$, presenting a minimum at 0, when there is an absence of focus on the image background. As $CE(f(r_{bkc}))$ diverges if $f(r_{bkc})=1$, we clamp $f(r_{bkc})$ between 0 and $1-10^{-7}$ for numerical stability.

Notice that $L_{1}$ evaluates to zero if all values in the LRP heatmaps are zero. Moreover, due to the heatmap normalization, the loss could be reduced with an unnatural growth of the heatmap ($\bm{H}_{bk}$) relevance inside the region of interest, instead of a reduction of the background relevance, which is our objective. To avoid these two undesired solutions to $L_{1}$, we included a second term in the heatmap loss, $L_{2}$. We called it heatmap foreground loss. Given an unnormalized heatmap $\bm{H}_{bk}$, $L_{2}^{bk}$ evaluates the absolute values of the elements inside the heatmap region of interest. If the sum of these values, $g(\bm{H}_{bk})$, falls within an expected range, $]C_{1},C_{2}[$, the loss and its gradient are both zero. However, the error function increases quadratically if the sum moves below $C_{1}$, or above $C_{2}$. We choose the constants $C_{1}$ and $C_{2}$ by analyzing a few heatmaps from a commonly trained classifier (not an ISNet), $\bm{H}_{bk}^{standard}$. $C_{1}$ is defined around the minimum observed value of $g(\bm{H}_{bk}^{standard})$, while $C_{2}$ is set around the maximum observed value for the function. The ISNet does not seem very sensible to the choice of these hyper-parameters. In all experiments we set $C_{1}=1$ and $C_{2}=25$, except for Appendix \ref{CheXpertClassification}, where we reduced $C_{1}$ to 0.1. For the VGG-19 backbone (dog breed classification task), we employed $C_{1}=0.05$ and $C_{2}=0.5$. The loss $L_{2}^{bk}$ is calculated per heatmap, and the scalar $L_{2}$ is its mean across all classes and mini-batch elements.

\begin{gather}
g(\bm{H}_{bk})=Sum(abs(\bm{H}_{bk}) \odot \bm{M}_{bk}) \\
\label{L2}
L_{2}^{bk}=
\begin{cases}
\frac{(C_{1}-g(\bm{H}_{bk}))^{2}}{C_{1}^{2}}, \mbox{ if } g(\bm{H}_{bk}) < C_{1} \\
0, \mbox{ if } C_{1} \le g(\bm{H}_{bk}) \le C_{2} \\
(g(\bm{H}_{bk})-C_{2})^{2}, \mbox{ if } g(\bm{H}_{bk}) > C_{2}
\end{cases} \\
L_{2}=\frac{1}{B.K}\sum_{b=1}^{B}\sum_{k=1}^{K}L_{2}^{bk}
\end{gather}

The division by $C_{1}^{2}$ in equation \ref{L2} ensures that zero valued heatmaps have high loss even when $C_{1}$ is small. Finally, we define the heatmap loss as a linear combination of the background and the foreground losses. We did not need to fine-tune the combination weights. We set $w_{1}=1$ and $w_{2}=3$ in all experiments, except for Appendix 1, where we set $w_{2}=1$. Our total loss function, $L_{IS}$, can be defined as:

\begin{equation}
L_{IS}=(1-P).L_{C}+P.L_{LRP}, \mbox{ where }L_{LRP}=w_{1}.L_{1}+w_{2}.L_{2}
\end{equation}

The main idea behind $L_{2}$ is to avoid unnatural (too low or too high) attention to the region of interest, but to make the heatmap loss ($L_{LRP}$) have minimal effect over the region when its LRP relevance is within an expected range. Thus, $L_{LRP}$ minimizes background focus, while the attention pattern inside the RoI is naturally guided by the classification loss ($L_{C}$) optimization.





\subsection{LRP Block Layers}
\label{layers}
%ok

Besides defining the LRP relevance propagation rules as neural network layers, we also need to modify their execution methodology to train an ISNet. For each layer in the classifier, a corresponding LRP layer is added to the LRP block to perform the relevance propagation through it. All LRP layers are based on an efficient LRP implementation in four steps. Considering the propagation of relevance through a classifier convolutional or dense layer L with ReLU activation, the implementation is defined as\cite{LRPBook}:

\begin{enumerate}
    \item Forward pass the layer L input, $\bm{x}_{L}$, through layer L, generating its output tensor $\bm{z}$ (without activation).
    \item For LRP-$\varepsilon$, modify each $\bm{z}$ tensor element ($z$) by adding $sign(z)\varepsilon$ to it. Defining the layer L output relevance as $\bm{R}_{L}$, perform its element-wise division by $\bm{z}$: $\bm{s}=\bm{R}_{L}/\bm{z}$.
    \item Backward pass: backpropagate the quantity $\bm{s}$ through the layer L, generating the tensor $\bm{c}$.
    \item Obtain the relevance at the input of layer L by performing an element-wise product between $\bm{c}$ and the layer input values, $\bm{x}_{L}$ (i.e., the output of layer L-1): $\bm{R}_{L-1}=\bm{x}_{L} \odot \bm{c}$.
\end{enumerate}

In the ISNet, we apply the following changes to the algorithm above:

In Step 1, we perform a forward pass through a copy of layer L, whose weights are shared with its clone (i.e., they have the same parameters). The input for the operation is layer L's original input, $\bm{x}_{L}$ (i.e., the output of layer L-1), captured by a skip connection between the LRP block and the classifier. Although the bias parameters are also shared, we do not directly optimize them for the $L_{LRP}$ minimization (in PyTorch we accomplish this with the detach function on the LRP layer shared bias). Knowing that the biases are responsible for relevance absorption during its propagation\cite{LRP}, this choice prevents the training process from increasing them to enforce an overall reduction of relevance. As in the original method, we do not use an activation function in this step.

Step 2 is unchanged. We note that LRP-0 can be unstable for ISNet training. Therefore, we opted to base the LRP layers we implemented on LRP-$\varepsilon$, having chosen $\varepsilon$ as $10^{-2}$. We do not employ LRP-$\gamma$, because it did not result in background focus minimization, due to the rule not treating positive and negative weights equally\cite{LRPBook}.

For Step 3, we propose a fundamental modification because using a backward pass during the forward propagation through the LRP block could cause conflicts in common deep learning libraries, complicating the subsequent backpropagation of the heatmap loss. Consequently, we substitute the backward pass in Step 3 with an equivalent operation: the forward propagation of $\bm{s}$ through a transposed version of layer L. If L is fully connected, its transposed counterpart is another linear layer, whose weights are the transpose of the original ones. Thus, Step 3 becomes: $\bm{c}=\bm{W}^{T} \cdot \bm{s}$. Similarly, convolutional layers have transposed convolutional layers as their counterpart, using the same padding, stride, and kernel size. As in Step 1, the transposed layer share weights with layer L, but this time it does not use bias parameters.

Step 4 is unchanged and, as in Step 1, $\bm{x}_{L}$ is carried by the skip connection. The obtained relevance value, $\bm{R}_{L-1}$, is forwarded to the next LRP block layer, which will perform the relevance propagation for the classifier layer L-1. 

Using italics to emphasize our changes to the original algorithm, we summarize the relevance propagation through the LRP layer corresponding to a convolutional or fully connected classifier layer L, which uses ReLU activation, as follows:
\begin{enumerate}
    \item Forward pass the layer L input, $\bm{x}_{L}$, through \textit{a copy of} layer L, generating its output tensor $\bm{z}$ (without activation). \textit{Use parameter sharing, use the detach() function on the biases. Get $\bm{x}_{L}$ via a skip connection with layer L.}
    \item For LRP-$\varepsilon$, modify each $\bm{z}$ tensor element ($z$) by adding $sign(z)\varepsilon$ to it. Defining the layer L output relevance as $\bm{R}_{L}$, perform its element-wise division by $\bm{z}$: $\bm{s}=\bm{R}_{L}/\bm{z}$.
    \textit{\item Forward pass the quantity $\bm{s}$ through a transposed version of the layer L (linear layer with transposed weights or transposed convolution), generating the tensor $\bm{c}$. Use parameter sharing, but set biases to 0.}
    \item Obtain the relevance at the input of layer L by performing an element-wise product between $\bm{c}$ and the layer input values, $\bm{x}_{L}$ (i.e., the output of layer L-1): $\bm{R}_{L-1}=\bm{x}_{L} \odot \bm{c}$.
\end{enumerate}

The above-defined convolutional and fully connected LRP layers are equivalent to the traditional LRP-0/LRP-$\varepsilon$ propagation rules. Therefore, the proposed execution methodology is not detrimental to the explanational ability of LRP. In the following subsections, we explain the LRP layers for other common operations in DNNs, namely, pooling layers and batch normalization. Finally, we explain the implementation of the LRP Z$^{B}$ rule for the first convolutional or fully connected layer in the classifier.

\subsubsection{LRP Layer for Pooling Layers}
\label{pool}
%ok

As linear operations, sum pooling and average pooling can be treated like convolutional layers, whose corresponding LRP layer was defined in section \ref{layers}. However, since pooling operations do not have trainable parameters, no weight sharing is needed. Furthermore, the $\bm{z}$ tensor in Step 1 can be explicitly defined as the pooling output (obtained via a skip connection). We represent convolutional kernels with a tensor $\bm{K}$ of shape (C1,C2,H,W), where C1 is the number of input channels, C2 of output ones, W the kernel width, and H its height. Then, for a convolution to be equivalent to pooling, C1 and C2 become the pooling number of channels, while W and H are defined by its kernel size. Naturally, the two layers use the same padding and stride parameters. To represent sum pooling, the convolutional kernel elements, $k_{c1,c2,h,w}$, are defined as constants:
 
\begin{equation}
k_{c1,c2,h,w}=
\begin{cases}
1,& \text{if }c1=c2\\
0,&  \text{otherwise}
\end{cases}
\label{weights}
\end{equation}

And considering average pooling with kernel size of (H,W), we have:

\begin{equation}
k_{c1,c2,h,w}=
\begin{cases}
\frac{1}{H.W},& \text{if }c1=c2\\
0,&  \text{otherwise}
\end{cases}
\end{equation}

For max pooling, the equivalent LRP layer adopts a winner-takes-all strategy, distributing all the relevance to the layer inputs that were chosen and propagated by the pooling operation. Therefore, to obtain the relevance at the input of the max pooling layer, we propagate its output relevance through an equivalent MaxUnpool layer. This operation, available in the PyTorch library, uses the indices of the maximal values propagated by max pooling to calculate its partial inverse, which sets all non-maximal inputs to zero. As in average/sum pooling, no parameter sharing is needed, and $\bm{z}$ in Step 1 can be directly obtained via a skip connection.

\subsubsection{LRP Layer for Batch Normalization}
\label{BN}
Batch normalization (BN) layers are linear operations that can be fused with an adjacent convolutional or linear layer during relevance propagation, creating a single equivalent convolutional/fully connected layer. Thus, we calculate the parameters of the equivalent layer and create its corresponding LRP layer with the methodology presented in section \ref{layers}.

One study\cite{BNLRP} presented equations to fuse batch normalization with an adjacent convolutional or linear layer. To analyze a convolution, followed by BN, and then by a ReLU activation function (a configuration used in DenseNets), we define: $\bm{K}$ of shape (C1,C2,H,W) as the convolution weights/kernels; $\bm{B}$ as the convolutional bias, of shape (C2); $\bm{\gamma}$ as batch normalization weights, of shape (C2); $\bm{\beta}$ as the BN bias, also of size (C2); $\bm{\mu}$ the per-channel (C2) mean of the batch-normalization inputs, and $\bm{\sigma}$ its standard deviation, defined as the square root of the input's variance plus a small value for numerical stability (a parameter of the BN layer). After replicating the tensors $\bm{\gamma}$ and $\bm{\sigma}$ in the C1, W, and H dimensions to match the shape of $\bm{K}$, we can define the equivalent convolutional layer weights, $\bm{K'}$, with element-wise divisions and multiplications:

\begin{equation}
\label{BN1}
\bm{K'}=\frac{\bm{K} \odot \bm{\gamma}}{\bm{\sigma}}
\end{equation}

While the equivalent convolution bias, $\bm{B'}$, is given by the following element-wise operations:

\begin{equation}
\label{BN2}
\bm{B'}=\bm{\beta}+\bm{\gamma} \odot \frac{\bm{B}-\bm{\mu}}{\bm{\sigma}}
\end{equation}

Dense neural networks also present BN layers between pooling and ReLU layers, thus an adjacent convolutional or linear layer is not available. Observing that the preceding pooling operation is also a linear operation (not followed by any activation function), we can fuse it with batch normalization. If we have average or sum pooling, we simply calculate its equivalent convolution, as explained in Section \ref{pool}, and perform the fusion according to equations \ref{BN1} and \ref{BN2}. 

In the case of max pooling, we start by imagining a convolutional layer performing identity mapping before the batch normalization. This imaginary layer has no bias or padding, its stride is unitary, and its weights, $\bm{K}$, have shape (C,C,1,1), where C is the max pooling number of channels. An element $k_{c1,c2,h,w}$ is then given by equation \ref{weights}. Thus, we can fuse the batch normalization with the identity convolution according to the equations \ref{BN1} and \ref{BN2}, creating a BN equivalent convolution. The LRP layer for relevance propagation through the sequence of MaxPool, BN, and ReLU is then defined by the 4-step procedure below:

\begin{enumerate}
    \item Forward pass the MaxPool output through the BN equivalent convolution, generating its output tensor $\bm{z}$ (without activation). Use parameter sharing, use the detach() function on the biases, and obtain the MaxPool output via a skip connection.
    \item For LRP-$\varepsilon$, modify each $\bm{z}$ tensor element ($z$) by adding $sign(z)\varepsilon$ to it. Defining the output relevance of the ReLU activation as $\bm{R}_{L}$, perform its element-wise division by $\bm{z}$: $\bm{s}=\bm{R}_{L}/\bm{z}$. 
    \item Forward pass the quantity $\bm{s}$ through a transposed version of the BN equivalent convolution (transposed convolution), generating a quantity $\bm{t}$. Employ parameter sharing, but set biases to 0. Then perform a MaxUnpool operation, using $\bm{t}$ as input, creating the tensor $\bm{c}$. 
    \item Obtain the relevance at the input of the MaxPooling layer by performing an element-wise product between $\bm{c}$ and the MaxPool input values, $\bm{x}_{L}$ (i.e., the output of layer L-1): $\bm{R}_{L-1}=\bm{x}_{L} \odot \bm{c}$.
\end{enumerate}

This methodology generates the same result as propagating the relevance first in the BN equivalent convolution and then in the max pooling LRP layer, but it has a lower computational cost.

\subsubsection{LRP Layer for Z$^{B}$ LRP Rule}
\label{lrpZb}
%ok

The LRP Z$^{B}$ rule\cite{LRPZb} considers the DNN input range in its formulation, and is used for the first layer in the neural network. For a convolutional or fully connected layer, it begins with a separation of the positive and negative elements of its weights, $\bm{W}$, and bias, $\bm{B}$. With $\bm{0}$ being a tensor of zeros:


\begin{gather}
\bm{W}^{+}=max(\bm{0},\bm{W})\\
\bm{B}^{+}=max(\bm{0},\bm{B})\\
\bm{W}^{-}=min(\bm{0},\bm{W})\\
\bm{B}^{-}=min(\bm{0},\bm{B})\\
\end{gather}

The parameters define three new layers: weights $l.\bm{W}^{+}$ and biases $l.\bm{B}^{+}$ define layer L$^{+}$, where $l$ is the lowest pixel value possible (in the common case where $l=0$, L$^{+}$ is ignored); weights $h.\bm{W}^{-}$ and biases $h.\bm{B}^{-}$ produce layer L$^{-}$, where h is the highest allowed input. The original parameters, $\bm{W}$ and $\bm{B}$ define a copy of the original layer L. The four-step procedure for convolutional/fully connected layers (defined in Section \ref{layers}) is then changed to:

\begin{enumerate}
    \item Forward pass the L layer input, $\bm{x}_{L}$, through a copy of layer L, creating $\bm{z}^{original}$. Forward an input of ones through layers L$^{+}$ and L$^{-}$, producing $\bm{z}^{+}$ and $\bm{z}^{-}$, respectively. Do not consider the non-linear activation. The classifier layer L shares parameters with the three instances. The bias values are not optimized to reduce the heatmap loss. Use a skip connection with layer L to obtain $\bm{x}_{L}$. Combine the three results in the following manner: $\bm{z}=\bm{z}^{original}-\bm{z}^{+}-\bm{z}^{-}$.
    \item Modify each $\bm{z}$ tensor element ($z$) by adding $sign(z)\varepsilon$ to it. Defining the layer L output relevance as $\bm{R}_{L}$, perform its element-wise division by $\bm{z}$: $\bm{s}=\bm{R}_{L}/\bm{z}$.
    \item Forward pass the quantity $\bm{s}$ through a transposed version of L, L$^{+}$ and L$^{-}$, creating the tensors $\bm{c}^{original}$, $\bm{c}^{+}$ and $\bm{c}^{-}$, respectively. Use parameter sharing, but set biases to 0.
    \item Being $\bm{x}_{L}$ the layer L input values, obtain the relevance at the input of layer L, $\bm{R}_{L-1}$, with: $\bm{R}_{L-1}=\bm{x}_{L} \odot \bm{c}^{original} -\bm{c}^{+}-\bm{c}^{-}$.
\end{enumerate}

If batch normalization follows the first classifier layer, placed before the ReLU nonlinear activation, the above procedure can implement the Z$^{B}$ rule for the equivalent convolutional/fully connected layer (the result of fusing the convolutional/linear layer with BN), explained in Section \ref{BN}. 

\subsubsection{Dropout}
%ok
The dropout operation can be defined as the random removal of layer input elements during the training procedure, and LRP block layers can automatically deal with it. For a layer L preceded by dropout, its inputs $\bm{x}_{L}$ shall have some zero values caused by the operation. According to the element-wise multiplication $\bm{R}_{L-1}=\bm{x}_{L} \odot \bm{c}$ (see Step 4 of the procedure in Section \ref{layers}), these null values will also make their respective relevances zero, thus being accounted for in the LRP propagation. 



\subsection{Explaining the Implicit Segmentation Precision}
%ok

Having defined the LRP layers of the LRP block, we can visualize the entire ISNet structure. For a common feed-forward classifier (without skip connections in its structure), a layer in the middle of the corresponding LRP block, propagating relevance through the classifier layer L, has 3 connections: one with the previous LRP layer, the second with the following LRP layer, and a skip connection with the classifier layer L. The first carries the relevance that can be seen at the layer L output, $\bm{R}_{L}$; the second brings the LRP layer result, $\bm{R}_{L-1}$ (equivalent to the relevance at the layer L input), to the next layer in the LRP block; and the last carries, from the classifier layer L, the information required for relevance propagation (e.g., L's input, $\bm{x}_{L}$), if it is convolutional, or output, $\bm{x}_{L+1}$, in the case of pooling. The Figure below exemplifies the ISNet architecture for a simple classifier. The LRP layer propagating relevance through layer $Li$ is named $LRPi$. In Figure \ref{ISNet}, $\bm{y'}$ is the classifier output $\bm{y}$ for one class, with all other outputs made zero. One map shall be created for each class, with each LRP propagation being executed in parallel, in a strategy analogous to mini-batch processing. Each layer in the figure presents a summarized description of the sequence of mathematical operations it comprises. For a detailed description of the LRP layers, please refer to Section \ref{layers}, which also explains the nomenclature in the figure, with a few exceptions: here we denoted the LRP-$\varepsilon$ stabilizer ($\varepsilon$) as "eps", element-wise multiplication was expressed as "*", and element-wise division as "/". The notation "Conv2D(\textbf{x};\textbf{W,B})" indicates a convolution with input \textbf{x}, weight matrix \textbf{W}, and bias vector \textbf{B}.

\begin{figure}[!h]
\includegraphics[width=0.5\textwidth]{ISNet2.png}
\centering
\caption{Representation of an ISNet. The classifier is on the left, and the equivalent LRP block on the right. Pooling layers and their LRP counterparts are in red, convolutional in green, and linear in yellow}
\label{ISNet}
\end{figure}

As expected, the ISNet structure has a contracting path, defined by the classifier, and an expanding path, the LRP block. We see that the LRP layers for pooling (in red) increase the size of their relevance inputs. Afterwards, an LRP correspondent for convolution (in green) reduces the number of channels in the relevance signal, and combines it with information from an earlier feature map containing higher resolution, which is brought from the classifier. As a consequence of the LRP rules, we naturally have a structure that combines context information from later classifier feature maps with high resolution from the earlier maps, which suffered less down-sampling by pooling operations. Thus, skip connections between an expanding and a contracting path allow the combination of information from later and earlier feature maps. Unlike common DNNs, our expanding path does not have independent weights, it shares all of its parameters with the contracting path. However, the idea of using skip connections to link a contracting path with an expanding one (combining high resolution and context information) is behind state-of-the art architectures used for image segmentation and object detection, such as the Fully Convolutional Networks for Semantic Segmentation\cite{SegmentationCNN}, U-Net\cite{unet}, Filter Pyramid Networks\cite{FPN}, and the YOLOv3\cite{yolov3}. In our work, instead of being used for explicit semantic segmentation\cite{SegmentationCNN}\textsuperscript{,}\cite{unet}, this concept allows the ISNet to precisely controls its attention, containing the LRP relevance in the image foreground. Thus, the model implicitly and precisely finds the image's region of interest, and it does not pay attention to areas outside of it.


\subsection{Challenges and Implementation Notes}
\label{ImpNotes}

Fundamentally, the ISNet architecture restrains the classifier during training, conditioning it to analyze only the relevant part of the image. This is the case even when there are undesired background features that could allow the model to reduce the classification loss rapidly and easily. Therefore, any change in the ISNet architecture needs to be cautiously implemented, because it can create a new and unintended way for the classier to minimize the heatmap loss. Namely, as a flexible model, the classifier will look for the easiest way to reduce $L_{LRP}$, which may be by ``cheating'' and finding a strategy to ``hide'' the undesired relevance in the heatmaps.

The ISNet performance is significantly affected by the definition of the heatmap loss. New LRP-based loss functions, tailored for specific applications, are a promising path for future research. However, some functions may prompt the aforementioned cheating solutions. For example, we may create a simple heatmap loss formulation, penalizing a ratio between the undesired relevance and the relevance inside the region of interest. If there is no mechanism to avoid such solution, the classifier may minimize $L_{LRP}$ by artificially boosting the relevance inside the region of interest. We tested this alternative heatmap loss, which was still based on the absolute value of the heatmaps (to minimize positive and negative undesired relevance). To optimize the function, the DNN created a fine and regular chessboard pattern inside the zones of interest, alternating between strong positive and strong negative relevance. The artificial pattern allows the region of interest to equally affect each class in the classifier, making it meaningless for the classification task. However, the high absolute values in the pattern strongly reduce the ratio in the alternative heatmap loss, allowing its minimization while the classifier focuses on the image background with a comparatively small quantity of absolute relevance. 

Regarding implementation, we need to be careful with in-place operations inside the classifier, since they may change values that will later be processed by the LRP block. Moreover, we explicitly chose to use deterministic operations during training in PyTorch. This was to ensure better correspondence between the LRP block and the classifier (which can be accomplished with a single command, use\_deterministic\_algorithms(True)). Finally, using PyTorch's forward hooks, we can easily store all relevant variables during the forward propagation through the classifier, allowing their later access by the LRP block. This is an effortless way to create the skip connections between the classifier and the block, even if the classifier is already defined and instantiated. To improve training stability, gradient clipping may be utilized. Clipping the gradient norm to 1, we were able to train with mixed precision, a configuration that reduces stability, but increases training speed. Unstable training can easily be identified, due to high training losses; in this case, decreasing learning rate and increasing the d hyper-parameter in the heatmap loss can help convergence.

The reliability of the standard segmentation-classification pipeline depends on the proper function of its segmenter. Naturally, if the segmenter removes parts of the image foreground, the classification scores will be compromised. Moreover, the improper removal of the background also affects the classifier performance. E.g, our experiments demonstrate that, if we skip the segmentation step from the pipeline after training, classification performance drops dramatically. Analogously to the traditional segmentation-classification approach, the ISNet relies on the proper operation of its implicit segmentation. I.e., to accurately classify an image, the ISNet must be able to identify its foreground.

The ISNet capacity to focus on the foreground is a consequence of the proper minimization of the heatmap loss. Our experiments show that, for an adequately optimized model, this capacity is highly generalizable and prompts correct classifier focus, even in out-of-distribution data, hindering shortcut learning. However, adequate optimization must be evaluated after training. Therefore, we emphasize that researchers should always: verify the convergence of all training losses, confirm if attention is concentrated at the images' foreground in the trained model's LRP heatmaps, and perform out-of-distribution evaluation. If the model fails at such tests, the researcher should retrain the network from another initialization point, consider changing hyper-parameters (especially P and d, Section \ref{loss}), and evaluate if the DNN and dataset sizes are compatible with the classification and segmentation tasks at hand.



\subsection{DenseNet based ISNet and Classifier Skip Connections}
%ok

The Densely Connected Convolutional Network (DenseNet) is characterized by the dense block, a structure where each layer receives, as input, the concatenation of the feature maps produced by every previous layer in the block\cite{DenseNet} (i.e., the block presents skip connections between each layer and all preceding ones). For LRP relevance propagation, all the connections must be considered. Therefore, a mirror image, inside the ISNet LRP block, of a dense block with S skip connections will also have S internal skip connections, now propagating relevance. Naturally, we also have skip connections between the classifier and the LRP block, which carry information from layer L (e.g., its input, $\bm{x}_{L}$) to the LRP layer that performs its relevance propagation. In the case of a DenseNet, $\bm{x}_{L}$ is no longer defined as the output of classifier layer L-1, but rather as the concatenated outputs of all layers preceding L in its dense block.

To understand the relevance skip connections, imagine that, in the classifier, a layer L propagates its output to layers L+i, with $i \in \{1,...,N\}$. We define $\bm{R}_{Inp(L+i),L}$ as the relevance at the input of layer L+i, considering only its input elements (or channels) connected to L. We define relevance at the input of layer L+i, considering all input channels, as $\bm{R}_{Inp(L+i)}$. $\bm{R}_{Inp(L+i)}$ (and $\bm{R}_{Inp(L+i),L}$) can be obtained with relevance propagation through L+i. Then, to further propagate the relevance through layer L, we set the relevance at L's output as $\bm{R}_{L}$, given by equation \ref{sumRel}. Proceeding with the propagation rules explained in section \ref{layers}, we can obtain $\bm{R}_{Inp(L)}$, the relevance at the input of layer L.

\begin{equation}
\bm{R}_{L}=\sum_{i=1}^{N}\bm{R}_{Inp(L+i),L}
\label{sumRel}
\end{equation}

In a DenseNet, a layer L inside one of its dense blocks is defined as a nonlinear mapping $H_{L}(\cdot)$, which can in turn comprise a standard feed-forward sequence of other layers, e.g., ReLU activation, convolution, and batch normalization. Since there is an absence of skip connections inside one layer L, we propagate relevance through the sequence in the standard manner. Equation \ref{sumRel} is useful if $H_{L}(\cdot)$ is defined as sequences in the form $C_{L}(convolution) \rightarrow BN_{L} \rightarrow ReLU_{L}$, $C_{L} \rightarrow ReLU_{L}$, $C_{L} \rightarrow Pool_{L} \rightarrow ReLU_{L}$, or combinations of the previous sequences. Figure \ref{SimpleDense} presents a simple example, considering a sequence of 4 convolutional layers in the classifier, L0, L1, L2, and L3, each defined as $C \rightarrow ReLU$ and receiving the outputs of all previous layers. The flow of relevance is observable in the LRP block. Note that different connections carry different channels of the $\bm{R}_{Inp(L+i)}$ tensor, $\bm{R}_{Inp(L+i),L}$. In the Figure, we define the input of layer Li as $\bm{x}_{i}$, which is a concatenation (in the channels dimension) of the previous layers' outputs, $\bm{y}_{j}$.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{DenseSimple.png}
\centering
\caption{Classifier and LRP block for 4 convolutional layers with skip connections in the classifier}
\label{SimpleDense}
\end{figure}

The most common proposal for $H_{L}(\cdot)$ in DenseNets is: $BN^{1}_{L} \rightarrow ReLU^{1}_{L} \rightarrow C^{1}_{L} \rightarrow BN^{2}_{L} \rightarrow ReLU^{2}_{L} \rightarrow C^{2}_{L}$. In Section \ref{layers}, we defined LRP layers to propagate relevance in sequences ending with ReLU activation. As such, they cannot be directly applied to $H_{L}(\cdot)$, and nor can we rely on equation \ref{sumRel}. Consider a dense block layer L, followed by N other layers, L+i ($i \in \{1,...,N\}$), which receive L's output. $\bm{R}_{L+i,L}^{ReLU1}$ is the relevance at the output of the first ReLU inside layer L+i ($ReLU^{1}_{L+i}$), considering only channels that came from layer L. Layer L+i starts by processing layer L's output through the sequence $BN^{1}_{L+i} \rightarrow ReLU^{1}_{L+i}$, producing a tensor that we shall call $\bm{y}_{L}^{L+i}$. We can define $\bm{y}_{L}$ as the concatenation, in the channels dimension, of the N $\bm{y}_{L}^{L+i}$ feature maps, one for each L+i layer. With these definitions, the below procedure calculates the relevance at the output of layer L's first ReLU activation, $\bm{R}_{L}^{ReLU1}$, based on the N relevances $\bm{R}_{L+i,L}^{ReLU1}$:

\begin{enumerate}

\item Fuse layer L's second convolution, $C^{2}_{L}$, with the first batch normalization operation in each of the N layers L+i, $BN^{1}_{L+i}$, obtaining equivalent convolutional kernels and biases according to equations \ref{BN1} and \ref{BN2}, respectively. Concatenate these parameters in their output channel dimension, creating the kernels and biases that define a single equivalent convolutional layer, which generates $\bm{y}_{L}$ from layer L's second ReLU output. Use it to recreate $\bm{y}_{L}$, but detach the bias parameters.

\item Concatenate the N relevances $\bm{R}_{L+i,L}^{ReLU1}$ (for each L+i layer) in the channel dimension, producing $\bm{R}_{L}^{conc}$. This tensor can be seen as the relevance at the output of the equivalent convolution created in step 1.

\item Propagate the relevance $\bm{R}_{L}^{conc}$ through equivalent layer from step 1, using the usual four-step procedure for convolutional layers (section \ref{layers}). The operation returns $\bm{R}_{L}^{ReLU2}$, the relevance at the output of the second ReLU activation in layer L.

\item Obtain $\bm{R}_{L}^{ReLU1}$ by propagating $\bm{R}_{L}^{ReLU2}$ through layer L's sequence of operations $C^{1}_{L} \rightarrow BN^{2}_{L} \rightarrow ReLU^{2}_{L}$, using the procedure in Section \ref{BN}. $\bm{R}_{L}^{ReLU1}$ can be propagated through earlier layers in the dense block by repeating these four steps.
\end{enumerate}

A DenseNet transition layer L can be formed by the following sequence: $BN^{1}_{L} \rightarrow ReLU^{1}_{L} \rightarrow C^{1}_{L} \rightarrow ReLU^{2}_{L} \rightarrow AvgPool_{L}$. $ReLU^{2}_{L}$ is not part of its original configuration, but we added it to simplify the relevance propagation, as our rules are defined for layers with ReLU activation. It did not seem to have a detrimental effect on the model. The transition layer sits between 2 dense blocks. It receives all outputs from the layers in the first block, B, and propagates its own result to every layer of the next one, B+1. Therefore, a layer in block B naturally considers the transition layer among its consecutive N layers during Step 1 of the above procedure; the same is true for the $BN \rightarrow ReLU$ sequence following the last dense block in a DenseNet (every layer in the last block shall consider it in Step 1). The relevance propagation through the transition layer L must take into account all its skip connections with the block B+1, so we use a 4-step procedure similar to the one above. There are only two changes. Since L ends in an average pooling instead of a convolution, we modify the fusion process in Step 1 to merge pooling and BN, a technique explained in Section \ref{BN}.  Furthermore, in Step 4, we obtain $\bm{R}_{L}^{ReLU1}$ after propagating $\bm{R}_{L}^{ReLU2}$ through the transition layer $C^{1}_{L}\rightarrow ReLU^{2}_{L}$ sequence, using the rules defined in Section \ref{layers}. We can treat the max pooling layer in the beginning of the DenseNet similarly, considering its skip connections with the first dense block. The code to automatically generate an ISNet from a DenseNet, in PyTorch, is available at https://github.com/PedroRASB/ISNet. 

In this study, we use an ISNet created with a DenseNet121 classifier. Our main reason for this choice is the DenseNet's small number of trainable parameters in relation to its depth (fewer than 8M parameters and 121 layers). Moreover, the architecture was successfully employed for detecting COVID-19 and other lung diseases using chest X-rays\cite{bassiCovid}\textsuperscript{,}\cite{chexnet}. We utilized the same DNN for facial attribute estimation, as the DenseNet is known for its robust performance when analyzing natural images\cite{DenseNet}. We modified the original DenseNet121 classifier by substituting its last layer with a layer containing the same number of neurons as the number of classes in the application, preceded by dropout of 50\%. In the dogs classification task we employed a VGG-19 backbone. The only change to the classifier was in its last layer, using 3 neurons to classify the 3 possible dog breeds.


\subsection{Alternative Models: U-Net Followed by Classifier, Standalone Classifier, Multi-task U-Net, Attention Gated Sononet, Vision Transformer, Guided Attention Inference Network, and Right for the Right Reasons}
\label{BaselineImplementations}

%talk about vgg
%talk about new baselines

In this study, we compare the ISNet with a more traditional methodology to segment and classify images: a U-Net (segmenter) followed by a classifier. The classification model is the same one used in the ISNet: VGG-19 in dog breed classification, and DenseNet121 in all other Tasks. The U-Net uses its original architecture proposal, which can be seen in Figure 1 of\cite{unet}. We trained the segmenter beforehand, using the same datasets that would be later used for classification, and employing the segmentation masks as targets. The same targets were used in the ISNet training. Analyzing the U-Net validation performance, we found the best threshold to binarize its outputs. For COVID-19 detection the optimal value was 0.4 when trained without synthetic bias, and 0.3 with it; for facial attribute estimation, 0.5 without synthetic bias, and 0.4 with it; for Stanford Dogs we used a threshold of 0.3.

The U-Net architecture\cite{unet} and its variants are a common model choice for biomedical image segmentation, including lung segmentation\cite{bassi2021covid19}\textsuperscript{,}\cite{covidSegmentation}\textsuperscript{,}\cite{TuberculosisUNet}. It was created with this type of task in mind and so is designed to obtain strong performances even in small datasets. The U-Net is a fully convolutional deep neural network with two parts. First, a contracting path, created with convolutions and pooling layers, captures the image's context information. Then, an expanding path uses transposed convolutions to enlarge the feature maps and perform precise localization. Skip connections between the paths allow the expanding path access to earlier high-resolution feature maps (before down-sampling), which carry accurate positional information. Its combination with the context information in the later feature maps results in the U-Net's ability to conduct precise image segmentation.

As is commonly done, the U-Net parameters are kept frozen when training the pipeline's classifier. The sequential model can be summarized as follows:

\begin{enumerate}
\item The U-Net processes the X-ray and segments the lung region.
\item We threshold the U-Net result and create a binary mask, where lungs are represented as 1 and the remaining image parts as 0. 
\item We use an element-wise multiplication between the X-ray and the mask to erase the image background.
\item The DenseNet121/VGG-19 classifies the segmented image.
\end{enumerate}

We also trained the classifier without segmentation, as a benchmark. We refer to it as standard classifier, it consists in a VGG-19 for Stanford Dogs, and a DenseNet121 in the remaining tasks. As a second baseline, we created a multi-task model. It consists of a U-Net with an additional classification head, connected to the U-Net bottleneck feature representation (i.e., the end of its contracting path). The model is trained with multi-task learning for the tasks of classification and semantic segmentation. Past studies have shown that such models are useful when analyzing images with cluttered backgrounds, because one task benefits the other, allowing the DNN to achieve better results on both\cite{MultiTask1}\textsuperscript{,}\cite{MultiTask2}. As the U-Net contracting path is based on the VGG architecture\cite{vggOriginal}, we employed a classification head that is similar to the VGG last layers. It comprises an adaptive average pooling layer, with output size of 7x7 and 1024 channels, leading to a sequence of 3 dense layers. The first two have 4096 neurons and ReLU activation. The output layer has one neuron accounting for each possible class, and it uses a softmax or sigmoid activation function (in multi-class or multi-label problems, respectively). To minimize overfitting, the first two dense layers are followed by dropouts of 50\%. Moreover, to accelerate and stabilize the training procedure we added batch normalization after each convolutional layer in the U-Net, before their ReLU activations. We experimented with different classification heads (e.g., two convolutional layers followed by global average pooling and a dense output layer), but we found no significant advantage in relation to the chosen structure. We trained the multi-task U-Net to perform segmentation and classification simultaneously, by minimizing a linear combination of a classification loss and a semantic segmentation loss (both being cross-entropy functions). The first error function compares the classification head outputs with classification labels. Meanwhile, the second compares the U-Net expanding path outputs to segmentation masks. Therefore, the trained multi-task U-Net produces classification scores with its classification head, and segmentation outputs with the expanding path.

Additionally, we trained an attention gated Sononet\cite{AGNet}. Attention gates are a visual attention mechanism designed to make CNNs automatically focus on important image regions and ignore irrelevant features. The model authors proposed them as an alternative for the segmentation-classification pipeline that is commonly used with medical images (which we also use as a baseline in this study). They present minimal computational overhead, and were effective for the classification of fetal ultrasound screenings, which are noisy and hard to interpret images \cite{AGNet}. In summary, an attention gate multiplies (element-wise) a convolutional layer feature map with attention coefficients. They aim to reinforce layer activations inside regions of interest, and attenuate activations in irrelevant areas. The attention gate computes the coefficients based on the gated layer feature map, and on a feature map from a later convolutional layer (e.g., the last). This map  contains more context information, but is coarse, limiting the mechanism's resolution. The model does not learn from or use segmentation masks. In this study we implemented the AG-Sononet, an attention gated CNN based on the VGG-16 architecture, which also bases the U-Net in the alternative segmentation-classification pipeline and the multi-task U-Net. We utilize the model PyTorch implementation developed by the method's authors. Its detailed description can be found in their paper\cite{AGNet}.

The second implemented network with an attention mechanism that does not consider semantic segmentation targets is the Vision Transformer\cite{VisionTransformer}. The model is based on the transformer, a structure that is highly successful in natural language processing. In summary, the transformer implements a self-attention mechanism, which relates diverse positions in its input sequence, producing a new representation of the sequence\cite{transformer}. Initially, the sequence elements are based on linear projections of patches in the input image, which are processed by a series of transformer encoders. Unlike all other tested models, the vision transformer is not a convolutional neural network. Specifically, we employed the ViT-B/16 architecture\cite{VisionTransformer}, which has 12 layers, breaks the input image into 16x16 patches, and contains 86M parameters. The specific model was chosen because its parameter count scale more closely matches with the other neural networks in this study. Its implementation is the standard PyTorch vision transformer model.

Four benchmark DNNs in this study control classifier attention by optimizing its explanations, with the aid of segmentation targets: GAIN, HAM, RRR and the ISNet Grad*Input ablation. They are important to empirically demonstrate the advantages of LRP optimization (ISNet). In the models, we employ the same classifier backbones as in the ISNet, i.e., VGG-19 for dog breed classification, and DenseNet121 for the remaining tasks. GAIN combines a standard classification loss with the attention mining loss and the external supervision loss\cite{GAIN}. Attention mining should make GAIN pay attention to all image regions correlated with the existing classes. Basically, GAIN uses GradCAM to locate the classifier attention for a given class (present in the image), removes the high attention areas from the figure (masking), and classifies it again. The class prediction score the DNN gives for the masked image constitutes the attention mining loss. When the image presents multiple ground-truth classes, multiple masked images are created (running GradCAM for the different classes) and classified. In this case, the attention mining loss is the arithmetic mean of the prediction scores for all present classes. If GradCAM attention covered all image regions related to the present classes, the DNN prediction scores for the masked images would be low, resulting in low attention mining loss. Thus, the minimization of the loss function extends the attention in GradCAM. The external supervision loss is the mean squared loss between the GradCAM heatmap for a present class (interpolated to the DNN input size) and the ground-truth segmentation target for the class. When multiple classes are present in one image, the losses for the diverse GradCAMs and targets are averaged.

Our GAIN implementation was based on a PyTorch version of the network\cite{GAINTorch}, which we modified to include the external supervision loss, and to also deal with multi-class single-label problems. Our modifications followed the guidelines in the paper presenting GAIN\cite{GAIN}. GradCAMs considered the DenseNet's feature map just before global average pooling, matching the original GAIN paper's choice of using the output of the classifier's last convolutional layer for GradCAM. Accordingly, in the VGG-19 GradCAM utilizes the last convolutional layer's output. This is the standard choice for producing GradCAM explanations in DenseNets and VGG. Classification loss was cross-entropy for the single-label problems, and binary cross-entropy for multi-label tasks. To linearly combine the three losses, we used the same weights employed in the original GAIN paper\cite{GAIN}, i.e., 1 for the classification loss, 1 for attention mining, and 10 for the external supervision. In preliminary tests, we experimented with other coefficients (1, 10, 100 and 1000), but they did not significantly improve results and generalization.

The network from the study titled "Right for the Right Reasons"\cite{RRR}, which we will refer to as RRR, optimizes input gradients to avoid a classifier's attention to its input background. Input gradients (gradients of the DNN outputs, with respect to its input), or saliency maps, are a simple and efficient explanation technique. In RRR, the authors use a special loss function term, dubbed right reasons, to minimize the input gradient elements in the background. In essence, the loss masks out the gradient foreground elements, and follows by applying a square loss, with targets zero. Utilizing a multilayer perceptron with two hidden layers as classifier, RRR was very successful in avoiding background attention and shortcut learning produced by background bias\cite{RRR}. However, we expect LRP optimization to surpass the optimization of input gradients when considering deep classifiers (Section \ref{mathBackground}).

The element-wise multiplication of input gradients and the input itself is said to be an improvement over simple input gradients, increasing sharpness\cite{GradInput}\textsuperscript{,}\cite{LRPvsGrad}. These heatmaps are known as Gradient*Input, and we use them for an ISNet ablation study. The ISNet Grad*Input is an ISNet, where the LRP heatmaps were substituted by Gradient*Input. Therefore, the ISNet Grad*Input uses the ISNet loss function, which is much more restrictive than the RRR loss, thus changing the optimization dynamics. In summary, RRR's right reasons loss can be reduced by an overall reduction in the magnitude of all the heatmap's elements. Meanwhile, due to its relative nature, the ISNet loss is only small when the background attention is small in relation to the foreground attention. Moreover, the loss foreground component avoids an overall change in the heatmap scale (Section \ref{loss}). The ISNet Grad*Input represents a controlled experiment, where only the optimized explanation is changed. It should indicate if the optimization of LRP is advantageous over that of Gradient*Input. For an in-depth comparison of the explanation methodologies, and the justification for the experiments with LRP, GradCAM, input gradients, and Gradient*Input, please refer to Section \ref{mathBackground}.

As the last state-of-the-art baseline DNN we utilized Hierarchical Attention Mining networks\cite{HAM} (HAM). The DNN is evaluated and explained in Appendix \ref{CheXpertClassification}, which regards CheXPert classification.


\subsection{ISNet Fundamentals}
\label{mathBackground}

Layer-wise relevance propagation sits among the most robust, interpretable, and high-resolution explanations to date\cite{LRPRobustness}\textsuperscript{,}\cite{LRPVsAttention}. Furthermore, it possesses two characteristics that are key for explanation optimization: it is a computationally efficient explanation methodology, as the production of a heatmap takes only a backpropagation of relevance through the neural network, and its procedures are differentiable. These two characteristics are essential for explanations that must be created during training and optimized by a loss function. However, we chose to employ LRP especially due to its mathematical fundamentals, which allow the ISNet optimization procedure to effectively minimize the influence of background bias on the classifier logits.

In this section, we justify the empirically verified (refer to the Results Section) ISNet capacity of avoiding background attention, and the shortcut learning caused by background bias. We begin by summarizing the mathematical fundamentals behind LRP, elucidating its relationship with Deep Taylor Decomposition. Afterwards, we discuss how the optimization of LRP controls the classifier response to background bias. Moreover, throughout the section we will justify the advantages of LRP optimization over the optimization of alternative explanation techniques, such as GradCAM\cite{GradCAM}, input gradients\cite{saliency}, and Gradient*Input\cite{GradInput}.

\subsubsection{LRP Mathematical Fundamentals}

A deep neural network implements a function of its input, $y=f(\bm{X})$, where y is the DNN logit. A first-order Taylor expansion can decompose the logit into a summation of terms, each one corresponding to one of the input elements ($x_{j}$). We want these terms to explain the differential contribution of the input elements with respect to a state of maximal uncertainty, when the logit is zero (50\% class probability in classification)\cite{LRP}. I.e., we explain how the input elements make the logit differ from zero. Therefore, the Taylor expansion reference point, $\bm{\tilde{X}}$, should be a root of the function $y=f(\bm{X})$. Moreover, to minimize the approximation error with relation to higher-order Taylor expansions (Taylor residuum, $\rho$), $\bm{\tilde{X}}$ should be close to the data point $\bm{X}$. Finally, for the explanations to be more meaningful, the reference point should ideally reside in the classification problem data manifold\cite{LRP}. As the reference point is a root, the zero-order Taylor term is zero, $f(\bm{\tilde{X}})=0$, and the logit is approximated by a summation of one term per input variable. Accordingly, these terms can form a heatmap, explaining the network output, y. The first-order Taylor expansion, considering a nearby root, becomes:

\begin{equation}
\label{taylorRelevance}
    f(\bm{X}) = f(\bm{\tilde{X}}) + \sum_{j}(x_{j}-\tilde{x}_{j})\at{\frac{\partial f}{\partial x_{j}}}{\bm{\tilde{X}}} + \rho \approx \sum_{j}(x_{j}-\tilde{x}_{j})\at{\frac{\partial f}{\partial x_{j}}}{\bm{\tilde{X}}}
\end{equation}

Although principled, adequate Taylor explanations are difficult to compute for deep neural networks. The function $f(\bm{X})$ is complicated and highly non-linear. Thus, finding a root $\bm{\tilde{X}}$ that satisfies all aforementioned requirements is a complex and analytically intractable problem\cite{LRPZb}. However, a deep neural network function, $f(\bm{X})$, is defined as a structure of simpler sub-functions, learned at each layer. Finding a Taylor reference for the simpler functions is easier, and approximate analytical solutions can be found for the Taylor expansion\cite{LRPZb}.

The LRP relevance propagation rules we employ in the LRP Block (LRP-$\varepsilon$ and LRP-z$^{B}$) are justified by the Deep Taylor Decomposition (DTD) framework \cite{LRPZb}\textsuperscript{,}\cite{LRPBook}. In a deep neural network, DTD propagates relevance, from the classifier logit to its inputs, according to local Taylor expansions, performed at each neuron. The neuron output relevance ($R_{k}$, received from the subsequent neurons) is viewed as a function of the neuron's inputs, $\bm{a}=[a_{j}]$. Then, a first-order Taylor expansion of $R_{k}(\bm{a})$ is performed, considering a reference point $\bm{\tilde{a}}$:

\begin{equation}
    R_{k}(\bm{a}) \approx R_{k}(\bm{\tilde{a}}) + \sum_{j}(a_{j}-\tilde{a}_{j})\at{\frac{\partial R_{k}}{\partial a_{j}}}{\bm{\tilde{a}}}
\end{equation}

The relevance is redistributed to the neuron inputs $a_{j}$ according to the first-order terms in the expansion (summed terms in the equation above)\cite{LRPBook}. Ideally, we would like to use a reference point ($\bm{\tilde{a}}$) that minimizes the zero-order term $R_{k}(\bm{\tilde{a}})$, and that is close to the data point $\bm{a}$, as the proximity reduces the Taylor residuum. Finding such reference point is still not simple or computationally inexpensive, considering the complexity of the function $R_{k}(\bm{a})$\cite{LRP}. Therefore, in order to obtain a closed-form solution for the Taylor expansion, LRP considers an approximation of $R_{k}(\bm{a})$ (dubbed approximate relevance model, $\hat{R}_{k}(\bm{a})$), and fixed choices of the reference point $\bm{\tilde{a}}$. These choices are justified by the approximate relevance model and the neuron's input domain\cite{LRPBook}. For a neuron with ReLU activation (Equation \ref{neuron}), a modulated ReLU activation is the common choice for $\hat{R}_{k}(\bm{a})$. It is defined in Equation \ref{relevanceModel}, where $c_{k}$ is a constant chosen to force the relevance model to match the true relevance at the data point $\bm{a}$, i.e., $R_{k}(\bm{a})=\hat{R}_{k}(\bm{a})$. For further explanation and theoretical justification of the relevance model, please refer to \cite{LRPBook}.

\begin{equation}
\label{neuron}
a_{k}=max(0,z_{k})=max(0,b_{k}+\sum_{j} w_{jk}a_{j})
\end{equation}

\begin{equation}
\label{relevanceModel}
\hat{R}_{k}(\bm{a})=max(0,\sum_{j} w_{jk}a_{j})c_{k}
\end{equation}

The LRP-$\varepsilon$ and LRP-0 rules are derived by selecting different reference points ($\bm{\tilde{a}}$) for the Taylor expansion\cite{LRPBook}. All these reference points satisfy $\tilde{a}_{j} \geq 0$, thus residing in the neuron's input domain (considering that it follows other neurons with ReLU activations). For LRP-0, we have $\bm{\tilde{a}}=\bm{0}$. For LRP-$\varepsilon$ the definition is: $\bm{\tilde{a}}=\frac{\varepsilon}{a_{k}+\varepsilon}\bm{a}$. It must be noted that the Euclidean distance between the LRP-$\varepsilon$ reference point and the actual data point, $\bm{a}$, is significantly smaller than the distance between $\bm{a}$ and the LRP-0 reference point\cite{LRPBook}. Therefore, in comparison to LRP-0, LRP-$\varepsilon$ reduces the Taylor residuum in relation to higher order Taylor expansions, providing more faithful and contextualized explanations\cite{LRPBook}.

\subsubsection{Optimizing for Background Bias Resistance: Why LRP?}

The standard segmentation-classification pipeline creates background bias resistance by having an auxiliary neural network removing the image background before feeding the figure to the classifier. The approach is simple and effective, considering a segmenter that is successful in its task. Empirically, we have found the alternative pipeline to be effective in avoiding shortcut learning induced by background bias (Table \ref{synth}). Yet, the utilization of two deep neural networks is a computationally expensive choice, mostly considering that segmenters are generally larger than the classifiers (please refer to Section \ref{speed}). Thus, our objective is to optimize a standalone classifier to not consider background information in its decisions.

Under this paradigm, one possible optimization strategy consists in minimizing the gradients of the DNN outputs with respect to the input variables that represent the background. This is the strategy adopted in the Right for the Right Reasons (RRR) neural network training procedure\cite{RRR}. The "Right Reasons" loss\cite{RRR} can be seen as a soft constraint added to the standard classification loss (e.g., cross-entropy), composing a new cost function to be minimized. The Right Reasons loss minimizes the background region in the input gradient. Thus, the classifier is optimized to implement a function ($f(\bm{X})$) that is locally invariant to changes in the DNN input background. A standard DNN training procedure locally optimizes $f(\bm{X})$ to match classification labels. Under adequate choices of dataset, DNN model and training strategy, the trained neural networks can generalize, being accurate not only on the training data. Accordingly, the study introducing RRR presented promising results, which indicate that the trained models can generalize, and classify the test data according to its foreground features, while ignoring background bias\cite{RRR}.

However, input gradients of large deep neural networks processing high-dimensional data are known to be noisy\cite{LRPvsGrad}. Accordingly, such explanations, also called saliency maps\cite{saliency}, are usually reserved for explaining the outputs shallower models\cite{LRPvsGrad}. Indeed, RRR was originally tested in networks with two hidden layers\cite{RRR}. In this study we evaluate the RRR's strategy utilizing much larger models, namely the VGG-19 and DenseNet121 architectures, with 19 and 121 layers, respectively. In this challenging scenario, our results indicate that the RRR could not efficiently minimize background bias and retain high classification accuracy (Table \ref{synth}). Therefore, we search for an explanation methodology that can improve resistance to background bias and maintain high accuracy when training large deep neural networks.

There are some key qualities we expect the optimized explanation strategy to have: it must be differentiable, consider both positive and negative evidence (refer to Section \ref{ImpNotes}), and be computationally efficient. Moreover, we search for an explanation methodology that can fundamentally justify why its optimization leads to resistance to background bias. This goal is achieved by using input gradients in RRR, as its optimization enforces the local invariance of the network output with relation to background changes. Another choice of explanation could be GradCAM\cite{GradCAM}, possibly the most popular DNN explanation technique in computer vision. However, the optimization of GradCAM has a weaker theoretical base: essentially, it represents constraining the last DNN convolution to produce zero activations in the regions that align with the network input's background. The desired solution for the optimization problem is a classifier that disconsiders the input background features. This solution assumes a natural and direct mapping, where activations in late feature maps align with the input features that caused such activations. Yet, due to the large receptive field in late DNN layers, the network can minimize late layers' background activations (and the background elements in GradCAM), while mapping information from the input background to late feature map regions that align with the input foreground. We named such undesired solution spurious mapping, and empirically demonstrated that it commonly occurs for models that optimize GradCAM (GAIN and HAM). Please refer to Section \ref{GAINComparison} for an in-depth analysis. Although we could not effectively minimize background attention with RRR, the model's explanations clearly displayed its background bias attention. Thus, unlike GAIN, the RRR classifier did not learn to produce deceiving explanations, indicating that input gradient optimization has a stronger theoretical fundamental.

Another possible explanation choice is Gradient*Input, i.e., the element-wise multiplication of the input gradient and the input itself. The explanation strategy was proposed to improve heatmap sharpness, in relation to saliency maps\cite{GradInput}. Essentially, as the optimization procedure only affects the gradient, not the input, optimizing Gradient*Input also minimizes the input gradients' background elements, like RRR. Therefore, Gradient*Input optimization shares the same mathematical fundamentals as input gradient optimization. Naturally, the technique is also differentiable, fast to compute and considers both positive and negative relevance. We utilized an ISNet ablation model, called ISNet Grad*Input, to experiment with the optimization of this explanation technique. It simply substitutes the LRP heatmaps by Gradient*Input explanations in the ISNet heatmap loss (Section \ref{loss}). The model performed well with a VGG-19 classifier backbone, retaining high accuracy, and providing more bias resistance than all tested neural networks, except for the original ISNet and the segmentation-classification pipeline (Table \ref{synth}). However, we were not able to make the model effectively converge with the deeper DenseNet121 backbone (Table \ref{synth}). This result indicates that Gradient*Input is still too noisy, hindering the stable convergence of the heatmap loss in deeper networks.

Finally, we justify our choice to optimize to optimize Layer-wise Relevance Propagation heatmaps; or, more specifically, LRP-$\varepsilon$\cite{LRP} maps. Empirically, past studies quantitatively demonstrate the technique to be robust and consistent, surpassing Gradient*Input\cite{LRPRobustness}. As we show in the LRP block, the strategy is differentiable. Second, its computational efficiency is similar to the calculation of input gradients (Section \ref{speed}). Third, LRP-$\varepsilon$ considers both positive and negative relevance. Finally, the technique is principled, because it approximates the sequential application of local Taylor expansions (per-neuron) in deep neural networks with ReLU activations. Interestingly, we had no success in preliminary experiments with the optimization of LRP rules that do not consider negative evidence (e.g., LRP-$\gamma$\cite{LRPBook}), or that are not justified by the deep Taylor framework (e.g., LRP-$\alpha \beta$\cite{LRPBook}): such optimization strategies could not produce background bias resistance.

By minimizing terms of a first-order Taylor expansion (Equation \ref{taylorRelevance}), we are minimizing the influence of the associated function inputs on the output variation, when the input moves from the Taylor reference point to the actual data point. As previously mentioned, to explain a classifier decision the ideal reference point is a function root near the data point. In this case, the minimization of terms associated with bias would minimize the bias influence over the function output variation, when its input moves from a point of maximal uncertainty (zero output) to the current data point. The creation of Taylor explanations of a neural network, considering an adequate choice of root, is a complex and analytically intractable problem. Thus, such explanations violate our requirement of computational efficiency. However, LRP-$\varepsilon$ is a fast procedure, which explains a DNN output by approximating a sequence of local Taylor expansions. For this reason, LRP-$\varepsilon$ optimization is an efficient and justifiable alternative to minimize the influence of biased input elements on the network's logits. We selected the LRP-$\varepsilon$ rule to propagate relevance through all DNN, except for its first layer. The input domain of the first DNN layer is diverse. In a DNN with ReLU activations and analyzing images, the first layer inputs range from 0 to 1 (standardized pixels), while other layers' inputs assume values in $\mathbf{R}^{+}$. While LRP-$\varepsilon$ is adequate for the remaining layers, the LRP-$z^{B}$ rule represents a more accurate choice of Taylor reference for the first layer\cite{LRPZb}. According to preliminary tests, the ISNet works with the LRP-$\varepsilon$ in layer one, but LRP-$z^{B}$ produced an accuracy improvement.

Meanwhile, LRP-0 explanations are particularly noisy and less interpretable than LRP-$\varepsilon$. In the LRP-$\varepsilon$ propagation rule (Equation \ref{LRPeEquation}), the $\varepsilon$ term not only avoids division by zero, but it absorbs some of the relevance that would have been propagated to the lower layer. The absorption reduces the influence of neurons with small activations ($z_{k}$ in Equation \ref{LRPeEquation}) over the relevance signal being propagated. Therefore, it reduces noise and contradiction across the multiple relevance values\cite{LRPBook}. As previously noticed, in relation to LRP-0, LRP-$\varepsilon$ represents a deep Taylor decomposition using reference points that are closer to the data points. Thus, the Taylor residuum is reduced, and the explanations become more contextualized and coherent with the network behavior\cite{LRPBook}.

After having explained the advantages of LRP-$\varepsilon$ over LRP-0, we must note an important fact: in deep neural networks utilizing ReLU activations (such as most state-of-the-art CNN architectures in computer vision), LRP-0 is equivalent to Gradient*Input explanations. This result assumes that the LRP-0 relevance propagation does not encounter a division by zero (or numerical instabilities). The equality demonstrates that the advantages of LRP-$\varepsilon$ over LRP-0 directly apply when comparing LRP-$\varepsilon$ and Gradient*Input. Therefore, such advantages are a theoretical justification of the ISNet superior empirical results over the ISNet Grad*Input and RRR, as seen in Table \ref{synth}. The ISNet represents, in essence, LRP-$\varepsilon$ optimization, while the ISNet ablation (ISNet Grad*Input) optimizes Gradient*Input, and RRR considers input gradients, which, from an optimization standpoint, are similar to optimizing Gradient*Input.

The equivalence of LRP-0 and Gradient*Input in deep ReLU networks was already discovered by past studies\cite{LRPvsGrad}. Here we point out a notation inconsistency. Some works mention an equivalence between LRP-$\varepsilon$ and Gradient*Input, but they demonstrate such equivalence by setting $\varepsilon=0$. Thus, the equality is being proved for LRP-0, not LRP-$\varepsilon$. LRP-0 can be seen as the extreme case of LRP-$\varepsilon$ (with $\varepsilon$ set as its smallest possible value, 0), and LRP-0 can be approximated with increasingly smaller values of $\varepsilon$. However, calling LRP-0 as LRP-$\varepsilon$ is confusing, especially considering the importance of the $\varepsilon$ constant, it is not a term that can be ignored. As previously explained, heatmaps produced by LRP-$\varepsilon$ and LRP-0 have different interpretations and approximation errors under the DTD point of view, distinct levels of noise, and diverse contextualization and coherence. Such differences are also clear when comparing the results of optimizing LRP-$\varepsilon$ (ISNet) and optimizing Gradient*Input (ISNet Grad*Input), as seen in Table \ref{synth}.

We finalize by referencing Appendix \ref{lrpVsGradInput}, where we formally demonstrate the equivalence between LRP-0 and Gradient*Input in ReLU neural networks. Afterwards, the appendix provides a novel view of LRP-$\varepsilon$: we define it as a modified gradient backpropagation procedure, followed by an element-wise multiplication of the back-propagated quantity and the neural network input. This view clarifies the effect of the $\varepsilon$ hyper-parameter over the signal being back-propagated, making the LRP-$\varepsilon$'s denoising quality explicit: the higher the $\varepsilon$, the more attenuated the impact of small neuron activations over the back-propagated signal. If $\varepsilon=0$, the modified procedure matches standard gradient backpropagation, and the final heatmaps match LRP-0 or, equivalently, Gradient*Input. In summary, we show that the hyper-parameter $\varepsilon$ controls a special denoising process, justified by the framework of deep Taylor decomposition, and important for the stable convergence of the ISNet optimization.



\subsection{Datasets}
\label{dataset}
%ok
\subsubsection{COVID-19 Chest X-ray}
%ok r1
\label{COVIDdataset}

In this study we employed the Brixia COVID-19 X-ray dataset\cite{BrixiaSet} as the source of the training and hold-out validation COVID-19 positive samples. It is one of the largest open databases regarding the disease, providing 4644 frontal X-rays showing visible symptoms of COVID-19 (i.e., samples with an assigned COVID-19 severity score, the Brixia Score, higher than 0). All images were collected from the same hospital, ASST Spedali Civili di Brescia, Brescia, Italy. They correspond to all triage and patient monitoring (in sub-intensive and intensive care units) samples collected between March 4th and April 4th 2020, thus reflecting the variability found in a real clinical scenario\cite{BrixiaSet}. The database includes AP (87\%) and PA projections, with CR (62\%) and DX modalities, and the images were obtained from the hospital's RIS-PACs system. Data is available in the DICOM format, and system manufacturers are Carestream and Siemens. All samples are accompanied by a COVID-19 severity score, the Brixia Score. We excluded the 59 images with 0 overall Brixia Score, assuming that they do not present radiological signs of COVID-19 that can be detected. The scores were assigned by the radiologist on shift, who is part of a team composed by about 50 radiologists. They operate in diverse radiology units of the hospital, have different expertise in chest imaging and a wide range of years of experience\cite{BrixiaSet}. Accordingly, our dataset's labeling accuracy reflects the radiologist on shift's. The patients (2315) mean age is 62.8 years, with standard deviation of 14.8 years. They are 64.4\% male. We randomly assigned 75\% of the samples (3483 images) for training, and the remaining for hold-out validation. The two subdivisions were not allowed to have images from the same patients.

The images of healthy and non-COVID-19 pneumonia patients in our training and hold-out validation datasets come from the CheXPert database\cite{irvin2019chexpert}. It is a large collection of chest X-rays (224,316), showing various lung diseases, assembled in the Stanford University Hospital, California, United States of America. Samples correspond to studies performed between October 2002 and July 2017 (before the COVID-19 pandemic, ensuring that there are no COVID-19 sample in the database). The dataset considers both inpatient and outpatient centers. The database's classification labels were created with natural language processing (NLP), utilizing radiological reports, and have an estimated accuracy surpassing 90\%\cite{irvin2019chexpert}. Considering class balance, we randomly gathered 4644 images of healthy patients (corresponding to 4644 different patients), and 4644 of pneumonia patients (from 3899 different patients). CheXPert uncertainty labels were treated as negative. We only considered AP and PA X-rays (excluding lateral views). 64.9\% and 74.3\% of normal and pneumonia X-rays are AP, respectively. We downloaded the images in the jpg format. Samples were also randomly divided with 25\% for hold-out validation and 75\% for training, employing a patient split. Pneumonia patients have a mean age of 62.3 years, with a standard deviation of 18.7 years. They are 57.1\% male. Healthy patients have a mean age of 51.7 years, with standard deviation of 18.2, and are 56.3\% male.

To avoid the effect of mixed dataset bias in our reported results, and to better understand how segmentation-based attention mechanisms improve the DNNs' generalization capability, an external test database (with dissimilar sources in relation to the training dataset) was assigned. Thus, we can test our DNNs in an o.o.d. dataset and evaluate the extent of shortcut learning. Regarding the COVID-19 class, we selected the dataset BIMCV COVID-19+ (iterations 1+2)\cite{BimcvSet} for evaluation. The database is also among the largest open sources of COVID-19 positive X-rays. The data was gathered from health departments in the Valencian healthcare system, Spain, considering multiple hospitals, mostly in the provinces of Alicante and Castellón\cite{BimcvSet}. Therefore, it is highly unlikely that this database shares patients with the Brixia dataset\cite{BrixiaSet}. Samples were acquired by querying the Laboratory Information System records from the Health Information Systems in the Comunitat Valenciana. They correspond to all consecutive imaging studies from patients with at least one positive PCR, IgM, IgG or IgA test for COVID-19, registered between February 16th and June 8th, 2020. We observe that the BIMCV COVID-19+ data acquisition time period includes the Brixia dataset's. Moreover, both databases are representative of the first pandemic wave in Europe, not including more recent COVID-19 variants. The data contains DX and CR X-ray modalities, and we selected only AP or PA views, excluding lateral X-rays. Among the chosen images, 53.2\% were CR, and 58.5\% AP. The dataset authors used natural image processing to label the X-rays, according to their radiological reports. The NLP model reported F1-Scores are above 0.9\cite{BimcvSet}. The model could output 3 COVID-19 related labels, which indicate the disease probability according to the radiological report: COVID-19, COVID-19 uncertain or COVID-19 negative. We obtained 1515 images, corresponding to all COVID-19 positive frontal X-rays, labeled as COVID-19, which had associated DICOM metadata. Therefore, COVID-19 samples in our test dataset were classified as a highly likely COVID-19 case by a radiologist, and correspond to a patient with at least one positive PCR or immunological test in the BIMCV COVID-19+ data acquisition time period. We chose to include only samples that were labeled as 'COVID-19' by the NLP model to minimize the number of false positive COVID-19 samples in our test dataset, and not rely solely on PCR or immunological tests. Moreover, our training dataset also only includes samples where radiologists found COVID-19 symptoms. The X-rays were produced with a wide variety of devices, from different manufactures (see Table 5 in the manuscript presenting the dataset\cite{BimcvSet}). The selected test COVID-19 patients (1145) have a mean age of 66 years, with standard deviation of 15.3 years, and they are 59.6\% male. The patients have 6516 reported COVID-19 tests, which are 62\% PCR, 16.5\% IgG, 11.1\% IgM and 10.4\% IgA. 56.6\% of the tests are positive (all subjects have at least one positive test). Considering the positive tests, 51.9\%, 23.4\%, 15\% and 9.8\% are PCR, IgG, IgA and IgM, respectively.

We also performed cross-dataset testing for the pneumonia and normal classes. We chose the ChestX-ray14 database\cite{chex14} as the source of test pneumonia X-rays. Like CheXPert, it is a large dataset, containing 108,948 frontal X-rays, which can display various diseases. The images were mined from the PCS system in the National Institutes of Health Clinical Center, Bethesda, United States of America. They correspond to studies between 1992 and 2015 (prior to the COVID-19 pandemic). The ChestX-ray14 dataset authors\cite{chex14} labeled their database with natural language processing, according to radiological reports (with an estimated accuracy over 90\%). Data is presented in the png format. We utilized 1295 pneumonia images, corresponding to 941 patients, over 18 years old. They present a mean age of 48 years, with standard deviation of 15.5 years, and are 58.7\% male. 

Normal test images were extracted from a database assembled in Montgomery County, Maryland, USA (80 images), and Shenzhen, China (326 images)\cite{ChineseDataset1}, published in 2014. The Montgomery images were obtained from the Montgomery County’s Tuberculosis screening program, in collaboration with the local Department of Health and Human Services. X-rays present the AP view, and were created with an Eureka stationary X-ray machine (CR). Meanwhile, the Shenzen X-rays were gathered in Shenzhen No.3 People’s Hospital, Guangdong Medical College (Shenzhen, China). The facility employed a Philips DR Digital Diagnost system, and the X-rays also present the AP view. They were collected during one month, as part of the hospital routine, mainly in September 2012. All images from the Montgomery and Shenzen dataset were acquired in the png format, and the database does not present multiple images per patient. The healthy patients have a mean age of 36.1 years (and standard deviation of 12.3 years) and are 61.9\% male. Because the images in this database were manually labeled by radiologists, we preferred to extract the test normal images from the Montgomery and Shenzhen database, and not from ChestX-ray14. Having an unbalanced test database, we will evaluate our DNNs with macro-average performance metrics.

The Brixia COVID-19 dataset presented only 11 images of patients younger than 20 years old, and the BIMCV COVID-19+ database has a single one. Thus, to avoid bias, patients under 18 were not included in the other classes, which had much higher proportions of pediatric patients in their original datasets. After removing pediatric patients, we observe that the three classes in the training dataset have similar demographics, reducing possible related biases. The utilization of jpg images in the ChexPert database may introduce compression artifacts. However, any related bias should not artificially boost evaluation results, since our test database does not contain jpg figures. Both employed COVID-19 databases were projected to be large and representative of the disease's first wave in Italy or Spain. Moreover, the BIMCV COVID-19+ demographic's is similar to the Brixia dataset's, and it is coherent with demographics of COVID-19+ in Spain at the time of data collection\cite{BimcvSet}. Our database has limitations that we must list. First, it does not include newer COVID-19 variants, as all samples were collected prior to July 2020. Furthermore, radiologists detected COVID-19 signs in all considered COVID-19+ X-rays. This characteristic facilitates DNN training and avoids false positives, but the database may underrepresent COVID-19 cases with very mild or hard to detect radiological manifestations. Moreover, the COVID-19 databases reflect a hospital environment, underrepresenting asymptomatic patients that did not seek medical attention. However, these limitations do not compromise our study's main goal regarding the COVID-19 detection task: to analyze if the ISNet can reduce shortcut learning and improve generalization by ignoring background bias.

The assembled database was chosen to resemble the characteristics of the most popular COVID-19 datasets: the use of dissimilar sources for different classes. Dataset mixing is required to work with some of the largest COVID-19 databases, and is also necessary in other classification problems, where a database does not contain all classes needed for the study. This characteristic favors dataset bias, and increases the possibility of background features correlating with classes. Therefore, our mixed COVID-19 dataset is an ideal scenario to test the ISNet ability to focus only in the region of interest (lungs), improving generalization and hindering shortcut learning.

Both the ISNet and the alternative segmentation-classification pipeline require segmentation masks for training. The ISNet utilizes them to calculate the heatmap loss during the training procedure, while the alternative models need them to train the segmenter. Thus, we utilized a U-Net, trained for lung segmentation in a previous study\cite{bassi2021covid19}, to create the segmentation targets, which we stored. We applied a threshold of 0.4 on the U-Net output to produce binary masks, which are valued 1 in the lung regions and 0 everywhere else; the chosen threshold maximized the segmenter's validation performance (intersection over union or IoU). The previous study authors\cite{bassi2021covid19} trained the U-Net using 1263 chest X-rays representing four classes: COVID-19 (327 images), healthy (327 images), pneumonia (327 images), and tuberculosis (282 images). They stated that the model achieved an IoU of 0.864 with the ground truth lung masks during testing. Please refer to\cite{bassi2021covid19} for a detailed explanation of the U-Net training procedure. We trained the ISNet and all alternative baselines using the COVID-19 detection datasets described in this section. When they were needed, all the models received the same segmentation targets, to produce a fairer comparison. For this reason, we do not use the U-Net from the previous study\cite{bassi2021covid19} in the alternative segmentation-classification pipeline. Instead, we trained another U-Net, using the same dataset that we employ for classification training. I.e., the segmentation targets that train the U-Net inside the alternative pipeline were created by the U-Net from\cite{bassi2021covid19}. It is probable that directly using the model from the previous study\cite{bassi2021covid19} in the alternative pipeline could boost its results, but in this case the alternative model would have been trained with different segmentation targets in relation to the ISNet. Moreover, these targets could be superior to the ones in our dataset, since some of them were manually created\cite{bassi2021covid19}.

Finally, to create an extreme test for the ISNet's attention mechanism, we added synthetic background bias to the described dataset: all COVID-19 images were marked with a triangle in their upper left corner, the normal class was marked with a square, and pneumonia with a circle. See figure \ref{triangle} for an example. Our data augmentation procedure can sometimes remove the shapes from the training samples (with rotations and translations), which may attenuate the background bias effect. However, the same is true for many natural background features (e.g., text and markers close to the X-ray borders), and the effect of the artificial bias should still be very solid. A common classifier, analyzing unsegmented images, can easily learn to identify the shapes, and to use them for improving classification accuracy. We evaluate the neural networks trained on the biased dataset in three different scenarios: with test images containing the same geometric shapes, with the original test images, and with deceiving bias. Deceiving bias means altering the correspondence between the classes and geometrical shapes during testing, which has catastrophic effects on neural networks that focus on the background bias. Specifically, in this scenario we add the circle to the normal images, the triangle to pneumonia, and the square to COVID-19. The strong and controllable synthetic bias, and the three testing scenarios, allow us to better assess the neural networks' resistance to background attention. If the ISNet performs equally in all test scenarios, we can conclude that the proposed attention mechanism is adequate to avoid shortcut learning caused by background bias. The artificial biased was applied to all training and validation samples, simulating a worst-case scenario.

\subsubsection{Tuberculosis Detection}

The NIAID TB Portals\cite{TBPortals} is a multi-national program for multi-domain tuberculosis data sharing. Lead by the National Institute of Allergy and Infectious Diseases (NIAID, National Institutes of Health, USA), the program offers open-access and curated radiological, genomic, clinical, laboratory and geographic data from prospective and retrospective TB cases\cite{TBPortals}. Data was acquired with international collaborations with multiple clinical research sites and academic research organizations in countries that are highly affected by drug-resistant TB. With over 7400 TB patient cases (January 2022 published version), it provides one of the largest databases of tuberculosis positive chest X-rays. Using the program's API and cohort creation system we downloaded every X-ray from patients diagnosed with tuberculosis of lung, confirmed by sputum microscopy with or without culture (diagnosis code A15.0, 3620 cases), confirmed by culture only (A15.1, 1427 cases) or confirmed histologically (A15.2, 46 cases). TB Portals mostly focus on drug-resistant-tuberculosis. In the downloaded data, 29.1\% present MDR non XDR resistance, 21.3 are drug-sensitive, 9.5\% XDR resistant, 4.64\% Mono DR, 1.87\% Poly DR and 1.47\% Pre-XDR. Regarding outcomes, 38.4\% were cured, 6.55\% died, 6.54\% completed, 5.32\% lost to follow up, 4.16\% are still on treatment, 1.33\% in palliative care, 0.68\% unknown and 0.01\% not reported. Data were collected in the following countries: Georgia (30.5\%), Ukraine (25.5\%), Belarus (14.9\%), Moldova (14.7\%), Romania (5.7\%), Kazakhstan (3.8\%), Azerbaijan (3.2\%), Nigeria (1.3\%) and India (0.3\%). The imaging data is available in the DICOM format, and represents both computed radiography and digitalized film X-rays (using flatbed scanners or digital cameras). After downloading the data, we manually reviewed the images, correcting files with wrong color-schemes (inverted DICOM MONOCHROME1 or 2 flag), and removing subpar samples (lateral views, figures with multiple X-rays in a single image or pictures with a small X-ray a large blank spaces). The dataset presented only 50 patients under 18. Thus, we removed underage patients from all databases (the normal class had a much higher percentage of children). We ended up with 6728 images, corresponding to 4227 patients, who are 74\% male, and have a mean age of 43 years with standard deviation of 13.8 years. We randomly divided them in 3 datasets (performing patient split): training (70\%), validation (20\%) and i.i.d. testing (10\%).

As the Brixia Dataset for COVID-19 detection, the NIAID TB Portals do not present control cases (TB-negative). To the best of our knowledge, there is no open-source TB dataset with equal size or larger than TB Portals presenting control and TB-positive cases collected from the same sources. Accordingly, as in COVID-19 detection, researchers need to employ dataset mixing to use TB Portals for DNN training, or resort to smaller databases. Accordingly, we randomly gathered 6728 normal X-rays from the CheXPert database (explained in Section \ref{COVIDdataset}), and used them as the control cases. They are 56.9\% male, with mean age of 51.9 years, and standard deviation of 18.1 years. They do not include children. We also randomly divided the normal X-rays into three datasets (employing patient split), with 70\% for training, 20\% for validation and 10\% for i.i.d. testing.

We chose the Montgomery and Shenzen database\cite{ChineseDataset1} (described in Section \ref{COVIDdataset}) as the o.o.d. test database. It is the most popular dataset choice for TB detection\cite{TBReview}. The database is much smaller than the training one (394 TB cases, 800 samples in total), but it presents normal and tuberculosis X-rays, collected from the same sources. From the TB images, 336 came from Shenzen, and 58 from Montgomery. Images were manually classified by the dataset authors, according to the X-rays associated clinical readings\cite{ChineseDataset1}. After removing pediatric patients, the o.o.d. test database ended up with 370 normal X-rays and 383 TB-positive cases. Healthy patients are 61.9\% male, with mean age of 36.1 years and standard deviation of 12.3 years. Meanwhile, TB patients are 69.2\% male, and have a mean age of 40 years, with standard deviation of 15.8 years. The o.o.d. dataset X-rays were acquired in Montgomery County, USA, and Shenzen, China, locations that did not contribute to our training database. Therefore, the probability of both datasets sharing cases is minimal. Comparing evaluation results in the i.i.d. and o.o.d. test datasets shall allow us to analyze if the problem of tuberculosis detection using mixed databases also fosters shortcut learning. Moreover, we will be able to evaluate if ignoring the background with the ISNet improves generalization, boosting performance on the o.o.d. test database.

To produce segmentation masks for our training and validation datasets we employed the same method that we used for COVID-19 detection: we applied the U-Net created in a previous study\cite{bassi2021covid19}. Furthermore, most images in the o.o.d. test dataset (663) have manually created segmentation targets \cite{ShenMasks}\textsuperscript{,}\cite{ChineseDataset1}, which we can use as a second gold standard to test our U-Nets' segmentation performance. Again, this model was not used inside the alternative segmentation-classification pipeline, whose segmenter we trained from scratch, employing the same tuberculosis database used for classification. We observe that the two classes in our training dataset have similar demographics. We must note that most of the X-rays were captured in a clinical setting. Thus, severe cases are more represented. Moreover, since our trained dataset focuses on drug-resistant TB, its percentage of drug-resistant TB cases is higher than the one found among new TB patients\cite{TBData}.

A limitation of our database is not containing sick patients without tuberculosis. We were able to find only one database that contained the classes healthy, TB-positive and sick but TB-negative. Their authors claim that it better simulates a clinical scenario, reducing the number of false positives in a real-world situation\cite{TBX11K}. However, not having a second source of sick but not TB cases, we are not able to create an o.o.d. test database with the three classes. We did not get sick but not TB samples from a general chest X-ray database (e.g., CheXPert\cite{irvin2019chexpert} or ChestX-ray14\cite{chex14}), because they do not have the TB class. However, they may contain tuberculosis cases and they do present TB manifestations among their classes (e.g., infiltration, pneumonia, atelectasis and pleural effusion\cite{TBSigns}). External evaluation is essential to assess shortcut learning. Although possibly reducing the usefulness of our model in a clinical setting, with two classes we will be able to achieve our major objective: to evaluate if the task promotes background bias, and to measure the ISNet effectiveness in hindering shortcut learning and improving generalization. Moreover, classifying normal and TB-positive makes our work in line with the vast majority of tuberculosis detection papers and datasets\cite{TBNat}\textsuperscript{,}\cite{TBReview}\textsuperscript{,}\cite{TBRev2}.

\subsubsection{Facial Attribute Estimation}

For the task of facial attribute estimation we employed images from the Large-scale CelebFaces Attributes Dataset, or CelebA\cite{celebA}. The database has 10000 identities, each one presenting 20 images. Binary labels were created by a professional labeling company, according to 40 facial attributes\cite{celebA}. As the database comprises in the wild images, the displayed faces show a wide variety of positions and sizes, and background clutter is present.

The CelebAMask-HQ dataset\cite{CelebAMaskHQ} presents a subset of 30000 high-quality images from CelebA, cropped and aligned. Furthermore, these images have manually-created segmentation masks, which indicate the face regions\cite{CelebAMaskHQ}. For this study, we selected the CelebA images that originated CelebAMask-HQ. Afterwards, we created their segmentation masks by applying translation, rotation and resizing to the CelebAMask-HQ masks; with this operation we reverted the CelebAMask-HQ crop and align procedure (described in\cite{CelebAHQ}, appendix C). We employ the dataset split suggested by the CelebAMask-HQ dataset authors\cite{CelebAMaskHQ}, which assigns 24183 samples for training, 2993 for hold-out validation and 2824 for testing. The proposed splits are subsets of the official CelebA training, validation and test datasets.

We chose to work with unaligned images that have strong background clutter, and whose face segmentation masks have a wide variety of shapes and positions. We think this setting constitutes a challenging test of the ISNet implicit segmentation capability, i.e., its ability of identifying and paying attention only to the faces. Moreover, the CelebA dataset authors\cite{celebA} show that their DNN naturally focuses more on the persons' faces when more facial attributes are classified. For this reason, we believe that the ISNet implicit segmentation task will be more difficult when it classifies a small number of attributes. Thus, we chose to work with 3 classes, to better assess the ISNet attention mechanism potential, and to better visualize the architectures' benefits. If we choose attributes that also have features outside of the face (e.g., gender), the ISNet architecture will change the natural classification strategy for one that ignores features outside of the face. This effect can be desirable or not, depending on the researcher's objective. Here we opted to classify three attributes that are exclusively present in the face: rosy cheeks, high cheekbones and smiling. The first two are considered identifying attributes, i.e., they can be used for user identification. Therefore, avoiding bias in their classification is a security concern.

As in COVID-19 detection, we also created an artificially biased dataset for facial attribute estimation. The second application constitutes a multi-label problem. Thus, we marked our images with a square in the lower-right corner, a circle in the lower-left, and a triangle in the upper-left, to indicate rosy cheeks, high cheekbones and smiling, respectively. The presence of the geometrical shape is correlated with the presence of the attribute. As before, the bias was added to all training and hold-out validation data. Refer to figure \ref{triangle} for an example of an image with three positive labels. For the deceiving bias evaluation, we invert the correlation of attributes and bias: now, the geometrical shape will appear when the attribute is not present. For example, the upper-left corner triangle will be present when the person is not smiling. This configuration simulates the worst-case scenario for deceiving bias in a multi-label classification problem. When a classifier is considering the geometrical shapes in its decisions, this configuration will reduce its accuracy the most.

\subsubsection{Dog Breed Classification}

Dog breed classification is the other natural RGB image classification task in this study. We utilized a subset of the Stanford Dogs Dataset\cite{StanfordDogs}. The original dataset is composed of 20580 images, displaying 120 different dog breeds. Its images and annotations were originally extracted from the ImageNet database\cite{imagenet}, excluding images smaller than 200x200\cite{StanfordDogs} and manually evaluating the samples. There are about 150 images per class. The dataset represents a fine-grained classification problem and is challenging due to a few aspects: there is little inter-class variation; there is large intra-class variation; dogs are presented in different poses, occlusion/self-occlusion, ages, and color; and the pictures present large background variation, including man-made environments\cite{StanfordDogs}.

The ISNet training time increases linearly with the number of classes in the application. Therefore, considering all 120 Stanford Dogs categories would require excessively long training time. Moreover, it would make the reproduction of our results more expensive and time consuming, especially when powerful computational resources are not available. Therefore, we work with a subset of Stanford Dogs, considering all images for three randomly selected classes: Pekingese, Tibetan Mastiff, and Pug.

The subset has 100 training images per class, which we randomly split, separating 20 images per class for hold-out validation. The test dataset has the images from the original Stanford Dogs test split: 49 Pekingese samples, 52 Tibetan Mastiff and 100 Pug. Our objective is to evaluate the ISNet capacity of focusing on images' foregrounds (e.g., the dogs). With only 240 training samples, this dataset will display the model's ability of ignoring the background when trained with small databases. Meanwhile, the other tasks showcase its behavior when trained in a different dataset scale, with thousands to tens of thousands of images.

The Stanford Dogs dataset has bounding boxes, but not semantic segmentation masks, which are required to train the ISNet and many of the benchmark DNNs. Therefore, we have created the ground-truth dog segmentation targets with DeepMAC\cite{deepMAC}, a pre-trained general-purpose semantic segmenter. The model produces segmentation outputs according to images and their bounding boxes, and it is specialized in novel class segmentation. I.e., segmenting objects whose classes were not seen during training. The trained model is openly available for download\cite{deepMAC}. Upon visual inspection, the dog segmentation masks it produced were highly accurate.

Stanford Dogs is not a dataset known for containing background bias, and our objective is to evaluate the neural networks' capacity of focusing on the foreground (the dogs) and avoiding the shortcut learning caused by background bias. Therefore, we only consider the synthetically biased version of the dataset. Here, we include a triangle in the top-left image corner for the Pug class, a circle in the bottom-left corner for Tibetan Mastiff, and a square in the bottom-right corner for Pekingese. Examples are provided in Figure \ref{triangle}. The synthetic background bias was inserted in all training and hold-out validation images. For the deceiving bias evaluation, the circle became associated to the Pekingese, the triangle to the Tibetan Mastiff, and the square to the Pug. The shapes' locations did not change (e.g., circles were still in the bottom-left image corner during deceiving bias evaluation).



\subsection{Data Processing and Augmentation}
\label{ProcessingAgumentation}

For COVID-19/TB detection, we loaded the X-rays as grayscale images to avoid the influence of color variations in the dataset. We then performed histogram equalization to further reduce dataset bias. Next, we re-scaled the pixel values between 0 and 1, reshaped the images (which can assume various sizes in the database) to 224x224 and repeated them in 3 channels. The shape of (3,224,224) is the most common input size for the DenseNet121, reducing computational costs while still being fine enough to allow accurate segmentation and lung disease detection\cite{bassi2021covid19}\textsuperscript{,}\cite{chexnet}\textsuperscript{,}\cite{bassiCovid}. Single-channel images can be used but, without a profound change in the classifier architecture, they would provide marginal benefits in the training time. Test images were made square (by the addition of black borders) before resizing to avoid any bias related to aspect ratio. The technique was not used for training because the models without segmentation-based attention mechanisms could learn to identify the added borders.

Since the DenseNet121 classifier is a very deep model and our dataset is small, we used data augmentation in the training dataset to avoid overfitting. The chosen procedure was: random translation (up to 28 pixels up/down, left/right), random rotations (between -40 and 40 degrees), and flipping (50\% chance). Besides the overfitting prevention, the process also makes the DNN more resistant to natural occurrences of the operations. We used online augmentation and substituted the original samples by the augmented ones. 

For facial attribute estimation, we loaded the photographs in RGB, re-scaled between 0 and 1, and reshaped the images to 224x224, which is a common input size for the DenseNet121 and for natural image classification. We employed the same online image augmentation procedure used in the COVID-19 detection task.

The dog breed classification images were loaded and processed like the facial attribute estimation samples. However, we did not employ data augmentation. As previously mentioned, the data augmentation procedure can partially or totally remove the synthetic background bias from the images. Therefore, not using data augmentation in this case simulates a worst-case scenario. By being entirely present in all training images, the background bias capacity of influencing the classifier is maximized. Accordingly, this scenario can act as an extreme test of the ISNet capacity of avoiding background attention and the consequent shortcut learning.

\subsection{Training Procedure}
\label{training}
To train the ISNet, we first set the heatmap loss E hyper-parameter to 1, as preliminary tests indicated low sensibility to the parameter. We then searched for the ideal P parameter for the ISNet loss function. During this tuning process, we trained on artificially biased datasets to ensure that P was high enough to make the models ignore the background even in extreme cases. When searching for hyper-parameter in all neural networks, we looked for a model resulting in minimal accuracy difference for the original validation set and its artificially biased versions. The P value that better balanced the two loss terms and provided the best results was 0.7 for COVID-19 detection, we used the same value in TB detection and dog breed classification, as we found little performance variation with similar P values. In facial attribute estimation, we used 0.5. During the hyper-parameter tuning procedure, we noticed that using weight decay makes it much more complicated to find P. This is because it seems to strongly favor a zero solution (i.e., the network minimizing its parameters to generate null heatmaps, optimizing the heatmap background loss and L2 penalization, but ignoring the classification task). Thus, we did not use L2 regularization. The heatmap loss d parameter was chosen as the lowest value that did not compromise training stability or reduced accuracy (considering values of 0.9, 0.996, 0.999 and 1). In COVID-19 and TB detection we used 0.9, in facial attribute estimation, 0.996, in dogs classification, 1. The heatmap loss B hyper-parameter was always set to 3. To produce the classification labels, we considered normal as class 0, pneumonia as class 1, and COVID-19 as class 2. In TB detection, class 0 is normal, and class 1 is tuberculosis. In facial attribute estimation, class 0 is rosy cheeks, class 1 high cheekbones, and class 2 smiling. In Stanford Dogs, the class order is Pekingese, Tibetan Mastiff and Pug. The ISNet Grad*Input loss hyper-parameters are: P=0.7 and d=0.996 in Stanford Dogs, P=07 and d=0.9 in COVID-19 detection, and P=0.5 and d=0.996 in facial attribute estimation. The RRR $\lambda_{1}$ parameter controls the strength of its right reasons loss\cite{RRR} term. The original paper presents experiments with large variations of the parameter, up to $10^{7}$. In our preliminary tests, we experimented with values up to $10^{12}$, around which the classification task was ignored, and the model fell to the accuracy levels of random guessing. The final choices for $\lambda_{1}$ were: $10^{6}$ in Stanford Dogs and COVID-19 detection, $10^{7}$ in facial attribute estimation, and $10^{8}$ in tuberculosis detection.

We employed a similar classification training procedure for all DNNs, to allow a fairer comparison. It started from a random parameter initialization. The chosen optimizer was stochastic gradient descent, using momentum of 0.99, and a mini-batch of 10 to 16 images. Only the ISNet Grad*Input in COVID-19 detection used 0.9 momentum, as it could to some extent improve the model's problematic loss convergence. We employed gradient clipping to limit gradient norm to 1, making the training procedure more stable. Learning rate was set to 10$^{-3}$, we suggest a reduction in the case of non-convergence of training losses. For the COVID-19 detection task we utilized cross-entropy as the classification loss, and for facial attribute estimation we used binary cross-entropy, as it is a multi-label problem. The network later evaluated on the test dataset is the one achieving the best validation performance during training. We trained all DNNs for 48 epochs in COVID-19 detection, and 96 in TB detection. We used 144 epochs for the ISNet in facial attribute estimation, but in this task we trained the alternative methodologies for 96 epochs, because they were already showing overfitting at that point. All DNNs were trained for 200 epochs for dog breed classification. The classification thresholds for facial attribute estimation were chosen to maximize validation maF1 in the trained DNNs. We could have used the same procedure to find a decision threshold in TB classification, as it is a binary classification problem. However, in preliminary tests we discovered that this methodology could make results in the o.o.d. test dataset worse and more unpredictable. Therefore, we decided to just select the output neuron with the highest activation as the winning one (i.e., employing the standard threshold of 0.5 after softmax activation).

For the alternative segmentation-classification pipeline, we began by training the U-Net for segmentation. We employed the same dataset, data augmentation and preprocessing steps used in the ISNet training procedure. We used stochastic gradient descent, with mini-batches of 10 to 16 samples and momentum of 0.99. Learning rate was set to 10$^{-4}$, except for the dogs segmentation task, where we used 10$^{-3}$. We employed the cross-entropy loss function. We trained using hold-out validation, until overfitting could be observed. Accordingly, we employed 600 epochs in the lung segmentation task (both when training for COVID-19 detection or TB detection), in face segmentation, 400, and in Stanford Dogs, 1000.

We optimized the multi-task U-Net for segmentation and classification simultaneously, with multi-task learning. Again, the model used the same datasets and preprocessing steps employed for the ISNet. Multi-task learning employed a hyper-parameter ($P$) to balance the classification ($L_{C}$) and segmentation ($L_{S}$) losses. Therefore, we minimize a loss term, $L$, defined as  $L=P.L_{S}+(1-P).L_{C}$. Analyzing validation error in preliminary tests, we ended up with a P value of 0.75 in both facial attribute estimation and COVID-19 detection. In TB detection the chosen value was 0.9. Stanford Dogs classification employed $P=0.999$. The segmentation loss function was cross-entropy. We trained with the same optimizer, classification loss functions, number of epochs and learning rate used with the other DNNs. The models were already showing overfitting by the end of the training procedure, and the best network was determined with hold-out validation.

\subsection{Statistical Methods for Results' Analysis}
\label{statisticalMethods}
Dog breed classification and the classification of chest X-rays as COVID-19, normal or pneumonia constitutes a multi-class, single-label problem. To find interval estimates for the classification performance metrics we employed Bayesian estimation. We utilized a Bayesian model\cite{bayesianEstimator} that takes a confusion matrix (displayed in Table \ref{confusion}) as input, and estimates the posterior probability distribution of metrics like class and average precision, recall and F1-Score. The model was expanded in a subsequent study\cite{bassi2021covid19}, to also estimate class and average specificity. For the posteriors' estimation we used Markov chain Monte Carlo (MCMC), implemented with the Python library PyMC3\cite{pymc}. We employed the No-U-Turn Sampler\cite{NUTS}, using 4 chains and 110000 samples, being the first 10000 for tuning. The estimated distributions allow us to report the means, standard deviations, and the 95\% highest density intervals of the performance metrics. The estimation Monte Carlo error is below 10$^{-4}$ for all scores. We adopted the same priors as the Bayesian model creators\cite{bayesianEstimator}. We utilized a macro-averaging and pairwise approach to calculate the overall area under the ROC curve (AUC), a technique developed for multi-class single-label problems\cite{MulticlassAUC}. We do not present interval estimates for this metric, because there is not an established methodology to calculate it (its authors\cite{MulticlassAUC} suggest bootstrapping, but it would not be feasible with the deep models and cross-dataset evaluation used in this study). We utilize the standard one-versus-rest methodology to calculate the per-class ROC AUC. Thus, we are able to employ an already established non-parametric approach\cite{DeLongAUC} to obtain the class AUCs' 95\% confidence intervals.

Facial attribute estimation is a multi-class multi-label classification problem, as is the task in Appendix \ref{chexBias}. Therefore, the aforementioned Bayesian model\cite{bayesianEstimator} is not adequate for it. To create the interval estimates for the performance metrics in this scenario, we employ the Wilson Score Interval. Working with a multi-label task, we calculate the AUC 95\% confidence intervals with the same non-parametric technique we previously utilized\cite{DeLongAUC}, as we consider the standard one-versus-rest AUC again.

In tuberculosis detection we deal with a binary classification task. We also utilize the Wilson Score Interval to create 95\% confidence intervals, and use the aforementioned non-parametric technique to calculate the ROC AUC interval estimate, considering the standard one-versus-rest AUC calculation once more\cite{DeLongAUC}. 



\section*{Data Availability}
In support of the findings of this study, the X-ray data from healthy and/or pneumonia positive subjects are available in the Montgomery and Shenzen databases, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/; in ChestX-ray14, https://paperswithcode.com/dataset/chestx-ray14; and in CheXpert, https://stanfordmlgroup.github.io/competitions/chexpert/. COVID-19 radiography data that support the findings of this study are available in The BrixIA COVID-19 project, https://brixia.github.io/; and in the BIMCV-COVID19+ database, http://bimcv.cipf.es/bimcv-projects/bimcv-covid19/. Tuberculosis-positive X-Rays are available in NIAID TB Portals, https://tbportals.niaid.nih.gov/. Images for facial attribute estimation, along with their segmentation masks, are available in the Large-scale CelebFaces Attributes (CelebA) Dataset,  https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html. The MIMIC-CXR-JPG database (v2.0.0), which supports the findings of this study, is available at https://physionet.org/content/mimic-cxr-jpg/2.0.0/. The Stanford Dogs dataset is available at http://vision.stanford.edu/aditya86/ImageNetDogs/.

\section*{Code Availability}
The code containing the ISNet PyTorch implementation is available at https://github.com/PedroRASB/ISNet.

\bibliography{sample}

\section*{Acknowledgments}
This study was financed by the Italian Institute of Technology (IIT). We gratefully acknowledge the HPC infrastructure and the Support Team at Fondazione Istituto Italiano di Tecnologia.

Data were obtained from the TB Portals (https://tbportals.niaid.nih.gov), which is an
open-access TB data resource supported by the National Institute of Allergy and
Infectious Diseases (NIAID) Office of Cyber Infrastructure and Computational Biology
(OCICB) in Bethesda, MD. These data were collected and submitted by members of the
TB Portals Consortium (https://tbportals.niaid.nih.gov/Partners). Investigators and other
data contributors that originally submitted the data to the TB Portals did not participate
in the design or analysis of this study.


\section*{Author Contributions}
P. R. A. S. B. developed the concept, implemented the neural networks, and analyzed the results. S. S. J. D. annotated lesions in the X-rays. A. C. supervised and reviewed the work.

\section*{Competing Interests}
The authors declare no competing interests.

\section*{Corresponding Author}
Correspondence to Pedro R.A.S. Bassi and Andrea Cavalli.


\appendix
\section{Appendix: Comparing ISNet X-ray Heatmaps to Radiologist's Annotations}
\label{radiologistAnalysis}

With the objective of evaluating the correlation between lung lesions and the ISNet's focus during X-ray classification, we compared the DNN's LRP heatmaps and the corresponding X-rays, with lesions localized by a radiologist. The specialist, Dr. Dertkigil, is a professor of radiology at the Department of Radiology of the University of Campinas (UNICAMP), Campinas, SP, Brazil. He practices medicine at the Clinics Hospital of the University of Campinas since 2009, and is the Director of the hospital's Radiology Service.

We randomly selected 30 X-rays from the external test databases, consisting of 10 pneumonia images, 10 COVID-19 images, and 10 tuberculosis samples. We selected only images that were correctly classified by the ISNets (true positives), because we aim to understand if the fundamentals for a correct DNN decision are similar to the reasoning of a human specialist. The radiologist was asked to mark lung lesions caused by the disease present in the X-rays. He had no access to the DNN outputs or heatmaps before completing the requested task. Afterwards, the annotated X-rays were superposed over the corresponding LRP heatmaps, produced by the ISNets, and we analyzed the resulting images. Annotations are presented in green; X-rays were equalized (histogram equalization), heatmaps were resized to the original X-ray shape and normalized (with red and blue values ranging between 0 and 255). LRP employed the LRP-e rule, and the softmax-based LRP rule\cite{LRPBook} for the output layer (ensuring more class-specific heatmaps).

Although we utilized annotated X-rays to evaluate the clinical soundness of the ISNets' LRP heatmaps, the classifiers were not trained to segment lesions in their heatmaps, nor were they informed of the lesions' locations during training. The ISNet architecture was created to hinder attention outside a region of interest (lungs), but it does not control the attention pattern inside this region (refer to Section \ref{loss}). Thus, the configuration of attention inside the lungs is a natural result of the classification loss optimization. For the precise localization of lesions, object detectors or semantic segmenters, trained with lesion masks, may be preferred in relation to classifiers.

When classifying images, classifiers can focus mostly on parts of the object of interest. Thus, explanation heatmaps generally highlight the objects partially\cite{SEC}. Accordingly, our DNNs may correctly classify a disease by focusing on just some of the lung lesions, or only parts of some lesions. We must also consider that classification accuracies were not perfect. Test performance metrics for the DNNs can be found in Table \ref{performance} for the model classifying healthy, pneumonia and COVID-19, and in Table \ref{tb} for the network classifying healthy and tuberculosis. Class F1-Scores for the classification of COVID-19, pneumonia and TB are 0.907 +/-0.006, 0.858 +/-0.007, and 0.748 +/-0.043, respectively. In conclusion, we did not expect a perfect correlation between the radiologist lesion annotations and the DNN LRP heatmaps. However, the existence of a correlation was clear, and its quality was superior for COVID-19, the class with the best F1-Score.

Figure \ref{rad} presents a few examples of X-rays and their heatmaps, superposed over the annotated X-rays. Red colors indicate regions that the DNN associated with the classified disease, and blue represents areas that reduced the DNN confidence for the disease. In the COVID-19 heatmap, blue areas can be associated to the pneumonia and healthy classes; in the pneumonia image, to COVID-19 and healthy; in the TB sample, to healthy. The green lines are the radiologist annotations, surrounding lesions caused by the conditions. As expected, high relevance for the diseases (red colors) partially covered the annotations, and not all annotations contained relevance. In the tuberculosis heatmap, some positive relevance (red) can be seen outside of the annotations. If we recreate the COVID-19 heatmap by executing LRP for pneumonia instead of COVID-19, blue regions encompassed by the green lines will become red. Moreover, when running LRP for the COVID-19 class with the pneumonia-positive image, blue regions inside annotations also become red. Thus, we observe a bit of confusion between damage produced by COVID-19 pneumonia and non-COVID-19 pneumonia, which can be explained by the similarity of two conditions' lesions. In the three Figure \ref{rad} X-rays we see annotations encompassing red regions, sometimes almost perfectly. Therefore, it clearly shows a correlation between the ISNet heatmaps and the radiologist's analysis.


\begin{figure}[!h]
\includegraphics[width=0.75\textwidth]{ComparisonRadiologist.png}
\centering
\caption{X-rays and their LRP heatmaps (blue and red), superposed over the image, annotated by the radiologist (green)}
\label{rad}
\end{figure}

We individually compared all annotated X-rays with their heatmaps. The images, radiologist's annotations, and heatmaps are available in the Supplementary Data 1. The individual analysis are presented in Supplementary Note 1. With a qualitative evaluation, we concluded that the correlation between annotations and red regions in the heatmaps is clearly observable. As exemplified in Figure \ref{rad}, the DNN generally focused on some of the annotated lesions, and paid more attention to parts of them. We noticed that the ISNets tend to avoid paying attention to lung regions that are overly opaque. When there was a very opaque lesion, the DNN preferred to focus on its borders or on other lesions. This behavior may be caused by our ground-truth lung segmentation targets (used to train the ISNets), which could not always identify strongly opaque regions as part of the lungs. Accordingly, we believe that the ISNet attention pattern could be improved if manual lung segmentation masks existed for all training X-rays. Many images had some relevance outside of annotations, but the majority had a large part of the red regions covered by annotations, including the reddest heatmap area. Comparing the three classes, the COVID-19 heatmaps presented the largest number of annotations encompassing red regions, and the smallest quantity of red outside of annotations. Accordingly, the correlation between the human specialist annotations and the disease positive relevance (red) in AI (Artificial Intelligence) heatmaps is stronger for the class with the highest test performance (F1-Score).

From a quantitative point of view, in 9 of the 10 COVID-19-positive images, at least half of the COVID-19 lesions marked by the radiologist encompassed red regions (i.e., areas that the DNN associated with COVID-19). All figures presented at least one annotation covering red areas. In 8 of the 10 X-rays, most of the COVID-19 relevance (red) was inside the radiologist's annotations. Moreover, in 8 of the 10 images the reddest heatmap area (i.e., the ISNet strongest focus for the COVID-19 class) was covered by an annotation. As in the Figure \ref{rad} example, a few annotations covered blue regions, which were more associated to pneumonia than to COVID-19 by the DNN. Only one of the 10 X-ray heatmaps showed a significant amount of relevance outside of the lungs. This is the only image in the 30 annotated samples with significant attention outside of the lungs, demonstrating the success of the ISNet attention mechanism. The undesired relevance was around the armpit, a region that some ground-truth lung segmentation targets falsely identified as lungs. Thus, the problem could be solved by training the ISNet with more accurate lung masks. Overall, the quantitative results indicate that the ISNet COVID-19 heatmaps and the radiologist's analysis are strongly correlated.

Of the 10 pneumonia-positive images, 7 presented at least 50\% of the radiologist's annotations covering pneumonia relevance (red). Furthermore, 9 had at least one annotation containing red areas. The only exception was an image with a single annotation, located in the lung region that overlaps the heart. However, our ground-truth segmentation targets trained the ISNet not to consider this area as region of interest. Of the 10 images, 4 had the reddest area inside an annotation. They also had most of the pneumonia relevance contained by the annotations. In line with the ISNet's F1-Scores, the quantitative results show that the correlation between the pneumonia heatmaps and the radiologist's analysis is not as strong as for the COVID-19 images. However, the results clearly indicate a correlation.

Finally, 9 of the 10 TB heatmaps have at least half of their annotations encompassing red regions (TB relevance). All images had at least one annotation covering a red area. Six samples had most of the TB relevance contained by annotations. Moreover, these 6 X-rays also had the reddest heatmap area inside an annotation. As the TB classification F1-Score, these quantitative results did could not match those found for COVID-19. However, a correlation is observable once more.

The qualitative and quantitative comparison between the ISNet heatmaps and the radiologist's annotations shows the existence of a correlation, which is the strongest for the class with the highest test performance. Even though the ISNet was not trained to segment lesions, the correlation indicates a consistency between a human specialist's reasoning and the decision rules naturally learned by the model. In line with the ISNet superior o.o.d. generalization performance (Tables \ref{performance} and \ref{tb}), we observe that the new attention mechanism diverted focus from background bias to the lesions.



\section{Appendix: CheXPert classification and comparison to Hierarchical Attention Mining}
\label{CheXpertClassification}

The two X-ray classification problems previously analyzed (COVID-19 and TB detection) clearly demonstrate background bias causing shortcut learning. Therefore, they exemplify situations where the ISNet use is very advantageous. Both problems consider mixed source training datasets, where X-rays from different classes were produced by diverse hospitals. In this appendix we examine if a large single-source X-ray database is also prone to background bias. Thus, we analyze if the task fits in the ISNet main use case: to hinder background bias attention and the consequent shortcut learning.

We consider a subset of CheXPert\cite{irvin2019chexpert} as the training dataset. The utilization of a subset reduces the experiments' computational cost, without detracting from the validity of our findings. As discussed in Section \ref{dataset}, the database is exceptionally large, and all its X-rays come from the same source, the Stanford University Hospital (California, United States of America). We selected all CheXPert frontal X-rays that contained the lung conditions atelectasis, consolidation, edema, pneumonia, and pneumothorax, along with the X-rays with the no finding label. In total, 112179 images were gathered; 29795 were positive for atelectasis, 13015 for consolidation, 49717 for edema, 17000 for no finding, 4683 for pneumonia, and 17700 for pneumothorax. The subset patients' mean age was 59.7 years, with standard deviation of 18.6 years. They were 55.8\% male. Hold-out validation utilized 25\% of the images, randomly selected.

For o.o.d. testing, we chose the MIMIC-CXR-JPG database (v2.0.0)\cite{MIMIC}\textsuperscript{,}\cite{MIMIC1}\textsuperscript{,}\cite{PhysioNet}. It is another large open dataset, which contains a set of labels with the same structure as CheXPert, considered in this study. Its 377110 X-rays were gathered from the Beth Israel Deaconess Medical Center (Massachusetts, United States of America), between 2011 and 2016. Labels were also created with natural language processing, constructed according to radiologists' reports. We treated absent and uncertainty labels as negative, as in CheXPert. To create the external evaluation database, we selected a subset of MIMIC's original test database, consisting in all frontal X-rays that were labeled positive for atelectasis, consolidation, edema, no finding, pneumonia, or pneumothorax. According to this strategy, 2265 samples were gathered; 763 were positive for atelectasis, 210 for consolidation, 722 for edema, 593 for no finding, 340 for pneumonia, and 108 for pneumothorax. The MIMIC database does not inform patient age or sex.

We trained the ISNet, DenseNet201, segmentation-classification pipeline (U-Net followed by DenseNet121), multi-task U-Net and AG-Sononet using the CheXPert subset. The numerical labels used in training were defined according to the alphabetical order of the 6 possible classes. The DNNs' definitions and implementations were the same as for the COVID-19 and TB detection tasks (refer to Section \ref{BaselineImplementations}). However, CheXPert and MIMIC classification are multi-label problems. Thus, binary cross-entropy was used as the classification loss function. The X-ray processing and augmentation procedure followed the guidelines in Section \ref{ProcessingAgumentation}. Finally, the training strategy is explained in Section \ref{training}. Specific changes in relation to the strategy used for COVID-19 detection were: training for 96 epochs, and using mini-batches from 16 to 32 samples. Again, we set $10^{-3}$ as the learning rate. Like in the other X-ray classification tasks, we utilized the U-Net trained in a previous study\cite{bassi2021covid19} to produce the ground-truth lung masks for CheXPert. The U-Net in the segmentation-classification pipeline was trained for 400 epochs, using such targets and learning rate of $10^{-4}$. The ISNet loss hyper-parameters were: d=1, P=0.7, and w$_{1}$=w$_{2}$=1. The RRR right reasons\cite{RRR} loss weight ($\lambda_{1}$) was 10.

We did not train GAIN\cite{GAIN} for this task, as the model training time was exceedingly long, due to the large dataset and the multi-label classification task, which greatly reduced the model's training speed (refer to Section \ref{speed}). Instead, we used the new application to implement a Hierarchical Attention Mining\cite{HAM} (HAM) DNN. Like GAIN, HAM utilizes GradCAM optimization to guide classifier attention. Moreover, both architectures' main objective is to produce classifiers' GradCAM explanations that are useful for weakly-supervised localization. However, HAM's loss functions are tailored for the task of lung lesion localization, especially in datasets containing lesion annotations. In the previous experiments, we observed that DNNs that do not learn from segmentation targets were ineffective against background bias. For HAM to consider such targets, lesion annotations must be available. Such annotations are rare for X-ray datasets, and are not available for large TB and COVID-19 databases, such as the ones we used. For this reason, HAM was not implemented for the previous X-ray classification tasks. However, the creators of HAM openly provided 6099 bounding-boxes (created by a radiologist) for 2345 CheXPert images\cite{HAM}. Therefore, we compare HAM to the ISNet in the task of CheXPert classification. To created more precise segmentation targets for the lesions (used only in HAM training), we employed the intersection of the provided bounding-boxes and the lung masks we produced for CheXPert. For the CheXPert subset, there were a total of 1769 lesion segmentation targets, 501 for atelectasis, 255 for consolidation, 543 for edema, 195 for pneumonia, and 275 for pneumothorax.

HAM combines multiple strategies to control the classifier attention. First, the model has a foreground attention block (FAB), a self-attention mechanism based on channel and spatial attention\cite{HAM}\textsuperscript{,}\cite{attentionSurvey}. As the AG-Sononet, such mechanism does not rely on segmentation targets, thus learning to focus on image regions that improve classification loss. HAM is a hierarchical classifier, it has one output classifying if there is any abnormality in the X-ray, along with outputs for each possible abnormal condition. A GradCAM heatmap of the binary output is called positive attention map, while the GradCAM heatmaps of the other outputs are named abnormality maps. The attention bound loss\cite{HAM} enforces the abnormality maps to be contained inside the positive attention map. The attention union loss\cite{HAM} compels the union of the abnormality maps to match the positive map. Finally, adaptive mean square error (AMSE) loss\cite{HAM} penalizes differences between the available lesion segmentation targets and the abnormality maps. Our HAM implementation followed the official public implementation\cite{HAM}, which uses a ResNet50\cite{ResNet} backbone. Unlike the original paper, we employed the input size of 224x224, matching the other DNNs. Training parameters also followed the strategy used for the other models, allowing a fairer comparison. HAM loss hyper-parameters followed the values in the original paper\cite{HAM}.

\subsection {Results and Discussion}

We express the o.o.d. test results (MIMIC dataset) in terms of AUC (Area Under the ROC Curve), as it is the standard for reporting MIMIC and CheXPert performances\cite{irvin2019chexpert}\textsuperscript{,}\cite{HAM}. Table \ref{chexpertAUCs} displays per-class and macro-average AUC scores, considering 95\% confidence. Statistical methods were the same as those used for facial attribute estimation (Section \ref{statisticalMethods}), as both tasks are multi-label classification problems. The ISNet achieved 0.735 +/-0.009 macro-average AUC for the i.i.d. validation dataset (CheXPert), the segmentation-classification pipeline 0.779 +/-0.008, the DenseNet121 0.802 +/-0.008, the multi-task U-Net 0.812 +/-0.007, the AG-Sononet 0.804 +/-0.008, HAM 0.792 +/-0.008, RRR 0.759 +/-0.008, and the vision transformer 0.733 +/-0.009. The validation performances may be seen as an optimistic approximation of the i.i.d. test performance, as the loss in the validation dataset was utilized to select the best performing DNNs.

\begin{table}[!h]
\centering
\caption{Test AUC for the deep neural networks trained with CheXPert and evaluated on MIMIC}
\label{chexpertAUCs}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[156]Q[98]Q[119]Q[81]Q[98]Q[102]Q[125]Q[150]},
  row{8} = {},
  row{9} = {},
  hlines,
  vlines,
}
Model                 & Atelectasis        & Consolidation      & Edema              & No Finding         & Pneumonia          & Pneumothorax       & {\textbf{Mean}\\\textbf{(macro-average)}} \\
\textbf{ISNet}        & {0.633 \\+/-0.024} & {0.656 \\+/-0.038} & {0.773 \\+/-0.022} & {0.764 \\+/-0.02}  & {0.606 \\+/-0.034} & {0.69 \\+/-0.054}  & {\textbf{0.687 }\\\textbf{+/-0.032}}      \\
{U-Net+\\DenseNet121} & {0.64\\+/-0.023}   & {0.688\\+/-0.039}  & {0.795\\+/-0.019}  & {0.774\\+/-0.022}  & {0.651\\+/-0.033}  & {0.777\\+/-0.041}  & {\textbf{0.721 }\\\textbf{+/-0.03}}       \\
DenseNet121           & {0.64 \\+/-0.023}  & {0.663 \\+/-0.04}  & {0.804 \\+/-0.018} & {0.782 \\+/-0.022} & {0.645 \\+/-0.033} & {0.783 \\+/-0.043} & {\textbf{0.72 }\\\textbf{+/-0.03}}        \\
{Multi-task \\U-Net~} & {0.686 \\+/-0.022} & {0.713 \\+/-0.037} & {0.807 \\+/-0.018} & {0.788 \\+/-0.022} & {0.666 \\+/-0.031} & {0.798 \\+/-0.046} & {\textbf{0.743 }\\\textbf{+/-0.029}}      \\
AG-Sononet            & {0.645 \\+/-0.023} & {0.624 \\+/-0.040} & {0.805 \\+/-0.018} & {0.792 \\+/-0.022} & {0.646 \\+/-0.032} & {0.771 \\+/-0.05}  & {\textbf{0.714 }\\\textbf{+/-0.031}}      \\
HAM                   & {0.629 \\+/-0.023} & {0.675 \\+/-0.039} & {0.795 \\+/-0.018} & {0.791 \\+/-0.022} & {0.637 \\+/-0.033} & {0.755 \\+/-0.049} & {\textbf{0.714 }\\\textbf{+/-0.031}}      \\
RRR                   & {0.618\\+/-0.024}  & {0.659\\+/-0.039}  & {0.793\\+/-0.019}  & {0.769\\+/-0.022}  & {0.616\\+/-0.034}  & {0.692\\+/-0.049}  & {\textbf{0.691}\\\textbf{+/-0.031}}       \\
Vision Transformer    & {0.569\\+/-0.025}  & {0.630\\+/-0.038}  & {0.766\\+/-0.02}   & {0.751\\+/-0.023}  & {0.601\\+/-0.033}  & {0.619\\+/-0.049}  & {\textbf{0.656}\\\textbf{+/-0.031}}       
\end{tblr}
\end{table}

Table \ref{chexpertAUCs} shows similar o.o.d. AUCs for the tested DNNs, almost all macro-average AUCs have some overlap in their 95\% confidence intervals. Our previous experiments demonstrated that the ISNet and the segmentation-classification pipeline are the two DNNs that better ignore background bias, and the two neural networks have the smallest gaps between the CheXPert validation AUC (i.i.d.) and the MIMIC test AUC (o.o.d.). However, the two models' average AUCs could not surpass a standard classifier (DenseNet121). Thus, ignoring the CheXPert background was not exceedingly beneficial for generalization performance, unlike what we observed in COVID-19 or TB detection.

CheXPert classification produced relatively small gaps between the i.i.d. validation AUCs (on CheXPert) and the o.o.d. test AUCs (on MIMIC). For the common DenseNet121 classifier, we observed a macro-average AUC gap of 0.082. Meanwhile, when comparing i.i.d. and o.o.d. evaluation performance in tuberculosis detection, the same DenseNet121 produced an AUC gap larger than 0.42, and past studies demonstrate similarly large gaps for COVID-19 detection\cite{ShortcutCovid}. The drastic differences in the performance gaps indicate that CheXPert training is much less prone to shortcut learning than the two previous X-ray classification problems, supporting the conclusion that the dataset contains less background bias. Moreover, the positive multi-task DNN results (Table \ref{chexpertAUCs}) indicate that CheXPert background features are more representative of clutter than bias, i.e., they have little correlation with the classes. Past studies pointed out that multi-task classification with segmentation may improve results when background clutter is present\cite{MultiTask2}, while our previous experiments proved that the multi-task model is not resistant to background bias.

Having these considerations in mind, with CheXPert we cannot assess whether HAM is robust to background bias. Thus, we resorted to adding artificial background bias to the CheXPert subset images. We utilized the same geometrical shapes used in the facial attribute estimation task (triangle, circle, and square, as exemplified in Figure \ref{triangle}), and placed them in the image corners. As the subset contains 6 possible classes, we vertically divided the 3 geometrical shapes in half, and each new geometrical shape was correlated with one class. After training on the artificially biased database, we tested the neural network on the MIMIC test subset. When the geometrical shapes were present in the test data, HAM's macro-averaged AUC was 0.951 +/-0.009. After the artificial bias removal, the score dropped to 0.627 +/-0.032. Therefore, the classifier decisions were clearly affected by the background bias. Like we observed for GAIN (Figure \ref{triangle}), the HAM LRP heatmaps clearly show the attention paid to the geometrical shapes, while its GradCAM explanations indicate little focus on the bias. Overall, HAM's GradCAM explanations match well with the segmentation targets. Again, the strong influence of the geometrical shapes on the DNN's F1-Scores prove that the LRP explanations' correlation with the classifier behavior surpasses GradCAM's. HAM and GAIN are two different architectures, whose attention mechanisms are both based on GradCAM optimization. Accordingly, both failed in avoiding background attention, and produced spurious GradCAM explanations. The similar behavior indicate that their failure is caused by fundamental weaknesses in the GradCAM methodology, corroborating with our analysis in Section \ref{GAINComparison}.

For comparison, we also trained the ISNet on the artificially biased dataset. Again, it minimized the influence of background bias: macro-average AUC was 0.7 +/-0.031 when the test data had the geometrical shapes. When they were removed, the score did not change. Moreover, the performance is remarkably similar to the value in Table \ref{chexpertAUCs}. As in previous experiments, the ISNet's explanations (LRP and GradCAM) show no attention to the artificial bias. Finally, as a baseline, we trained a DenseNet121 on the artificially biased CheXPert subset. It had 0.97 +/-0.006 macro-average AUC with the geometrical shapes, and 0.622 +/-0.032 without them. Once more, in the presence of background bias, the ISNet strongly surpassed the other DNNs. HAM's macro-average AUC scores are remarkably similar to the standard DenseNet121's. Moreover, the LRP heatmaps for both DNNs show strong focus on bias. In the other experiments, GAIN's LRP heatmaps revealed attention to bias and the region of interest (Figure \ref{triangle}), resulting in a smaller performance gap between evaluation with and without the geometrical shapes. Thus, we conclude that GAIN's attention mining loss (absent in HAM) made the DNN pay attention to input features in the foreground and the background. Meanwhile, the two losses that compare GradCAM heatmaps to segmentation targets (GAIN's external supervision and HAM's AMSE) could not effectively avoid background attention.

Figure \ref{chexBias} provides examples of the DNNs' heatmaps for artificially biased images. It shows CheXPert validation X-rays. Naturally, attention to the geometrical shapes must be observable in training, test and validation samples. By choosing validation X-rays, we are able to display its lesion annotations (created from radiologist's bounding boxes\cite{HAM}). The two X-rays are positive for edema, and the green annotations refer to this condition. The semicircle is the geometrical shape associated to edema. The bottom X-ray in Figure \ref{chexBias} is also positive for atelectasis and consolidation (indicated by two adjacent rectangles, which form a square). Notice that the HAM GradCAM heatmaps have higher resolution than the ISNet GradCAM, because HAM's last convolutional feature map is larger. Attention to the semi-circle is visible in all LRP heatmaps, except for the ISNet's. Moreover, HAM's LRP explanations are similar to the common DenseNet121's. Again, we observe that HAM's GradCAM heatmaps hid the attention to the geometrical shapes. They are similar to the lesion annotations, but LRP and HAM's F1-Scores reveal that the model mostly focused on bias. The other benchmark DNNs were already tested with artificially biased datasets, considering both single-label and multi-label classification (Figure \ref{triangle}).

\begin{figure}[!h]
\includegraphics[width=1\textwidth]{AppendixFigure.png}
\centering
\caption{Artificially biased validation X-rays, their lesion (edema) annotations, and  heatmaps for multiple DNNs. Images are positive for the class edema, associated with the geometrical shape in the X-ray bottom left corner. In LRP heatmaps, red indicates areas that the DNN associated with edema. Blue areas reduced the edema classification score or were more representative of other conditions}
\label{chexBias}
\end{figure}

We must note that HAM's authors do not claim robustness against background bias, nor tested this functionality\cite{HAM}. Furthermore, for a fair comparison to the other DNNs in Table \ref{chexpertAUCs}, we trained HAM with subset of the CheXPert dataset, and employed a smaller input size and higher learning rate in relation to those used in the original HAM paper\cite{HAM}, which may have reduced the model performance. We do not state that GradCAM-based methodologies cannot improve attention in any application. Instead, the results in this study show that GradCAM the models are not reliable in the presence of background bias, unlike the ISNet. Please refer to Section \ref{GAINComparison} for an in-depth analysis.

The results from the experiments with the unaltered CheXPert images (without the geometrical shapes) point out that strong background bias and the consequent shortcut learning are not intrinsic to X-ray classification. However, many X-ray classification tasks require dataset mixing, as large databases may not contain all classes of interest. The experiments in this study indicate mixing as the main cause of background bias. In mixed X-ray datasets, images from different classes come from dissimilar sources (hospitals), which can present unique background characteristics. In line with these conclusions, we find studies analyzing applications with mixed X-ray datasets and showing large o.o.d. generalization gaps, shortcut learning and strong background bias\cite{ShortcutCovid}. Meanwhile, other works have trained DNNs on large single-source chest X-ray datasets and displayed smaller o.o.d. generalization gaps\cite{LabelShift}. Moreover, they pointed out other causes for such gaps, like label shift between different datasets\cite{LabelShift}.

The multiple experiments in this study clearly delimit the ISNet best use case. First, datasets without background bias are not the ideal application for the new model, as shown by the results in this appendix. Furthermore, the ISNet is not meant to boost i.i.d. accuracy. As clearly seen in the tests with artificially biased datasets, focus on background bias improves i.i.d. performances. Thus, by avoiding such attention, the ISNet can reduce accuracy on i.i.d. evaluation databases, to enhance real-world performance. The ideal use for the ISNet, according to our experiments, is to improve generalization when the training dataset presents background bias. The occurrence of background bias is common in practical applications, as was demonstrated by the tasks of COVID-19 detection and TB detection. The ISNet strongly improved o.o.d. generalization for the tasks. The new architecture's efficiency in dealing with background bias was further exemplified by the datasets containing artificial background bias (in facial attribute estimation and COVID-19 detection). The presence of background bias can be revealed by large gaps between i.i.d. and o.o.d. performances\cite{ShortcutCovid}\textsuperscript{,}\cite{ShortcutLearning}, along with strong background attention in a standard classifier's explanation heatmaps (Figure \ref{maps} and \ref{triangle}). However, we note that shortcut learning is not only caused by background bias\cite{ShortcutLearning}, e.g., foreground features may also be biased. Moreover, besides shortcut learning, generalization gaps can have other causes, such as label shift between databases\cite{LabelShift}.


\section{Appendix: the Denoising Property of LRP-$\varepsilon$}
\label{lrpVsGradInput}

An early DNN explanation technique are saliency maps\cite{saliency}, which consist on the absolute value of the DNN logit's gradient with relation to the input image ($abs(\nabla_{X}(y_{c}))$). Basically, large values in the input gradient ($\nabla_{X}(y_{c})$) indicate features whose perturbation would cause large logit changes. Thus, saliency maps consider input pixels with large gradients as important for the DNN decision. However, saliency maps are considered noisy and the absolute value operation avoids the differentiation of positive and negative class evidence\cite{LRPvsGrad}. Gradient*Input explanations\cite{GradInput} create heatmaps with an element-wise multiplication between the input gradient and the input itself, i.e., $\bm{X} \odot \nabla_{X}(y_{c})$. The explanations should improve sharpness over saliency maps and differentiate positive and negative evidence\cite{LRPvsGrad} (i.e., input features that would increase or decrease the class c logit $y_{c}$ if they were higher). It has been proposed\cite{LRPvsGrad} that LRP-0\cite{LRP} heatmaps are equivalent to Gradient*Input explanations\cite{GradInput} when the only non-linearity in the DNN is the ReLU function. In Subsection \ref{equivalence}, we formally demonstrate the equivalence. With relation to LRP-0, LRP-$\varepsilon$ explanations are less noisy and more coherent\cite{LRPBook}. From the deep Taylor decomposition standpoint, they represent a reduction of the first-order Taylor approximation residuum\cite{LRPBook} (Section \ref{mathBackground}). In Subsection \ref{LRPDenoise}, we reformulate LRP-$\varepsilon$, to more clearly expose its denoising quality and advantages over LRP-0 and Gradient*Input.

\subsection{LRP-0 and Gradient*Input Equivalence}
\label{equivalence}
The equivalence between LRP-0 and Gradient*Input is valid for neural networks with ReLU activations. Moreover, we must consider that no division by zero is performed over the entire LRP-0 procedure; i.e., none of the neural network layers outputs a zero element $z_{k}$ (Equation \ref{LRP0}). In practice, the LRP-0 propagation over all DNN layers is numerically unstable. Assuming stability, we demonstrate the two explanations' equivalence for a dense layer. Such demonstration does not lack generality, because convolutional layers can be expressed as fully-connected layers with sparse parameters, while batch normalization and pooling can be also reformulated as equivalent convolutions or dense layers.

We start by rearranging the LRP-0 rule (Equation \ref{LRP0}). We now include superscripts to indicate the layer number. The value $w_{jk}^{L}$ refers to the weights connecting layer L input $a_{j}^{L}$ to its output $z_{k}^{L}$ (before the ReLU activation), $b_{k}^{L}$ is the additive bias parameter for the output $z_{k}^{L}$, $R_{k}^{L+1}$ is the LRP relevance of layer L output $z_{k}^{L}$ (which is also the relevance at the layer L+1 input, $a_{k}^{L+1}$), and $R_{j}^{L}$ is the relevance of layer L's input $a_{j}^{L}$. The rearranged LRP-$\varepsilon$ rule is:

\begin{gather}
\label{lrp0}
G_{j}^{L}=\sum_{k}\frac{w_{jk}^{L}R_{k}^{L+1}}{z_{k}^{L}}\\
\label{q}
\mbox{where: } G_{j}^{L}=R_{j}^{L}/a_{j}^{L}
\end{gather}

Considering the ReLU activation, we express the forward-pass of a dense layer in Equation \ref{fwd}. Notice that the ReLU output for layer L is also the input of layer L+1 ($a_{k}^{L+1}$).

\begin{gather}
\label{fwd}
a_{k}^{L+1}=ReLU(z_{k}^{L})=
\begin{cases}
z_{k}^{L}, \mbox{ if } z_{k}^{L} > 0 \\
0,  \mbox{ otherwise }
\end{cases}\\
\mbox{where: } z_{k}^{L}=b_{k}^{L}+\sum_{j} w_{jk}^{L}a_{j}^{L}
\end{gather}

For layer L+1, we utilize the subscript k to refer to its inputs, as they are element-wise functions (ReLU) of layer L outputs, which also use subscripts k. Considering Equations \ref{q} and \ref{fwd}, we can reformulate Equation \ref{lrp0}:

\begin{equation}
\label{lrp0b}
G_{j}^{L}=\sum_{k}\frac{w_{jk}^{L}R_{k}^{L+1}}{z_{k}^{L}}=
\sum_{k}\frac{w_{jk}^{L}G_{k}^{L+1}a_{k}^{L+1}}{z_{k}^{L}}=
\sum_{k}w_{jk}^{L}G_{k}^{L+1}\frac{ReLU(z_{k}^{L})}{z_{k}^{L}}
\end{equation}

We simplify equation \ref{lrp0b} by using the unit step function, H($\cdot$):

\begin{gather}
\label{QRule}
G_{j}^{L}=\sum_{k}w_{jk}^{L}H(z_{k}^{L})G_{k}^{L+1}\\
\mbox{where: } H(z_{k}^{L})=
\label{unitStep}
\begin{cases}
1, \mbox{ if } z_{k}^{L} > 0 \\
0, \mbox{ if } z_{k}^{L} < 0 
\end{cases}
\end{gather}

Equation \ref{QRule} expresses a recursive propagation rule (from the neural network logit to the input layer) of the quantity $G_{j}^{L}$, which is associated to the layer 
L input $a_{j}^{L}$. The new propagated signal is not conservative ($\sum_{j} G_{j}^{L}$ is not the same for all layers L, unlike $\sum_{j} R_{j}^{L}$). However, the conservative LRP relevance, $R_{j}^{L}$, can always be reconstructed with the element-wise multiplication of the $G_{j}^{L}$ quantity and the layer L input value itself, $a_{j}^{L}$ (Equation \ref{q}). Therefore, if we propagate $G_{j}^{L}$ until the neural network input layer (producing the tensor $\bm{G}^{0}=[G_{j}^{0}]$), we can obtain the LRP-0 heatmap ($\bm{R}^{0}=[R_{j}^{0}]$) by element-wise multiplying $\bm{G}^{0}$ and the DNN input image ($\bm{X}=[X_{j}]=[a_{j}^{0}]$). Thus, the LRP-0 propagation is equivalent of the propagation of $G_{j}^{L}$ followed by an element-wise multiplication with the DNN input.

\begin{equation}
R_{j}^{0}=G_{j}^{0}X_{j}
\end{equation}

Now, to prove the equivalence between LRP-0 and Gradient*Input heatmaps, we must simply show that $G_{j}^{0}$ is the gradient of the DNN logit with relation to the input element $X_{j}$, i.e., $G_{j}^{0}=\partial y_{c}/\partial X_{j}$. Consider $y_{c}$ the neural network logit for class c, and assume that we are propagating the gradient (and the LRP-0 relevance) for class c. The logit gradient backpropagation (backward-pass from output to input) through the fully-connected layer L can be derived according to Equation \ref{chainRule}, which follows the calculus' chain rule.

\begin{equation}
\label{chainRule}
    \frac{\partial y_{c}}{\partial a_{j}^{L}}= \sum_{k} \frac{\partial z_{k}^{L}}{\partial a_{j}^{L}} \frac{\partial y_{c}}{\partial z_{k}^{L}}= 
    \sum_{k}  w_{jk}^{L} \frac{\partial y_{c}}{\partial z_{k}^{L}}=
    \sum_{k}  w_{jk}^{L} \frac{\partial a_{k}^{L+1}}{\partial z_{k}^{L}}\frac{\partial y_{c}}{\partial a_{k}^{L+1}}=
    \sum_{k}  w_{jk}^{L} \frac{\partial ReLU(z_{k}^{L})}{\partial z_{k}^{L}}\frac{\partial y_{c}}{\partial a_{k}^{L+1}}
\end{equation}

The derivative of the ReLU function is the unit step function, H($\cdot$). Thus, we have a simple recurrence rule for the gradient backpropagation in DNNs composed of fully-connected layers with ReLU activation:

\begin{equation}
\label{gradRecurrent}
    \frac{\partial y_{c}}{\partial a_{j}^{L}}= \sum_{k} w_{jk}^{L} H(z_{k}^{L}) \frac{\partial y_{c}}{\partial a_{k}^{L+1}}
\end{equation}

We see that the recurrent backpropagation rule for the gradient $\partial y_{c}/\partial a_{j}^{L}$ (Equation \ref{gradRecurrent}) is the same as the rule for the quantity $G_{j}^{L}$ (Equation \ref{QRule}). Now we compare the initial values of $G_{j}^{L}$ and $\partial y_{c}/\partial a_{j}^{L}$, i.e., the G value and gradient for the outputs of the DNN last layer. The DNN logits are indicated as $y_{k}=z_{k}^{Lmax}$, where Lmax indicates the DNN last layer. Starting with the gradient, we have:

\begin{gather}
\label{initialGRad}
\frac{\partial y_{c}}{\partial y_{k}}=
\begin{cases}
1, \mbox{ if } k=c \\
0,  \mbox{ otherwise }
\end{cases}
\end{gather}

LRP-0 considers that the logit relevance is the logit value for the explained class logit ($R_{c}^{logits}=y_{c}$), and 0 otherwise ($R_{c' \neq c}^{logits}=0$)\cite{LRP}. Thus, considering Equation \ref{q}, we can define the G quantity for the logits, $G_{k}^{logits}$:

\begin{gather}
\label{initialQ}
G_{k}^{logits}=
\begin{cases}
1, \mbox{ if } k=c \\
0,  \mbox{ otherwise }
\end{cases}
\end{gather}

As we are explaining logits, we disconsider the last layer non-linear activation, viewing it as a linear layer. Thus, for back-propagating gradients ($\partial y_{c}/\partial y_{k}$) or $G_{k}^{logits}$ through it, we ignore the unit step functions in equations \ref{gradRecurrent} and \ref{QRule}, producing:

\begin{equation}
\label{iGrad}
    \frac{\partial y_{c}}{\partial a_{j}^{Lmax}}=  w_{jc}^{Lmax}
\end{equation}

\begin{equation}
\label{iQ}
G_{j}^{Lmax}=w_{jc}^{Lmax}
\end{equation}

Equations \ref{iGrad} and \ref{iQ} prove that the starting values for $\partial y_{c}/\partial a_{j}^{L}$ and $G_{j}^{L}$ are the same. Because the two quantities are initially identical and follow the same propagation rules (Equations \ref{gradRecurrent} and \ref{QRule}), they are equal throughout the entire propagation procedure:

\begin{equation}
\label{equals}
G_{j}^{L}=\frac{\partial y_{c}}{\partial a_{j}^{L}}
\end{equation}

Taking into account Equations \ref{equals} and \ref{q}, we conclude that Gradient*Input ($X_{j}\partial y_{c}/\partial X_{j}$) explanations are equivalent to LRP-0 heatmaps ($R_{j}^{0}$), considering the requirements that ReLU is the only non-linearity in the DNN, and that LRP-0 is numerically stable:

\begin{equation}
X_{j}\frac{\partial y_{c}}{\partial X_{j}}=X_{j}G_{j}^{0}=a_{j}^{0}G_{j}^{0}=R_{j}^{0}
\end{equation}

\subsection{LRP-$\varepsilon$ Denoising Property and Advantages over LRP-0 and Gradient*Input}
\label{LRPDenoise}

We must emphasize the differences between LRP-$\varepsilon$ and LRP-0. Besides ensuring numerical stability (by avoiding divisions by zero), the $\varepsilon$ stabilizer (Equation \ref{LRPeEquation}) reduces the explanation noise, improves heatmap coherence and contextualization, and reduces the Taylor approximation error\cite{LRP}\textsuperscript{,}\cite{LRPBook} (Section \ref{mathBackground}). When $\varepsilon$ is not zero, Gradient*Input is not equivalent to LRP-$\varepsilon$. In this case, equation \ref{LRP-e_Q} substitutes equation \ref{lrp0b}.

\begin{gather}
\label{LRP-e_Q}
G_{j}^{L}=\sum_{k}\frac{w_{jk}^{L}R_{k}^{L+1}}{z_{k}^{L}+sign(z_{k}^{L})\varepsilon}=
\sum_{k}\frac{w_{jk}^{L}G_{k}^{L+1}a_{k}^{L+1}}{z_{k}^{L}+sign(z_{k}^{L})\varepsilon}=
\sum_{k}w_{jk}^{L}G_{k}^{L+1}\frac{ReLU(z_{k}^{L})}{z_{k}^{L}+sign(z_{k}^{L})\varepsilon}=
\sum_{k}w_{jk}^{L}A(z_{k}^{L})G_{k}^{L+1} \\
\label{AS}
\mbox{where: } A(z_{k}^{L})=\frac{ReLU(z_{k}^{L})}{z_{k}^{L}+sign(z_{k}^{L})\varepsilon}=
\begin{cases}
\frac{z_{k}^{L}}{z_{k}^{L}+\varepsilon}, \mbox{ if } z_{k}^{L} \geq 0 \\
0,  \mbox{ otherwise }
\end{cases}
\end{gather}

The function $A(z_{k}^{L})$ is not the unit step (Equation \ref{unitStep}) if $\varepsilon \neq 0$. Therefore, the propagation rule for the G quantity in LRP-$\varepsilon$ is different from the rule for the gradient backpropagation (Equation \ref{gradRecurrent}). Accordingly, for LRP-$\varepsilon$, G is not the gradient, and LRP-$\varepsilon$ heatmaps are not equivalent to Gradient*Input explanations. Equation \ref{AS} can be seen as an attenuated unit step function ($\varepsilon$ is a positive constant, empirically set to 0.1 in our experiments). It is a non-linear continuous function, which is zero for negative $z_{k}^{L}$ (like the unit step), but that monotonically increases for positive $z_{k}^{L}$, saturating at 1 (which is the unit step output for all positive $z_{k}^{L}$). The lower the parameter $\varepsilon$, the faster $A(z_{k}^{L})$ approximates 1, matching the discontinuous unit step function when $\varepsilon=0$. The function approximates the unit step for large values of $z_{k}^{L}$ ($z_{k}^{L}>>\varepsilon$), but its outputs are smaller for small $z_{k}^{L}$. Accordingly, the propagation rule in equation \ref{LRP-e_Q} approximates the gradient backpropagation (Equation \ref{gradRecurrent}) when $G_{k}^{L+1}$ is associated to a large or negative neuron output ($z_{k}^{L}>>\varepsilon$ or $z_{k}^{L}<0$). However, the attenuated step reduces the impact of small neuron activations ($z_{k}^{L}$) on the propagated quantity ($G_{j}^{L}$). I.e., for small but positive $z_{k}^{L}$, the resulting small $A(z_{k}^{L})$ makes the $w_{jk}^{L}A(z_{k}^{L})G_{k}^{L+1}$ term (Equation \ref{LRP-e_Q}) significantly smaller than $w_{jk}^{L}H(z_{k}^{L})G_{k}^{L+1}$, the equivalent term in gradient backpropagation (Equation \ref{QRule} or \ref{gradRecurrent}). The attenuated unit step function is illustrated in Figure \ref{atStep} for multiple values of $\varepsilon$. Higher $\varepsilon$ more strongly attenuates the importance of small neuron activations on $G_{j}^{L}$, and $G_{j}^{L}$ matches the gradient when $\varepsilon=0$, representing no attenuation. Therefore, we demonstrated that $\varepsilon$ controls a denoising process, which is justified by the deep Taylor decomposition framework, indicating a relationship between the noise reduction and a reduction in the Taylor first-order approximation error, which leads to more coherent and contextualized explanations\cite{LRPBook}.

\begin{figure}[!h]
\includegraphics[width=0.5\textwidth]{AttenuatedStep.png}
\centering
\caption{Illustration of the attenuated unit step function, $ReLU(x)/(x+sign(x)\varepsilon)$, implicitly used by LRP-$\varepsilon$ to denoise the signal backpropagation. Different lines correspond to diverse values of $\varepsilon$, indicated in the image's upper left corner}
\label{atStep}
\end{figure}

The ISNet Grad*Input is an ablation experiment, where the ISNet LRP heatmaps were substituted by Gradient*Input explanations. Its comparison to the ISNet empirically demonstrates that the effect caused by $\varepsilon$ is important for the stability and convergence of the ISNet loss functions, resulting in higher bias resistance, easier and more stable training, and better accuracy, especially for deeper architectures (refer to the Section \ref{resultsSynth} and Table \ref{synth}).

\end{document}