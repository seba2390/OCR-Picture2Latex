In the last decade, multi-robot SLAM has been becoming increasingly popular, which promotes the development of distributed PGO \cite{choudhary2017distributed,tian2019distributed,tron2014distributed,eric2020geod}.

Choudhary \ea \cite{choudhary2017distributed} present a two-stage algorithm that implements either Jacobi Over-Relaxation or  Successive Over-Relaxation as distributed linear system solvers. Similar to centralized methods, \cite{choudhary2017distributed} first evaluates the chordal initialization \cite{carlone2015initialization} and then improves the initial guess with a single Gauss-Newton step. However, one step of Gauss-Newton method in most cases can not lead to sufficient convergence for distributed PGO. In addition, no line search is performed in \cite{choudhary2017distributed} due to the communication limitation, and thus, the behaviors of the single Gauss-Newton step is totally unpredictable and might result in bad solutions.

Tian \ea \cite{tian2019distributed} present the distributed certifiably correct PGO using Riemannian block coordinate descent method, which is later generalized to asynchronous and parallel distributed PGO \cite{tian2020asynchronous}. Specially, their method makes use of Riemannian staircase optimization to solve the semidefinite relaxation of distributed PGO and is guaranteed to converge to global optimal solutions under moderate measurement noise. Following our previous works \cite{fan2020mm,fan2019proximal}, they implement Nesterov's method for acceleration as well. Contrary to our MM methods, a major drawback of \cite{tian2019distributed} is that their method has to precompute red-black coloring assignment for block aggregation and keep part of the blocks in idle for estimate updates. In addition, although several strategies for block selection (e.g., greedy/importance sampling) and Nesterov's acceleration (e.g., adaptive/fixed restarts) are adopted in \cite{tian2019distributed} to improve the convergence, most of them are either inapplicable without a master node or at the sacrifice of computational efficiency and theoretical guarantees. In contrast, our MM methods are much faster (see Section~\ref{section::experiemnt}) but have no such restrictions for acceleration. More recently, Tian \ea  further apply Riemannian block coordinate descent method to distributed PGO with robust loss kernels \cite{tian2021kimera}. However, they solve robust distributed PGO by trivially updating the weights using graduated nonconvexity \cite{yang2020graduated} and no formal proofs of convergence are provided. Again, { this is  in contrast to the work} presented here that has provable convergence to first-order critical points for a broad class of robust loss kernels.

Tron and Vidal \cite{tron2014distributed} present a consensus-based method for distributed PGO using Riemannian gradient. The authors derive a condition for convergence guarantees related with the stepsize of the method and the degree of the pose graph. Nonetheless, their method estimates rotation and translation separately, fails to handle robust loss kernels, and needs extra computation to find the convergence-guaranteed stepsize. 

Cristofalo \ea \cite{eric2020geod} present a novel distributed PGO method using  Lyapunov theory and multi-agent consensus. Their method is guaranteed to converge if the pose graph has certain topological structures. However, \cite{eric2020geod} updates rotations without exploiting the translational measurements and only applies to pairwise consistent PGO with nonrobust loss kernels.



In comparison to these aforementioned techniques, our MM methods have the mildest conditions (not requiring any specific pose graph structures, any extra computation for preprocessing, any master nodes for information aggregation, etc.) to converge to first-order critical points, apply to a broad class of robust loss kernels in robotics and computer vision, and manage to implement decentralized acceleration with convergence guarantees. Most importantly, as is shown in \cref{section::experiemnt}, our MM methods outperform existing state-of-the-art methods in terms of both efficiency and accuracy on a variety of SLAM benchmark datasets.