{\highlight In distributed optimization, MM methods are one of the most popular first-order optimization methods  \cite{hunter2004tutorial,sun2016majorization}. As mentioned before, MM methods solve an optimization problem by iteratively minimizing an upper bound of the objective function such that the objective value is nonincreasing. Recall that $G(X|\Xk)$ and $H(X|\Xk)$ majorize $F(X)$ and decouple poses from different nodes; see \cref{prop::G,prop::lG}, respectively. Therefore, we might make use of MM methods where distributed PGO is reduced to independent optimization problems that can be solved in parallel.  In \cref{section::mm::update}, we propose MM methods for distributed PGO using $G(X|\Xk)$ and $H(X|\Xk)$. Then, in \cref{section::mm::algorithm}, we present the algorithm and prove that the proposed method is guaranteed to converge to first-order critical points.}

\vspace{-0.75em}

\subsection{Update Rule}\label{section::mm::update}
According to \cref{prop::G,prop::lG}, $G(X|\Xk)$ and $\lG(X|\Xk)$ are surrogate functions majorizing $F(X)$:
\begin{equation}\label{eq::lGGFeq}
	\lG(X|\Xk) \geq G(X|\Xk) \geq F(X),
\end{equation}
\begin{equation}\label{eq::lGGFneq}
\lG(\Xk|\Xk) = G(\Xk|\Xk) = F(\Xk).
\end{equation}
Following the notion of MM methods \cite{hunter2004tutorial}, we propose the following update rule:
\begin{equation}\label{eq::minlG}
	\Xkh\gets\arg\min_{X\in \XX} \lG(X|\Xk),
\end{equation}
\begin{equation}\label{eq::minG}
	\Xkp\gets\arg\min_{X\in \XX} G(X|\Xk).
\end{equation}
Here, $\Xkh$  in \cref{eq::minlG} is first solved and used to initialize $\Xkp$ in \cref{eq::minG}. Also, \cref{eq::lG2,eq::G2} indicate that \cref{eq::minlG,eq::minG} are equivalent to  $|\AA|$ independent optimization problems of $\Xa\in\XX^\alpha$ within a single node $\alpha$:
\begin{equation}\label{eq::xlG}
	\Xakh\gets\arg\min_{\Xa\in \XXa}\lG^\alpha(\Xa|\Xk),\quad \forall\alpha\in\AA,
\end{equation}
\begin{equation}\label{eq::xG}
	\Xakp\gets\arg\min_{\Xa\in \XXa}G^\alpha(\Xa|\Xk),\quad \forall\alpha\in\AA,
\end{equation}
where  $\Xakh$ in \cref{eq::xlG} is the initial guess to solve $\Xakp$ in \cref{eq::xG}. We remark that \cref{eq::xlG,eq::xG} can be  solved within a single node $\alpha\in\AA$. Recalling from \cref{eq::lGMsum} that $H^\alpha(\Xa|\Xk)=\sum_{i=1}^{n_\alpha}H_i^{\alpha}(X_i^{\alpha}|\Xk)$, we further reduce \cref{eq::xlG} to $n\triangleq\sum_{\alpha\in\AA} n_\alpha$ independent optimization problems on a single pose $\Xa_i\in SE(d)$:
\vspace{-0.5em}
\begin{multline}\label{eq::xlGi}
	\Xakh_i\gets\arg\min_{X_i^\alpha\in SE(d)}\lG_i^\alpha(\Xa_i|\Xk),\\
	\quad\quad\forall\alpha\in\AA\text{ and } i\in \{1,\,\cdots,\,n_\alpha\}.
\end{multline}
In \hyperapp{appendix::K}{K}, we have shown that \cref{eq::xlGi} admits an efficient closed-form solution involving only matrix multiplication and singular value decomposition \cite{umeyama1991least}.

From \cref{eq::lGGFeq,eq::lGGFneq}, we conclude that \cref{eq::minlG,eq::minG,eq::xlG,eq::xG} result in
\begin{subequations}
	\begin{equation}\label{eq::FlG}
		F(\Xk)=\lG(\Xk|\Xk)\geq\\ \lG(\Xkh|\Xk)\geq F(\Xkh),
	\end{equation}
	\begin{equation}\label{eq::FG}
		F(\Xk)=G(\Xk|\Xk)\geq\\ G(\Xkp|\Xk)\geq F(\Xkp)
	\end{equation}
\end{subequations}
which indicate  $F(\Xkh)\leq F(\Xk)$ and $F(\Xkp)\leq F(\Xk)$. Therefore,  \cref{eq::minlG,eq::minG,eq::xlG,eq::xG} are a reasonable update rule for distributed PGO.  In particular, we remark that \cref{eq::minlG,eq::minG,eq::xlG,eq::xG} combine the strengths of our previous work \cite{fan2019proximal,fan2020mm}. Even though \cref{eq::minlG,eq::xlG}  are motivated by \cite{fan2019proximal},  \cref{eq::minG,eq::xG} make better use of the  information within a single node, and thus, take fewer iterations. In contrast to \cite{fan2020mm}, since \cref{eq::minlG,eq::xlG} have an efficient closed-form solution to \cref{eq::xlGi} that yields sufficient improvement,  the time-consuming local minimization of \cref{eq::minG,eq::xG} is avoided  as long as $\Xkp$ and $\Xakp$ are initialized with $\Xkh$ and $\Xakh$. Most importantly, as is shown later in \cref{prop::mm}, the proposed  updated rule of \cref{eq::minlG,eq::minG,eq::xlG,eq::xG} has provable convergence to first-order critical points.


\subsection{Algorithm}\label{section::mm::algorithm}

\setlength{\textfloatsep}{7pt}

\begin{algorithm}[t]
	\caption{The $\mm$ Method}
	\label{algorithm::mm}
	\begin{algorithmic}[1]
		\State\textbf{Input}: An initial iterate $X^{(0)}\in \XX$ and $\zeta \geq \xi \geq 0$.
		\State\textbf{Output}: A sequence of iterates $\{\Xk\}$ and $\{\Xkh\}$.\vspace{0.2em} 
		\vspace{0.1em}
		\For{$\sk\gets 0,\,1,\,2,\,\cdots$}
		\vspace{0.1em}
		\For{node $\alpha\gets 1,\,\cdots,\, |\AA|$}
		\vspace{0.1em}
		\State  retrieve $\Xbk$ from $\beta\in\NN_{\alpha}$ \label{line::alg1::comm}
		\vspace{0.1em}
		\State evaluate $\nGammaak$, $\lnGammaak$, $\nabla_{\Xa} F(\Xk)$  \label{line::grad_F}
		\vspace{0.1em}
		\State $\Xakh\gets \arg\min\limits_{\Xa\in\XXa }\lG^\alpha(\Xa|\Xk)$ using \cref{algorithm::lG}\label{line::xlG}
		\vspace{0.1em}
		\State $\Xakp\gets$ improve $\arg\min\limits_{\Xa\in\XXa }G^\alpha(\Xa|\Xk)$\label{line::mm_opt} with $\Xakh$ as the initial guess \label{line::xG}
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
	\caption{Solve $\Xakh\gets \arg\min\limits_{\Xa\in \XXa}\lG^\alpha(\Xa|\Xk)$}
	\label{algorithm::lG}
	\begin{algorithmic}[1]
		\State\textbf{Input}: $\Xak$, $\lnGammaak$, $\nabla_{\Xa} F(\Xk)$.
		\vspace{0.1em}
		\State\textbf{Output}: $\Xakh$.\vspace{0.2em} 
		\For{$i\gets 1,\,\cdots,\, n_\alpha$}\label{line::xlG1}
		\vspace{0.1em}
		\State $\Xakh_i \gets \arg\min\limits_{\Xa_i\in SE(d)} \lG_i^\alpha(\Xa_i|\Xk)$ using \hyperapp{appendix::K}{K}\label{line::alg2::xak}
		\vspace{0.1em}
		\EndFor
		\vspace{0.1em}
		\State retrieve $\Xakh$ from $\Xakh_i$ in which $i = 1,\,\cdots,\,n_\alpha$\label{line::xlG2}
	\end{algorithmic}
\end{algorithm}

The proposed update rule results in the $\mm$ method for distributed PGO (\cref{algorithm::mm}). The outline of  $\mm$  is as follows:
\begin{enumerate}[leftmargin=0.45cm]
%\item In practice, $\xi > 0$ and $\zeta > 0$ in \cref{eq::G} and \cref{eq::lG} are recommended to be set close to zero such that $G(X|\Xk)$ and $\lG(X|\Xk)$ are tighter upper bounds of $F(X)$.
\item In line~\ref{line::alg1::comm} of \cref{algorithm::mm}, each node $\alpha$ performs one inter-node communication round to retrieve $\Xbk$ from its neighbors $\beta\in\NN_{\alpha}$. Note that that no other inter-node communication is required.
\item In lines~\ref{line::grad_F} of \cref{algorithm::mm}, each node $\alpha$ evaluates $\nGamma$, $\lnGamma$, $\nabla_{\Xa} F(\Xk)$ with $\Xak$ and $\Xbk$ where $\beta\in\NN^\alpha$ are neighbors of node $\alpha$. 
\item In line~\ref{line::xlG} of \cref{algorithm::mm}, we obtain the intermediate solution $\Xakh$ using \cref{algorithm::lG}. We have proved that the resulting $\Xakh$ is already sufficient to guarantee the convergence to first-order critical points.
\item In line~\ref{line::alg2::xak} of \cref{algorithm::lG}, there exists an exact and efficient closed-form solution to $\Xakh$ using \hyperapp{appendix::K}{K}.
%\item In line~\ref{line::xlG} of \cref{algorithm::mm}, $\min\limits_{\Xa\in\XXa }\lG^\alpha(\Xa|\Xk)$ has an efficient closed-form solution that only involves singular value decomposition and matrix multiplication.

\item In line~\ref{line::xG} of \cref{algorithm::mm}, we use $X^{\alpha(\sk+\shalf)}$ to initialize \cref{eq::xG}, and improve the final solution $\Xakp$ through iterative optimization such that $G^{\alpha}(\Xakp|\Xk)\leq G^{\alpha}(X^{\alpha(\sk+\shalf)}|\Xk)$. Note that $\Xakp$ does not have to be a local optimal solution to \cref{eq::xG}, nevertheless, $\Xakp$ is still expected to have a faster convergence than $X^{\alpha(\sk+\shalf)}$.
\end{enumerate}

\begin{remark}
	\highlight
	In line~\ref{line::alg1::comm} of \cref{algorithm::mm}, node $\alpha$ does not retrieve all the poses in $\Xbk$---only poses related to inter-node measurements are needed and exchanged. This also applies to line~\ref{line::alg4::comm} of \cref{algorithm::amm_c_x}, and lines~\ref{line::alg5::comm1} and \ref{line::alg5::comm} of \cref{algorithm::amm} in the following sections.
\end{remark}

\begin{remark}
	$\mm$  (\cref{algorithm::mm}) requires no inter-node communication except for  line~\ref{line::alg1::comm} of \cref{algorithm::mm} that is used to evaluate $\nGammaak$, $\lnGammaak$, $\nabla_{\Xa} F(\Xk)$, which, as mentioned before, can be distributed with one inter-node communication round between neighboring nodes $\alpha$ and $\beta$ without introducing any additional computation.
\end{remark}



Since  $\Xakh$ in \cref{eq::xlG} has a closed-form solution that can be efficiently computed, and \cref{eq::xG} does not require $\Xakp$ to be a local optimal solution, the overall computational efficiency of the $\mm$ method is significantly improved in contrast to \cite{fan2019proximal,fan2020mm}. More importantly, the $\mm$ method still converges to first-order critical points as long as the following assumption holds.

\begin{assumption}\label{assumption::mm}
For $\Xakp$ and $X^{\alpha(\sk+\shalf)}$, it is assumed that
\begin{equation}
G^{\alpha}(\Xakp|\Xk)\leq G^{\alpha}(X^{\alpha(\sk+\shalf)}|\Xk)
\end{equation}
for each node $\alpha=1,\,2\,\cdots,\, |\AA|$.
\end{assumption}

Note that \cref{assumption::mm} can be satisfied with ease as long as line~\ref{line::xG} of \cref{algorithm::mm} is initialized with $\Xakh$. Then, we have the following proposition about the convergence of $\mm$  (\cref{algorithm::mm}).

\begin{prop}\label{prop::mm}
If \cref{assumption::loss,assumption::neighbor,assumption::mm} hold, then for a sequence of $\{\Xk\}$ and $\{\Xkh\}$ generated by  \cref{algorithm::mm},  we have
\begin{enumerate}[(a)]
\item\label{prop::mm1} $F(\Xk)$ is nonincreasing as $\sk\rightarrow\infty$;
\item\label{prop::mm2}  $F(\Xk)\rightarrow F^\infty$ as $\sk\rightarrow\infty$;
\item\label{prop::mm3} $\|\Xkp-\Xk\|\rightarrow 0$ as $\sk\rightarrow\infty$ if $\xi>0$;
\item\label{prop::mm4} $\|\Xkh-\Xk\|\rightarrow 0$ as $\sk\rightarrow\infty$ if $\zeta > \xi > 0$;
\item\label{prop::mm5}  if $\zeta > \xi > 0$, then there exists $\epsilon > 0$ such that 
\begin{equation}
	\nonumber
	\min\limits_{0\leq\sk< \mathsf{K}}\|\grad\, F(\Xkh)\|\leq \sqrt{\frac{2}{\epsilon}\cdot\dfrac{F(X^{(0)})-F^\infty}{{\sK+1}}}
\end{equation}
for any $\mathsf{K}\geq 0$;
\item\label{prop::mm6}  if $\zeta > \xi> 0$, then $\grad\, F(\Xk)\rightarrow \0$ and $\grad\, F(\Xkh)\rightarrow \0$ as $\sk\rightarrow \infty$.
\end{enumerate}
\end{prop}
\begin{proof}
See \hyperapp{appendix::E}{E}.
\end{proof}



\begin{remark}
In contrast to other distributed PGO algorithms \cite{tian2019distributed,tron2014distributed,choudhary2017distributed,eric2020geod},  $\mm$  has the mildest conditions for convergence and apply to a broad class of loss kernels.
\end{remark}