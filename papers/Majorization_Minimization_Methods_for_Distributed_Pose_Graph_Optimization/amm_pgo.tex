{
	\highlight
	The adaptive restart is essential for the convergence of accelerated MM methods. In  $\ammc$  (\cref{algorithm::ammc}), the adaptive restart scheme needs a master node to evaluate $F(\Xkp)$ and $\sF^{(\sk)}$ and guarantee the convergence. On the other hand, if there is no master node, the adaptive restart scheme requires substantial amount of inter-node communication, making  $\ammc$  unscalable for large-scale distributed PGO. Recently, we developed an adaptive restart scheme for distributed PGO that does not require a master node while generating convergent iterates \cite{fan2020mm}.  Nevertheless, the adaptive restart scheme in \cite{fan2020mm}  is conservative and suffers from unnecessary restarts that hinder acceleration and yield slower convergence. Thus, we need to redesign the adaptive restart scheme for distributed PGO without master node to maximize the performance of accelerated MM methods. To address this issue, in \cref{subsection::amm::restart}, we develop a novel adaptive restart scheme that requires no master node and is fully decentralized. Then, in \cref{section::amm::algorithm}, we propose the accelerated MM method for distributed PGO without master that has provable convergence to first-order critical points. In particular, the resulting accelerated MM method, which needs no master node and is fully decentralized, empirically has no loss of computational efficiency in contrast to  $\ammc$  with master node; see \cref{section::experiemnt} for more details. 
}

\vspace{-0.75em}
\subsection{Adaptive Restart}\label{subsection::amm::restart}
{ 
Recall that $\ammc$'s adaptive restart scheme guarantees the convergence by keeping $F(\Xkp)\leq \sF^{(\sk)}$, where the master node only evaluates and compares $F(\Xkp)$ and  $\sF^{(\sk)}$. This suggests that if we could achieve $F(\Xkp)\leq \sF^{(\sk)}$ without evaluating and comparing $F(\Xkp)$ and  $\sF^{(\sk)}$, no master node will be needed. We also  note that if there is a sequence of $\{F^{\ak}\}$ and $\{\lF^{\ak}\}$  for each node $\alpha$ such that
\begin{equation}\label{eq::Fsum}
F(\Xk) = \sum_{\alpha\in\AA} F^{\ak},
\end{equation}
\begin{equation}\label{eq::lFsum}
	\lF^{(\sk)} = \sum_{\alpha\in\AA} \lF^{\ak},
\end{equation}
\begin{equation}\label{eq::FlF}
	F^{\akp} \leq \lF^{\ak},
\end{equation}
then $F(\Xkp)=\sum_{\alpha\in\AA}F^{\akp}\leq \sum_{\alpha\in\AA}\sF^{\ak}=\sF^{(\sk)}$. Therefore,  the sequence above of $\{F^{\ak}\}$ and $\{\lF^{\ak}\}$  is sufficient to  keep $F(\Xkp)\leq \sF^{(\sk)}$  despite that $F(\Xkp)$ and $\lF^{(\sk)}$ are not explicitly evaluated and compared. More importantly, an adaptive restart scheme without master node can be developed with the sequence.  In rest of this section, we will construct  $\{F^{\ak}\}$ and $\{\lF^{\ak}\}$  satisfying \cref{eq::FlF,eq::Fsum,eq::lFsum},  which further results in the adaptive restart scheme for distributed PGO without a master node. }

{ For notational simplicity, we define $\Delta G^\alpha(X|\Xk):\XX\rightarrow\R$ related to the majorization gap of  $G(X|\Xk)$ over $F(X)$:
	\vspace{-0.5em}
\begin{multline}\label{eq::DGa}
	\Delta G^\alpha(X|\Xk)\triangleq -
	\frac{\xi}{2} \big\|\Xa-\Xak\big\|^2 + \\
	\frac{1}{2}\sum_{\beta\in\NN_-^{\alpha}}\sum_{(i,j)\in \aEE^{\ab}}\left(F_{ij}^{\ab}(X)-E_{ij}^{\ab}(X|\Xk)\right)+\\
	\frac{1}{2}\sum_{\beta\in\NN_+^{\alpha}}\sum_{(i,j)\in \aEE^{\ba}}\left(F_{ij}^{\ba}(X)-E_{ij}^{\ba}(X|\Xk)\right)
\end{multline}
where $F_{ij}^{\ab}(X)$, $F_{ij}^{\ba}(X)$, $E_{ij}^{\ab}(X|\Xk)$, $E_{ij}^{\ba}(X|\Xk)$  are given in \cref{eq::Faaabij,eq::Eab}. From $\Delta G^\alpha(X|\Xk)$ in \cref{eq::DGa}, we recursively define $F^{\ak}$, $\sF^{\ak}$, $G^{\ak}$ according to:
\begin{enumerate}[leftmargin=0.45cm]
\item If $\sk=-1$, each node $\alpha$ initializes $F^{\alpha(-1)}$ and $\sF^{\alpha(-1)}$ with
\vspace{-0.75em}
\begin{multline}\label{eq::Fa0}
\!\!\!\!\!\!\!\!\!\!\!\!\!	F^{\alpha(-1)} \!\triangleq \!\!\!\!\sum_{(i,j)\in \aEE^{\alpha\alpha}}\!\! F_{ij}^{\aa}(X^{(0)})+	\frac{1}{2}
\!\sum_{\beta\in\NN_-^{\alpha}}\sum_{(i,j)\in \aEE^{\ab}}\!\! F_{ij}^{\ab}(X^{(0)}) +\\
	\frac{1}{2}\sum_{\beta\in\NN_+^{\alpha}}\sum_{(i,j)\in \aEE^{\ba}}\!\! F_{ij}^{\ba}(X^{(0)}),
\end{multline}
\begin{equation}\label{eq::sFa0}
	\sF^{\alpha(-1)}\triangleq F^{\alpha(-1)}.
\end{equation}
\item If $\sk\geq 0$, each node $\alpha$ recursively updates $G^{\ak}$, $F^{\ak}$ and $\sF^{\ak}$  according to
\begin{equation}\label{eq::Gakp}
	G^{\ak} \triangleq G^\alpha(\Xak|\Xkm) + F^{\akm},
\end{equation}
\begin{equation}\label{eq::Fakp}
	F^{\ak} \triangleq G^{\ak} +  \Delta G^\alpha(\Xk|\Xkm),
\end{equation}
\begin{equation}\label{eq::lFakp}
	\sF^{\ak}  \triangleq (1-\eta)\cdot\sF^{\akm} + \eta\cdot F^{\ak}
\end{equation}
where $\eta\in(0,\,1]$.\\[-1em]  
\end{enumerate}
In \hyperapp{appendix::G}{G}, we have proved that such a sequence of $\{F^{\ak}\}$ and $\{\lF^{\ak}\}$ satisfies \cref{eq::FlF,eq::Fsum,eq::lFsum} as long as $\sG^{\akp} \leq \sF^{\ak}$, which yields the following proposition.}

{
\begin{prop}\label{prop::FsF}
For $G^{\ak}$, $F^{\ak}$, $\sF^{\ak}$ in \cref{eq::sFa0,eq::Fa0,eq::Gakp,eq::Fakp,eq::lFakp}, we have
\begin{enumerate}[(a)]
\item\label{prop::FsF1} $F(\Xk)=\sum_{\alpha\in\AA} F^{\ak}$ where $F(\Xk)$ is given in \cref{eq::obj};\\[-0.8em]
\item\label{prop::FsF2} $\sF^{(\sk)}=\sum_{\alpha\in\AA}\sF^{\ak}$  where $\sF^{(\sk)}$ is given in \cref{eq::lFk};\\[-0.8em]
\item\label{prop::FsF4} $F^{\akp}\leq\sF^{\akp} \leq \sF^{\ak}$ if $\sG^{\akp} \leq \sF^{\ak}$.
%\item\label{prop::FsF3} $F^{\ak}\leq G^{\ak}$.
\end{enumerate}
\end{prop}
\begin{proof}
	See \hyperapp{appendix::G}{G}.
\end{proof}


It can be concluded from Propositions \ref{prop::FsF}\ref{prop::FsF1} and \ref{prop::FsF}\ref{prop::FsF2} that the resulting $\{F^{\ak}\}$ and $\{\lF^{\ak}\}$ satisfies \cref{eq::Fsum,eq::lFsum}, and \cref{prop::FsF}\ref{prop::FsF4} indicates that \cref{eq::FlF} holds if $\sG^{\akp} \leq \sF^{\ak}$. In \hyperapp{appendix::H}{H}, we have also proved that the following steps  are sufficient to lead to $\sG^{\akp} \leq \sF^{\ak}$:
\begin{enumerate}[leftmargin=0.45cm]
	\item  Update $\Xakp$ by solving \cref{eq::GYak} at node $\alpha$;
	\item Compute $G^{\akp}$ with \cref{eq::Gakp} at node $\alpha$;
	\item If $G^{\akp}>\sF^{\akp}$, update $\Xakp$ again by solving  \cref{eq::xG}  and reduce $s^{\akp)}$  at node $\alpha\in\AA$. 
\end{enumerate}
Then, we not only obtain a sequence of $\{F^{\ak}\}$ and $\{\lF^{\ak}\}$  satisfying \cref{eq::FlF,eq::Fsum,eq::lFsum}, but also an adaptive restart scheme using $G^{\ak}$, $F^{\ak}$, $\sF^{\ak}$ to keep $F(\Xkp)\leq \sF^{(\sk)}$. Note that $F(\Xkp)$ and $\lF^{(\sk)}$ are neither evaluated nor compared. Instead, we evaluate and compare $G^{\akp}$ and $\lF^{\ak}$ independently at each node $\alpha$. Moreover, according to  \cref{eq::Faaabij,eq::GMa,eq::DGa},  it is tedious but straightforward to show that $G^{\ak}$, $F^{\ak}$, $\sF^{\ak}$ in \cref{eq::Fa0,eq::sFa0,eq::Fakp,eq::lFakp,eq::Gakp} can be computed with one inter-node communication round between node $\alpha$ and its neighbors $\beta\in\NN_{\alpha}$. We emphasize that such an adaptive restart scheme differs from those in $\ammc$ and \cite{fan2019proximal,li2015accelerated,zhang2004nonmonotone} that have a master node to  evaluate and compare $F(\Xkp)$ and $\sF^{(\sk)}$. In contrast, the resulting adaptive restart scheme keeps $F(\Xkp)\leq \lF^{(\sk)}$ but needs no master node,  and thus, is well-suited for distributed PGO without master node.
}


\begin{remark}
	\highlight
	$F_{ij}^{\ab}(X)-E_{ij}^{\ab}(X|\Xk)$ and $F_{ji}^{\ba}(X)-E_{ji}^{\ba}(X|\Xk)$ are majorization gaps for inter-node measurements related to nodes $\alpha$ and $\beta$.  According to \cref{eq::DGa}, $\Delta G^{\alpha}(\Xa|\Xk)$ takes half of  $F_{ij}^{\ab}(X)-E_{ij}^{\ab}(X|\Xk)$ and $F_{ji}^{\ba}(X)-E_{ji}^{\ba}(X|\Xk)$ for inter-node measurements in node $\alpha$.  Then, \cref{eq::Fakp} uses $\Delta G^\alpha(\Xa|\Xk)$ to compute $F^{\ak}$ where majorization gaps $F_{ij}^{\ab}(X)-E_{ij}^{\ab}(X|\Xk)$ and $F_{ji}^{\ba}(X)-E_{ji}^{\ba}(X|\Xk)$ are evenly redistributed between nodes $\alpha$ and $\beta$. 
\end{remark}

\input{algorithm_amm}


\vspace{-0.5em}
\subsection{Algorithm}\label{section::amm::algorithm}
With the adaptive restart scheme using $G^{\ak}$, $F^{\ak}$, $\sF^{\ak}$ to keep $F(\Xkp)\leq \sF^{(\sk)}$, we obtain the $\ammd$ method (\cref{algorithm::amm}) for distributed PGO, where ``$\#$'' indicates that no master node is needed.

The outline of  $\ammd$  is similar to  $\ammc$  and the key difference is the adaptive restart scheme:

\begin{enumerate}[leftmargin=0.45cm]
%\item  In lines~\ref{line::alg5::sk}, \ref{line::alg5::Yk} of \cref{algorithm::amm}, each node $\alpha$ computes $\Yak$ used for  Nesterov's acceleration.
	
\item In lines~\ref{line::alg5::comm1}, \ref{line::alg5::comm} of \cref{algorithm::amm}, each node $\alpha$ performs one inter-node communication round to retrieve $\Xbk$ and $Y^{\bk}$ from its neighbors $\beta\in\NN^\alpha$. We remark that no other inter-node communication is required.

\item In lines~\ref{line::alg5::Fakp2}, \ref{line::alg5::lFa0}, \ref{line::alg5::Fakp1}, \ref{line::alg5::lFakp} of \cref{algorithm::amm} and lines~\ref{line::alg6::Gakh1}, \ref{line::alg6::Gakp1}, \ref{line::alg6::Gakh2}, \ref{line::alg6::Gakp2} of \cref{algorithm::amm_x}, each node $\alpha$  evaluates $F^{\ak}$, $\sF^{\ak}$, $G^{\akh}$, $G^{\akp}$ that are used for adaptive restart. Note that $\Xbk$ and $X^{\bkm}$ from node $\alpha$'s neighbors  $\beta\in\NN^\alpha$ are needed.

\item In lines~\ref{line::alg6::restart1} to \ref{line::alg6::restart4} of \cref{algorithm::amm_x}, each node $\alpha$ performs independent adaptive restart such that $G^{\akh}\leq \lF^{\ak}$ and $G^{\akp}\leq \lF^{\ak}$, which also results in $F(\Xkp)\leq \sF^{(\sk)}$ and a nonincreasing sequence of $\sF^{(\sk)}$ for distributed PGO without master node. 
\item In lines~\ref{line::alg6::check1} to \ref{line::alg6::check2} of \cref{algorithm::amm_x}, $G^{\akp}$ is guaranteed to yield sufficient improvement over $\lF^{\ak}$ compared to $G^{\akh}$.
\end{enumerate}
Furthermore, $\ammd$  converges to first-order critical points as the following propositions states. 

\input{algorithm_amm_x}
%\input{algorithm_amm_phi}


 

\begin{prop}\label{prop::amm}
	{\highlight If \cref{assumption::mm,assumption::loss,assumption::neighbor} hold, $\psi>0$ and $\phi>0$,} then for a sequence of  $\{X^{(\sk)}\}$ and $\{X^{(\skh)}\}$  generated by \cref{algorithm::amm}, we have
	\begin{enumerate}[(a)]
		\item\label{prop::amm1}  $\lF^{(\sk)}$  is nonincreasing;
		\item\label{prop::amm2} $F(X^{(\sk)})\rightarrow F^\infty$ and $\lF^{(k)}\rightarrow F^\infty$  as $\sk\rightarrow\infty$;
		\item\label{prop::amm3} $\|\Xkp-\Xk\|\rightarrow 0$ as $\sk\rightarrow\infty$ if $\zeta>\xi>0$;
		\item\label{prop::amm4} $\|\Xkh-\Xk\|\rightarrow 0$ as $\sk\rightarrow\infty$ if $\zeta>\xi>0$;
		\item\label{prop::amm5} if $\zeta \geq\xi>0$, then there exists $\epsilon > 0$ such that 
		\begin{equation}
			\nonumber
			\min\limits_{0\leq\sk< \mathsf{K}}\|\grad\, F(\Xkh)\|\leq 2\sqrt{\frac{1}{\epsilon}\cdot\dfrac{F(X^{(0)})-F^\infty}{{\sK+1}}}
		\end{equation}
		for any $\mathsf{K}\geq 0$;
		\item\label{prop::amm6} if $\zeta \geq\xi>0$, then $\grad\, F(\Xk)\rightarrow \0$ and $\grad\, F(\Xkh)\rightarrow \0$ as $\sk\rightarrow \infty$.
	\end{enumerate}
\end{prop}
\begin{proof}
	See \hyperapp{appendix::H}{H}.
\end{proof}

In spite of no master node, \cref{prop::amm} indicates that  $\ammd$  has provable convergence as long as each node $\alpha\in\AA$ can communicate with its neighbors $\beta\in \NN^\alpha$. Thus,  $\ammd$  eliminates the bottleneck of communication for distributed PGO without master node. In addition,  $\ammd$  also reduces unnecessary adaptive restarts compared to \cite{fan2020mm}, and thus makes better of Nesterov's acceleration and has faster convergence.  
%In addition, note that the $\ammd$ method results in 
%$F(\Xkp)\leq G(\Xkp|\Xk) \leq \sF^{(\sk)}$
%instead of
%$F(\Xkp)\leq G(\Xkp|\Xk) \leq F(\Xk)$ in \cite{fan2020mm}. Recalling $F(\Xk)\leq \sF^{(\sk)}$, we conclude that the $\ammd$ method prevents unnecessary adaptive restarts and mitigates the impacts that $G(\Xkp|\Xk)$ is an upper-bound of $F(\Xkp)$, which is expected to make better use of Nesterov's method for acceleration and have faster convergence than \cite{fan2020mm}. 