
In this section, we evaluate the performance of our MM methods ($\mm$, $\ammc$ and $\ammd$) for distributed PGO on the simulated \textsf{\small Cube} datasets and a number of 2D and 3D SLAM benchmark datasets \cite{rosen2016se}.  In terms of $\mm$, $\ammc$ and $\ammd$, $\eta$, $\xi$, $\zeta$, $\psi$ and $\phi$ in \cref{algorithm::amm,algorithm::mm,algorithm::ammc} are $5\times10^{-4}$, $1\times10^{-10}$, $1.5\times 10^{-10}$, $1\times 10^{-10}$ and $1\times10^{-6}$, respectively, for all the experiments. In addition, $\mm$, $\ammc$ and $\ammd$ can  take at most one iteration when solving \cref{eq::xG,eq::GYak} to improve the estimates.   All the experiments have been performed on a laptop with an Intel Xeon(R) CPU E3-1535M v6 and 64GB of RAM running Ubuntu 20.04.



\vspace{-0.5em}


\subsection{\textsf{\small Cube} Datasets}
\input{fig_cube}

\input{fig_cube_results}

In this section, we test and evaluate our MM methods for distributed PGO on $20$ simulated \textsf{\small Cube} datasets (see \cref{fig::cube})  with $5$, $10$ and $50$ robots. In the experiment, a simulated {\sf\small Cube} dataset has $12 \times 12 \times 12$ cube grids with $1$ m side length, a path of $3600$ poses along the rectilinear edge of the cube grid, odometric measurements between all the pairs of sequential poses, and loop-closure measurements between nearby but non-sequential poses that are randomly available with a probability of $0.1$. We generate the odometric and loop-closure measurements according to the noise models in \cite{rosen2016se} with an expected translational RMSE  of $0.02$ m and an expected angular RMSE of $0.02\pi$ rad. The centralized chordal initialization \cite{carlone2015initialization} is implemented such that distributed PGO with different number of robots have the same initial estimate. The maximum number of iterations is $1000$. 



We evaluate the convergence of $\mm$, $\ammc$ and $\ammd$ in terms of the relative suboptimality gap and Riemannian gradient norm. For reference, we also make comparisons against $\amm$ \cite{fan2020mm}. Note that $\amm$ is the original accelerated MM method for distributed PGO whose adaptive restart scheme is conservative and might  prohibit Nesterov's acceleration.

\textbf{Relative Suboptimality Gap.} We implement the certifiably-correct $\sesync$ \cite{rosen2016se} to get the globally optimal objective value $F^*$  of distributed PGO with the trivial loss kernel (\cref{example::trivial}), making it possible to compute the relative suboptimality gap $(F-F^*)/F^*$ where $F$ is the objective value for each iteration. The results are in \cref{fig::cube_f}.

\textbf{Riemannian Gradient Norm.}  We also compute the Riemannian gradient norm of distributed PGO with the trivial, Huber  and Welsch loss kernels in \cref{example::huber,example::GM,example::trivial} for evaluation. Note that it is difficult to find globally optimal solutions to distributed PGO if Huber and Welsch loss kernels are used. The results are in \cref{fig::cube_g_huber,fig::cube_g_gm,fig::cube_g_trivial}.



In \cref{fig::cube_g_gm,fig::cube_g_huber,fig::cube_f,fig::cube_g_trivial}, it can be seen that $\mm$, $\ammc$, $\ammd$ and $\amm$ have a faster convergence if the number of robots (nodes) decreases. This is expected since $G(X|X^{(k)})$ and $H(X|X^{(k)})$ in \cref{eq::G,eq::lG} result in tighter approximations for distributed PGO with fewer robots (nodes). In addition, \cref{fig::cube_g_gm,fig::cube_g_huber,fig::cube_g_trivial} suggest that the convergence rate of $\mm$, $\ammc$, $\ammd$ and $\amm$ also relies on the type of loss kernels. Nevertheless, $\ammc$, $\ammd$ and $\amm$ accelerated by Nesterov's method outperform  unaccelerated $\mm$  by a large margin for any number of robots and any types of loss kernels, which means that Nesterov's method improves the convergence of distributed PGO. In particular, Figs. \ref{fig::cube_f}(a), \ref{fig::cube_g_trivial}(a), \ref{fig::cube_g_huber}(a), \ref{fig::cube_g_gm}(a) indicate that $\ammd$  with $50$ robot still converges faster than $\mm$ with $5$ robots despite that the later has a much smaller number of robots. Therefore, we conclude that Nesterov's method accelerates the convergence of distributed PGO.



We emphasize the convergence comparisons of Nesterov's accelerated $\ammc$, $\ammd$ and $\amm$ that merely differ from each other by the adaptive restart schemes---$\ammc$ has an additional master node to aggregate information from all the robots (nodes), whereas $\ammd$ and $\amm$ are restricted to one inter-node communication round per iteration among neighboring robots (nodes). Notwithstanding limited local communication, as is shown in \cref{fig::cube_f,fig::cube_g_huber,fig::cube_g_gm}, $\ammd$ has a convergence rate comparable to that of $\ammc$ using a master node while being significantly faster than $\amm$.  {\highlight In particular,  $\ammd$ reduces adaptive restarts by $80\%$ to $95\%$ compared to $\amm$ on the \textsf{\small Cube} datasets}, and thus, is expected to make better use of Nesterov's acceleration. Since $\ammd$ and $\amm$ differ in the adaptive restart schemes, we attribute the faster convergence of $\ammd$ to its redesigned adaptive restart scheme. These results suggest that $\ammd$ is advantageous over other methods for very large-scale distributed PGO where computational and communicational efficiency are equally important.

\vspace{-0.5em}

%\input{table_dataset}



\vspace{-0.5em}
\subsection{Benchmark Datasets}\label{subsection::experiment::benchmark}
In this section, we evaluate our MM methods ($\mm$, $\ammc$ and $\ammd$) for distributed PGO on a number of 2D and 3D SLAM benchmark datasets \cite{rosen2016se} (see \datasetinfo). We use the trivial loss kernel and there are no outliers such that the globally optimal solution can be exactly computed with $\sesync$ \cite{rosen2016se}. For each dataset, we also make comparisons against $\sesync$ \cite{rosen2016se}, distributed Gauss-Seidel ($\dgs$) \cite{choudhary2017distributed} and the Riemannian block coordinate descent ($\rbcd$) \cite{tian2019distributed} method, all of which are the state-of-the-art algorithms for centralized and distributed PGO. The $\sesync$ and $\dgs$ methods use the recommended settings in \cite{choudhary2017distributed,rosen2016se}. We implement two Nesterov's accelerated variants of $\rbcd$  \cite{tian2019distributed}, i.e., one with greedy selection rule and adaptive restart ($\rbcdc$) and the other with uniform selection rule and fixed restart ($\rbcdd$)\footnote{In the experiments, we run $\rbcdd$ \cite{tian2019distributed} with fixed restart frequencies of 30, 50 and 100 iterations for each dataset and report the best results.}. As mentioned before, $\ammc$ and $\ammd$ can take at most one iteration when updating $\Xakp$ using \cref{eq::GYak,eq::xG}, which is similar to $\rbcdc$ and $\rbcdd$. An overview of the aforementioned methods is given in \cref{table::cmp_method}.

\input{table_cmp_method}

\input{table_cmp_dataset}

\textbf{Number of Iterations.} First, we examine the convergence of $\mm$, $\ammc$, $\ammd$, $\dgs$ \cite{choudhary2017distributed}, $\rbcdc$ \cite{tian2019distributed}  and $\rbcdd$ \cite{tian2019distributed} w.r.t. the number of iterations. The distributed PGO has 10 robots and all the methods are initialized with distributed Nesterov's accelerated chordal initialization \cite{fan2020mm}.

The objective values of each method with 100, 250 and 1000 iterations are reported in \cref{table::comp} and the reconstruction results using $\ammd$ are shown in \refbenchmark{1}{2}. For almost all the benchmark datasets, $\ammc$ and $\ammd$ outperform the other methods ($\mm$, $\dgs$, $\rbcdc$ and $\rbcdd$). While $\rbcdc$ and $\rbcdd$ have similar performances in four relatively  easy datasets---{\sf CSAIL}, {\sf sphere}, {\sf torus} and {\sf grid}---$\ammc$ and $\ammd$ achieve much better results in the other more challenging datasets in particular if there are no more than 250 iterations. As discussed later, $\ammc$ and $\ammd$ have faster convergence to more accurate estimates without any extra computation and communication in contrast to $\rbcdc$ and $\rbcdd$. Last but not the least, \cref{table::comp} demonstrates that the accelerated $\ammc$ and $\ammd$ converge significantly faster than the unaccelerated $\mm$, which further validates the usefulness of Nesterov's method.

\figbenchmark

\input{fig_succ_iter}

We also compute the performance profiles \cite{dolan2002benchmarking} based on the number of iterations. Given a  tolerance $\Delta\in(0,\,1]$, the objective value threshold $F_{\Delta}(p)$ of a PGO problem $p$
is 
\vspace{-0.15em}
\begin{equation}\label{eq::Fdel}
F_{\Delta}(p) = F^* + \Delta\cdot\big(F^{(0)}-F^*\big)
\vspace{-0.15em}
\end{equation}
where  $F^{(0)}$ and $F^*$ are the initial and globally optimal objective values, respectively. Let $I_{\Delta}(p)$ denote the minimum number of iterations that  a PGO method takes to reduce the objective value to $F_\Delta(p)$, i.e.,
\vspace{-0.15em}
\begin{equation}
\nonumber
I_{\Delta}(p)\triangleq\min_{\sk}\big\{\sk\geq 0| F^{(\sk)}\leq F_{\Delta}(p)\big\}
\vspace{-0.15em}
\end{equation}
where $F^{(\sk)}$ is the objective value at iteration $\sk$. Then, for a problem set $\mathcal{P}$, the performance profiles of a PGO method is the percentage of problems solved w.r.t. the number of iterations $\sk$:
\vspace{-0.25em}
\begin{equation}
\nonumber
\substack{\text{\normalsize percentage of problems solved}\\ \text{\normalsize at iteration $\sk$}} \triangleq \frac{\big|\{p\in\mathcal{P}|I_{\Delta}(p)\leq \sk\}\big|}{|\mathcal{P}|}.
\end{equation}

The performance profiles based on the number of iterations over a variety of 2D and 3D SLAM benchmark datasets (see \datasetinfo) are shown in \cref{fig::succ_iter}. The tolerances evaluated are $\Delta=1\times10^{-2}$, $5\times10^{-3}$, $1\times10^{-3}$ and $1\times10^{-4}$. We report the performance of $\mm$, $\ammc$, $\ammd$, $\dgs$ \cite{choudhary2017distributed}, $\rbcdc$ \cite{tian2019distributed} and $\rbcdd$ \cite{tian2019distributed} for distributed PGO with 10 robots (nodes). As expected, $\ammc$ and $\ammc$ dominates the other methods ($\mm$, $\dgs$, $\rbcdc$ and $\rbcdd$) in terms of the convergence for all the tolerances $\Delta$, which means that  both of them are better choices for distributed PGO. 


In \cref{table::comp} and \cref{fig::succ_iter}, we emphasize that $\ammd$ requiring no master node achieves comparable performance to that of $\ammc$ using a master node, and is a lot better than all the other methods with a master node ($\rbcdc$) and without ($\mm$, $\dgs$ and $\rbcdd$). Even though $\rbcdc$ and $\rbcdd$ are similarly accelerated with Nesterov's method, we remark that $\rbcdd$ without a master node suffers a great performance drop compared to $\rbcdc$, and more importantly, $\rbcdd$ has no convergence guarantees to first-order critical points. These results reverify that $\ammd$ is more suitable for very large-scale distributed PGO with limited local communication. 

 Note that all of $\mm$, $\ammc$, $\ammd$, $\dgs$ \cite{choudhary2017distributed}, $\rbcdc$ \cite{tian2019distributed} and $\rbcdd$ \cite{tian2019distributed} have to exchange poses of inter-node measurements with the neighbors, and thus, need almost the same amount of communication per iteration. However, \cref{fig::succ_iter} indicates that $\ammc$ and $\ammd$ have much faster convergence in terms of the number of iterations, which also means less communication for the same level of accuracy. In addition, $\rbcdc$ and $\rbcdd$ have to keep part of the nodes in idle during optimization and rely on red-black coloring for block aggregation and random sampling for block selection, which induce additional computation and communication. In contrast, neither $\ammc$ nor $\ammd$ has any extra practical restrictions except \cref{assumption::loss,assumption::mm,assumption::neighbor,assumption::master}.

\input{fig_succ_time}

\textbf{Optimization Time.} We evaluate the optimization time of $\ammc$ and $\ammd$ with different numbers of robots (nodes) against the centralized baseline $\sesync$ \cite{rosen2016se}. To improve the  time efficiency of our methods, $\Xakp$ in \cref{eq::xG,eq::GYak}  uses the same rotation as $\Xakh$ in \cref{eq::xlG,eq::HYak} and merely updates the translation. Due to the different numbers of robots (nodes), the centralized chordal initialization \cite{carlone2015initialization} is used for all the runs.

Similar to the number of iterations, we use the performance profiles to evaluate $\ammc$ and $\ammd$ in terms of the optimization time. Recall from \cref{eq::Fdel} the objective value threshold $F_{\Delta}(p)$ where $p$ is the PGO problem and $\Delta\in(0,\,1]$ is the tolerance. Since the average optimization time per node is directly related with the speedup, we measure the efficiency of a distributed PGO method with $N$ nodes by computing the average optimization time $T_{\Delta}(p,N)$ that each node takes to reduce the objective value to $F_{\Delta}(p)$:
\vspace{-0.25em}
\begin{equation}
\nonumber
T_{\Delta}(p,N)=\tfrac{T_\Delta(p)}{N},
\vspace{-0.25em}
\end{equation}
where $T_\Delta(p)$ denotes the total optimization time of all the $N$ nodes. We remark that the centralized optimization method has $N=1$ node and $T_{\Delta}(p,N)=T_{\Delta}(p)$. Let $T_{\sesync}$ denote the optimization time that $\sesync$ needs to find the globally optimal solution. The performance profiles assume a distributed PGO method solves problem $p$  for some $\ratio\in[0,\,+\infty)$ if $T_{\Delta}(p,N)\leq \ratio\cdot T_{\sesync}$. Note that $\ratio$ is the scaled average optimization time per node and $\sesync$ solves problem $p$ globally at $\ratio=1$. Then, as a result of \cite{dolan2002benchmarking}, the performance profiles evaluate the speedup of distributed PGO methods for a given optimization problem set $\mathcal{P}$ using  the percentage of problems solved w.r.t. the scaled average optimization time per node $\ratio\in[0,\,+\infty)$: 
\vspace{-0.25em}
\begin{equation}
	\nonumber
	\substack{\text{\normalsize percentage of problems}\\ \text{\normalsize  solved at $\ratio$}} \triangleq\! \frac{\big|\{p\in\mathcal{P}|T_{\Delta}(p,\,N)\!\leq\! \ratio\cdot T_{\sesync}\}\big|}{|\mathcal{P}|}.
\end{equation}

\cref{fig::succ_time} shows the performance profiles based on the scaled average optimization time per node. The tolerances evaluated are $\Delta=1\times10^{-2}$, $1\times10^{-3}$, $1\times10^{-4}$ and $1\times10^{-5}$. We report the performance of $\ammc$ and $\ammd$ with $10$, $25$ and $100$ robots (nodes). For reference, we also evaluate the performance profile of the centralized PGO baseline $\sesync$ \cite{rosen2016se}.  As the results demonstrate, $\ammc$ and $\ammd$ are significantly faster than $\sesync$ \cite{rosen2016se} in most cases for modest accuracies of $\Delta=1\times 10^{-2}$ and $\Delta=1\times 10^{-3}$, for which the only challenging case is the {\sf CSAIL} dataset, whose chordal initialization is already very close to the globally optimal solution.  In spite of the performance decline for smaller tolerances of $\Delta=1\times 10^{-4}$ and $\Delta=1\times 10^{-5}$, $\ammc$ and $\ammd$ with 100 robots (nodes) still achieve a $2.5\sim 20\mathrm{x}$ speedup of optimization time over $\sesync$ for more than $70\%$ of the benchmark datasets, not to mention that the average optimization time per node of $\ammc$ and $\ammd$ decreases with the number of robots (nodes). {\highlight Note that the communication overhead is not considered in the experiments.} Nevertheless  \cref{fig::succ_time} indicates that $\ammc$ and $\ammd$ are promising  as fast parallel backends for very large-scale PGO and real-time multi-robot SLAM.

In summary, $\ammc$ and $\ammd$ achieve the state-of-the-art performance for distributed PGO and enjoy a significant multi-node speedup compared to the centralized baseline \cite{rosen2016se} for modestly but sufficiently accurate estimates.

\subsection{Robust Distributed PGO} 

In this section, we evaluate the robustness of $\ammd$ against the outlier inter-node loop closures. Similar to \cite{chang2020kimera,lajoie2020door}, we first use the distributed pairwise consistent measurement set maximization algorithm ($\pcm$) \cite{mangelson2018pairwise} to reject spurious inter-node loop closures and then solve the resulting distributed PGO using $\ammd$ with the trivial, Huber  and Welsch loss kernels in \cref{example::GM,example::trivial,example::huber} . 

We implement $\ammd$ on the 2D {\sf intel} and 3D {\sf garage} datasets (see \datasetinfo) with 10 robots (nodes). For each dataset, we add false inter-node loop closures with uniformly random rotation and translation errors in the range of $[0,\,\pi]$ rad and $[0,\,5]$ m, respectively. In addition, after the initial outlier rejection using the $\pcm$ algorithm \cite{mangelson2018pairwise}, we initialize $\ammd$ with distributed Nesterov's accelerated chordal initialization \cite{fan2020mm} for all the loss kernels. 


The absolute trajectory errors (ATE)  of $\ammd$ w.r.t. different {\highlight outlier ratios} of inter-node loop closures are  in \cref{fig::outlier}. The ATEs are computed against the outlier-free results of $\sesync$ \cite{rosen2016se}  and  averaged over 10 Monte Carlo runs. 

\input{fig_outlier}

In \cref{fig::outlier}(a), $\pcm$ \cite{mangelson2018pairwise} rejects most of the outlier inter-node loop closure for the {\sf intel} dataset and $\ammd$ solves the distributed PGO problems regardless of the loss kernel types and {\highlight outlier ratios}. Note that $\ammd$ with the Welsch loss kernel has larger ATEs (avg. $0.057$ m) against $\sesync$ \cite{rosen2016se} than those with the trivial and Huber loss kernels (avg. $0.003$ m), and we argue that this is related to the loss kernel types. The ATEs are evaluated based on $\sesync$ using the trivial loss kernel, which is in fact identical/similar to distributed PGO with the trivial and Huber loss kernels but different from that with the Welsch loss kernel. Thus, the estimates from the trivial and Huber loss kernels are expected to be more close to those of $\sesync$, which result in smaller ATEs compared to the Welsch loss kernel  if no outliers.

\figgarage

For the more challenging {\sf garage} dataset, as is shown in \cref{fig::outlier}(b), $\pcm$ fails for {\highlight outlier ratios} over $0.4$, and further, distributed PGO with the trivial and Huber loss kernels results in ATEs as large as $65$ m. In contrast, distributed PGO with the Welsch loss kernel still successfully estimates the poses with an average ATE of $2.5$ m despite the existence of  outliers---note that the {\sf garage} dataset has a trajectory over $7$ km. For the {\sf garage} dataset, a qualitative comparison of distributed PGO with different loss kernels is also presented in \refgarage, where the Welsch loss kernel still has the best performance. The results are not surprising since the Welsch loss kernel is known to be more robust against outliers than the other two loss kernels \cite{barron2019cvpr}.  



The results above indicate that our MM methods can be applied to distributed PGO in the presence of outlier inter-node loop closures when combined with robust loss kernels like Welsch and other outlier rejection techniques like $\pcm$ \cite{mangelson2018pairwise}. In addition, we emphasize again that our MM methods have provable convergence to first-order critical points for a broad class of robust loss kernels, whereas the convergence guarantees of existing distributed PGO methods \cite{choudhary2017distributed,tian2019distributed,tron2014distributed,eric2020geod} are restricted to the trivial loss kernel.