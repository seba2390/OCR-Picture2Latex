Pose graph optimization (PGO) is a nonlinear and nonconvex optimization problem estimating unknown poses from noisy relative pose measurements. PGO associates each pose  with a vertex and each relative pose measurement with an edge such that the optimization problem is well represented through a graph. PGO has important applications in a number of areas, including but not limited to robotics \cite{cadena2016past,rosen4advances,thrun2005probabilistic}, autonomous driving \cite{geiger2012we}, and computational biology \cite{singer2011angular,singer2011three}. Recent advances \cite{rosen2014rise,rosen2016se,rosen2020scalable,dellaert2012factor,fan2019cpl,fan2020cpl,carlone2016planar,carlone2015lagrangian,grisetti2009nonlinear,briales2017cartan} suggest that PGO can be well solved using iterative optimization. Nevertheless, the aforementioned techniques \cite{rosen2014rise,rosen2016se,rosen2020scalable,dellaert2012factor,carlone2016planar,carlone2015lagrangian,grisetti2009nonlinear,fan2019cpl,briales2017cartan,fan2020cpl}  are difficult to distribute across a network due to communication and computational limitations, and  are only applicable to small- and medium-sized problems with at most tens of thousands poses.  In addition, their centralized pipelines are equivalent to using a master node to aggregate information from the entire network, and thus, fail to meet  privacy requirements one may wish to impose \cite{li2019coordinated,zhang2019complete}.

In multi-robot simultaneous localization and mapping (SLAM)  \cite{cunningham2010ddf,aragues2011multi,cunningham2013ddf,saeedi2016multiple,dong2015distributed,lajoie2020door,cieslewski2018data,tchuiev2020distributed,chang2020kimera,tian2021kimera}, each robot estimates not only its own poses but those of the others as well to build an environment map. Even though such a problem can be solved by PGO,  communication between  robots is restricted and multi-robot SLAM has more unknown poses than single-robot SLAM. Thus, instead of using centralized PGO \cite{rosen2014rise,rosen2016se,rosen2020scalable,dellaert2012factor,carlone2016planar,fan2020cpl,fan2019cpl,carlone2015lagrangian,grisetti2009nonlinear,briales2017cartan}, it is more reasonable to formulate this large-sized estimation problem involving multiple robots as distributed PGO---each robot in multi-robot SLAM is represented as a node and two nodes (robots) are said to be neighbors if there exists a noisy relative pose measurement between them (a more detailed description of distributed PGO can be found in \cref{section::problem}). In most cases, it is assumed that inter-node communication only occurs between neighboring nodes and most of these iterative optimization methods are infeasible  due to the expensive communication cost of solving linear system and performing line search \cite{rosen2014rise,rosen2016se,rosen2020scalable,dellaert2012factor,carlone2016planar,carlone2015lagrangian,grisetti2009nonlinear,fan2019cpl,briales2017cartan,fan2020cpl}, which renders distributed PGO  more challenging than centralized PGO.

In this paper, we propose majorization minimization (MM) methods \cite{hunter2004tutorial,sun2016majorization} for distributed PGO. As the name would suggest, MM methods have two steps. First, in the majorization step, we construct a surrogate function that majorizes the objective function, i.e., the surrogate function is greater to the objective function except for the current iterate where both of them attain the same value. Then, in the minimization step, we minimize the surrogate function instead of the original objective function to improve the iterate. MM methods remain difficult, albeit straightforward, in practical use, e.g., the surrogate function, whose construction and minimization can not be more difficult than solving the optimization problem itself, is usually unknown, and MM methods might fail to converge and suffer from slow convergence. Thus, the implementation of MM methods on large-scale, complicated and nonconvex optimization problems like distributed PGO is nontrivial, and inter-node communication requirements impose extra restrictions making it more so. All of these issues  are  addressed both theoretically and empirically in the rest of this paper.

This paper extends the preliminary results in \cite{fan2019proximal,fan2020mm}, where we developed MM methods for centralized and distributed PGO that are guaranteed to converge to first-order critical points. In \cite{fan2019proximal,fan2020mm}, we also introduced and elaborated on the use of Nesterov's method \cite{nesterov1983method,nesterov2013introductory} and adaptive restart \cite{o2015adaptive} for the first time to accelerate the convergence of PGO. Beyond the initial results in \cite{fan2019proximal,fan2020mm}, this paper presents completely redesigned MM methods for distributed PGO and provides more comprehensive theoretical and  empirical results. In particular, our MM methods in this paper are capable of handling a broad class of robust loss kernels, no longer require each iteration to attain a local optimal solution to the surrogate function for the convergence guarantees, and adopt a novel adaptive restart scheme for distributed PGO without a master node to make full use of Nesterov's acceleration.

In summary, the contributions of this paper are the follows:
\begin{enumerate}
\item We derive a class of surrogate functions that suit well with MM methods for distributed PGO. These surrogate functions apply to a broad class of robust loss kernels in robotics and computer vision.
\item We develop MM methods for distributed PGO that are guaranteed to converge to first-order critical points under mild conditions. Our MM methods for distributed PGO  implement a novel update rule such that each iteration does not have to minimize the surrogate function to a local optimal solution.
\item We leverage Nesterov's methods and adaptive restart to accelerate MM methods for distributed PGO and achieve significant improvement in convergence without any compromise of theoretical guarantees.
\item We present a decentralized adaptive restart scheme to make full use of Nesterov's acceleration such that accelerated MM methods for distributed PGO without a master node are almost as fast as those requiring a master node.
\end{enumerate}

The rest of this paper is organized as follows. \cref{section::work} reviews the state-of-the-art methods for distributed PGO. \cref{section::notation} introduces mathematical notation and preliminaries that are used in this paper. \cref{section::problem} formulates the problem of distributed PGO. \cref{section::loss,section::major_pgo} present surrogate functions for individual loss terms and the overall distributed PGO, respectively, which are fundamental to our MM methods. \cref{section::mm,section::amm,section::ammc} present unaccelerated and accelerated MM methods for distributed PGO that are guaranteed to converge to first-order critical points, which are the major contributions of this paper. \cref{section::experiemnt} implements our MM methods for distributed PGO on a number of simulated and real-world SLAM datasets and make extensive comparisons against existing state-of-the-art methods \cite{choudhary2017distributed,tian2019distributed}. \cref{section::conclusion} concludes this paper and discusses future work.