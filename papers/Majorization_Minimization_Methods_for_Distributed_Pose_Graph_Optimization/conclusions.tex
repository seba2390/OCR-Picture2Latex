We presented majorization minimization (MM) methods for distributed PGO that has important applications in multi-robot SLAM. Our MM methods had provable convergence for a broad class of robust loss kernels in robotics and computer vision. Furthermore, we elaborated on the use of Nesterov's method and adaptive restart for acceleration and developed accelerated MM methods $\ammc$ and $\ammd$ without sacrifice of convergence guarantees. In particular, we designed a novel adaptive restart scheme making  $\ammd$  without a master node comparable to  $\ammc$  using a master node for information aggregation. The extensive experiments on numerous 2D and 3D SLAM datasets indicated that our MM methods outperformed existing state-of-the-art methods and  robustly handled distributed PGO with outlier inter-node loop closures.

Our MM methods for distributed PGO can be improved as  follows.  A more tractable and robust initialization technique is definitely beneficial to the accuracy and efficiency of distributed PGO. Even though our MM methods have  reliable performances against outliers, a more complete theoretical analysis for robust distributed PGO is still necessary. We might also extend our MM methods for differentiable distributed PGO \cite{pineda2022theseus}.  In addition, our MM methods can be implemented as local solvers for distributed certifiably correct PGO \cite{tian2019distributed}  to handle poor or random initialization. Since all the nodes are now assumed to be synchronized, it is necessary and useful to extend our MM methods for asynchronous distributed PGO. {\highlight Lastly, real multi-robot tests might make the results of our MM methods more convincing where not only the optimization time but also the communication overhead can be validated.}

