{\highlight
	In the last several decades, a number of accelerated first-order optimization methods have been proposed \cite{nesterov1983method,nesterov2013introductory}. Even though most of them were originally developed for convex optimization, it has been recently found that these accelerated methods also work well for nonconvex optimization \cite{ghadimi2016accelerated,jin2018accelerated,li2015accelerated}. In our previous works \cite{fan2019proximal,fan2020mm}, we proposed to use Nesterov's method to accelerate  distributed PGO, which yield much faster convergence. Since  $\mm$  is a first-order optimization method, it is possible to exploit Nesterov's method for acceleration. In \cref{section::ammc::nesterov}, we implement Nesterov's method to accelerate MM methods for distributed PGO. Then, in \cref{section::ammc::adaptive}, we introduce the adaptive restart scheme \cite{o2015adaptive}  to guarantee the convergence if a master node exists. Lastly, in \cref{section::ammc::algorithm}, we propose the accelerated MM method for distributed PGO with master and  prove that such a method converges to first-order critical points.
}  

\vspace{-0.5em}
\subsection{Nesterov's Method}\label{section::ammc::nesterov}
According to \cref{prop::G,prop::lG}, \cref{eq::minG,eq::minlG} are {\highlight proximal operators} of  $F(X)$, making it possible to implement Nesterov's method \cite{nesterov1983method,nesterov2013introductory} for acceleration and resulting in the following update rule for $\Xakh$ and $\Xakp$:
\begin{equation}\label{eq::sk}
	s^{\akp}=\tfrac{\sqrt{4{s^{\ak}}^2+1}+1}{2},
\end{equation}
\begin{equation}\label{eq::gammak}
	\lambda^{\ak}= \tfrac{s^{\ak}-1}{s^{\akp}},
\end{equation}
\begin{equation}\label{eq::Yak}
	Y^{\ak}= X^{\ak}+\lambda^{\ak}\cdot\big(\Xak-\Xakm\big),
\end{equation}
\begin{equation}\label{eq::HYak}
	\Xakh=\arg\min\limits_{\Xa\in\XX^\alpha }\lG^\alpha(\Xa|\Yk),
\end{equation}
\begin{equation}\label{eq::GYak}
	\Xakp=\arg\min\limits_{\Xa\in\XX^\alpha }G^\alpha(\Xa|\Yk).
\end{equation}
In \cref{eq::HYak,eq::GYak}, $G^\alpha(\cdot|\Yk):\XX^\alpha\rightarrow\R$ and $\lG^\alpha(\cdot|\Yk):\XX^\alpha\rightarrow\R$ are surrogate functions at $\Yak$:
\begin{multline}\label{eq::GXY}
	G^\alpha(X^\alpha|Y^{(\sk)}) =\frac{1}{2}\|\Xa-\Yak\|_{\nGamma^{\ak}}^2+\\
	\big\langle\nabla_{\Xa} F(\Yk),\,{\Xa-\Yak}\big\rangle,
\end{multline}
\vspace{-2em}
\begin{multline}\label{eq::lGXY}
	\lG^\alpha(X^\alpha|Y^{(\sk)}) =\frac{1}{2}\|\Xa-\Yak\|_{\lnGamma^{\ak}}^2+\\
	\big\langle\nabla_{\Xa} F(\Yk),\,{\Xa-\Yak}\big\rangle,
\end{multline}
where $\nGamma^{\ak}$ and $\lnGamma^{\ak}$  are the same as these in $G^\alpha(\cdot|\Xk)$ and $\lG^\alpha(\cdot|\Xk)$ in \cref{eq::GMa,eq::lGMa}. 

The key insight of Nesterov's method is to exploit the momentum $\Xak-\Xakm$ for acceleration, which is essentially governed by \cref{eq::sk,eq::gammak,eq::Yak}. Note that Nesterov's method using \cref{eq::sk,eq::gammak,eq::Yak,eq::HYak,eq::GYak} is  equivalent to \cref{eq::xG,eq::xlG} when $s^{\ak}=1$ and $\lambda^{\ak}=0$, and then increasingly affected by the momentum as $s^{\ak}$ and $\lambda^{\ak}$ increase.

Nesterov's method is known to converge quadratically for convex optimization while the unaccelerated MM method only has linear convergence  \cite{nesterov1983method,nesterov2013introductory}. Even though distributed PGO is a nonconvex optimization problem, similar to \cite{fan2019proximal,fan2020mm},  \cref{eq::gammak,eq::Yak,eq::GYak,eq::HYak,eq::sk} using  Nesterov's method for acceleration  empirically have significant speedup while introducing  almost no extra computation or communication compared to the $\mm$ method. Thus, it is preferable to adopt Nesterov's method to accelerate distributed PGO.

\vspace{-0.5em}
\subsection{Adaptive Restart}\label{section::ammc::adaptive}
In spite of faster convergence, Nesterov's accelerated distributed PGO using \cref{eq::sk,eq::gammak,eq::Yak,eq::GYak,eq::HYak} is no longer nonincreasing, and might fail to converge due to the nonconvexity of PGO. Fortunately, such a problem can be remedied with an adaptive restart scheme \cite{o2015adaptive,fan2020mm,fan2019proximal} as the following.

Let $\sF^{(\sk)}$ be an exponential moving averaging of $F(X^{(0)})$, $F(X^{(1)})$, $\cdots$, $F(\Xk)$ :
\begin{equation}\label{eq::lFk}
	\sF^{(\sk)}\triangleq \begin{cases}
		F(X^{(0)}),& \sk=0,\\
		(1-\eta)\cdot\sF^{(\skm)}+\eta\cdot F(\Xk),&\text{otherwise}
	\end{cases}
\end{equation}
where $\eta\in(0,\,1]$.  {\highlight Following \cite{fan2019proximal,li2015accelerated,zhang2004nonmonotone}, the adaptive restart scheme guarantees the convergence by keeping $F(\Xkp)\leq \lF^{(\sk)}$. Even though it is not obvious, $F(\Xkp)\leq \lF^{(\sk)}$  can be achieved with the following steps}:
\begin{enumerate}[leftmargin=0.45cm]
\item  Update $\Xkh$ and $\Xkp$ by solving \cref{eq::GYak,eq::HYak} for each node $\alpha\in\AA$;
\item If $F(\Xkh)>\sF^{(\sk)}$, update $\Xkh$ again by solving  \cref{eq::xlG} for each node $\alpha\in\AA$;
\item If $F(\Xkp)>\sF^{(\sk)}$, update $\Xkp$ again by solving  \cref{eq::xG}  and reduce $s^{\akp)}$  for each node $\alpha\in\AA$.  
\end{enumerate}
{\highlight Due to space limitation, the complete analysis of the adaptive restart scheme  is left in  \hyperapp{appendix::F}{F} where more details are presented.}

\begin{remark}
Since $\eta\in(0,\,1]$ in \cref{eq::lFk}, then $\sF^{(\skp)}\leq \sF^{(\sk)}$ as long as $F(\Xkp)\leq \sF^{(\sk)}$.  In \hyperapp{appendix::F}{F}, we have proved that $F(\Xkp)\leq \sF^{(\sk)}$ hods if  $\Xkh$ and $\Xkp$ are updated with  \cref{eq::xG,eq::xlG} for each node $\alpha\in\AA$. Therefore,  the adaptive restart scheme above results in a nonincreasing sequence of $\sF^{(\sk)}$.  Furthermore,  \hyperapp{appendix::F}{F} indicates that such an adaptive restart scheme is sufficient to guarantee the convergence to first-order critical points under mild conditions.
\end{remark}


Note that one has to aggregate information across the network to evaluate and compare $\sF^{(\sk)}$, $F(\Xkh)$, $F(\Xkp)$ using \cref{eq::lFk,eq::F}. Thus, a master node capable of communicating with each node $\alpha\in\AA$ is required. In the rest of this section, we make the following assumption about the existence of such a master node.

\begin{assumption}\label{assumption::master}
	There is a master node to retrieve $\Xak$ and $\Xakh$ from each node $\alpha\in\AA$ and evaluate $\sF^{(\sk)}$, $F(\Xkh)$, $F(\Xkp)$.  
\end{assumption}

\subsection{Algorithm}\label{section::ammc::algorithm}
\input{algorithm_amm_c}
\input{algorithm_amm_c_x}

Implementing Nesterov's method  and the adaptive restart scheme, we obtain the $\ammc$ method for distributed PGO (\cref{algorithm::ammc}), where ``$*$'' indicates  the existence of a master node. 

The outline of  $\ammc$  is as follows:
\begin{enumerate}[leftmargin=0.45cm]
\item In lines~\ref{line::alg3::sk}, \ref{line::alg3::Yk} of \cref{algorithm::ammc}, each node $\alpha$ computes $\Yk$ for Nesterov's acceleration that is related with $s^{\ak}\in[1,\,\infty)$ and $\lambda^{\ak}\in[0,\,1)$.
\vspace{0.25em}
\item  In line~\ref{line::alg4::comm} of \cref{algorithm::amm_c_x}, each node $\alpha$ performs one inter-node communication round to retrieve $\Xbk$ and $Y^{\bk}$ from its neighbors $\beta\in\NN^\alpha$.
\item  In line~\ref{line::alg3::mcomm1} of \cref{algorithm::ammc} and lines~\ref{line::alg4::mcomm1}, \ref{line::alg4::mcomm2}, \ref{line::alg4::mcomm3} of \cref{algorithm::amm_c_x}, each node $\alpha$ performs one inter-node communication round to send $\Xakh$ and $\Xakp$ to the master node.
\vspace{0.2em}
\item In lines~\ref{line::alg4::DFaYk} of \cref{algorithm::amm_c_x}, each node $\alpha$ evaluates $\nGammak$, $\lnGammaak$, $\nabla_{X^{\alpha}} F(\Xk)$, $\nabla_{X^{\alpha}} F(\Yk)$ using $\Xak$, $\Yak$, $\Xbk$, $Y^{\bk}$ where $\beta\in\NN^\alpha$ are neighbors of node $\alpha$.
\item In lines~\ref{line::alg3::lF0}, \ref{line::alg3::lFk} of \cref{algorithm::ammc} and lines~\ref{line::alg4::Fxk}, \ref{line::alg4::Fxkh}, \ref{line::alg4::Fxkp} of \cref{algorithm::amm_c_x},  the master node evaluates  $\sF^{(\sk)}$, $F(\Xkh)$, $F(\Xkp)$ that are used for adaptive restart.
\vspace{0.2em}
\item In lines~\ref{line::alg4::restart_s1} to  \ref{line::alg4::restart_e2} of \cref{algorithm::amm_c_x}, the master node performs adaptive restart  to keep $F(\Xkh)\leq \sF^{(\sk)}$ and $F(\Xkp)\leq \sF^{(\sk)}$, which yields a nonincreasing sequence of $\lF^{(\sk)}$ to guarantee the convergence. 
\vspace{0.25em}
\item In lines~\ref{line::alg4::xG1}, \ref{line::alg4::xG2} of \cref{algorithm::amm_c_x}, note that $\Xakp$ does not have to be a local optimal solution to \cref{eq::xG}.
%\item In lines~\ref{line::alg4::restart_s1} to  \ref{line::alg4::restart_e2} of \cref{algorithm::amm_c_x}, $\psi>0$ guarantees that $F(\Xkh)$ and $F(\Xkp)$ yield sufficient improvement over $\sF^{(\sk)}$ in terms of $\|\Xkh-\Xk|$ and $\|\Xkp-\Xk\|$.
\vspace{0.25em}
\item In lines~\ref{line::alg4::restart_s3} to \ref{line::alg4::restart_e3} of \cref{algorithm::amm_c_x}, $F(\Xkp)$ is guaranteed to yield sufficient improvement over $\sF^{(\sk)}$ compared to $F(\Xkh)$. %Note that $\phi>0$ is recommended to  set close to zero.
\end{enumerate}
In spite of acceleration, $\ammc$  converges to first-order critical points  as the following proposition states.

\begin{prop}
{\highlight If \cref{assumption::mm,assumption::loss,assumption::neighbor,assumption::master} hold, $\psi>0$ and $\phi>0$,}  then for a sequence of $\{\Xk\}$ and $\{\Xkh\}$ generated by \cref{algorithm::ammc}, we have
\begin{enumerate}[(a)]
	\item\label{prop::ammc1}  $\lF^{(\sk)}$  is nonincreasing;
	\item\label{prop::ammc2}  $F(X^{(\sk)})\rightarrow F^\infty$ and $\lF^{(k)}\rightarrow F^\infty$  as $\sk\rightarrow\infty$;
	\item\label{prop::ammc3} $\|\Xkp-\Xk\|\rightarrow 0$ as $\sk\rightarrow\infty$ if $\xi>0$ and $\zeta>0$;
	\item\label{prop::ammc4} $\|\Xkh-\Xk\|\rightarrow 0$ as $\sk\rightarrow\infty$ if $\zeta\geq\xi>0$;
	\item\label{prop::ammc5} if $\zeta \geq\xi>0$, then there exists $\epsilon > 0$ such that 
	\begin{equation}
		\nonumber
		\min\limits_{0\leq\sk< \mathsf{K}}\|\grad\, F(\Xkh)\|\leq 2\sqrt{\frac{1}{\epsilon}\cdot\dfrac{F(X^{(0)})-F^\infty}{{\sK+1}}}
	\end{equation}
	for any $\mathsf{K}\geq 0$;
	\item\label{prop::ammc6} if $\zeta > \xi> 0$, then 
	$\grad\, F(\Xk)\rightarrow \0$ and
	$\grad\, F(\Xkh)\rightarrow \0$ as $\sk\rightarrow \infty$.
\end{enumerate}
\label{prop::ammc}
\end{prop}
\begin{proof}
	See \hyperapp{appendix::F}{F}.
\end{proof}


\begin{remark}
If $\eta=1$  in \cref{eq::lFk}, $F(\Xk)=\sF^{(\sk)}$, and $F(\Xk)$ is also nonincreasing according to \cref{prop::ammc}\ref{prop::ammc1}. While $F(\Xk)$  might fail to be nonincreasing, we still recommend to choose $\eta\ll 1$  that empirically yields fewer adaptive restarts and faster convergence for distributed PGO.
\end{remark}

\begin{remark}
$\psi>0$ and $\phi>0$ in  \cref{algorithm::amm_c_x} guarantee that $F(\Xkh)$ and $F(\Xkp)$ yield sufficient improvement over $\sF^{(\sk)}$ in terms of $\|\Xkh-\Xk\|$ and $\|\Xkp-\Xk\|$, and are recommended to set close to zero to avoid unnecessary adaptive restarts and make full use of Nesterov's  acceleration.
\end{remark}

%\begin{remark}
%We obtain from \cref{eq::lFk} that $\sF^{(\sk)}=F(\Xk)$ if $\eta=1$, which and  \cref{prop::ammc}\ref{prop::ammc1} suggest $F(\Xk)$ is nonincreasing. On the other hand, even though the monotonicity of $F(\Xk)$ no longer holds if $\eta<1$, \cref{prop::ammc} indicates that $F(\Xk)$ still converges to a local optimal solution.
%\end{remark}


