\section{Theoretical results} \label{sec:theory}

The main theoretical results of this paper are that the LA-MCMC algorithm described in Section~\ref{sec:numerics} inherits approximately the same $1/T$ convergence rate for the mean squared error as MCMC with exact evaluations and that our slight tweak to the acceptance probably allows LA-MCMC to converge even for heavier tailed distributions. We summarize the results and their implications here and refer to Appendix \ref{app:theory} for detailed discussion.

Denote by $K$ the transition kernel of a discrete-time Markov chain on $\mathbb{R}^{d}$ with unique stationary distribution $\pi_{X}$ that has density $\pi(x)$. We wish to study the stochastic processes that approximate $K$, despite not being Markov chains. Note that most of the calculations in this section are similar to those in the ``approximate'' Markov chain literature (see, e.g.,  \citet{Johndrowetal2015,Medinaetal2018,PillaiSmith2014,Rudolfetal2018}).  The biggest differences are:
\begin{enumerate}
    \item Our processes are not quite Markov chains. This does not substantially change any calculations, but does require us to be slightly more careful in a few steps.
    \item We make slightly atypical assumptions about our Lyapunov functions. These are justified in Section \ref{SubsecInheritLyap}.
\end{enumerate}

The purpose of the ``tweak'' in \eqref{eq:Lypunov-correction} was to force our main algorithm to satisfy the same Lyapunov condition as the baseline MCMC algorithm that it is approximating. To be more precise, we assume that the Metropolis-Hastings transition kernel $K$ associated with proposal kernel $q_t$ and target distribution $\pi_{X}$ satisfies:
\begin{assumption} (Lyapunov inequality). There exists $V: \mathbb{R}^{d} \to [1, \infty)$ and constants $0 < \alpha \leq 1$ and $0 \leq \beta < \infty$ so that 
\begin{equation}
\label{eq:lyapunov}
    (K V)(x, \cdot) \leq (1-\alpha) V(x) + \beta
\end{equation}
for all $x \in \mathbb{R}^{d}$. 
\label{assumption:lyapunov-inequality-simple}
\end{assumption}
%
\begin{assumption}
(Geometric ergodicity). Let Assumption \ref{assumption:lyapunov-inequality-simple} (or Assumption~\ref{assumption:lyapunov-inequality}; see Appendix \ref{app:theory}) hold. There exist $0 < R < \infty$ and $0 \leq \gamma < 1$ so that 
\begin{equation}
    \sup_{x \, : \, V(x) \leq 4 \beta / \alpha}{\|K^{s}(x, \cdot) - \pi(\cdot)\|_{TV}} \leq R \gamma^s
\end{equation}
for all $s \geq 0$.
\label{assumption:geometric-ergodicity}
\end{assumption}
Note that we denote by $K^{s}(x,\cdot)$ the $s$-step transition density for a chain sampled from $K$ with starting point $x$, so that $K^{1}(x,\cdot) = K(x,\cdot)$. 

We now link to Algorithm \ref{alg:la-mcmc} with the same proposal $q_t$ and target $\pi_X$. Let $\{ X_{t}\}_{t \geq 0}$ be the result of a run of this algorithm, 
%\todo{Check that the $X_t$ and $X_t'$ without hats here and below, and in Algorithm 1, are consistent with the theory in the Appendix.}
%
and let $\{ \widehat{\mathcal{L}}_{t}\}_{t \geq 0}$ and $\{ X_{t}' \}_{t \geq 0}$ be the sequence of surrogates and proposals computed during the run. We make the following assumption about the ``goodness" of our approximation:

\begin{assumption} 
(Approximation goodness). The parameters of Algorithm \ref{alg:la-mcmc}  are such that the approximations $\widehat{\mathcal{L}}_{t}$ to the log-density satisfy
\begin{equation}
    \left \vert \widehat{\mathcal{L}}_{t}(X_{t}) - \mathcal{L}(X_{t}) \right \vert + \left \vert \widehat{\mathcal{L}}_{t}(X_{t}') - \mathcal{L}(X_{t}') \right \vert  \leq  \left \vert Q_{V}(X_{t}, X_{t}') \right \vert 
\end{equation}
deterministically, where $Q_{V}(X_{t}, X_{t}')$ is as defined in \eqref{EqQvDef}. 
\label{assumption:good-appr}
\end{assumption}
%
Fix a function $f : \mathbb{R}^{d} \to [-1,1]$ with $\pi(f) = 0$. Our main theoretical result is:
\begin{theorem}
Let Assumptions \ref{assumption:lyapunov-inequality-simple}, \ref{assumption:geometric-ergodicity}, and \ref{assumption:good-appr} hold and let $\{X_t\}_{t=1}^{T}$ be generated by Algorithm \ref{alg:la-mcmc} with starting point $x$. Then there exists a constant $C > 0$ so that
\begin{equation}
\left|\mathbb{E}\left[ \left( \frac{1}{T} \sum_{t=1}^{T} f(X_t) \right)^2 \right] \right| \leq C \frac{\log(T)^{3}}{T}
\end{equation}
for all $T > T_{0}(x)$ sufficiently large.
\label{thm:convergence-rate}
\end{theorem}
%
%
To give Theorem \ref{thm:convergence-rate} some context, the ``usual'' MCMC estimate satisfies $\mathbb{E}[(\hat{\pi}_{T}(f))^2] \leq C/T$ for all $T > T_{0}$ sufficiently large 
%\todo{Aaron, please check ``burn-in'' qualifier I added to the theorem statement above. Should we modify this or make it more precise at all?}
under the same Lyapunov and minorization assumptions (Assumptions \ref{assumption:lyapunov-inequality-simple} and \ref{assumption:geometric-ergodicity}---see \citet{meyn2012markov}). Furthermore, the usual bound is sharp. Our bound is nearly identical, giving up only logarithmic terms. Informally, this suggests that using our algorithm has nearly the same finite-time guarantees as typical MCMC algorithms and, depending on the specific problem, may be substantially cheaper to run.