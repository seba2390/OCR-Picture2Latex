\section{Local approximation MCMC}
\label{sec:numerics}

We assume that our target distribution has a density $\pi(x) = \exp{\mathcal{L}(x)}$ on $\mathbb{R}^d$. Our goal is to construct an algorithm that exploits regularity in $\pi$ to reduce the computational cost of simulating from the target distribution. As discussed in the Introduction, replacing target density evaluations with a continually and infinitely refined surrogate model, within MCMC, can asymptotically generate samples from the exact target distribution \citep{Conradetal2016}. Given a finite number of MCMC samples, however, replacing the target density with such an approximation introduces a \textit{surrogate bias}. In this section, we present a LA-MCMC algorithm that extends \citet{Conradetal2016} in two significant ways: (i) we use the trade-off between surrogate bias and Monte Carlo variance to develop a refinement strategy with near-optimal convergence guarantees; and (ii) we introduce a parameter that allows LA-MCMC to easily characterize distributions with heavier tails given little \textit{a priori} knowledge, while enabling additional speedups when the user has substantial \textit{a priori} knowledge.

Our approach will build a \textit{local polynomial} approximation \citep{Connetal2009,Kohler2002,Stone1977} of a function $g:\mathbb{R}^{d} \to \mathbb{R}$, where $g$ is chosen such that evaluating $\pi(x)$ is trivial given $g(x)$. For instance, $g$ might be the logarithm of the target density, $\mathcal{L}$; or it might be the log-likelihood function in a Bayesian setting, if the prior density is relatively simple to evaluate. Let the ``evaluated set'' $\mathcal{S}_n = \{x_1, \hdots, x_n\}$ comprise the set of parameter values $x_i \in \mathbb{R}^d$ at which we have evaluated $g(x_i)$. For now, we take this set as given; later, we discuss how to construct it. With an appropriate construction of $\mathcal{S}$, local approximations allow refinement to focus on regions where the target distribution has greater mass. Polynomial approximations are appealing in this setting because they can be built easily, are cheap to evaluate, and have known analytic derivatives.

\subsection{Local polynomial approximations}

We construct local polynomial approximations using weighted regression \citep{Kohler2002}. Let $\mathcal{P}$ be a polynomial space over $\mathbb{R}^{d}$. The approximation at each $x \in \mathbb{R}^{d}$ is $\hat{g}(x, \mathcal{S}_n)$ such that 
\begin{equation}
    \hat{g}(x, \mathcal{S}_n) = \argmin_{m \in \mathcal{P}}{\sum_{i=1}^{n} (m(x_i)-g(x_i))^2 W(x, x_i)},
    \label{eq:local-polynomial-estimate}
\end{equation}
where $W(x, x^{\prime})$ is a locally supported kernel. Typically, we choose the $k$-nearest neighbor kernel 
\begin{equation}
    W(x, x^{\prime}) = \begin{cases}
    1 & \mbox{if $x^{\prime} \in \mathcal{B}_k(x)$} \\
    0 & \mbox{otherwise,}
    \end{cases}
    \label{eq:hat-kernel}
\end{equation}
where $\mathcal{B}_k(x)$ is the smallest ball centered at $x$ containing $k$ elements of $\mathcal{S}_n$. Define a basis $\Phi = \{\phi_j\}_{j=1}^{q}$ for $\mathcal{P}$, let $\boldsymbol{\phi}(x) = [\phi_1(x), \hdots, \phi_q(x)]^T$, and define $\boldsymbol{x}_k(x, \mathcal{S}_n) = \{\tilde{x} \in \mathcal{S}_n: W(x, \tilde{x})>0\}$. The elements of $\boldsymbol{x}_k(x, \mathcal{S}_n)$ are the $k$ nearest neighbors to $x$ in $\mathcal{S}_n$. The optimal polynomial kernel estimate is 
\begin{equation}
    \hat{g}(x, \mathcal{S}_n) = \boldsymbol{\phi}(x)^T \mathbf{a}(x, \boldsymbol{x}_k(x, \mathcal{S}_n))
\end{equation}
such that 
\begin{equation}
    \mathbf{a}(x, \boldsymbol{x}_k) = \argmin_{\boldsymbol{\alpha} \in \mathbb{R}^{q}}{\|\mathbf{V}(\boldsymbol{x}_k) \boldsymbol{\alpha} - \mathbf{g}(\boldsymbol{x}_k)\|_{\mathbf{W}(x, \boldsymbol{x}_k)}^2},
    \label{eq:local-polynomial-estimate-matrix-form}
\end{equation}
where $\|\mathbf{x}\|_{\mathbf{A}}^2 = \mathbf{x}^T \mathbf{A} \mathbf{x}$, 
\begin{subequations}
\begin{equation}
    \mathbf{W}(x, \boldsymbol{x}_k) = \diag{[W(x, x_1), \hdots, W(x, x_k)]},    
\end{equation}
$\mathbf{V}(\boldsymbol{x}_k)$ is the Vandermonde matrix 
\begin{equation}
    \mathbf{V}(\boldsymbol{x}_k) = \left[ \begin{array}{c}
    \boldsymbol{\phi}(x_1)^T \\
    \vdots \\
    \boldsymbol{\phi}(x_k)^T
    \end{array} \right],
\end{equation}
and
\begin{equation}
    \mathbf{g}(\boldsymbol{x}_k) = \left[ \begin{array}{c}
     g(x_1) \\
     \vdots \\
     g(x_k)
     \end{array} \right].
\end{equation}
\end{subequations}
Assuming $\mathbf{V}(\boldsymbol{x}_k)$ has full column rank and $k \geq q$, the local polynomial approximation exists and is unique \citep{Kohler2002,Stone1977}. The solution to \eqref{eq:local-polynomial-estimate-matrix-form} is 
\begin{equation}
    \mathbf{a}(x, \boldsymbol{x}_k(x, \mathcal{S}_n)) = (\mathbf{V}^T \mathbf{W} \mathbf{V})^{-1} \mathbf{V}^T \mathbf{g}.
\end{equation}

\subsubsection{Error analysis}
\label{sec:erroranalysis}

Now we derive local error bounds that, for a fixed evaluated set $\mathcal{S}_n$, depend on the number of nearest neighbors, the size of the ball containing them, and the local behavior of the surrogate. We assume that the kernel $W(x, x^{\prime})$ is the hat kernel defined in \eqref{eq:hat-kernel} and that $\mathcal{P}$ is the space of polynomials of degree less than or equal to $p$; 
we write $q = \text{dim}( \mathcal{P})$. 

We first investigate the local behavior of the polynomials in the ball $\mathcal{B}_k(x)$. Let 
\begin{equation*}
\Delta(x) = \max_{x_i \in\boldsymbol{x}_k(x,\mathcal{S}_n)}{\|x-x_i\|}
\end{equation*}
be the radius of $\mathcal{B}_k(x)$---the ball that contains $k$ points in the evaluated set $\mathcal{S}_n$. We note that the radius of $\mathcal{B}_k(x)$ is not uniform in $x \in \mathbb{R}^{d}$. Analogous to \eqref{eq:local-polynomial-estimate-matrix-form}, define $k$ Lagrange polynomials
\begin{equation}
    \lambda_j(x) = \argmin_{m \in \mathcal{P}}{\sum_{i=1}^{k} (m(x_i)-\delta_{ij})^2},
\end{equation}
where $\delta_{ij}$ is the Dirac delta. Here, the summation is over the $k$ nearest neighbors to $x$, i.e., $x_i \in \boldsymbol{x}_k(x)$. Let $\boldsymbol{\lambda}(x) = [\lambda_1(x), \hdots, \lambda_k(x)]^T$. The nearest neighbors $\boldsymbol{x}_k(x)$ are called $\Lambda$-poised in $\mathcal{B}_k(x)$ if
\begin{equation}
    \max_{x^{\prime} \in \mathcal{B}_k(x)}{\|\boldsymbol{\lambda}(x^{\prime})\|_2} = \Lambda_2(x) \leq \Lambda.
    \label{eq:poisedness}
\end{equation}
This definition of $\Lambda$-poisedness is slightly different than in \citet{Connetal2009}, which uses the infinity norm. Since the number of nearest neighbors $k$ is fixed and finite, however, 
these definitions are equivalent. $\Lambda$-poisedness measures how well distributed the points are within the ball. For example, consider points $i$ and $j$ in a unit ball and quadratic polynomials. The $i^{\text{th}}$ Lagrange polynomial is close to one at point $i$ and close to zero at point $j$. (If the number of nearest neighbors $k$ is exactly the number required to interpolate ($k=q$) then the Lagrange polynomial will be exactly one or zero at these points.) 
%
The quadratic Lagrange polynomial is narrower if the points are close together and wider if they are far apart. The wider parabola has a smaller poisedness constant than the narrower one. 

 Assuming $\Lambda$-poisedness and that $g(x)$ is at least $(p+1)$ times differentiable, we have the error bound
% \begin{multline}
\begin{align}
    \vert \hat{g}(x, \mathcal{S}_n) - g(x) \vert \leq \frac{k}{(p+1)!} \Lambda \Delta(x)^{p+1} \sup_{x^{\prime} \in \mathcal{B}_k(x)}{g^{(p+1)}(x^{\prime})},
    \label{eq:local-error-bound}
\end{align}
% \end{multline}
where $p$ is the polynomial degree and $g^{(p+1)}(x)$ is the $(p+1)^{\text{th}}$ derivative of $g(x)$ \citep{Connetal2009}. The radius $\Delta(x)$ decreases as points are added to $\mathcal{S}_n$ and thus the error bound decreases, assuming that points are chosen in a way that maintains $\Lambda$-poisedness.

\subsubsection{Local refinements}
\label{sec:localrefinements}
Given a surrogate $\hat{g}(x, \mathcal{S}_n)$, we can improve its accuracy in a neighborhood around $x$ by performing a \textit{local refinement}. A local refinement adds a new point $x^{*}$ to the evaluated set---i.e., $\mathcal{S}_{n+1} = \mathcal{S}_n \cup \{x^{*}\}$---and thus allows the local error bound \eqref{eq:local-error-bound} to decrease. Randomly choosing a nearby point tends to form clusters \citep{RoteTichy1996}, however, and therefore refining using a random point inside $\mathcal{B}_k(x)$ fails to maintain $\Lambda$-poisedness.

We instead choose the refinement location based on the poisedness constant $\Lambda_2(x)$ defined in \eqref{eq:poisedness}. Computing $\Lambda_2(x)$ by solving the optimization problem defined in \eqref{eq:poisedness} also defines the point 
\begin{equation}
    x_{\lambda}(x) = \argmin_{x^{\prime} \in \mathcal{B}_k(x)}{\|\boldsymbol{\lambda}(x^{\prime})\|_2}.
    \label{eq:poisedness-refinement-location}
\end{equation}
If $\boldsymbol{x}_k(x)$ is poorly poised in $\mathcal{B}_k(x)$ ($\Lambda_2(x) \gg 1$) then the new point $x_{\lambda}(x)$ will be relatively far from any clusters in $\boldsymbol{x}_k(x)$ and will tend to improve the poisedness of the updated set. Conversely, if $\boldsymbol{x}_k(x)$ is well poised in $\mathcal{B}_k(x)$ ($\Lambda_2(x) \approx 1$), then the updated set remains well poised. We therefore refine the surrogate by setting $\mathcal{S}_{n+1} = \mathcal{S}_n \cup \{x_{\lambda}(x)\}$.

Having discussed {how} to refine, we must also discuss \textit{when}, i.e., under what conditions, to refine. Our primary criterion will be derived from the local error bound \eqref{eq:local-error-bound}; this process is described in Section~\ref{sec:bias-variance-trade-off}. In the meantime, we discuss an additional but natural secondary criterion, which is to trigger a refinement if the poisedness constant exceeds a user-prescribed threshold $\bar{\Lambda}$ (i.e., if $\Lambda_2(x) > \bar{\Lambda}$ then set $\mathcal{S}_{n+1} = \mathcal{S}_n \cup \{x_{\lambda}(x)\}$). In practice, we find that adding $x_{\lambda}(x)$ to the evaluated set whenever a refinement is triggered tends to maintain poisedness automatically, and that the secondary criterion thus rarely triggers refinements. Computing the poisedness constant by solving \eqref{eq:poisedness} can also be computationally burdensome (although still significantly cheaper than an expensive density evaluation). We therefore often ``turn off'' this secondary refinement criterion by setting $\bar{\Lambda} = \infty$, and only compute the poisedness constant when refinements are triggered by the primary criterion. 

\subsection{Sampling methods using local approximations}

Each step of a typical MCMC algorithm consists of three stages: (i) propose a new state, (ii) compute the acceptance probability, and (iii) accept or reject the proposed state. LA-MCMC adds a fourth stage---possibly refine the surrogate model---and replaces the target density evaluations in stage (ii) exclusively with cheaper surrogate evaluations; see Algorithm \ref{alg:la-mcmc}. The refinement frequency ensures that the error incurred by using a surrogate model balances the Monte Carlo variance.

\subsubsection{Bias-variance trade-off} \label{sec:bias-variance-trade-off}

We now present a heuristic for balancing bias and variance in LA-MCMC,  deferring a rigorous discussion to Section \ref{sec:theory}.

Fix a function $f$ and Markov chain $\{X_{t}\}_{t \geq 0}$ with stationary measure $\pi$. For $T \in \mathbb{N}$, denote by $\hat{\pi}_{T}(f) = \frac{1}{T} \sum_{t=1}^{T} f(X_{t})$ the usual Monte Carlo estimate of $\pi(f) = \mathbb{E}[f(X)]$---the expectation with respect to $\pi$. 
Under modest conditions (see, e.g.,  \citet{meyn2012markov}), the asymptotic bias exists and satisfies 
\begin{equation}
\lim_{T \rightarrow \infty} T \, \left \vert \mathbb{E}[\pi_{T}(f)] - \pi(f) \right \vert = 0
% \in (0,\infty)
\end{equation}
while the asymptotic variance exists and satisfies
\begin{equation}
\lim_{T \rightarrow \infty} T \, \var{(\hat{\pi}_{T}(f))}  = C_v \in (0,\infty).
\end{equation}
When this happens, there is (asymptotically) no trade-off: the bias quickly becomes a negligible source of error.

In the context of LA-MCMC,  more care is required because of an additional bias from the surrogate model. We assume that $\pi$ has density $\pi(x) = \exp{ \mathcal{L}(x) }$, and view LA-MCMC as an ``approximate'' MCMC algorithm that tries to target\footnote{We note that our $\widehat{\pi}(x) = \exp \widehat{\mathcal{L}}(x)$ often fails to be a probability density. We resolve this technical problem in Section \ref{sec:theory}, and somewhat surprisingly our approach means that this problem has very little impact on the following heuristic calculations.} $\hat{\pi}(x) = \exp \widehat{\mathcal{L}}(x)$, where $\widehat{\mathcal{L}}(x) \approx \mathcal{L}(x)$. In particular, we assume that the surrogate $\widehat{\mathcal{L}}$ adheres to the error bound in \eqref{eq:local-error-bound}, which is a pointwise condition of the form: 
\begin{equation}
 \vert \widehat{\mathcal{L}}(x) - {\mathcal{L}}(x) \vert \leq C_{b} \Delta(x)^{p+1}
 \label{eq:transition-kernel-radius-bound}
\end{equation} 
for $C_b>0$. We will show in Section \ref{sec:theory} that the bias incurred from using this surrogate model inherits a similar error bound,
\begin{equation}
    \left| \hat{\pi}(f) - \pi(f) \right| \leq C_{b}^{\prime} \bar{\Delta}^{p+1},
    \label{eq:expectation-error}
\end{equation}
where $C_b^{\prime}>0$ and, informally, we can think of $\bar{\Delta}^{p+1}$ as the maximum of $\Delta(x)^{p+1}/V(x)$, where $V(x) > 0$ is a penalty function to be described below. We will thus use the local radius $\Delta(x)^{p+1}$ as an \textit{error indicator}, and sequentially refine our approximation so that $\bar{\Delta} = \bar{\Delta}(T)$ decays with time $T$.

We would like to balance the additional error introduced by the surrogate model against the usual ``Monte Carlo'' error of the original Markov chain. % As is typically the case when balancing different sources of error, 
We choose our parameters so that the upper bound on the surrogate error decays at the same rate as the Monte Carlo error. 
%
To ensure that the pointwise surrogate error is small enough, we partition the chain into 
consecutive intervals called \emph{levels}, $\ell = 0, 1, 2, \ldots$, and prescribe a piecewise constant error threshold for each level $\ell$,
\begin{equation}
    \gamma_\ell(x) = \gamma_0 \ell^{-\gamma_1} V(x),
    \label{eq:error-threshold}
\end{equation}
with $\gamma_0, \gamma_1 > 0$.
Here, $V(x) > 0$ is a $\pi$-integrable penalty function that allows the error threshold to be larger in low-probability regions (e.g., in the tails of the distribution). We assume that $V(x)$ satisfies the Lyapunov inequality \eqref{eq:lyapunov}; this requirement guides how to choose the function. As a reasonable default, we set $V(x) = \exp{(\nu_0 \|x-\bar{x}\|^{\nu_1})}$, where $\nu_0>0$, $0 < \nu_1 \leq 1$, and $\bar{x}$ is a user-prescribed estimate of the centroid of the distribution (e.g., the mean or mode). During MCMC, we locally refine the surrogate model if the error indicator $\Delta(x)^{p+1}$ at the current state exceeds the current threshold $\gamma_\ell(x)$. 

Level $\ell$ ends at step $T_{\ell}$, when the MCMC variance is of the same size as the squared surrogate bias:
\begin{equation}
    \left ( \gamma_0 \ell^{-\gamma_1} \right )^2 \sim C_{v} T^{-1}_\ell.
    \label{eq:bias-variance}
\end{equation}
The number of steps in the $\ell^{\text{th}}$ level is, therefore, 
\begin{equation}
    T_{\ell} = \tau_0 \ell^{2\gamma_1},
\end{equation}
where $\tau_0 = C_v / \gamma_0^2$ is the length of the first level. In practice, we set the the level as a function of the MCMC step $t$, 
\begin{equation}
    \ell(t) = \lfloor (t/\tau_0)^{1/(2\gamma_1)} \rfloor.
    \label{eq:leveldefn}
\end{equation}
The length of each level, $\tau_{\ell} = T_{\ell}-T_{\ell-1}$, will then decrease with $\ell$ if $\gamma_1 < 0.5$. This is undesirable because then, beyond some critical step $t^* < \infty$, the level length will become less than one. If this occurs, we will need to increment the level more than once per MCMC step. Rather than addressing this complication, we simply require that lengths of the levels strictly increase with $\ell$ by setting $\gamma_1 > 0.5$. 

The constant $\tau_0$ is rarely known, and we treat it as an algorithmic parameter. Given the initial error threshold $\gamma_0>0$, error decay rate $\gamma_1>0.5$, initial level length $\tau_0 \geq 1$, and the Lyapunov function $V(x)$, we locally refine whenever the error indicator at an accepted state in the chain exceeds the decaying error threshold. See Algorithm \ref{alg:la-mcmc} for a complete summary. 

\subsubsection{Tail correction}
\label{sec:tailcorrection}

There is a major technical difficulty in making the heuristic from Section \ref{sec:bias-variance-trade-off} precise: it is possible that our surrogate function has small pointwise error as in \eqref{eq:transition-kernel-radius-bound}, but fails to be globally integrable (and thus cannot possibly be an unnormalized probability density). This results in large practical and theoretical difficulties: the associated stochastic process may wander off to infinity, and the pointwise bound in \eqref{eq:transition-kernel-radius-bound} will not give any bound on the Monte Carlo error. In previous work (e.g., \citet{Conradetal2016}), we avoided this problem by working on compact state spaces or obtaining much stronger pointwise approximations than \eqref{eq:transition-kernel-radius-bound}. Here, we obtain much stronger results by slightly tweaking our algorithm's acceptance probability.

Our tweak uses the Lyapunov function of the MCMC algorithm with exact evaluations, $V(x)$, to change the acceptance probability. Rather than computing the typical Metropolis-Hastings acceptance probability targeting $\hat{\pi}$, we slightly \textit{increase} the chances of moves that would decrease the Lyapunov function and \textit{decrease} the chances of moves that would increase it. \citet{roberts1996geometric} give very general conditions under which we can easily define the Lyapunov function.

More precisely, given the current state $x_t$ and proposed state $x^{\prime}_t$, we tweak the usual Metropolis-Hastings acceptance probability with the estimate 
\begin{equation}
    \log{\pi(x^{\prime}_t)} \approx \widetilde{\mathcal{L}}(x^{\prime}_t) \equiv \widehat{\mathcal{L}}(x^{\prime}_t) + Q_V(x_t, x^{\prime}_t),
    \label{eq:Lypunov-correction}
\end{equation}
where 
%
\begin{equation}
    Q_V(x_t, x^{\prime}_t) = \begin{cases}
    \ \, \, \eta (\gamma(x^{\prime}_t) + \gamma(x_t))   & \mbox{if } V(x'_t) < V(x_t)\\
    - \eta(\gamma(x^{\prime}_t) + \gamma(x_t)) \ & \mbox{if } V(x'_t) \geq V(x_t),
    \end{cases}
    \label{EqQvDef}
\end{equation} 
and $\gamma(\cdot) = \gamma_{\ell(t)}(\cdot)$ is defined by \eqref{eq:error-threshold} and \eqref{eq:leveldefn}. Here, $\widehat{\mathcal{L}}(x^{\prime}_t)$ is an approximation of the log-density using the local polynomial approximation. The factor $\gamma(x^{\prime}_t) + \gamma(x_t)$ causes the correction to become less apparent as the error threshold decays, 
% $\gamma(x^{\prime}_t) + \gamma(x_t) \rightarrow 0$, 
$\gamma_{\ell(t)}(\cdot) \to 0$,
and $\eta \geq 0$ is a user-defined parameter to control the correction. 

\begin{algorithm}
 \begin{algorithmic}
    \State Set initial state $X_0$ and surrogate model $\widehat{\mathcal{L}}_0$ for the log-target density 
    \For{$t \gets 1$ to $\infty$}
    \State Propose $X^{\prime} \sim q_t(\cdot \vert X_t)$
    \State
    \State Possibly refine at $X_t$: $(\gamma(X_t), \widehat{\mathcal{L}}_t) = $ \Call{CheckAndRefine}{$X_t$, $\widehat{\mathcal{L}}_{t-1}$, $t$}
    \State
    \State Compute error threshold at the proposed point $$\gamma(X^{\prime}) = \gamma_0 (\lfloor (t/\tau_0)^{1/(2\gamma_1)} \rfloor)^{-\gamma_1} V(X^{\prime})$$
    \State
    \State Compute $\widetilde{\mathcal{L}}_t(X^{\prime})$ via \eqref{eq:Lypunov-correction} and evaluate the acceptance probability
    \begin{equation*}
        \alpha(X_t, X^{\prime}) = \min{\left(1, \exp{(\widetilde{\mathcal{L}}_t(X^{\prime})-\widehat{\mathcal{L}}_t(X_t))} \frac{q_t(X_t \vert X^{\prime})}{q_t(X^{\prime} \vert X_t)} \right)}
    \end{equation*}
    \State
    \State Accept/reject step:
    \begin{equation*}
        X_{t+1} = \begin{cases}
        X^{\prime} & \mbox{with probability } \alpha(X_t, X^{\prime}) \\
        X_t & \mbox{else}
    \end{cases}
    \end{equation*}
    \EndFor
    \Procedure{CheckAndRefine}{$x$, $\widehat{\mathcal{L}}$, $t$}
  \State Compute the local error threshold $$\gamma(x) = \gamma_0 (\lfloor (t/\tau_0)^{1/(2\gamma_1)} \rfloor)^{-\gamma_1} V(x)$$ and (optionally) compute the poisedness constant $\Lambda_2(x)$
  \State
  \If{$\Delta(x)^{p+1}>\gamma(x)$ or (optionally) $\Lambda_2(x) > \bar{\Lambda}$} 
    \State $\widehat{\mathcal{L}}^{*}$ = \Call{RefineSurrogate}{$x$, $\widehat{\mathcal{L}}$}
  \Else{}
    $\widehat{\mathcal{L}}^{*} = \widehat{\mathcal{L}}$
  \EndIf
  \Return The error threshold $\gamma(x)$ and $\widehat{\mathcal{L}}^{*}$
  \EndProcedure
  \State
  \Procedure{RefineSurrogate}{$x$, $\widehat{\mathcal{L}}$}
  \State Compute poisedness-based refinement location $x_{\lambda}(x)$ defined in \eqref{eq:poisedness-refinement-location}
  \If{$x_{\lambda}(x) \notin \mathcal{S}_n$} 
    Set $\mathcal{S}_n \gets \mathcal{S}_n \cup \{x_{\lambda}(x)\}$ and $n \gets n+1$
  \EndIf
  \If{$x_{\lambda}(x) \in \mathcal{S}_n$} 
    Randomly choose $X^{*} \in \mathcal{B}_k(x)$ and set $\mathcal{S}_n \gets \mathcal{S}_n \cup \{X^{*}\}$ and $n \gets n+1$
  \EndIf
  \Return The updated surrogate model $\widehat{\mathcal{L}}^{*}$ using $\mathcal{S}_n$
  \EndProcedure
 \end{algorithmic}
 \caption{Pseudocode for the LA-MCMC algorithm. LA-MCMC requires eight parameters: (i) the initial error threshold $\gamma_0>0$, (ii) the error threshold decay rate $\gamma_1>0.5$, (iii) the maximum poisedness constant $\bar{\Lambda}$, (iv) the length of the first level $\tau_0 \geq 1$, (v) the tail-correction parameter $\eta \geq 0$, (vi) the number of nearest neighbors $k$ used to construct the local polynomial surrogate, (vii) the degree of the local polynomial surrogate $p$, and (viii) a guessed Lyapunov function $V$.}
 \label{alg:la-mcmc}
\end{algorithm}

\edits{
\subsubsection{Algorithm parameters}

We briefly describe some heuristics for choosing the input parameters to Algorithm~\ref{alg:la-mcmc}. Ideally, we choose the initial threshold $\gamma_{0}$ so that the required ball size $\Delta$ on the first error level is similar to the standard deviation of the posterior. Although the latter quantity is unknown, we can often estimate it using derivative information around the posterior mode. We have found that the decay rate $\gamma_{1} > 0.5$ does not have much impact on the performance of the algorithm (see Section~\ref{sec:1d-example}). We normally set $\tau_0=1$. Typically we use quadratic surrogates, i.e., $p=2$; we further discuss this choice in Sections~\ref{sec:1d-example} and \ref{sec:tracer}. We usually prescribe the number of nearest neighbors to be $k = 2q$, where $q = \text{dim}( \mathcal{P})$ is the dimension of the polynomial space. (In principle we only need $k \geq q$, but the extra regression points add stability.)

There is usually a great deal of flexibility in the choice of Lyapunov function, as it should only have a large impact on the tails of the target distribution. In order to prevent the stochastic process from wandering to infinity, it must satisfy $\pi(V) < \infty$.  If one has knowledge of the tail behavior of the target density $\pi$, choosing $V(x) \propto 1/\sqrt{\pi(x)}$ often works well. If we merely have bounds on the tails, we denote by $\Pi$ a distribution with heavier tails than $\pi$ and try $V(x) \propto 1/\sqrt{\Pi(x)}$. We choose the tail correction parameter $\eta$ to be as small as possible without letting the chain wander out to infinity. Typically, setting $\eta=0$ and slowly increasing the parameter until the chains stop wandering is sufficient. Finally, the bound on the poisedness constant, $\bar{\Lambda}$, is mostly included for reasons related to our proof technique. As discussed in Section~\ref{sec:localrefinements}, we often set  $\bar{\Lambda} = \infty$, and even the theory allows for very large values.}

\edits{Efficiently finding $\boldsymbol{x}_k(x, \mathcal{S}_n)$, i.e., the $k$ nearest neighbors to a point $x$, is a non-trivial but fortunately well studied problem. We store points of the evaluated set $\mathcal{S}_n$ in $k$d-trees, which enable efficient nearest neighbor searching. Specifically, we use the $k$d-tree implementation in the library \texttt{nanoflann} \citep{nanoflann}, which is quite efficient; as reported in \citet{nanoflann}, it can build $k$d-trees from point sets of size $n=10^6$ in microseconds. (This benchmark is for $d=3$, but tree construction cost scales only linearly with $d$.) For the examples we have studied, which typically have $n < 10^4$ and $d \leq 12$, $k$d-tree construction cost is negligible. 
Our current implementation rebuilds the tree structure at each refinement step; this process ensures that the time needed to search the tree, a task invoked much more frequently, remains small. For problems where tree construction time is non-negligible, online point addition and re-balancing approaches, as discussed in \citet{doi:10.1080/10618600.2016.1231064}, could be beneficial to incorporate into the present workflow.

} 
