\subsection{Inferring aquifer transmissivity} \label{sec:tracer}

% Now we demonstrate the usefulness of LA-MCMC in a more computationally demanding example: inferring the spatially heterogeneous transmissivity of an unconfined aquifer. The goal of this example is to demonstrate the performance of LA-MCMC for a model that is \emph{computationally} similar to models used by practitioners. While our model is physically motivated and similar to that used in the study \citet{matott2012screening}, it is simplified in various ways. CONTINUE

Now we demonstrate the usefulness of LA-MCMC in \edits{a more computationally demanding example: inferring the spatially heterogeneous transmissivity of an unconfined aquifer. Although this example is physically motivated, our main goal is to highlight important aspects of the LA-MCMC algorithm and its performance. We choose this example because it is related to examples used in previous work \citep{Conradetal2018} and because similar models are used in the groundwater literature---see, for example, \citet{matott2012screening,janettietal2010,al2018,jardani2012,willmann2007,pooletal2015,govTransmissivity200340}. Though our model is idealized, it is not unreasonably different from many models that are used in practice. Moreover, explaining state-of-the-art groundwater models is well beyond the scope of this paper. We refer to existing work (e.g., \citet{janettietal2010}) for a detailed discussion of inferring transmissivity fields in hydrological applications, and here we focus on the computational demonstration of LA-MCMC.
}
%Now we demonstrate the usefulness of LA-MCMC in a large-scale example: inferring the spatially heterogeneous transmissivity of an unconfined aquifer. 
In this problem, the likelihood contains a set of coupled partial differential equations that model transport of a nonreactive tracer through the aquifer; the tracer concentration is then observed, with noise, at selected locations in the domain. Each likelihood evaluation is thus computationally intensive. 
% The transmissivity is a spatially distributed field that is described with $d=10$ parameters. 

More specifically, the tracer is advected and diffused through the unconfined aquifer (a groundwater resource whose top boundary is not capped by an impermeable layer of rock/soil) by a steady \edits{state} velocity field. 
\edits{
We model the aquifer's log-transmissivity, which determines the permeability of the soil, as a random field on the unit square $\mathcal{D} = [0,1]^2$, parameterized as
\begin{equation}
    \log{ \kappa(z) } = \sum_{i=1}^{d} \kappa_i \sqrt{\lambda_i} e_i(z),
    \label{eq:log-transmissivity}
\end{equation}
where $\kappa_i$ are scalar coefficients, $z \in \mathcal{D}$, and $\{ (\lambda_i, e_i(z))\}_{i=1}^d$ are the $d=9$ leading eigenvalues/eigenfunctions of the integral operator associated with the squared exponential kernel $k(z, z^{\prime}) = \exp{(-\|z-z^{\prime}\|^2/ 2 L^2 )}$, with $L=0.1$. In other words, we have 
\begin{equation}
     \int_{\mathcal{D}} k(z, z^{\prime}) e_i(z^{\prime}) \, dz^\prime = \lambda_i e_i(z) \, .
\end{equation}
Figure~\ref{fig:tracer-transport-domain} shows the ``true'' log-transmissivity, where the true $\kappa_i$ are drawn from a standard normal distribution.}
%The aquifer is divided into ten irregularly shaped regions with a river running along the right-hand side (Figure \ref{fig:tracer-transport-domain}). 
An ``injection'' well is located at \edits{$(x_w, y_w) = (0.45, 0.6)$}. Given a value of the field $\kappa(z)$, we first compute the steady state hydraulic head $h$ by solving
\begin{subequations}
\begin{equation}
    - \nabla \cdot (\kappa h \nabla h) = f_h
\end{equation}
on the domain $\mathcal{D} \ni z \equiv (x,y)$, with Dirichlet boundary conditions \edits{$h(x,y)=1$ for $x=0$, $x=1$, $y=0$, or $y=1$.} We set the source term above to:
\edits{
\begin{equation*}
    f_h(x,y) = 250 \exp{(-((x-x_w)^2+(y-y_w)^2)/0.005)},
\end{equation*}
}
\label{eq:hydraulic-head}
\end{subequations}
which models hydraulic forcing due to well pumping. The velocity is then
\begin{equation}
    u = -\kappa h \nabla h.
    \label{eq:tracer-velocity}
\end{equation}
The hydraulic head and velocity resulting from the ``true'' parameter values (Figure~\ref{fig:tracer-transport-domain}) are illustrated in Figure~\ref{fig:tracer-transport-hydraulic-head}. Given the steady-state velocity field $u$ and an initial tracer concentration $c(x, y, 0) = 0$, advection and diffusion of the tracer throughout the domain is modeled by a time-dependent concentration field $c(x,y,t)$ that obeys the following transport equation:
\begin{equation}
    \frac{\partial c}{\partial t} + \nabla \cdot ((d_m \mathbf{I} + d_{\ell} u u^T) \nabla c) - u^T \nabla c = -f_t
    \label{eq:tracer-concentration}
\end{equation}
with \edits{$d_m=0.05$, $d_{\ell}=0.001$, and} 
\begin{equation}
    f_t(x,y) = \exp{(-((x-x_w)^2+(y-y_w)^2)/0.005)}.
\end{equation}
The tracer forcing $f_t(x,y)$ models tracer leakage into the domain. The concentration field at time $t = 1$ is shown in \moreedits{Figure \ref{fig:tracer-transport-concentration}}. 
%We numerically implement this tracer-transport model using the finite element method, via the software package FEniCS \citep{FENICS}. We evolve the tracer concentration in time using an adaptive time integration scheme implemented in Sundials \citep{sundials}. 

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{log-transmissivity.pdf}
    \caption{The ``true'' log-transmissivity for an unconfined aquifer model. The coefficients in \eqref{eq:log-transmissivity} are sampled from a standard Gaussian distribution. The diamond denotes a well location.}
    \label{fig:tracer-transport-domain}
\end{figure}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{head.pdf}
    \caption{The steady-state hydraulic head and velocity field computed by solving \eqref{eq:hydraulic-head} and \eqref{eq:tracer-velocity} given the ``true'' parameter values are sampled from a standard Gaussian distribution. The white diamond represents the location of a well in the aquifer.}
    \label{fig:tracer-transport-hydraulic-head}
\end{figure}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{concentration-64.pdf}
    \caption{Tracer concentration computed at time $t=1$ by solving \eqref{eq:tracer-concentration} given the ``true'' parameter values in Figure~\ref{fig:tracer-transport-domain} and the steady state velocity field shown in Figure \ref{fig:tracer-transport-hydraulic-head}. The white diamond represents the location of a well in the aquifer, and the circles are sensor locations used in the Bayesian inference problem.}
    \label{fig:tracer-transport-concentration}
\end{figure}

Our goal is to infer the log-transmissivity parameters $(\kappa_i)_{i=1}^d$ given observations of the tracer concentration. We endow each $\kappa_i$  with an independent standard Gaussian prior distribution, $\kappa_i \sim N(0, 1)$. 
%
\edits{We could also infer other model parameters (e.g., $d_m$, $d_l$, or the forcing functions $f_h$ and $f_t$) by including them in the set of inferred parameters. For the sake of this example, however, we assume that these parameters are known and therefore fixed.}
%
Data are collected on a $10 \times 10$ array of points evenly spaced in $x \in [0, 1]$ and $y \in [0, 1]$. These locations are marked in Figure~\ref{fig:tracer-transport-concentration}. We make observations at $10$ evenly spaced times $t \in [0, 1]$. 
\edits{This is a relatively large amount of data---typical observation arrays only partially observe the aquifer, although there are often still hundreds of observations (see, e.g., \citet{al2018,janettietal2010}). The large data set here is relatively informative, such that the posterior distribution is quite concentrated relative to the prior and the inference problem is thus more computationally challenging.}
The observations are modeled as
\begin{equation}
    y = f(\kappa) + \varepsilon,
\end{equation}
where $f(\kappa)$ is the \textit{forward model} induced by the partial differential equations above, mapping the log-transmissivity parameters $\kappa \equiv (\kappa_i)_{i=1}^d$ to a prediction of the tracer concentration at the chosen sensor locations/times. The sensor noise is $\varepsilon \sim N(0, 10^{-4} {I})$, yielding the conditional distribution 
%\todo{is the noise variance $10^{-5}$ or $10^{-2}$? Not consistent above and below. AD: It should be $10^{-2}$}
\begin{equation}
y \vert \kappa \sim  N(f(\kappa), 10^{-4} I).
\end{equation}
The posterior density then follows from Bayes' rule:
\begin{eqnarray}
    \pi(\kappa \vert y) &\propto& \pi(y \vert \kappa) \pi(\kappa) \, .
    % \\ &=& N(y; f(\kappa), 10^{-2} \mathbf{I}) N(\kappa; \bar{\kappa}, 0.25 \mathbf{I}).
    \label{eq:tracer-posterior}
\end{eqnarray}
To avoid a so-called ``inverse crime,'' \edits{where data are generated from the same numerical model used to perform inference \citep{kaipiosomersalo2006},} we generate the data $y$ by solving the forward model with a well-refined $65 \times 65$ numerical discretization using the ``true'' parameters, and then perform the inference using a $25 \times 25$ numerical discretization of the forward model.

Evaluating the forward model at every MCMC step is computationally prohibitive. The run time of the model is \edits{$\mathcal{O}(1)$ seconds on an Intel Core i7-7700 CPU at 3.60GHz, with some variability depending on the value of log-transmissivity parameters $\kappa$.} Generating \edits{$10^6$ samples thus takes 5--10 days} 
%\todo{This number should be consistent with what we claim for the exact chain below. Roughly 10 days? Just under 10 days? 6--10 days? 5--10 days? AD: 5--10 is about what it is taking in practice.} 
%\todo{Also, the hardware should be consistent with the i7-7700 mentioned below.} 
of computation time. LA-MCMC is essential to making this Bayesian inference problem computationally feasible. 
%Here, since prior density evaluations are trivial, we apply the approximation capability of LA-MCMC to the log-likelihood function, $\log p(y \vert \kappa)$. 
\edits{We note that high performance and parallel computing resources could certainly reduce the computational cost of each model evaluation. Parallel computing can even enable the shared construction of surrogate models using concurrent chains, as described in our previous work \citep{Conradetal2018}. Since our goal here is to demonstrate the impact of new local approximation strategies, however, we focus on a serial implementation. A useful hardware- and implementation-independent measure of computational cost is the number of refinements (i.e., expensive model/target density evaluations) in a given run, which we discuss below.
} % \todo{YM: What do you think of the last sentence? Wanted to blunt some discussion of less impressive wallclock time/etc. AS: I like it. AD: agreed.}

% The error bounds in Section~\ref{sec:erroranalysis} assume that the local polynomial approximations are chosen from a polynomial space $\mathcal{P}$ over $\mathbb{R}^d$ of \emph{total degree} $p$. We implement this choice here, but we also compare it empirically with other choices of polynomial approximation space. 

Recall that the local polynomial surrogate \eqref{eq:local-polynomial-estimate} requires computing $q = \text{dim}(\mathcal{P})$ coefficients, and in the total-degree setting of our theory, we have $q = {{d+p}\choose{p}}$. This quantity grows rapidly with the dimension $d$ of the parameters for $p>1$.
Retaining a total-degree construction, we could set $p=0$ and thus include only a constant term in $\mathcal{P}$. This results in a constant surrogate model within each ball; now the number of terms in the local polynomial approximation is always one, independent of the parameter dimension. Similarly, setting $p=1$ lets $q$ grow only linearly with $d$. Yet in the one-dimensional example of Section~\ref{sec:1d-example}, we showed that including higher-degree terms in the approximation can reduce the number of expensive target density evaluations required to achieve a given accuracy (see Figure \ref{fig:1d-example-order}).

We implement these total-degree local polynomial approximations but also compare them empirically with other choices of polynomial approximation space---in particular, truncations that retain higher-order terms more selectively. A common practice in high-dimensional  approximation is to employ \emph{sparse truncations} of the relevant index set \citep{BlatmanSudret2011}. Let $\psi_s(\kappa_i)$ be a univariate polynomial of degree $s$ in $\kappa_i$. (Typically, we choose an orthogonal polynomial family, but \moreedits{this choice} is immaterial.) Each $d$-variate polynomial basis function $\phi_{\bm{\alpha}}(\kappa) \coloneqq \prod_{i=1}^{d} \psi_{\alpha_i}(\kappa_i)$ is defined by a multi-index $\bm{\alpha} \in \mathbb{N}_0^d$, where the integers $\alpha_i \geq 0$ are components of $\bm{\alpha}$. Now define the $\ell^\nu$ norm for $\nu > 0$ (a quasi-norm for $0 < \nu < 1$): 
% \todo{potential notation overload: we used $\nu_0$ and $\nu_1$ to define the Lyapunov function earlier. Is this $\nu$ without subscript okay? AD: I did not notice this, but am probably too familiar with this notation to be a good judge. Perhaps we change it to be safe? This $\nu$ is pretty isolated to this section though so maybe we are okay?}
\begin{equation}
    \|\bm{\alpha}\|_{\nu} = \left(\sum_{i=1}^{d} \alpha_i^{\nu}\right)^{1/\nu}.
\end{equation}
Given a maximum degree $p$, we can build our \emph{local} polynomial approximations in the polynomial space $\mathcal{P}_\nu^{p,d}$ spanned by basis functions with multi-indices $\|\bm{\alpha}\|_{\nu} \leq p$. When $\nu=1$, this recovers a total-degree expansion, which is consistent with the error bounds in Section~\ref{sec:erroranalysis}. However, the dimensionality of $\mathcal{P}_\nu^{p,d}$ decreases as $\nu \rightarrow 0$. We define the special case of $\nu=0$ to be a polynomial space with no cross terms: each multi-index has one nonzero element, and that element is at most $p$. 
%We can think of the polynomial approximations with the truncation $\nu < 1$ as being sparse in that they are ``missing'' cross terms.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{marginals.pdf} 
    \caption{The diagonal and lower diagonal subplots show the one- and two-dimensional marginal distributions of the posterior distribution \eqref{eq:tracer-posterior} estimated using LA-MCMC with \edits{$\nu=0.75$, $\gamma_0=2$, $\gamma_1=0.5$, $\bar{\Lambda}=\infty$, $\tau_0 = 1$, $\eta = 0$, $k=75$, $p=3$, and $V(x) = \exp{(\|\kappa-\kappa_{\text{MAP}}\|)}$. Here, $\kappa_{\text{MAP}} = \argmax_{\kappa}{\pi(\kappa \vert y)}$ is the posterior mode. The subplot on the upper right shows the two-dimensional prior marginal for any pair of parameters.}}
    \label{fig:tracer-transport-marginals}
\end{figure}

We now apply the LA-MCMC algorithm, \edits{as well as an exact MCMC algorithm for comparison,} to the posterior distribution of the tracer transport problem. \edits{All MCMC chains use the same (fixed) random-walk proposal $q_t = \mathcal{N}(\kappa_t, 0.005 I)$, where $\kappa_t$ is the current state of the chain.} We tested different configurations of LA-MCMC, parameters of which are given in Table~\ref{tab:tracer-mcmc-parameters}. The Lyapunov function $V$ used in the tail correction is defined using the posterior mode, $\bar{\kappa} = \argmax_{\kappa}{\pi(\kappa \vert y)}$, \edits{which we obtain by maximizing the log-posterior density using optimization algorithms in \texttt{nlopt} \citep{johnson2014nlopt}}. The rows in Table~\ref{tab:tracer-mcmc-parameters} with $\nu =1$ correspond to using total-degree linear and quadratic local polynomial approximations (consistent with our theory). Setting $\nu < 1$ defines a sparse approximation. The required number of nearest neighbors $k$ is chosen to be slightly more than the number of points $q = \text{dim}(\mathcal{P}_\nu^{p,d}) $ required to interpolate. 
%\todo{Is there a rationale or formula for fixing $k$, given $q$? Also, should we add a column of $q$ values to Table~\ref{tab:tracer-mcmc-parameters}? Also, why is $k$ bold? I think perhaps $\nu$ and then $p$ should be the first two columns, then $q$ and $k$.}
Reducing $\nu$ reduces $q$ and $k$. Chains produced by all these algorithmic configurations yield essentially identical posterior estimates. A trace plot of selected states from the LA-MCMC chain corresponding to the $p=3$ configuration is shown in  Figure \ref{fig:tracer-mixing}, \edits{and compared to a trace plot of the exact-model chain}. One- and two-dimensional marginals of the posterior distribution are shown in Figure \ref{fig:tracer-transport-marginals}. While this figure is generated with $\nu=0.75$ and $p=3$, the results are essentially identical for the other algorithmic configurations. \edits{An exact chain of $10^6$ steps took about $6$ days to complete, whereas the LA-MCMC chains of the same length with quadratic and cubic surrogate models took roughly $6$ to $10$ hours, and with linear models took just over one day. These timings are somewhat imprecise, however, as other computational tasks were competing for resources on the same workstation (a Intel Core i7-7700 CPU at 3.60GHz) used for our runs.} %\todo{Check this and edit as needed. I removed the table column, and made the timings vague since they are not ``clean.'' Upon re-reading the reviewer comments, I feel like we should at least mention wallclock times.}
% \todo{Same number of MCMC steps in each case? AD: Yes, always $10^6$} 
% \todo{How is 3 days for the exact model consistent with the comment above, which is 1 or 2 seconds per model evaluation, so that $10^6$ seconds = 10 days? AD: There are three things besides the number of model evaluations that can affect this: (i) The overhead from the algorithm (the nearest neighbor search is the most expensive part) (ii) The model evaluation is $\mathcal{O}(1)$ but varies quite a lot depending how the specific transmissivity field and (iii) other things happening on my computer that affect its memory. Bullet (iii) really explains most of this since I'm running multiple MCMC chains at once for this project and writing code for another project at the same time. I really should have put this on a cluster with a proper scheduler but I didn't ... if the reviewers/editors really care I can rerun them properly.}
% \todo{Is the fact that the difference in wall clock times doesn't equal the ratio of number of model evaluations simply due to overhead of the local approximation construction? AD: Partially (see previous comment)}

%Figures \ref{fig:tracer-mixing-linear}, \ref{fig:tracer-mixing-quadratic}, and \ref{fig:tracer-mixing-cubic} show that all of these scenarios explore the posterior distribution.


% IN PROGRESS HERE

% Carefully choosing higher-order terms to include the local polynomial surrogate model can drastically affect the performance of LA-MCMC. We generate five chains using LA-MCMC---one using a linear surrogate model, two with quadratic surrogate models, and two with cubic surrogate models. Table \ref{tab:tracer-mcmc-parameters} shows the parameters supplied to Algorithm \ref{alg:la-mcmc}; the bold columns show how varying the parameter $\eta$, which determines how to truncate the polynomial expansion, affects the required number of nearest neighbors $k$. We choose $k$ to be slightly more than the number of points $q$ required to interpolate. \todo{any particular rationale or formula for fixing $k$, given $q$? Also, should we add a column of $q$ values to the table? }
% For larger $\eta$, the number of terms in the expansion must also increase. Figure \ref{fig:tracer-mixing-linear} shows that LA-MCMC with a locally linear surrogate model successfully explores the posterior distribution---similar mixing plots for LA-MCMC with higher order local surrogate models show these chains exploring the same distribution.
% %Figures \ref{fig:tracer-mixing-linear}, \ref{fig:tracer-mixing-quadratic}, and \ref{fig:tracer-mixing-cubic} show that all of these scenarios explore the posterior distribution.

\begin{table}[h!]
    \centering
    % \begin{tabular}{c|c|c|c|c|c}
    %     degree $p$ & $\nu$ & $k$ & \# to interpolate ($q$) & $\gamma_0$ & Run time (days) \\ \hline \hline
    %     Exact & --- & --- & --- & --- & $2.89$ \\
    %     $1$ & $1$ & $20$ & $10$ & $1.9$ & $1.25$ \\
    %     %$2$ & $0.1$ & $25$ & $19$ & $0.6$ & $2.13$ \\
    %     $2$ & $1$ & $65$ & $55$ & $1$ & 0.33 \\
    %     %$3$ & $0.1$ & $40$ & $28$ & $0.1$ & 2.10 \\
    %     $3$ & $0.75$ & $75$ & $64$ & $1$ & 0.48
    % \end{tabular}
    \begin{tabular}{c|c|c|c|c}
        degree $p$ & $\nu$ & $k$ & \# to interpolate ($q$) & $\gamma_0$ \\ \hline \hline
        Exact & --- & --- & --- & ---\\
        $1$ & $1$ & $20$ & $10$ & $1.9$ \\
        $2$ & $1$ & $65$ & $55$ & $1$ \\
        $3$ & $0.75$ & $75$ & $64$ & $1$ 
    \end{tabular}
    \caption{Parameter configurations for Algorithm \ref{alg:la-mcmc}, used to generate samples from the posterior distribution \eqref{eq:tracer-posterior} of the PDE/tracer transport problem. In all cases $\gamma_1 = 0.5$, $\eta = 0$, $\log{V(\kappa)} = \|\kappa-\kappa_{\text{MAP}}\|$, and $\bar{\Lambda} = \infty$. \edits{Here, $\kappa_{\text{MAP}} = \argmax_{\kappa}{\pi(\kappa \vert y)}$ is the posterior mode, which we obtain by maximizing the log-posterior density using \texttt{nlopt} \citep{johnson2014nlopt}}.}
    \label{tab:tracer-mcmc-parameters}
\end{table}

\begin{figure}
  \centering
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{mixing-exact.pdf} \\[\abovecaptionskip]
    \small (a) Exact MCMC mixing
  \end{tabular}
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{mixing.pdf} \\[\abovecaptionskip]
    \small (a) LA-MCMC mixing
  \end{tabular}
  
  \caption{PDE/tracer transport problem: trace plot of \edits{three parameters of (a) an exact MCMC chain and (b) an LA-MCMC chain using a locally cubic ($p=3$) surrogate model with $\nu = 0.75$.} Additional parameters are defined on the last row of Table \ref{tab:tracer-mcmc-parameters}.}
  \label{fig:tracer-mixing}
\end{figure}

% \begin{figure}
%   \centering
  
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingQuadratic.png} \\[\abovecaptionskip]
%     \small (a) $\nu = 1$
%   \end{tabular}
  
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingQuadraticDiagonal.png} \\[\abovecaptionskip]
%     \small (b) $\nu = 0.5$
%   \end{tabular}
  
%   \caption{Trace plot of the LA-MCMC chain using quadratic surrogate models with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. The vertical line marks the half-way point in the chain---we typically discard the first half as ``burn-in.''}
%   \label{fig:tracer-mixing-quadratic}
% \end{figure}

% \begin{figure}
%   \centering
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingCubicDiagonal.png} \\[\abovecaptionskip]
%     \small (a) $\nu = 0.5$
%   \end{tabular}
  
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingCubicReduced.png} \\[\abovecaptionskip]
%     \small (a) $\nu = 0.75$
%   \end{tabular}
  
%   \caption{Trace plot of the LA-MCMC chain using cubic surrogate models with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. The vertical line marks the half-way point in the chain---we typically discard the first half as ``burn-in.''}
%   \label{fig:tracer-mixing-cubic}
% \end{figure}

\begin{figure}
  \centering
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{refinements.pdf} \\[\abovecaptionskip]
    \small (a) Refinements per MCMC step 
  \end{tabular}
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{refinements-chopped.pdf} \\[\abovecaptionskip]
    \small (b) Refinements per MCMC step (rescaled)
  \end{tabular}
  
  \caption{PDE/tracer transport problem: number of likelihood evaluations (i.e., refinements) as a function of MCMC steps, for LA-MCMC chains with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. Panel (a) shows the total number of refinements for all chains, and panel (b) rescales the vertical axis to emphasize how many fewer model evaluations LA-MCMC requires than the exact case. For local surrogate models with $p \geq 2$, we see significant reductions (of three to four orders of magnitude) over the number of likelihood evaluations required by exact MCMC.}
  \label{fig:tracer-refinements}
\end{figure}

Though all these chains are successful in characterizing the posterior distribution, the required number of (expensive) likelihood evaluations---and hence the overall computational cost---depend on a non-trivial relationship between the accuracy of the local approximations and the number of nearest neighbors required to define them. 
\edits{As noted earlier, though, the number of such likelihood evaluations is a robust and translatable measure of computational cost, independent of implementation and hardware, and also most meaningful in the setting where model evaluations dominate all other costs of the MCMC machinery.} 
Figure~\ref{fig:tracer-refinements} shows the number of likelihood evaluations $n$ as a function of the number of MCMC steps $t$ for each experiment in Table \ref{tab:tracer-mcmc-parameters}. \edits{We see that the quadratic and cubic local approximations (purple and blue lines) require far fewer expensive likelihood evaluations. In all cases, we see a reduction relative to the number of likelihood evaluations that would be required by exact MCMC. However, this improvement is drastically increased, by orders of magnitude, when choosing $p>1$.}
%We see that total-degree quadratic approximations (red line) require more likelihood evaluations for a given $t$ than linear approximations (purple line). Sparse ($\nu = 0.5$) quadratic models require fewer likelihood evaluations at a given $t$, however. And while cubic models with $\nu = 0.75$ require more likelihood evaluations than the other cases, reducing $\nu$ to 0.5 in the cubic case makes the number of  evaluations commensurate with those of the total-degree quadratic approximations. In all cases, we see significant improvements---between three and four orders of magnitude---over the $10^6$ likelihood evaluations that would be required by exact MCMC (at $t =10^6$, i.e., the right boundary of the horizontal axis).

It is important to note that our approach to controlling the bias-variance tradeoff, and thus the error thresholds for triggering refinement via \eqref{eq:local-error-bound} and \eqref{eq:error-threshold}, are derived for expansions of total degree $p$ and thus in principle applicable only for $\nu=1$. We use the same rules here for $\nu<1$, but this approach cannot guarantee that bias and variance decay at the same rate. Indeed, a full analysis of the local sparse approximations may depend on understanding the magnitudes of mixed derivatives of the target function $g$; this can become quite problem-specific, and we defer such an investigation to future work. The empirical results for $\nu<1$ here are intended to be practical and exploratory.

% We see that replacing a linear surrogate model with a quadratic one (purple and red lines) increases the total number of likelihood evaluations because the number of terms in the expansion grows exponentially. However, this can be offset by carefully choosing the terms in the polynomial expansion (blue line). When we replace the surrogate model with a truncated cubic polynomial expansion we see that the number of likelihood evaluations is substantially decreased. Here, we require that the error decay as if the expansion where third-order but do not include all of the terms in this expansion. Strictly speaking, this heuristic no longer guarantees that the squared bias and variance decay at the same rate.

The choice of local approximation not only affects the overall computational cost of each chain, but also its mixing. Figure~\ref{fig:tracer-autocorrelation-ess}(a) shows the number of effectively independent samples (i.e., the effective sample size (ESS) \citep{Wolffetal2004}) produced by each configuration, as a function of the number of MCMC steps $t$. \edits{The chain using exact density evaluations generates effectively independent samples fastest \textit{as a function of $t$}. Mixing is slightly faster with cubic local approximations than with local quadratic or linear approximations. Figures \ref{fig:tracer-autocorrelation-ess}(b)--(c), however, show a non-trivial relationship between complexity of the surrogate model and how quickly it generates ESS \textit{as a function of the number of model evaluations $n$}.} Chains employing local linear approximations ($p=1$) \edits{(red line) do not mix as efficiently as the sparse local cubic approximation or the total-degree quadratic approximation (blue and purple lines). Because the number of model evaluations in the $p=1$ case is not so drastically reduced over the exact case (see Figure~\ref{fig:tracer-refinements}(a)), poorer mixing leads to worse performance in the overall metric of ESS per model evaluation. On the other hand, the $p=3$ case achieves the same ESS with 5000 model evaluations that the exact chain achieves with $5\times 10^5$ evaluations, an improvement of two orders of magnitude.

Recall that Figure~\ref{fig:tracer-transport-marginals} shows strong posterior correlations for this problem. We speculate that log-likelihood approximations that include cross terms (i.e., $p>1$) can more easily approximate the true posterior density and, therefore, significantly outperform local approximations that do not include these factors.} 
%
% \todo{Side note, just for us: this may also related to the choice of whether to approximate the forward model or the log-likelihood. If we did a $p=1$ approximation of the forward model, we would have cross terms in the log-likelihood, as the approximation would be locally quadratic! On the other hand, we'd need to build an approximation for each model output/observation, which is expensive. AD: This is an interesting point. Definitely beyond scope to explore here, but maybe there is a way to use this. True, we would have to build a surrogate for each output but we could use the same nearest neighbors for each component. Since this is the expensive part and it doesn't grow with dimension maybe we would be okay ...}
%
In theory, even a locally constant model will yield convergence as $\Delta \to 0$. \edits{In practice, we find that the choice of basis for the local approximation may have a substantial impact. While $p=2$ or $p=3$ are good default choices, a method for adaptively selecting polynomial basis functions as we learn about correlations in the posterior could possibly improve the algorithm. We leave this to future work.
}


\begin{figure}
  \centering

%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_Autocorrelation.pdf} \\[\abovecaptionskip]
%     \small (a) Autocorrelation of $\kappa_0$
%   \end{tabular}
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{ess_samples.pdf} \\[\abovecaptionskip]
    \small (a) ESS per MCMC step 
  \end{tabular}
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{ess_refinements.pdf} \\[\abovecaptionskip]
    \small (b) ESS per model evaluation
  \end{tabular}

    \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{ess_refinements-chopped.pdf} \\[\abovecaptionskip]
    \small (c) ESS per model evaluation (rescaled)
  \end{tabular}
  
  \caption{PDE/tracer transport problem, computational efficiency for LA-MCMC chains with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. (a) effective sample size (ESS) as a function of the number of MCMC steps; (b-c) ESS as a function of the number of likelihood evaluations (refinements). Panel (c) rescales the horizontal axis to more clearly show how quickly LA-MCMC with cubic and quadratic surrogate models generates independent samples as a function of the number of refinements.}
  \label{fig:tracer-autocorrelation-ess}
\end{figure}

% In all cases, we see drastic improvements over MCMC with exact evaluations: LA-MCMC requires fewer than $1500$ likelihood evaluations to generate $10^6$ samples and, in some cases requires fewer than $500$. This is an improvement of three to four orders of magnitude. Furthermore, the ratio of generated samples to likelihood evaluations increases as the chain gets longer. Surrogates built using higher-order polynomial expansions are more accurate given a fixed ball containing all of the nearest neighbors. However, such surrogates also require more nearest neighbors. 

