\section{Theoretical results}

\subsection*{Outline of results}

\begin{enumerate}
  \item From previous papers: under broad conditions, we get pointwise convergence of our log-target or log-likelihood estimator, as long as we do not wander off to infinity.
  \begin{itemize}
    \item We do still need random refinement since \textcolor{red!90!black}{since we cannot rule out all of the points we happen to have sampled looking like near-perfect fits even when they are not.}
    \item[] \textcolor{red!90!black}{What would it mean for the points to look like ``near-perfect fits''?  The error indicator is not aware of any structure of the density itself.  It only ``knows'' the ball size.  Is it not enough to for the radius to go to zero everywhere?  I suppose to be rigorous, the ball size goes to zero in some region under the ``Lyapunov umbrella'' (not everywhere in state space).  Perhaps this where I'm confused.} AS: Oops - I was being dumb. Yes, this should be fine. See [ZZX].
  \end{itemize}
    \item If the original kernel $K$ satisfied a Lyapunov function, then \textit{all} of our approximate kernels $\hat{P}$ will satisfy the \textit{same} Lyapunov inequality.  If we only know good level sets for the Lyapunov function outside of some compact set $k$, then we will get the \textit{original} Lyapunov function outside of $K$.
    \item Consequence of 1 and 2: If the original kernel $K$ satisfied a \textbf{minorization condition} on a compact set, then the new kernel will eventually satisfy a minorization condition on the same compact set. The minorizing measure will be the same, but the minorizing constant will be slightly worse. Remark: We really do need (2) here, as the result is otherwise false in general. The issue is that the original minorization inequality talks about short walks started from anywhere within a small set $C$. These short walks might exit $C$, and so we need to be sure that (with very high probability) they stay within a slightly-larger compact set $C" \supset C$ over their short walk. The Lyapunov condition (2) will guarantee this. Of course, instead one could simply restrict to one-step minorization bounds, where this is free.
    \item Immediate consequence of 1,2,3: Under these conditions, the approximate kernel will eventually have a unique stationary measure. Furthermore, this stationary measure will give very little mass to the ``tails'' as measured by the Lyapunov function.
    \item Immediate consequence of 3: If the original kernel $K$ is ``nice'' in the above sense, then the new kernel will eventually have a CLT, concentration of measure, etc etc. Furthermore, the constants appearing in those bounds will not be ``too much worse'' than the original constants. Thus, the error of a ``large'' MCMC sample will be bounded by something like $\mbox{[(Relaxation time of original kernel]}*\mbox{[some small multiplicative factor, e.g. 10]}/\sqrt{\mbox{[running time]}} + \mbox{[error due to bias]}$. Furthermore, as a consequence of 4, this ``error due to bias'' will not have a large term coming from the tails. In particular, the heuristic bias calculations in Andy's Ph.D. should be rigorous up to (i) errors in the occupation measure of the tails (which are controllable by the Lyapunov function + Markov's inequality), and (ii) the usual fact that bounds you get from concentration-of-measure have an extra $\log{n}$-or-so factor relative to bounds you get from the CLT (details of this ``$\log{n}$'' will depend on e.g. the Lyapunov function).
    \item Consequence: since the error due to bias goes to $0$ more quickly than $1/\sqrt{\mbox{[number of model evaluations]}}$ in many situations, we should get the desired faster-than-sqrt(something) convergence.
\end{enumerate}

\textbf{Some small concern:} One of the original motivations was to concentrate samples in the ``real'' mode (which might look e.g. like a kidney bean). The above theory can do this if you have a good guess about the Lyapunov function, but otherwise it more-or-less requires a certain amount of infill for the entire small set of the Lyapunov function (which might look e.g. like a large ball containing the original kidney bean). On the one hand, it is nice that the theory doesn't require infill if you have a good guess; on the other hand, we probably don't have a good a-priori guess. On the gripping hand, in practice we expect that in fact we should get a good Lyapunov function for the kidney bean around the same time that we get one for the surrounding ball, so in fact the theory claims here should more-or-less line up with the practice, even if there are some calculations in the middle that are very pessimistic.

\subsection{Geometric ergodicity of approximate chain}

\begin{assumption}
  Given transition kernels $Q$ and $K$, the Lyapunov condition states that for $L \in \{Q, K\}$, $$(LV)(x) = \int V(y) L(x, dy) \leq (1-a)V(x)+b,$$ where $V \geq 0$ is a function and $a>0$, $b<\infty$ are constants.
  \label{assumption:lyapunov-condition}
\end{assumption}

 Aside: the sets $C$ and $C^{\prime}$ are compact sets that contain almost all of the mass of the target. Thus, informally we can think of $C$ as being the ``mode'' and $C^{c}$ as being the ``tail.''

\begin{assumption}
  Minorization condition: Let $C = \{x \,:\, V(x) \leq 4b/a\}$.  For all $x \in C$, $$K^{s}(x,\cdot) \geq \epsilon \pi(\cdot)$$ for some $\epsilon > 0$, some $s \in \mathbb{N}$, and some measure $\pi$.
  \label{assumption:minorization-condition}
\end{assumption}

\begin{assumption}
  Approximation Condition: Let $C^{\prime} = \{x \,:\, V(x) \leq 20 b s/(\epsilon a) \}$. For all $x \in C^{\prime}$, $$\|K(x,\cdot) - Q(x,\cdot)\|_{\mathrm{TV}} \leq \epsilon/2.$$
  \label{assumption:approximation-condition}
\end{assumption}

\begin{lemma}
Under assumptions \ref{assumption:lyapunov-condition}, \ref{assumption:minorization-condition}, and \ref{assumption:approximation-condition}, for all $x \in C$, $Q^{s}(x,\cdot) \geq \epsilon \pi(\cdot) / 4.$
\end{lemma}

\begin{proof}
For $x \in C$, let $\{X_{t}\} \sim Q$ with starting point $X_{0} = x$. Appling \ref{assumption:lyapunov-condition} repeatedly, $$\mathbb{E}[V(X_{t})] \leq (1-a)^{t} V(x) + b/a \leq 5b/a.$$  Thus, by Markov's inequality,

\begin{equation}
  Q^{t}(x,C') \geq 1 - \epsilon/4s
  \label{eq:EscapeProbabilityInequality}
\end{equation}

\noindent for all $t \in \mathbb{N}$. We then note, for $x \in C$,

\begin{eqnarray}
  \| K^{s}(x,\cdot) - Q^{s}(x,\cdot) \|_{\mathrm{TV}} &\leq& \sum_{t=0}^{s-1} (1 - Q^{t}(x,C')) + \sum_{t=0}^{s-1} \sup_{x \in C'} \| Q(x,\cdot) - K(x,\cdot) \|_{\mathrm{TV}} \\
  &\leq& \frac{\epsilon}{4} + \frac{\epsilon}{2},
\end{eqnarray}

\noindent where in the last line we used \eqref{eq:EscapeProbabilityInequality} to bound the first term and assumption \ref{assumption:approximation-condition} to bound the second.  Combining this with assumption \ref{assumption:minorization-condition} completes the proof.
\end{proof}

\subsection*{Technical Lemmas and Proof Sketch}

\subsubsection*{Comparing Densities}
First, we check that good pointwise estimates of the \textit{unnormalized} log-likelihood will lead to good pointwise estimates of the \textit{normalized} probability density:

\begin{lemma}
Let $p_1 = Z_1^{-1} f_1$, $p_2 = Z_2^{-1} f_2$ with $Z_{1}, Z_{2} \in (0, \infty)$ be two probability measures on the same state space. Then we have the pointwise inequality $$\vert p_1 - p_2 \vert \leq p_1 \frac{\vert Z_2 - Z_1 \vert }{Z_{2}} + \frac{|f_1 - f_2|}{Z_2}.$$
\label{lem:LemmaCloseNorm}
\end{lemma}
\begin{proof}
One has the pointwise identity\todo{was $Z_{2}$} $$p_1 - p_2 = \frac{1}{Z_{2}} \left( f_1 \frac{Z_{2} - Z_{1}}{\textcolor{red!90!black}{Z_{1}}} + f_{1} - f_{2} \right) = p_1 \frac{Z_{2} - Z_{1}}{Z_{2}} + \frac{f_{1} - f_{2}}{Z_{2}}.$$

The result follows from the triangle inequality.
\end{proof}

This gives the harder-to-state but easier-to-use corollary:

\begin{lemma}
Fix $f_{1} \,:\, \mathbb{R}^{d} \mapsto [0,\infty)$ with $Z_{1} \equiv \int f_{1}(x) dx < \infty$ and $\sup_{x} f_{1}(x) \equiv M < \infty$ and define $p_{1} = Z_{1}^{-1} f_{1}$. Assume there exists a compact set $C \subset \mathbb{R}^{d}$ for which \todo{was $C$}

\begin{equation}
\int_{\textcolor{red!90!black}{C^{c}}} p_{1}(x) dx < \epsilon,
\label{eq:CloseEst1}
\end{equation}

\noindent Then there exists $\delta = \delta(Z_{1}, M, \vol{(C)}) > 0$ \todo[color=red!25]{Why do we need to define $\delta$?} and $A = A(Z_{1},M, \vol{(C)}) < \infty$ so that for all $f_{2} \,:\, \mathbb{R}^{d} \mapsto [0,\infty)$ (and corresponding $Z_{2} = \int f_{2}(x) dx < \infty$, $p_{2} = Z_{2}^{-1} f_{2}$) satisfying:

\begin{enumerate}
\item for the same compact set $C$, \todo{was $C$} $$\int_{\textcolor{red!90!black}{C^{c}}} p_{2}(x) dx < \epsilon,$$
\item and
\begin{equation}
\sup_{x \in C} \vert f_{1}(x) - f_{2}(x) \vert < \epsilon,
\label{eq:CloseEst2}
\end{equation}
\end{enumerate}

\todo[inline,color=red!25]{Concern: in the local approximation algorithm we do satisfy (2) in $C$.  However, it is \emph{not} true that $\int f_{2}(x) dx < \infty$.  In fact, the opposite is almost definately true: $\int f_{2}(x) dx = \infty$.  We could tweak this by satisfying $Z_{2} = \int_{C} f_{2}(x) dx < \infty$ and $p_{2} = Z_{2}^{-1} f_{2}$ for $x \in C$ and $p_2 = 0$ otherwise.  This implies that $\int_{C^{c}} p_2(x) dx = 0$.}

\noindent we have $$\| p_{1} - p_{2} \|_{\mathrm{TV}} \leq A \epsilon.$$
\end{lemma}

\begin{proof}
By inequalities \eqref{eq:CloseEst1} and \eqref{eq:CloseEst2},

\begin{align*}
\vert Z_{1} - Z_{2} \vert &\leq \int_{C} \vert f_{1}(x) - f_{2}(x) \vert dx + \int_{C^{c}} \vert f_{1}(x) - f_{2}(x) \vert dx \\
&\leq \int_{C} \epsilon dx + \int_{C^{c}} f_{1}(x) dx + \int_{C^{c}} f_{2}(x) dx \\
&\leq \epsilon(\vol{(C)} + 2).
\end{align*}

\noindent Thus, for all $0 < \epsilon < \textcolor{red!90!black}{\frac{1}{2} Z_{1} / (\vol{(C)} + 2)}$\todo{was $\frac{1}{2} Z_{1} (\vol{(C)} + 2)$} sufficiently small, we have $\vert Z_{1} - Z_{2} \vert < \frac{1}{2} Z_{1}$ \textcolor{blue!90!black}{(note: this implies that $1/Z_{2} \leq 2/Z_{1}$)}. For such $\epsilon$, Lemma \ref{lem:LemmaCloseNorm} gives for all $x \in C$ the pointwise bound:

\begin{align*}
\vert p_{1}(x) - p_{2}(x) \vert &\leq  p_1 \frac{\vert Z_2 - Z_1 \vert}{Z_2}  + \frac{\vert f_1 - f_2\vert }{Z_2} \\
&\leq \frac{2M}{Z_{1}} |Z_{2} - Z_{1}| + \frac{2}{Z_{1}} \epsilon \\
&\leq \epsilon \left( \frac{2M}{Z_{1}}(\vol{(C)} + 2) + \frac{2}{Z_{1}} \right).
\end{align*}

\noindent Combining this with Inequality \eqref{eq:CloseEst2}, we have for all measurable subsets $E \subset \mathbb{R}^{d}$

\begin{align*}
|p_{1}(E) - p_{2}(E)| &\leq |p_{1}(E \cap C) - p_{2}(E \cap C)| + |p_{1}(E \cap C^{c}) - p_{2}(E \cap C^{c})| \\
&\leq \sup_{x \in C} |p_{1}(x) - p_{2}(x)| + p_{1}(C^{c}) + p_{2}(C^{c}) \\
&\leq \epsilon  \left( \frac{2M}{Z_{1}}(Vol(C) + 2) + \frac{2}{Z_{1}}  + 2 \right).
\end{align*}

\noindent This completes the proof.

\end{proof}

We quickly check that Lyapunov conditions imply small mass in the tails:

\begin{lemma}
Let $K$ be the transition kernel of an ergodic Markov chain that satisfies the  Lyapunov inequality

\begin{equation}
  (KV)(x) \leq (1-a) V(x) + b
  \label{eq:IneqLyapSimp}
\end{equation}

\noindent for some function $V \geq 0$ and constants $0 < a < 1$, $0 < b < \infty$. Let $\pi$ be the stationary measure of $K$. Then $$\pi(\{x \, : \, V(x) > C\}) \leq \frac{b}{aC}.$$
\end{lemma}

\begin{proof}
  Let $X_{0} \sim \pi$. Using stationarity in the first line, and Inequality \eqref{eq:IneqLyapSimp} in the second, $$\E[V(X_{0})] = \E[V(X_{1})] \leq (1 - a) \E[V(X_{0})] + b.$$  Solving, one has $$\E[V(X_{0})] \leq \frac{b}{a}.$$  The result then follows from Markov's inequality.
\end{proof}

Relatedly, we check that Lyapunov conditions imply all compact sets get hit often:

\begin{lemma}

Let $Q$ be a reversible transition kernel for which the measure $Q(x,\cdot)$ has a density $q_{x}(\cdot)$ that satisfies

\begin{equation}
\inf_{x \in A, y \in B} q_{x}(y) > 0
\label{eq:IneqMinComp}
\end{equation}

\noindent for all pairs of compact sets $A,B \subset \mathbb{R}^{d}$.

[AS: I will definitely have to clean up the following assumptions at some point.]
Let $\{p_{t}\}_{t \geq 0}$ be a sequence of probability densities. This sequence may be random, but we assume it is adapted to a filtration $\{ \mathcal{F}_{t} \}_{t \geq 0}$. Let $K_{t}$ be the Metropolis-Hastings kernel with proposal $Q$ and target $p_{t}$. Assume that, for all $t$, $K_{t}$ satisfies the Lyapunov inequality $$(K_{t}V)(x) \leq (1-a) V(x) + b$$
for the \textit{same} continuous function $V \geq 0$ and constants $a> 0$, $b <\infty$. Assume also $\lim_{r \rightarrow \infty} \inf_{x \, : \, |x| \geq r}V(x) \rightarrow \infty$.

Finally, We let $\{X_{t}\}_{t \geq 0}$ be the ``obvious" Markov chain associated with $\{K_{t}\}_{t \geq 0}$, and let $\{X_{t}^{\ast} \}_{t \geq 0}$ be the ``obvious" sequence of points that are proposed (but perhaps not accepted) during the Metropolis-Hastings algorithm. We assume that $$\mathbb{P}[X_{t+1} \in \cdot | \mathcal{F}_{t}, X_{t}] = K_{t}[X_{t},\cdot]$$
and $$\mathbb{P}[X_{t+1}^{\ast} \in \cdot | \mathcal{F}_{t}, X_{t}] = Q[X_{t},\cdot].$$

[AS: If we talk about $V$ growing with some rate, we can replace the following conclusion ``all compact sets are hit infinitely often" with actual bounds on how long it takes to hit a certain number of times. I think we probably want to do this, but I give the easy lemma first until we're a bit more sure of what we want.] Then for any measurable compact set $A \subset \mathbb{R}^{d}$ with strictly positive Lebesgue measure,
\[
\lim_{n \rightarrow \infty} |\{1 \leq m \leq n \, : \, X_{m}^{\ast} \in A\}| = \infty
\]
almost surely.

\end{lemma}

\begin{proof}
By the standard proof of Lyapunov stability,
\[
\lim_{n \rightarrow \infty} |\{1 \leq m \leq n \, : \, V(X_{n}) \leq \frac{4b}{a} \}| = \infty
\]
almost surely. Since $V$ is continuous and grows, the set $\{x \, :\, V(x) \leq \frac{4b}{a} \}$ is compact and so the result follows from Inequality \eqref{eq:IneqMinComp}.

\end{proof}

\section{Trying Again}

The theory in the previous section is for a fixed approximate chain. This isn't terrible, but it would probably be better to deal with the ``true" adaptive chain. We try to do that nw.

\subsection*{General Coupling and Variance Estimates for Good Approximation Processes}

We begin by setting up notation:

\begin{assumption}
Let $Q$ be a reversible transition kernel for which the measure $Q(x,\cdot)$ has a density $q_{x}(\cdot)$ that satisfies

\begin{equation}
\inf_{x \in A, y \in B} q_{x}(y) > 0
\label{eq:IneqMinComp}
\end{equation}

\noindent for all pairs of compact sets $A,B \subset \mathbb{R}^{d}$.

[AS: I will definitely have to clean up the following assumptions at some point.]
Let $\{p_{t}\}_{t \geq 0}$ be a sequence of probability densities. This sequence may be random, but we assume it is adapted to a filtration $\{ \mathcal{F}_{t} \}_{t \geq 0}$. Let $K_{t}$ be the Metropolis-Hastings kernel with proposal $Q$ and target $p_{t}$. Assume that, for all $t$, $K_{t}$ satisfies the Lyapunov inequality $$(K_{t}V)(x) \leq (1-a) V(x) + b$$
for the \textit{same} continuous function $V \geq 0$ and constants $a> 0$, $b <\infty$. Assume also $\lim_{r \rightarrow \infty} \inf_{x \, : \, |x| \geq r}V(x) \rightarrow \infty$.

Finally, We let $\{X_{t}\}_{t \geq 0}$ be the ``obvious" Markov chain associated with $\{K_{t}\}_{t \geq 0}$, and let $\{X_{t}^{\ast} \}_{t \geq 0}$ be the ``obvious" sequence of points that are proposed (but perhaps not accepted) during the Metropolis-Hastings algorithm. We assume that $$\mathbb{P}[X_{t+1} \in \cdot | \mathcal{F}_{t}, X_{t}] = K_{t}[X_{t},\cdot]$$
and $$\mathbb{P}[X_{t+1}^{\ast} \in \cdot | \mathcal{F}_{t}, X_{t}] = Q[X_{t},\cdot].$$

\end{assumption}


\subsection*{Guaranteeing Good Approximation Property}

\subsection*{Putting It Together}
