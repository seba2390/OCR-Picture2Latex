\subsection{Inferring aquifer transmissivity} \label{sec:tracer}

Now we demonstrate the usefulness of LA-MCMC in a large-scale example: inferring the spatially heterogeneous transmissivity of an unconfined aquifer. In this problem, the likelihood contains a set of coupled partial differential equations that model transport of a nonreactive tracer through the aquifer; the tracer concentration is then observed, with noise, at selected locations in the domain. Each likelihood evaluation is thus  computationally intensive. 
% The transmissivity is a spatially distributed field that is described with $d=10$ parameters. 

More specifically, the tracer is advected and diffused through the unconfined aquifer (a groundwater resource whose top boundary is not capped by an impermeable layer of rock/soil) by a steady velocity field. The aquifer is divided into ten irregularly shaped regions with a river running along the right-hand side (Figure \ref{fig:tracer-transport-domain}). A well is located at $(x_w, y_w) = (0.2, 0.75)$. Soil properties are parameterized by the transmissivity $\kappa(x,y)$, which we take to be piecewise constant in each region. Given a value of $\kappa$, we first compute the steady state hydraulic head $h$ by solving
\begin{subequations}
\begin{equation}
    - \nabla \cdot (\kappa h \nabla h) = f_h
\end{equation}
on the unit square $[0,1]^2 \ni (x,y)$, with Dirichlet boundary conditions $h(0, y) = (y + 5)/2$, $h(1, y) = (y + 3)/2$, and homogeneous Neumann boundary conditions on the $y=0$ and $y=1$ boundaries. We set the source term above to:
\begin{equation*}
    f_h(x,y) = 10 \exp{(-((x-x_w)^2+(y-y_w)^2)/0.005)},
\end{equation*}
\label{eq:hydraulic-head}
\end{subequations}
which models hydraulic forcing due to well pumping. The velocity is then
\begin{equation}
    u = -\kappa h \nabla h.
    \label{eq:tracer-velocity}
\end{equation}
The hydraulic head and velocity resulting from the ``true'' parameter values in Table \ref{tab:tracer-log-kappa} are illustrated in Figure \ref{fig:tracer-transport-hydraulic-head}. Given the steady-state velocity field $u$ and an initial tracer concentration $c(x, y, 0) = 0$, advection and diffusion of the tracer throughout the domain is modeled by a time-dependent concentration field $c(x,y,t)$ that obeys the following transport equation:
\begin{equation}
    \frac{\partial c}{\partial t} + \nabla \cdot ((d_m \mathbf{I} + d_{\ell} u u^T) \nabla c) - u^T \nabla c = -f_t
    \label{eq:tracer-concentration}
\end{equation}
with $d_m=0.01$, $d_{\ell}=0.01$, and $f_t(x,y) = f_h(x,y)$. The initial conditions $c_0(x,y)$ and tracer forcing $f_t(x,y)$ model an initial tracer release at the well, followed by continued leakage into the domain. The concentration field at time $t = 1$ is shown in Figure \ref{fig:tracer-transport-hydraulic-head}. We numerically implement this tracer-transport model using the finite element method, via the software package FEniCS \citep{FENICS}. We evolve the tracer concentration in time using an adaptive time integration scheme implemented in Sundials \citep{sundials}. 

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_Domain.png}
    \caption{Spatial domain for the unconfined aquifer model. Each of the $10$ colored areas corresponds to a region of constant transmissivity. The white diamond in the top left denotes a well location, and there is a river along the right side of the domain.}
    \label{fig:tracer-transport-domain}
\end{figure}
\begin{table}[h!]
    \centering
    % \begin{tabular}{c|c|c|c}
    \begin{tabular}{c|c|c}

        Parameter & ``Truth'' & Prior mean $\bar{\theta}$ 
        % & MAP 
        \\ \hline \hline
        % $\theta_1 = \log{(\kappa_1)}$ & $-1$ & $-1.1$ & $\approx -1.1$\\
        % $\theta_2 = \log{(\kappa_2)}$ & $-0.25$ & $-0.1$ & $\approx -0.025$\\
        % $\theta_3 = \log{(\kappa_3)}$ & $-0.5$ & $-0.6$ & $\approx -0.52$ \\
        % $\theta_4 = \log{(\kappa_4)}$ & $-2$ & $-1.8$ &  $\approx -1.8$ \\
        % $\theta_5 = \log{(\kappa_5)}$ & $-0.5$ & $-0.4$ & $\approx -0.48$\\
        % $\theta_6 = \log{(\kappa_6)}$ & $-3$ & $-2.5$ &  $\approx -2.8$ \\
        % $\theta_7 = \log{(\kappa_7)}$ & $-1$ & $-1.2$ & $\approx -0.9$ \\
        % $\theta_8 = \log{(\kappa_8)}$ & $-1$ & $-0.9$ & $\approx -0.9$ \\
        % $\theta_9 = \log{(\kappa_9)}$ & $-1.5$ & $-1.3$ & $\approx -1.5$ \\
        % $\theta_{10} = \log{(\kappa_{10})}$ & $-1.25$ & $-1.25$ & $\approx -1.3$
        $\theta_1 = \log{(\kappa_1)}$ & $-1$ & $-1.1$ \\
        $\theta_2 = \log{(\kappa_2)}$ & $-0.25$ & $-0.1$ \\
        $\theta_3 = \log{(\kappa_3)}$ & $-0.5$ & $-0.6$ \\
        $\theta_4 = \log{(\kappa_4)}$ & $-2$ & $-1.8$ \\
        $\theta_5 = \log{(\kappa_5)}$ & $-0.5$ & $-0.4$ \\
        $\theta_6 = \log{(\kappa_6)}$ & $-3$ & $-2.5$  \\
        $\theta_7 = \log{(\kappa_7)}$ & $-1$ & $-1.2$  \\
        $\theta_8 = \log{(\kappa_8)}$ & $-1$ & $-0.9$  \\
        $\theta_9 = \log{(\kappa_9)}$ & $-1.5$ & $-1.3$  \\
        $\theta_{10} = \log{(\kappa_{10})}$ & $-1.25$ & $-1.25$
    \end{tabular}
    \caption{Parameter values used to generate the data, along with the prior mean for each of the ten log-transmissivity parameters. Each parameter corresponds to a numbered region of Figure \ref{fig:tracer-transport-domain}.}
    \label{tab:tracer-log-kappa}
\end{table}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_HydraulicHead.pdf}
    \caption{The steady-state hydraulic head and velocity field computed by solving \eqref{eq:hydraulic-head} and \eqref{eq:tracer-velocity} given the ``true'' parameter values in Table \ref{tab:tracer-log-kappa}. The white diamond represents the location of a well in the aquifer.}
    \label{fig:tracer-transport-hydraulic-head}
\end{figure}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_ConcentrationStep5.pdf}
    \caption{Tracer concentration computed at time $t=1$ by solving \eqref{eq:tracer-concentration} given the ``true'' parameter values in Table \ref{tab:tracer-log-kappa} and the steady state velocity field shown in Figure \ref{fig:tracer-transport-hydraulic-head}. The white diamond represents the location of a well in the aquifer, and the circles are sensor locations used in the Bayesian inference problem.}
    \label{fig:tracer-transport-concentration}
\end{figure}

Our goal is to infer the log-transmissivity parameters $\theta_i = \log{(\kappa_i)}$ for $i \in \{1,\ldots,10\}$, given observations of the tracer concentration. We endow these parameters with a Gaussian prior distribution, $\theta \sim N(\bar{\theta}, 0.05 {I})$, where $\bar{\theta}$ is the prior mean given in Table \ref{tab:tracer-log-kappa} and ${I}$ is the identity matrix of appropriate dimension. Data are collected at multiple sensor locations: at the well $(x_w, y_w) = (0.2, 0.75)$, and at a $9 \times 9$ array of points evenly spaced in $x \in [0.1, 0.9]$ and $y \in [0.1, 0.9]$. These locations are marked in Figure \ref{fig:tracer-transport-concentration}. We make observations at $10$ evenly spaced times $t \in [0, 1]$. The observations are modeled as,
\begin{equation}
    y = f(\theta) + \varepsilon,
\end{equation}
where $f(\theta)$ is the \textit{forward model} induced by the partial differential equations above, mapping the log-transmissivity parameters $\theta$ to a prediction of the tracer concentration at the chosen sensor locations/times. The sensor noise is $\varepsilon \sim N(0, 10^{-2} {I})$, yielding the conditional distribution 
%\todo{is the noise variance $10^{-5}$ or $10^{-2}$? Not consistent above and below. AD: It should be $10^{-2}$}
\begin{equation}
y \vert \theta \sim  N(f(\theta), 10^{-2} I).
\end{equation}
The posterior density then follows from Bayes' rule:
\begin{eqnarray}
    \pi(\theta \vert y) &\propto& \pi(y \vert \theta) \pi(\theta)
    % \\ &=& N(y; f(\theta), 10^{-2} \mathbf{I}) N(\theta; \bar{\theta}, 0.25 \mathbf{I}).
    \label{eq:tracer-posterior}
\end{eqnarray}
To avoid a so-called ``inverse crime,'' \edits{where synthetic data are generated from the exact model used to perform inference \citep{kaipiosomersalo2006},} we generate the data $y$ by solving the forward model with a well-refined $100 \times 100$ numerical discretization using the ``true'' parameters in Table \ref{tab:tracer-log-kappa}, and then perform the inference using a $50 \times 50$ numerical discretization of the forward model.

Evaluating the forward model at every MCMC step is computationally prohibitive. The run time of the model is approximately $5$--$15$ seconds (on a 2.6 GHz ten-core Xeon processor), depending on the values of log-transmissivity parameters $\theta$. Generating $10^5$ samples would thus take roughly $10$ days of computation time. LA-MCMC is essential to making this Bayesian inference problem computationally feasible. Here, since prior density evaluations are trivial, we apply the approximation capability of LA-MCMC to the log-likelihood function, $\log p(y \vert \theta)$. 

% The error bounds in Section~\ref{sec:erroranalysis} assume that the local polynomial approximations are chosen from a polynomial space $\mathcal{P}$ over $\mathbb{R}^d$ of \emph{total degree} $p$. We implement this choice here, but we also compare it empirically with other choices of polynomial approximation space. 

Recall that the local polynomial surrogate \eqref{eq:local-polynomial-estimate} requires computing $q = \text{dim}(\mathcal{P})$ coefficients, and in the total-degree setting of our theory, we have $q = {{n+p}\choose{p}}$. This quantity grows rapidly with the dimension $d$ of the parameters for $p>1$. % (In the present example $d=10$.) 
Retaining a total-degree construction, we could set $p=0$ and thus include only a constant term in $\mathcal{P}$. This results in a constant surrogate model within each ball; now the number of terms in the local polynomial approximation is always one, independent of the parameter dimension. Similarly, setting $p=1$ lets $q$ grow only linearly with $d$. Yet in the one-dimensional example of Section~\ref{sec:1d-example}, we showed that including higher-degree terms in the approximation can reduce the number of expensive target density evaluations required to achieve a given accuracy (see Figure \ref{fig:1d-example-order}).

In the present higher-dimensional example ($d=10$), we will implement these total-degree local polynomial approximations but also compare them empirically with other choices of polynomial approximation space---in particular, truncations that retain higher-order terms more selectively. A common practice in high-dimensional  approximation is to employ \emph{sparse truncations} of the relevant index set \citep{BlatmanSudret2011}. Let $\psi_s(\theta_i)$ be a univariate polynomial of degree $s$ in $\theta_i$. (Typically, we choose an orthogonal polynomial family, but this choice is immaterial.) Each $d$-variate polynomial basis function $\phi_{\bm{\alpha}}(\theta) \coloneqq \prod_{i=1}^{d} \psi_{\alpha_i}(\theta_i)$ is defined by a multi-index $\bm{\alpha} \in \mathbb{N}_0^d$, where the integers $\alpha_i \geq 0$ are components of $\bm{\alpha}$. Now define the $\ell^\nu$ norm for $\nu > 0$ (a quasi-norm for $0 < \nu < 1$): 
% \todo{potential notation overload: we used $\nu_0$ and $\nu_1$ to define the Lyapunov function earlier. Is this $\nu$ without subscript okay? AD: I did not notice this, but am probably too familiar with this notation to be a good judge. Perhaps we change it to be safe? This $\nu$ is pretty isolated to this section though so maybe we are okay?}
\begin{equation}
    \|\bm{\alpha}\|_{\nu} = \left(\sum_{i=1}^{d} \alpha_i^{\nu}\right)^{1/\nu}.
\end{equation}
Given a maximum degree $p$, we can build our \emph{local} polynomial approximations in the polynomial space $\mathcal{P}_\nu^{p,d}$ spanned by basis functions with multi-indices $\|\bm{\alpha}\|_{\nu} \leq p$. When $\nu=1$, this recovers a total-degree expansion, which is consistent with the error bounds in Section~\ref{sec:erroranalysis}. However, the dimensionality of $\mathcal{P}_\nu^{p,d}$ decreases as $\nu \rightarrow 0$. We define the special case of $\nu=0$ to be a polynomial space with no cross terms: each multi-index has one nonzero element, and that element is at most $p$. 
%We can think of the polynomial approximations with the truncation $\nu < 1$ as being sparse in that they are ``missing'' cross terms.

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_PosteriorMarginalDensities.pdf} 
    \caption{One- and two-dimensional marginal distributions of the posterior distribution \eqref{eq:tracer-posterior} estimated using LA-MCMC with $\nu=1$, $\gamma_0=500$, $\gamma_1=0.5$, $\bar{\Lambda}=\infty$, $\tau_0 = 1$, $\eta = 1$, $k=75$, $p=2$, and $V(x) = \exp{(10^{-4} \|\theta-\theta_{\text{MAP}}\|)}$. Here, $\theta_{\text{MAP}} = \argmax_{\theta}{\pi(\theta \vert y)}$ is the posterior mode, \edits{which we obtain by maximizing the log-posterior using optimization algorithms in \texttt{nlopt} \citep{johnson2014nlopt}}. In the diagonal subplots, the red and blue lines show the prior and posterior marginals, respectively. Vertical lines denote the ``true'' parameter values.}
    \label{fig:tracer-transport-marginals}
\end{figure}

We now apply the LA-MCMC algorithm to the posterior distribution of the tracer transport problem. We tested five different configurations of LA-MCMC, parameters of which are given in Table~\ref{tab:tracer-mcmc-parameters}. The Lyapunov function $V$ used in the tail correction is defined using the posterior mode, $\bar{\theta} = \argmax_{\theta}{\pi(\theta \vert y)}$. The first two rows of Table~\ref{tab:tracer-mcmc-parameters}, with $\nu =1$, correspond to using total-degree linear and quadratic local polynomial approximations (consistent with our theory). The lower three rows, with $\nu < 1$, are sparse quadratic and cubic approximations. The required number of nearest neighbors $k$ is chosen to be slightly more than the number of points $q = \text{dim}(\mathcal{P}_\nu^{p,d}) $ required to interpolate. 
%\todo{Is there a rationale or formula for fixing $k$, given $q$? Also, should we add a column of $q$ values to Table~\ref{tab:tracer-mcmc-parameters}? Also, why is $k$ bold? I think perhaps $\nu$ and then $p$ should be the first two columns, then $q$ and $k$.}
Reducing $\nu$ reduces $q$ and $k$. Chains produced by all these algorithmic configurations yield essentially identical posterior estimates. A trace plot of selected states from the LA-MCMC chain corresponding to the $p=1$ configuration is shown in  Figure \ref{fig:tracer-mixing-linear}; traces for LA-MCMC with higher-degree local polynomial approximations are qualitatively similar, and thus omitted here for brevity. One- and two-dimensional marginals of the posterior distribution are shown in Figure \ref{fig:tracer-transport-marginals}. While this figure is generated with $\nu=1$ and $p=2$, the results are essentially identical for the other algorithmic configurations.

%Figures \ref{fig:tracer-mixing-linear}, \ref{fig:tracer-mixing-quadratic}, and \ref{fig:tracer-mixing-cubic} show that all of these scenarios explore the posterior distribution.


% IN PROGRESS HERE

% Carefully choosing higher-order terms to include the local polynomial surrogate model can drastically affect the performance of LA-MCMC. We generate five chains using LA-MCMC---one using a linear surrogate model, two with quadratic surrogate models, and two with cubic surrogate models. Table \ref{tab:tracer-mcmc-parameters} shows the parameters supplied to Algorithm \ref{alg:la-mcmc}; the bold columns show how varying the parameter $\eta$, which determines how to truncate the polynomial expansion, affects the required number of nearest neighbors $k$. We choose $k$ to be slightly more than the number of points $q$ required to interpolate. \todo{any particular rationale or formula for fixing $k$, given $q$? Also, should we add a column of $q$ values to the table? }
% For larger $\eta$, the number of terms in the expansion must also increase. Figure \ref{fig:tracer-mixing-linear} shows that LA-MCMC with a locally linear surrogate model successfully explores the posterior distribution---similar mixing plots for LA-MCMC with higher order local surrogate models show these chains exploring the same distribution.
% %Figures \ref{fig:tracer-mixing-linear}, \ref{fig:tracer-mixing-quadratic}, and \ref{fig:tracer-mixing-cubic} show that all of these scenarios explore the posterior distribution.

\begin{table}[h!]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c}
        degree $p$ & $\nu$ & $k$ & \# to interpolate ($q$) & $\gamma_0$ & $\gamma_1$ & $\eta$ & $\log{V(\theta)}$ \\ \hline \hline
        1 & $1$ & $15$ & $11$ & $3500$ & $1$ & $0.025$ & $10^{-2} \|\theta-\bar{\theta}\|$ \\
        2 & $1$ & $75$ & $66$ & $10^{4}$ & $1$ & $0.1$ & $10^{-2} \|\theta-\bar{\theta}\|$ \\
        2 & $0.5$ & $30$ & $21$ & $7500$ & $1$ & $0.01$ & $10^{-1} \|\theta-\bar{\theta}\|$ \\
        3 & $0.75$ & $80$ & $76$ & $7500$ & $1$ & $0.01$ & $0.05 \|\theta-\bar{\theta}\|$ \\
        3 & $0.5$ & $35$ & $31$ & $7500$ & $1$ & $0.01$ & $10^{-3} \|\theta-\bar{\theta}\|$ 
    \end{tabular}
    \caption{Parameter configurations for Algorithm \ref{alg:la-mcmc}, used to generate samples from the posterior distribution \eqref{eq:tracer-posterior} of the PDE/tracer transport problem. In all cases $\bar{\Lambda} = \infty$.}
    \label{tab:tracer-mcmc-parameters}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=0.475\textwidth]{fig_MixingCubicDiagonal.pdf}
  \caption{PDE/tracer transport problem: trace plot of two states of an LA-MCMC chain using a locally cubic ($p=3$) surrogate model with $\nu = 0.5$, which excludes any cross terms. The additional parameters defined on the last row of Table \ref{tab:tracer-mcmc-parameters}.}
  \label{fig:tracer-mixing-linear}
\end{figure}

% \begin{figure}
%   \centering
  
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingQuadratic.png} \\[\abovecaptionskip]
%     \small (a) $\nu = 1$
%   \end{tabular}
  
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingQuadraticDiagonal.png} \\[\abovecaptionskip]
%     \small (b) $\nu = 0.5$
%   \end{tabular}
  
%   \caption{Trace plot of the LA-MCMC chain using quadratic surrogate models with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. The vertical line marks the half-way point in the chain---we typically discard the first half as ``burn-in.''}
%   \label{fig:tracer-mixing-quadratic}
% \end{figure}

% \begin{figure}
%   \centering
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingCubicDiagonal.png} \\[\abovecaptionskip]
%     \small (a) $\nu = 0.5$
%   \end{tabular}
  
%   \begin{tabular}{@{}c@{}}
%     \includegraphics[width=0.475\textwidth]{fig_MixingCubicReduced.png} \\[\abovecaptionskip]
%     \small (a) $\nu = 0.75$
%   \end{tabular}
  
%   \caption{Trace plot of the LA-MCMC chain using cubic surrogate models with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. The vertical line marks the half-way point in the chain---we typically discard the first half as ``burn-in.''}
%   \label{fig:tracer-mixing-cubic}
% \end{figure}

\begin{figure}
\centering
  \includegraphics[width=0.475\textwidth]{fig_Refinements.pdf} 
  \caption{PDE/tracer transport problem: number of likelihood evaluations as a function of MCMC steps, for LA-MCMC chains with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. In all cases, we see a significant improvements (of three to four orders of magnitude) over the $10^6$ likelihood evaluations required by exact MCMC.}
  \label{fig:tracer-refinements}
\end{figure}

Though all these chains are successful in characterizing the posterior distribution, the required number of likelihood evaluations---and hence the overall computational cost---depend on a non-trivial relationship between the accuracy of the local approximations and the number of nearest neighbors required to define them. Figure \ref{fig:tracer-refinements} shows the number of likelihood evaluations $n$ as a function of the number of MCMC steps $t$ for each experiment in Table \ref{tab:tracer-mcmc-parameters}. We see that total-degree quadratic approximations (red line) require more likelihood evaluations for a given $t$ than linear approximations (purple line). Sparse ($\nu = 0.5$) quadratic models require fewer likelihood evaluations at a given $t$, however. And while cubic models with $\nu = 0.75$ require more likelihood evaluations than the other cases, reducing $\nu$ to 0.5 in the cubic case makes the number of  evaluations commensurate with those of the total-degree quadratic approximations. In all cases, we see significant improvements---between three and four orders of magnitude---over the $10^6$ likelihood evaluations that would be required by exact MCMC (at $t =10^6$, i.e., the right boundary of the horizontal axis).

It is important to note that our approach to controlling the bias-variance tradeoff, and thus the error thresholds for triggering refinement via \eqref{eq:local-error-bound} and \eqref{eq:error-threshold}, are derived for expansions of total degree $p$ and thus in principle applicable only for $\nu=1$. We use the same rules here for $\nu<1$, but this approach cannot guarantee that bias and variance decay at the same rate. Indeed, a full analysis of the local sparse approximations may depend on understanding the magnitudes of mixed derivatives of the target function $g$; this can become quite problem-specific, and we defer such an investigation to future work. The empirical results for $\nu<1$ here are intended to be practical and exploratory.

% We see that replacing a linear surrogate model with a quadratic one (purple and red lines) increases the total number of likelihood evaluations because the number of terms in the expansion grows exponentially. However, this can be offset by carefully choosing the terms in the polynomial expansion (blue line). When we replace the surrogate model with a truncated cubic polynomial expansion we see that the number of likelihood evaluations is substantially decreased. Here, we require that the error decay as if the expansion where third-order but do not include all of the terms in this expansion. Strictly speaking, this heuristic no longer guarantees that the squared bias and variance decay at the same rate.

The choice of local approximation not only affects the overall computational cost of each chain, but also its mixing. Figure \ref{fig:tracer-autocorrelation-ess}(a) shows the decay of autocorrelation of the first variable $\theta_0$ along the chain for each of the five tested LA-MCMC configurations. Figure~\ref{fig:tracer-autocorrelation-ess}(b) shows the number of effectively independent samples (i.e., the effective sample size \citep{Wolffetal2004}) produced by each configuration, as a function of the number of MCMC steps $t$. Chains employing local linear approximations ($p=1$) tend to mix more poorly than the others, while the local cubic approximation appears to produce the best mixing on this problem. 
%
Figure \ref{fig:tracer-autocorrelation-ess}(c), however, shows that increasing the complexity of the surrogate model does not necessarily make the algorithm more efficient \textit{as a function of the number of model evaluations} $n$. Under the ESS per model evaluation metric, we find that the most efficient scheme is a local quadratic model with no cross terms ($\nu=0.5$), followed by a total degree quadratic model. Intuitively, one would expect that higher-degree polynomials become less important as the local ball size decreases and, therefore, the extra model evaluations required to build a higher-degree surrogate yield diminishing returns. In theory, even a locally constant model will yield convergence, but in practice we find that local approximations of moderate degree (e.g., linear and quadratic) are helpful for efficiently exploring the target distribution in a reasonable number of MCMC steps.

\begin{figure}
  \centering

  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_Autocorrelation.pdf} \\[\abovecaptionskip]
    \small (a) Autocorrelation of $\theta_0$
  \end{tabular}
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_ESS.pdf} \\[\abovecaptionskip]
    \small (b) ESS per MCMC step 
  \end{tabular}
  
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_ESS_refinements.pdf} \\[\abovecaptionskip]
    \small (c) ESS per model evaluation
  \end{tabular}
  
  \caption{PDE/tracer transport problem, computational efficiency for LA-MCMC chains with parameters defined in Table \ref{tab:tracer-mcmc-parameters}. (a) autocorrelation as a function of lag between samples; (b) effective sample size (ESS) as a function of the number of MCMC steps; (c) ESS as a function of the number of likelihood evaluations (refinements).}
  \label{fig:tracer-autocorrelation-ess}
\end{figure}

% In all cases, we see drastic improvements over MCMC with exact evaluations: LA-MCMC requires fewer than $1500$ likelihood evaluations to generate $10^6$ samples and, in some cases requires fewer than $500$. This is an improvement of three to four orders of magnitude. Furthermore, the ratio of generated samples to likelihood evaluations increases as the chain gets longer. Surrogates built using higher-order polynomial expansions are more accurate given a fixed ball containing all of the nearest neighbors. However, such surrogates also require more nearest neighbors. 

