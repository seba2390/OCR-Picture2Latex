\section{Conclusions}
\label{sec:concs}
This paper investigated the design and convergence properties of local approximation MCMC (LA-MCMC) algorithms, which exploit regularity of the target density to build and refine local polynomial surrogate models during sampling. When density evaluations are expensive, these algorithms can reduce the computational cost of MCMC by orders of magnitude.
% in practical examples. 

We introduce a new LA-MCMC algorithm with two key features: (1) a rate-optimal refinement strategy that balances the decay of surrogate-induced bias with the decay of Monte Carlo variance; and (2) a modification to the Metropolis acceptance step of the algorithm, which helps ensure stability of the resulting chain.
%
Together these features enable broader applicability and stronger theoretical guarantees than previous efforts. Earlier versions of LA-MCMC \citep{Conradetal2016} employed a cross-validation heuristic in conjunction with random refinements of the surrogate model; the latter made possible a guarantee of ergodicity on compact state spaces or under strong tail conditions. In the present effort, we dispense with both cross-validation and random refinement, using instead the more principled bias-variance control strategy mentioned above. Not only is the resulting sampling method asymptotically exact; here we also show that the error in LA-MCMC estimates decays at approximately the expected $1/\sqrt{T}$ rate, where $T$ is the number of MCMC steps. Moreover, this rate holds without the restrictive tail conditions that were required, both in theory and in practice, for stability and convergence of previous versions of the algorithm. 
%
Using our refinement strategy, we also observe that the rate at which new target density evaluations are demanded decays with $T$. This means that as a function of the number of target density evaluations, convergence is \textit{faster} than that of standard geometrically ergodic MCMC, accelerating as $T$ increases, in accordance with the underlying function approximation machinery.

% This work investigated the convergence of local approximation MCMC (LA-MCMC) after a finite number of MCMC steps. This method reduces the computational cost of MCMC by leveraging regularity in the target density to build a local polynomial surrogate model during sampling. Replacing target density evaluations with the computationally cheaper surrogate model reduces the computational expense of MCMC. We extended previous theory---which showed that LA-MCMC is asymptotically exact given incremental and infinite surrogate refinement---by showing that the error decays at approximately the expected $1/\sqrt{T}$ rate, where $T$ is the number of MCMC steps. Additionally, a small tweak to the acceptance ratio probably ensures that LA-MCMC convergences even for heavier tailed distributions, which was a significant limitation of previous versions of the algorithm. We also devised a rate-optimal refinement strategy based on a trade-off between the surrogate bias and the Monte Carlo variance. 

Our numerical examples demonstrate both the predicted convergence rates and stability of the sampler for target distributions with heavier tails. We also show how LA-MCMC can be employed in a computationally intensive inference problem, where evaluations of the likelihood require solving a coupled set of nonlinear partial differential equations; in this example, motivated by inverse problems in groundwater hydrology, LA-MCMC reduces the number of expensive likelihood evaluations by roughly two orders of magnitude. 

While the present algorithm is well suited to expensive target densities on parameter spaces of moderate dimension (e.g., $d=9$ in the current PDE example), \moreedits{it becomes more challenging to apply in higher dimensions, even $d=15$ in our experience. The fundamental reason is that generic isotropic local polynomial approximation is subject to the curse of dimensionality, with at least ${{d+p}\choose{p}}$ nearest neighbors required to evaluate the surrogate at a given point. Hence a larger number of  target density evaluations are required as $d$ increases, and the benefits of function approximation over sampling begin to diminish. As with any function approximation problem, the key to handling higher dimensions is to exploit some form of structure, e.g., anisotropy or sparsity. For instance,} future work could investigate the use of sparse local polynomial approximations as way of reducing the complexity of the local surrogate in higher dimensions. The preliminary empirical study in this paper shows that sparse local polynomial approximation can be helpful, but further analysis---perhaps building on the analysis of \emph{global} sparse polynomial approximations of likelihood functions in Bayesian inverse problems \citep{schillings2014sparsity} \moreedits{and exploiting anisotropy}---is needed to understand the convergence properties of such approximations within LA-MCMC. \moreedits{Other methods of explicit dimension reduction \citep{zahm2018certified} may also be useful in this setting: the idea would be to find a low-dimensional subspace in which the posterior departs most strongly from the prior and to apply our LA-MCMC machinery to the log-likelihood function only within that subspace. This approach has antecedents in \citet{Cui2016etal}. In the present paper, however, we have focused on the essential approximation and sampling machinery of LA-MCMC and on the analysis of its convergence. Combinations with other techniques, while practically important, are left to future work.}



% ** BLAH still editing here quickly, sorry! Going to get knocked off plane wifi in a sec, though. Will finish on the next leg. **

% \moreedits{In general, building surrogate models in high dimension suffers from the curse of dimensional. Therefore, generically applying LA-MCMC to high dimensional problems is difficult. We advocate merging LA-MCMC with dimension reduction or the discovery of sparse local polynomial approximations. However, a detailed study these approaches is beyond the scope of this paper.}

% We also demonstrated the algorithm's performance in practice using two toy examples and by solving a Bayesian inference problem that requires repeated numerical PDE solves. In the two toy examples, we showed: 
% \begin{enumerate}
%     \item The bias-variance trade-off is a practical way of guaranteeing that the error decay rate is the same as MCMC with exact evaluations.
%     \item Our algorithmic tweak to the acceptance ratio indeed allows LA-MCMC to generate samples from heavier tailed distributions, even in cases where previous iterations of the algorithm wandered away from the distribution's mode.
% \end{enumerate}
% Finally, we demonstrated that LA-MCMC can be employed within Bayesian inference problems, such as the inferring the transmissivity field of an unconfined aquifer. In this example, we also showed that sparse polynomial basis expansions can mitigate the number of degrees of freedom that define the surrogate model. 

An open source implementation of the LA-MCMC algorithm introduced in this paper is available as part of the MUQ library (\url{http://muq.mit.edu}).
