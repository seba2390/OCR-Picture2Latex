\subsection{One-dimensional toy example} \label{sec:1d-example}

We first use the one-dimensional density
\begin{equation}
    \log{\pi(x)} \propto -0.5 x^2 + \sin{(4 \pi x)}, \ x \in \mathbb{R}
\end{equation}
to demonstrate how changing various algorithmic parameters affects the performance of LA-MCMC. Figure \ref{fig:1d-example-density} shows binned MCMC samples computed with a random-walk Metropolis algorithm that uses exact target evaluations, compared with binned samples from an LA-MCMC algorithm that uses the same proposal. The sample histograms match very closely. Figure \ref{fig:1d-example-error} shows the error indicator $\Delta(x)^{p+1}$ and error threshold \eqref{eq:error-threshold} computed in a single run of LA-MCMC. Both the error indicator and threshold depend on the current state $X_t$: the indicator depends on the local ball size, and the threshold depends on the Lyapunov function $V(X_t) = \exp{(\|X_t\|)}$. Intuitively, the Lyapunov function relaxes the refinement threshold in the tails of the distribution and thus prevents excessive refinement in low probability regions, where the ball size $\Delta(x)$ tends to be large. 

LA-MCMC algorithmic parameters for the runs described here and below are $\gamma_0=0.1$,  $\bar{\Lambda} = \infty$, $\tau_0=1$, $\eta = 0$, $k=2(p+1)$, and $V(x) = \exp{(\|x\|)}$ (see Algorithm \ref{alg:la-mcmc}). We use local polynomial degree $p=2$ and $\gamma_1 = 1$ unless otherwise indicated.

Let $\sigma^2_t$ and $\breve{\sigma}^2_t$ be running $t$-sample estimates of the target variance computed using sequences of samples $\{X_t\}_{t>0}$ generated by MCMC with exact evaluations (which we refer to as ``exact MCMC'' for shorthand) and LA-MCMC, respectively. As a baseline for comparison, we also compute a `high fidelity' approximation of the variance from 50 realizations of exact MCMC: $\bar{\sigma}^2 = \sum_{i=1}^{50} \sigma_{T,i}^2$, with $T = 10^{6}$. Then we evaluate the error in variance estimates produced by exact MCMC and LA-MCMC,
\begin{equation}
    \begin{array}{ccc}
        e_t = \vert \bar{\sigma}^2 - \sigma_t^2 \vert & \mbox{and} & \breve{e}_t = \vert \bar{\sigma}^2 - \breve{\sigma}_t^2 \vert.
    \end{array}
\end{equation}
We compute the expectations of these errors by averaging $e_t$ and $\breve{e}_t$ over multiple independent realizations of each chain.  

The bias-variance trade-off used to construct our algorithm (see Section \ref{sec:bias-variance-trade-off}) ensures that the error in an expectation computed with LA-MCMC decays at essentially the same rate as the error in an expectation computed with exact MCMC. However, we need to tune the initial error threshold $\gamma_0$ and initial level length $\tau_0$ to ensure that the expected errors are of the same magnitude. In general, we set $\tau_0=1$. The initial error threshold also determines the initial 
% \todo{why ``expected?'' AD: I changed this to the initial local radius. Although, this isn't quite right. If we where to freeze the error threshold and run a long chain this would be an upper bound on the radius at each point $x$.}
local radius $\Delta(x) = \gamma_0^{1/(p+1)} V(x)$. As a heuristic, we choose $\gamma_0$ so that the initial radius is smaller than the radius of a ball containing the non-trivial support of the target density. 

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_LA-MCMC.pdf}
    \caption{Binned MCMC samples computed with exact evaluations (grey line) and using local approximations (red line).}
    \label{fig:1d-example-density}
\end{figure}

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_LA-error.pdf}
    \caption{The local error indicator $\Delta(x)^{p+1}$ (blue line) and error threshold $\gamma_\ell(x)$ (Equation \eqref{eq:error-threshold}---red line) used to trigger refinement. The sharp lower bound for the error threshold (bottom border of the red region) is the error threshold with $V(x) = 1$. We, however, allow $V(x) = \exp(\|x\|)$ to be larger in the tails, which relaxes the error threshold in low probability regions. This example run triggered $472$ refinements. Many more (unnecessary) refinements would be required if the Lyapunov function did not relax the allowable error in the tails.}
    \label{fig:1d-example-error}
\end{figure}

The bias-variance trade-off renders LA-MCMC insensitive to the error decay rate $\gamma_1$, {as long as} $\gamma_1 \geq 0.5$. This is borne out in Figure \ref{fig:1d-example-gamma}(a), which shows that the error in the variance estimate decays at the same ($1/\sqrt{t}$) rate  as in the exact evaluation case for all values of $\gamma_1$ except $\gamma_1 = 0.25$. 
%
% \todo{Is what follows the explanation of why we need $\gamma_1 > 0.5$? Maybe this should move into Section 2.2.1. AD: This is why we need $\gamma_1>0.5$. I added a similar discussion in section 2.2.1---maybe we can remove it here now? Although there is some redundancy, I don't really mind having both. thoughts?} 
%
Recall that we impose a piecewise constant error threshold $\gamma_0 \ell^{-\gamma_1}$, fixed for each level $\ell(t)$. If $\gamma_1 < 0.5$, then the level lengths $\tau_l$ decrease as $t \rightarrow \infty$; see Section~\ref{sec:bias-variance-trade-off}. In our practical implementation, we can increase $\ell$ at most once per MCMC step and, therefore, when the level length is less than one step, the error threshold cannot decay quickly enough. In this case, the surrogate bias dominates the error and, as we see in Figure \ref{fig:1d-example-gamma}(a), the error decays more slowly as a function of MCMC steps. 

Figure \ref{fig:1d-example-gamma}(b) shows that, since we do not evaluate the target density every MCMC step, the convergence of LA-MCMC as a function of the number of \emph{density evaluations} $n$ is in general much \emph{faster} than in the exact evaluation case.

\begin{figure}
  \centering
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_Convergence_MCMCSteps_gamma.pdf} \\[\abovecaptionskip]
    \small (a)
  \end{tabular}

  \vspace{\floatsep}

  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_Convergence_TargetEvals_gamma.pdf} \\[\abovecaptionskip]
    \small (b)
  \end{tabular}

  \caption{The expected errors in variance ($e_t$ and $\breve{e}_t$) averaged over $50$ MCMC chains as a function of (a) MCMC steps $t$ and (b) the number of target density evaluations $n$.  The LA-MCMC construction ensures that, if $\gamma_1  \geq 0.5$, the expected error decays at essentially the same $1/\sqrt{t}$ rate as in the exact evaluation case (Theorem~\ref{thm:convergence-rate}). Since we do not need to evaluate the target density at every MCMC step, and in fact evaluate the target much less frequently over time (see Figure~\ref{fig:1d-example-expected-refinements}), the error decays much more quickly as a function of the number of target density evaluations.}
  \label{fig:1d-example-gamma}
\end{figure}

Approximating the target density with polynomials of higher degree $p$ increases the efficiency of LA-MCMC; we explore this in Figure \ref{fig:1d-example-order}. Figure \ref{fig:1d-example-order}(a) shows that controlling the bias-variance trade-off ensures the error decay rate---as a function of the number of MCMC steps $t$---is the same regardless of $p$. Since the local error indicator is $\Delta(x)^{p+1}$, larger values of $p$ achieve the same error threshold with larger radius $\Delta(x) < 1$. Using higher-degree polynomials therefore requires \emph{fewer} target density evaluations, as shown in Figure \ref{fig:1d-example-order}(b). Yet higher-degree polynomials also require more evaluated points inside the local ball $\mathcal{B}_k(x)$. In this one-dimensional example, the local polynomial requires $p+1$ points to interpolate, and here we choose $k=2(p+1)$ nearest neighbors to solve the regression problem \eqref{eq:local-polynomial-estimate}. We thus see diminishing returns as $p$ increases: higher order polynomials achieve the same accuracy with larger $\Delta(x)$ but require more target density evaluations within each ball $\mathcal{B}_k(x)$. 
% \todo{Say anything about how this tradeoff will evolve with dimension $d$? Interesting to discuss, but maybe skip it now. AD: I had a similar thought but at this point it feels like wild speculation since we have only discussed a 1D example so far. We somewhat address this with the different types of expansions in the tracer sample, but not completely. I think this fits in with a way more general problem of how do we choose/adapt the polynomial basis? So, probably skip for now.}

\begin{figure}
  \centering
  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_Convergence_MCMCSteps_order.pdf} \\[\abovecaptionskip]
    \small (a)
  \end{tabular}

  \vspace{\floatsep}

  \begin{tabular}{@{}c@{}}
    \includegraphics[width=0.475\textwidth]{fig_Convergence_TargetEvals_order.pdf} \\[\abovecaptionskip]
    \small (b)
  \end{tabular}

  \caption{The expected errors $e_t$ and $\breve{e}_t$ averaged over $50$ MCMC chains as a function of (a) MCMC steps $t$, and (b) the number of target density evaluations $n$. Controlling the bias-variance trade-off ensures that error decays at essentially the same $1/\sqrt{t}$ rate as in the exact evaluation case. As we increase the order of the local polynomial approximation, however, LA-MCMC requires fewer target density evaluations $n$ to achieve the same error.}
  \label{fig:1d-example-order}
\end{figure}

As the number of MCMC steps $t \rightarrow \infty$, the rate at which LA-MCMC requires new target density evaluations---i.e., the refinement rate---slows significantly. Figure \ref{fig:1d-example-expected-refinements} illustrates this pattern: the number of target density evaluations increases much more slowly than $t$. While the bias-variance trade-off ensures that the error decay rate remains $1/\sqrt{t}$, viewed in terms of the number of target density evaluations $n$, the picture is different. If target density evaluations are the dominant computational expense---as is typical in many applications---then LA-MCMC generates samples more efficiently as $t \rightarrow \infty$.
% as a function of $n$. 

\begin{figure}[h!]
\centering
    \includegraphics[width=0.475\textwidth]{fig_ExpectedRefinements.pdf}
    \caption{The expected number of refinements $n(t)$, computed from $50$ independent LA-MCMC chains, given different local polynomial degrees $p$.
    % LA-MCMC parameters are $\gamma_0=0.1$, $\gamma_1 = 1$, $\bar{\Lambda}=\infty$, $\tau_0=1$, $\eta = 0$, $k=2(p+1)$, and $V(x) = \exp{(\|x\|)}$. 
    The refinement rate decreases as $t \rightarrow \infty$, making MCMC more efficient as $t$ increases.}
    \label{fig:1d-example-expected-refinements}
\end{figure}
