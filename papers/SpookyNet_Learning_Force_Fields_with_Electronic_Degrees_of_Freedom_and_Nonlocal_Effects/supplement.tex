\documentclass[%
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
reprint,
%preprint,
%preprintnumbers,
nofootinbib,
%nobibnotes,
%bibnotes,
amsmath,amssymb,amsfonts,
%aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
floatfix,
altaffilletter,
%citeautoscript,
showkeys,
%draft, %mark overfull lines
]{revtex4-2}
\setcitestyle{super}

\usepackage{microtype}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esvect} %nicer vectors with \vv{} instead of \vv{}
\usepackage{mathtools}
\usepackage{bm}
\usepackage[hidelinks]{hyperref}
\usepackage{pbox}
\usepackage{threeparttable}
\usepackage{xr}

%arabic section numbers
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

%S prefix for equations etc.
\renewcommand{\thepage}{S\arabic{page}} 
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}

%checkmark symbols
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}


\def\bibsection{\section*{\refname}}

\newcommand\todo[1]{\textcolor{red}{\textbf{[TODO: #1]}}}

\newcommand\comment[1]{\textcolor{blue}{\textbf{[#1]}}}

\newcommand{\nn}{SpookyNet}

%%% HELPER CODE FOR DEALING WITH EXTERNAL REFERENCES
\usepackage{xr}
\makeatletter
\newcommand*{\addFileDependency}[1]{
	\typeout{(#1)}
	\@addtofilelist{#1}
	\IfFileExists{#1}{}{\typeout{No file #1.}}
}
\makeatother

\newcommand*{\myexternaldocument}[1]{
	\externaldocument[M]{#1}
	\addFileDependency{#1.tex}
	\addFileDependency{#1.aux}
}
%%% END HELPER CODE

\myexternaldocument{main}

\makeatletter\@input{xxmain.tex}\makeatother
\begin{document}
	
%opening
\title{Supporting Information for \nn{}: Learning Force Fields with \\Electronic Degrees of Freedom and Nonlocal Effects}
\author{Oliver T. Unke}
\email{oliver.unke@googlemail.com}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin, 10587 Berlin, Germany}
\affiliation{DFG Cluster of Excellence ``Unifying Systems in Catalysis'' (UniSysCat), Technische Universit\"at Berlin, 10623 Berlin, Germany}

\author{Stefan Chmiela}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin, 10587 Berlin, Germany}

\author{Michael Gastegger}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin, 10587 Berlin, Germany}
\affiliation{DFG Cluster of Excellence ``Unifying Systems in Catalysis'' (UniSysCat), Technische Universit\"at Berlin, 10623 Berlin, Germany}

\author{Kristof T. Sch\"utt}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin, 10587 Berlin, Germany}

\author{Huziel E.\ Sauceda}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin, 10587 Berlin, Germany}
\affiliation{
	BASLEARN, BASF-TU joint Lab, Technische Universit\"at Berlin, 10587 Berlin, Germany
}

\author{Klaus-Robert M\"uller}
\email{klaus-robert.mueller@tu-berlin.de}
\affiliation{Machine Learning Group, Technische Universit\"at Berlin, 10587 Berlin, Germany}
\affiliation{Department of Artificial Intelligence, Korea University, Anam-dong, Seongbuk-gu, Seoul 02841, Korea}
\affiliation{Max Planck Institute for Informatics, Stuhlsatzenhausweg, 66123 Saarbr\"ucken, Germany}
\affiliation{BIFOLD -- Berlin Institute for the Foundations of Learning and Data, Berlin, Germany}
\affiliation{Google Research, Brain team, Berlin, Germany}




\maketitle
	

\section{Completeness of atomic descriptors in \nn{}}
\label{sec:atomic_descriptors}
\begin{figure*}
	\includegraphics[width=\textwidth]{figures/descriptors.pdf}
	\caption{Pairs of distinct atomic environments where all neighboring atoms (red) have the same distance from the central atom (blue). The distributions of angles (Eq.~\ref{eq:angles}) and dihedrals (Eq.~\ref{eq:dihedrals}) are visualized and values of the angular power spectrum invariants (Eq.~\ref{eq:power_spectrum}) for different angular momenta $l=1,\dots4$ (p, d, f, g) are given for each structure (the s invariant simply counts the number of neighbors and is therefore omitted). Since all distances to neighboring atoms are identical, descriptors need to be able to at least resolve angular information to distinguish the structures (\textbf{a}). However, for some structures, the power spectrum invariants may be degenerate for small values of $l$ (\textbf{b} and \textbf{d}). Some structures even have identical angular distributions, in which case the power spectrum invariants are equal for all $l=0,\dots,\infty$ and information about dihedrals is necessary to distinguish the environments (\textbf{c}). Note that some environments cannot even be distinguished when information about dihedrals is included (\textbf{d}).\cite{pozdnyakov2020incompleteness}}
	\label{fig:descriptors}
\end{figure*}
Many ML algorithms for constructing potential energy surfaces make use of some sort of descriptor to represent atoms in their chemical environment. As long as this description is \emph{complete}, any  atom-centered property (including atomic decompositions of extensive properties such as energy) can be predicted from the descriptors.\cite{glielmo2018efficient} In this context, \emph{completeness} means that structures which are not convertible into each other (by translations, rotations, or permutations of equivalent atoms) map to different descriptors.

A simple descriptor for the environment of an atom~$i$ at position~$\vv{r}_{i}$ consists of the set of distances $r_{ij} = \lVert \vv{r}_{ij}\rVert$ (with $\vv{r}_{ij} = \vv{r}_{j}- \vv{r}_{i}$) to neighboring atoms $j$, and the set of angles
\begin{equation}
\alpha_{ijk} = \mathrm{arccos}\left(\frac{\langle \vv{r}_{ij}, \vv{r}_{ik} \rangle}{\lVert \vv{r}_{ij}\rVert \lVert \vv{r}_{ik}\rVert}\right)
\label{eq:angles}
\end{equation}
between all possible combinations of neighboring atoms $j$ and $k$. For environments consisting of multiple different species, separate sets of distances (and angles) are necessary for each element (or combination of elements). However, for simplicity, it is assumed here that all atoms are identical. A disadvantage of using angles in the descriptor is that their computation scales $\mathcal{O}(n^2)$ with the number of neighbors $n$, because all combinations must be considered. An alternative to encode angular information, which scales $\mathcal{O}(n)$, is to replace the set of angles with invariants of the form
\begin{equation}
a_{i,l} = \sum_{m=-l}^{l} \left(\sum_{j=1}^{n} Y_l^m(\vv{r}_{ij})\right)^2
\label{eq:power_spectrum}
\end{equation}
derived from the angular power spectrum, where $Y_l^m$ are the spherical harmonics (see Eq.~\ref{Meq:spherical_harmonics}). In the following, $a_{i,l}$ for $l=0,1,2,3,4$ are called s, p, d, f, and g invariants because of their relation to the symmetries of atomic orbitals. The disadvantage here is that when using a finite number ($l=0,\dots,L$) of power spectrum invariants as angular descriptor, some environments with different sets of angles may lead to the same descriptor. For example, square planar and terahedral environments have the same s and p invariants ($L=1$), so it is necessary to include at least d invariants ($L=2$) in the descriptor to differentiate them (see Fig.~\ref{fig:descriptors}B).\\

There is a widespread belief in the literature that sets of distances and angles are sufficient for a \emph{complete} description of atomic environments.\cite{von2015fourier,kocer2020continuous} However, it was recently demonstrated that this is not the case, and even including the set of dihedrals
\begin{equation}
\delta_{ijkl} = \mathrm{arccos}\left(\frac{\langle \vv{r}_{ij}\times\vv{r}_{ik}, \vv{r}_{ik}\times\vv{r}_{il} \rangle}{\lVert\vv{r}_{ij}\times\vv{r}_{ik}\rVert \lVert \vv{r}_{ik}\times\vv{r}_{il}\rVert}\right)
\label{eq:dihedrals}
\end{equation}
between triplets of neighboring atoms does not lead to a \textit{complete} description in general.\cite{pozdnyakov2020incompleteness} In this context, it is interesting to investigate the \emph{completeness} of the atomic descriptors~$\mathbf{f}$~(see Eq.~\ref{Meq:atomic_descriptor})  learned by \nn{} and compare its ability to distinguish different structures to other popular approaches. For this purpose, five pairs of distinct atomic environments (shown in Fig.~\ref{fig:descriptors}), with geometries that are particularly difficult to separate, are considered. Then, different models are trained to predict scalar labels of $1$ (for one of the environments) and $-1$ (for the other environment) from the descriptors of the central atoms (blue) in each pair. It can be observed that models either learn to predict the labels with virtually zero error (up to numerical precision), i.e.\ the environments can be distinguished, or a value of $0$ is predicted for both central atoms, i.e.\ their environments are mapped to the same descriptor and a compromise between the contradictory labels has to be found. The results are summarized in Table~\ref{tab:descriptors}. For evaluating PhysNet\cite{unke2019physnet} and DimeNet,\cite{klicpera2020directional} the reference implementations available from \url{https://github.com/MMunibas/PhysNet} and  \url{https://github.com/klicperajo/dimenet} are used. PaiNN\cite{schutt2021equivariant} and NequIP\cite{batzner2021se} are evaluated using in-house implementations. For BPNN and SchNet, the implementations available in SchNetPack\cite{schutt2018schnetpack} are used. The FCHL18/19\cite{faber2018alchemical,christensen2020fchl} models are evaluated using the QML package.\cite{qmlpackage} All models were trained on multiple randomly rotated versions of the environments shown in Fig.~\ref{fig:descriptors}. This was done to prevent models picking up on differences due to floating point imprecision, which otherwise may make environments distinguishable even when their descriptors are degenerate (up to numerical noise).
\begin{table}
	\begin{threeparttable}
		\begin{tabular} {l l c c c c c }
			\toprule
			\bf model & \bf scaling & \bf A & \bf B & \bf C & \bf D & \bf E \\
			\midrule
			\multicolumn{7}{c}{\bf hand-crafted descriptors}\\
			BPNN\cite{behler2007generalized} & $\mathcal{O}(n^2)$ & \cmark & \cmark & \xmark & \cmark & \xmark \\
			FCHL19\cite{christensen2020fchl} & $\mathcal{O}(n^2)$ & \cmark & \cmark & \xmark & \cmark & \xmark \\
			FCHL18\cite{faber2018alchemical} & $\mathcal{O}(n^3)$\tnote{*} & \cmark & \cmark & \cmark\tnote{**} & \cmark & \xmark \\
			\midrule
			\multicolumn{7}{c}{\bf learned descriptors}\\
			SchNet\cite{schutt2018schnet} & $\mathcal{O}(n)$ &(\cmark) & (\cmark) & (\cmark) & (\cmark) & (\cmark) \\
			PhysNet\cite{unke2019physnet} & $\mathcal{O}(n)$ & (\cmark) & (\cmark) & (\cmark) & (\cmark) & (\cmark) \\
			DimeNet\cite{klicpera2020directional} & $\mathcal{O}(n^2)$ & \cmark & \cmark & (\cmark) & \cmark & (\cmark) \\
			NequIP\cite{batzner2021se} & $\mathcal{O}(n)$ &\cmark & (\cmark) & (\cmark) & (\cmark) & (\cmark) \\
			PaiNN\cite{schutt2021equivariant} & $\mathcal{O}(n)$ & \cmark & (\cmark) & (\cmark) & (\cmark) & (\cmark) \\
			\nn{} & $\mathcal{O}(n)$ & \cmark & \cmark & (\cmark) & (\cmark) & (\cmark) \\
			\bottomrule
		\end{tabular}
		\begin{tablenotes}\footnotesize
			\item[*] when dihedrals are included
			\item[**] only distinguishable with dihedrals
		\end{tablenotes}
	\end{threeparttable}
	\caption{Ability of models to differentiate the atomic environments shown in Fig.~\ref{fig:descriptors} (\cmark:~distinguishable, \xmark:~indistinguishable) and the scaling of their computational cost  with respect to the number of neighbors $n$. Message-passing neural networks with learned descriptors can distinguish all environments when there are $T\geq2$ message-passing steps. However, when only a single step is used ($T=1$), the environments marked with (\cmark) become indistinguishable.}
	\label{tab:descriptors}
\end{table}

Most models based on hand-crafted descriptors can only distinguish environments when their sets of distances and angles (and in some cases dihedrals) differs. Message-passing neural networks (MPNNs) on the other hand can learn to distinguish all environments shown in Fig.~\ref{fig:descriptors}, provided that at least $T\geq2$ message-passing steps are used. The amount of information that can be resolved with a single message-passing step ($T=1$) is often related to the power spectrum invariants (see Eq.~\ref{eq:power_spectrum}) and different MPNNs mainly differ in the maximum order $L$ which they can resolve in a single update (with the exception of DimeNet,\cite{klicpera2020directional} which uses angles directly but scales $\mathcal{O}(n^2)$ with the number of neighbors $n$). \nn{} uses an update with a maximum order of $L=2$, which is sufficient to differentiate most common chemical environments (as long as they are distinguishable by distances and angles). It would be possible to introduce higher order interactions with the symmetry of f-, or even g-orbitals into the update step (see Eq.~\ref{Meq:local_interaction}), so that additional environments (e.g.\ Fig.~\ref{fig:descriptors}d) become distinguishable with a single update, but this increases the computational cost and is found to give little benefit (in terms of additional accuracy for predictions) in practice.\\

%It should be noted that even if a descriptor cannot distinguish all atomic environments, this does not necessarily mean that a model relying on this descriptor must fail when predicting extensive properties such as the potential energy. For example, even though the environments of the central atoms (blue) shown in Fig.~\ref{fig:descriptors}C cannot be distinguished with a descriptor based on just distances and angles, the descriptors of neighboring atoms (red) will differ between both structures. Consequently, when the potential energy is modeled as a sum over atomic contributions, it is possible to compensate for the degenerate atoms with the energy contributions predicted for their neighbors.



\begin{table*}
	\begin{tabular}{r c c c c}
		\toprule
		& \multicolumn{2}{c}{\textbf{energy} [kcal~mol$^{-1}$]} & \multicolumn{2}{c}{\textbf{forces} [kcal~mol$^{-1}$~\AA$^{-1}$]}\\ 
		$n_{\rm train}$ & MAE & RMSE & MAE & RMSE\\
		\midrule
		10 & \quad 11.698 (2.440) & \quad 18.650 (4.889) & \quad 14.426 (3.280) & \quad 40.302 (16.473) \\
		100 & \quad 4.011 (1.688) & \quad 6.183 (1.969) & \quad 5.782 (1.436) & \quad 14.345 (4.730) \\
		1\,000 & \quad 0.607 (0.030) & \quad 1.646 (0.304) & \quad 1.360	(0.039) & \quad 5.326 (2.614) \\
		10\,000 &  \quad 0.078 (0.002) &  \quad 0.282 (0.009) &  \quad 0.249 (0.007) &  \quad 0.998	(0.032)\\
		100\,000 &  \quad 0.020	(0.001) &  \quad 0.071	(0.003)	&  \quad 0.071 (0.001)	& \quad 0.326 (0.012)\\
		1\,000\,000 &  \quad 0.020	(0.002)	&  \quad  0.036	(0.003) &  \quad  0.063	(0.006)	&  \quad 0.165	(0.015)\\
		\bottomrule
	\end{tabular}
	\caption{Mean absolute errors (MAEs) and root mean square errors (RMSEs) of energies and forces for the random CH$_4$ dataset\cite{ch4} suggested in Ref.~\citenum{pozdnyakov2020incompleteness}. Results are averaged over 16 ($n_{\rm train} = 10$), 8 ($n_{\rm train} = 100$), or 4 ($n_{\rm train} \geq 1000$) random splits and the standard deviation between runs is given in brackets.}
	\label{tab:ceriotti_ch4_results}
\end{table*}


\begin{figure}
	\includegraphics[width=\columnwidth]{figures/ceriotti_ch4_learning_curve.pdf}
	\caption{Energy learning curves for the random CH$_4$ dataset\cite{ch4} suggested in Ref.~\citenum{pozdnyakov2020incompleteness}. \nn{} (black) is compared to feedforward neural networks trained on 2-body and/or 3-body features (red, blue, magenta), see Ref.~\citenum{pozdnyakov2020incompleteness} for details.}
	\label{fig:ceriotti_ch4_learning_curves}
\end{figure}

\citeauthor{pozdnyakov2020incompleteness} propose a different \emph{completeness} test based on a data set of $\sim$7.7M CH$_4$ structures\cite{ch4} that were generated by randomly placing H atoms in a 3~\AA{} sphere around the C atom (see Ref.~\citenum{pozdnyakov2020incompleteness} for details). Due to the strongly distorted geometries, potential energies in this data set vary by $\sim$1400 kcal~mol$^{-1}$ and forces by $\sim$10700 kcal~mol$^{-1}$~\AA$^{-1}$. Further, this way of sampling will lead to many structures with (nearly) degenerate sets of angles (see Fig.~\ref{fig:descriptors}c) and is thus particularly challenging to learn. For this task, it is to be expected that models relying on \emph{incomplete} descriptors improve at a slower rate (and eventually cease to improve at all) when increasing the number of training data.\cite{pozdnyakov2020incompleteness} The performance of \nn{} on this data set for different training set sizes is summarized in Table~\ref{tab:ceriotti_ch4_results}. With only 10 training points ($\sim$0.00013\% of the data), \nn{} reaches prediction errors that correspond to a relative absolute error of just $\sim$1\% (with respect to the energy range covered in the data set). Chemical accuracy (absolute errors $<$1~kcal~mol$^{-1}$) is reached with as few as 1000 training points. The learning curve (see Fig.~\ref{fig:ceriotti_ch4_learning_curves}) shows that the performance of \nn{} increases steadily when more data is used for training while being about two orders of magnitude more data-efficient than other methods. 

\begin{figure*}
	\includegraphics[width=\columnwidth]{figures/spin_effect.pdf}
	\caption{Local chemical potential for a random carbene chosen from the QMSpin database (the optimized geometries for the singlet/triplet states are shown). The chemical potential for a model without spin embeddings lacks features, whereas a model with spin embeddings learns a rich representation with significant differences between singlet and triplet states.}
	\label{fig:different_spin_gummybears}
\end{figure*}

\begin{figure*}
	\includegraphics[width=\columnwidth]{figures/activation.pdf}
	\caption{Generalized SiLU activation (Eq.~\ref{Meq:activation}). For $\alpha=1,\beta=\infty$, $\mathrm{silu}(x)$ is equivalent to $\mathrm{max}(x,0)$ (also known as $\mathrm{ReLU}$ activation), whereas for $\alpha=2,\beta=0$, the identity function is obtained.}
	\label{fig:activation}
\end{figure*}



\begin{table*}
	\begin{tabular}{c c c c c c c c c c c c c c c c c c c c c c c c}
		\toprule
		element & & & Z & 1s & 2s & 2p & 3s & 3p & 4s &  3d & 4p & 5s & 4d & 5p & 6s & 4f & 5d & 6p & vs & vp & vd & vf & \\
		\midrule
		H & $\mathbf{d}'_{1} =$ & $[$ & 1 & 1 & 0 & 0 & 0 & 0 & 0 &  0 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 1 & 0 &  0 &  0 & $]^{\mathsf{T}}$\\
		C & $\mathbf{d}'_{6} =$ & $[$ & 6 & 2 & 2 & 2 & 0 & 0 & 0 &  0 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 2 & 2 &  0 &  0 & $]^{\mathsf{T}}$\\
		N & $\mathbf{d}'_{7} =$ & $[$ & 7 & 2 & 2 & 3 & 0 & 0 & 0 &  0 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 2 & 3 &  0 &  0 & $]^{\mathsf{T}}$\\
		O & $\mathbf{d}'_{8} =$ & $[$ & 8 & 2 & 2 & 4 & 0 & 0 & 0 &  0 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 2 & 4 &  0 &  0 & $]^{\mathsf{T}}$\\
		P & $\mathbf{d}'_{15} =$ & $[$ & 15 & 2 & 2 & 6 & 2 & 3 & 0 &  0 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 2 & 3 &  0 &  0 & $]^{\mathsf{T}}$\\
		S & $\mathbf{d}'_{16} =$ & $[$ & 16 & 2 & 2 & 6 & 2 & 4 & 0 &  0 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 2 & 4 &  0 &  0 & $]^{\mathsf{T}}$\\
		Fe & $\mathbf{d}'_{26} =$ & $[$ & 26 & 2 & 2 & 6 & 2 & 6 & 2 &  6 & 0 & 0 &  0 & 0 & 0 &  0 &  0 & 0 & 2 & 0 &  6 &  0 & $]^{\mathsf{T}}$\\
		I & $\mathbf{d}'_{53} =$ & $[$ & 53 & 2 & 2 & 6 & 2 & 6 & 2 & 10 & 6 & 2 & 10 & 5 & 0 &  0 &  0 & 0 & 2 & 5 & 10 &  0 & $]^{\mathsf{T}}$\\
		Au & $\mathbf{d}'_{79} =$ & $[$ & 79 & 2 & 2 & 6 & 2 & 6 & 2 & 10 & 6 & 2 & 10 & 6 & 1 & 14 & 10 & 0 & 1 & 0 & 10 & 14 & $]^{\mathsf{T}}$\\
		Rn & $\mathbf{d}'_{86} =$ & $[$ & 86 & 2 & 2 & 6 & 2 & 6 & 2 & 10 & 6 & 2 & 10 & 6 & 2 & 14 & 10 & 6 & 2 & 6 & 10 & 14 & $]^{\mathsf{T}}$\\
		\bottomrule
	\end{tabular}
	\caption{Examples of element descriptors. Here, unscaled descriptors $\mathbf{d}'_Z$ are shown. The entries encode information about the ground state electron configuration (e.g.\ 1s$^2$2s$^2$2p$^2$ for C), the total number of electrons/nuclear charge (e.g.\ $Z=7$ for N), and the number of electrons in the valence shells (e.g.\ vs$^2$vp$^4$ for O). The descriptors used in Eq.~\ref{Meq:nuclear_embedding} are given by $\mathbf{d}_Z=\mathbf{d}'_Z\oslash\mathbf{d}'_{86}$, where $\oslash$ denotes Hadamard (element-wise) division (such that all entries of $\mathbf{d}_Z$ lie between $0$~and~$1$, which is desirable for numerical reasons). In this work, it is assumed that $Z_{\rm max}=86$ covers most practical applications, but descriptors for heavier elements could be derived analogously (and the scaling procedure adapted accordingly if necessary).}
	\label{tab:species_descriptor}
\end{table*}

\begin{table*}
	\begin{tabular}{c c c c c c c c c c}
		\toprule
		loss weights (see Eq.~\ref{Meq:loss}) & \qquad\qquad\qquad & \quad & \multicolumn{3}{c}{$\alpha_E = \alpha_F = \alpha_\mu = 1$} & \quad\quad & \multicolumn{3}{c}{$\alpha_E = \alpha_\mu = 1$, $\alpha_F = 100$} \\
		& & &
		\textbf{energy} & \textbf{forces} &
		\textbf{dipole} & & \textbf{energy} & \textbf{forces} &
		\textbf{dipole}\\
		\midrule
		\textbf{known molecules/} & MAE & & 18.513 & 39.863 & 39.380 &  & 10.620 & 14.851& 121.38 \\
		\textbf{unknown conformations} & RMSE & & 29.885 & 65.810  & 58.874 & & 16.782 & 25.330 & 165.82\\
		\midrule
		\textbf{unknown molecules/} & MAE & & 20.490 & 46.567 & 41.766 & & 13.151 & 17.326 & 120.50\\
		\textbf{unknown conformations} & RMSE & & 28.740 & 74.700 & 61.062 & & 17.891 & 26.179 & 162.32 \\
		\bottomrule
	\end{tabular}
	\caption{Mean absolute errors (MAEs) and root mean square errors (RMSEs) of energies (meV), forces
		(meV~\AA$^{-1}$) and dipole moments (mD) for the QM7-X\cite{hoja2021qm7} dataset. Here, \nn{} is trained with either a low or high force weight $\alpha_F$ in the loss function (see Eq.~\ref{Meq:loss}).}
	\label{tab:qm7x_results_low_force_weight}
\end{table*}
	
	\bibliography{references}

\end{document}
