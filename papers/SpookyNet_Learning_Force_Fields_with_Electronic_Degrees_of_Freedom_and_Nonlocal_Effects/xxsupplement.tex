\begin{filecontents}{supplement.aux}
\relax
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pozdnyakov2020incompleteness}
\citation{pozdnyakov2020incompleteness}
\citation{glielmo2018efficient}
\citation{von2015fourier,kocer2020continuous}
\citation{pozdnyakov2020incompleteness}
\citation{unke2019physnet}
\citation{klicpera2020directional}
\citation{schutt2021equivariant}
\citation{batzner2021se}
\citation{schutt2018schnetpack}
\citation{faber2018alchemical,christensen2020fchl}
\citation{qmlpackage}
\citation{behler2007generalized}
\citation{christensen2020fchl}
\citation{faber2018alchemical}
\citation{schutt2018schnet}
\citation{unke2019physnet}
\citation{klicpera2020directional}
\citation{batzner2021se}
\citation{schutt2021equivariant}
\newlabel{FirstPage}{{}{S1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {title}{Supporting Information for SpookyNet{}: Learning Force Fields with \\Electronic Degrees of Freedom and Nonlocal Effects}{S1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {S1}Completeness of atomic descriptors in SpookyNet{}}{S1}{section*.2}\protected@file@percent }
\newlabel{sec:atomic_descriptors}{{S1}{S1}{}{section*.2}{}}
\newlabel{eq:angles}{{S1}{S1}{}{equation.1.1}{}}
\newlabel{eq:power_spectrum}{{S2}{S1}{}{equation.1.2}{}}
\newlabel{eq:dihedrals}{{S3}{S1}{}{equation.1.3}{}}
\citation{klicpera2020directional}
\citation{ch4}
\citation{pozdnyakov2020incompleteness}
\citation{ch4}
\citation{pozdnyakov2020incompleteness}
\citation{ch4}
\citation{pozdnyakov2020incompleteness}
\citation{pozdnyakov2020incompleteness}
\citation{ch4}
\citation{pozdnyakov2020incompleteness}
\citation{pozdnyakov2020incompleteness}
\citation{pozdnyakov2020incompleteness}
\citation{ch4}
\citation{pozdnyakov2020incompleteness}
\citation{pozdnyakov2020incompleteness}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces Pairs of distinct atomic environments where all neighboring atoms (red) have the same distance from the central atom (blue). The distributions of angles (Eq.\nobreakspace  {}\ref  {eq:angles}) and dihedrals (Eq.\nobreakspace  {}\ref  {eq:dihedrals}) are visualized and values of the angular power spectrum invariants (Eq.\nobreakspace  {}\ref  {eq:power_spectrum}) for different angular momenta $l=1,\dots  4$ (p, d, f, g) are given for each structure (the s invariant simply counts the number of neighbors and is therefore omitted). Since all distances to neighboring atoms are identical, descriptors need to be able to at least resolve angular information to distinguish the structures (\textbf  {a}). However, for some structures, the power spectrum invariants may be degenerate for small values of $l$ (\textbf  {b} and \textbf  {d}). Some structures even have identical angular distributions, in which case the power spectrum invariants are equal for all $l=0,\dots  ,\infty $ and information about dihedrals is necessary to distinguish the environments (\textbf  {c}). Note that some environments cannot even be distinguished when information about dihedrals is included (\textbf  {d}).\cite  {pozdnyakov2020incompleteness}\relax }}{S2}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:descriptors}{{S1}{S2}{Pairs of distinct atomic environments where all neighboring atoms (red) have the same distance from the central atom (blue). The distributions of angles (Eq.~\ref {eq:angles}) and dihedrals (Eq.~\ref {eq:dihedrals}) are visualized and values of the angular power spectrum invariants (Eq.~\ref {eq:power_spectrum}) for different angular momenta $l=1,\dots 4$ (p, d, f, g) are given for each structure (the s invariant simply counts the number of neighbors and is therefore omitted). Since all distances to neighboring atoms are identical, descriptors need to be able to at least resolve angular information to distinguish the structures (\textbf {a}). However, for some structures, the power spectrum invariants may be degenerate for small values of $l$ (\textbf {b} and \textbf {d}). Some structures even have identical angular distributions, in which case the power spectrum invariants are equal for all $l=0,\dots ,\infty $ and information about dihedrals is necessary to distinguish the environments (\textbf {c}). Note that some environments cannot even be distinguished when information about dihedrals is included (\textbf {d}).\cite {pozdnyakov2020incompleteness}\relax }{figure.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {S1}{\ignorespaces Ability of models to differentiate the atomic environments shown in Fig.\nobreakspace  {}\ref  {fig:descriptors} ({\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 51}:\nobreakspace  {}distinguishable, {\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 55}:\nobreakspace  {}indistinguishable) and the scaling of their computational cost with respect to the number of neighbors $n$. Message-passing neural networks with learned descriptors can distinguish all environments when there are $T\geq 2$ message-passing steps. However, when only a single step is used ($T=1$), the environments marked with ({\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 51}) become indistinguishable.\relax }}{S2}{table.caption.4}\protected@file@percent }
\newlabel{tab:descriptors}{{S1}{S2}{Ability of models to differentiate the atomic environments shown in Fig.~\ref {fig:descriptors} (\cmark :~distinguishable, \xmark :~indistinguishable) and the scaling of their computational cost with respect to the number of neighbors $n$. Message-passing neural networks with learned descriptors can distinguish all environments when there are $T\geq 2$ message-passing steps. However, when only a single step is used ($T=1$), the environments marked with (\cmark ) become indistinguishable.\relax }{table.caption.4}{}}
\citation{hoja2021qm7}
\citation{hoja2021qm7}
\bibdata{supplementNotes,references}
\bibcite{pozdnyakov2020incompleteness}{{1}{2020{}}{{Pozdnyakov\ \emph  {et~al.}}}{{Pozdnyakov, Willatt, Bart{\'o}k, Ortner, Cs{\'a}nyi,\ and\ Ceriotti}}}
\bibcite{glielmo2018efficient}{{2}{2018}{{Glielmo\ \emph  {et~al.}}}{{Glielmo, Zeni,\ and\ De~Vita}}}
\bibcite{von2015fourier}{{3}{2015}{{Von~Lilienfeld\ \emph  {et~al.}}}{{Von~Lilienfeld, Ramakrishnan, Rupp,\ and\ Knoll}}}
\bibcite{kocer2020continuous}{{4}{2020}{{Kocer\ \emph  {et~al.}}}{{Kocer, Mason,\ and\ Erturk}}}
\bibcite{unke2019physnet}{{5}{2019}{{Unke\ and\ Meuwly}}{{}}}
\bibcite{klicpera2020directional}{{6}{2020}{{Klicpera\ \emph  {et~al.}}}{{Klicpera, Gro{\ss },\ and\ G{\"u}nnemann}}}
\bibcite{schutt2021equivariant}{{7}{2021}{{Sch{\"u}tt\ \emph  {et~al.}}}{{Sch{\"u}tt, Unke,\ and\ Gastegger}}}
\bibcite{batzner2021se}{{8}{2021}{{Batzner\ \emph  {et~al.}}}{{Batzner, Smidt, Sun, Mailoa, Kornbluth, Molinari,\ and\ Kozinsky}}}
\bibcite{schutt2018schnetpack}{{9}{2018{}}{{Sch{\"u}tt\ \emph  {et~al.}}}{{Sch{\"u}tt, Kessel, Gastegger, Nicoli, Tkatchenko,\ and\ M{\"u}ller}}}
\bibcite{faber2018alchemical}{{10}{2018}{{Faber\ \emph  {et~al.}}}{{Faber, Christensen, Huang,\ and\ Von~Lilienfeld}}}
\bibcite{christensen2020fchl}{{11}{2020}{{Christensen\ \emph  {et~al.}}}{{Christensen, Bratholm, Faber,\ and\ Anatole~von Lilienfeld}}}
\bibcite{qmlpackage}{{12}{2017}{{Christensen\ \emph  {et~al.}}}{{Christensen, Faber, Huang, Bratholm, Tkatchenko, M{\"u}ller,\ and\ von Lilienfeld}}}
\bibcite{behler2007generalized}{{13}{2007}{{Behler\ and\ Parrinello}}{{}}}
\bibcite{schutt2018schnet}{{14}{2018{}}{{Sch{\"u}tt\ \emph  {et~al.}}}{{Sch{\"u}tt, Sauceda, Kindermans, Tkatchenko,\ and\ M{\"u}ller}}}
\bibcite{ch4}{{15}{2020{}}{{Pozdnyakov\ \emph  {et~al.}}}{{Pozdnyakov, Willatt,\ and\ Ceriotti}}}
\@writefile{lot}{\contentsline {table}{\numberline {S2}{\ignorespaces Mean absolute errors (MAEs) and root mean square errors (RMSEs) of energies and forces for the random CH$_4$ dataset\cite  {ch4} suggested in Ref.\nobreakspace  {}\citenum  {pozdnyakov2020incompleteness}. Results are averaged over 16 ($n_{\rm  train} = 10$), 8 ($n_{\rm  train} = 100$), or 4 ($n_{\rm  train} \geq 1000$) random splits and the standard deviation between runs is given in brackets.\relax }}{S3}{table.caption.6}\protected@file@percent }
\newlabel{tab:ceriotti_ch4_results}{{S2}{S3}{Mean absolute errors (MAEs) and root mean square errors (RMSEs) of energies and forces for the random CH$_4$ dataset\cite {ch4} suggested in Ref.~\citenum {pozdnyakov2020incompleteness}. Results are averaged over 16 ($n_{\rm train} = 10$), 8 ($n_{\rm train} = 100$), or 4 ($n_{\rm train} \geq 1000$) random splits and the standard deviation between runs is given in brackets.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces Energy learning curves for the random CH$_4$ dataset\cite  {ch4} suggested in Ref.\nobreakspace  {}\citenum  {pozdnyakov2020incompleteness}. SpookyNet{} (black) is compared to feedforward neural networks trained on 2-body and/or 3-body features (red, blue, magenta), see Ref.\nobreakspace  {}\citenum  {pozdnyakov2020incompleteness} for details.\relax }}{S3}{figure.caption.7}\protected@file@percent }
\newlabel{fig:ceriotti_ch4_learning_curves}{{S2}{S3}{Energy learning curves for the random CH$_4$ dataset\cite {ch4} suggested in Ref.~\citenum {pozdnyakov2020incompleteness}. \nn {} (black) is compared to feedforward neural networks trained on 2-body and/or 3-body features (red, blue, magenta), see Ref.~\citenum {pozdnyakov2020incompleteness} for details.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{S3}{section*.12}\protected@file@percent }
\bibcite{hoja2021qm7}{{16}{2021}{{Hoja\ \emph  {et~al.}}}{{Hoja, Sandonas, Ernst, Vazquez-Mayagoitia, DiStasio~Jr,\ and\ Tkatchenko}}}
\bibstyle{apsrev4-2}
\citation{REVTEX42Control}
\citation{apsrev42Control}
\@writefile{lof}{\contentsline {figure}{\numberline {S3}{\ignorespaces Local chemical potential for a random carbene chosen from the QMSpin database (the optimized geometries for the singlet/triplet states are shown). The chemical potential for a model without spin embeddings lacks features, whereas a model with spin embeddings learns a rich representation with significant differences between singlet and triplet states.\relax }}{S4}{figure.caption.8}\protected@file@percent }
\newlabel{fig:different_spin_gummybears}{{S3}{S4}{Local chemical potential for a random carbene chosen from the QMSpin database (the optimized geometries for the singlet/triplet states are shown). The chemical potential for a model without spin embeddings lacks features, whereas a model with spin embeddings learns a rich representation with significant differences between singlet and triplet states.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S4}{\ignorespaces Generalized SiLU activation (Eq.\nobreakspace  {}\ref  {Meq:activation}). For $\alpha =1,\beta =\infty $, $\mathrm  {silu}(x)$ is equivalent to $\mathrm  {max}(x,0)$ (also known as $\mathrm  {ReLU}$ activation), whereas for $\alpha =2,\beta =0$, the identity function is obtained.\relax }}{S4}{figure.caption.9}\protected@file@percent }
\newlabel{fig:activation}{{S4}{S4}{Generalized SiLU activation (Eq.~\ref {Meq:activation}). For $\alpha =1,\beta =\infty $, $\mathrm {silu}(x)$ is equivalent to $\mathrm {max}(x,0)$ (also known as $\mathrm {ReLU}$ activation), whereas for $\alpha =2,\beta =0$, the identity function is obtained.\relax }{figure.caption.9}{}}
\newlabel{LastBibItem}{{16}{S4}{}{section*.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {S3}{\ignorespaces Examples of element descriptors. Here, unscaled descriptors $\mathbf  {d}'_Z$ are shown. The entries encode information about the ground state electron configuration (e.g.\ 1s$^2$2s$^2$2p$^2$ for C), the total number of electrons/nuclear charge (e.g.\ $Z=7$ for N), and the number of electrons in the valence shells (e.g.\ vs$^2$vp$^4$ for O). The descriptors used in Eq.\nobreakspace  {}\ref  {Meq:nuclear_embedding} are given by $\mathbf  {d}_Z=\mathbf  {d}'_Z\oslash \mathbf  {d}'_{86}$, where $\oslash $ denotes Hadamard (element-wise) division (such that all entries of $\mathbf  {d}_Z$ lie between $0$\nobreakspace  {}and\nobreakspace  {}$1$, which is desirable for numerical reasons). In this work, it is assumed that $Z_{\rm  max}=86$ covers most practical applications, but descriptors for heavier elements could be derived analogously (and the scaling procedure adapted accordingly if necessary).\relax }}{S5}{table.caption.10}\protected@file@percent }
\newlabel{tab:species_descriptor}{{S3}{S5}{Examples of element descriptors. Here, unscaled descriptors $\mathbf {d}'_Z$ are shown. The entries encode information about the ground state electron configuration (e.g.\ 1s$^2$2s$^2$2p$^2$ for C), the total number of electrons/nuclear charge (e.g.\ $Z=7$ for N), and the number of electrons in the valence shells (e.g.\ vs$^2$vp$^4$ for O). The descriptors used in Eq.~\ref {Meq:nuclear_embedding} are given by $\mathbf {d}_Z=\mathbf {d}'_Z\oslash \mathbf {d}'_{86}$, where $\oslash $ denotes Hadamard (element-wise) division (such that all entries of $\mathbf {d}_Z$ lie between $0$~and~$1$, which is desirable for numerical reasons). In this work, it is assumed that $Z_{\rm max}=86$ covers most practical applications, but descriptors for heavier elements could be derived analogously (and the scaling procedure adapted accordingly if necessary).\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {S4}{\ignorespaces Mean absolute errors (MAEs) and root mean square errors (RMSEs) of energies (meV), forces (meV\nobreakspace  {}\r A$^{-1}$) and dipole moments (mD) for the QM7-X\cite  {hoja2021qm7} dataset. Here, SpookyNet{} is trained with either a low or high force weight $\alpha _F$ in the loss function (see Eq.\nobreakspace  {}\ref  {Meq:loss}).\relax }}{S5}{table.caption.11}\protected@file@percent }
\newlabel{tab:qm7x_results_low_force_weight}{{S4}{S5}{Mean absolute errors (MAEs) and root mean square errors (RMSEs) of energies (meV), forces (meV~\AA $^{-1}$) and dipole moments (mD) for the QM7-X\cite {hoja2021qm7} dataset. Here, \nn {} is trained with either a low or high force weight $\alpha _F$ in the loss function (see Eq.~\ref {Meq:loss}).\relax }{table.caption.11}{}}
\newlabel{LastPage}{{}{S5}{}{}{}}
\end{filecontents}