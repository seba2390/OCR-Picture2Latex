\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_data_2021

\PassOptionsToPackage{square,comma,numbers,sort&compress}{natbib}

% ready for submission
\usepackage[preprint]{neurips_data_2021}

% to compile a preprint version, add the [preprint] option:
%     \usepackage[preprint]{neurips_data_2021}
% This will indicate that the work is currently under review.

% to compile a camera-ready version, add the [final] option:
%     \usepackage[final]{neurips_data_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_data_2021}

% Submissions to the datasets and benchmarks are non-anonymous. If you do want to compile an anonymous version for other purposes, you can add the [anonymous] option:
%     \usepackage[anonymous]{neurips_data_2021}
% This will hide all author names.

\usepackage{wrapfig}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm2e}
\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{float}
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}
\usepackage{subcaption}

\renewcommand{\topfraction}{1}
\renewcommand{\dbltopfraction}{1}
\renewcommand{\floatpagefraction}{1}
\renewcommand{\textfraction}{0}
\newcommand{\hideseg}[1]{} %hide

\usepackage{xspace}
%COMMENTS FOR ISMINI
\newcommand{\il}[1]{\textsf{\textbf{\color{magenta}{\small{[IL: #1]}}}}}
%COMMENTS FOR MEHDI
\newcommand{\mm}[1]{\textsf{\textbf{\color{blue}{\small{[MM: #1]}}}}}
\newcommand*{\myalign}[2]{\multicolumn{1}{#1}{#2}}


\title{Chest ImaGenome Dataset for Clinical Reasoning}

% \author[1,*]{Joy T. Wu}
% \author[2]{Nkechinyere N. Agu}
% \author[1,3]{Ismini Lourentzou}
% \author[4,$\dag$]{Arjun Sharma} 
% \author[5,$\dag$]{Joseph Paguio}
% \author[5,$\dag$]{Jasper Seth Yao}
% \author[6,$\dag$]{Edward Christopher Dee}
% \author[4,$\dag$]{William Mitchell}
% \author[1]{Satyananda Kashyap}
% \author[1]{Andrea Giovannini}
% \author[4]{Leo A. Celi}
% % & any others who helped
% \author[1,*]{Mehdi Moradi} 
% % order tbd -- happy to discuss

% \affil[1]{IBM Almaden Research Center, San Jose, CA 95120, USA}
% \affil[2]{Rensselaer Polytechnic Institute, Troy, NY 12180, USA}
% \affil[3]{Virginia Polytechnic Institute and State University, Blacksburg, VA 24061, USA}
% \affil[4]{MIT Critical Data, Cambridge, MA 02139, USA}
%\affil[5]{Albert Einstein Healthcare Network-Philadelphia Campus, PA 19141, USA}
% \affil[6]{MIT Harvard Medical School, Boston, MA 02115, USA}

% \affil[*]{Corresponding author(s): Joy T. Wu (research@joytywu.net), Mehdi Moradi (mmoradi@us.ibm.com)}

% \affil[$\dag$]{These authors contributed equally to this work}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author[1,*]{Joy T. Wu}
% \author[2]{Nkechinyere N. Agu}
% \author[1,3]{Ismini Lourentzou}
% \author[4,$\dag$]{Arjun Sharma} 
% \author[5,$\dag$]{Joseph Alexander Paguio}
% \author[5,$\dag$]{Jasper Seth Yao}
% \author[6,$\dag$]{Edward Christopher Dee}
% \author[4,$\dag$]{William Mitchell}
% \author[1]{Satyananda Kashyap}
% \author[1]{Andrea Giovannini}
% \author[4]{Leo A. Celi}
% % & any others who helped
% \author[1,*]{Mehdi Moradi} 

%\hide{\thanks{Use footnote for providing further information
%    about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}}

\author{Joy T. Wu\textsuperscript{1}, Nkechinyere N. Agu\textsuperscript{2}, Ismini Lourentzou\textsuperscript{3}, Arjun Sharma\textsuperscript{4}, Joseph A. Paguio\textsuperscript{5}, \and \textbf{Jasper S. Yao\textsuperscript{5}, Edward C. Dee\textsuperscript{6}, William Mitchell\textsuperscript{4}, Satyananda Kashyap\textsuperscript{1},} \and \textbf{Andrea Giovannini\textsuperscript{1}, Leo A. Celi\textsuperscript{4}, Mehdi Moradi\textsuperscript{1}} \\
\textsuperscript{1}{IBM Almaden Research Center, San Jose, CA 95120, USA}\\
\textsuperscript{2}{Rensselaer Polytechnic Institute, Troy, NY 12180, USA}\\
\textsuperscript{3}{Virginia Polytechnic Institute and State University, Blacksburg, VA 24061, USA}\\
\textsuperscript{4}{MIT Critical Data, Cambridge, MA 02139, USA}\\
\textsuperscript{5}{Albert Einstein Healthcare Network-Philadelphia Campus, PA 19141, USA}\\
\textsuperscript{6}{Harvard Medical School, Boston, MA 02115, USA} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle

\begin{abstract}
  %In recent years, with the release of multiple large datasets, the automatic interpretation of chest X-ray (CXR) images with deep learning models have become feasible for specific abnormalities or for generating preliminary reports. However,   reports of performance reaching similar levels to that of radiologists, 
Despite the progress in automatic detection of radiologic findings from chest X-ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global "weak" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe $242,072$ images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$ combinations of relation annotations between $29$ CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over $670,000$ localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from $500$ unique patients.

%In our work, a joint rule-based natural language processing (NLP) and CXR atlas-based bounding box detection pipeline are used to automatically label 242072 frontal MIMIC CXRs locally. 
\end{abstract}

\input{sections/intro}
\input{sections/related}
\input{sections/method}
\input{sections/data_silver}
\input{sections/data_gold}
\input{sections/evaluation}
\input{sections/usage}
% \input{sections/code}
% \input{sections/contributions}
% \input{sections/conflicts}
% \input{sections/figures_tables}

%\newpage
\input{sections/acknowledgements}
\bibliographystyle{plainnat}
\bibliography{bibliography.bib}
\newpage
%\input{sections/checklist}

\newpage
\input{sections/supplement_A}
%\input{sections/tables_supplement}


\end{document}


% from Ismini (External) to Everyone:    2:37  PM
% https://www.tablesgenerator.com/
% from Ismini (External) to Everyone:    2:38  PM
% \href{link}{text}
% from Satyananda Kashyap (IBM) to Everyone:    2:38  PM
% \usepackage{hyperref}