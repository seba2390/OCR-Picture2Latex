\section*{Methods}

The Chest ImaGenome dataset was derived from the MIMIC-CXR dataset \cite{johnson2019mimic}, which has been de-identified. This derived dataset retains the added annotations and the source image tags but not the CXR images, which users are expected to separately download from the \href{https://PhysioNet.org/content/mimic-cxr/2.0.0/}{\textbf{MIMIC-CXR database}}. The institutional review boards of the Massachusetts Institute of Technology (No. 0403000206) and Beth Israel Deaconess Medical Center (BIDMC)(2001-P-001699/14) both approved the use of the MIMIC database for research. All authors working with the data have individually completed required HIPPA training and been granted data access approval from PhysioNet.

\vspace{-5pt}
\subsection*{Silver Dataset Construction}
\vspace{-2pt}
%\textbf{Silver dataset construction}: 
The Chest ImaGenome dataset construction is inspired by the Visual Genome dataset \cite{krishna2017visual}. Whereas Visual Genome utilized web-based and crowd-sourced methods to manually collect annotations, the Chest ImaGenome harnessed NLP, a CXR ontology, and image segmentation techniques to automatically structure and add value to existing CXR images and their free-text reports, which were collected from radiologists in their routine workflow. We used atlas-based bounding box extraction techniques to structure the anatomies on $242,072$ frontal CXR images, anteroposterior (AP) or posteroanterior (PA) view, and used a rule-based text-analysis pipeline to relate the anatomies to various CXR attributes (finding, diseases, technical assessment, devices, etc) extracted from $217,013$ reports. Altogether, we automatically annotated $242,072$ scene graphs that locally and graphically describe the frontal images associated with these reports (one report can have one or more frontal images). Our goal is to not only locally label attributes relevant for key anatomical locations on the CXR images, but also to extract documented radiology knowledge from a large corpus of CXR reports to aid future semantics-driven and multi-modal clinical reasoning works. 

\begin{table}[!ht]
  \centering
    \caption{Parallels between the Chest ImaGenome and Visual Genome datasets.}
  \label{tab:cg_vg_parallels}
  \footnotesize{
    \begin{tabular}{|p{4em}|p{21em}|p{16em}|}
    \toprule
    \textbf{Element} & \textbf{Chest ImaGenome} & \textbf{Visual Genome} \\
    \midrule
    \midrule
    Scene & One frontal CXR image in the current dataset. & One (non-medical) everyday life image. \\
    \midrule
    Questions & For now, there is only one question per CXR, which is taken from the patient history (i.e., reason for exam) section from each CXR report. & One or more questions that the crowd source annotators decided to ask about the image where the information from each question and the image should allow another annotator to answer it. \\
    \midrule
    Answers & N/A currently. However, report sentences are biased towards answering the question asked in the reason for exam sentence;hence, the knowledge graph we extract from each report should contain the answer(s). & This was collected as answer(s) to the corresponding question(s) asked of the image. \\
    \midrule
    Sentences (Region descriptions) & Sentences from the finding and impression sections of a CXR report describing the exam as collected from radiologists in their routine radiology workflow. & True natural language descriptive sentences about the image collected from crowd-sourced everyday annotators. \\
    \midrule
    % Region & Bounding box coordinates that encompass all the anatomical structures described in a report sentence (can be easily derived from the scene graph json). & Bounding box coordinates that include all the objects mentioned in a sentence that describes the image, e.g., The boy (object 1) is beside the bus (object 2). \\
    % \midrule
    Objects (nodes) & Anatomical structures or locations that have bounding box coordinates on the associated CXR image, and is indexed to the UMLS ontology \cite{bodenreider2004unified}. & The people and physical objects with bounding box coordinates on the image and indexed to WordNet ontology \cite{miller1995wordnet}. \\
    \midrule
    Attributes (nodes) & Descriptions that are true for different anatomical structures visualized on the CXR image (e.g., There is a right upper lung [object] opacity [attribute]), indexed to the UMLS ontology \cite{bodenreider2004unified}. No Bbox coordinates. & Various descriptive properties of the objects in the image (e.g., The shirt [object] is blue [attribute]), indexed to WordNet ontology \cite{miller1995wordnet}. No Bbox coordinates. \\
    \midrule
    Relations: object and attribute & The relationship(s) between an anatomical object and its attribute(s) from the same CXR image (e.g., There is a [relation] right upper lung [object] opacity [attribute]). & The relationship(s)  between an object and its attribute(s) from the same image ( e.g., The shirt [object] is [relation] blue [attribute]). \\
    \midrule
    Relations: object and object & The comparison relationship (index to UMLS \cite{bodenreider2004unified}) between the same anatomical object from two sequential CXR images for the same patient (e.g., There is a new [relation] right lower lobe [current and previous anatomical objects] atelectasis [attribute]). & The relationship (indexed to WordNet \cite{miller1995wordnet}) between objects in the same image (e.g., The boy [object 1] is beside [relation] the bus [object 2]). \\
    \midrule
    Relations: parent and child & To make the graph for each image logically consistent and correct as learnable and consumable radiology knowledge, affirmed parent-child relations between nodes are embedded in the scene graphs -- i.e., if a child attribute is related to an object, then its parent would be too (e.g., if right lung has consolidation [child], then it also has lung opacity [parent]). & N/A due to different graph construction strategy and goals. The annotators were asked to describe any (but not all) relations they observe in an image. \\
    \midrule
    Scene graph & Constructed from the objects, the attributes and the relationships between them for the image. & Same but the nodes and edges overall would be more varied than Chest ImaGenome for now. \\
    \midrule
    Sequence* & A super-graph for a set of chronologically ordered series of exams for the same patient. & N/A, but would be a graph for a video in the non-medical context. \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-15pt}
\end{table}

Table \ref{tab:cg_vg_parallels} describes the parallels between the Chest ImaGenome and Visual Genome datasets. The key differences are in the construction methodology, the currently much smaller range of possible objects and attributes (due to having only the CXR imaging modality), and the introduction of comparison relations between sequential images in the Chest ImaGenome dataset. We define the nodes and edges in the graph (Supplementary Table \ref{tab:define_nodes_edges}) based on clinical relevance and resources in the context for medical imaging exams like CXRs. In addition, two \textbf{key assumptions} are made in the construction of the Chest ImaGenome dataset: 

 1) CXR imaging observations can be normalized to relationships between the visualized anatomical locations (object nodes) and the abnormalities, devices or other CXR descriptions (attribute nodes) that the locations contain. Thus, the variety of detected objects is confined by the granularity of anatomical location detection on images and from reports. 

 2) The exam timestamps in the original MIMIC-CXR dataset can be used to chronologically order the CXR exams from the same patient within the original MIMIC CXR dataset's collection period and there are minimal missing exams for each patient. This is based on discussions with the MIMIC team and MIMIC-CXR's documented data collection strategy. The original data curators included all CXR exams in the radiology imaging archives for patients who were at any time point admitted to the BIDMC's Emergency Department within a continuous 2-year-period. Therefore, we related any comparison descriptions (normalized to `improved', `worsened' and `no change') of attribute(s) in different anatomical location(s) to the same anatomical location(s) on the exam image(s) immediately before the current exam. Clinically, the extracted comparison relations are intended to allow longitudinal modeling of disease progression for different CXR anatomies.

% Each attribute node can belong to one of six different radiology semantic categories and can be positively (affirmed) or negatively (specifically negated in reports) associated with different CXR objects. For affirmed relations, additional cues are provided in the scene graph JSONs when the relation is uncertain.

% \include{sections/table1_parallels}


% \section*{Figures \& Tables}

% Table generated by Excel2LaTeX from sheet 'CG_VG_parallels'

% \hideseg{
% \begin{table}[t!]
%   \centering
%     \caption{Parallels between the Chest ImaGenome and Visual Genome datasets.}
%   \label{tab:cg_vg_parallels}
%   \footnotesize{
%     \begin{tabular}{|p{4em}|p{21em}|p{16em}|}
%     \toprule
%     \textbf{Element} & \textbf{Chest ImaGenome} & \textbf{Visual Genome} \\
%     \midrule
%     \midrule
%     Scene & One frontal CXR image in the current dataset. & One (non-medical) everyday life image. \\
%     \midrule
%     Questions & For now, there is only one question per CXR, which is taken from the patient history (i.e., reason for exam) section from each CXR report. & One or more questions that the crowd source annotators decided to ask about the image where the information from each question and the image should allow another annotator to answer it. \\
%     \midrule
%     Answers & N/A currently. However, report sentences are biased towards answering the question asked in the reason for exam sentence;hence, the knowledge graph we extract from each report should contain the answer(s). & This was collected as answer(s) to the corresponding question(s) asked of the image. \\
%     \midrule
%     Sentences (Region descriptions) & Sentences from the finding and impression sections of a CXR report describing the exam as collected from radiologists in their routine radiology workflow. & True natural language descriptive sentences about the image collected from crowd-sourced everyday annotators. \\
%     \midrule
%     % Region & Bounding box coordinates that encompass all the anatomical structures described in a report sentence (can be easily derived from the scene graph json). & Bounding box coordinates that include all the objects mentioned in a sentence that describes the image, e.g., The boy (object 1) is beside the bus (object 2). \\
%     % \midrule
%     Objects (nodes) & Anatomical structures or locations that have bounding box coordinates on the associated CXR image, and is indexed to the UMLS ontology \cite{bodenreider2004unified}. & The people and physical objects with bounding box coordinates on the image and indexed to WordNet ontology \cite{miller1995wordnet}. \\
%     \midrule
%     Attributes (nodes) & Descriptions that are true for different anatomical structures visualized on the CXR image (e.g., There is a right upper lung [object] opacity [attribute]), indexed to the UMLS ontology \cite{bodenreider2004unified}. No Bbox coordinates. & Various descriptive properties of the objects in the image (e.g., The shirt [object] is blue [attribute]), indexed to WordNet ontology \cite{miller1995wordnet}. No Bbox coordinates. \\
%     \midrule
%     Relations: object and attribute & The relationship(s) between an anatomical object and its attribute(s) from the same CXR image (e.g., There is a [relation] right upper lung [object] opacity [attribute]). & The relationship(s)  between an object and its attribute(s) from the same image ( e.g., The shirt [object] is [relation] blue [attribute]). \\
%     \midrule
%     Relations: object and object & The comparison relationship (index to UMLS \cite{bodenreider2004unified}) between the same anatomical object from two sequential CXR images for the same patient (e.g., There is a new [relation] right lower lobe [current and previous anatomical objects] atelectasis [attribute]). & The relationship (indexed to WordNet \cite{miller1995wordnet}) between objects in the same image (e.g., The boy [object 1] is beside [relation] the bus [object 2]). \\
%     \midrule
%     Relations: parent and child & To make the graph for each image logically consistent and correct as learnable and consumable radiology knowledge, affirmed parent-child relations between nodes are embedded in the scene graphs -- i.e., if a child attribute is related to an object, then its parent would be too (e.g., if right lung has consolidation [child], then it also has lung opacity [parent]). & N/A due to different graph construction strategy and goals. The annotators were asked to describe any (but not all) relations they observe in an image. \\
%     \midrule
%     Scene graph & Constructed from the objects, the attributes and the relationships between them for the image. & Same but the nodes and edges overall would be more varied than Chest ImaGenome for now. \\
%     \midrule
%     Sequence* & A super-graph for a set of chronologically ordered series of exams for the same patient. & N/A, but would be a graph for a video in the non-medical context. \\
%     \bottomrule
%     \end{tabular}%
%     }
% \end{table}
% }

The construction of the Chest ImaGenome dataset builds on the works of \cite{wu2020ai,wu2020automatic}. In summary, the text pipeline \cite{wu2020ai} first sections the report and retains only the finding and impression sentences, and then utilizes a CXR concept dictionary (lexicons) to spot and detect the context (negated or affirmed) of 271 different CXR related named-entities from each retained sentence. The lexicons were curated in advance by two radiologists in consensus using a concept expansion and vocabulary grouping engine \cite{coden2012spot}. A set of sentence-level filtering rules are applied to disambiguate some of the target concepts (e.g., `collapse' mention in CXR report can be about lung `collapse' or related to spinal fracture as in vertebral body `collapse'). Then the named-entities for CXR labels (attributes) are associated with the name-entities for anatomical location(s) described in the same sentence with a SpaCy natural language parser \cite{spacy}. 

Using a CXR ontology constructed by radiologists, a scene graph assembly pipeline corrected obvious attribute-to-anatomy assignment errors (e.g., lung opacity wrongly assigned to mediastinum). Finally, the attributes for each of the target anatomical regions from repeated sentences are grouped to the exam level. The result is that, from each CXR report, we extract a radiology knowledge graph where CXR anatomical locations are related to different documented CXR attribute(s). The "reason for exam" sentence(s) from each report, which contain free text information about prior patient history, are separately kept in the final scene graph JSONs. Patient history information is critical for clinical reasoning but is a piece of information that is not technically part of the "scene" for each CXR. 

For detecting the anatomical "objects" on the CXR images that are associated with the extracted report knowledge graph, a separate anatomy atlas-based bounding box pipeline extracts the coordinates of those anatomies from each frontal image. This pipeline is an extension of prior work that covers additional anatomical locations in this dataset \cite{wu2020automatic}. In addition, we manually validated or corrected the bounding boxes for $1,071$ CXR images (with and without disease, and excluded gold standard subjects) to train a Faster-RCNN CXR bounding box detection model, which we used to correct failed bounding boxes (too small or missing) from the initial bounding box extraction pipeline (~7\%). Finally, for quality assurance, we manually annotated $303$ images that had missing bounding boxes for key CXR anatomies (lungs and mediastinum).

Extracting comparison relations between sequential exams at the anatomical level is another goal for the Chest ImaGenome dataset. After checking with the MIMIC team and reviewing their dataset documentation, we assume that the timestamps in the original MIMIC-CXR dataset can be used to chronologically order the exams for each patient. We then correlated all report descriptions of changes (grouped as improved, worsened, or no change) between sequential exams with the anatomical locations described at the sentence level. To extract these comparison descriptions, we used a concept expansion engine \cite{coden2012spot} to curate and group relevant comparison vocabularies used in CXR reports. These comparison relations extracted between anatomical locations from sequential CXRs are only added to the final scene graphs for every patient's second or later CXR exam(s), i.e., comparison relations described in the first study of each patient in the MIMIC-CXR dataset are not added to the Chest ImaGenome dataset.

% graph figure
\begin{figure}[tb!]
\centering
\includegraphics[width=\textwidth]{figures/chest_imaGenome_graph_sample_fig1_2bboxes.pdf}
\vspace{-0.4cm}
\caption{A radiology knowledge graph extracted for one CXR report (grey), with patient history from indication for exam (orange), anatomical locations (blue) and their associated attributes, including anatomical findings (pink), diseases (yellow), technical assessment (purple) and devices (green) nodes. The blue anatomy nodes (a.k.a. objects) also have corresponding bounding box coordinates on the CXR image, which are shown for two examples. }
\label{fig1.cxr_graph}
\vspace{-15pt}
\end{figure}


Finally, we have mapped all object and attribute nodes and comparison relations in the dataset to a Concept Unique Identifier (CUI) in the Unified Medical Language System (UMLS) \cite{bodenreider2004unified}. The UMLS ontology has incorporated the concepts from the Radlex ontology [31], which targets the radiology domain. Choosing UMLS to index the Chest ImaGenome dataset widens its future applications in clinical reasoning tasks, which would invariably require medical concepts and relations outside the radiology domain. An example of a CXR scene graph is shown in Figure \ref{fig1.cxr_graph}.

\vspace{-5pt}
\subsection*{Gold Standard Dataset Collection}
\vspace{-2pt}
%\textbf{Gold standard dataset collection}: 
In collaboration with clinicians (radiology and internal medicine M.D.'s) from multiple academic institutions, we curated a dual validated gold standard dataset to 1) evaluate the quality of the silver Chest ImaGenome dataset we automatically generated, and 2) to serve as a benchmark resource for future research using the dataset. Due to resource constraints, we created the gold standard dataset using a validation plus correction strategy. We randomly sampled 500 unique patients from the Chest ImaGenome dataset that had two or more sequential CXR exams. Overall, we targeted three aspects of the scene graph dataset generation process to evaluate separately: A) the object-to-attribute relations (i.e., CXR knowledge graph) extracted from individual reports, B) the object-to-object comparison relations extracted between sequential CXR reports, and C) the anatomical location detection (i.e., the bounding box extraction pipeline) for the CXR images. For details about the gold standard dataset annotation process, see Supplementary (Section \ref{gold_annot_supp}).

% \textbf{\textit{A) Evaluating CXR knowledge graph extraction from reports}}:
% The report knowledge graph for the \textit{first} CXR of the 500 patients was manually reviewed and corrected as necessary for relation extraction between the anatomical locations (objects) and the CXR attributes. From piloting trials, we found that manually annotating multiple targets at a document level lead to a slow and complex task with poor recall. However, sometimes information from prior sentences is necessary to annotate both the anatomical locations and the attributes correctly. Therefore, we set up the annotation task at the sentence level. Sentences from each report are ordered as per the original report, and the phrase boundary for each attribute was marked out for the annotators, where the phrases used for detecting each attribute were curated by consensus between two radiologists from previous work \cite{wu2020ai}. 

% Since we are targeting a large set of possible anatomical locations (object) to attribute combinations, the annotation was streamlined into the four steps below to minimize the cognitive overload for each step. Step 1 and 2 are dually annotated by two clinicians (one fully trained radiologist and one M.D.), with disagreements resolved by consensus review. Step 3 and 4 are single annotated. A random subset of annotations for 500 sentences from step 4 are sampled and dual annotated to estimate inter-annotator agreement. Cleaned results from step 4 constitute the final gold-standard CXR knowledge graph ground truth for the 500 reports. The 4 annotation steps are:
% 1) For each sentence and NLP extracted attribute combination, indicate whether the NLP context (affirmed or negated) for the attribute was correct. If not, correct it.
% 2) For each sentence, indicate whether the NLP extracted anatomical location(s) were described or implied by the reporting radiologist. If not, remove the location. If missing, add the location. If unsure, the annotator can look in previous sentences from the same report.
% 3) For recall, manually annotate missed objects and/or attributes for sentences with no NLP extractions (a much smaller subset).
% 4) Automatically derive all object-attribute relation combinations from results in 1-3 and then filter out the obviously wrong object-to-attribute relations for each sentence using the CXR ontology. For the remaining object-to-attribute relations for each sentence, the task was to indicate whether the logical statement of \textit{``object X contains (or does not contain) attribute Y''} is true or false. Probable relation is still defined to be true for this annotation. Annotating for uncertain relations is beyond the scope of this project. For future data expansion, we have kept the NLP cues for the certainty for each object-attribute relation in the scene graph json. 

% \textbf{\textit{B) Evaluating comparison relation extraction}}: The \textit{second} CXR exam report for the 500 patients were reviewed for comparison relation extraction. In the Chest ImaGenome dataset, comparison relation is only extracted for an CXR exam if there was a previous exam in the dataset for the patient. Therefore, we reviewed the second CXR report for each of the 500 unique patients sampled for the gold standard dataset. %The annotation was also conducted in Excel with one sentence per row for annotation from the second exam.
% The annotation was also conducted on a sentence level.
% However, the annotator is also shown the whole previous CXR report for context. Similarly, We split the annotation task up to several steps, where step 1 and 2 are dually annotated and disagreement resolved via consensus. Step 3 and 4 were single annotated. A subset of 500 sentences from the final annotations was reviewed by a second annotator for assessing inter-annotator agreement: 1) Given the previous report and the current report sentence, indicate whether the extracted comparison cue (improved, worsened, no change) is correct. If not, correct it. 2) Building from step 1 for each sentence, given a validated or corrected comparison cue, validate whether all the anatomical location(s) extracted are correct. If incorrect or missing, correct it. 3) Building from step 2 for each sentence, given each correct comparison cue and anatomical location relation, indicate whether the attributes assigned to the location described or implied in the sentence is correct or not. If not, correct it. 3) For recall, review all sentences with no comparison cue extractions and manually de-novo annotate these sentences.

% \textbf{\textit{C) Evaluating anatomy object detection for CXR images}}: The first and second CXR images for the same 500 patients were dual validated and corrected for the bounding box objects (i.e., 1000 frontal CXR images altogether). Given the resources we had, we selected 28 anatomical objects that are clinically most important for frontal CXRs interpretations. The bounding box coordinates automatically extracted from the bounding box (Bbox) pipeline were first plotted on resized and padded 224x224 images. From piloting, we determined that this image size is sufficiently large to annotate the anatomies that we are targeting. The plotted images are displayed one at a time to annotators. %via a custom Jupyter Notebook that we had setup to allow bounding box coordinates and label annotations.  %such details are not needed (jupyter etc)
% We set up the annotation task on two panels, one for lung-related bounding boxes and another for mediastinum-related bounding boxes. %(see Figure \ref{fig:annotation_panels}). 
% Four medical residents were trained to perform this task after reviewing a set of 20-30 training examples from a radiologist. Since the inter-annotator agreement is high (mean IoU > 0.96 for all objects), the final cleaned-up gold standard dataset uses the average coordinates from two annotators for each bounding box.