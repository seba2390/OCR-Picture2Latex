\section*{Supplementary Material}

\section{Additional Chest ImaGenome Terminology Descriptions}
% \begin{table}[ht]
%   \centering
%     \caption{Parallels between the Chest ImaGenome and Visual Genome datasets.}
%   \label{tab:cg_vg_parallels}
%   \footnotesize{
%     \begin{tabular}{|p{4em}|p{21em}|p{16em}|}
%     \toprule
%     \textbf{Element} & \textbf{Chest ImaGenome} & \textbf{Visual Genome} \\
%     \midrule
%     \midrule
%     Scene & One frontal CXR image in the current dataset. & One (non-medical) everyday life image. \\
%     \midrule
%     Questions & For now, there is only one question per CXR, which is taken from the patient history (i.e., reason for exam) section from each CXR report. & One or more questions that the crowd source annotators decided to ask about the image where the information from each question and the image should allow another annotator to answer it. \\
%     \midrule
%     Answers & N/A currently. However, report sentences are biased towards answering the question asked in the reason for exam sentence;hence, the knowledge graph we extract from each report should contain the answer(s). & This was collected as answer(s) to the corresponding question(s) asked of the image. \\
%     \midrule
%     Sentences (Region descriptions) & Sentences from the finding and impression sections of a CXR report describing the exam as collected from radiologists in their routine radiology workflow. & True natural language descriptive sentences about the image collected from crowd-sourced everyday annotators. \\
%     \midrule
%     % Region & Bounding box coordinates that encompass all the anatomical structures described in a report sentence (can be easily derived from the scene graph json). & Bounding box coordinates that include all the objects mentioned in a sentence that describes the image, e.g., The boy (object 1) is beside the bus (object 2). \\
%     % \midrule
%     Objects (nodes) & Anatomical structures or locations that have bounding box coordinates on the associated CXR image, and is indexed to the UMLS ontology \cite{bodenreider2004unified}. & The people and physical objects with bounding box coordinates on the image and indexed to WordNet ontology \cite{miller1995wordnet}. \\
%     \midrule
%     Attributes (nodes) & Descriptions that are true for different anatomical structures visualized on the CXR image (e.g., There is a right upper lung [object] opacity [attribute]), indexed to the UMLS ontology \cite{bodenreider2004unified}. No Bbox coordinates. & Various descriptive properties of the objects in the image (e.g., The shirt [object] is blue [attribute]), indexed to WordNet ontology \cite{miller1995wordnet}. No Bbox coordinates. \\
%     \midrule
%     Relations: object and attribute & The relationship(s) between an anatomical object and its attribute(s) from the same CXR image (e.g., There is a [relation] right upper lung [object] opacity [attribute]). & The relationship(s)  between an object and its attribute(s) from the same image ( e.g., The shirt [object] is [relation] blue [attribute]). \\
%     \midrule
%     Relations: object and object & The comparison relationship (index to UMLS \cite{bodenreider2004unified}) between the same anatomical object from two sequential CXR images for the same patient (e.g., There is a new [relation] right lower lobe [current and previous anatomical objects] atelectasis [attribute]). & The relationship (indexed to WordNet \cite{miller1995wordnet}) between objects in the same image (e.g., The boy [object 1] is beside [relation] the bus [object 2]). \\
%     \midrule
%     Relations: parent and child & To make the graph for each image logically consistent and correct as learnable and consumable radiology knowledge, affirmed parent-child relations between nodes are embedded in the scene graphs -- i.e., if a child attribute is related to an object, then its parent would be too (e.g., if right lung has consolidation [child], then it also has lung opacity [parent]). & N/A due to different graph construction strategy and goals. The annotators were asked to describe any (but not all) relations they observe in an image. \\
%     \midrule
%     Scene graph & Constructed from the objects, the attributes and the relationships between them for the image. & Same but the nodes and edges overall would be more varied than Chest ImaGenome for now. \\
%     \midrule
%     Sequence* & A super-graph for a set of chronologically ordered series of exams for the same patient. & N/A, but would be a graph for a video in the non-medical context. \\
%     \bottomrule
%     \end{tabular}%
%     }
% \end{table}

% \newpage

%\subsection*{Data description - supplementary}
% describes nodes in graph
\begin{table}[h]
  \centering
     \caption{Semantic category of nodes and edges in CXR knowledge graphs. All nodes are mapped to UMLS CUIs in the scene graph jsons. All object nodes have corresponding bounding box coordinates on frontal CXRs except ones with *. All nodes and edges are evaluated with the gold standard dataset except the edges marked with **, which are modifiers of the context edges.}
%   \caption{Add caption}
    \label{tab:define_nodes_edges} 
    \resizebox{\textwidth}{!}{
    \begin{tabular}{|l|c|p{25em}|}
    \toprule
    \textbf{Category ID} & \textbf{type} & \multicolumn{1}{l|}{\textbf{names}} \\
    \midrule
    \midrule
    technicalassessment & attribute node & low lung volumes, rotated, artifact, breast/nipple shadows, skin fold \\
    \midrule
    texture & attribute node & opacity, alveolar, interstitial, calcified, lucency \\
    \midrule
    anatomicalfinding & attribute node & lung opacity, airspace opacity, consolidation, infiltration, atelectasis, linear/patchy atelectasis, lobar/segmental collapse, pulmonary edema/hazy opacity, vascular congestion, vascular redistribution, increased reticular markings/ild pattern, pleural effusion, costophrenic angle blunting, pleural/parenchymal scarring, bronchiectasis, enlarged cardiac silhouette, mediastinal displacement, mediastinal widening, enlarged hilum, tortuous aorta, vascular calcification, pneumomediastinum, pneumothorax, hydropneumothorax, lung lesion, mass/nodule (not otherwise specified), multiple masses/nodules, calcified nodule, superior mediastinal mass/enlargement, rib fracture, clavicle fracture, spinal fracture, hyperaeration, cyst/bullae, elevated hemidiaphragm, diaphragmatic eventration (benign), sub-diaphragmatic air, subcutaneous air, hernia, scoliosis, spinal degenerative changes, shoulder osteoarthritis, bone lesion \\
    \midrule
    disease & attribute node & pneumonia, fluid overload/heart failure, copd/emphysema, granulomatous disease, interstitial lung disease, goiter, lung cancer, aspiration, alveolar hemorrhage, pericardial effusion \\
    \midrule
    nlp   & attribute node & abnormal, normal (with respect to an anatomy/object node) \\
    \midrule
    tubesandlines & attribute node & chest tube, mediastinal drain, pigtail catheter, endotracheal tube, tracheostomy tube, picc, ij line, chest port, subclavian line, swan-ganz catheter, intra-aortic balloon pump, enteric tube \\
    \midrule
    device & attribute node & sternotomy wires, cabg grafts, aortic graft/repair, prosthetic valve, cardiac pacer and wires \\
    \midrule
    \midrule
    majorstructure & object node & right lung, left lung, mediastinum \\
    \midrule
    subanatomy & object node & right apical zone, right upper lung zone, right mid lung zone, right lower lung zone, right hilar structures, right costophrenic angle, left apical zone, left upper lung zone, left mid lung zone, left lower lung zone, left hilar structures, left costophrenic angle, upper mediastinum, cardiac silhouette, trachea, right hemidiaphragm, left hemidiaphragm, right clavicle, left clavicle, spine, right atrium, cavoatrial junction, svc, carina, aortic arch, abdomen, right chest wall*, left chest wall*, right shoulder*, left shoulder*, neck*, right arm*, left arm*, right breast*, left breast* \\
    \midrule
    \midrule
    context & edge  & yes (has/present in), no (not have/not present in)\\
    \midrule
    comparison & edge  & improved, worsened, no change \\
    \midrule
    severity** & edge  & hedge, mild, moderate, severe \\
    \midrule
    temporal** & edge  & acute, chronic \\
    \bottomrule
    \end{tabular}
    }%
  %\label{tab:addlabel}%
\end{table}

\newpage
\section{Scene Graph JSON}\label{jsonsg}
Below are examples from a scene graph JSON used for explanation for the silver dataset.

\subsection{Scene Graph JSON - first level}\label{json1}
\begin{footnotesize}
\begin{verbatim}
{
 `chest_imageimage_id': `10cd06e9-5443fef9-9afbe903-e2ce1eb5-dcff1097',
 `viewpoint': `AP', `patient_id': 10063856, `study_id': 56759094,
 `gender': `F', `age_decile': `50-60',
 `reason_for_exam': `___F with hypotension.  Evaluate for pneumonia.',
 `StudyOrder': 2, `StudyDateTime': `2178-10-05 15:05:32 UTC',
 `objects': [ <...list of {} for each object...> ],
 `attributes':[ <...list of {} for each object...> ],
 `relationships':[ <...list of {} of comparison relationships between objects 
 from sequential exams for the same patient...> ] 
}
\end{verbatim}
\end{footnotesize}


\subsection{Scene Graph JSON - objects field}\label{json2}
\begin{footnotesize}
\begin{verbatim}
{
  `object_id': `10cd06e9-5443fef9-9afbe903-e2ce1eb5-dcff1097_right upper lung zone',
  `x1': 48, `y1': 39, `x2': 111, `y2': 93,
  `width': 63, `height': 54,
  `bbox_name': `right upper lung zone',
  `synsets': [`C0934570'],
  `name': `Right upper lung zone',
  `original_x1': 395, `original_y1': 532,
  `original_x2': 1255, `original_y2': 1268,
  `original_width': 860, `original_height': 736
}
\end{verbatim}
\end{footnotesize}


\subsection{Scene Graph JSON - attributes field}\label{json3}
\begin{footnotesize}
\begin{verbatim}
{
  `right lung': True, `bbox_name': `right lung',
  `synsets': [`C0225706'], `name': `Right lung',
  `attributes': [[`anatomicalfinding|no|lung opacity',
  `anatomicalfinding|no|pneumothorax',  `nlp|yes|normal'],
  [`anatomicalfinding|no|pneumothorax']],
  `attributes_ids': [[`CL556823', `C1963215;;C0032326', `C1550457'],
  [`C1963215;;C0032326']],
  `phrases': [`Right lung is clear without pneumothorax.', 
  `No pneumothorax identified.'],
  `phrase_IDs': [`56759094|10', `56759094|14'],
  `sections': [`finalreport', `finalreport'],
  `comparison_cues': [[], []],
  `temporal_cues': [[], []],
  `severity_cues': [[], []],
  `texture_cues': [[], []],
  `object_id': `10cd06e9-5443fef9-9afbe903-e2ce1eb5-dcff1097_right lung'
}
\end{verbatim}
\end{footnotesize}


\subsection{Scene Graph JSON - relationships field}\label{json4}
\begin{footnotesize}
\begin{verbatim}
{
  `relationship_id': `56759094|7_54814005_C0929215_10cd06e9_4bb710ab',
  `predicate': ``['No status change']'',
  `synsets': [`C0442739'],
  `relationship_names': [`comparison|yes|no change'],
  `relationship_contexts': [1.0],
  `phrase': `Compared with the prior radiograph, there is a persistent veil 
  -like opacity\n over the left hemithorax, with a crescent of air surrounding 
  the aortic arch,\n in keeping with continued left upper lobe collapse.',
  `attributes': [`anatomicalfinding|yes|atelectasis',
  `anatomicalfinding|yes|lobar/segmental collapse',
  `anatomicalfinding|yes|lung opacity', `nlp|yes|abnormal'],
  `bbox_name': `left upper lung zone',
  `subject_id': `10cd06e9-5443fef9-9afbe903-e2ce1eb5-dcff1097_left upper lung zone',
  `object_id': `4bb710ab-ab7d4781-568bcd6e-5079d3e6-7fdb61b6_left upper lung zone'
}
\end{verbatim}
\end{footnotesize}


\subsection{Scene Graph - Enriched RDF JSON format}
\begin{footnotesize}\label{json5}
\begin{verbatim}
{
 <study_id_i> : [
                  [[node_id_1, node_type_1], [node_id_2, node_type_2], relation_name_A],
                  [[node_id_1, node_type_1], [node_id_3, node_type_3], relation_name_B],
                    ...
                ],
 <study_id_i+1>:[
                  [[node_id_1, node_type_1], [node_id_2, node_type_2], relation_name_A],
                  [[node_id_1, node_type_1], [node_id_3, node_type_3], relation_name_B],
                    ...
                ],
}   
\end{verbatim}
\end{footnotesize}


\section{Gold Dataset Annotation - Details}
\label{gold_annot_supp}

The `gold dataset' is a randomly sampled subset (500 unique patients) from the automatically generated Chest ImaGenome dataset, i.e., the `silver dataset', that has been manually validated or corrected. The primary purpose of the `gold dataset' is to evaluate the quality of labels in the `silver dataset'. For this purpose, we evaluated the Chest ImaGenome dataset along with the 3 components below (A-B). The annotations for each component were collected in stages to reduce the cognitive workload for the annotators. The annotators are all M.D.s with 2 to 10 or more years of clinical experience. One of the annotators is a radiologist trained in the United States, who has over 6 years of radiology experience and specializes in reading imaging exams from the Emergency Department (ED) setting. The annotation tasks were delegated to the annotators according to their clinical experience, which we think are all more than sufficient for the tasks. Component A and B were annotated by the radiologist and an M.D. and component C was annotated by 4 M.D.'s.


\vspace{+10pt}
\textbf{\textit{A) Evaluating CXR knowledge graph extraction from reports}}
\vspace{+5pt}

The report knowledge graph for the \textit{first} CXR of the 500 patients was manually reviewed and corrected as necessary for relation extraction between the anatomical locations (objects) and the CXR attributes. From piloting trials, we found that manually annotating multiple targets at a document level lead to a slow and complex task with poor recall. However, sometimes information from prior sentences is necessary to annotate both the anatomical locations and the attributes correctly. Therefore, we set up the annotation task at the sentence level. Sentences from each report are ordered as per the original report, and the phrase boundary for each attribute was marked out for the annotators, where the phrases used for detecting each attribute were curated by consensus between two radiologists from previous work \cite{wu2020ai}. 

Since we are targeting a large set of possible anatomical locations (object) to attribute combinations, the annotation was streamlined into the four steps below to minimize the cognitive overload for each step. Steps 1 and 2 are dual annotated by two clinicians (one fully trained radiologist and one M.D.), with disagreements resolved by consensus review. Steps 3 and 4 are single annotated. A random subset of annotations for 500 sentences from step 4 are sampled and dual annotated to estimate inter-annotator agreement. Cleaned results from step 4 constitute the final gold-standard CXR knowledge graph ground truth for the 500 reports. 

This annotation component was set up in Excel and was broken down into the following four steps below. In our Excel setup, all sentences from each report are available to the annotators (they can just scroll up or down). The sentences are ordered by `row\_id' sequentially within each report. Unique patients and reports have the same IDs as shown in the figures below.

\textbf{Step 1} - For each sentence and NLP extracted attribute combination, decide whether the NLP context (affirmed or negated) for the attribute was correct. If not, correct it. Figure \ref{fig:object-attribute-step1} shows how this task was set up in Excel. The annotators' task is to make sure the extracted attribute (yellow label\_name column) has the correct context given the sentence from the report. This `context' is used as the relation between the location and the attribute in the final annotated result.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.35]{figures/annot/object_attribute_annot_step1.pdf}
\caption{Step 1: Annotate all attributes per sentence.}
\label{fig:object-attribute-step1}
\end{figure}

\textbf{Step 2} - For each sentence, decide whether the NLP extracted anatomical location(s) were described or implied by the reporting radiologist. If not, remove the location (in yellow column `bboxes\_corrected). If missing, add the location. If unsure (e.g., if lung is mentioned but not sure if it is the right or left lung), the annotator can look in previous sentences from the same report. The task was set up as shown in Figure \ref{fig:object-attribute-step2}.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.29]{figures/annot/object_attribute_annot_step2.pdf}
\caption{Step 2: Annotate all locations per sentence.}
\label{fig:object-attribute-step2}
\end{figure}

\textbf{Step 3} - For recall, manually annotate missed objects and/or attributes for sentences with no NLP extractions (a much smaller subset). For this, we used Excel's filtering function to look at all sentences with no automated extractions (empty cells) and de novo added the manual annotations.

\textbf{Step 4} - Firstly, all rows from steps 1-3 where the annotations differed between the two annotators were reviewed and resolved together by consensus. Then we automatically derived all object-attribute relation combinations for each sentence from steps 1-3's results. The obviously wrong object-to-attribute relations were filtered out for each sentence using the CXR ontology. For the remaining object-to-attribute relations for each sentence, the task was to indicate whether the logical statement of \textit{``object X contains (or does not contain) attribute Y''} is true or false, as shown in Figure \ref{fig:object-attribute-step4}. Probable relation is still defined to be true for this annotation. Annotating for uncertain relations is beyond the scope of this project. However, for future dataset expansion, we have kept the NLP cues for the certainty for each object-attribute relation in the scene graph JSON. 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.33]{figures/annot/object_attribute_annot_step4.pdf}
\caption{Step 4: Annotate all logically correct statements/relations for each sentence.}
\label{fig:object-attribute-step4}
\end{figure}

Since step 4 was single annotated, to estimate the final inter-annotator agreement, we randomly sampled 500 sentences for dual annotations. This %\href{https://physionet.org/content/chest-imagenome/1.0.0/utils/annotation_utils/object_attribute_relation_annotation/object_attribute_relations_estimated_interannotator_agreement.txt}{\textbf{\textit{annotated result}}} 
annotated result is also shared on PhysioNet.

\vspace{+10pt}
\textbf{\textit{B) Evaluating comparison relation extraction}}: 
\vspace{+5pt}

The \textit{second} CXR exam report for the 500 patients was reviewed for comparison relation extraction. The annotation was also set up in Excel and conducted at the sentence level. However, the annotator is also shown the whole previous CXR report for context. Similarly, we split the annotation task up into several steps, where steps 1 and 2 are dual annotated and disagreement resolved via consensus. Steps 3 and 4 were single annotated. A %\href{https://physionet.org/content/chest-imagenome/1.0.0/utils/annotation_utils/object_object_comparison_annotation/comparisons_relations_estimated_interannotator_agreement.txt}{\textbf{\textit{subset of 500 sentences}}}
subset of 500 sentences from the final annotations was reviewed by a second annotator for assessing inter-annotator agreement.

\textbf{Step 1} - Given the previous report and the current report sentence, decide whether the extracted comparison cue(s) (improved, worsened, no change) is/are correct. If not, correct it/them. In this step, the annotators are asked to validate or correct the column `comparison' in Figure \ref{fig:object-comparison-step12}.

\textbf{Step 2} - Building from step 1 for each sentence, given a validated or corrected comparison cue, validate whether all the anatomical location(s) extracted are correct (column `bbox' in Figure \ref{fig:object-comparison-step12}). If incorrect or missing, remove or add the correct location(s) to the column.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.31]{figures/annot/object_object_comparison_annot_step1_2.pdf}
\caption{Step 1 and 2: Annotate change relations for different anatomical locations}
\label{fig:object-comparison-step12}
\end{figure}

\textbf{Step 3} - Building from step 2 for each sentence, given each correct comparison cue and anatomical location relation, decide whether the attributes assigned to the location described or implied in the sentence are correct or not. If not, correct it. Figure \ref{fig:object-comparison-step3} illustrates how step 3 was set up, where the annotators' task is to validate or correct the `label\_name' column with respect to the `bbox', `relation' and `comparison' columns for each sentence.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.3]{figures/annot/object_object_comparison_annot_step3.pdf}
\caption{Step 3: Annotate change relations for different anatomical locations with respect to attribute}
\label{fig:object-comparison-step3}
\end{figure}

\textbf{Step 4} - For recall, we used the filtering function in Excel to isolate all sentences with no comparison cue extractions from step 3. Sentences with missing comparison annotations were manually de-novo annotated.

\vspace{+10pt}
\textbf{\textit{C) Evaluating anatomy object detection for CXR images}}: 
\vspace{+5pt}

The first and second CXR images for the same 500 patients were dual validated and corrected for the bounding box objects (i.e., 1000 frontal CXR images altogether). Given the resources we had, we selected 28 anatomical objects (out of 36 available) that are clinically most important for frontal CXRs interpretations. The automatically extracted bounding box coordinates were first plotted on resized and padded 224x224 images. From piloting, we determined that this image size is sufficiently large to annotate the anatomies that we were targeting. The plotted images were displayed one at a time to annotators via a custom Jupyter Notebook that we had set up to allow bounding box coordinates and label annotations. We set up the annotation task on two panels, one for %\href{https://physionet.org/content/chest-imagenome/1.0.0/utils/annotation_utils/bbox_object_annotation/Correct_lung_bboxes_template.ipynb}{\textbf{\textit{lung related bounding boxes}}} 
lung-related bounding boxes (Figure \ref{fig:bboxes-lung-panel}) and another for %\href{https://physionet.org/content/chest-imagenome/1.0.0/utils/annotation_utils/bbox_object_annotation/Correct_mediastinum_bboxes_template.ipynb}{\textbf{\textit{mediastinum related and other bounding boxes}}} 
mediastinum related and other bounding boxes (Figure \ref{fig:bboxes-mediastinum-panel}). 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.35]{figures/annot/lung_related_bbox_panel.pdf}
\caption{Bbox annotations - lung related Bboxes panel}
\label{fig:bboxes-lung-panel}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.35]{figures/annot/mediastinum_related_bbox_panel.pdf}
\caption{Bbox annotations - mediastinum related and other Bboxes panel}
\label{fig:bboxes-mediastinum-panel}
\end{figure}

Four M.D.'s were trained to perform this task after reviewing a set of 20-30 training examples with a radiologist. Since the inter-annotator agreement is high (mean IoU > 0.96 for all objects), the final cleaned %\href{https://physionet.org/content/chest-imagenome/1.0.0/gold_dataset/gold_bbox_coordinate_annotations_1000images.csv}{\textbf{\textit{gold standard bbox coordinates}}} 
gold standard bbox coordinates use the average coordinates from two annotators for each bounding box.


\newpage
\section{Dataset Usage Supporting Files}
\label{gold_supp}

% Can also put this in supplementary material
\noindent \textbf{gold\_all\_sentences\_500pts\_1000studies.txt} contains all the sentences tokenized from the original MIMIC-CXR reports that were used to create the gold standard dataset. We include this file because sentences with no relevant object, attribute or relation descriptions did not make it into the gold standard dataset. We renamed `subject\_id' from MIMIC-CXR dataset to `patient\_id' in Chest ImaGenome dataset to avoid confusion with field names for relationships in the scene graphs. Otherwise, the ids are unchanged. Sentences in the tokenized file are assigned to `history', `prelimread', or `finalreport' in the `section' column. The `sent\_loc' column contains the order of the sentences as in the original report. Minimal tokenization has been done to the sentences.

\noindent \textbf{gold\_bbox\_scaling\_factors\_original\_to\_224x224.csv} contains the scaling `ratio' and the paddings (`left', `right', `top', and `bottom') added to square the image after resizing the original MIMIC-CXR dicoms to 224x224 sizes. These ratios were used to rescale the annotated coordinates for 224x224 images back to the original CXR image sizes.

\noindent \textbf{auto\_bbox\_pipeline\_coordinates\_1000\_images.txt} contains the bounding box coordinates that were automatically extracted by the Bbox pipeline for the different objects for images in the gold standard dataset. It is in a tabular format like with the ground truth for easier evaluation purposes.

\noindent \textbf{object-bbox-coordinates\_evaluation.ipynb} notebook calculates the bounding box object detection performance using ground truth files from the 4 M.D. annotators , as well as consolidating the final \textbf{gold\_bbox\_coordinate\_annotations\_1000images.csv}.

\noindent \textbf{Preprocess\_mimic\_cxr\_v2.0.0\_reports.ipynb} processes the reports (tokenize sentences and sort them into history, prelim or final report sentences) from the original MIMIC-CXR v2.0.0 and save output as \textbf{silver\_dataset/cxr-mimic-v2.0.0-processed-sentences\_all.txt}. Only sentences with object or attribute extractions ended up in the final scene graph jsons in the Chest ImaGenome dataset.

\noindent The \textbf{semantics} directory contains the object (\textbf{objects\_detectable\_by\_bbox\_pipeline\_v1.txt} and \textbf{objects\_extracted\_from\_reports\_v1.txt}), attribute (\textbf{attribute\_relations\_v1.txt}) and comoparison (\textbf{comparison\_relations\_v1.txt}) relations labels in the Chest ImaGenome dataset. It also contains \textbf{semantics/label\_to\_UMLS\_mapping.json}, which maps all Chest ImaGenome concepts to UMLS CUIs \cite{bodenreider2004unified}.



\newpage
\section{Dataset Evaluation}

% \begin{figure}[!ht]
% \centering
% \includegraphics[scale=0.45]{figures/Figure_6_lung_mediastinum_clavicle_bboxes.pdf}
% \caption{Sample CXR case with 17 overlaying anatomical bounding boxes.}
% \label{fig:bbox-sample}
% \end{figure}

Table \ref{tab:object-detect} reports anatomical location level object-to-attribute relations extraction performance by the scene graph extraction pipeline. The report numbers are calculated by a combination of notebooks: `generate\_scenegraph\_statistics.ipynb', `object-attribute-relation\_evaluation.ipynb' and `object-bbox-coordinates\_evaluation.ipynb'.

\begin{table}[th]
\centering
\caption{CXR image object detection evaluation results. \** These anatomical locations are extracted by the Bbox pipeline but they are not manually annotated in the gold standard dataset due to resource constraints. \*** The mediastinum bounding boxes were not directly annotated due to resource constraints. Mediastinum's bounding box boundary can be derived from the ground truth for the upper mediastinum and the cardiac silhouette.
}\label{tab:object-detect}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Bbox name \\ (object)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Object-attribute relations  \\  frequency (500 reports)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Relationships F1\\ (500 reports)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Bbox IoU \\ (over 1000 images)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\% Bboxes corrected \\ (1000 images)\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\% Relations missing \\ Bbox coordinates \\ (over whole dataset)\end{tabular}}} \\ \hline
left lung & 1453 & 0.933 & 0.976 & 9.90\% & 0.03\% \\ \hline
right lung & 1436 & 0.937 & 0.983 & 6.30\% & 0.04\% \\ \hline
cardiac silhouette & 633 & 0.966 & 0.967 & 9.70\% & 0.01\% \\ \hline
mediastinum & 601 & 0.952 & ** & ** & 0.02\% \\ \hline
left lower lung zone & 609 & 0.932 & 0.955 & 8.60\% & 2.36\% \\ \hline
right lower lung zone & 580 & 0.902 & 0.968 & 6.00\% & 2.27\% \\ \hline
right hilar structures & 572 & 0.934 & 0.976 & 4.10\% & 1.91\% \\ \hline
left hilar structures & 571 & 0.944 & 0.971 & 4.30\% & 2.28\% \\ \hline
upper mediastinum & 359 & 0.940 & 0.994 & 1.40\% & 0.12\% \\ \hline
left costophrenic angle & 298 & 0.908 & 0.929 & 9.60\% & 0.63\% \\ \hline
right costophrenic angle & 286 & 0.918 & 0.944 & 6.90\% & 0.39\% \\ \hline
left mid lung zone & 173 & 0.940 & 0.967 & 5.70\% & 2.79\% \\ \hline
right mid lung zone & 169 & 0.830 & 0.968 & 5.30\% & 2.31\% \\ \hline
aortic arch & 144 & 0.965 & 0.991 & 1.40\% & 0.62\% \\ \hline
right upper lung zone & 117 & 0.873 & 0.972 & 5.80\% & 0.04\% \\ \hline
left upper lung zone & 83 & 0.811 & 0.968 & 6.40\% & 0.22\% \\ \hline
right hemidiaphragm & 78 & 0.947 & 0.955 & 7.90\% & 0.15\% \\ \hline
right clavicle & 71 & 0.615 & 0.986 & 2.80\% & 0.50\% \\ \hline
left clavicle & 67 & 0.642 & 0.983 & 3.00\% & 0.51\% \\ \hline
left hemidiaphragm & 65 & 0.930 & 0.944 & 11.30\% & 0.14\% \\ \hline
right apical zone & 58 & 0.852 & 0.969 & 5.40\% & 1.99\% \\ \hline
trachea & 57 & 0.983 & 0.995 & 0.90\% & 0.24\% \\ \hline
left apical zone & 47 & 0.938 & 0.963 & 6.20\% & 2.40\% \\ \hline
carina & 41 & 0.975 & 0.994 & 0.80\% & 1.47\% \\ \hline
svc & 19 & 0.973 & 0.995 & 0.70\% & 0.66\% \\ \hline
right atrium & 14 & 0.963 & 0.979 & 4.00\% & 0.18\% \\ \hline
cavoatrial junction & 5 & 1.000 & 0.977 & 4.30\% & 0.25\% \\ \hline
abdomen & 80 & 0.904 & * & * & 0.26\% \\ \hline
spine & 132 & 0.824 & * & * & 0.10\% \\ \hline
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}


\newpage
\section{Pictorial Overview of Model Architectures}
Due to space limitations, we present overview figures for the models designed for Example Tasks 1 and 2 here.
\label{clinical_applications}

\begin{figure}[h]
\center
\includegraphics[width=0.9\textwidth,height=4cm]{figures/siamese.pdf}
\caption{Example Task 1 Model Overview. Given a pair of CXR images, we extract features for the anatomical regions of interest with a pretrained ResNet autoencoder, concatenate representations and pass them through a dense layer and a final classification layer.}
\vspace{-0.3cm}
\label{fig:siamese}
\end{figure} 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/ML-GCN.pdf}
\caption{Example Task 2 Model Overview. Given a pair of CXR images, we extract features for the anatomical regions of interest with a pretrained Faster R-CNN and a GCN to learn the label dependencies.}
\label{fig:gcn}
\vspace{-0.1cm}
\end{figure}

\newpage
\section{Qualitative Evaluation}
In Figure \ref{tab:findings}, we visualize the output from our model for the anatomical finding predictions of costophrenic angles and enlarged cardiac silhouette.
In Figure \ref{tab:gradcam}, we present an additional example, showing that the model is able to provide accurate localization information as well as predict the correct finding, i.e., showing accurate localization.


\begin{table}[h]
\begin{subtable}[t]{0.5\textwidth}
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{ccc}
\textbf{{Image 1}} & \textbf{{CS}}  & \textbf{{RCA}} \\ 
\includegraphics[width=0.4\textwidth,height=0.4\textwidth]{figures/vis/viz1.png} & \includegraphics[width=0.4\textwidth,height=0.4\textwidth]{figures/vis/cs_1.png} & \includegraphics[width=0.4\textwidth,height=0.4\textwidth]{figures/vis/rca_1.png} \\[0.15cm]
\myalign{l}{Ground Truth} & \myalign{l}{\textbf{No findings}} & \myalign{l}{\textbf{No findings}} \\[0.15cm] 
%\myalign{l}{CheXGCN} & \myalign{l}{\color{red} \textbf{L4} } & \myalign{l}{\color{red} \textbf{L1, L2} } \\[0.15cm] 
\myalign{l}{Our model \cite{agu2021anaxnet}} & \myalign{l}{\color{green} \textbf{No findings}} & \myalign{l}{\color{green} \textbf{No findings}} \\ 
\end{tabular}
}
\end{subtable} 
\hspace{0.2cm}
\begin{subtable}[h]{0.5\textwidth}
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{ccc}
\textbf{{Image 2}} & \textbf{{RCA}}  & \textbf{{LCA}} \\
\includegraphics[width=0.4\textwidth,height=0.4\textwidth]{figures/vis/viz2.png} & \includegraphics[width=0.4\textwidth,height=0.4\textwidth]{figures/vis/rca_2.png} & \includegraphics[width=0.4\textwidth,height=0.4\textwidth]{figures/vis/lca_2.png} \\[0.15cm] 
\myalign{l}{Ground Truth} & \myalign{l}{\textbf{L2}} & \myalign{l}{\textbf{L2}} \\[0.15cm]  
%\myalign{l}{CheXGCN} & \myalign{l}{\color{red} \textbf{No findings}} & \myalign{l}{\color{red} \textbf{No findings}} \\[0.15cm]  
\myalign{l}{Our model \cite{agu2021anaxnet}} & \myalign{l}{\color{green} \textbf{L2}} & \myalign{l}{\color{green} \textbf{L2}}
\end{tabular}
}
\end{subtable}
\vspace{0.2cm}
\captionof{figure}{Examples of the prediction results. The overall chest X-ray image is shown alongside two anatomical regions, and predictions are compared against the ground-truth labels.
} 
\label{tab:findings}
\end{table}

\begin{figure}[h]
  \centering
   \resizebox{0.8\textwidth}{!}{
  \subfloat[Original Image]{
  \includegraphics[width=0.3\textwidth, height=0.3\textwidth]{figures/vis/orig_ecs_viz.png}
  \label{fig:f1}}
  %\hfill
  %\subfloat[GlobalView \scriptsize{(Grad-CAM)}]{
  %\includegraphics[width=0.3\textwidth, height=0.3\textwidth]{figures/vis/ecs_viz.png}  \label{fig:f2}}
  \hfill
  \subfloat[Our model \cite{agu2021anaxnet}]{
  \includegraphics[width=0.3\textwidth, height=0.3\textwidth]{figures/vis/ecs_box.png} \label{fig:f3}}
  }
  \caption{Example image with enlarged cardiac silhouette, showing that the trained model detects the finding in the correct bounding box.}
  \label{tab:gradcam}
\end{figure}