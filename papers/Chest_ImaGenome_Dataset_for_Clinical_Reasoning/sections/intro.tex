\section*{Introduction}
Chest X-rays (CXR) are among the commonly ordered radiology exams, mostly for screening but also for diagnostic purposes. Recently, multiple large CXR imaging datasets have been released by the research community~\cite{johnson2019mimic,demner2016preparing,wang2017chestx,irvin2019chexpert}. These can be used to develop automatic abnormality detection or report generation algorithms. For detecting specific abnormalities from images, natural language processing (NLP) algorithms have been used to extract "weak" global image-level labels (CXR abnormalities) from the associated CXR reports \cite{irvin2019chexpert,wu2020ai,smit2020chexbert,bustos2020padchest}. For automatic report generation, self-supervised joint text and image architectures \cite{wang2018tienet,li2018hybrid,zhang2018learning,liu2019clinically,zhang2020radiology}, first inspired by the image captioning related work in the non-medical domain \cite{vinyals2015show,xu2015show,karpathy2015deep,plummer2015flickr30k,gan2017semantic}, have been used to produce preliminary free-text radiology reports. However, both approaches lack rigorous localization assessment for explainability, namely whether the model attended to the relevant anatomical location(s) for predictions. This missing feature is critical for clinical applications. The joint image and text learning strategy are also known to learn heavy language priors from the text reports without having learned to interpret the imaging features \cite{rohrbach2018object,agrawal2016analyzing}. Furthermore, even though architectures suitable for comparing imaging changes are available \cite{li2020siamese,li2020automated}, limited work has focused on automatically deriving comparison relations between exams from large datasets for the purpose of training imaging models that can track progress for a wide variety of CXR findings or diseases.

To the best of our knowledge, no prior work in CXR has attempted to automatically extract relations between CXR attributes (labels) from reports and their anatomical locations (objects with bounding box coordinates) on the images as documented by the reporting radiologists, nor has there been any localized relation annotations between sequential CXR exams. Research on these two topics is valuable because radiology reports in effect are records of radiologists' complex clinical reasoning processes, where the anatomical location of observed imaging abnormalities is often used to narrow down on potential diagnoses, as well as for integrating information from other clinical modalities (e.g. CT findings, labs, etc) at the anatomical levels. Sequential exams are also routinely used by bedside clinicians to track patients' clinical progress after being started on different management paths. Therefore, documentations comparing sequential exams are prevalent in CXR reports and are clinically meaningful relations to learn about. Automatically extracting radiology knowledge graphs and disease progression information from reports will help improve explainability evaluation and widen downstream clinical applications for CXR imaging algorithm development.

% These are complex clinical reasoning tasks that require a solid understanding of the relationships between radiology imaging features and anatomical locations, from which external knowledge can be used to narrow down on the disease differential diagnosis.

Many algorithms for object detection and domain-knowledge-driven reasoning require a starting dataset that has localized labels on the images and meaningful relationships between them. In the non-medical domain, large locally labeled graph datasets (e.g., Visual Genome dataset \cite{krishna2017visual}) have enabled the development of algorithms that can integrate both visual and textual information and derive relationships between observed objects in images \cite{xu2017scene,li2017scene,yang2018graph}. In addition, they have spurred a whole domain of research in visual question answering (VQA) and visual dialogue (VD), with the aim of developing interactive AI algorithms capable of reasoning over information from multiple sources \cite{antol2015vqa,das2017visual,de2017guesswhat}. These location, relation and semantics aware systems aim to capture important elements in image data in relation to complex human languages, in order to conversationally interact with humans about the visual content. In the medical domain, such systems may help with automatic image and text information retrieval tasks from databases or improve end-user trust by allowing clinicians to interactively question trained models to assess the consistency of predictions.

In this paper, we present the Chest ImaGenome dataset, a large multi-modal (text and images) chronologically ordered scene graph dataset for frontal chest x-ray (CXR) images. This dataset is an important step towards addressing the missing link of large locally labeled graph datasets in the medical imaging domain. The goal for releasing this dataset is to spur the development of algorithms that more closely reflect radiology expertsâ€™ reasoning processes. In addition, automatically describing localized imaging features in recognized medical semantics is the first step towards connecting potentially predictive pixel-level features from medical images with the rest of the digital patient records and external medical ontologies. These connections could aid both the development of anatomically relevant multi-modal fusion models and the discovery of localized imaging fingerprints, i.e., patterns predictive of patient outcomes. Through  \href{https://doi.org/10.13026/wv01-y230}{\textbf{PhysioNet's credentialed access}} (\href{https://physionet.org/content/chest-imagenome/view-license/1.0.0/}{see \textbf{license}}), we make the first Visual Genome-like graph dataset in the CXR domain accessible for the research community.
%old link: https://PhysioNet.org/projects/BOFnNTGyCvTT6GMLVzeS

