% \section*{Figures \& Tables}

% Table generated by Excel2LaTeX from sheet 'CG_VG_parallels'
\begin{table}[h!]
  \centering
  %\caption{Add caption}
  \footnotesize{
    \begin{tabular}{|p{4em}|p{21em}|p{16em}|}
    \toprule
    \textbf{Element} & \textbf{Chest ImaGenome} & \textbf{Visual Genome} \\
    \midrule
    \midrule
    Scene & One frontal CXR image in the current dataset. & One (non-medical) everyday life image. \\
    \midrule
    Questions & For now, there is only one question per CXR, which is taken from the patient history (i.e. reason for exam) section from each CXR report. & One or more questions that the crowd source annotators decided to ask about the image where the information from each question and the image should allow another annotator to answer it. \\
    \midrule
    Answers & N/A currently. However, report sentences are biased towards answering the question asked in the reason for exam sentence;hence, the knowledge graph we extract from each report should contain the answer(s). & This was collected as answer(s) to the corresponding question(s) asked of the image. \\
    \midrule
    Sentences (Region descriptions) & Sentences from the finding and impression sections of a CXR report describing the exam as collected from radiologists in their routine radiology workflow. & True natural language descriptive sentences about the image collected from crowd-sourced everyday annotators. \\
    \midrule
    % Region & Bounding box coordinates that encompass all the anatomical structures described in a report sentence (can be easily derived from the scene graph json). & Bounding box coordinates that include all the objects mentioned in a sentence that describes the image, e.g. The boy (object 1) is beside the bus (object 2). \\
    % \midrule
    Objects (nodes) & Anatomical structures or locations that have bounding box coordinates on the associated CXR image, and is indexed to the UMLS ontology \cite{bodenreider2004unified}. & The people and physical objects with bounding box coordinates on the image and indexed to WordNet ontology \cite{miller1995wordnet}. \\
    \midrule
    Attributes (nodes) & Descriptions that are true for different anatomical structures visualized on the CXR image (e.g., There is a right upper lung [object] opacity [attribute]), indexed to the UMLS ontology \cite{bodenreider2004unified}. No Bbox coordinates. & Various descriptive properties of the objects in the image (e.g., The shirt [object] is blue [attribute]), indexed to WordNet ontology \cite{miller1995wordnet}. No Bbox coordinates. \\
    \midrule
    Relations: object and attribute & The relationship(s) between an anatomical object and its attribute(s) from the same CXR image (e.g., There is a [relation] right upper lung [object] opacity [attribute]). & The relationship(s)  between an object and its attribute(s) from the same image ( e.g., The shirt [object] is [relation] blue [attribute]). \\
    \midrule
    Relations: object and object & The comparison relationship (index to UMLS \cite{bodenreider2004unified}) between the same anatomical object from two sequential CXR images for the same patient (e.g., There is a new [relation] right lower lobe [current and previous anatomical objects] atelectasis [attribute]). & The relationship (indexed to WordNet \cite{miller1995wordnet}) between objects in the same image (e.g., The boy [object 1] is beside [relation] the bus [object 2]). \\
    \midrule
    Relations: parent and child & To make the graph for each image medically and logically consistent and correct as learnable and consumable radiology knowledge, affirmed parent-child relations between nodes are embedded in the scene graphs -- i.e., if a child attribute is related to an object, then its parent would be too (e.g., if right lung has consolidation [child], then it also has lung opacity [parent]). & N/A due to different graph construction strategy and goals. The annotators were asked to describe any (but not all) relations they observe in an image. \\
    \midrule
    Scene graph & Constructed from the objects, the attributes and the relationships between them for the image. & Same but the nodes and edges overall would be more varied than Chest ImaGenome for now. \\
    \midrule
    Sequence* & A super-graph for a set of chronologically ordered series of exams for the same patient. & N/A, but would be a graph for a video in the non-medical context. \\
    \bottomrule
    \end{tabular}%
    }
  \caption{\label{tab:cg_vg_parallels} Parallels between the Chest ImaGenome and Visual Genome datasets.}
\end{table}%

\vspace{-10pt}
The construction of the Chest ImaGenome dataset builds on the works of \cite{wu2020ai,wu2020automatic}. In summary, the text pipeline \cite{wu2020ai} first sections the report and retains only the finding and impression sentences. Then it uses a CXR concept dictionary (lexicons) to spot and detect the context (negated or affirmed) of 271 different CXR related named-entities from each retained sentence. The lexicons were curated in advance by two radiologists in consensus using a concept expansion and vocabulary grouping engine \cite{coden2012spot}. A set of sentence level filtering rules are applied to disambiguate some of the target concepts (e.g., 'collapse' mention in CXR report can be about lung 'collapse' or related to spinal fracture as in vertebral body 'collapse'). Then the named-entities for CXR labels (attributes) are associated with the name-entities for anatomical location(s) described in the same sentence with a natural language parser, SpaCy \cite{spacy}. 

Using a CXR ontology constructed by radiologists, a scene graph assembly pipeline corrected obvious attribute-to-anatomy assignment errors (e.g. lung opacity wrongly assigned to mediastinum). Finally, the attributes for each of the target anatomical regions from repeated sentences are grouped to the exam level. The result is that, from each CXR report, we extract a radiology knowledge graph where CXR anatomical locations are related to different documented CXR attribute(s). The "reason for exam" sentence(s) from each report, which contain free text information about prior patient history, are separately kept in the final scene graph JSONs. Patient history information is critical for clinical reasoning but is a piece of information that is not technically part of the "scene" for each CXR. 

For detecting the anatomical "objects" on the CXR images that are associated with the extracted report knowledge graph, a separate anatomy atlas-based bounding box pipeline extracts the coordinates of those anatomies from each frontal image. This pipeline is an extension of \cite{wu2020automatic} that covers additional anatomical locations in this dataset. In addition, we manually validated or corrected the bounding boxes for 1071 CXR images (with and without disease, and excluded gold standard subjects) to train a Faster RCNN CXR bounding box detection model, which we used to correct failed bounding boxes (too small or missing) from the initial bounding box extraction pipeline (~7\%). Finally, for quality assurance, we manually annotated 303 images that had missing bounding boxes for key CXR anatomies (lungs and mediastinum).

Extracting comparison relations between sequential exams at the anatomical level is another goal for the Chest ImaGenome dataset. After checking with the MIMIC team and reviewing their dataset documentation, we assume that the timestamps in the original MIMIC CXR dataset can be used to chronologically order the exams for each patient. We then correlated all report descriptions of changes (grouped as improved, worsened, or no change) between sequential exams with the anatomical locations described at the sentence level. To extract these comparison descriptions, we used a concept expansion engine \cite{coden2012spot} to curate and group relevant comparison vocabularies used in CXR reports. These comparison relations extracted between anatomical locations from sequential CXRs are only added to the final scene graphs for every patient's second or later CXR exam(s) -- i.e., comparison relations described in the first study of each patient in the MIMIC-CXR dataset are not added to the Chest ImaGenome dataset.

Finally, we have mapped all object and attribute nodes and comparison relations in the Chest ImaGenome dataset to a Concept Unique Identifier (CUI) in the Unified Medical Language System (UMLS) \cite{bodenreider2004unified}. The UMLS ontology has incorporated the concepts from the Radlex ontology [31], which is constructed for the radiology domain. Choosing UMLS to index the Chest ImaGenome dataset widens its future applications in clinical reasoning tasks, which would invariably require medical concepts and relations outside the radiology domain. An example of a CXR scene graph is shown in Figure \ref{fig1.cxr_graph}.





% With the first assumption, CXR anatomical locations are defined as the ``object" nodes for an image, from which we extract bounding box (Bbox) coordinates with an automatic Bbox extraction pipeline \cite{}. All the labels described in the different anatomical locations in the reports are defined as ``attribute" nodes related to the corresponding object nodes for the image. 
%The attribute nodes do not have corresponding Bboxes, hence the CXR scene graphs are heterogeneous compared to Visual Genome. 
% Each attribute nodes can belong to one of six different radiology semantic categories and can be positively (affirmed) or negatively (specifically negated in reports) associated with different CXR objects. For affirmed relations, additional cues are provided in the scene graph jsons when the relation is uncertain. 



% We made the second assumption based on MIMIC-CXR's documented data collection strategy, where the original CXR curators included all CXR exams (including from outpatient department) of patients who were at any time point admitted to the BIDMC's Emergency Department within a continuous 2 year period. Using this assumption, we correlate any comparison descriptions (improved, worsened, or no change) in the report between the current and previous CXR exam at the level of anatomical ``object(s)" and with respect to the described scene graph ``attribute(s)".





% Figures, tables, and their legends, should be included at the end of the document. Figures and tables can be referenced in \LaTeX{} using the ref command, e.g. Figure \ref{fig:stream} and Table \ref{tab:example}. 

% Authors are encouraged to provide one or more tables that provide basic information on the main ‘inputs’ to the study (e.g. samples, participants, or information sources) and the main data outputs of the study. Tables in the manuscript should generally not be used to present primary data (i.e. measurements). Tables containing primary data should be submitted to an appropriate data repository.

% Tables may be provided within the \LaTeX{} document or as separate files (tab-delimited text or Excel files). Legends, where needed, should be included here. Generally, a Data Descriptor should have fewer than ten Tables, but more may be allowed when needed. Tables may be of any size, but only Tables which fit onto a single printed page will be included in the PDF version of the article (up to a maximum of three). 

% Due to typesetting constraints, tables that do not fit onto a single A4 page cannot be included in the PDF version of the article and will be made available in the online version only. Any such tables must be labelled in the text as ‘Online-only’ tables and numbered separately from the main table list e.g. ‘Table 1, Table 2, Online-only Table 1’ etc.

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{stream}
% \caption{Legend (350 words max). Example legend text.}
% \label{fig:stream}
% \end{figure}

% \begin{table}[ht]
% \centering
% \begin{tabular}{|l|l|l|}
% \hline
% Condition & n & p \\
% \hline
% A & 5 & 0.1 \\
% \hline
% B & 10 & 0.01 \\
% \hline
% \end{tabular}
% \caption{\label{tab:example}Legend (350 words max). Example legend text.}
% \end{table}