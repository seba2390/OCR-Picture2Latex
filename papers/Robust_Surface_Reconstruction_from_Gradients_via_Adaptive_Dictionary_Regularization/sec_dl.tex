%!TEX root = G2S_ICIP.tex

Given normal vectors corrupted by noise or other non-idealities, solving \eqref{eq:surf_ls} directly generally produces a rough, bumpy surface, even when the underlying true surface is smooth. Thus, in this work, we propose an adaptive dictionary regularizer that can be combined with the least squares model \eqref{eq:surf_ls} to more accurately estimate the underlying surface. In particular, we propose to solve the dictionary learning problem
\begin{align} \label{eq:dl_surf}
\argmin_{z,B,D} & ~ \frac{1}{2} \left \| A z - v \right \|_2^2 + \lambda \bigg(\sum_{j = 1}^c \left \| P_j z - D b_j \right \|_2^2 + \mu^2 \left \| B \right \|_0 \bigg) \nonumber \\
\text{s.t.} & ~ \left \| d_i \right \|_2 = 1, \ \ \left \| b_j \right \|_{\infty} \leq a, \ \ \forall i,j.
\end{align}
Here, $P_j$ is a patch extraction operator that extracts a vectorized $c_x \times c_y$ spatial patch from $z$, $D \in \mathbb{R}^{c_x c_y \times K}$ is a dictionary matrix whose columns $d_i$ are vectorized $c_x \times c_y$ atoms, and $B \in \mathbb{R}^{K \times c}$ is a matrix of sparse codes, where column $b_j$ of $B$ defines the (usually sparse) linear combination of atoms used to represent the patch $P_j z$ of $z$. Also, $\left \| \ . \ \right \|_0$ is the familiar $\ell_0$ ``norm" and $\lambda, \mu > 0$ are regularization parameters. 

We include the constraints $\|b_j\|_{\infty} \leq a$, where $a$ is typically very large, to prevent pathologies that could theoretically arise during minimization since \eqref{eq:dl_surf} is non-coercive with respect to $B$, but the constraint is inactive in practice \cite{sairajfes2}. In addition, we constrain the columns of $D$ to be unit-norm to avoid scaling ambiguity between $D$ and $B$ \cite{kar}. Note that \eqref{eq:dl_surf} is a non-convex problem.

By solving \eqref{eq:dl_surf}, we are attempting to estimate our surface $z$ by numerically integrating its gradient fields through a least squares functional while simultaneously enforcing that local patches of the reconstructed surface should have sparse representations with respect to the dictionary $D$. As $D$ itself is learned, our proposed algorithm can automatically adapt to the underlying properties of the surface and its gradients. Intuitively, since the same dictionary of atoms is used to (sparsely) represent all patches, the atoms can learn universal features of the surface. This effectively equips \eqref{eq:dl_surf} with global information to estimate each patch, which (as we will show) can yield robust reconstructions when the gradients are noisy/corrupted.

\begin{figure*}
\centering
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/pyramid_true_view1.png}
  \caption{Ground Truth}
\end{subfigure}
~ %\quad
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/pyramid_1_20_dls_view1.png}
  \caption{\textbf{DLS}}
\end{subfigure}
~ 
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/pyramid_1_20_glss_view1.png}
  \caption{SR}
\end{subfigure}
~ %\quad
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/pyramid_1_20_tv_view1.png}
  \caption{TV}
\end{subfigure}
~
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/pyramid_1_20_sim2_view1.png}
  \caption{DCTLS}
\end{subfigure}
\caption{Surface reconstructions of the Tent dataset with SNR = 20 dB.}
\label{fig:tent}
\end{figure*}

\begin{figure*}
\centering
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/vase_true_view3.png}
  \caption{Ground Truth}
\end{subfigure}
~ %\quad
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/vase_2_30_dls_view3.png}
  \caption{\textbf{DLS}}
\end{subfigure}
~ 
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/vase_2_30_glss_view3.png}
  \caption{SR}
\end{subfigure}
~ %\quad
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/vase_2_30_tv_view3.png}
  \caption{TV}
\end{subfigure}
~
\begin{subfigure}[b]{0.18\textwidth}
  \includegraphics[width=\textwidth]{figures/vase_2_30_sim2_view3.png}
  \caption{DCTLS}
\end{subfigure}
\caption{Surface reconstructions of the Vase dataset with SNR = 30 dB.}
\label{fig:vase}
\end{figure*}

\subsection{Dictionary Learning on Surfaces (DLS) Algorithm}
We propose to solve \eqref{eq:dl_surf} via a block coordinate descent-type algorithm where we alternate between updating $z$ with $(D,B)$ fixed and updating $(D,B)$ with $z$ fixed. Henceforward, we refer to this algorithm as the Dictionary Learning on Surfaces (DLS) method. 

\subsubsection{(D, B) updates}

Let $P$ be the matrix with columns $P_j z$. With $z$ fixed, the minimization of \eqref{eq:dl_surf} with respect to $(D,B)$ is
\begin{equation}  \label{dbupdate}
\begin{array}{rl}
\displaystyle\min_{B,D} & \left \| P - DB \right \|_F^2 +  \mu^2 \left \| B \right \|_0 \vspace{0.1cm} \\
\text{s.t.} & \left \| d_i \right \|_2 = 1, \ \ \left \| b_j \right \|_{\infty} \leq a, \ \forall i,j.
\end{array}
\end{equation}
We solve \eqref{dbupdate} via a block coordinate descent method where we iteratively minimize the cost with respect to the $i$th row of $B$ and the $i$th atom, $d_i$, of $D$ for every $1 \leq i \leq K$ with all other variables held fixed. A full derivation of this step can be found in \cite{sairajfes2,dinokat2016}, which we omit here due to space considerations.

\subsubsection{$z$ update}
With $D$ and $B$ fixed, our problem becomes
\begin{equation} \label{nupdate}
\min_{z} \ \frac{1}{2} \left \| A z - v \right \|_2^2 \\ +  \lambda \sum_{j = 1}^c \left \| P_j z - D b_j \right \|_2^2.
\end{equation}
The cost in \eqref{nupdate} can be written in the form $f(z)+g(z)$, where 
\begin{equation}
f(z) = \frac{1}{2} \left \| A z - v \right \|_2^2, \ \ \ g(z) = \lambda \sum_{j = 1}^c \left \| P_j z - D b_j \right \|_2^2.
\end{equation}
%$f(z) = \frac{1}{2} \left \| A z - v \right \|_2^2$ and \\ $g(z) = \lambda \sum_{j = 1}^C \left \| P_j z - D b_j \right \|_2^2$.
We utilize a proximal gradient \cite{parboyd} strategy to solve \eqref{nupdate}, iteratively updating $z$ according to
\begin{equation} \label{nproxupdate}
z^{k+1} = \textbf{prox}_{\tau g}(z^{k} - \tau \nabla f(z^{k})),
\end{equation}
where
\begin{equation} \label{nprox}
\textbf{prox}_{\tau g} (y) := \argmin_{x} \ \frac{1}{2} \left \| y - x \right \|_2^2 +  \tau g(x).
\end{equation}
Defining $\tilde{z}^{k} := z^{k} - \tau \nabla f(z^{k})$, we see that \eqref{nproxupdate} and \eqref{nprox} imply that $z^{k+1}$ satisfies the normal equation
\begin{equation}\label{znormal}
\bigg(I + 2 \tau \lambda \sum_{j=1}^c P_j^T P_j \bigg) z^{k+1} = \tilde{z}^{k} + 2 \tau \lambda \sum_{j=1}^c P_j^T D b_j.
\end{equation}
The matrix on the left hand side of \eqref{znormal} is diagonal, so its inverse can be cheaply computed to solve for $z^{k+1}$. Note that \eqref{nupdate} is a simple least squares problem and, as such, could be minimized by other methods (e.g., conjugate gradients).
