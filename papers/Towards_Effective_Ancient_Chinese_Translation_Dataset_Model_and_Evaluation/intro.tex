\section{Introduction}

%介绍研究任务（翻译）

%阐述研究现状、总结不足（没有结合文言文特点设计预训练）：文言文经常需要依靠人工去做，但是比较麻烦 依赖人力资源，费时费力，不便于传播和理解；现有文言文的翻译模型技术，比如guwen unilm和anchibert，缺少文言文中自身的一些特征

%解决思路：文言文具有哪些特点（双音词扩充+默写；生成能力+表征能力）

%展开idea

%总结贡献

% Ancient Chinese is a valuable cultural heritage in the World. However, when coming across the sentence ``\begin{CJK*}{UTF8}{gbsn}余虽好修姱以鞿羁兮，謇朝谇而夕替\end{CJK*}" in the famous Chinese poet Qu Yuan's ``Li Sao" (The Lament), many people will face a considerable challenge in understanding its meaning due to language differences caused by the time and the changable linguistic structure. Therefore, our work is aimed at utilizing the unique features of ancient Chinese to pre-train a machine translation model that accurately translates ancient Chinese into modern Chinese, contributing to the dissemination of Chinese culture.

Ancient Chinese literature is a long-cherished cultural legacy not only for Chinese people but for all of humanity. However, due to the evolution of the Chinese language over time, it can be challenging for modern readers to fully comprehend these works. In order to bring ancient Chinese literature back to modern life, we conduct our work towards effective ancient Chinese translation.
%bridge the gap between the public and ancient Chinese literature, we conduct our work on ancient Chinese machine translation.

Currently, the translation of ancient Chinese is predominantly carried out by professionals, but this process is time-consuming and labor-intensive, impeding the widespread dissemination and comprehension of the knowledge embedded in ancient Chinese.
With the help of deep learning, recent work has focused on using the pre-training strategy in machine translation for ancient Chinese~\cite{DBLP:journals/corr/abs-2009-11473, DBLP:conf/nlpcc/YangCC21, DBLP:conf/acl-lchange/ChangSYD21}. However, these works merely follow the paradigm of English-centered pre-training, ignoring the characteristics of ancient Chinese.

In this work, we present \textbf{Erya} targeting effective ancient Chinese translation.
We first collect and clean ancient Chinese corpus to construct the \textit{Erya dataset} including both monolingual ancient data and ancient-modern parallel data, and further classify them according to textual and chronological characteristics. With the dataset and classification criteria, we propose \textit{Erya benchmark} for comprehensive ancient Chinese translation evaluation.
%which currently stands as the most extensive ancient Chinese resource.  


Considering that recent large language models (LLMs)~\cite{llm_survey} can be improved by supervised fine-tuning~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, we utilize the parallel data from Erya dataset to train our model. 
In order to incorporate the characteristics of ancient Chinese, 
%we make an analogy between our model and the process of students learning ancient Chinese, thus guiding the design of our model \textbf{``Erya''}.
we devise two training strategies for Erya model in analogy to the process of ancient Chinese learning. 
% Our approach can enhance the generation and representation ability of the model simultaneously.
Firstly, we propose \textit{disyllabic aligned substitution (DAS)} to narrow the representation gap between aligned ancient-modern word pairs.
% to  ancient and modern Chinese, , similar to a translation task but involves the random substitution of aligned ancient-modern word pairs. %, utilizing the linguistic similarity between them
In addition, we design \textit{dual masked language model (DMLM)} with a bidirectional decoder to optimize both ancient and modern representations. 
% the source and target sentences masked by a dynamic dual masking strategy as its input and then performs mask predicting. 
%This process can be interpreted as cloze tests with given prompts.
%Finally, we combine these two targets together to enhance generation and representation ability of the model simultaneously, and ultimately achieve a beneficial translation performance.



%%%%%

%%%%%
% We conduct extensive experiments on Erya benchmark, and the results suggest the effectiveness of our proposed Erya strategy.





The contributions of our work are listed as follows:  
\begin{enumerate}
\item We build \textbf{Erya dataset}, the largest ancient Chinese dataset to the best of our knowledge, and design \textbf{Erya benchmark} for comprehensive evaluation.
\item We propose the \textbf{Erya model}, which is specifically designed for ancient Chinese translation and leverage the features of ancient Chinese.
\item We evaluate the performance of some large language models and commercial translation services on Erya benchmark, and the results validate the superiority of Erya model on both zero-shot and fine-tuning settings.
\end{enumerate}
%and compare their results with the performance of both zero-shot and fine-tuned Erya. The results show that Erya model performs better than almost all large language models and commercial translation service.



% Since the appearance of neural machine translation, machine translation techniques between various languages have gradually matured, represented by seq2seq\cite{sutskever2014sequence} framework and Transformer\cite{vaswani2017attention} using self-attention mechanism. Nowadays, with the advent of a new wave of  pre-trained language models, the understanding and generation capabilities of language models trained on large-scale data sets have been continuously improved and strengthened. The pre-trained encoders including BERT\cite{DBLP:conf/naacl/DevlinCLT19}, UniLM\cite{DBLP:conf/nips/00040WWLWGZH19} are able to extract context-dependent text representations, which lay the foundation for their application to natural language processing tasks including translation\cite{DBLP:conf/iclr/ZhuXWHQZLL20}.

%Ancient Chinese is an extensive and profound language with a changeable linguistic structure that brings great challenges to translation. However, there are few specialized studies and pre-trained models on translation from ancient to modern Chinese. So it becomes necessary to develop a specific pre-trained model in the context of ancient Chinese.

%Considering that recent large language models (LLMs)~\cite{llm_survey} can be improved by supervised fine-tuning, we utilize labeled xxx translation pairs (parallel data) to pre-train xxx.


%To enable pretraining at a large scale, we created an open source dataset \textbf{ERYA}  of ancient Chinese monolingual corpus and ancient-modern Chinese parallel corpus.


% to be continued 写contribution 总结 贡献 实验结论
%1. 提出了尔雅的与训练模型，相比cpt可以提升xx%
%2. 提出了目前最大的，单语数据集，效果怎么样，有多大
%3. 我们对于大模型进行了评测，chatgpt和百度翻译, 我们和最近的大语言模型，在现有的大模型上进行了综合和全面的评测，这些大模型在文言文翻译上的性能都不如erya，突出了我们的模型的重要性，可以写两句。

