\section{Erya Model}
\label{sec:model}
% We propose a pretraining framework “Jieyu” \added{specially tuned for ancient Chinese, } 
% Our model is trained on both monolingual and parallel corpus, and 
% in which we design two pretraining tasks: Disyllabic Random Aligned Substitution Pretraining (DRASP) and Dynamic Dual Masked Language Model (DDMLM). For DRASP, the decoder is pretrained in uni-directional mode, while for DDMLM is in bi-directional mode. The overall framework is illustrated in figure \ref{fig:example}. In this section, we first describe the CMLM task, the second subsection focus on our noising method on Monolingual data. Finally we introduce our aligned masking method

% Formally, Our training data has two main parts, monolingual dataset and parallel dataset. The monolingual dataset is denoted as $D_{mono}=\{d_1,d_2,d_3...,d_n\}$, while the parallel dataset consists of $m$ parallel data, denoted as $D_{para} = \{p_1,p_2...p_m\}$.In the description below, $p_i=(X_i,Y_i)$, where $X_i$ is the source text in the language pair $p_i$, and $Y_i$ is the corresponding target text. For monolingual corpora, we create pseudo bilingual text by copying the sentence, namely, $X=Y$.And $\theta$ denotes the parameters of our model.
% Each sentence $X_i$ can be written as $X_i = \{x_1,x_2....x_d\}$, in which each $x_l$ denotes a word and $d$ is the length of this sentence.

In this section, we propose our Erya model which is specially designed for ancient Chinese translation. Considering the recent success of supervised fine-tuning~\cite{DBLP:conf/nips/Ouyang0JAWMZASR22}, our Erya model is further fine-tuned on the existing CPT~\cite{DBLP:journals/corr/abs-2109-05729} model using Erya parallel data (\ie ancient-modern translation pairs) with two training tasks: \textbf{disyllabic aligned substitution (DAS)} and \textbf{dual masked language modeling (DMLM)}. %For DAS, the decoder is pretrained in uni-directional mode, while for DMLM is in bi-directional mode. 
The overall framework is illustrated in Figure~\ref{fig:example}. %In this section, %we first describe the DRASP task, then we introduce our DMLM task.

%Formally, our dataset consists of $m$ parallel data, denoted as $D$. In the description below, $p=(X,Y)$, where $X$ is the source text in the language pair $p$, and $Y$ is the corresponding target text. $\theta$ denotes the parameters of our model.

Formally, we denote parallel data as $\mathcal{D}$, with each translation pair as $(X, Y)$.

 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{PipelineAAA-cropped.pdf}
  \caption{\textbf{The illustrated training framework of Erya model.}}
  \label{fig:example}
\end{figure}


\subsection{Disyllabic Aligned Substitution}
In order to boost the linguistic similarity between ancient and modern Chinese, we devise a substitution approach to close the word representation gap between the two languages following previous work~\cite{DBLP:conf/emnlp/LinPWQFZL20}.

% \subsubsection{Disyllabic Word Alignment Function}
% While training on parallel corpus, we replace and mask according to the alignment information of the source sentence and the target sentence, and then reconstruct the mask.In this part,inspired by the two translation methods of expanding monosyllabic words into disyllabic words and keeping proper nouns untranslated in the translation of ancient Chinese, we design two masking methods of \textbf{bisyllabic word alignment mask} and \textbf{proper word alignment mask}.


\subsubsection{Training Objective:}
The disyllabic aligned substitution is a generation task: given a noised ancient sentence where the ancient-modern aligned words are randomly substituted by its modern translation, the model needs to generate the translation autoregressively using the following training objective:
\begin{equation}\label{eq:DAS}
    \mathcal{L}^{DAS} = - \sum_{(X, Y)\in \mathcal{D}} \log P (Y|\tilde{X}) = - \sum_{(X, Y)\in \mathcal{D}} \log P (y_1, \dots, y_i |\tilde{X}, y_{<i}),
\end{equation}
where $\tilde{X}=C(X, Y)$ is the disyllabic word alignment function.

% \begin{figure}[!h]
%   \centering
%   \vspace{-1em}
%   \includegraphics[width=0.85\textwidth]{align_example.pdf}
%   \vspace{-1em}
%   \caption{Disyllabic alignment example.}
%   \label{fig:align}
%   \vspace{-2em}
% \end{figure}

\subsubsection{Disyllabic Word Alignment:}\label{Disyllabic Align Strategy}
In ancient Chinese translation, disyllabic expansion~\cite{caomiao} is a widespread method, \ie a monosyllabic ancient character is often translated into a disyllabic word (\eg \begin{CJK*}{UTF8}{gbsn}解\end{CJK*} $\rightarrow$ \begin{CJK*}{UTF8}{gbsn}理解\end{CJK*} in Figure%~\ref{fig:align})
~\ref{fig:example}). We explore our parallel dataset and find that 99.8\% of the pairs contain such alignment, and an average of 62.5\% of characters in source sentences can be aligned.


We design an efficient and effective strategy called disyllabic word alignment in three steps:
(1) Source sentence $X$ is split at the character level and target sentence $Y$ is segmented by the THULAC toolkit\footnote{\url{http://thulac.thunlp.org/}}.
(2) A monosyllabic character of the source and a disyllabic word of the target are matched if they contain a common character, forming a set of alignment pairs. Then, we filter out the target words that occur in the source sentence, which may be proper nouns and do not need to be translated. 
(3) We replace the aligned characters in $X$ with its counterpart in $Y$ by probability $P_{DA}=0.7$ (experimented in Section~\ref{sec:pda}). 
 %to obtain sets of aligned words between both sides.
 
 %{We have collected modern Chinese dictionaries in this step. Since monosyllabic words in classical Chinese are often reserved and expanded into disyllabic words, we have designed a bisyllabic word alignment method as follows.}


% Finally The replaced $\tilde{X} = C(X, Y)$ is utilized by pre-training methods.

 % which is significantly higher than the reported rate of CeMAT~\cite{DBLP:conf/acl/LiLZWL22} by dictionary, which is $6\%$.


\subsection{Dual Masked Language Modeling}
\label{sec:dmlm}
As suggested by previous work~\cite{DBLP:conf/acl/LiLZWL22}, applying masked language modeling for both the encoder and decoder can enhance the model's representational ability and further improve translation performance. With such insight, we train our model using dual masked language modeling (DMLM). 
% (When initializing the ancient encoder-decoder architecture, we mainly used CPT-encoder and decoder.)

We randomly mask tokens at both sides independently with a dynamic probability of $[0.1, 0.2]$ and $[0.2, 0.5]$ for ancient and modern sentences, respectively. Each token has an 80\% probability of being masked, a 10\% probability of being replaced with a random token, and a 10\% probability of being unchanged following BERT~\cite{DBLP:conf/naacl/DevlinCLT19}. As for the pair $(X, Y)$, we denote the masked-token subset as $(X^{mask}, Y^{mask})$. Hence, the DMLM objective can be formulated as:

\begin{equation}\label{eq:dmlm}
\begin{aligned}
\small
\mathcal{L}^{DMLM} & = &\lambda& \mathcal{L}^{enc} &+& (1-\lambda) \mathcal{L}^{dec} \\
& = - &\lambda& \sum_{x_i\in X^{mask}} \log{P(x_i | \hat{X})} &-& (1-\lambda) \sum_{y_i \in Y^{mask}} \log{P(y_i | \hat{X}, \hat{Y})},
\end{aligned}
\end{equation}
where $\hat{X}$ and $\hat{Y}$ denote the unmasked tokens in $X$ and $Y$, respectively. We set the coefficient $\lambda=0.3$ following~\cite{DBLP:conf/acl/LiLZWL22}. Note that the decoder is bi-directional and it predicts the masked tokens non-autoregressively.

% X_{\backslash  X^{mask}}
% To pre-train with DMLM for the pair $(X, Y)$, tokens in $X$ and $Y$ are both randomly masked by dynamic dual masking to obtain $(\hat{X}, \hat{Y})$. Then, both encoder and bi-directional decoder are to perform mask-predicting task. Denoting $X^{mask}, Y^{mask}$ as the masked-token subset, the tasks can be formulated as:

%  Encoder conducting masked language model (MLM): 
 
%     \begin{equation}
%             \mathcal{L}^{enc} = -  \sum_{x_i\in X^{mask}} \log{P(x_i | \hat{X})}
%     \end{equation}
   
% Bi-directional decoder conducting conditional MLM:
    
%     \begin{equation}
%             \mathcal{L}^{dec} = - \sum_{y_i \in Y^{mask}} \log{P(y_i | \hat{X}, \hat{Y})}
%     \end{equation}

%     The difference w.r.t. uni-directional decoder is that each $y_j\in \hat{Y}$ could attend to all $y_n\in \hat{Y}$, instead of $Y_{n<j}$.

% Finally, total loss of DMLM is 
    
%     \begin{equation}\label{eq:dmlm}
%             \mathcal{L}^{DMLM} = \lambda \mathcal{L}^{dec} + (1-\lambda) \mathcal{L}^{enc}
%     \end{equation}

% Where $\lambda=0.7$ is the weight of loss.
    
%\end{itemize}

%we should predict $y_i^{mask}$ given $X_i$ and $Y_i\backslash y^{mask}$.The probability of each $y_n^i\in y_n^{mask}$ is independently calculated:

%$$P(y_i^{mask}|X_i,Y_i\backslash y^{mask})$$

%Considering the fact that the masked words, which are denoted as $y_i^{mask}$ are independent,decoder end can not be limited to auto regression. Following the method of CeMAT~\cite{DBLP:conf/acl/LiLZWL22} we use a bidirectional decoder instead. When Masking, we design a masking strategy to enhance model training.

% Although bilingual sentence pairs can be directly used to train the model together with the conventional DMLM, it is challenging for sentence pairs created from monolingual corpora because of identical source and target sentences. Therefore, we introduce a masking strategy to enhance model training on monolingual corpora.

% \subsubsection{Dynamic Dual Masking:}
% %\subsubsection{Masking Strategy}

% %To further enhance the representational ability of model with rich alignment information, the input pair $(\hat{X}, \hat{Y})$ of DMLM is constructed by two consecutive steps: 
% To further enhance the representational ability of model with bi-directional decoder, the input pair $(\hat{X}, \hat{Y})$ of DMLM is constructed by DM in two steps: 

% %\begin{enumerate}
% %    \item Disyllabic-switching masking: Following the practice of \ref{Disyllabic Align Strategy}, $(X, Y)$ is transformed to $(\tilde{X}, Y)$ by $C(X, Y)$. Then, $\tilde{Y}$ is attained simply by masking out the aligned indexes.
% %    \item Dynamic dual masking: Random masks are added to both sides of input independently with dynamic masking rate of $p_{src}\in[0.1, 0.2], p_{tgt}\in[0.2, 0.5]$. Each chosen token is masked by a probablity of $0.8$, switched to a random token by $0.1$ and remains unchanged by $0.1$. After this process $(\tilde{X}, \tilde{Y})$ is transformed to $(\hat{X}, \hat{Y})$.
% %\end{enumerate}
% \begin{enumerate}
%     \item Random tokens are chosen at both sides of input independently with a dynamic probability of $p_{src}\in[0.1, 0.2], p_{tgt}\in[0.2, 0.5]$.
%     \item Each chosen token is masked by a probability of $0.8$, switched to a random token by $0.1$ and remains unchanged by $0.1$. 
% \end{enumerate}


\subsection{Erya Multi-task Training}
\label{sec:mul}
Combining the above objectives~\ref{eq:DAS} and~\ref{eq:dmlm}, the final training loss is:
\begin{equation} \label{eq:combine}
    \mathcal{L} = (1-\mu) \mathcal{L}^{DAS} + \mu \mathcal{L}^{DMLM},
\end{equation}
where $\mu$ is a weight to balance two objectives. In Section~\ref{sec:lambda}, we find $\mu=0.3$ can achieve a better translation performance. In order to mitigate the gap between training and inference, we include an additional epoch of translation training directly using parallel data.

% Note that in the zero-shot scenario, we include an additional epoch of translation training to acquaint the Erya model with the translation task.

% We set $\mu=0.3$ in order to accustom the model towards translation scenario. 

%In dynamic dual masking, for both encoder and decoder input mask, the same words are dropped, and the ratio of the mask is dynamically obtained between [0.3,0.4].The training object is formulate as this formula: 

%$$L = -\sum_{X^{'},Y^{'}}\lambda\sum_{y^j\in y^{mask}}log P(y^j|X^{'},Y^{'})+(1-\lambda)\sum_{x^j\in x^{mask}}$$

% In which, our input is $(X^{'},Y^{'})=(mask(X),mask(Y))$,where X and Y have the same text content.We use X and Y seperately to distinguish the input of encoder and decoder.
%In which, our input is $(X^{'},Y^{'})=(mask(X),mask(Y))$.We use X and Y seperately to distinguish the input of encoder and decoder.

% \begin{comment}

%\subsection{Transformation}

%For the original sentence in the corpus, we input it into the encoder with noise, and expect to denoise and output the reconstructed target sentence at the decoder. The decoder generates word by word in the way of auto regression.Our stategy consists of three steps.
%\begin{enumerate}
%    \item \textbf{Random Masking:} Masking the words in sentences randomly,the masked sentence is $X\backslash x^{mask}$
%    \item \textbf{Sentence Breaks Removing:} Considering the sentence break practice in the process of learning Chinese, we expect the model to learn to make sentence breaks in a way similar to MLM on the basis of understanding the structure, sentence structure, and content of the Chinese language. we remove all the sentence breaks and punctuation in the sentence.
%    \item \textbf{Disrupting:} Since there are a lot of inverted sentence patterns in classical Chinese, we expect the model to be able to understand the sentence patterns in classical Chinese, we use the method of disrupting words.
%\end{enumerate}
%When adding noise, we apply all the stategies on one sentence at the same time.



    



% \deleted{We first extract the same words in the original sentence and the target sentence, and form several binary groups.Then for each $y_{q_k}$ in the target sentence, we check whether the vocabulary $y_{q_k-1}y_{q_k}$ or $y_{q_k}y_{q_{k}+1}$ exists in the Chinese dictionary.If one of them exists,take $y_{q_k-1}y_{q_k}$ as an example, then we denote $(x_p_k,y_{q_k-1}y_{q_k})$ as an alignment;If both exist, we choose one randomly with probability $\frac{1}{2}$;If none of then exist, then we skip this word. In this way, we get the source sentence-target sentence alignment information containing noise, on this basis we can perform a masking method similar to mRASP.}




% \subsubsection{CMLM}

% \comment{If mono stage section were to be preserved, we could simply state that it is roughly the same as above except for alignment strategy.}

% \begin{itemize}

%     \item \textbf{Pretraining Task: } Lorem Ipsum

%     \item \textbf{Noising Strategy: } Lorem Ipsum
    
% \end{itemize}




