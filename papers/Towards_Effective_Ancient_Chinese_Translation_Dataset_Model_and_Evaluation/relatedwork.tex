\section{Related Work}
\subsection{Pre-training in Neural Machine Translation}
Leveraging large scale corpora, pre-training is an effective method to acquire general language representations and achieve superior performance on various downstream tasks. Current works such as GPT-3~\cite{DBLP:conf/nips/BrownMRSKDNSSAA20} and BERT~\cite{DBLP:conf/naacl/DevlinCLT19} are based on the Transformer architecture, leading to improvements in language understanding tasks. Moreover, numerous sequence-to-sequence pre-trained models (\eg BART~\cite{DBLP:conf/acl/LewisLGGMLSZ20} and T5~\cite{DBLP:journals/jmlr/RaffelSRLNMZLL20}) aim to correspond better with text generation tasks.

%太长了，需要删减，比如deltaLM，mT6
Furthermore, researchers transfer the idea of pre-training from monolingual to multilingual. For example, XLM~\cite{DBLP:conf/nips/ConneauL19}, mBART~\cite{DBLP:journals/tacl/LiuGGLEGLZ20}, and mT5~\cite{DBLP:conf/naacl/XueCRKASBR21} utilize multilingual corpora to learn representations of multiple languages for translation. In addition, several works~\cite{DBLP:conf/emnlp/ChiDMHSMHSW21,DBLP:journals/corr/abs-2106-13736,DBLP:conf/emnlp/LinPWQFZL20} pre-train models on language parallel pairs to narrow the semantic distance among the same word across different languages. CeMAT~\cite{DBLP:conf/acl/LiLZWL22} further proposes a new pre-training task, which combines both multilingual corpora and parallel data, to achieve a better translation performance.
%. For example, $\Delta$LM~\cite{DBLP:journals/corr/abs-2106-13736} and mT6~\cite{DBLP:conf/emnlp/ChiDMHSMHSW21} are models making use of translation data. Compared with $\Delta$LM, whose pre-training task is translation pair span corruption, mT6 is trained on two more pre-training tasks, known as machine translation and translation span corruption. Translation pair span corruption is to randomly mask several words after connecting the source sentence with its translation, while translation span corruption additionally limits the words masked are among the source sentences. What's more, mRASP~\cite{DBLP:conf/emnlp/LinPWQFZL20} proposed random aligned substitution, making use of the aligning information between source sentences and its translation, in order to narrow the semantic space among different languages. On top of that, CeMAT~\cite{DBLP:conf/acl/LiLZWL22} proposed a new pre-train task, employed both the original multiple corpus and the translation data, to achieve a better translation performance.


% LLM 

\subsection{Ancient Chinese Domain Tasks}
In the realm of ancient Chinese information processing, fundamental tasks consist of automated sentence segmentation, word segmentation, word representation, dataset construction, and ancient-modern translation.

%介绍一下有什么用，讲原因（文言文没有标点），有一些工作进行探索，such as，按照时间来排
%word representation可以删
% For sentence segmentation and word segmentation, some studies are based on deep learning framework like BiLSTM-CRF~\cite{cheng-etal-2020-integration}, BERT, and CNN. 
% For the word representation, current studies focus on the difference of translation performance between the representation of characters and words~\cite{DBLP:conf/ialp/ZhangYZ15}, how to identify new words in ancient literatures, and how to annotate and disambiguate word meanings for polysemous words~\cite{shu-etal-2021-gu}.
% For the datasets about ancient Chinese, there has been datasets focusing on polysemous words within a particular ancient literature~\cite{DBLP:conf/naacl/PanWOK22}, the level of difficulty to understand Chinese classical poetry, and gender studies within ancient books during a specific time period~\cite{DBLP:conf/lrec/ZininX20}.
% All these works inspire the further research on translation between ancient Chinese and modern Chinese.

% Relevant studies on sentence segmentation~\cite{cheng-etal-2020-integration} and word segmentation~\cite{TP-toolbox-web} are proposed to deal with the issue that ancient Chinese corpora lack punctuation. In addition, since the existence of polysemous words, some studies focus on ancient Chinese word representation~\cite{shu-etal-2021-gu,DBLP:conf/ialp/ZhangYZ15}. As for the construction of corresponding datasets, there are studies focusing on the alignment approach~\cite{DBLP:journals/talip/LiuYQL20}, polysemous words~\cite{DBLP:conf/naacl/PanWOK22}, poetry understanding~\cite{Leiliu}, and gender analysis~\cite{DBLP:conf/lrec/ZininX20}.

Relevant studies on sentence segmentation~\cite{cheng-etal-2020-integration} and word segmentation~\cite{TP-toolbox-web} are proposed to deal with the issue that ancient Chinese corpora lack punctuation. In addition, since the existence of polysemous words, some studies focus on ancient Chinese word representation~\cite{shu-etal-2021-gu}. As for the construction of corresponding datasets, there are studies focusing on the alignment approach~\cite{DBLP:journals/talip/LiuYQL20}, polysemous words~\cite{DBLP:conf/naacl/PanWOK22}, and poetry understanding~\cite{Leiliu}.


%guwen-unilm，semi-supervised的大写需要确认一下
For ancient-modern translation, AnchiBERT~\cite{DBLP:journals/corr/abs-2009-11473} proposed the first pre-trained language model in the ancient Chinese domain with the masked token prediction task, while Guwen-UNILM~\cite{DBLP:conf/nlpcc/YangCC21} applied a two-stage pre-training framework using ancient Chinese corpora and ancient-modern translation pairs. Besides, a semi-supervised translation model~\cite{DBLP:conf/acl-lchange/ChangSYD21} was developed to predict both the translation and its particular era, improving the performance with additional chronological context. However, these studies merely apply the general translation method without considering the unique characteristics of ancient Chinese.

%So we make an analogy between our model and the process of students learning classical Chinese, to design our pre-train tasks, in order to match the translation tasks better.
%这个可以用在intro或者methods


%LLM
