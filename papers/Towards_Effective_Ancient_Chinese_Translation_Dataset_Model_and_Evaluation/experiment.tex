\section{Experiment}
\subsection{Experimental Setup}
In our experiments, we utilize Erya benchmark for zero-shot and fine-tuning evaluation using automatic and human evaluations. 
%For Erya model training, we use all other parallel texts in Erya dataset.

%Since our training contains pre-training stage and fine-tuning stage, we divide Erya-dataset into two parts for each stage. As for the dataset used in fine-tuning stage, we classify it according to the classification system described in section 3.2. The more refined statistics for it are as follows:
%, in order to better evaluate the performance of our model across different text domains. 


\subsubsection{Baseline Methods}
In order to better evaluate Erya model, we choose several baselines for comparison: 
(1) CPT-base~\cite{DBLP:conf/naacl/DevlinCLT19} has demonstrated satisfactory results on Chinese tasks. 
(2) AnchiBERT~\cite{DBLP:journals/corr/abs-2009-11473} and Guwen-UNILM~\cite{DBLP:conf/nlpcc/YangCC21} are translation models designed for ancient Chinese.
(3) text-davinci-003, gpt-3.5-turbo\footnote{\url{https://platform.openai.com/docs/models/gpt-3-5}}, and ERNIE Bot (\begin{CJK*}{UTF8}{gbsn}文心一言\end{CJK*})\footnote{ \url{https://yiyan.baidu.com/}} are recent LLMs and can achieve excellent performance on various downstream tasks. text-davinci-003 and gpt-3.5-turbo are from the GPT-3.5 series and are mainly trained on English corpus, while ERNIE Bot specifically considers Chinese.
(4) Baidu Translate\footnote{\url{https://fanyi-api.baidu.com/product/11}} is a commercial translation service that supports ancient Chinese translation.


% As ChatGPT has shown good performance in many NLP fields during our research, we test  to generate the translations as well.



\subsubsection{Implementation Details}
(1) For CPT-base and Guwen-UNILM, we fine-tune them  following the recommended hyper-parameters from the original papers. % Particularly, we further fine-tune Guwen-UNILM on the dataset provided in the original paper and conduct zero-shot generation evaluation on the Erya benchmark.
(2) For AnchiBERT, we combine it with a randomly-initialized decoder and follow the further training practices and hyper-parameters in its original paper. 
(3) For text-davinci-003 and gpt-3.5-turbo, we leverage OpenAI API and prompt the system with the sentence ``\begin{CJK*}{UTF8}{gbsn}将这句话翻译为现代汉语：\end{CJK*}".
(4) For Baidu Translate, we just input the ancient Chinese and receive the translated output.
(5) For ERNIE Bot, we can only randomly select 100 sentences using manual input for human evaluation due to the lack of an API usage license. 
% In the mentioned experiments, the models including Baidu Translate, gpt-3.5-turbo, text-davinci-003, Ernie Bot and AnchiBERT are all based on zero-shot generation. On the other hand, CPT and Guwen-UNILM are fine-tuned models.


%提到follow CPT-base，去掉一些超参数；学习率不对
For our Erya model training, we apply the AdamW optimizer~\cite{DBLP:conf/iclr/LoshchilovH19} with a learning rate of 3e-5 and a batch size of 256. We call the model after multi-task training (Equation~\ref{eq:combine}) \textbf{Erya4FT} and the model with an additional translation training \textbf{Erya}.
Then, we utilize Erya for zero-shot translation and employ Erya4FT to fine-tune on specific datasets with a batch size of 64. We employ the text generation library TextBox~\cite{tang-etal-2022-textbox} for implementation.

% We train the model for 8 epochs before fine-tuning it for the translation task. 





\subsubsection{Evaluation Metrics}
% To better evaluate our translation results, in addition to BLEU, we also use BERTScore, which computes similarity between each token in the candidate sentence and reference sentence using contextual embeddings~\cite{DBLP:conf/iclr/ZhangKWWA20}. The higher scores the generated translation texts get on both evaluation metrics indicates that the translation quality is better.

We use BLEU~\cite{DBLP:conf/acl/PapineniRWZ02} and BERTScore~\cite{DBLP:conf/iclr/ZhangKWWA20} for automatic evaluation: (1) BLEU applies N-gram matching to measure the similarity between candidate output and reference translation. (2) BERTScore calculates the similarity between each token in candidate and reference sentences by utilizing contextual embeddings. % higher score on both evaluation metrics indicates better translation quality for generated texts.
% (3) Comet is also a neural framework for training multilingual machine translation evaluation models, which behaves well in correlation with human judgments~\cite{DBLP:conf/wmt/ReiSAZFGLCM22}.
% (2) BLEURT is a learned evaluation metric based on BERT that can model human judgments~\cite{DBLP:journals/corr/abs-2004-04696}  
% () COMET is also a neural framework for training multilingual machine translation evaluation models, which performs similarly to human evaluation~\cite{DBLP:conf/wmt/ReiSAZFGLCM22}.

% \subsection{Experiment Result}
% \begin{table}[h]\label{tab:main result}
%     \centering
%     \label{table3}
%     \caption{Translation performance comparison of different baselines on fine-tuning stage datasets. 
%     % BS stands for BERTScore and CT is short for COMET.
%     BS stands for BERTScore.
%     Bold and underlined fonts indicate the best and the second best score.}
%     \resizebox{1.0\columnwidth}{!}{
%     \small
%     \begin{tabular}{c|c|cc|cc|cc|cc|cc|cc}
%     \hline
%     \multirow{2}{*}{~}&\multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c|}{\textit{Book of Han}} &\multicolumn{2}{c|}{\textit{New Tang History}}&\multicolumn{2}{c|}{\textit{Ming History}}&\multicolumn{2}{c|}{\textit{Taiping Guangji}}&\multicolumn{2}{c|}{\textit{Xu Xiake's Travels}}&\multicolumn{2}{c}{\textbf{AVG}}\\
%     % \cline{2-17}
%     & &BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS\\\hline
%     \multirow{6}{*}{\textbf{Zero-Shot}}
%     &\textbf{AnchiBERT} &\underline{24.41}&81.1&\underline{30.85}&84.0&30.77&83.5&\underline{18.96}&78.2&19.28&80.5&24.85&81.5  \\
%     &\textbf{Guwen-UNILM} &22.31&\underline{81.3}&30.13&84.9&31.75&84.1&18.59&\underline{78.5}&20.75&81.2&24.71&82.0  \\
%     &\textbf{text-davinci-003} &15.17&74.9&20.58&77.2&20.66&76.7&14.85&75.2&18.43&79.1&17.94&76.6\\
%     &\textbf{gpt-3.5-turbo} &17.66&76.4&20.47&76.4&21.48&76.6&16.82&76.1&20.07&80.3&19.3&77.2\\
%     &\textbf{Baidu Translation} &24.26&79.7&29.07&82.4&\textbf{38.89}&\underline{84.5}&16.86&74.9&\textbf{43.53}&\textbf{86.8}&\textbf{30.52}&\underline{81.7}\\
%     &\textbf{Erya} &\textbf{27.60}&\textbf{82.8}&\textbf{33.10}&\textbf{85.2}&\underline{35.45}&\textbf{85.6}&\textbf{22.35}&\textbf{80.3}&\underline{31.32}&\underline{85.6}&\underline{29.96}&\textbf{83.9} \\
%     \hline
%     \multirow{2}{*}{\textbf{Fine-Tune}}
%     &\textbf{CPT} &\underline{32.79}&\underline{84.3}&\underline{41.33}&\underline{88.1}&\underline{43.66}&\underline{87.8}&\underline{25.75}&\underline{81.2}&41.33&88.1&\underline{36.97}&\underline{85.9} \\
%     &\textbf{Erya} &\textbf{34.92}&\textbf{85.2}&\textbf{42.38}&\textbf{88.5}&\textbf{44.28}&\textbf{88.0}&\textbf{27.05}&\textbf{81.7}&\underline{42.53}&\textbf{88.4}&\textbf{38.23}&\textbf{86.36} \\
%     \hline
    
%     \end{tabular}
%     }
%     \label{tab:my_label}
% \end{table}
\subsection{Experiment Results}
% \begin{table}[h]
%     \centering
%     \label{table3}
%     \caption{\textbf{Translation performance comparison of different baselines on fine-tuning stage datasets.
%     % BS stands for BERTScore and CT is short for COMET.
%     BS stands for BERTScore.
%     Bold and underlined fonts indicate the best and the second best score.}}
%     \label{tab:main result}
%     \resizebox{1.0\columnwidth}{!}{
%     \small
%     \begin{tabular}{c|c|cc|cc|cc|cc|cc|cc}
%     \hline
%     \multirow{2}{*}{~}&\multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c|}{\textit{Book of Han}} &\multicolumn{2}{c|}{\textit{New Tang History}}&\multicolumn{2}{c|}{\textit{Ming History}}&\multicolumn{2}{c|}{\textit{Taiping Guangji}}&\multicolumn{2}{c|}{\textit{Xu Xiake's Travels}}&\multicolumn{2}{c}{\textbf{AVG}}\\
%     % \cline{2-17}
%     &&BLEU&BESC&BLEU&BESC&BLEU&BESC&BLEU&BESC&BLEU&BESC&BLEU&BESC\\\hline
%     \multirow{6}{*}{\textbf{zero-shot}}
%     &\textbf{AnchiBERT} &\underline{24.4}&81.1&\underline{30.9}&84.0&30.8&83.5&\underline{19.0}&78.2&19.3&80.5&24.9&81.5  \\
%     &\textbf{Guwen-UNILM} &22.3&\underline{81.3}&30.1&\underline{84.9}&31.8&84.1&18.6&\underline{78.5}&20.8&81.2&24.7&82.0  \\
%     &\textbf{text-davinci-003} &15.2&74.9&20.6&77.2&20.7&76.7&14.9&75.2&18.4&79.1&18.0&76.6\\
%     &\textbf{gpt-3.5-turbo} &17.7&76.4&20.5&76.4&21.5&76.6&16.8&76.1&20.1&80.3&19.3&77.2\\
%     &\textbf{Baidu Translation} &24.3&79.7&29.1&82.4&\textbf{38.9}&\underline{84.5}&16.9&74.9&\textbf{43.5}&\textbf{86.8}&\textbf{30.5}&\underline{81.7}\\
%     &\textbf{Erya} &\textbf{27.6}&\textbf{82.8}&\textbf{33.1}&\textbf{85.2}&\underline{35.5}&\textbf{85.6}&\textbf{22.4}&\textbf{80.3}&\underline{31.3}&\underline{85.6}&\underline{30.0}&\textbf{84.0} \\
%     \hline
%     \multirow{3}{*}{\textbf{Fine-Tune}}
%     &\textbf{Guwen-UNILM}&28.7&83.0&38.9&87.1&36.6&85.0&22.2&79.8&35.2&86.2&32.3&84.2\\
%     &\textbf{CPT} &\underline{32.8}&\underline{84.3}&\underline{41.3}&\underline{88.1}&\underline{43.7}&\underline{87.8}&\underline{25.8}&\underline{81.2}&\underline{41.3}&\underline{88.1}&\underline{37.0}&\underline{85.9} \\
%     &\textbf{Erya} &\textbf{34.9}&\textbf{85.2}&\textbf{42.4}&\textbf{88.5}&\textbf{44.3}&\textbf{88.0}&\textbf{27.1}&\textbf{81.7}&\textbf{42.5}&\textbf{88.4}&\textbf{38.2}&\textbf{86.4} \\
%     \hline
%     \end{tabular}
%     }
%     \label{tab:my_label}
% \end{table}
\begin{table}[h]
    \centering
    \label{table3}
    \caption{\textbf{Translation performance comparison on Erya benchmark. 
    % Translation performance comparison of different baselines on fine-tuning stage datasets.
    % BS stands for BERTScore and CT is short for COMET.
    % BS stands for BERTScore.
    Bold and underlined fonts indicate the best and the second best score.}}
    \label{tab:main result}
    \resizebox{1.0\columnwidth}{!}{
    \small
    \begin{tabular}{c|c|cc|cc|cc|cc|cc|cc}
    \hline
    \multirow{2}{*}{\textbf{Settings}}&\multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c|}{\textit{\textbf{Book of Han}}} &\multicolumn{2}{c|}{\textit{\textbf{New Tang History}}}&\multicolumn{2}{c|}{\textit{\textbf{Ming History}}}&\multicolumn{2}{c|}{\textit{\textbf{Taiping Guangji}}}&\multicolumn{2}{c|}{\textit{\textbf{Xu Xiake's Travels}}}&\multicolumn{2}{c}{\textbf{Average}}\\
    % \cline{2-17}
    &&BLEU&BERTScore&BLEU&BERTScore&BLEU&BERTScore&BLEU&BERTScore&BLEU&BERTScore&BLEU&BERTScore\\\hline
    \multirow{6}{*}{\textbf{Zero-shot}}
    &\textbf{AnchiBERT} &\underline{24.4}&81.1&\underline{30.9}&84.0&30.8&83.5&\underline{19.0}&78.2&19.3&80.5&24.9&81.5  \\
    &\textbf{Guwen-UNILM} &22.3&\underline{81.3}&30.1&\underline{84.9}&31.8&84.1&18.6&\underline{78.5}&20.8&81.2&24.7&\underline{82.0}  \\
    &\textbf{text-davinci-003} &15.2&74.9&20.6&77.2&20.7&76.7&14.9&75.2&18.4&79.1&18.0&76.6\\
    &\textbf{gpt-3.5-turbo} &17.7&76.4&20.5&76.4&21.5&76.6&16.8&76.1&20.1&80.3&19.3&77.2\\
    &\textbf{Baidu Translate} &24.3&79.7&29.1&82.4&\textbf{38.9}&\underline{84.5}&16.9&74.9&\textbf{43.5}&\underline{86.8}&\underline{30.5}&81.7\\
    &\textbf{Erya} &\textbf{29.9}&\textbf{85.3}&\textbf{34.5}&\textbf{88.4}&\underline{37.1}&\textbf{88.0}&\textbf{24.1}&\textbf{81.6}&\underline{34.2}&\textbf{88.2}&\textbf{32.0}&\textbf{86.3} \\
    \hline
    \multirow{3}{*}{\textbf{Fine-Tuning}}
    &\textbf{Guwen-UNILM}&28.7&83.0&38.9&87.1&36.6&85.0&22.2&79.8&35.2&86.2&32.3&84.2\\
    &\textbf{CPT} &\underline{32.8}&\underline{84.3}&\underline{41.3}&\underline{88.1}&\underline{43.7}&\underline{87.8}&\underline{25.8}&\underline{81.2}&\underline{41.3}&\underline{88.1}&\underline{37.0}&\underline{85.9} \\
    &\textbf{Erya4FT} &\textbf{34.9}&\textbf{85.2}&\textbf{42.4}&\textbf{88.5}&\textbf{44.3}&\textbf{88.0}&\textbf{27.1}&\textbf{81.7}&\textbf{42.5}&\textbf{88.4}&\textbf{38.2}&\textbf{86.4} \\
    \hline
    \end{tabular}
    }
    \label{tab:my_label}
\end{table}
\subsubsection{Zero-shot Translation}
From the result in Table~\ref{tab:main result}, we can obverse that Erya can achieve superior ancient Chinese translation ability among all baselines with the best BLEU and BERTScore. Erya can outperform previous ancient Chinese models (+7 BLEU) and GPT series models (+12 BLEU) by a large margin. When compared with commercial Baidu Translate, we also achieve better BERTScore (+4.6) and BLEU (+1.5) scores. 

%Note that since Baidu Translate is specially designed for ancient Chinese translation, it may have been trained on the examples on our Erya benchmark while we exclude the benchmark data during the Erya training. 
Note that Baidu Translate may have been trained on the examples on our Erya benchmark, while we exclude the benchmark data during the Erya training. The BERTScore of Erya in the zero-shot can even reach the performance of Erya4FT fine-tuned on specific datasets.


% Compared with previously-proposed ancient Chinese models, Erya exhibits strong zero-shot ability by +5.2 BLEU and +2.3 BERTScore averagely. It also surpasses GPT-3.5 models by a even larger margin (over +10 BLEU), and is only second to Baidu Translation at 0.5 BLEU. It is noteworthy that the fine-tuning sets of baseline models' own might overlap with our test set, and we have ruled out such possibility in the training process of Erya. 


% In view of the rather small size of Erya (145M vs 175B of GPTs), we conclude that our model is more parameter-efficient and adaptive, thus providing a better option in the field of ancient Chinese translation. 

\subsubsection{Fine-tuned Translation}
To further evaluate the domain transfer capability of Erya, we fine-tune Erya4FT on specific datasets. 
After fine-tuning, Erya4FT obtains +6.2 BLEU gain than zero-shot and is +1.2 BLEU superior to the fine-tuned CPT on average. This demonstrates Erya's potential of accustoming and transferring to a specific domain and the effectiveness of our training task. % Moreover, Erya takes dominance on BERTScore over all five sets, revealing its representation ability from DMLM.

In summary, our model is rather small (145M) but effective compared with GPT series (175B). Large language models may struggle to effectively process specialized corpus content, such as ancient Chinese. This further emphasizes the importance of designing smaller models specifically tailored for ancient Chinese.
Hence, Erya is a more parameter-efficient and well-performing option for effective ancient Chinese translation on both zero-shot and fine-tuning scenario. 
%%It achieves eminent translation performance under the zero-shot setting,
%It achieves eminient zero-shot translation performance with the support of our training method and dataset, 
%and the performance can be enhanced by fine-tuning on specific domains.
%As is illustrated in Table \ref{tab:main result}, compared with CPT, Erya achieves the best result by large over all the 5 fine-tune datasets and 3 evaluation metrics except 
%%COMET of \textit{Ming History}, \textit{Xu Xiake's Travels} and BLEU of \textit{Xu Xiake's Travels}.
%a few measurements.
%%\begin{CJK*}{UTF8}{gbsn}明史, 徐霞客游记\end{CJK*} and BLEU of \begin{CJK*}{UTF8}{gbsn}徐霞客游记\end{CJK*}.


%Averagely, Erya acquires +1.26 BLEU, +0.46 BERTScore and +0.242 COMET against CPT, which shows its general capability regardless of the amount or textual characteristics of fine-tune dataset. The dominance of BERTScore demonstrates Erya's context-aware ability. More specifically, up to +2.13 BLEU \& +0.9 BERTScore is achieved on \textit{Book of Han}, and +4.35 COMET for \textit{Xu Xiake's Travels}, revealing the potential of ancient-Chinese oriented pre-training.


%Another notable result is that CPT also performs remarkably, with relatively many metrics being the best or second best. This implies large scale modern Chinese pre-training could benefit ancient Chinese translation, and subsequent Erya pre-training enhances it even further.

%\subsubsection{Comparison with Baselines}

%We compare Erya model with previous-proposed ancient Chinese models, OpenAI GPT models and Baidu translation. 
%Erya model outperforms AnchiBERT and Guwen-UNILM significantly by margins of +13.38 BLEU and +13.52 BLEU 
%% 这里需要全部计算并详细报告吗？
%along with other metrics. GPT models are not comparable even with the aforementioned two models, which shows its shortcomings in translation genuineness. Baidu outperforms Erya at \textit{Xu Xiake's Travels} on BLEU, but is still exceeded by Erya at +16.42 BLEU. 

% baseline table
% \begin{table}[htbp]\label{tab:main result}
%     \centering
%     \label{table3}
%     \caption{Translation performance comparison of different baselines on fine-tuning stage datasets. 
%     % BS stands for BERTScore and CT is short for COMET.
%     BS stands for BERTScore.
%     Bold and underlined fonts indicate the best and the second best score.}
%     \resizebox{1.0\columnwidth}{!}{
%     \small
%     \begin{tabular}{c|cc|cc|cc|cc|cc}
%     % \renewcommand\arraystretch{1.5}
%     % \toprule
%     \hline
%     %\multirow{2}{*}{\textbf{Model}}& \multicolumn{3}{c|}{\begin{CJK*}{UTF8}{gbsn}汉书\end{CJK*}} &\multicolumn{3}{c|}{\begin{CJK*}{UTF8}{gbsn}新唐书\end{CJK*}}&\multicolumn{3}{c|}{\begin{CJK*}{UTF8}{gbsn}明史\end{CJK*}}&\multicolumn{3}{c|}{\begin{CJK*}{UTF8}{gbsn}太平广记\end{CJK*}}&\multicolumn{3}{c}{\begin{CJK*}{UTF8}{gbsn}徐霞客游记\end{CJK*}}\\
%     \multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c|}{\textit{Book of Han}} &\multicolumn{2}{c|}{\textit{New Tang History}}&\multicolumn{2}{c|}{\textit{Ming History}}&\multicolumn{2}{c|}{\textit{Taiping}}&\multicolumn{2}{c}{\textit{Xu Xiake's Travels}}\\
%     % \cline{2-17}
%     &BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS&\\\hline
%     \textbf{AnchiBERT} &24.41&81.1&30.85&84.0&30.77&83.5&18.96&78.2&19.28&80.5  \\
%     \textbf{Guwen-UNILM} &22.31&81.3&30.13&84.9&31.75&84.1&18.59&78.5&20.75&81.2  \\
%     \textbf{CPT} &\underline{32.79}&\underline{84.3}&\underline{41.33}&\underline{88.1}&&\underline{43.66}&\underline{87.8}&\underline{25.75}&\underline{81.2}&76.36&41.33&88.1&\textbf{85.51} \\
%     \textbf{Erya} &\textbf{34.92}&\textbf{85.2}&\textbf{82.04}&\textbf{42.38}&\textbf{88.5}&\textbf{83.97}&\textbf{44.28}&\textbf{88.0}&\underline{83.57}&\textbf{27.05}&\textbf{81.7}&\textbf{80.71}&\underline{42.53}&\textbf{88.4}&\underline{81.40} \\
%     \hline

%     \textbf{text-davinci-003} &15.17&74.9&20.58&77.2&20.66&76.7&14.85&75.2&18.43&79.1\\
%     \textbf{gpt-3.5-turbo} &17.66&76.4&20.47&76.4&21.48&76.6&16.82&76.1&20.07&80.3\\
%     \textbf{Baidu Translation} 
%     &24.26&79.7&29.07&82.4&38.89&84.5&16.86&74.9&\textbf{43.53}&86.8\\
%     \textbf{Erya} & xxx \\
%     \hline
    
%     \end{tabular}
%     }
%     \label{tab:my_label}
% \end{table}
% 
% \begin{table}[h]\label{tab:main result}
%     \centering
%     \label{table3}
%     \caption{Translation performance comparison of different baselines on fine-tuning stage datasets. 
%     % BS stands for BERTScore and CT is short for COMET.
%     BS stands for BERTScore.
%     Bold and underlined fonts indicate the best and the second best score.}
%     \resizebox{1.0\columnwidth}{!}{
%     \small
%     \begin{tabular}{c|c|cc|cc|cc|cc|cc|cc}
%     \hline
%     \multirow{2}{*}{~}&\multirow{2}{*}{\textbf{Model}}& \multicolumn{2}{c|}{\textit{Book of Han}} &\multicolumn{2}{c|}{\textit{New Tang History}}&\multicolumn{2}{c|}{\textit{Ming History}}&\multicolumn{2}{c|}{\textit{Taiping Guangji}}&\multicolumn{2}{c|}{\textit{Xu Xiake' Travels}}&\multicolumn{2}{c}{\textbf{AVG}}\\
%     % \cline{2-17}
%     & &BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS&BLEU&BS\\\hline
%     \multirow{6}{*}{\textbf{Zero-Shot}}
%     &\textbf{AnchiBERT} &\underline{24.41}&81.1&\underline{30.85}&84.0&30.77&83.5&\underline{18.96}&78.2&19.28&80.5&24.85&81.5  \\
%     &\textbf{Guwen-UNILM} &22.31&\underline{81.3}&30.13&84.9&31.75&84.1&18.59&\underline{78.5}&20.75&81.2&24.71&82.0  \\
%     &\textbf{text-davinci-003} &15.17&74.9&20.58&77.2&20.66&76.7&14.85&75.2&18.43&79.1&17.94&76.6\\
%     &\textbf{gpt-3.5-turbo} &17.66&76.4&20.47&76.4&21.48&76.6&16.82&76.1&20.07&80.3&19.3&77.2\\
%     &\textbf{Baidu Translation} &24.26&79.7&29.07&82.4&\textbf{38.89}&\underline{84.5}&16.86&74.9&\textbf{43.53}&\textbf{86.8}&\textbf{30.52}&\underline{81.7}\\
%     &\textbf{Erya} &\textbf{27.60}&\textbf{82.8}&\textbf{33.10}&\textbf{85.2}&\underline{35.45}&\textbf{85.6}&\textbf{22.35}&\textbf{80.3}&\underline{31.32}&\underline{85.6}&\underline{29.96}&\textbf{83.9} \\
%     \hline
%     \multirow{2}{*}{\textbf{Fine-Tune}}
%     &\textbf{CPT} &\underline{32.79}&\underline{84.3}&\underline{41.33}&\underline{88.1}&\underline{43.66}&\underline{87.8}&\underline{25.75}&\underline{81.2}&41.33&88.1&\underline{36.97}&\underline{85.9} \\
%     &\textbf{Erya} &\textbf{34.92}&\textbf{85.2}&\textbf{42.38}&\textbf{88.5}&\textbf{44.28}&\textbf{88.0}&\textbf{27.05}&\textbf{81.7}&\underline{42.53}&\textbf{88.4}&\textbf{38.23}&\textbf{86.36} \\
%     \hline
    
%     \end{tabular}
%     }
%     \label{tab:my_label}
% \end{table}



\subsection{Further Analysis}
\subsubsection{Ablation Study}
%In this part, we want to construct ablation experiments to test the effectiveness of our proposed strategies. In contrast to the previous pre-training task, we have made several improvements according to the properties of this translation task. First, we propose DASP to narrow the representation gap between ancient Chinese and modern Chinese. Second, we utilize DMLM to train the whole model with a bidirectional decoder, and force the decoder to get information from decoder by the setting of mask ratio. Finally, we set the mask ratio dynamically. 

To analyze the effect of the proposed Erya multi-task training in Equation~\ref{eq:combine}, we design the following variants to compare the performance of zero-shot and fine-tuned translation: 

\begin{itemize}[leftmargin=*]
\itemsep0em 
\item[$\bullet$] w/o DAS: the variant removes DAS in the training stage.
\item[$\bullet$] w/o DMLM: the variant does not use DMLM when training.
\item[$\bullet$] w/o dynamic mask: we remove the dynamic mask mentioned in Section~\ref{sec:dmlm}, but with a fixed ratio of 0.15 and 0.35 for the encoder and decoder respectively.
\item[$\bullet$] w/o translation training: we remove the additional translation training after the DAS and DMLM training.

%set the ratio of mask dynamically, but directly set the mask ratio of encoder and decoder as 0.15 and 0.35.
\end{itemize}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=\linewidth]{abalation(3).pdf}
%     \caption{\textbf{Ablation analysis on Erya collection.}}
%     \label{fig:ablation}
% \end{figure}

% \begin{table}[htbp]
% \caption{\textbf{Ablation analysis on Erya benchmark using the BLEU metric.}}
% \label{tab:ablation}
% \centering
% \small
% \resizebox{0.8\columnwidth}{!}{
% \begin{tabular}{c|c|c|c|c|c|c|c}%四个c代表有四列且内容居中
% \toprule%第二道横线 
% &\textbf{Models}&\textbf{\textit{Book of Han}}&\textbf{\textit{New Tang History}}&\textbf{\textit{Ming History}}&\textbf{\textit{Xu Xiake's Travels}}&\textbf{\textit{Taiping Guangji}}&\textbf{Avg.}\\
% \midrule%第三道横线 
% \multirow{4}{*}{\textbf{Zero-shot}}&\textbf{Erya}&\textbf{29.9}&\textbf{34.5}&\textbf{37.1}&\textbf{34.2}&\textbf{24.1}&\textbf{32.0} \\
% &\textbf{w/o DMLM}&26.9&32.3&35.2&30.3&21.9&29.3 \\
% &\textbf{w/o dynamic mask}&26.6&32.0&35.2&30.1&22.2&29.2 \\
% &\textbf{w/o translation training}&27.6&33.1&35.5&31.3&22.4&30.0 \\
% \midrule%第三道横线 
% \multirow{4}{*}{\textbf{Fine-Tuning}}&\textbf{Erya4FT}&\textbf{34.9}&\textbf{42.4}&\textbf{44.3}&\textbf{42.5}&\textbf{27.1}&\textbf{38.2} \\
% &\textbf{w/o DAS}&34.4&41.7&44.0&42.0&26.5&37.7 \\
% &\textbf{w/o DMLM}&33.9&41.9&44.1&42.1&26.8&37.8 \\
% &\textbf{w/o dynamic mask}&34.4&42.2&43.9&42.2&26.9&37.9 \\
% \bottomrule%第四道横线
% \end{tabular}}
% \end{table}
\begin{table}[htbp]
\caption{\textbf{Ablation analysis on Erya benchmark using the BLEU metric.}}
\label{tab:ablation}
\centering
\small
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c}%四个c代表有四列且内容居中
\toprule%第二道横线 
&\textbf{Models}&\textbf{\textit{Book of Han}}&\textbf{\textit{New Tang History}}&\textbf{\textit{Ming History}}&\textbf{\textit{Xu Xiake's Travels}}&\textbf{\textit{Taiping Guangji}}&\textbf{Avg.}\\
\midrule%第三道横线 
\multirow{4}{*}{\textbf{Zero-shot}}&\textbf{Erya}&\textbf{29.9}&\textbf{34.5}&\textbf{37.1}&\textbf{34.2}&\textbf{24.1}&\textbf{32.0} \\
&\textbf{w/o DAS}&28.9&33.3&36.8&32.3&23.4&30.9 \\
&\textbf{w/o DMLM}&28.7&32.9&36.6&32.0&23.2&30.7 \\
&\textbf{w/o translation training}&27.6&33.1&35.5&31.3&22.4&30.0 \\
\midrule%第三道横线 
\multirow{4}{*}{\textbf{Fine-Tuning}}&\textbf{Erya4FT}&\textbf{34.9}&\textbf{42.4}&\textbf{44.3}&\textbf{42.5}&\textbf{27.1}&\textbf{38.2} \\
&\textbf{w/o DAS}&34.4&41.7&44.0&42.0&26.5&37.7 \\
&\textbf{w/o DMLM}&33.9&41.9&44.1&42.1&26.8&37.8 \\
&\textbf{w/o dynamic mask}&34.4&42.2&43.9&42.2&26.9&37.9 \\
\bottomrule%第四道横线
\end{tabular}}
\end{table}


From Table~\ref{tab:ablation}, we can see that: 
%(1) Both DAS and DMLM have a consistent positive effect over the model across five evaluation datasets, at +0.53 and +0.492 BLEU averagely, which shows the effectiveness of DAS and DMLM strategy.
(1) Both DAS and DMLM consistently have a positive effect across five evaluation datasets. For the zero-shot setting, Erya achieves +1.1 BLEU with DAS and +1.3 BLEU with DMLM. % The reason why w/o DAS is excluded in zero-shot group is DMLM alone cannot empower the model with zero-shot capability.
For the fine-tuning setting, Erya4FT achieves +0.5 BLEU with DAS and +0.4 BLEU with DMLM. 
%which shows the effectiveness of DAS and DMLM strategy.
%(2) For fine-tuning group, the significance of DAS is higher than that of DMLM, while adding DMLM further improves model's translation capability, indicating the superiority of joint training.
(2) Dynamic mask ratio plays a positive role in model performance. Enabling it results in +0.3 BLEU gain for the fine-tuning scenario. 
(3) The additional translation is beneficial in the zero-shot translation scenario, which can alleviate the gap between downstream translation and denoising training.
(4) The positive effect of each component is more significant in zero-shot than in fine-tuning, which implies our strategies are more vital for zero-shot translation. 

%\begin{enumerate}
%    \item Both DAS and DMLM have a positive effect over the model consistently across five evaluation datasets, at +0.53 and +0.492 BLEU averagely, which shows the effectiveness of DAS and DMLM strategy.
%    \item The significance of DAS is higher than that of DMLM, while adding DMLM further improves model's translation capability, indicating the superiority of joint training. 
%    \item Dynamic mask ratio plays a role in DMLM performance and the whole model. Enabling it results in +0.316 BLEU gain.
%\end{enumerate}


\subsubsection{Performance Comparison w.r.t. Substitution Ratio} \label{sec:pda}
%The strategy of DASP to align the words and make corresponding substitution enables to narrow the representation gap between two languages. 
The substitution ratio $P_{DA}$ in DAS affects the degree to close the representations of ancient and modern Chinese. 
%When this ratio equals to zero, DAS can be seen as the translation task. 
Here, we vary the ratio to study its influence on translation. We select three sets of the Erya benchmark, covering all the literary styles. From the results in Figure~\ref{fig:ana1}, we set $P_{DA}$ as 0.7 for better overall performance.

%To investigate this, we pre-train our model with a varying ratio of substitution and fine-tune it on the translation task. 

% \begin{table}[htbp]\label{tab:ana1}
% \caption{\textbf{Analysis of different substitution ratio on \textit{New Tang History} dataset}}
% \small
% \centering
% \begin{tabular}{c|ccccc}%四个c代表有四列且内容居中
% \toprule%第二道横线 
% \textbf{Ratio of Substitution}&0&0.3&0.5&\textbf{0.7}&1.0\\
% \midrule%第三道横线 
% \textbf{BLEU}&42.28&42.25&42.32&\textbf{42.51}&41.81 \\
% \midrule%第三道横线 
% \textbf{BERTScore}&88.3&88.4&88.4&\textbf{88.5}&88.4 \\
% % \midrule%第三道横线 
% % \textbf{Comet}&83.88&83.86&83.82&\textbf{83.97}&83.81 \\
% \bottomrule%第四道横线
% \end{tabular}
% \end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{zeroshot.pdf}
    \caption{\textbf{BLEU scores of different substitution ratios.}}
    \label{fig:ana1}
\end{figure}


\subsubsection{Performance Comparison w.r.t. the Weights of two Losses} \label{sec:lambda}
The weight $\mu$ in Equation~\ref{eq:combine} balances the model's performance toward representation and generation. Here we vary $\mu$ among: 0.3, 0.5, and a decreasing strategy from 0.5 to 0.
Table~\ref{tab:ana2} shows that with $\mu=0.3$  the model performs better almost in each dataset. We speculate that the two tasks can incorporate better by emphasizing more on generation capacity.




% \begin{itemize}
%     \item $L_{unif}=0.5\times L_{DMLM}+0.5\times L_{DAS}$
%     \item $L_{uni-dom}=0.3\times L_{DMLM}+0.7\times L_{DAS}$
%     \item $L_{uni-inc}$, which means $L_{stage1}=0.5\times L_{DMLM}+0.5\times L_{DAS}$ for the first two epochs, $L_{stage2}=0.3\times L_{DMLM}+0.7\times L_{DAS}$ for the next two epochs, and finally $L_{stage3}=1.0\times L_{DAS}$ for three epochs
% \end{itemize}

\begin{table}[htbp]
\caption{\textbf{BLEU scores with different $\mu$.}}
\label{tab:ana2}
\centering
\small
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c}%四个c代表有四列且内容居中
\toprule%第二道横线
\textbf{$\mu$}&\textbf{\textit{Book of Han}}&\textbf{\textit{New Tang History}}&\textbf{\textit{Ming History}}&\textbf{\textit{Xu Xiake's Travels}}&\textbf{\textit{Taiping Guangji}}&\textbf{Avg.}\\
\midrule%第三道横线 
\textbf{0.3}&\textbf{34.9}&42.4&\textbf{44.3}&\textbf{42.5}&\textbf{27.1}&\textbf{38.2}\\
\textbf{0.5}&34.2&\textbf{42.5}&43.8&42.0&26.8&37.9 \\
\textbf{0.5$\rightarrow$0}&\textbf{34.9}&42.4&44.1&42.1&26.7&38.0\\
\bottomrule%第四道横线
\end{tabular}
}
\end{table}


% \subsubsection{Performance Comparison w.r.t. Pseudo Translation Pairs}
% Note that during our process of data acquisition, we also collect the monolingual corpus. As for this monolingual corpus, we utilize Baidu Translation API to form fake translation pairs, which is mixed with the original translation pairs, used together in the pre-training stage.

% \begin{table}[htbp]\label{tab:ana2}
% \caption{\textbf{Analysis of Pseudo Translation Pairs on Erya collection}}
% \centering
% \small
% \begin{tabular}{c|ccccc}%四个c代表有四列且内容居中
% \toprule%第二道横线 
% &\textbf{\textit{Book of Han}}&\textbf{\textit{New Tang History}}&\textbf{\textit{Ming History}}&\textbf{\textit{Xu Xiake's Travels}}&\textbf{\textit{Taiping}}\\
% \midrule%第三道横线 
% \textbf{Erya}&33.61&40.56&43.85&41.78&26.39 \\
% \midrule%第三道横线 
% \textbf{Erya with Pseudo Pairs}&33.24&40.80&43.47&41.54&26.09 \\
% \bottomrule%第四道横线
% \end{tabular}
% \end{table}


\subsection{Human Evaluation}
In addition to automatic evaluation, we further carry out a human evaluation. We randomly select 20 ancient texts from five categories of Erya benchmark, and gather the translations produced by gpt-3.5-turbo, ERNIE Bot, Baidu Translate, CPT, Erya, and Erya4FT. These translations are randomized to facilitate impartial human assessment.

We invite three domain experts majored in Ancient Chinese Literature evaluators to assess the quality of generated texts based on three criteria: faithfulness (\begin{CJK*}{UTF8}{gbsn}信\end{CJK*}, degree of accuracy exhibited by the translated text concerning the ancient text), expressiveness (\begin{CJK*}{UTF8}{gbsn}达\end{CJK*}, degree of fluency and clarity of the text), and elegance (\begin{CJK*}{UTF8}{gbsn}雅\end{CJK*}, degree of appropriateness and elegance of the text). In addition, we design the overall criterion to evaluate how likely the generated text is produced by human.
We adopt a 5-point Likert scale as the scoring mechanism, in which 5-point means ``very satisfying'', and 1-point means ``very terrible''.

From the results shown in Table~\ref{tab:human}, we can see that our Erya model outperforms almost all the baselines by a significant margin under both zero-shot and fine-tuning settings. This demonstrates the effectiveness of our multi-task training approach and our high-quality datasets.
%This superiority can be attributed to our multi-task training approach, which combines DAS and DMLM. By doing so, our model is able to reduce the gap in representation between the two languages and simultaneously enhance its representation capability.


\begin{table}[htbp]
\caption{\textbf{Human evaluation on Erya benchmark.}}
\label{tab:human}
\centering
\small

\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{c|c|c|ccc}%四个c代表有四列且内容居中
\toprule%第二道横线 
\textbf{Settings}&\textbf{Models}&\textbf{Overall}&\textbf{Faithful}&\textbf{Expressive}&\textbf{Elegant}\\
\midrule%第三道横线 
\multirow{4}{*}{\textbf{Zero-shot}}&\textbf{Baidu}&2.83&3.23&2.80&2.75\\
&\textbf{gpt-3.5-turbo}&\textbf{3.55}&3.42&3.80&\textbf{3.75} \\
&\textbf{ERNIE Bot}&3.45&3.48&3.75&3.48 \\
&\textbf{Erya}&3.47&\textbf{4.22}&\textbf{3.81}&3.46\\
\midrule
\multirow{2}{*}{\textbf{Fine-tuning}}&\textbf{CPT}&3.72&3.93&3.84&3.54 \\
&\textbf{Erya4FT}&\textbf{3.80}&\textbf{4.01}&\textbf{3.92}&\textbf{3.61} \\
\midrule%第三道横线
~&\textbf{Gold}&4.23&4.27&4.40&4.05\\
\bottomrule%第四道横线
\end{tabular}
}
\end{table}

% \begin{table}[htbp]
% \caption{\textbf{Human evaluation on Erya collection}}
% \centering
% \small
% \resizebox{1.0\columnwidth}{!}{
% \begin{tabular}{c|cccccc|c}%四个c代表有四列且内容居中
% \toprule%第二道横线 
% \textbf{Models}&\textbf{Baidu}&\textbf{gpt-3.5-turbo}&\textbf{ERNIE Bot}&\textbf{CPT}&\textbf{Guwen-UNILM}&\textbf{Erya}&\textbf{Gold}\\
% \midrule%第三道横线 
% \textbf{Faithful}&0&0&0&0&0&0&0 \\
% \textbf{Expressive}&0&0&0&0&0&0&0 \\
% \textbf{Elegant}&0&0&0&0&0&0&0 \\
% \midrule%第三道横线 
% \textbf{Overall}&0&0&0&0&0&0&0 \\
% \bottomrule%第四道横线
% \end{tabular}
% }
% \end{table}




