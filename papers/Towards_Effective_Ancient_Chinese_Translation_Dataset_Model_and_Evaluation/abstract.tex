\begin{abstract}
Interpreting ancient Chinese has been the key to comprehending vast Chinese literature, tradition, and civilization. In this paper, we propose \textbf{Erya} for ancient Chinese translation. 
%composed of Erya dataset, pre-training model and evaluation benchmark. 
From a dataset perspective, we collect, clean, and classify ancient Chinese materials from various sources, forming the most extensive ancient Chinese resource to date. 
From a model perspective, we devise Erya training method oriented towards ancient Chinese. We design two jointly-working tasks: disyllabic aligned substitution (DAS) and dual masked language model (DMLM). 
From an evaluation perspective, we build a benchmark to judge ancient Chinese translation quality in different scenarios and evaluate the ancient Chinese translation capacities of various existing models.
%Pre-trained on Erya parallel corpus, 
Our model exhibits remarkable zero-shot performance across five
domains, with over +12.0 BLEU against GPT-3.5 models and better human evaluation results than ERNIE Bot. Subsequent fine-tuning further shows the superior transfer capability of Erya model with +6.2 BLEU gain. % It also outperforms fine-tuned CPT by +1.3 BLEU, validating the effectiveness of our training method. 
%We will release all the above-mentioned resources to facilitate the research on ancient Chinese.
We release all the above-mentioned resources at \href{https://github.com/RUCAIBox/Erya}{https://github.com/RUCAIBox/Erya}.

%\keywords{Neural machine translation \and Ancient Chinese resource \and Pre-training}
\end{abstract}