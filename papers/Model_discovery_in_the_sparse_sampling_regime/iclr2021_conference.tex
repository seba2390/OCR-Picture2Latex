
\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}

\title{Model discovery in the sparse sampling regime}
\iclrfinalcopy
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Gert-Jan Both, Georges Tod \& Remy Kusters  \\
Université de Paris, INSERM U1284 \\
Center for Research and Interdisciplinarity (CRI) \\
F-75006 Paris, France \\
\texttt{remy.kusters@cri-paris.org} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

To improve the physical understanding and the predictions of complex dynamic systems, such as ocean dynamics and weather predictions, it is of paramount interest to identify interpretable models from coarsely and off-grid sampled observations. In this work we investigate how deep learning can improve model discovery of partial differential equations when the spacing between sensors is large and the samples are not placed on a grid. We show how leveraging physics informed neural network interpolation and automatic differentiation, allow to better fit the data and its spatiotemporal derivatives,  compared to more classic spline interpolation and numerical differentiation techniques. As a result, deep learning based model discovery allows to recover the underlying equations, even when sensors are placed further apart than the data’s characteristic length scale and in the presence of high noise levels. We illustrate our claims on both synthetic and experimental data sets where combinations of physical processes such as (non)-linear advection, reaction and diffusion are correctly identified.

%Model discovery aims at finding the differential equation underlying a data set. When the spacing between sensors becomes larger than the data's characteristic length scale, classical model discovery methods fail, even in the absence of noise. We trace this error back to the spacing-dependent error of numerical differentiation. Contrarily, deep learning based model discovery techniques use automatic differentiation and we show here that they are able to discover the underlying equation when sensors are placed further apart than this characteristic length scale, even when the acquired data is highly noisy. The use of automatic differentiation also releases the constraint of sampling on a grid and we show that off-grid sampling increases performance even further. We corroborate our observations with two experiments: the cable equation in 1D and the advection-diffusion equation in 2D. On both, classical model discovery fails while neural network-based techniques consistently recover the correct equation. These findings solidify the case for using neural network based model discovery for sparsely sampled and noisy data.
\end{abstract}

\section{Introduction}
Mathematical models are central in modelling complex dynamical processes such as climate change, the spread of an epidemic or to design aircrafts. To derive such models, conservation laws, physical principles and phenomenological behaviors are key. However, some systems are too complex to model with a purely bottom up approach \cite{bolton2019applications,sanchez2020artificial}. When observational data is present, automated model discovery tools are becoming increasingly more useful to derive partial differential equations directly from the data. The classical method for data driven model discovery is to apply sparse regression on a set of pre-selected features, the so-called library. In the case of partial differential equations, this library is constructed from a set of (higher)-order spatial derivatives. Model discovery is thus effectively a two-step process: first construct the library, then apply sparse regression. Numerically differentiating the data to construct the library using finite differences is extremely sensitive to noise; in practice, usually splines are fitted first and then differentiated. Splines model the data as piece-wise polynomials, but this expansion breaks down when the spacing between two sensors is large. These methods, which we refer to as classical methods, thus fundamentally limit model discovery to densely sampled data sets: even when no noise is present, the error incurred by the numerical differentiation corrupts the library and renders the sparse regression algorithm useless. The limits of classical interpolation methods have long been known and are often cited as a reason to use neural networks instead. Automatic differentiation can then be used to calculate the derivatives \cite{baydin2017automatic}, resulting in much more accurate derivatives. Previous works \cite{long2018,both2019,both2020} have shown that using neural networks to create a surrogate of the data allows model discovery in noisy and small data sets. 

In this paper we systematically study how sample spacing influences model discovery and compare neural-network based interpolation with classical methods. Our focus is the influence of the differentiation method used to construct the (higher-order) derivatives and its impact on model discovery, in particular when the spacing between two sensors $\Delta x$ is larger than the underlying equations' characteristic length scale $l_c$. As NN-based model discovery method we use DeepMoD, which is able to combine NN-based interpolation with any sparse regression method \cite{both2019,both2020}. By using an identical sparse regression algorithm for both the classical method and DeepMoD, we can isolate the effect of interpolation on the library and the discovered equation. Our results show that NN-based interpolators, in contrast to classical methods, can recover the underlying equation when $\Delta x > l_c$. Furthermore, we show that NN-based interpolation can succeed even when $\Delta x \gg l_c$ by either randomly sampling or displacing the sampling grid over time. We corroborate our findings with experimental data sets of the 2D advection-diffusion equation and the 1D cable equation. In both cases, DeepMoD, discovers the underlying equation in this sparsely sampled regime, contrarily to classical methods. Our findings solidify the case for deep learning methods by showing that they succeed in a regime where classical methods fundamentally fail.

\section{Related works}

\paragraph{Sensor placement} 
There exists a vast literature on determining optimal sensor placement for control theory or signal reconstruction based on a library of features, emphasizing the importance of sampling in the limit of sparse data \cite{brunton2013,manohar2018,wang2019}. While many of these sampling strategies have been developed to either reconstruct multi-scale data-sets \cite{champion2019}, flow-fields \cite{brunton2015,loiseau2017sparse} or other physical properties of a system \cite{schaeffer2018extracting}, research on the exact role of spatial and temporal sensor density or distribution for model discovery has received limited attention. 

\paragraph{Sparse regression-based model discovery} Using sparse regression to discover differential equations was popularized by algorithms such as SINDY \cite{brunton_discovering_2016} and PDE-find \cite{rudy_data-driven_2017} and has received considerable interest for both ODEs \cite{mangan2017,messenger2020} as well as for PDEs \cite{rudy2017,long2018, vaddireddy2020}. These approaches have since been expanded to automated hyper-parameter tuning \cite{champion_data-driven_2019, maddu_stability_2019}; a Bayesian approach for model discovery using Sparse Bayesian Learning \cite{yuan_machine_2019}, model discovery for parametric differential equations \cite{rudy_deep_2019}.

\paragraph{Deep learning-based model discovery} With the advent of Physics Informed Neural Networks \cite{raissi_physics_2017, raissi_physics_2017-1}, a neural network has become one of the prime approaches to create a surrogate of the data and perform sparse regression either on the networks prediction \cite{schaeffer_learning_2017, berg_data-driven_2019} or within the loss function of the neural network \cite{both2019,both2020}. Alternatively, Neural ODEs were also used to discover the unknown governing equation \cite{rackauckas_universal_2020} from physical data-sets. Different optimisation strategy based on the method of alternating direction is considered in \cite{chen_deep_2020}, and graph based approaches have been developed recently \cite{seo_differentiable_2019, sanchez-gonzalez_graph_2018}. Finally, \cite{cranmer_lagrangian_2020, greydanus_hamiltonian_2019} directly encode symmetries in neural networks using respectively the Hamiltonian and Lagrangian framework.


\section{Methods}

\paragraph{Sparse regression}

A popular approach to discover a PDE from a spatio-temporal data set is to apply sparse regression on a library of candidate terms $\Theta$, e.g. solve, 
\begin{equation}
    u_t = f(1,u, u_x, ...) =   \Theta \cdot \xi,
\end{equation} 
to obtain the coefficient vector $\xi$. Here $u_t$ is the temporal derivative and each column in $\Theta$ is a candidate term for the underlying equation, typically a combination of polynomial and spatial derivative functions (e.g. $u$, $u_x$, $uu_x$). To promote the sparsity of this solution an $l_1$ regularization is added to the problem, leading to the so-called Lasso regression:

\begin{equation}
    \xi^* = \min_{\xi} \left \lVert u_t - \Theta \cdot \xi\right\rVert^2 + \lambda \sum_i |\xi_i|.
\label{eq:reg}
\end{equation}

Here $\lambda$ controls the strength of the regularization, and hence the resulting level of sparsity. In this paper we use the Lasso as a sparse regression algorithm, with $\lambda$ determined by 5-fold cross-validation. The resulting coefficients are normalized by the $l_2$ norm of the feature vector, $\hat{\xi}_i = \xi_i\cdot ||\Theta_i||_2 / ||u_t||_2$ and thresholded. The exact value of the threshold can significantly influence the resulting equation. We use exactly the same Lasso and threshold for both DeepMoD and the classical methods so as to eliminate the influence of the exact variable selection algorithm used. Our goal is to compare how the feature library $\Theta$ and temporal derivative $u_t$ as generated by either DeepMoD or a classical method differ, and its resulting effect on model discovery.


\paragraph{Numerical differentiation}

The features of the library $\Theta$ consists of (higher-order) derivatives, which need to be differentiated from the observed data. Numerical differentiation is typically performed either by finite differences or by fitting a spline on the data and subsequently differentiating this spline. Finite difference methods directly operate on the observed data to calculate the derivative. In this paper, we use a standard second order accurate central difference scheme. Finite differences is computationally cheap and easy to scale to higher dimensions, but suffers from sensitivity to noise and requires samples to be closely spaced for accurate results; the truncation error of the scheme given above scales with the grid sampling, $h$, as $\mathcal{O}\left( h^2\right)$. In the sparse regime where $\Delta x \to l_c$, higher order schemes will not further improve this method. Furthermore, FD requires samples on the edges of the domain to be discarded, and for small data-sets and higher order schemes this can become a significant fraction of the total data.

A more accurate and widely used alternative is to fit a spline to the data and differentiate it. When fitting using splines, the data is approximated by a piece-wise polynomial with enforced continuity at the edges. Splines yield more accurate results in practice, but do not scale easily to higher dimensions, especially when using splines of higher order. This hinders model discovery, which requires these higher orders due to the derivatives in the feature library; by using a fifth-order spline to approximate the data, we effectively approximate the 3rd order derivative with only a second order polynomial, hence hindering its application to model discovery.

\paragraph{DeepMoD} \cite{both2019,both2020}\footnote{github.com/PhIMaL/DeePyMoD} is a neural network-based model discovery algorithm. It uses a neural network to learn both a noiseless surrogate of the data $\hat{u}$ and a coefficient vector $\xi$ by minimizing,

\begin{equation}
\mathcal{L} = \frac{1}{N}\sum_{i=1}^{N}\left( u_i - \hat{u}_i \right) ^2 +\frac{1}{N}\sum_{i=1}^{N}\left( (\hat{u}_t)_i - \Theta_{i}(\xi \cdot g)\right)^2.
\label{eq:deepmod}
\end{equation}

Here $\Theta$ and $\hat{u}_t$ are calculated by automatic differentiation of the neural network output $\hat{u}$. $g$ is a mask which sets the active terms, i.e. the terms that feature in the differential equation. The first term learns the data mapping $(x, t) \to \hat{u}$, while the second term constrains the network to solutions of the partial differential equation given by $\hat{u}_t, \Theta$ and $\xi \cdot g$. During training, the coefficients $\xi$ are determined by solving the least squares problem corresponding to the second part of eq. \ref{eq:deepmod}. The mask $g$ is updated separately by a sparse regression algorithm. The mask $g$ thus selects which terms are in the equation, while $\xi$ are the coefficients of these active terms. The value of the threshold can impact the discovered equation. To remove this factor from our comparison, we use exactly the same method to find the sparse coefficient vector $\xi^*$ for DeepMoD and the classical methods. More details on DeepMoD can be found in \cite{both2019,both2020}.


We emphasize two key differences with classical methods: 1) DeepMoD uses automatic differentiation to calculate $\Theta$, and the accuracy of the derivatives is thus not fundamentally linked to the sample distancing as with numerical differentiation. 2) By including the regression term within the loss function, we regularise the solution of the neural network $\hat{u}$, with the learned solution of the PDE. The result of fitting a spline is solely based on the data, whereas with DeepMoD it is also influenced by the constrained of the underlying equation (i.e. $\xi$ and $g$). We show in the next section that these two differences allow model discovery in extremely sparse and noisy data sets, whereas classical methods fail.


\section{Results}

\subsection{Synthetic data - Burgers equation} 

We consider a synthetic data set of the Burgers equation $u_t = \nu u_{xx} - u u_x$, with a delta peak initial condition $u(x, t=0) = A \delta(x)$ and domain $t \in [0.1, 1.1], x \in [-3, 4]$. This problem can be solved analytically (see Appendix A) to yield a solution dependent on a dimensionless coordinate $z = x / \sqrt{4 \nu t}$. We recognize the denominator as a \textit{time-dependent} length scale: a Burgers data set sampled with spacing $\Delta x$ thus has a time-dependent dimensionless spacing $\Delta z(t)$. We are interested in the smallest characteristic length scale, which for this data set is $l_c = \sqrt{4 \nu t_0}$, where $t_0=0.1$ is the initial time of the data set. We set $A=1$ and $\nu=0.25$, giving $l_c = \sqrt{0.1} \approx 0.3$.

Splines do not scale effectively beyond a single dimension, making it hard to fairly compare across both the spatial and temporal dimensions. We thus study the effect of spacing only along the spatial axis and minimize the effect of discretization along the temporal axis by densely sampling 100 frames, i.e. $\Delta t = 0.01$. Along the spatial axis we vary the number of samples between 4 and 40, equivalent to $0.5 < \frac{\Delta x}{l_c} < 5$. We study the relative error $\epsilon$ as the sum of the relative errors for all the derivatives, normalized over every frame, 

\begin{equation}
    \epsilon = \sum_{i=1}^{3} \left\langle\frac{\left\lVert\partial_{x}^{i} u_j - \partial_{x}^i \hat{u}_j\right\rVert_2}{\left\lVert \partial_{x}^{i} u_j \right\rVert_2}\right\rangle_j
    \label{eq:error}
\end{equation}

where $i$ sums the derivatives and $j$ runs over the frames. The derivatives are normalised every frame by the $l_2$ norm of the ground truth to ensure $\epsilon$ is independent of the magnitude of $u$. $\epsilon$ does not take into account the nature of noise (e.g. if it is correlated and non-gaussian), nor if the correct equation is discovered. However, taken together with a success metric (i.e if the right equation was discovered), it does serve as a useful proxy to the quality of the interpolation. 

Figure \ref{fig:burgers}b) shows $\epsilon$ as a function of the relative spacing $\Delta x / l_c$ and whether the correct equation was discovered. The error when using splines (yellow) increases with $\Delta x$ and, as expected, we are unable to discover the correct equation for $\Delta x > 0.8 l_c$ (dots indicate the correct equation is discovered and triangles indicates it failed to do so). Considering the NN-based DeepMoD method, sampled on a grid (green), we observe that it is able to accurately interpolate and discover the correct equation up to $\Delta x \approx 1.2 l_c$.  The reason for this is that NN-based interpolation constructs a surrogate of the data, informed by both the spatial and the temporal dynamics of the data set, while classical interpolation is intrinsically limited to a single time frame. 

In figure \ref{fig:burgers}c) we consider the same graph with $20\%$ white noise on the data. Despite smoothing, the spline is unable to construct an accurate library and fails to discover the correct equation in every case. DeepMoD stands in stark contrast, discovering the correct equation with comparable relative error as in the $0\%$ noise case. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{sampling_stategy.pdf}
    \caption{a) The three sampling strategies considered. b) and c) Error in the function library (Eq. \ref{eq:error}) as function of the distance between the senors $\Delta x$, normalized with $l_c = \sqrt{4 \nu t_0}$, for b) noise-less data and c) 20$\%$ of additive noise. The yellow symbols correspond to a spline interpolation and the green, blue and red correspond to the NN-based model discovery with various sampling strategies. The circles indicate that model discovery was successful while the triangles indicate that the incorrect model was discovered. The horizontal dashed line indicates the smallest characteristic length-scale in the problem: $\Delta x / l_c = 1$. }
    \label{fig:burgers}
\end{figure}

\paragraph{Off-grid sampling} Whereas higher-order splines are constrained to interpolating along a single dimension, DeepMoD uses a neural network to interpolate along both the spatial and temporal axis. This releases us from the constraint of on-grid sampling, and we exploit this by constructing an alternative sampling method. We observe that for a given number of samples $n$, DeepMoD is able to interpolate much more accurately if these samples are randomly drawn from the sampling domain. We show in figure \ref{fig:burgers}b and c (Red) that the relative error $\epsilon$ in the sparse regime, can be as much as two orders of magnitude lower compared to the grid-sampled results at the same number of samples. We hypothesize that this is due to the spatio-temporal interpolation of the network. By interpolating along both axes, each sample effectively covers its surrounding area, and by randomly sampling we cover more of the spatial sampling domain. Contrarily, sampling on a grid leaves large areas uncovered; we are effectively sampling at a much lower resolution than when using random sampling.

To test whether or not this improvement is intrinsically linked to the randomness of sampling, we also construct an alternative sampling method called shifted-grid sampling. Given a sampling grid with sensor distance $\Delta x$, shifted-grid sampling translates this grid a distance $\Delta a$ every frame, leading to an effective sample distance of $\Delta a \ll \Delta x$. This strategy, similarly as random sampling varies the sensor position over time, but does so in a deterministic and grid-based way. We show this sampling strategy in figure \ref{fig:burgers}a, while panels b and c confirm our hypothesis; shifted grid sampling (Blue) performs similarly to random sampling. Shifted-grid sampling relies on a densely sampled temporal axis 'compensating' for the sparsely sampled spatial axes. This makes off-grid sampling beneficial when either the time or space axis, but not both, can be sampled with a high resolution. In the experimental section we show that if both axes are sparsely sampled, we do not see a strong improvement over grid sampling. 

\subsection{Experimental data - 2D Advection-Diffusion}

In an electrophoresis experiment, a charged dye is pipetted in a gel over which a spatially uniform electric field is applied (see Figure \ref{figure:AD}a)). The dye passively diffuses in the gel and is advected by the applied electric field, giving rise to an advection-diffusion equation with advection in one direction: $u_t = D (u_{xx} + u_{yy}) + v u_y$. \cite{both2019} showed that \emph{DeepMoD} could discover the correct underlying equation from the full data-set (size 120 x 150 pixels and 25 frames). Here, we study how much we can sub-sample this data and still discover the advection-diffusion equation.

\begin{figure*}[t]
    \centering
    \includegraphics[width = 0.95 \textwidth]{AD_final}
    \caption{a) Experimental setup of the gel electrophoresis. b) Three time frames of the density  with a spatial resolution of 20x25. c) and d) Success diagram for the experimental data indicating correct model discovery (Yellow indicates the correct AD equation $u_t = D (u_{xx} + u_{yy}) + v u_y$ is discovered) as function of the spatial and temporal resolution for c) grid sampling and d) random sampling. e) Obtained mask and coefficients ($D = 0.025$ and and $v = (0, 0.2)$) for the artificial data-set as function of the noise level (11x11 spatial resolution). Hereby, yellow indicates the terms selected by the algorithm and the red dashed box the terms that are expected in the PDE. f) Success diagrams for various levels of additive noise, comparing the result of DeepMoD with a grid and random sampling strategy and the classical LassoCV algorithm on a Finite Difference (FD)-based library (after SVD filtering of the different frames).}
    \label{figure:AD}
\end{figure*}

In figure \ref{figure:AD} c) and d) we perform grid based as well as a random sub-sampling of the data. The neural network-based method discovers the advection-diffusion equation on as few as 6 x 8 spatial sampling points with 13 time-points, or with 20 x 25 pixels on only 3 time-points. The minimum number of required samples is similar for grid and random sampling, confirming that when both axes are poorly sampled, there is no benefit to sample randomly. 

The smallest characteristic length scale in the problem is the width of the dye at $t=t_0$, which we estimate as $l_c \approx 10$ pixels. For the data presented in figure \ref{figure:AD}c) and \ref{figure:AD}d), at a resolution below $10 \times 13$ sensors classical approaches would inherently fail, even if no noise was present in the data set. This is indeed what we observe: using a finite difference-based library we were unable to recover the advection-diffusion equation, even after denoising with SVD (See Appendix A for details).

%However, we do observe that below this value we occasionally discover the correct PDE, despite the experimental noise present in the data. 

The use of a neural network and random sampling lead to non-deterministic behaviour: the neural network training dynamics depend on its initialization and two randomly sampled datasets of similar size might not lead to similar results. In practice this leads to a 'soft' decision boundary, where a fraction of a set of runs with different initialization and datasets fail. We discuss and study this issue in appendix B.



\paragraph{Noiseless synthetic data set} 

To further confirm our results from the previous section, we simulate the experiment by solving the 2D advection-diffusion with a Gaussian initial condition and experimentally determined parameters ($D = 0.025$ and and $v = (0, 0.2)$. Figure \ref{figure:AD}e) shows the selected terms and their magnitude as functions of the applied noise levels for a highly subsampled data-set (grid sampling, spatial resolution of 11x11 and temporal resolution 14). The correct AD equation is recovered up to noise levels of 100$\%$ (See figure \ref{figure:AD}e), confirming the noise robustness of the NN-based model discovery. In panel f) we compare the deep-learning based model discovery using grid and random sampling with classical methods for various noise levels and sensor spacing with a fixed temporal resolution of 81 frames (Data for the FD was pre-processed with a SVD filter, see SI for further details). We confirm that, similarly to the Burgers example of the previous section, the correct underlying PDE is discovered even below the smallest characteristic length-scale in the problem (indicated by a red dashed line in figure \ref{figure:AD}f).

This figure confirms our three main conclusions: 1) In the noiseless limit, classical approaches are only slightly less performing than NN-based model discovery for grid sampling. 2) Increasing the noise level dramatically impacts classical model discovery while barely impacting NN-based model discovery and 3) random sampling over space considerably improves performance, allowing model discovery with roughly 4-8 times fewer sample points for this particular data-set (depending on the noise level). 

\subsection{Experimental data - Cable equation}
 Applying a constant voltage to a RC-circuit with longitudinal resistance (see figure \ref{figure:CE} a) result in time-dependent voltage increase throughout the circuit due to the charging of the capacitors. This rise is modeled by the cable equation, which is essentially a reaction-diffusion equation $ u_t =  u_{xx}/ (R_l C) + u/(R_m C)$ with $C$ the capacitance, $R_l$ the longitudinal resistance and $R_m$ the parallel resistance of the circuit. The discrete nature of the experiment automatically gives $\Delta x = O(l_c)$. We consider an extreme case where we only have seven sensors throughout the circuit (i.e. spatial axis), but take 2500 samples along the time axis. Figure \ref{figure:CE}b shows the measured voltage at these seven elements. Initially, all the capacitors are uncharged and we observe a sharp voltage increase at the first element. As the capacitors charge, this further propagates through the circuit, charging the capacitors and resulting in the curves shown in the figure. We apply both a classical approach with the library generated with splines and DeepMoD to a varying amount of elements. Figure \ref{figure:CE} c and d show that the DeepMoD discovers the cable equation with as few as seven elements, whereas classical methods are unable to find the cable equation at any number of elements. 

\begin{figure}
\centering
    \includegraphics[width = 0.95 \textwidth]{FigureCE}
    \caption{a) Schematic overview of the electronic setup to generate the cable equation. b) The voltage drop $u$ as function of time for various positions along the circuit for a circuit with 7 elements. The mask obtained for c) NN-based and d) cross validated Lasso with spline based library model discovery (Yellow indicates the term was recovered by the algorithm). The red boxes indicate the two expected terms in the equation.}
    \label{figure:CE}
\end{figure}

\section{Discussion and future work}
In this paper we showed how a deep learning approach allows to discover partial differential equations from coarsely and off-grid sampled observations in time and space. The correct equations are discovered, even when the sensor spacing is larger than some data set’s characteristic length scale- an inaccessible regime when using numerical differentiation procedures. We have also shown that the presence of noise quickly deteriorates the performance of classical methods, whereas the neural network based method is much less affected. However, in the limit of very sparse data, model discovery can be sensitive to the exact positioning of the sensors, hence sensitive to where exactly on the grid the samples are drawn. Future work could investigate the upper limit of the characteristic length scale above which the approach consistently starts failing and how it relates to the spectrum of the data. We will also focus in the future on including more structure in the interpolation for better convergence and initialization robustness, for example using Gaussian Processes.
%In this paper we showed that the neural network-based approach allows model discovery of extremely sparse data. It is able to discover the correct equation even when the sensor spacing is larger than the data sets characteristic length scale, an inaccessible regime when using numerical differentiation procedures. We have also shown that the presence of noise quickly deteriorates the performance of classical methods, whereas the neural network based method, DeepMoD, is minimally affected. Released from the constraint of sampling on a grid, we tested alternative sampling strategies such as random and shifted-grid sampling, which were shown to lead to much better performance. We thus uncovered a fundamental difference with classical methods: to achieve optimal performance with NN-based methods, data should be obtained by exploring as much space as possible over time.
%In the limit of very sparse data, NN-based model discovery can be sensitive to the exact positioning of the sensors, hence sensitive to the selected random set of samples or where exactly on the grid the samples are drawn. We explored this further in Appendix B. Now that we have established that non-grid based sampling has many advantages for NN-based function approximation, future work can integrate more complex sampling strategies. Moreover, follow up work will focus on including more structure in the function approximator, for example using Gaussian Processes, to obtain a more robust method. We suspect that this could also help with complex data sets where neural networks have trouble converging. Even though NN-based model discovery has proven to interpolate accurately with sample spacing superior to the characteristic length-scale, future work could investigate the upper limit of the samples distance before our approach consistently starts failing. In particular, an analysis of how the ratio of the sample spacing relative to the frequency spectrum of a given data set impacts the model discovery.  
 

\section*{Acknowledgments}
This work received support from the CRI Research Fellowship. We thank the Bettencourt Schueller Foundation long term partnership and NVidia for supplying the GPU under the Academic Grant program. We would also like to thank the authors and contributors of Numpy (\cite{harris_array_2020}), Scipy (\cite{virtanen_scipy_2020}), Scikit-learn (\cite{pedregosa2011scikit}), Matplotlib (\cite{hunter_matplotlib_2007}), Ipython (\cite{perez_ipython_2007}), and Pytorch (\cite{paszke_pytorch_2019}) for making our work possible through their open-source software. The authors declare no competing interest.

\bibliography{references}
\bibliographystyle{iclr2021_conference}

\appendix

\section{Reproducibility}

\subsection{Hyperparameters}

\paragraph{DeepMoD:} In this paper we use the neural network based model discovery tool DeepMoD \footnote{github.com/PhIMaL/DeePyMoD}. Every experiment uses a neural network with tanh-activation functions and 4 layers of 30 neurons with random initialization, and an Adam optimizer with default learning rate $10^{-3}$ and $\beta = (0.9,0.9)$. The sparsity scheduler has a patience of 500 epochs and a periodicity of 50 epochs \cite{both2020}.We use a cross validated, thresholded Lasso sparsity selection with a threshold of 0.2 and otherwise default parameters from the Sklearn implementation \cite{pedregosa2011scikit}

\paragraph{Spline interpolation:} For fitting the spline interpolation in both the Burgers as well as the Cable equation, we use a smoothing parameter of $s = 0.01$ in the case of noisy data and 5th order splines.

\paragraph{Finite difference and SVD filter:} To construct the function library of the 2D Advection diffusion equation we use a second-order accurate central difference scheme. For the 2D advection-diffusion data, the data was denoised using by decomposing it using the SVD and (\cite{harris_array_2020}) selecting the 3 largest modes from the signal.

\paragraph{Noise on synthetic data:} We add white noise to the data with a strength relative to the standard deviation of the data, i.e. $50\%$ noise corresponds to $0.5 \cdot \sigma$.

\subsection{Data preparation}

\paragraph{Burgers equation:} Using the Cole-Hopf transform, the Burgers equation described in the main text reduces to the heat equation and can be solved exactly for a delta peak initial condition to give, 

\begin{equation}
    u(x, t) = \sqrt{\frac{\nu}{t\pi}}\left(\frac{(e^{R}-1)e^{-z^2}}{1 + \frac{\left(e^{R}-1\right)}{2}\text{erfc}(z)}\right).
    \label{eq:burgers_dimless}
\end{equation}

where $R = A / 2 \nu$ and $z = x / \sqrt{4\nu t}$, a dimensionless coordinate. The characteristic length-scale is thus the smallest on in the system; for our case study $l_c = \sqrt{ 4 \nu t}|_{t=t_0}$. We use a function library containing all combinations of up to third order spatial derivative and second order polynomials in $u$, for a total of 12 terms, i.e.,
\begin{equation}
   \Theta = \left[1,u_x,u_{xx},u_{xxx}, u, uu_x, uu_{xx}, uu_{xxx},u^2,u^2u_x, u^2u_{xx}, u^2u_{xxx}\right].
\end{equation}

\paragraph{Cable equation:} We measured the passive voltage drop across a RC-circuit coupled to a longitudinal resistance (See Fig. \ref{figure:CE}A). This voltage drop across the circuit typically serves to model the passive voltage transmission through a dendrite, and is described by the so-called cable equation,
\begin{equation}
    u_t = \frac{1}{R_l C} u_{xx} - \frac{1}{R_m C} u.
\end{equation}
Here $C$ is the capacitance. $R_l$ the longitudinal resistance and $R_m$ the membrane resistance. This equation can be discretizised by an electric circuit, consisting of a serial set of $n$ longitudinal resistors, $r_i$, membrane resistors, $r_m$, and capacitors, $c_m$. Using Ohm’s ans Kirchhoff’s law, the discretized versioan of an array of these elements read, 
\begin{equation}
\frac{du_{i}}{dt}= \frac{(u_{i-1}+2 u_i + u_{i+1})}{c_{m}r_{l}} - \frac{u_i}{c_{m}r_{m}}.
\end{equation}

We use a breadboard using structures imitating GMMs, using only standard electronics hardware ($r_m = 10 k \Omega$, $r_c = 270 \Omega$ and $c_m = 680 mF$). We applied a voltage profile across the electronics structure using an arbitrary wave generator (AWG) (\textit{mhinstek MHS-2300A})  and used a dual channel oscilloscope (\textit{Voltcraft DSO-1062D}) to measure the voltage at various positions along the circuitry. These positions along the circuitry are the the spatial dimension of the cable equation. We varied the amount of elements between 5 and 13, mimicking various spatial discretizations. At every sensor, we collected 2500 data-points. We trained the model discovery algorithm on a function library up to first order polynomials and second order derivatives.

\subsection{2D Advection Diffusion}

\paragraph{Experiment:} We consider the 2D advection-diffusion process described by,
\begin{equation}
  u_t = -\nabla\cdot\left(-D\nabla u + \vec{v} u \right). 
\end{equation}

Here $\vec{v}$ is the velocity vector describing the advection and $D$ is the diffusion coefficient. We measure a time-series of images from an electrophoresis experiment, tracking the advection-diffusion of a charged purple loading dye under the influence of a spatially uniform electric field. We capture a set of 25 images with a resolution of 120x150 pixels and show the resultant 2D density field for three separate time-frames (in arbitrary units) in Fig. \ref{figure:AD}a, by subtracting the reference image (no dye present). The dye displays a diffusive and advective motion with constant velocity $\vec{v}$, which is related to the strength of the applied electric field. We use this data-set to asses the impact of temporal as well as spatial sensor density on the model discovery task.  We used a cross validated thresholded Lasso sparsity selection with a threshold of 0.05 and a function library containing all combinations of up to third order spatial derivative and second order polynomials in $u$, for a total of 10 terms, 
\begin{equation}
    {\Theta} = \left[1,u_x,u_y, u_{xx},u_{yy}, u_{xy}, u_{xxx}, u_{yyy}, u_{xxy}, u_{xyy}\right].
\end{equation}
%
%\begin{figure}
%    \centering
%    \includegraphics[width =\textwidth]{burgers_deriv.pdf}
%    \caption{Caption}
%    \label{fig:my_label}
%\end{figure}

\section{Sensitivity to the random sampling}

In this Appendix we discuss the sensitivity of the deep learning based approach, DeepMoD, w.r.t. the set of random samples selected, in particular is the limit $\Delta x /l_c > 1$. In order to show the impact of the random set of samples drawn we perform 10 runs of DeepMoD with otherwise identical parameters (1000 samples drawn and $10\%$ white noise and otherwise identical parameters as discussed in Appendix A). In Fig. \ref{figure:app}a we show the outcome for $\Delta x/l_c = 2 > 1$ indicated that in 7 of the 10 cases the correct equation is discovered and in 3 of the 10 cases this is not the case. In Fig. \ref{figure:app}b) we repeat this as function of $\Delta x/l_c$ and show that the larger the average distance between the samples becomes, the more pronounced the discrepancy between discovered models becomes. We have also tested the impact of the initialization of the neural network on the outcome, with identical set of samples and parameters, but this had little impact to the obtained PDE.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95 \textwidth]{appendix.pdf}
    \caption{a) Coefficients obtained for the Burgers equation with 10$\%$ white noise for 10 separate runs with 10 sets of randomly sampled data-sets. b) Fraction of correctly discovered equations over 10 runs (with 10$\%$ white noise and 1000 samples per run) as function of the average distance between the samples, $\Delta x$, relative the the smallest characteristic length-scale $l_c$.}
    \label{figure:app}
\end{figure*}
\end{document}
