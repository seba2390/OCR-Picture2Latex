\chapter{Introduction}
\label{chap:intro}

% What is computer vision
Computer vision is the task of inferring properties of the world from the
observed visual data. The observed visual data can originate from a variety of
sensors such as color or depth cameras, laser scans etc.
The properties of the world range from low-level material properties
such as reflectance to high-level object properties such as 3D shape and
pose. The field of computer vision encompasses a broad range of problems
involving a variety of sensor data and world properties. Some example
problems include: `Inferring 3D pose and shape of an object from a depth image';
`Inferring the actions of the persons from video' etc.
% Why is it hard?
Computer vision is hard because of variability in lighting, shape and texture in
the scene. Moreover, there is sensor noise and the image signal is non-additive
due to occlusion. Vision problems are inherently ambiguous as the sensor data
is an incomplete representation of the richer 3D world. As a result, probabilistic
frameworks are typically employed to deal with such ambiguity.

% Briefly about components in a vision system
Following~\cite{prince2012computer}, there are three main components in any vision
system: \textit{model}, \textit{learning} algorithm and \textit{inference} algorithm.
The \textit{Model} forms the core of any vision system describing the mathematical
relationship between the observed data and the desirable properties of the world.
The set of parameters and/or structure of this mathematical model
is learned using a \textit{learning} algorithm. Once the model
is learned, an \textit{inference} algorithm is used to predict the world properties
from a given observation.

Since models form the core of any vision system, let us briefly discuss the
two broad categories in computer vision models:
\textit{Generative} and \textit{Discriminative}
models, which can be viewed as complementary and inverse to each other.
Let us denote the observed data as a vector of random variables $\obs \in \mathbb{R}^k$ and
the target world properties as another vector of random variables $\target \in \mathbb{R}^l$.
For example, $\obs$ can be a vectorized representation of image pixels and
$\target$ can be a vector representing the parameterized shape of an object
in the image. Generative models characterize the probability of observed data
given the world properties $P(\obs|\target,\params)$ (called `likelihood')
as well as a prior on target variables $P(\target)$,
where $\params$ denotes the parameters of the model.
Some example generative models include graphics systems and probabilistic graphical models.
In this thesis, we use the
term `generative' more loosely in the sense that any model which characterizes
the likelihood $P(\obs|\target,\params)$ and/or prior over target
variables $P(\target)$ is considered a `generative' model.
Discriminative models characterize the probability of world properties
given the observed data $P(\target|\obs,\params)$ (called `posterior distribution').
In other words, generative models model the image formation process as a function
of world parameters, whereas discriminative approaches model the target world
parameters as a function of the given image.
Once the model is defined, a learning algorithm is used to learn the model
parameters $\theta$ and then an inference algorithm is used to predict the
posterior distribution $P(\target|\obs,\params)$. In Chapter~\ref{chap:models},
we will discuss more about these models along with the inference and learning in them.

Depending on the type of model, a specialized learning or inference algorithm
may not be required. For example, in the case of manually specified generative models
(e.g.\ graphics system or fully specified Bayesian network),
there is no need for a specialized learning
algorithm since all the model parameters are already hand specified,
but specialized inference techniques are required to invert such
models. In the case of discriminative models, where the
posterior distribution $P(\target|\obs,\params)$ is directly modeled, (e.g.\ neural
networks or random forests), the inference mechanism just reduces to a simple
evaluation of the model.

This dissertation focuses on improving the \textit{inference} in
prominent computer vision models. Inference plays a crucial role in any vision
system as this would produce the desired end result for a given observation.
The breakthroughs in computer vision technology are often marked by the advances
in the inference techniques as even the model design is often dictated by the
complexity of the inference in them.
The inference result is what matters at the end and
having a model with high fidelity but with no feasible or practical inference scheme
(for instance recent photo-realistic graphics systems) is of little use for
addressing vision problems.
Thus, better inference techniques not only improve the existing computer
vision systems but also help to develop better models. This thesis work
proposes techniques for better inference in existing and widely used computer
vision models.

\section{Thesis Overview}

In this section, we will discuss the objective of this thesis followed by the
organization and contributions of various chapters.

\subsection{Objective}

The main aim of the work presented in this thesis is to improve the performance
of inference algorithms used in different computer vision models. Inference is
highly inter-linked with model design and depending on the type of model, we
propose different techniques to improve inference.

Generative models characterize the image formation process and the inference is
typically performed via Bayesian inference techniques. Despite their
intuitive appeal, the use of generative models in vision is hampered by the
difficulty of posterior inference~(estimating $P(\target|\obs,\params)$).
Existing inference techniques are either often too
complex or too slow to be practical. In this thesis, we aim to alleviate some
of these inference challenges in generative models. Specifically, we concentrate
on improving two different inference schemes for generative vision models: First one is
Markov chain Monte Carlo (MCMC) inference for inverting graphics
engines and the second one is message passing inference in layered graphical models.
A common strategy that we follow to improce inference in generative models is
to learn a new discriminative model that is separate from the given generative
model and propose modified inference schemes that make use of this new discriminative
model for better inference.

Discriminative models directly model the posterior distribution $P(\target|\obs,\params)$
of the desired world parameters given the observed data. Thus the inference amounts
to simple evaluation of the model. One of the main limitations of inference
in discriminative models in the lack of principled techniques to incorporate
our prior knowledge about the task. This is especially the case with the prominent
convolutional neural network (CNN) models. In this thesis, we concentrate on CNN
models and propose techniques to improve inference in them. We modify the original
CNN models and make them more amenable for the incorporation of prior knowledge.

In summary, the aim of this thesis is to improve inference in general computer
vision models. We do this by leveraging machine learning techniques to learn
a new model for inference that is either separate from the original model (in case
of generative models) or modifying the original model itself (in case of
discriminative models). The work in this thesis deals
with the construction and learning of such inference models and how such models
can be integrated into the original vision models for better inference. We propose
techniques for inference in diverse computer vision models ranging from hand-specified
graphics systems to freely-parameterized neural network models. We concentrate on three
types of models which are prevalent in modern computer vision systems:
1.~Graphics systems; 2.~Layered graphical models and 3.~Convolutional neural networks.


\subsection{Organization and Contributions}

Since models form the core part of any vision system and this thesis involves the construction of
new models for inference, in Chapter~\ref{chap:models}, we give an overview of different
computer vision models along with the learning and inference mechanisms that are
usually employed in them. In addition, we review some existing techniques that aim to
improve inference in vision models by combining generative and discriminative approaches.

\vspace{-0.3cm}
\paragraph{Part I: Inference in Generative Vision Models} In Part I
(Chapters~\ref{chap:infsampler} and~\ref{chap:cmp})
of the thesis, we propose techniques for inference in generative computer vision models.

\vspace{-0.3cm}
\paragraph{Chapter~\ref{chap:infsampler} - The Informed Sampler}
In this Chapter, we propose a new sampling technique for inference
in complex generative models like graphics systems. Markov chain Monte Carlo (MCMC)
sampling is one of the most widely used and most generic inference scheme
in such complex generative models. Although generic, in practice,
MCMC sampling suffers from slow convergence unless the posterior is a unimodal low-dimensional
distribution. By leveraging discriminative learning techniques with ancillary clustering
and random forest models, we devise a mixture sampling technique that helps in faster mixing
without losing the acceptance rate. We call this `Informed Sampler' and demonstrate
it using challenging generative graphics models and a popular model of
human bodies~\cite{hirshberg2012coregistration}. Our method is similar in spirit
to ‘Data Driven Markov Chain Monte Carlo’ methods~\cite{zhu2000integrating}.

\vspace{-0.3cm}
\paragraph{Chapter~\ref{chap:cmp} - Consensus Message Passing}
In this Chapter, we concentrate on layered and loopy graphical models that are prevalent
in computer vision applications. When factors (relationship between the variables)
in such graphical models are from a pre-defined
family of distributions, inference is generally performed using
standard message passing techniques such as `expectation propagation'~\cite{Minka2001} and
`variational message passing'~\cite{Winn2005}. We observe that these inference techniques
fail to converge or even diverge when the graphical model is loopy with a large number of
variables. The failure of these inference techniques can be attributed to the algorithm's inability
to determine the values of a relatively small number of influential variables which we call
global variables (e.g.\ light in a scene). Without accurate estimation of these global
variables, it can be very difficult for message passing to make meaningful progress on the
other variables in the model. As a remedy, we exploit the layered structure of the model
and learn ancillary random forest models that learn to predict these influential variables
and use them for better message passing inference. We call this method `Consensus Message Passing' (CMP)
and demonstrate it on a variety of layered vision models. Experiments show that CMP
leads to significantly more accurate inference results whilst preserving the computational
efficiency of standard message passing.

\vspace{-0.3cm}
\paragraph{Part II: Inference in Discriminative Vision Models}
In Part II (Chapters~\ref{chap:bnn},~\ref{chap:vpn} and~\ref{chap:binception}) of the
thesis, we focus on inference in discriminative CNN models.

\vspace{-0.3cm}
\paragraph{Chapter~\ref{chap:bnn} - Learning Sparse High Dimensional Filters}
2D spatial convolutions form the basic unit of CNN models.
Spatial convolutions are perhaps the simplest, fastest and
most used way of propagating information across pixels.
Despite their staggering success in a
wide range of vision tasks, spatial convolutions have several drawbacks: There
are no well-established ways of incorporating prior knowledge into spatial filters;
Spatial convolutions quickly get intractable when filtering data of
increasing dimensionality; and the receptive fields of the filters are image-agnostic.
Spatial convolutions are usually confined to a local neighborhood
of pixels and thus many deep layers of spatial convolutions or post-processing conditional
random field (CRF) formulations are required for long-range propagation of
information across pixels. Bilateral filtering~\cite{aurich1995non, tomasi1998bilateral},
on the other hand, provides a simple yet powerful
framework for long range information propagation across pixels. But the traditional use of
bilateral filtering is confined to a manually chosen parametric from, usually a Gaussian filter.
In this chapter, we generalize the bilateral filter parameterization using a sparse high-dimensional
linear approximation and derive a gradient descent algorithm, so the filter parameters can be learned from data.
We demonstrate the use of learned bilateral filters in several diverse applications where Gaussian bilateral
filters are traditionally employed: color up-sampling, depth up-sampling~\cite{kopf2007joint}
and 3D mesh denoising~\cite{fleishman2003bilateral}. The ability to learn
generic high-dimensional sparse filters allows us to stack several parallel and sequential filters like in
convolutional neural networks (CNN) resulting in a generalization of 2D CNNs which we call `Bilateral Neural
Networks' (BNN). We demonstrate the use of BNNs using an illustrative segmentation problem and sparse character
recognition. Gaussian bilateral filters are also employed for mean-field inference in fully connected
conditional random fields (DenseCRF)~\cite{krahenbuhl2012efficient}. Existing works on DenseCRFs
are confined to using Gaussian pairwise potentials due to the traditional use of Gaussian kernels
in bilateral filtering. By learning bilateral filters, we remove the need of confining
to Gaussian pairwise potentials which has the added advantage of directly learning the pairwise potentials
for a given task. We showcase the use of learning edge potentials in DenseCRF with experiments
on semantic segmentation and material segmentation. In summary, we propose a general technique
for learning sparse high-dimensional filters that help in improving the model and inference in DenseCRF
models and also generalizes 2D CNNs.

\vspace{-0.3cm}
\paragraph{Chapter~\ref{chap:vpn} - Video Propagation Networks}
Videos carry redundant information across frames and the information propagation across video
frames is valuable for many computer vision applications such as video segmentation, color
propagation etc. In this chapter, we propose a novel neural network architecture for video
information propagation. We leverage learnable bilateral filters, developed in the previous
chapter, and propose a `Video Propagation Network' (VPN) that processes video frames in an
adaptive manner. The model is applied online: it propagates information forward without
the need to access future frames other than the current ones. In particular we combine
two components, a temporal bilateral network for dense and video adaptive filtering,
followed by a spatial network to refine features and increased flexibility.
We present experiments on video object segmentation and semantic video segmentation
and show increased performance comparing to the best previous task-specific methods,
while having favorable runtime. Additionally we demonstrate our approach on an example
regression task of propagating color in a grayscale video.

\vspace{-0.3cm}
\paragraph{Chapter~\ref{chap:binception} - Bilateral Inception Networks}
In this Chapter, we propose a new CNN module which we call the `Bilateral Inception' (BI)
module that can be inserted into \emph{existing} segmentation CNN models. BI modules help in image adaptive
long-range information propagation across intermediate CNN units at multiple scales.
We show empirically that this alleviates some of the need for standard post-processing inference
techniques such as DenseCRF.~In addition, our module helps in recovering the full resolution of segmentation result,
which is generally lost due to max-pooling and striding. Experiments on different base
segmentation networks and datasets showed that our BI modules result in
reliable performance gains in terms of both speed and accuracy in comparison to traditionally
employed DenseCRF/Deconvolution techniques and also recently introduced dense pixel prediction techniques.

\section{List of Publications}

\nobibliography*

The contributions in this thesis mainly comprise of work from the following publications
~\cite{jampani2014,jampani15aistats,kiefel15bnn,arxivpaper,gadde16bilateralinception,
jampani16vpn}:

\begin{itemize}

\item \bibentry{jampani2014}.~\cite{jampani2014}
\item \bibentry{jampani15aistats}.~\cite{jampani15aistats}
\item \bibentry{kiefel15bnn}.~\cite{kiefel15bnn}
\item \bibentry{arxivpaper}.~\cite{arxivpaper}
\item \bibentry{gadde16bilateralinception}.~\cite{gadde16bilateralinception}
\item \bibentry{jampani16vpn}.~\cite{jampani16vpn}

\end{itemize}

The following publications~\cite{jampani15wacv,jampani15wacv,gadde2016efficient}
are a part of my PhD research but are outside the scope of this thesis:

\begin{itemize}

\item \bibentry{jampani15wacv}.~\cite{jampani15wacv}
\item \bibentry{sevilla:CVPR:2016}.~\cite{sevilla:CVPR:2016}
\item \bibentry{gadde2016efficient}.~\cite{gadde2016efficient}

\end{itemize}
