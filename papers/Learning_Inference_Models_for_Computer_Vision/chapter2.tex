\chapter{Models and Inference in Computer Vision}
\label{chap:models}

Models play a central role in the study of both biological and artificial
vision. Helmholtz, in the 19th century, popularized the human vision as a result
of psychological inference in learned models~\cite{frey2003advances,cahan1993hermann}
as opposed to native processing in lower visual system or eyes.
With the advent of computers in the 20th century, researchers are able to
formulate, learn and evaluate several computational models of vision.
More recently, with powerful parallel computing hardware like graphics processing units
(GPU), researchers are able to learn and do inference in highly complex models with even millions
of parameters. In this chapter, we present an overview of different computer vision models and
discuss the inference and learning techniques therein. Since this thesis work
mainly constitutes the development of new inference techniques, we emphasize the
difficulty of inference in different models and discuss several remedies proposed
in the literature.

\section{Models in Computer Vision}

Models describe the mathematical relationship between the observed data and the
desired properties of the world. Computer vision models are often
probabilistic in nature due to the inherent ambiguity in vision problems.
Due to the broad range of problems in computer vision, there is no single model
that can work well for various vision tasks. Depending on the nature of problem and
the availability of data, different models work well for different scenarios.
Visual data is complex with variability arising due
world properties such as occlusion, lighting, texture, geometry, depth ordering
etc. It is very difficult to model the relationship between all the aspects of the world
and the visual data, and do inference therein. Vision models usually are highly
specific to one or few aspects of the world.

As briefly mentioned in Chapter~\ref{chap:intro}, computer vision models can
be broadly classified into two types: \textit{Generative} and \textit{Discriminative}
models, which can be viewed as complementary and inverse to each other.
Generative models characterize the probability of observed data given the world
properties $P(\obs|\target,\params)$ and discriminative models characterize
the probability of world properties given the observed data $P(\target|\obs,\params)$,
where $\params$ denotes the parameters of the model.
In other words, generative models model the image formation process as a function
of world parameters, whereas discriminative approaches model the desired world
parameters as a function of the given image. As mentioned in Chapter~\ref{chap:intro},
we use the term `generative' more loosely in the sense that any model which characterizes
the likelihood $P(\obs|\target,\params)$ and/or prior over target
variables $P(\target)$ is considered a `generative' model.
Next, we give an overview of these two complementary models discussing the
advantages and disadvantages of both.

\begin{figure}[th!]
	\centering
	% \hfill
  \subfigure[Graphics Renderings]{
		\setlength\fboxsep{-0.3mm}
		\setlength\fboxrule{0pt}
		\parbox[b]{5cm}{
			\centering
			\small
      \fbox{\includegraphics[width=5cm]{figures/cryengine.jpg}}\\
      \vspace{4.5mm}
			\fbox{\includegraphics[width=5cm]{figures/cryengine_2.jpg}}\\
      \vspace{4.5mm}
			\fbox{\includegraphics[width=5cm]{figures/lumberyard.png}} \\
		}
		\label{fig:sample-graphics}
	}
  \hspace{1cm}
	\subfigure[A face generative model]{
		\begin{tikzpicture}

			% \draw[red!20] (-2.4, 2.7) -- (2, 2.7);
			% \draw[red, dotted] (-2.4, 2.7) -- (2, 2.7);
			%
			% \draw[red!20] (-2.4, 5.5) -- (2, 5.5);
			% \draw[red, dotted] (-2.4, 5.5) -- (2, 5.5);

			\node[obs] 											(x)			{$x_i$}; %
			\node[latent, above=of x]                			(z)			{$z_i$}; %

			\factor[above=of z, yshift=10mm] 					{times}		{below left: $\times$} {} {}; %

			\node[latent, above=of times, yshift=-5mm]			(s)			{$s_i$}; %

			\factor[above=of x, yshift=1mm] 					{noise1}	{left:Gaussian} {} {}; %

			\node[latent, left=of times]                		(r)			{$r_i$}; %

			\factor[above=of r] 								{pr}		{} {} {}; %

			\factor[above=of s, yshift=10mm] 					{inner}		{left:Product} {} {}; %

			\node[latent, above=of inner, yshift=-5mm] 			(n)			{$\mathbf{n}_i$}; %
			\node[latent, right=of inner]                		(l)			{$\mathbf{l}$}; %

			\factor[above=of l] 								{pl}		{} {} {}; %

			\factor[above=of n] 								{pn}		{} {} {}; %

			\factoredge {z} 		{noise1} 		{x}; %
			\factoredge {s} 		{times} 		{z}; %
			\factoredge {r} 		{times} 		{}; %
			\factoredge {n} 		{inner} 		{s}; %
			\factoredge {l} 		{inner} 		{}; %
			\factoredge {} 			{pn} 			{n}; %
			\factoredge {} 			{pl} 			{l}; %
			\factoredge {} 			{pr} 			{r}; %

			\plate {} {(pn) (x) (r)} {}; %

			% \fill (r.south) ++ (0, -0.52) circle (3pt) [fill=red] { };
			%
			% \fill (l.south) ++ (0, -0.52) circle (3pt) [fill=red] { };
			%
			% \draw [-stealth, red] (-1.45, 2.85) -- (-1.45, 3.15);
			%
			% \draw [-stealth, red] (1.45, 5.65) -- (1.45, 5.95);

			% \node[yshift=-0.9cm] at (r.south) { $\textcolor{red}{\Delta_i^\mathbf{r}}$ };

			% \node[yshift=-0.9cm] at (l.south) { $\textcolor{red}{\Delta^\mathbf{l}}$ };

			\node[yshift=0.35cm, xshift=-0.83cm] at (inner) { Inner };

		\end{tikzpicture}
		\label{fig:face-model}
	}
  \subfigure[Example face data]{
		\setlength\fboxsep{-0.3mm}
		\setlength\fboxrule{0pt}
		\parbox[b]{3.4cm}{
			\centering
			\small
			\fbox{\includegraphics[width=1.3cm]{figures/sample_N.png}}\\
			Normal $\{\mathbf{n}_i \}$ \vspace{4.5mm} \\
			\fbox{\includegraphics[width=1.3cm]{figures/sample_S.png}} \\
			Shading $\{ s_i \}$ \vspace{4.5mm} \\
			\fbox{\includegraphics[width=1.3cm]{figures/sample_R.png}} \\
			Reflectance $\{ r_i \}$ \vspace{4.5mm} \\
			\fbox{\includegraphics[width=1.3cm]{figures/sample_X.png}} \\
			Observed image $\{ x_i \}$\\
		}
		\label{fig:face-data}
	}
	\mycaption{Sample generative models in computer vision}
  {(a)~Sample renderings from modern graphics engines~\cite{cryengine,lumberyard}.
  Modern graphics provide renderings with stunning level of realism and vision
  problems can be approached as inverting such graphics systems. Images courtesy
  from the official websites of CryEngine~\cite{cryengine} and Lumberyard~\cite{lumberyard}.
  (b)~A layered graphical model (factor graph) for faces (explained in Sec.~\ref{sec:pgm}).
	Here the vision problem
  could be inferring the reflectance map, normal map and light direction from a given
  face image. (c) Sample face data from Yale B face dataset~\cite{Georghiades2001,Lee2005}.}
	\label{fig:sample-gen-models}
\end{figure}

\section{Generative Vision Models}
\label{sec:gen-models}

A conceptually elegant view on computer vision is to consider a generative
model of the physical image formation process. The observed image becomes a
function of unobserved variables of interest (for instance the presence and positions
of objects) and nuisance variables (for instance light sources and shadows).
%
When building such a generative model, we can think of a scene description
$\target$ that produces an image $\obs=G(\target,\theta)$ using a deterministic
rendering engine $G$ with parameters $\params$, or more generally,
results in a distribution over images, $P(\obs|\target,\params)$.
%
Generative models provide a powerful framework for probabilistic reasoning and
are applicable across a wide variety of domains, including computational biology,
natural language processing, and computer vision. For example, in computer vision,
one can use graphical models to express the process by which a face is lit and
rendered into an image, incorporating knowledge of surface normals, lighting and
even the approximate symmetry of human faces. Models that make effective use of this
information will generalize well, and they will require less labelled training data
than their discriminative counterparts (e.g.\ random forests or neural networks) in order
to make accurate predictions.

Given an image observation $\obs$ and a prior over scenes $P(\target)$ we can
then perform \textit{Bayesian inference} to obtain the posterior distribution over the
target variables $P(\target | \obs,\params)$
(also called `posterior inference'):

\begin{equation}
P(\target | \obs,\params) =
\frac{P(\obs|\target,\params)P(\target)}{P(\obs)} =
\frac{P(\obs|\target,\params)P(\target)}{\sum_{\target'} P(\obs|\target',\params)P(\target')}.
\label{eqn:bayes}
\end{equation}

The summation in the denominator runs over all the possible values of $\target$ variable and
would become integral in the case of continuous
variables. $P(\obs|\target,\params)$ is the \emph{likelihood} of the observed data. Inspecting
the above Bayesian formula shows that it is straight forward to compute the numerator
as that involves simply evaluating the generative model. The key difficulty with
Bayesian inference is computing the denominator in Eq.~\ref{eqn:bayes}.
Often, it is not feasible to evaluate the summation
over all the possible target variables even for slightly non-trivial models. This
makes it difficult to obtain closed-form solutions for posterior inference resulting
in the development and use of several approximate inference techniques such
as Markov Chain Monte Carlo (MCMC) sampling and variational inference, which we
will briefly discuss later.

There are different types of generative models with varying model fidelity and
complexity of inference therein. In general, generative models which accurately
model the image formation process (e.g.\ graphics engines) have complex
non-linear functions resulting in a challenging inference task. Building
a good generative model for a given task involves finding a good trade-off
between model fidelity and inference complexity.

Figure~\ref{fig:sample-gen-models} shows some sample generative models in computer vision.
Advances in graphics and physics of light transport resulted in generative models with
high fidelity as is evident in modern computer games and animation movies.
But it is difficult to invert such complex models.
Probabilistic graphical models provide the widely adopted framework
for generative computer vision models. Although graphical models have
less fidelity and only model one or two aspects of the world properties,
they are generally preferred over graphic systems as inference
in them is faster and more reliable. Next, we will discuss these two prominent
generative vision models: \textit{Inverse graphics} and
\textit{Probabilistic graphical models}.

\subsection{Inverse Graphics}
\label{sec:inv-graphics}

Modern graphics engines
(e.g., game engines like CryEngine~\cite{cryengine} and Lumberyard~\cite{lumberyard})
leverage dedicated hardware setups and
provide real-time renderings with stunning level of realism.
Some sample renderings from modern game engines~\cite{cryengine,lumberyard}
are shown in Fig.~\ref{fig:sample-graphics}.
Vision problems can be tackled with posterior
inference (Eq.~\ref{eqn:bayes}) in such accurate computer graphics systems.
This approach for solving vision problems can be understood as
\textit{Inverse Graphics}~\cite{baumgart1974inversegraphics}. The
target variables $\target$ correspond to the input to the graphics system and
the observation variables are the output $\obs=G(\target,\theta)$. The
deterministic graphics system $G$ can be converted into a probabilistic generative
model by defining a prior over the target variables (input to the graphics system) $P(\target)$
and also defining an approximate likelihood function $P(\obs|G(\target,\theta))$
characterizing the model imperfections. If the model imperfections are neglected,
the likelihood can be given using the \emph{delta} function
$P(\obs|\target) = \delta(\obs-G(\target,\theta))$.
An example `inverse graphics' problem, which we tackle in the next chapter,
is depicted in Fig.~\ref{fig:invgraphicsteaser},
where the graphics engine renders a depth image given 3D body mesh
and camera parameters, and a vision problem would be the inverse estimation
of body shape given a depth image.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figures/invGraphicsDemo.pdf}}
\mycaption{An example `inverse graphics' problem}{A graphics engine renders a 3D
  body mesh and a depth image using an artificial camera. By Inverse
  Graphics we refer to the process of estimating the posterior
  probability over possible bodies given the depth image.}
\label{fig:invgraphicsteaser}
\end{center}
\end{figure}

Modern graphic engines are based on physics principles and thus most of the
rendering parameters $\params$ are set to mimic the real world physics. Learning in graphics
generative models mainly involves learning the prior $P(\target)$
over the target world properties which are input to the graphics system.
Depending on the type of model, several learned priors are proposed in the
literature. An example is the SCAPE model~\cite{anguelov2005scape,hirshberg2012coregistration}
for modeling the prior over human body shape and pose for the example
shown in Fig.~\ref{fig:invgraphicsteaser}.

Since modern rendering engines involve complex non-linear functions, it is
usually not feasible to obtain a closed-form solution for the posterior distribution
(Eq.~\ref{eqn:bayes}). Even several approximate inference techniques like
variational optimization techniques~\cite{koller2009probabilistic} cannot
be easily employed for posterior inference in complex graphics systems.
`Monte Carlo' sampling provides a generic inference technique for
such complex models and can be used even when the internals
of the graphics systems are not known. The aim of sampling methods is to
characterize the posterior distribution with \emph{independent and identically
distributed} samples. There exists many Monte Carlo sampling
strategies such as uniform sampling, rejection sampling,
importance sampling~\cite{hammersley1954poor,rosenbluth1955monte} etc.
Here, we limit our discussion to the widely used Markov Chain Monte Carlo (MCMC)
sampling techniques.

\subsubsection*{MCMC Sampling}
%
MCMC sampling~\cite{metropolis1953} is a
particular instance of sampling methods that generates
a sequence of target variables (samples) by simulating a \textit{reversible}
Markov chain.

\begin{equation}
\target_1 \to \target_2 \to \target_3 \to \dots  \to \target_n
\end{equation}•

This Markov chain of samples approximates the target distribution (posterior
distribution in the case of `inverse graphics'). The Markov property states that
at every sequence step $t$, given the present sample $\target_t$ in the sequence,
the next sample $\target_{t+1}$ is independent of all the previous samples
$P(\target_{t+1}|\target_t,\dots,\target_1) = P(\target_{t+1}|\target_t)$.
%

Let us denote the target distribution with $\pi(\cdot)$. In the inverse
graphics setting, the target distribution is the posterior $\pi(\target)=P(\target | \obs,\params)$.
And, let us denote the transition probability between the two states (samples) in the
Markov chain be $T(\target_{t+1}|\target_{t})$ or $T(\target_t\to\target_{t+1}) \in [0,1]$. One way to ensure that the
Markov chain is \emph{reversible} and converging to the target distribution is to check whether the
following \emph{detailed balance condition} holds for any two states
$\target_t$ and $\targetProp$~\cite{koller2009probabilistic}:

\begin{equation}
\pi(\target_t) T(\target_t \to \targetProp) = \pi(\targetProp) T(\targetProp \to \target_t)
\end{equation}

Note that the above detailed balance condition is satisfied when the transition probability distribution is
close to the target distribution. Since we do not know the target distribution, designing
transition probability distributions that satisfies the detailed balance condition is difficult.
The Metropolis-Hastings (MH) algorithm~\cite{metropolis1953}, instead of devising special transition probabilities,
introduces the \emph{acceptance probability} to each Markov chain transition
$A(\target_t \to \targetProp) \in [0,1]$. The detailed balance condition then becomes:

\begin{equation}
\pi(\target_t) T(\target_t \to \targetProp) A(\target_t\to\targetProp) =
\pi(\targetProp) T(\targetProp \to \target_t) A(\targetProp\to\target_t)
\end{equation}

It can be verified~\cite{koller2009probabilistic} that the following acceptance
probability satisfies the above detailed balance condition:

\begin{equation}
A(\target_t\to\targetProp) = \min\left(1,\frac{\pi(\targetProp)
T(\targetProp \to \target_t)}{\pi(\target_t) T(\target_t \to \targetProp)} \right)
\end{equation}

With the use of the above acceptance rule, instead of designing task-specific
transition probability distributions,
any transition probability distribution $T$ with non-zero probability over
the range of all target variables can be used for MH sampling. $T$ is also
called `proposal distribution' since it is used to propose the next sample in the
Markov chain, which is then either accepted or rejected based on the acceptance probability.
Below, we summarize the Metropolis-Hastings (MH) MCMC algorithm.

\paragraph{Metropolis-Hastings (MH) MCMC:}
Sampling from $\pi(\cdot)$ consists of repeating the
following two steps~\cite{liu2001montecarlo}:

\begin{enumerate}
\item Propose a transition using a \textit{proposal distribution $T$}
  and the current state $\target_t$
\begin{equation*}
\targetProp \sim T(\cdot|\target_t)
\end{equation*}
\item Accept or reject the transition based on Metropolis Hastings (MH) acceptance rule:
\begin{equation*}
\target_{t+1} = \left\{
  \begin{array}{cl}
      \targetProp,  & \textrm{rand}(0,1) < \min\left(1,\frac{\pi(\targetProp)
T(\targetProp \to \target_t)}{\pi(\target_t) T(\target_t \to \targetProp)} \right), \\
     \target_t, & \textrm{otherwise.}
  \end{array}
\right.
\end{equation*}
\end{enumerate}

% Kernel T
Different MCMC techniques mainly differ in the type of the
proposal distribution $T$. Note that we do not need to compute the target (posterior)
probabilities, but only the \textit{ratio} of
posterior probabilities $\frac{\pi(\targetProp)}{\pi(\target_t)}$.
This makes MCMC sampling suitable for the inverse graphics setting
where it is not feasible to get a closed-form solution for the normalization
constant in the posterior distribution (denominator in Eq.~\ref{eqn:bayes}).

The key aspect of the MH sampling is the number of steps it requires until
it converges to the target distribution. If the proposal distribution is very
different from the target distribution, the samples tend to be frequently rejected
resulting in a long wait for convergence. In practice, it is difficult to
measure the convergence of any sampler since we do not know the target
distribution. In Chapter~\ref{chap:infsampler}, we discuss several diagnostic
measures that indicate the convergence of MCMC sampling. In the case of
`inverse graphics', each forward rendering step takes a considerable amount of time and
we would like the sampler to accept as many samples as possible. A key for improving
the MH sampling efficiency is to design the proposal distributions that match
the target posterior distribution. In Chapter~\ref{chap:infsampler}, we devise
such technique by leveraging discriminative learning techniques for learning
the proposal distribution. Refer to~\cite{liu2001montecarlo,koller2009probabilistic}
for more details on MCMC sampling. In Chapter~\ref{chap:infsampler}, we study
the behavior of MCMC sampling and its variants for inverting graphics
engines and propose techniques for improving the sampling efficiency.

\subsection{Probabilistic Graphical Models}
\label{sec:pgm}

Probabilistic graphical models (PGM) provide a rigorous mathematical framework,
based on probability and graph theory, for modeling the relationship between
the world and image properties.
PGMs have been popular not only in computer vision but also in
related fields such as natural language processing,
speech processing, etc. Several model representations, learning and inference
schemes haven been developed in the PGM literature and even a concise description
of them would be an inundating task and is outside the scope of this thesis. Refer
to~\cite{koller2009probabilistic} for a comprehensive overview of PGMs.
PGMs generally represent input-output relationships with
factorized functions and are typically confined to a restricted domain so that
efficient inference techniques can be applied.

PGMs are popular models of choice when the joint distribution of all the target
and observation variables can be factorized into independent distributions
each involving a subset of variables. This factorization of the joint distribution
is represented with the structure of the graph, where each node represents a subset
of variables and edges between the nodes represent the joint or conditional
distributions between the corresponding node variables.

\begin{figure}[t]
	\centering
		\begin{tikzpicture}

			\node[latent] (var_a) {a};
			\node[latent, right=of var_a, xshift=4mm] (var_f) {f};
			\node[latent, right=of var_f, xshift=4mm] (var_ce) {c,e};
			\node[latent, below=of var_f, yshift=-4mm] (var_b) {b};
			\node[latent, below=of var_ce, yshift=-4mm] (var_d) {d};

			\factor[left=of var_a, xshift=-2mm] {factor_a} {$i$} {} {};
			\factor[left=of var_f, xshift=-2mm] {factor_f} {$m$} {} {};
			\factor[below right=of var_a, xshift=5mm, yshift=-5mm] {factor_ab} {$j$} {} {};
			\factor[above right=of var_b, xshift=4mm, yshift=4mm] {factor_fceb} {$l$} {} {};
			\factor[right =of var_b, xshift=2mm] {factor_bd} {$k$} {} {};
			\factor[right =of var_f, xshift=2mm] {factor_fce} {$n$} {} {};

			\edge[-] {factor_a} {var_a};
			\edge[-] {factor_f} {var_f};
			\edge[-] {var_a} {var_b};
			\edge[-] {var_b} {var_ce};
			\edge[-] {var_f} {factor_fceb};
			\edge[-] {var_b} {var_d};
			\edge[-] {var_f} {var_ce};

			\edge[->, transform canvas={xshift=8.4mm,yshift=-2.4mm,scale=0.8},black!60!green] {var_ce} {factor_fceb};
			\edge[->, transform canvas={xshift=4mm,yshift=-2mm,scale=0.8},black!60!green] {var_f} {factor_fceb};
			\edge[->, transform canvas={xshift=6.5mm,yshift=-4mm,scale=0.8},red] {factor_fceb} {var_b};
			\edge[->, transform canvas={xshift=0.56cm,yshift=-5.7mm,scale=0.8},black!60!green] {var_b} {factor_bd};
			\edge[->, transform canvas={xshift=0.2cm,yshift=-4mm,scale=0.8},red] {factor_ab} {var_b};

		\end{tikzpicture}
		\label{fig:example-graph}
	\mycaption{An example factor graph}{Variable nodes (circles) and factor nodes (squares) representing
	the factorized function
	$P(a,b,c,d,e,f)=\psi_i(a)\psi_j(a,b)\psi_k(b,d)\psi_l(b,c,e,f)\psi_m(f)\psi_n(c,e,f)$. Also shown are sample
	variable-to-factor messages (green arrows) and, factor-to-variable messages (red arrows).}
\end{figure}

\subsubsection{Factor Graph Representation}

Factor graphs provide a useful visualization and mathematical formalization for representing probabilistic
graphical models. Factor graphs are \emph{bipartite} graphs where nodes in the graph
are divided into two types: `variable' nodes represented as circles and `factor' nodes
represented as squares. Variable nodes represent the random variables in the graphical model and
factor nodes represent the statistical relationship between the variable nodes that they are connected to.
Every edge in the factor graph connects a variable node to factor node. That is,
there are no \emph{direct} edges among the variable nodes or factor nodes. An example
factor graph is shown in Fig.~\ref{fig:example-graph}, where we represent variable nodes
as $a,b,c,\cdots$ and factor nodes as $i,j,k,\cdots$. The factor function associated with
a factor node $a$ is represented as $f_a$ and the states of variables associated with
a variable node $i$ is represented as $i$.
The joint function in Fig.~\ref{fig:example-graph},
$P(a,b,c,d,e)$ is factorized into five independent factors
$\psi_i(a)\psi_j(a,b)\psi_k(b,d)\psi_l(b,c,e,f)\psi_m(f)f_n(c,e,f)$.
Each factor in the graph represent a function and the product of all factor functions makes
up the joint function. In the context of probabilistic generative models, these
functions are probability distributions. The edges in the factor can be either directed
or un-directed representing the joint and conditional distributions respectively.
In the case of discrete variables, the factor functions are probability tables
assigning the probabilities for all the states of the factor variables. And, in the
case of continuous variables, the factor functions are probability density functions.
Another example of factor graph, representing a generative model of faces, is
shown in Fig.~\ref{fig:face-model}. We will discuss more about this face model later
in this section.

Graphical representations like factor graphs have several advantages. They provide
an intuitive way of representing the statistical dependencies between the variables.
Given the factor graph, it is easy to visualize \emph{conditional independence}
between variables.
For a factor graph with undirected edges, variables $s$ and $t$ are independent
given a set of variables $\nu$ ($s \ci t | \nu$)
if every path between $s$ and $t$ have some node $v\in\nu$.
For instance, in the factor graph shown in Fig.~\ref{fig:example-graph},
variable $d$ is independent of $a,c,e$ and $f$ given $b$ is observed: $d \ci a,c,e,f | b$.
Perhaps the most important advantage of graphical representations like factor graphs
is that we can perform Bayesian inference by using the graph structure. For example, the widely used
\emph{message-passing} inference is performed by passing messages between factor and variable nodes.

\subsubsection{Message Passing} Message Passing (MP) inference forms a general class
of algorithms that are used to estimate the factor graph distributions,
i.e. maximizing or minimizing the joint distribution (P(a,b,c,d,e,f) in Fig.~\ref{fig:example-graph}).
MP inference proceeds by passing messages (distributions or density functions)
between variable and factor nodes. Here, we describe the message passing in the
famous `Sum-Product' belief-propagation (BP) algorithm.
Messages are probability distributions that represent the beliefs
over the variables to/from which the messages are being sent.
MP inference in factor graphs has two types of messages: Messages
from variable to factor nodes $\mu_{a\rightarrow i}$ (green arrows in Fig.~\ref{fig:example-graph})
and messages from factor to variable nodes $\mu_{i\rightarrow a}$
(red arrows in Fig.~\ref{fig:example-graph}).
In the case of Sum-Product BP, these messages are defined as follows.

\textit{Variable-to-Factor Message:} A message from variable to factor node
is the product of all the messages that the variable node receives from its
neighboring factor nodes except the recipient factor node.
In Fig.~\ref{fig:example-graph}, the message $\mu_{b \rightarrow m}$
from variable $b$ to factor $m$ is the product of the messages that $b$ receives,
i.e. $\mu_{k \rightarrow b} \mu_{l \rightarrow b}$. In general, a message from
a variable node $g$ to factor node $r$ is defined as:

\begin{equation}
\mu_{g \rightarrow r}(x_g) = \prod_{\hat{r}\in N(g)\setminus \{r\}} \mu_{\hat{r} \rightarrow g}(x_g),
\end{equation}

where $N(g)$ is the set of neighboring nodes to $g$ and $x_g$ represent the states
of $g$.

\textit{Factor-to-Variable Message:} A message from factor to variable node
is first computed as the product of factor function with all the incoming messages
from variables except the target variable. The resulting product is then marginalized
over all the variables except the one associated with the target node variables.
For example, in Fig.~\ref{fig:example-graph}, the message from $l$ to $b$ is computed
by marginalizing the product $\psi_l(b,c,e,f)\mu_{c,e\rightarrow l}\mu_{f\rightarrow l}$
over non-target variables $c,e,f$. In general, a message from factor node $r$
to variable node $g$ is defined as:

\begin{equation}
\mu _{r\to g}(x_g)=\sum _{x':V_{r}\setminus g} \psi_{r}(x')\prod_{\hat{g}\in
N(r)\setminus \{g\}}\mu _{\hat{g}\to r}(x_{\hat{g}}),
\end{equation}

where $N(r)$ represents the neighboring nodes of $r$ and $V_r$ represent all the variables
attached to $r$. The summation in the above equation becomes an integral when
dealing with continuous variables.

In a typical message passing inference run, the above messages are repeatedly computed
and sent between factor and variable nodes. Depending on the structure of the factor graph,
different message priorities are used. Upon convergence or passing messages for a
pre-defined number of iterations, the marginal distribution at each variable node is
computed as the product of all its incoming messages from the neighboring factor nodes:

\begin{equation}
P(x_g) = \prod_{\hat{r}\in N(g)} \mu_{\hat{r} \rightarrow g}(x_g).
\end{equation}

Messages represent a probability of each possible state of a variable.
In the case of discrete variables with small number of states, it is easy to represent
messages as probability distribution tables. However, in the case of continuous variables,
the messages must be full functions of the variables. Some approximations are typically
required to represent messages with continuous distributions.
Different message passing algorithms differ in the way the messages are approximated.
Some MP algorithms assume Gaussian form for the messages with either restricting
the type of factor functions~\cite{weiss2001correctness} that always result in Gaussian messages or
projecting the computed messages into Gaussian form~\cite{Minka2001}.
Some other MP algorithms use mixture of Gaussians~\cite{sudderth2010nonparametric} or set of
particles/samples~\cite{ihler2009particle} for representing messages.


\subsubsection{Variational Inference}
Except for small graphical models, exact Bayesian inference (say, with
the above mentioned Sum-Product BP) in PGMs is
usually not possible. This is especially true for computer vision models which
typically involve several hundreds or thousands of variables.
Variational Bayesian inference is one of the widely used technique for performing
inference in PGMs. These methods try to find an
approximation $Q(\target)$ to the true posterior distribution $P(\target|\obs)$
via optimization techniques.
This approximate distribution is usually taken from a known simpler family
of distributions (such as exponential family) and the optimization is
performed over the space of that family of distributions. For example, for the
above joint distribution function $P(a,b,c,d,e,f)$,
an approximation could be a factorized distribution
$Q = Q_1(a) Q_2(b) Q_3(c) Q_4(d) Q_5(e) Q_6(f)$, each from an exponential family of distributions.
The most commonly used optimization function is minimizing the
Kullback-Leibler (KL) divergence of $P$ from $Q$ in order to find a close approximation
to $P$:

\begin{equation}
D_{KL}(Q||P) = \underbrace{\sum_z Q(\target) \frac{Q(\target)}{P(\target,\obs)}}_{-\mathcal{L}(Q)} +
\log P(\obs)
\label{eqn:kl}
\vspace{-0.2cm}
\end{equation}

Minimizing the above KL-divergence translates to maximizing the variational
lower bound $\mathcal{L}(Q)$ as $P(\obs)$ is constant.
$\mathcal{L}(Q)$ is also called \textit{energy functional} or
\textit{variational free energy} as it can be written as the sum of energy
$\mathbb{E}_Q[\log P(\target,\obs)]$ and the entropy of $Q$. $Q$ is chosen
in such a way that $\mathcal{L}(Q)$ becomes tractable and maximizable.

In general, the energy functional is maximized by passing messages between different
variable nodes in the factor graph (e.g. factor graphs in
Figs.~\ref{fig:example-graph},~\ref{fig:face-model}).
Following~\cite{koller2009probabilistic}, depending on the type of approximations
to $Q$ and the energy functional, methods for maximizing $\mathcal{L}(Q)$ can
be categorized into three types. The first category of methods optimizes approximate
versions of the energy functions by passing messages in the simplified versions of the
given factor graph. This includes the
\textit{loopy belief propagation}~\cite{frey1998revolution,weiss2000correctness} algorithm.
The second category of methods try to maximize the exact energy functional but
uses approximate message propagation steps, for example approximating complex
messages with distributions from the exponential family. This is equivalent to using
relaxed consistency constraints on $Q$. These class of methods are also known as
\textit{expectation propagation} (EP)~\cite{Minka2001} algorithms.
The third commonly used category of methods maximize the exact energy functional
but restrict $Q$ to simple factorized distributions, which is called \textit{mean-field}
approximation. One of the most commonly used message passing technique for
optimizing the energy functional is \textit{variational message passing} (VMP)~\cite{Winn2005}.
In Chapter~\ref{chap:cmp}, we show how EP and VMP fail to converge for inference
in model shown in Fig.~\ref{fig:face-model} and propose a remedy for that.

Several other inference techniques such as MCMC sampling can be used for inference
in PGMs. Refer to~\cite{fox2012tutorial} for a tutorial on variational Bayesian
inference and to~\cite{koller2009probabilistic,wainwright2008graphical}
for a comprehensive review of various inference techniques in PGMs.

\subsubsection{Two Example Models}
\label{sec:example_modesl_crf}
Here, we give a brief overview of two popular types of
PGMs in vision which are later used in this thesis:
Layered graphical models and fully connected CRFs.

\paragraph{Layered Graphical Models:} Several vision models are hierarchical
in nature and can be naturally expressed
with layered graphical models. Figure~\ref{fig:face-model} shows an example
layered \textit{factor graph} model for faces. Here, a vision task could be:
Given an observation of pixels $\mathbf{x} = \{x_i\}$,
we wish to infer the reflectance value $r_i$ and normal vector $\mathbf{n_i}$
for each pixel $i$ (see Fig.~\ref{fig:face-data}).
The model shown in Fig.~\ref{fig:face-model} represents the following approximate image
formation process: $x_i = (\mathbf{n_i} \cdot \mathbf{l}) \times r_i + \epsilon$,
thereby assuming Lambertian reflection and an infinitely distant directional
light source with variable intensity. Each factor in the graph is a
conditional probability distribution providing the factorization for the
joint distribution:

\begin{equation}
P(\obs, \mathbf{z}, \mathbf{r}, \mathbf{s}, \mathbf{n}, \mathbf{l})
= P(\mathbf{n}) P(\mathbf{l}) P(\mathbf{r}) \prod_i P(x_i) P(x_i | z_i)
P(z_i | r_i, s_i) P(s_i | \mathbf{n}_i, \mathbf{l}),
\end{equation}

where $s_i$ and $z_i$ represent the intermediate shading and non-noisy
image observation variables. We omitted the model parameters $\params$
in the above equation for the sake of simplicity.
A vision task could to estimate the posterior distribution
$P(\mathbf{r}, \mathbf{s}, \mathbf{n}, \mathbf{l} | \obs)$.
Note that this generative model is only a crude
approximation of the true image formation process
(e.g. each pixel is modeled independently and it does not account for
shadows or specularities). Such approximations are customary to PGMs as
several PGM inference techniques cannot be applied for models with complex
non-linear factors. Note that even for a relatively small image of
size $96 \times 84$, the face model contains over 48,000 latent variables
and 56,000 factors, and as we will show in Chapter~\ref{chap:cmp},
standard message passing routinely fails to converge to
accurate solutions.

\definecolor{city_1}{RGB}{128, 64, 128}
\definecolor{city_2}{RGB}{244, 35, 232}
\definecolor{city_3}{RGB}{70, 70, 70}
\definecolor{city_4}{RGB}{102, 102, 156}
\definecolor{city_5}{RGB}{190, 153, 153}
\definecolor{city_6}{RGB}{153, 153, 153}
\definecolor{city_7}{RGB}{250, 170, 30}
\definecolor{city_8}{RGB}{220, 220, 0}
\definecolor{city_9}{RGB}{107, 142, 35}
\definecolor{city_10}{RGB}{152, 251, 152}
\definecolor{city_11}{RGB}{70, 130, 180}
\definecolor{city_12}{RGB}{220, 20, 60}
\definecolor{city_13}{RGB}{255, 0, 0}
\definecolor{city_14}{RGB}{0, 0, 142}
\definecolor{city_15}{RGB}{0, 0, 70}
\definecolor{city_16}{RGB}{0, 60, 100}
\definecolor{city_17}{RGB}{0, 80, 100}
\definecolor{city_18}{RGB}{0, 0, 230}
\definecolor{city_19}{RGB}{119, 11, 32}
\begin{figure}[t]
  \scriptsize % scriptsize
  \centering
 \fcolorbox{white}{city_1}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Road~~
 \fcolorbox{white}{city_2}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Sidewalk~~
 \fcolorbox{white}{city_3}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Building~~
 \fcolorbox{white}{city_4}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Wall~~
 \fcolorbox{white}{city_5}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Fence~~
 \fcolorbox{white}{city_6}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Pole~~
 \fcolorbox{white}{city_7}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Traffic Light~~
 \fcolorbox{white}{city_8}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Traffic Sign~~\\
 \fcolorbox{white}{city_9}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Vegetation~~
 \fcolorbox{white}{city_10}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Terrain~~
 \fcolorbox{white}{city_11}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Sky~~
 \fcolorbox{white}{city_12}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Person~~
 \fcolorbox{white}{city_13}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Rider~~
 \fcolorbox{white}{city_14}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Car~~
 \fcolorbox{white}{city_15}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Truck~~\\
 \fcolorbox{white}{city_16}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Bus~~
 \fcolorbox{white}{city_17}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Train~~
 \fcolorbox{white}{city_18}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Motorcycle~~
 \fcolorbox{white}{city_19}{\rule{0pt}{1pt}\rule{1pt}{0pt}} Bicycle~~\\
  \subfigure[Sample Image]{%
    \includegraphics[width=.45\columnwidth]{figures/frankfurt00000_008206_given.png}
  }\hspace{4.5mm}
  \subfigure[Ground Truth Semantics]{%
    \includegraphics[width=.45\columnwidth]{figures/frankfurt00000_008206_gt.png}
  }
  \mycaption{Illustration of semantic segmentation task}{A sample image from
  Cityscapes street scene dataset~\cite{Cordts2015Cvprw} and the corresponding
  ground truth semantic labels.}
\label{fig:example-seg}
\end{figure}


\paragraph{Fully Connected CRFs:} Fully connected conditional random fields,
also known as DenseCRFs, are CRF models where every variable in the image is connected to
every other variable via pairwise edge potentials. For illustration purposes,
let us consider the task of semantic segmentation which is labelling each pixel
in a given image with a semantic class. See Fig.~\ref{fig:example-seg} for an
illustration. For the segmentation problem, DenseCRFs
are generally used to encode the prior knowledge about the problem:
`Pixels that are spatially and photometrically similar are more likely to have the same label'.

For an image $\obs$ with $n$ pixels, the semantic segmentation task is to produce a
labelling $\target$ with discrete values
$\{y_1,\dots,y_n\}$ in the label space $y_i\in\{1,\ldots,\mathcal{L}\}$. The DenseCRF model has
unary potentials $\psi_u(y)\in\mathbb{R}$, e.g., these can be the output of CNNs.
The pairwise potentials, as introduced in~\cite{krahenbuhl2012efficient},
are of the form $\psi^{ij}_p(y_i,y_j) = \mu(y_i,y_j) k(\f_i,\f_j)$
where $\mu$ is a label compatibility matrix, $k$ is a Gaussian kernel
$k(\f_i,\f_j)=\exp(-(\f_i-\f_j)^{\top}\Sigma^{-1}(\f_i-\f_j))$ and
the vectors $\f_i$ are feature vectors at each point. Commonly used features are
position and color values at the pixels (e.g., $\f=(x,y,r,g,b)^\top$).
In the DenseCRF model, the energy functional for an image $\obs$ thus reads:

\begin{equation}
P(\target|\obs)\propto\exp(-\sum_i\psi_u(y_i)-\sum_{i>j}\psi^{ij}_p(y_i,y_j)).
\end{equation}

Because of the dense connectivity, exact MAP or marginal inference is intractable. The main result
of~\cite{krahenbuhl2012efficient} is to derive the mean-field approximation for this model and to relate it
to bilateral filtering which enables tractable approximate inference.
As described above, mean-field approximation is a type of variational inference
where the approximate distribution $Q$ is considered to be fully-factorized
across pixels:
$Q=\prod_{i \in n} Q_i(x_i)$. Variational inference then solves for $Q$ by minimizing
the KL divergence of $P$ from $Q$(see Eq.~\ref{eqn:kl}).The work of~\cite{krahenbuhl2012efficient}
showed that the inference can be performed with efficient bilateral filtering
~\cite{aurich1995non, smith97ijcv, tomasi1998bilateral, adams2010fast} operations.
Specifially, mean-field inference results in a fixed point equation which
can be solved iteratively $t=0,1,\ldots$ to update the marginal distributions $Q_i$:

\begin{equation}
Q^{t+1}_i(x_i) = \frac{1}{Z_i} \exp(-\psi_u(x_i) - \sum_{l \in \mathcal{L}}\underbrace{\sum_{j \ne i}
\psi^{ij}_p(x_i,l) Q^{t}_j(l)}_\text{bilateral filtering}),
\label{eq:mfupdate}
\end{equation}

where $Z_i$ denotes a normalization constant and can be easily computed as $Q_i$ is
a single dimensional distribution. Although we used semantic segmentation task
for illustration purposes, DenseCRFs are shown to be useful for tackling
other tasks such as material segmentation~\cite{bell2015minc}, optical flow
estimation~\cite{sun2013fully} and intrinsic image decomposition~\cite{bell2014intrinsic}.

One of the fundamental limitations of the existing use of DenseCRFs is the
confinement of pairwise potentials $\psi^{ij}_p(y_i,y_j)$ to be Gaussian as
bilateral filtering is traditionally implemented with a Gaussian kernel. In Chapter~\ref{chap:bnn},
we show how we can learn a more general form of bilateral filters and
apply that technique for learning pairwise edge potentials in DenseCRF.

\subsection{Advantages and Limitations}
\label{sec:gen-adv-limits}

This generative modeling view is appealing as it is relatively easy to incorporate our knowledge
of physics and light transport into models and was advocated since the late
1970~\cite{horn1977imageintensities,
grenander1976patterntheory,zhu1997learning,mumford2010patterntheory,
mansinghka2013approximate,yuille2006vision}.
For example, the knowledge of how light reflects on objects with different
material properties or the knowledge of how roads and buildings are structured
are relatively easy to incorporate into generative models.
Due to incorporation of strong prior knowledge into the systems, generative
models usually work better when there is little or no data available for a
particular problem.
Since a single generative model can model different world and image
characteristics, it can be used for many different applications.
In addition, it is easier to diagnose the flaws in generative model as most
of the model is manually designed.

% Depressive perspective: 30 years of non-working models
Despite its intuitive appeal and advantages,
in practice, generative models are used only for a few vision problems.
The few successes of the idea have been in limited settings.
In the successful examples, either
the generative model was restricted to few high-level latent
variables, e.g.,~\cite{oliver2000humaninteractions},
or restricted to a set of image transformations in a fixed reference
frame, e.g.,~\cite{black2000imageappearance},
or it modeled only a limited aspect such as
object shape masks~\cite{eslami2012shapeboltzmann},
or the generative model was merely used to generate
training data for a discriminative model~\cite{shotton2011kinect,gaidon2016virtual,
ros2016synthia,richter2016playing,shafaei2016play}.
%
With all its intuitive appeal, its beauty and simplicity, it is fair to say
that the track record of generative models in computer vision is poor.
%
As a result, the field of computer vision is now dominated by
efficient but data-hungry discriminative models,
the use of empirical risk minimization for learning,
and energy minimization on heuristic objective functions for inference.

% What is the challenge in this picture?
Why did generative models not succeed? There are two key problems that
need to be addressed, the design of an accurate generative model, and
the inference therein.
The first key problem which is the design of accurate generative model is partly
addressed by recent advances in graphics. Although modern graphics provide
rendering with stunning level of realism, priors of world parameters are difficult
to characterize. This results in complex priors together with more complex
forward models for accurate generative models which in turn results in
difficult inference.

This brings us to the second key problem in the generative world view which is
the difficulty of posterior inference at test time.
%
This difficulty stems from a number of reasons:
\emph{first}, the target variable $\target$ is typically high-dimensional and so is
the posterior.
%
\emph{Second}, given $\target$, the image formation process realizes complex and
\emph{dynamic} dependency structures, for example when objects occlude or
self-occlude each other. These intrinsic ambiguities result in multi-modal
posterior distributions.
%
\emph{Third}, while most renderers are real-time, each simulation of the
forward process is expensive and prevents exhaustive enumeration. Overall,
the limitations of generative approaches out-weigh their advantages making
them not succeed in building practical computer vision systems.

Despite these limitations, we still believe in the usefulness of generative
models in computer vision, but argue that we need to leverage existing discriminative
or even heuristic computer vision methods for alleviating some of the difficulties in the
posterior inference. Inference techniques proposed in this thesis are steps
in this direction.

\section{Discriminative Vision Models}

With the advances in internet and image capturing technology, there is an explosive
growth of visual data during the last few years. Moreover, presence of crowd-sourcing
platforms like `Amazon Mechanical Turk'~\cite{mturk} make it easier to annotate large
amounts of data by millions of people. Discriminative models directly model the
contingency of world properties on the observed data $P(\target|\obs)$.
Unlike generative models, discriminative
models are task-specific and learning takes the central role in defining the model.
Discriminative models comprise of functions directly approximating
the posterior distribution $P(\target|\obs, \params)$, where $\params$ denote
the parameters of the model. Supervised learning with annotated training data
is usually employed to fit the model parameters to the given task.
Since discriminative models directly characterize
the posterior distribution, inference is reduced to simple evaluation of
the model.

Due to the availability of large amounts of training data and the computing power
that can handle rich high-capacity models, discriminative models have been very
successful in many vision problems. In addition, inference is fast
since this involves a simple evaluation of the model. This makes discriminative
models particularly attractive for many practical applications. Many
mathematical functions that are rich enough to capture the relationship between
the observation and target variables can be used as discriminative models.
Hence, many types of discriminative models have been used in the computer vision
literature.

Discriminative models are traditionally partitioned into two modules:
\textit{feature extraction} and \textit{prediction} modules.
Before the advent of modern convolutional neural networks (CNNs),
these two components are studied separately in the literature.
We briefly discuss these two components in the discriminative models.

\paragraph{Feature Extraction:}
Depending on the type of vision task, features are extracted either at all
pixels (points) in the observed data or only at some key points. For example,
registering two images taken from different view-points requires finding
corresponding points (key points) in each image and then matching.
For such tasks, an additional step of key point detection is required before
feature computation. For image classification, a single feature vector
is extracted for the entire image.

An ideal feature representation should be compact, efficient to compute
and invariant to specific transformations. As an example,
for semantic segmentation, features should be invariant to intra-class
variations such as illumination, scale, rotation, object articulations, etc.,
while being sensitive to changes across different semantic categories.
Several feature extraction schemes have
been proposed in the vision literature, most of them are hand crafted.
Some popular choices include SIFT~\cite{lowe1999object}, HoG~\cite{bay2006surf},
SURF~\cite{dalal2005histograms}, DAISY~\cite{tola2008fast}, etc.
Models for feature extraction and prediction are plentiful and discussing
all of them is outside the scope of this thesis.
With the recent advances in CNNs, feature extraction is coupled with
prediction which are learned together end-to-end.

\paragraph{Prediction:}
Once the image features are extracted, the task is to estimate the
posterior distribution $P(\target|\f(\obs))$, where $\f(\obs)$ denotes the features.
A common strategy is to learn a rich parametric or non-parametric model with
supervised learning techniques. This makes the availability of training data
crucial for discriminative approaches. Several learning based prediction
models have become popular in tackling vision tasks including support vector
machines (SVM)~\cite{cortes1995support},
boosting~\cite{schapire1990strength, freund1995desicion},
random forests~\cite{breiman2001random,ho1995random},
deep convolutional neural networks~\cite{lecun1998gradient},
etc. Refer to~\cite{friedman2001elements}
for a review of different prediction techniques.
Next, we briefly review random forests and CNN models as we either make use of
or propose extensions to these models in this thesis.

\subsection{Random Forests}
\label{sec:forests}

Random forests~\cite{breiman2001random,ho1995random}
are an ensemble of $K$ randomly trained prediction (classification or regression)
trees, where each tree $T(\target|\f(\obs),\params^k)$ represents a non-linear
function approximating the posterior $P(\target|\f(\obs))$.
The trees are typically binary trees and can be viewed as performing a
discriminative hierarchical clustering of the feature space.
And a simple model fit (e.g.,\ linear model) is used in each cluster.

Trees are grown incrementally from the root node to the leaves and
each node represents a partition of the feature space. These partitions can be any linear
or non-linear functions, but the simple axis-aligned partitions are the most used ones
due to their simplicity and efficiency. For simplicity, let us assume the partition functions
are axis-aligned. At each node, a feature $\kappa$ and
its split value $\tau$ are chosen to split the feature space,
so as to minimize an energy function $E$. Let us
consider training the $j^{th}$ node in a $k^{th}$ tree. Let all the data points falling
in that node be $\mathcal{S}_j$ (due to splitting of its ancestor nodes)
and $\mathcal{T}_j$ denotes the discrete set of
\textit{randomly} selected feature axes $\{(\kappa,\tau)_i\}$
(feature indices and their corresponding values)
for the node $j$. Training the $j^{th}$ node corresponds to choosing the optimal
split $\theta^k_j \in \mathcal{T}_j$ among the randomly chosen splits that
minimizes an energy function $E$:

\begin{equation}
\theta^k_j = \argmin_{\gamma \in \mathcal{T}_j} E(\mathcal{S}_j,\gamma)
\end{equation}

Depending on the type of task and data, different energy functions $E$ are
used. Each split $\gamma$ partitions the training data $\mathcal{S}_j$ in
the node $j$ into two parts $\mathcal{S}^L_j$ and $\mathcal{S}^R_j$ which are
assigned to left and right child nodes respectively.
A common energy function measures how well a regression/classification model
fit the data in each of the left and right child nodes created by a split $\gamma$:

\begin{equation}
E(\mathcal{S}_j,\gamma) = - (M(\mathcal{S}^L_j, \beta) + M(\mathcal{S}^R_j, \beta)).
\label{eqn:forest_energy}
\end{equation}

Where $M(\mathcal{S}_j,\beta)$ denotes the model likelihood i.e., how well the model with parameters
$\beta$ can explain the data $\mathcal{S}_j$.
For example, in the case of regression tasks, $M$ can be a linear regression fit
and in the case of classification
(like in semantic segmentation), $M$ can be the classification accuracy.
Like this, the trees are recursively grown by splitting the leaf nodes into left and right
child nodes. The set of all node splits $\theta^k=\{\theta^k_j\}_{j=1,\cdots,J}$ represents
the parameters of the $k^{th}$ tree.
Once a tree is trained, a simple prediction
model is fitted to the data in the leaf nodes. A deep tree might overfit the data
and a shallow tree would under-fit the data and miss important structure. Restricting
the tree size corresponds to regularizing the model and size should be adaptively
chosen based on the training data. Some training stopping criteria include setting the
maximum depth of the trees; minimum number of data points in each node;
a threshold for energy function $E$, etc.

Random forests are distinguished from other tree-based
supervised learning techniques such as boosted decision trees, by the way different
trees are trained in a forest. Each tree in a random forest is trained independently
and randomness is added either in terms of choosing a random subset of training data for each
tree (called \textit{bagging}~\cite{breiman1996bagging}) and/or randomly
choosing the split candidates (feature indices and their values) at each node.
Typically, the estimates across the trees $T(\target|\f(\obs),\params^k)$
are averaged to get the final model $P(\target|\f(\obs))$:

\begin{equation}
P(\target|\f(\obs)) = \frac{1}{K}\sum_{k} T(\target|\f(\obs),\params^k).
\end{equation}

Due to the randomness, different trees are identically distributed
resulting in a low-variance estimate when the final estimate is taken as the average
across the trees.
Random forests are highly flexible and several different types of models are
conceivable with using a combination of different splitting criteria.
Due to their simplicity and flexibility, random forests have become a popular
choice for supervised learning in vision.
Random forests are easy to implement and train. Also, they can be easily adapted
to a wide range of classification and regression tasks with relatively simple
changes to the model. Moreover, they are non-parametric in
nature with the ability to consume large amounts of training data.
Random forests are successfully applied for vision tasks such as
human pose estimation~\cite{shotton2011kinect}, semantic segmentation~\cite{shotton2008semantic}, etc.
In the case of semantic segmentation, a popular model is to extract TextonBoost
features~\cite{shotton2006textonboost}
at each pixel and then train a random forest classifier to predict the class
label at each pixel.
One of the crucial advantages of random forests with respect
to neural networks is that the loss function $E$ need not be differentiable.
In Chapter~\ref{chap:infsampler}, we use random forest models
to improve inference in inverse graphics via our informed
sampler approach. In Chapter~\ref{chap:cmp}, we use random forests
for predicting messages resulting in improved variational inference in
layered graphical models. Refer to~\cite{Criminisi2013} for a comprehensive
overview of random forests and their applications in computer vision and medical
image analysis.

\subsection{Convolutional Neural Networks}
\label{sec:cnn}

Neural networks are a class of models with complex parametric non-linear
functions relating the input $\obs$ to the target $\target$. The complex non-linear
function is usually realized by stacking a series of simple and differentiable
linear and non-linear functions:

\begin{equation}
P(\target|\obs,\params) = \func_1(\func_2(\cdots \func_k(\obs,\theta_k)\cdots, \params_2),\params_1).
\end{equation}

Learning involves finding the parameters $\{\params_1, \params_2, \cdots, \params_k\}$
that best approximates the desired relationship between the input and target variables.
The component functions are usually simple linear functions such as convolutions
$\func(\mathbf{s},\theta)=\mathbf{W(\params)}\mathbf{s}+b$ (where $\mathbf{s} \in \mathbb{R}^q$,
$\mathbf{W} \in \mathbb{R}^{p \times q}$) interleaved
with simple non-linear functions such as rectified linear units (ReLU) $\func(\mathbf{s})=max(0,\mathbf{s})$.
A linear function together with a non-linearity is usually called a single layer
in the network. Intermediate layers in a neural network are also called \textit{hidden}
layers and the number of units in intermediate layers determine the \textit{width}
of the network.
A theoretical result~\cite{csaji2001approximation,hornik1991approximation}
is that any complex continuous function can be approximated by a simple two
layered neural network, given sufficient number of intermediate units (width of
the network).
From a practical point of view, neural networks are attractive because of their
fast inference (simple forward pass through the network) and an end-to-end prediction
(going from input to output variables without the intermediate handcrafted feature
extraction) capabilities.

Convolutional neural networks (CNN) are a special class of neural networks
tailored for processing 2D or higher dimensional visual data on a grid.
The main characteristic of CNNs is the use of spatial convolutions instead of
\textit{fully-connected} matrix-vector multiplications for building linear
functions. This greatly reduces the amount of parameters due to parameter sharing
across different spatial locations and speeds up the
network computation and training. One of the main hurdles for the success of
CNNs was the lack of computational resources required to train models
with millions of parameters. Recent availability of large datasets together
with efficient model and training implementations in GPUs made it possible
to successfully apply CNNs to real-world vision tasks. Since CNNs typically have
millions of parameters, they are highly prone to overfit the training data.
Advances in simple yet powerful regularization techniques (such as
DropOut~\cite{srivastava2014dropout}) are another reason for the successful deployment of
CNN models. Currently, CNNs are state-of-the-art
in many traditional vision problems such as image classification~\cite{he2015deep,krizhevsky2012imagenet},
object detection~\cite{girshick2014rich,redmon2015you,ren2015faster},
semantic segmentation~\cite{long2014fully,chen2016deeplab}, etc.

\begin{figure}[t!]
 \centering
 \includegraphics[width=0.9\columnwidth]{figures/supplementary/lenet_cnn_network}
 \mycaption{Sample CNN architecture for character recognition}
 {LeNet-7~\cite{lecun1998mnist} architecture generally used for character recognition
 (used for Assamese character recognition in Chapter~\ref{chap:bnn}).
 `C$n$' corresponds to convolution layer with $n\times n$ filters; `MP$n$' corresponds
 to max-pooling layer with window size $n$; `IP' corresponds to fully-connected
 inner-product layer; `ReLU' and `TanH' corresponds to rectified linear units
 and tanh non-linear layers and `Softmax' layer produces probabilities for 183
 output classes.}
\label{fig:lenet7}
\end{figure}

CNN architectures are typically composed of the following layers: Convolution,
pooling, non-linearity, fully-connected (FC) and loss layers.
Convolution layers are simple spatial convolutions,
pooling layers do spatial downsampling
and FC layers connect each output unit to all input units. Non-linear layers
are simple yet important functions that model non-linearities in the CNN
model. Some popular non-linear functions include ReLU, TanH, sigmoid function,
etc. Loss layers are problem specific layers that are used at the end of the
network and implement the differentiable empirical loss $\sigma(\hat{\target}, \target^*)$
between the predicted target $\hat{\target}$ and the ground truth target $\target^*$.
Figure~\ref{fig:lenet7} shows a sample CNN architecture generally used for character
recognition. The input is a grayscale image $\obs$ with the size $96\times96$ and the
output is a vector of probabilities for each class $P(\target|\obs)$. Also shown
are the sizes of intermediate CNN representations.

Training the parameters $\params_k$ of each layer involves back-propagating
the empirical loss from the loss layers backwards to the early CNN layers.
To avoid over-fitting to the training data, the loss is usually augmented
with a regularization over the network parameters. The optimization objective
for a given dataset with $m$ training instances is given as an average loss $L$:

\begin{equation}
L(\params) = \frac{1}{m} \sum_{i=1}^{m} \sigma(\hat{\target_i}, \target_i^{*}) + \lambda r(\params),
\end{equation}

where $r$ denotes the regularization over the parameters $\params$ with weight $\lambda$.
Then the parameters $\theta$ are updated using gradient descent methods
such as stochastic gradient descent (SGD), AdaDelta~\cite{zeiler2012adadelta},
Adam~\cite{kingma2014adam}, etc.
The parameter update steps to update a single parameter $\params^i \in \params$ in SGD are given as:

\begin{align}
\begin{split}
v_{t+1} &= \mu v_t - \gamma \nabla L(\params^i_t)
\\
\params^i_{t+1} &= \params_t + v_{t+1}
\end{split}
\end{align}

where $v, \gamma, \mu \in \mathbb{R}$,
$v_t$ denotes the parameter update in the previous step and
$\nabla L(\params^i_t)$ denotes the gradient of loss $L$ with respect
to the parameter $\params^i$. Thus the parameter update $v_{t+1}$ is a weighted
combination of the previous update and the negative gradient of loss $L$.
The weights $\mu$ and $\gamma$ are called \textit{momentum} and
\textit{learning rate} respectively which are generally chosen to obtain
good performance on a given validation data.
In practice, since the size of the dataset $m$ is large, only a small subset (batch)
of dataset is used for computing the loss and updating parameters in each step.

Instead of computing the gradients of loss $L$ with respect to all the network parameters
in one step, the gradients are back-propagated across the layers.
Thus, one of the fundamental requirements for a component function (except the first layer)
in CNNs is that it should be differentiable with respect to both its inputs
and its parameters. Once the network is trained,
inference is a simple forward pass through the network. Like in many discriminative
models, inference in CNNs amounts to evaluation of the model.

In Chapter~\ref{chap:bnn}, we generalize the standard spatial convolutions found
in CNNs to sparse high-dimensional filters and in Chapter~\ref{chap:binception},
we propose an efficient CNN module for long-range spatial propagation of information
across intermediate CNN representations. There are several other types of
neural networks, such as recurrent neural networks,
that are currently being used in the computer vision community.
Refer to~\cite{Goodfellow-et-al-2016-Book,888} for more details regarding CNNs or
neural networks in general.

\subsection{Advantages and Limitations}

Discriminative models are mainly data-driven methods where inference amounts to
simple evaluation of the model. In general, discriminative models are
fast, robust to model mismatch and are also high performing when
trained with enough amounts of data. Discriminative models are attractive
because of their practical utility and also their flexibility in terms of
being able to use same model architecture and training for different vision tasks.

\begin{figure}[t!]
 \centering
 \includegraphics[width=0.9\columnwidth]{figures/seg_net_illustration.pdf}
 \mycaption{Sample CNN architecture for semantic segmentation}
 {Semantic segmentation CNN architectures typically consists of convolution (Conv.),
 pooling (Pool) and $1\times1$ convolution layers (FC) interleaved with non-linearities (ReLU).
 The use of pooling results in lower resolution CNN output which is generally up-sampled
 with either interpolation, deconvolution and/or CRF techniques. CRF techniques also help
 in incorporating prior knowledge about semantic segmentation.}
\label{fig:seg-net}
\end{figure}

On the other hand, discriminative models also have several limitations.

\begin{itemize}
\item Discriminative models are data hungry and typically fail to work where
there is little data available. Availability of large datasets for several vision
tasks and online annotation tools like Amazon Mechanical Turk~\cite{mturk} help
in mitigating this limitation.
\item Since discriminative models tend to have a large number of parameters
which are directly learned from data, when the performance is not as expected,
it is difficult to find the cause of the problem and accordingly modify the models.
As a result, there are no guaranteed ways
to find the right model architecture for a given task.
These problems are generally handled with either regularizations on model complexity
or with trial-and-error strategy on various model architectures.
\item One of the fundamental limitations of discriminative models is the lack of key
approaches to inject prior knowledge into the models. This is especially true
in the case of end-to-end trained models like CNNs. In the case of hand crafted
features, we can inject some prior context in the form of image or pixel features.
It is not easy to inject the knowledge of generative models into discriminative
approaches such as CNNs. For example, in the case of semantic segmentation,
post-processing steps such as DenseCRFs are generally employed to model the
relationship (prior knowledge) between the pixels and output labels.
Figure~\ref{fig:seg-net} shows a prominent CNN architecture for semantic segmentation.
\item Discriminative models are task-specific and a single trained model can not
be easily transferred to other vision problems.
Although several transfer learning techniques~\cite{pan2010survey} exists that
can help transfer the knowledge across different discriminative models,
they are generally task specific and have limited success.
One of the main advantages of CNNs, in comparison to other discriminative models,
is that a CNN trained on image classification task is shown to perform reasonably well on other related
tasks such as semantic segmentation, object detection etc. with only minor adaptions
to the new task.

\end{itemize}

Recently, hybrid models combining generative and discriminative models are proposed
to alleviate some of the limitations in both and make use of their complementary
advantages. We will discuss these models in the next section.

\section{Combining Generative and Discriminative Models}
\label{sec:gen-disc-comb}

As we have argued in the previous sections, generative and discriminative
models have complementary advantages and limitations. Generative models have the
advantage of incorporating prior knowledge while being slow; whereas discriminative
models are fast and robust but it is difficult to incorporate prior knowledge.
Typically, generative models suffer from high bias due to model mismatch, whereas
discriminative models suffer from higher variance.
In general, discriminative models work well and are robust to model
mismatch when the available annotated training data is large. If the
available data is small in comparison to the required model complexity, we need
ways to constrain model parameters with the use of prior knowledge. Generative
models provide principled ways to incorporate such prior knowledge and can even
make use of unlabelled data which is generally abundant.
The work of~\cite{jordan2002discriminative} is one of the first comparative studies on generative and
discriminative models resulting in the common knowledge of using discriminative
models when the data is abundant, otherwise use generative approach for
a given problem.

We hypothesize that combining generative and discriminative
models can leverage the advantages in both. At the same time, combining these complementary
models can also bring forward the limitations in both. The generative and discriminative
models are often studied in isolation, but during the past decade,
several synergistic combinations of generative and discriminative models have been proposed.

There are 3 ways in which generative and discriminative models can be combined.
1. Use a generative model to improve the model or the inference in the discriminative model
(indicated as `Generative $\rightarrow $ Discriminative'); 2. Use a discriminative model for
improving the model and/or inference in the generative model (indicated as
`Discriminative $\rightarrow $ Generative'); and 3. Hybrid generative and discriminative
models (indicated as `Generative $\leftrightarrow $ Discriminative').

\subsubsection{Generative $\rightarrow $ Discriminative}

One way to use generative models for improving discriminative models is by
feature extraction using generative models. The work of~\cite{jaakkola1999exploiting} showed that
the gradients of the generative models
can be used as features in discriminative models.
The gradients of a generative model are called `Fisher vectors' and are
particularly useful for building kernel functions (Fisher kernels)
that can be used in kernel based techniques such as SVMs.

Another popular way to incorporate generative prior knowledge
is to provide prior constraints while training discriminative models.
For instance, CNN models can be trained
with extra loss layers encoding the prior relationship between the output
variables. The overall training loss is a combination
of the discriminative prediction loss and also a generative prior loss.
A related strategy for training CNNs is to first train a discriminative CNN
using generative prior loss with large amounts of unlabelled data,
and then fine-tune the network using discriminative prediction loss with
limited labelled data. Instead of training a single discriminative
model with prior constraints,~\cite{tu2010auto} proposed to train a sequence of
discriminative predictors, each taking as input not only the input features
but also features from previous stage predictions. This way, it is easy to
incorporate the prior constraints on output target variables using features
extracted on predictions. This technique is called `Auto-Context'~\cite{tu2010auto} and
the sequence of predictors is usually trained with stacked generalization
method~\cite{wolpert1992stacked}. Despite being simple, auto-context method
is shown to be powerful and useful in many vision problems
(for e.g.,~\cite{tu2010auto,jiang2009efficient,jampani15wacv}).

More recently, structured prediction layers~\cite{ionescu2015matrix,zheng2015conditional,schwing2015fully,chandra2016fast}
are introduced into
discriminative CNN frameworks. These layers are mainly adapted from the models and inference
techniques in generative models. For example, in the case of semantic segmentation
CNNs, mean-field inference in fully-connected CRFs can be formulated as
recurrent neural network modules~\cite{domke2013learning,zheng2015conditional}
and is used to augment the existing
CNN architectures resulting in better performance. The work of~\cite{chandra2016fast}
proposed a way to incorporate Gaussian CRFs into end-to-end trained semantic
segmentation CNNs. In Chapter~\ref{chap:bnn}, we generalize the standard spatial convolutions
in CNNs to sparse high-dimensional filters and show it can be used to incorporate
structured prior knowledge into CNNs resulting in better model and inference.
In Chapter~\ref{chap:binception}, we propose a specialized structured prediction
module to be used in CNNs for dense pixel prediction tasks such as semantic
segmentation.

\subsubsection{Discriminative $\rightarrow $ Generative}

Although generative models provide an elegant formalism to encode prior
knowledge about the problem, their use is mainly hampered by the difficulty in
the posterior inference. Since discriminative models directly characterize the posterior
distribution, they have the potential to be useful for inference in a corresponding
generative model.

Since many of the generative models require approximate
Bayesian inference for estimating the posterior distribution, some components of
the Bayesian inference can be completely replaced with discriminative models.
\textit{Inference machines}~\cite{Ross2011} are a successful example of such technique.
Inference machines pose the message passing inference in a given generative model
as a sequence of computations that can be performed efficiently by training
discriminative models like random forests. Instead of learning complex potential
functions and computing messages between the variables, discriminative predictors
that directly learn to pass the messages are proposed. This technique
is shown to perform well on real world tasks~\cite{Ross2011,ramakrishna2014pose,shapovalov2013spatial}
such as human pose estimation, 3D surface layout estimation and 3D point cloud estimation.
Inference machines help bridging the gap between the message
passing and random forest techniques. Similar technique~\cite{Heess2013} is also shown to be
useful to predict messages for
expectation propagation~\cite{Minka2001} inference in generative models. More recently,
~\cite{lin2015deeply} proposed to use discriminative deep learning models for predicting
messages in message passing inference.

By completely replacing the components of Bayesian inference with discriminative predictors,
we lose the theoretical guarantees from the original inference
techniques. However,
discriminative models can still be used to improve the inference process.
Data driven Markov chain Monte Carlo (DDMCMC)~\cite{tu2002image} methods
leverage discriminative models to speed up the MCMC inference.
DDMCMC methods have been used in image segmentation~\cite{tu2002image}, object
recognition~\cite{zhu2000integrating}, and human pose estimation~\cite{lee2004proposal}.
In Chapters~\ref{chap:infsampler} and~\ref{chap:cmp}, we propose
principal techniques for leveraging discriminative models for Bayesian
inference in inverse graphics and layered graphical models respectively.

Another way of using discriminative models to improve
generative approaches is to use discriminative prediction loss for training
the generative model parameters. This is called `discriminatively training
generative models'~\cite{bouchard2004tradeoff,holub2005discriminative,yakhnenko2005discriminatively}
and is akin to using a generative prior loss for training discriminative models.
Such models are also called hybrid models~\cite{lasserre2006principled} (discussed
more below) if different parameters are used for defining discriminative and
generative models.

\subsubsection{Generative $\leftrightarrow $ Discriminative}

It is possible to define both discriminative and generative models for the same
task and train them together. This synergistic training
can help in a better model fit in both. With
such hybrid models, it is possible to  train with both unlabelled and labelled
data together~\cite{lasserre2006principled,minka2005discriminative}.

Recent advances in deep learning showed that neural network models can also be
used as good approximators for generative models of images
(for e.g.,~\cite{dosovitskiy2015learning,gregor2015draw,theis2015generative}). Thus, it is
possible to define a hybrid model with different neural networks approximating
the corresponding generative and discriminative models for a task, and then train them together.
One popular model in this category is `Auto-encoding variational Bayes'
~\cite{kingma2013auto,rezende2014stochastic}. Here,
a generative model with exponential family distributions is approximated with
a neural network. At the same time, variational Bayesian posterior inference in that model is
approximated with a different neural network. Both generative and discriminative
(inference) networks are trained by minimizing the variational lower bound (Eq.~\ref{eqn:kl}).
The work of ~\cite{eslami2016attend} uses such hybrid models with recurrent neural networks
and an attention mechanism to tackle vision problems involving multiple unknown number of objects in an image.
These models are shown to perform well on small scale vision problems such as
character recognition and are not scaled for tackling mainstream vision problems.
The formulation of such hybrid models is elegant and has potential to be useful
for many vision tasks.

Very recently, in a similar spirit to auto-context,~\cite{saining16} proposes to learn
a top-down CNN for capturing the contextual relationships between the
target variables. The top-down generative CNN learns to predict the target variables
from the surrounding target variables (context).
This top-down CNN is then coupled with original discriminative
CNN to serve as top-down constraints for intermediate CNN representations.
As an advantage over the auto-context framework where different models are learned
at different stages, a single discriminative model is learned and shown to be sufficient.

Hybrid generative and discriminative CNN models are a very active area of
research with different architectures being proposed frequently.
We only discussed a few model architectures here.
It is plausible that
in the near future, hybrid generative and discriminative models dominate
the field of computer vision.

\section{Discussion and Conclusions}

In this chapter, we have discussed various generative and discriminative computer vision models.
Models form the core of any computer vision system,
we choose to discuss some prominent models in relation
to this thesis while highlighting the advantages and limitations in
popular generative and discriminative models.

Generative models are conceptually elegant to incorporate prior knowledge about
the task while their use is mainly hampered by the difficulty of posterior inference.
Discriminative models, on the other hand, are robust to model mismatch while
being fast but require large amount of labelled data and there is a lack of standard
approaches for incorporating prior knowledge into them. Hybrid generative and
discriminative models, discussed in the previous section, try to bridge the gap
between these two complementary approaches.

The main aim of this thesis is to improve inference with in various computer
vision models. In Part I of the thesis, we concentrate on improving inference
in generative vision models. We do this by learning separate discriminative models
and propose algorithms for better inference in prominent generative models in
vision namely inverse graphics models (Chapter~\ref{chap:infsampler})
and layered graphical models (Chapter~\ref{chap:cmp}).

In Part II of the thesis, we concentrate on improving inference in discriminative
vision models. Since inference is simple evaluation of the model in discriminative models, we
propose techniques for modifying the model itself enabling the introduction of
prior knowledge into CNN models. Specifically, we generalize the standard spatial convolutions
in prominent CNN models to sparse high-dimensional filtering (Chapter~\ref{chap:bnn}) and then
propose a neural network approach for propagating information across video frames (Chapter~\ref{chap:vpn}).
In Chapter~\ref{chap:binception}, we propose a new CNN module that can be added to existing
segmentation CNN architectures that helps in image adaptive filtering of intermediate CNN units.
