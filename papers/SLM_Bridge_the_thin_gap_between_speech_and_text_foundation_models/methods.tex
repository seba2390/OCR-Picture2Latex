\section{Model: The Adapter Sandwich}
\label{sec:adapter}
SLM glues a pretrained speech encoder with a pretrained LLM using a simple adapter where the adapter is sandwiched between the two frozen models, as illustrated in the Figure~\ref{fig:model_architecture}. We use SLM to refer to the combination of a pretrained LLM, a pretrained speech encoder, and the adapter. 

SLM supports two input modalities with speech and text inputs. The speech input $S_{1:U}$ of length $U$ is fed into speech encoder which generates speech embedding $S_{1:U}^{D}$ with dimension $D$. The speech encoder is taken from a pretrained encoder-decoder ASR model whose decoder is discarded. The embeddings $S_{1:U}^{D}$ are down-sampled to $S_{1:U'}^{D}$ by about a factor of 4x, as described further in Section \ref{sec:reduction}. This reduction allows longer speech inputs. The down-sampled speech embedding sequence $S_{1:U'}^{D}$ is then fed into an adapter, which in our case is simply a few transformer layers, as few as 2 layers in our experiments.

The text input $X_{1:T}$ of length $T$ is embedded by the embedding layer of the LLM. The text embedding sequence $X_{1:T}^{E}$ is then concatenated along the time dimension with the output sequence from the adapter $S_{1:U'}^{E}$ to get $X_{1:T}^{E} || S_{1:U'}^{E}$ (i.e. {\em ``\{text instruction\} \{audio\}}''). Note that the adapter output dimension $E$ as the text embedding. The concatenated embedding sequence $X_{1:T}^{E} || S_{1:U'}^{E}$ is fed into the rest of the LLM transformer stack.

We use the next-token-prediction loss as the training objective for the adapter, using a mixture of tasks (see Section \ref{sec:data_tasks}). The target sequence can be either speech transcripts, translation sentences, or any open ended generation targets. Intuitively, SLM adapter is trained to implicitly map the reduced speech embedding $S_{1:U'}^{D}$ into the same representation space as text embedding, so that the adapted speech embedding $S_{1:U'}^{E}$ can be ``understood'' by the {\em frozen} LLM. 

By supporting both speech and text inputs in the manner described above, we hypothesize that the text input serves as an effective prompt for speech inputs, allowing the model to follow instructions. In Section~\ref{sec:instruction} we demonstrate several examples tasks, lending credibility to this hypothesis.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig2.pdf}
\caption{\textbf{Model architecture of SLM.} The encodings from the output of the speech encoder is downsampled and adapted to the input textual embedding representation of the frozen LLM.}
\label{fig:model_architecture}
    \vspace*{-.15in}
\end{figure}

\subsection{Speech sequence length reduction}
\label{sec:reduction}

We reduce the speech encodings sequence to similiar length as the corresponding text word-piece sequences. This is important for improving both training and inference efficiency and allows the model to handle long speech inputs. A uniform reduction is applied, where the output sequences are reduced at a fixed rate. In our experiments, we discard 3/4 of the frames randomly and thus reduce speech encoding sequence to only 1/4 of its original length.
