\section{Discussion}
\label{sec:discussion}

\subsection{Adaptation depth}

To gain a deeper understanding of the required adapter depth for successfully integrating pretrained models from speech and language modalities, we varied the number of transformer layers in the adapter from $1$ to $8$. We observed notable performance improvement from 1 to 2 layers, but the performance saturated after 2 layers. This implies that the representation gap between the pretrained speech model and text model might be narrower than expected, and can be bridged by a shallow adaptation from speech encoding to the LLM embedding space. 

We also experiment different adaptation approaches, such as low-rank adaptation~\cite{hu2021lora} or using a more sophisticated Flamingo-style approach~\cite{alayrac2022flamingo} to inject the speech information into the LLM transformer stacks via cross-attention, which will be included in future work.

\subsection{Impact of pretrained LLMs}

We compared different LLM checkpoints in T5 family: mT5~\cite{xue2020mt5}, mT5-LM-adapted~\cite{vu2022overcoming}, mT0-MT~\cite{muennighoff2022crosslingual} (used in this work), T5~\cite{raffel2020exploring}, T5-flan~\cite{chung2022scaling}, and observed that the pretrained LLM plays a crucial role in both training efficiency and model quality after adaptation. We found that those LLMs pretrained with LM objective~\cite{bengio2000neural} (such as mT5-LM-adapted, mT0-MT, T5-flan) require significantly less time to train adaptation compared to LLMs solely pretrained with masked language model objective~\cite{devlin2018bert} (such as mT5 and T5). For example, with the same computational resources, adapting T5 or mT5 takes a few days to converge, while T5-Flan and mT0-mt takes a few hours.

The intrinsic capabilities of pretrained LLMs determine instructions following quality of the trained SLM. For LLMs without zero-shot instruction capabilities (T5 and mT5), the adapted SLM is not able to perform 0-shot instruction following either. When the LLMs have poor performances on certain downstream tasks (such as QA task), the adapted SLM also exhibits poor accuracy on those tasks.

This again confirms that the thin adapter layer itself only provides the transformation from speech modality to text modality, but does not store world-knowledge as in LLMs.

In this work, we only compared different encoder-decoder variants of LLM. However, the proposed adaptation approach also applies to decoder-only LLMs. In future work, we will present a more comprehensive comparison between both encoder-decoder and decoder-only LLMs.

\subsection{Train adapter only v.s finetune LLM}

In previous sections, we presented a general SLM for a wide range of speech tasks without the need of altering the weights of the original speech model or LLMs. In this section, we explore further finetuning SLM on any downstream corpus with LLMs unfrozen. This can be used to tailor to a specific downstream task to achieve optimal quality.

By finetuning SLM adapter using the in-domain contextual biasing training set, the WER decreases from $8.6\%$ to $1.7\%$ compared to the 0-shot case. By allowing LLM encoder unfreezing, the WER further decreases to $1.0\%$, see details in the Table \ref{tab:biasing_results}.

By finetuning SLM on CoVoST2 dataset with the LLM encoder unfrozen, we observed BLEU score increases from $33.0$ to $37.4$, which is on-par with the current SOTA CoVoST2 AST performance from AudioPaLM~\cite{rubenstein2023audiopalm}.

\subsection{End-to-end speech-to-X v.s. cascaded ASR+LLM}

A question that often comes up is whether the end-to-end model has an edge over a cascade pipeline where the speech is fed to an ASR system and the transcripts are send to LLMs. To answer this, we ran ablation studies on AST task, where we applied the same speech and text foundation model as in SLM: USM LAS model for ASR, and mT0-MT for text-to-text translation. Specifically, we prompted the mT0-MT model using a similar instruction {\em ``Translate this from \{src\_lang\} to \{tgt\_lang\}''} as we used in SLM training mixture.

We observed that the cascaded pipeline has significantly worse performance than the end-to-end SLM (i.e., CoVoST2 Fr-to-En BLEU degraded from $38$ to $32$). This is presumably due to ASR errors. One potential approach to improve the cascaded system is to further finetune LLMs on ASR transcripts, which will effectively improve the robustness to ASR errors, but will apparently requires further finetuning and alters the original capabilities of LLMs.

% \pr{I would be super interested in a small discussion on how the learned embeddings of the audio (after feeding through USM + downsampling + adapter) relate to the text tokens. There is an intuition that this method works because the adaptor makes the speech "look like text tokens", and it would be cool to quantify that somehow. For instance, if you just do k-nearest-neighbours against the text-token-embeddings, can you directly recover the text corresponding to the speech in the audio?}
