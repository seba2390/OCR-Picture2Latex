\section{Experiments}
\label{sec:expts}

\subsection{Pretrained Foundation Models}

We used the Universal Speech Model (USM)~\cite{zhang2023google} as our speech foundation model. USM encoder of 2B parameters was first pretrained with BEST-RQ \cite{chiu2022self} and subsequently finetuned with a LAS decoder of 128M parameters on the 75-language YouTube corpus as introduced in \cite{zhang2023google}. 
%\todo{TODO: weihan, model size, 2B, model types: LAS for uniform embedding, language supported: 100+?, vocab size}

We used T5 family~\cite{raffel2020exploring} as the text foundation model, which has the encoder-decoder architecture. Specifically, we adopted mT0-MT XXL checkpoint with 13B model size that was readily available~\cite{muennighoff2022crosslingual}. This model was trained on mC4 corpus~\cite{xue2020mt5} which covers 101 languages with multilingual instruction tuning capability.

\subsection{Training}
\label{sec:train}
The adapter is a transformer stack with $L$ layers, where $L$ is a hyper-parameter. By default we use two layers of transformer as the adapter ($L=2$) unless stated otherwise. We adopt the same transformer implementation as used in mT0-MT XXL. The transformer layer size is also the same as in mT0-MT XXL, where the number of heads is 64, the head dimension is 64, the embedding dimension is 4096, and the projection layer dimension is 10240. This total parameter count of the adapter is 156M. The adapter parameters are learned from scratch.

We used a data mixture of combining all tasks described in the Section \ref{sec:data_tasks}, where all tasks have the unified input-output format:

\begin{itemize}
\itemsep 0in
    \item {\em Text prompt input}, instruction about the task to perform,
    \item {\em Speech input}, content as audio, and
    \item {\em Text response output}, target responses.
\end{itemize}

The mixing ratio is proportional to the number of data samples in each task. We used 250k multilingual sentence piece vocabulary. The training objective is the cross-entropy loss for next token prediction in a sequence, which is the standard language modeling objective. To preserve the capabilities of existing speech and text models, we froze both of the speech and text foundation models during training and only trained the adapter, as mentioned before. 

\subsection{Evaluation}
\label{sec:eval}

We first evaluated our model on conventional speech recognition and translation tasks.  For 0-shot instruction following, we demonstrate quantitative results on a contextual biasing ASR, where we instructed the model with real-time context. Furthermore, we also show empirical studies on 0-shot open-ended question answering tasks. 

\begin{enumerate}
\itemsep 0in
    \item \textit{Speech Recognition}: We evaluated on the test set of SpeechStew ASR~\cite{chan2021speechstew}, VoxPopuli ASR~\cite{wang2021voxpopuli}, and FLEURS ASR~\cite{conneau2023fleurs}. The performance was computed in terms of word error rate (WER) using the JiWER implementation~\cite{morris2004and}.
    % We normalise the text by ignoring capitalisation and punctuation before computing the WER. We also applied Whisper normalization to English recognition results, in order to have a fair comparison with the baseline models. 
    \item \textit{Speech Translation}: We evaluated on CoVoST2 AST task~\cite{wang2020covost} and report BLEU scores on X-to-En test sets using the SacreBLEU and corpusBLEU implementations~\cite{papineni2002bleu, post2018call}.
% We do not perform any normalization to the text before computing BLEU.

    \item \textit{Speech Recognition with Contextual Biasing}: This task evaluates the model's ability on recognizing speech using runtime context (i.e., named entities). We provide the model with real-time retrieved entities in the text prompts. We report WERs on the multi-context TTS corpora in~\cite{munkhdalai2023nam+}, where W\_PREFIX and WO\_PREFIX evaluate the in-domain performance: each utterance is assigned a correct bias entity + distractor entities; ANTI evaluates the out-of-domain performance: each utterance is associated with distractor entities only. The original corpora contains variants scaling from 0 to 3K bias entities assigned to each utterance. For simplicity, we combined the entities across test-set variants and constructed a single retrieval database with 4.55K bias entities in total, and scored each utterance against it. 

    \begin{itemize}
        \itemsep 0in
        \item ANTI: The transcript truths simulate the voice assistant traffic, examples include ``whatâ€™s the weather'', ``turns the lights to 100\%''. 
        \item W\_PREFIX: The transcript truths contain prefixed patterns such as ``open \$APPS'', ``call \$CONTACTS'', ``play \$SONGS''. 
        \item WO\_PREFIX: The transcript truths are entities chosen from \$APPS, \$CONTACTS, \$SONGS.  
    \end{itemize}  
    
\end{enumerate}

