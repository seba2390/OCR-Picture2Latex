\section{Training Data Mixtures}
% \zelin{Please reference the papers regarding the origin of the data-sets or first appeared.}

\label{sec:data_tasks}
SLM was trained using a mixture of supervised learning tasks where the inputs include speech signals, text instructions; and the output is the text string in different tasks such as ASR transcripts and speech translation sentences.
\begin{enumerate}
\itemsep 0in
    \item Speech Recognition: The fixed instruction for this task is {\em ``Recognize this speech in \{lang\}''}, where we replace {\em\{lang\}} with the actual language name for the input speech. We used multilingual YouTube corpus \cite{zhang2023google} for this task which contains 75 languages harvested from YouTube and amounts to 90k hours.
    \item Speech Translation: The model takes a speech input and generates its corresponding translation for a specified target language. The instruction for this task is {\em ``Translate this speech from \{src\_lang\} to \{tgt\_lang\}''}, where we replace {\em\{src\_lang\}} with the actual language name for the input speech, and {\em\{tgt\_lang\}} with the target language name to be translated into. We use CoVoST2 corpus \cite{wang2020covost} for this task, which is a speech to text translation dataset covering translations from 21 languages into English and from English into 15 languages, totaling about 2.9k hours of audio.
    \item Speech Instruction Tuning: The model takes a speech input and a text input as instruction, and predicts an appropriate answer following this instruction. Different from previous tasks using a fixed instruction, this task has varied instructions for different data samples such as dialog generation, named entity recognition and question answering. The task is to train the model to adeptly follow diverse instructions, avoiding over-fitting to any fixed instructions above, which is critical to the success of downstream 0-shot instruction following tasks. We used Alpaca dataset \cite{alpaca}, in which data samples contain {\em \{instruction, input, output\}}, where {\em instruction} describes the task the model should perform, {\em input} is an input context for the task, and {\em output} is the answer to the instruction. The speech input was generated using a TTS system described in ~\cite{JiaTTS2021, shenTTS2021}.
    % Note that only around 40\% data have an input. So we dropped all data samples without input, and the final speech-Alpaca dataset contains around 20K data points.
\end{enumerate}

