% \begin{keywords}
% \end{keywords}
%
\section{Introduction}
\label{sec:intro}
Recent advances in foundation models of text and speech have offered new opportunities to build strong speech-language models without a large amounts of paired speech-text data. Text foundation models have demonstrated impressive capabilities and performance on a wide range of language tasks \cite{gpt4-technical-report, anil2023palm}, and audio foundation models have recently advanced the state-of-the-art in speech recognition and understanding tasks \cite{zhang2023google, radford2023robust}. Developing effective approaches that unify foundation models of both modalities is a natural way of building strong speech understanding models without requiring a large amount of paired speech-text data.
\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{fig1.jpeg}
\caption{SLM consists of a frozen pretrained speech model, a frozen pretrained LLM and an adapter to bridge from speech to textual embeddings. Therefore, SLM extends LLM's instruction-following capabilities beyond text to speech inputs and successfully performs multiple 0-shot tasks.}
\label{fig:model}
    \vspace*{-0.15in}
\end{figure}

In previous work, a joint Speech Language Model (SLM)~\cite{wang2023speech} was introduced using an adapter-based approach~\cite{houlsby_etal-2019} to unify pretrained speech and text models for an end-to-end English dialog understanding task, namely, MultiWoz~\cite{dstc11}. In this work, we refine the proposed SLM using multilingual speech and language foundation models to unlock new multitask and 0-shot capabilities. In contrast to the previous version of SLM, in this work the two foundation models are kept frozen to safeguard their inherent capabilities and an adapter is trained to bridge the two modalities. The adapter takes the output of the speech encoder, applies a uniform subsampling approach to reduce the sequence length, and learns to map the audio representation into the textual representation space that can be interpreted by the frozen LLM. 

The key contributions of this work are:

\begin{itemize}
    \item A lightweight and efficient approach to glue frozen speech and text foundation models with a simple adapter, maximally preserving the native capabilities in the pretrained foundation models.
    \item A robust and generalizable model that achieves strong performance on a variety of speech tasks including ASR, AST and speech biasing.
    \item The proposed system demonstrates novel cross-modality zero-shot instruction-following capabilities, with speech as inputs and text as instructions.
\end{itemize}

We describe our approach and model in Section~\ref{sec:adapter}, the training data and tasks in Section~\ref{sec:data_tasks}, experiment setup in Section~\ref{sec:expts}, illustrate several zero-shot capabilities in Section~\ref{sec:instruction}, and report quantitative results on ASR, AST and biasing tasks in Section~\ref{sec:results}. The implication of our results are discussed in Section~\ref{sec:discussion} and summarized in Section~\ref{sec:conclusions}.

% \pr{Note: could be interesting to compare this at a high level to flamingo. Instead of cross attention, you have adapter + injection into embeddings layer. I guess this is much simpler, which is an advantage.}
% \mq{we can mention that we will compare to flamingo style or other style adapter in future work, but since we are not allowed to publish flamingo, we won't include any results in this writting}

\section{Related Work}

To place this work in the context of the existing literature, we review a few representative models in this realm.
% https://arxiv.org/pdf/2305.11000.pdf
In SpeechGPT~\cite{zhang2023speechgpt}, the speech input is converted into HuBERT~\cite{hsu2021hubert} units which are then treated similar to the text tokens as input to the LLM. The entire model is then fine-tuned on different speech tasks. The capabilities of the model are illustrated using anecdotal examples, but the quantitative effectiveness of their method is unclear since no metrics on benchmark tasks are reported.
The approach of AudioPaLM and AudioLM ~\cite{rubenstein2023audiopalm, borsos2023audiolm} focus on pretraining a multimodal (audio and text) foundation model using an extended vocabulary with audio and text tokens to allow audio generation. In contrast, our work focuses on utilizing two frozen pretrained foundation models.
% https://arxiv.org/pdf/2305.11834.pdf
Pengi~\cite{deshmukh2023pengi} feeds audio into a frozen language model by using the output of a speech encoder, with the encodings being treated as the prefix to the standard text prompt. The focus of Pengi is on acoustic classification of sounds, emotions and music. 
% https://arxiv.org/pdf/2305.05665.pdf
ImageBind~\cite{girdhar2023imagebind} attempts to learn a joint embedding across six modalities including images, text, audio, depth and thermal data. The model expects relevant image data in the input and cannot readily learn cross-modal capabilities, for example, from audio-text paired data. 
% https://arxiv.org/pdf/2305.10790.pdf
Listen, Think, and Understand (LTU)~\cite{gong2023listen} leverages LLM's reasoning capabilities to improve acoustic scene understanding, which was trained on a large audio QA dataset. Instead of acoustic scene understanding, our work focuses on spoken language tasks such as ASR, AST and other zero-shot language tasks.
In AudioToken~\cite{yariv2023audiotoken}, an adapter is used to concatenate acoustic embeddings to text embeddings. Similar to our work, they trained only the adapter while keeping the acoustic encoder and the text-to-image diffusion model frozen. However, the focus of their work is image generation, while this work aims to improve spoken language tasks.
