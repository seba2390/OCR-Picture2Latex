
\section{Results}
\label{sec:results}

\subsection{Speech Recognition}
\label{sec:asr_results}

We present the results of ASR evaluation in Table \ref{tab:asr_results} on an English corpus using SpeechStew~\cite{chan2021speechstew}, as well as on multilingual corpora using Voxpopuli~\cite{wang2021voxpopuli} and FLEURS~\cite{conneau2023fleurs}. The instructions during evaluation are similar to the ones in training (e.g., {\em ``Recognize this speech in \{lang\}''}). Note that, all ASR evaluations are performed on out-of-domain tasks, since we didn't include any training data from SpeechStew, Voxpopuli, or FLEURS in the training mixture. We compare performance of our SLM model to USM baselines\cite{zhang2023google} trained on the same YouTube dataset as used in our training mixture. 

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
    \toprule
        Eval set % USM-CTC
        & USM-LAS  & SLM & SLM-FT \\ \hline
            \midrule

        \multicolumn{4}{c}{English} \\
\hdashline
        Common Voice        %& 14.3 
        & $12.6$ & $10.8$ & $7.5$ \\
        AMI (ihm)           %& 16.0
        & $16.6$ & $18.4$ & $15.4$ \\
        AMI (sdm)           %& 37.8
        & $36.3$ & $40.7$ & $36.9$\\
        Librispeech (clean) %& 5.3
        & $3.2$  &  $4.8$ & $2.6$\\
        Librispeech (other) %& 9.8
        & $5.5$  &  $7.4$ & $5.0$ \\
        Switchboard        % & 10.8
        & $10.6$ & $12.7$ & $10.3$ \\
        Tedium              %& 4.1
        &  $2.9$ &  $3.4$ & $2.9$ \\
        Wall Street Journal %& 6.5
        &  $4.8$ &  $4.4$ & $3.0$ \\
            \midrule
        \multicolumn{4}{c}{Multilingual} \\
            \hdashline
        Voxpopuli  & 13.1 &$14.0$ & $13.0$ \\
        FLEURS &13.3 & $13.8$ & $12.4$ \\
\bottomrule
    \end{tabular}
    \caption{\textbf{Speech recognition results.} For most languages, we computed Word Error Rate (WER \%) after removing capitalization, punctuation and text normalization. For Chinese, Japanese, Thai, Lao, and Burmese character error rate (CER \%) is computed similar to Whisper~\cite{radford2023robust}. Voxpoluli WER is an average of 14 languages. FLEURS WER is an average of 54 languages in Fleurs which are also present in the YouTube corpus. For SpeechStew dataset, Whisper normalization was applied on references and predictions. We also report performance of a fine-tuned SLM (SLM-FT) after training SLM text encoder on YouTube corpus.}
    \label{tab:asr_results}
\vspace{-.1in}
    
\end{table}

\begin{table}
  \label{table:top-level-results}
  %\begin{adjustwidth}{-.5in}{-.5in}  
  \centering
  \begin{tabular}{lc}
    \toprule
    \multirow{1}{*}{Model}     & BLEU$\uparrow$ (X-to-En)  \\
    \midrule
    \midrule
    Whisper \cite{radford2023robust} & $29.1$    \\
    mSLAM-CTC \cite{bapna2022mslam} & $25.2$    \\
    MAESTRO \cite{chen2022maestro} & $25.2$     \\
    USM-M \cite{zhang2023google} & $30.7$ \\
    Mu$^{2}$SLAM \cite{cheng2023mu} & $27.1$  \\
    AudioPaLM-2 \cite{rubenstein2023audiopalm} & $37.8$   \\
    \midrule
    SLM & $33.0$ \\
    SLM-FT & $37.4$ \\
    \bottomrule
  \end{tabular}
  %\end{adjustwidth}
    \caption{\textbf{Speech translation results on CoVoST2 test set.}}
    \label{tab:ast_results}
        \vspace*{-.15in}
\end{table}

\subsection{Speech Translation}
\label{sec:ast_results}

We report speech translation performance on CoVoST2~\cite{wang2020covost} corpus in Table \ref{tab:ast_results}, where the performance is averaged over 21 pairs of X-to-En translation. Here again, the instruction during evaluation and training are the same (e.g., {\em ``Translate this speech from \{src\_lang\} to \{tgt\_lang\}''}). In this case, the response from the model are scored without any normalization.

\subsection{Zero-shot Instruction Following}
\label{sec:instruction}

\subsubsection{Speech Recognition with Contextual Biasing}
\label{sec:biasing_results}
 
We evaluated the contextual biasing ASR as a general speech recognition task using the same instruction to that used in other ASR tasks, as shown in the 2nd column in Table \ref{tab:biasing_results}. Typically, a specific list of phrases is given for each speech utterance. We provided this list in the prompt and instructed the model to pick the most relevant phrase, i.e., {\em ``Recognize this speech in language English using potential mention - \{biasing entity\}''}. 
We used an off-the-shelf speech retriever \cite{wang2023speech} to retrieve top-1 entity mention from the speech, and replace {\em \{biasing entity\}} with it in the prompt above.

In the 0-shot instruction prompt experiment (C-ASR), we observe the SLM model gives about 46.2\% relative ($32.7 \rightarrow 17.6$) performance gain. We also demonstrate that further WER reductions can be achieved by fine-tuning the model parameters on task specific training corpora (C-ASR-FT): e.g., fine-tune adapter only: ($17.6 \rightarrow 7.8$), fine-tune T5 encoder: ($17.6 \rightarrow 5.1$).  

\begin{table}[h]
    \centering
    
    \begin{tabular}{lcccc} 
    \toprule

        Prompt type          & ASR  & C-ASR & \multicolumn{2}{c}{C-ASR-FT} \\
                  &   &  & (Adapter) & (T5-Enc) \\
            \midrule
\midrule
        ANTI       &  $10.3$& $10.4$ & $11.8$ & $11.2$ \\
        W\_PREFIX  &  $14.8$& $8.6$ & $1.7$ & $1.0$ \\
        WO\_PREFIX &  $32.7$& $17.6$ &  $7.8$ & $5.1$\\ 
            \bottomrule

    \end{tabular}
    \caption{\textbf{ASR contextual (C) biasing WERs.} \underline{ASR} corresponds to using the same prompt as training time {\em ``Recognize this speech in language English''}; \underline{C-ASR} corresponds to 0-shot instruction prompt; \underline{C-ASR-FT} corresponds to variants where adapter / T5-Enc model weights are further fine-tuned on task specific training corpora, which consists of 12K synthetic TTS examples with equal coverage on \$APPS, \$CONTACTS and \$SONGS, the carrier phrase patterns of the examples closely match the W\_PREFIX test-set but are combined with non-overlapping named entities for generalization.}
    \label{tab:biasing_results}
\vspace{-.2in}
    
\end{table}


\begin{table*}[h]
\vspace{-.1in}
\centering
\rowcolors{6}{gray!25}{}
\begin{tabular}{ll}
\toprule
\textbf{Instruction} & \textbf{Prediction} \\
    \midrule
\midrule
\multicolumn{2}{l}{\textbf{Audio Content}: I don't feel well since last night, I had headache and I took a pill of Ibuprofen. I went to bed early.}
\\
\hdashline

\hdashline

\multirow{2}{*}{Recognize this speech:} & I don't feel well since last night, I had headache and \\
& I took a pill of Ibuprofen. I went to bed early. \\
What symptoms are mentioned? & Headache. \\
What medicines are mentioned? & Ibuprofen. \\
What happened to this person? & This person is sick. \\
Translate this speech to Chinese: &  \begin{CJK*}{UTF8}{gbsn}从昨晚我就不太舒服，我头疼，我吃了片布洛芬。我很早睡了。 \end{CJK*} \\
\midrule
\rowcolor{white}
\multicolumn{2}{l}{\textbf{Audio Content}: can you open hay day please} \\
\hdashline

\hdashline

Recognize this speech: &  Can you open hi day please? \\
\rowcolor{gray!25} Recognize this speech with potential mentions - &\\ \rowcolor{gray!25}"Hey Ya!", "Happy Day", "Bad Day", "hay day" & Can you open hay day please? \\

\midrule
\rowcolor{white}
\multicolumn{2}{l}{\textbf{Audio Content}: Can I reserve a double-room for 4 nights, for 2 adults and a kid? Also we'd like to add breakfast.} \\
\hdashline

\hdashline
How would you answer this? & Sure, when will you be arriving? \\
How would you answer this if you don't & Sorry we don't have double-room available for 4 nights. Do you \\ \rowcolor{gray!25} have such a room? &   consider a single bed room? \\
\rowcolor{white}
\multirow{3}{*}{Translate this speech to French: } &
Est-ce que je peux réserver une chambre double pour 4 nuits, pour\\ & 2 adultes et un enfant ? \\ \rowcolor{white}& Nous aimerions également ajouter le petit-déjeuner. \\

\midrule
\midrule

\multicolumn{2}{l}{\textbf{Audio Content} (From NQ dataset): Give the formula for the following substance carbonic acid} \\
\hdashline

\hdashline

\rowcolor{white}

How do you answer this?  & H2CO3 / [\textcolor{teal}{Groundtruth: H2CO3 (equivalently OC(OH)2)}] \\

\midrule
\multicolumn{2}{l}{\textbf{Audio Content} (From NQ dataset): The resting stage of the cell cycle is} \\
\hdashline

\hdashline
\rowcolor{white}

How do you answer this?  & The phase where the cell does not divide. / [\textcolor{teal}{Groundtruth:}\\

& \textcolor{teal}{A phase where the cell has left the cycle and has stopped dividing}] \\

\bottomrule
\end{tabular}
\caption{\textbf{Zero-shot instruction following examples.} Given the same speech inputs, SLM can respond differently according to instructions. Particularly, we show predicted answers on audio Natural Question corpus. SLM is able to follow the instruction to answer the spoken question.}
\label{0-shot}
\vspace{-.1in}
\end{table*}


\subsubsection{Open-ended generation}

We prompted SLM with more diverse instructions, ranging from dialog generation, named entities recognition, and question answering (QA). See Table \ref{0-shot} for illustrative samples.

In particular, we tested the speech-based QA capabilities using Natural Question dataset~\cite{kwiatkowski2019natural}, where we verbalized the text questions into spoken versions using TTS, and prompted SLM with the instruction {\em How do you answer this?}

We observe that indeed SLM is capable of following the instruction and answering question. Like in standard LLMs, the freely-generated answers may be hallucinated. To investigate whether the hallucination was introduced from the adaptation process or inherited from the LM, we ran the original mT0-MT language model on the text-based NQ dataset, and found that similar hallucinations were present in mT0-MT for open-ended QA tasks.

\label{sec:open-ended}
