\documentclass[11pt]{article} 
\usepackage{amsfonts,amssymb,amsmath,amsthm,dsfont,graphicx,hyperref,mathrsfs,euscript,bm}
\usepackage[title, titletoc]{appendix}
\usepackage[onehalfspacing]{setspace}
\usepackage{ifthen,caption,subcaption,lscape}
\hypersetup{pdfborder = {0 0 0},colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue}
\usepackage{natbib}
\usepackage[shortlabels]{enumitem}

\usepackage{xcolor}

\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}

\newtheorem{thm}{Theorem}
\newtheorem{coro}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{assumption}{Assumption}

\numberwithin{equation}{section}
%\numberwithin{thm}{section}
%\numberwithin{lem}{section}


%%% REMARK
\theoremstyle{definition}
\newtheorem{remark_tmp}{Remark}[section]
\newenvironment{remark}
	{ \begin{remark_tmp} 	}
	{ 
		%\renewcommand{\qedsymbol}{$\bm{\ell}acksquare$}
		%\medskip\qed 
		\medskip\hfill{\LARGE$\lrcorner$}
		\end{remark_tmp} 
	}

\allowdisplaybreaks[1]

\setlength{\parindent}{1em}

\renewcommand*{\arraystretch}{.6}

% New operators: trace, argmin, argmax, etc
	\DeclareMathOperator*{\argmin}{arg\,min}
	\DeclareMathOperator*{\argmax}{arg\,max}
		
% % % % % % % % % % % % % % Some Notations % % % % % % % % % % % % % %
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
\newcommand{\I}{\mathds{1}}
\newcommand{\cval}{\mathfrak{c}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\u}{\overline{u}}

\def\check{\widecheck}
\def\m{\mathcal}
\def\b{\boldsymbol}
\def\ls{\lesssim}
\def\t{\breve}
\def\d{\mathrm{d}}

\newcommand{\Center}{\theta}
\newcommand{\Scale}{\vartheta}



% % % % % % % % % % % % % % Bold letters % % % % % % % % % % % % % %
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}

\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}
\newcommand{\bGamma}{\boldsymbol{\Gamma}}


\begin{document}

% % % % % % % % % % % % % % Title Page % % % % % % % % % % % % % %
\title{\vspace{-0.0in} Higher-order Refinements of Small Bandwidth Asymptotics for Density-Weighted Average Derivative Estimators\thanks{Prepared for the Conference in Honor of James L. Powell at UC-Berkeley, March 25--26, 2022. We thank the conference participants for their comments. Cattaneo gratefully acknowledges financial support from the National Science Foundation through grants SES-1947805 and DMS-2210561, Jansson gratefully acknowledges financial support from the National Science Foundation through grant SES-1947662, and Masini gratefully acknowledges financial support from the National Science Foundation through grant DMS-2210561.}
\bigskip }
\author{Matias D. Cattaneo\thanks{Department of Operations Research and Financial Engineering, Princeton University.} \and
	    Max H. Farrell\thanks{Booth School of Business, University of Chicago.} \and
	    Michael Jansson\thanks{Department of Economics, UC Berkeley.} \and
	    Ricardo Masini\thanks{Center for Statistics and Machine Learning, Princeton University.}}
\maketitle


\begin{abstract}
    The density weighted average derivative (DWAD) of a regression function is a canonical parameter of interest in economics. Classical first-order large sample distribution theory for kernel-based DWAD estimators relies on tuning parameter restrictions and model assumptions leading to an asymptotic linear representation of the point estimator. Such conditions can be restrictive, and the resulting distributional approximation may not be representative of the underlying sampling distribution of the statistic of interest, in particular not being robust to bandwidth choices. Small bandwidth asymptotics offers an alternative, more general distributional approximation for kernel-based DWAD estimators that allows for, but does not require, asymptotic linearity. The resulting inference procedures based on small bandwidth asymptotics were found to exhibit superior finite sample performance in simulations, but no formal theory justifying that empirical success is available in the literature. Employing Edgeworth expansions, this paper shows that small bandwidth asymptotics lead to inference procedures with demonstrable superior higher-order distributional properties relative to procedures based on asymptotic linear approximations.
\end{abstract}

\textit{Keywords:} density weighted average derivatives, Edgeworth expansions, small bandwidth asymptotics.
\thispagestyle{empty}
\clearpage

\setcounter{page}{1}
\pagestyle{plain}




% % % % % % % % % % % % % % Beginning of the Document % % % % % % % % % % % % % %
\pagestyle{plain}



% % % % % % % % % % % % % % 
% % % % % % % % % % % % % % 
\section{Introduction}

Identification, estimation and inference in the context of two-step semiparametric models has a long tradition in econometrics \citep{Powell_1994_HandbookCh}. Canonical two-step semiparametric parameters are finite dimensional functionals of some other unknown infinite dimensional parameters in the model (e.g., a density or regression function), a leading example being the density weighted average derivative (DWAD) of a regression function \citep{Stoker_1986_ECMA}. This paper seeks to honor the many contributions of Jim Powell to semiparametric theory in econometrics by juxtaposing the higher-order distributional properties of \citet{Powell-Stock-Stoker_1989_ECMA}'s two-step kernel-based DWAD estimator under two alternative large sample approximation regimes: one based on the classical asymptotic linear representation, and the other based on a more general quadratic distributional approximation.\footnote{Jim Powell's contributions to semiparametric theory are numerous. \citet{Honore-Powell_1994_JOE}, \citet{Powell-Stoker_1996_JoE}, \citet{Blundell-Powell_2004_RESTUD}, \citet{AradillasLopez-Honore-Powell_2007_IER}, \citet{Ahn-Ichimura-Powell-Ruud_2018_JBES}, and \citet{Graham-Niu-Powell_2023_JOE} are some of the most closely connected to the our work. These papers employ U-statistics methods for two-step kernel-based estimators similar to those considered herein. See \citet{Powell_2017_JEP} for more discussion and references. }

In a landmark contribution, \citet{Powell-Stock-Stoker_1989_ECMA} proposed a kernel-based DWAD estimator and obtained first-order, asymptotically linear distribution theory employing ideas from the U-statistics literature, along with plug-in standard error estimators, to develop valid inference procedures in large samples. This work sparked a wealth of subsequent developments in the econometrics literature: \citet{Robinson_1995_ECMA} obtained Berry-Esseen bounds, \citet{Powell-Stoker_1996_JoE} considered mean square error expansions, \citet{Nishiyama-Robinson_2000_ECMA,Nishiyama-Robinson_2001_ChBook,Nishiyama-Robinson_2005_ECMA} developed Edgeworth expansions, and \citet{Newey-Hsieh-Robins_2004_Ecma} investigated bias properties, just to mention a few contributions. The two-step semiparametric estimator in this literature employs a preliminary kernel-based estimator of a density function, which requires choosing two main tuning parameters (a bandwidth and a kernel function), and their ``optimal'' choices depend on the goal of interest (e.g., point estimation vs. inference) as well as the features of the underlying data generating process (e.g., smoothness of the unknown density and dimensionality of the covariates).  

Classical first-order distribution theory for kernel-based DWAD estimators has focused on cases where tuning parameter restrictions and model assumptions lead to an asymptotic linear representation of the two-step semiparametric point estimator \citep[][for overviews]{Newey-McFadden_1994_Handbook,Ichimura-Todd_2007_Handbook}, that is, the two-step estimator is approximated by a sample average based on the so-called influence function. This approach can lead to semiparametric efficient inference procedures in large samples, but the implied distributional approximation may not be ``robust'' to tuning parameter choices and/or model features. More specifically, the limiting distribution obtained based on the asymptotic linear representation is invariant to the way that the preliminary nonparametric estimators are constructed, and requires potentially high smoothness levels of the underlying unknown functions and thus the use of higher-order kernels. At its core, asymptotic linear approximations assume away the contribution of additional terms forming the statistic of interest, despite the fact that these terms do contribute to the sampling variability of the two-step semiparametric estimator and, more importantly, do reflect the effect of tuning parameter choices in finite samples.

\citet{Cattaneo-Crump-Jansson_2014a_ET} proposed an alternative distributional approximation for kernel-based DWAD estimators that allows for, but does not require, asymptotic linearity. The key idea is to capture the joint contribution to the sampling distribution of both linear and quadratic terms forming the kernel-based DWAD estimator. To operationalize this idea, \citet{Cattaneo-Crump-Jansson_2014a_ET} introduced an asymptotic experiment where the bandwidth sequence is allowed to vanish at a speed that would render the classical asymptotic linear representation invalid because the quadratic term becomes first order even in large samples, which they termed ``small bandwidth'' asymptotics. This framework was carefully developed to obtain a distributional approximation that explicitly depends on both linear and quadratic terms, thereby forcing a more careful analysis of how the quadratic term contributes to the sampling distribution of the statistic.

Small bandwidth asymptotics inference methods for kernel-based DWAD estimators were found to perform well in simulations \citep{Cattaneo-Crump-Jansson_2010_JASA,Cattaneo-Crump-Jansson_2014a_ET,Cattaneo-Crump-Jansson_2014b_ET}, but no formal justification for its finite sample success is available in the literature. Methodologically, this alternative distributional approximation leads to a new way of conducting inference (e.g., constructing confidence interval estimators) because the original standard error formula proposed by \citet{Powell-Stock-Stoker_1989_ECMA} must be modified to make the asymptotic approximation valid across the full range of allowable bandwidths (including the region where asymptotic linearity fails). Theoretically, however, the empirical success of small bandwidth asymptotics could in principle come from two distinct sources: (i) it could deliver a better distributional approximation to the sampling distribution of the point estimator; or (ii) it could deliver a better distributional approximation to the sampling distribution of the Studentized t-statistic because the standard error formula was modified.

Employing Edgeworth expansions \citep{Bhattacharya-Rao1976_book,Hall1992_book}, this paper shows that the small bandwidth asymptotics approximation framework leads to inference procedures with demonstrable superior higher-order distributional properties relative to procedures based on asymptotic linear approximations. We study both standardized and Studentized t-statistics, under both asymptotic linearity and small bandwidth asymptotic regimes, and show that both standardized and Studentized t-statistics emerging from the small bandwidth regime offer higher-order corrections as measured by the second cummulant underlying their Edgeworth expansions. An immediate implication of our results is that the small bandwidth asymptotic framework delivers both a better distributional approximation (Theorem \ref{thm:EE-standard}, standardized t-statistic) and leads to a better standard error construction (Theorem \ref{thm:EE-student}, Studentized t-statistic). Therefore, our results have both theoretical and practical implications for empirical work in economics, in addition to providing a theory-based explanation for prior simulation-based findings exhibiting better numerical performance of inference procedures constructed using small bandwidth asymptotics relative to those constructed using classical distributional approximations.

The closest antecedent to our work is \citet{Nishiyama-Robinson_2000_ECMA,Nishiyama-Robinson_2001_ChBook}, who also studied Edgeworth expansions for kernel-based DWAD estimators. Their expansions, however, were motivated by the asymptotic linear approximation to the point estimator, and hence can not be used to compare and contrast to the distributional approximation emerging from the alternative small bandwidth asymptotic regime. Therefore, from a technical perspective, this paper also offers novel Edgeworth expansions that allow for different standardization and Studentization schemes, thereby allowing us to plug-and-play when comparing the two competing asymptotic frameworks. More specifically, Theorem \ref{thm:EE-standard} below concerns a generic standardized t-statistic and is proven based on Theorem \ref{App A: General EE Standarized} in the appendix, which may be of independent technical interest due to is generality. Theorem \ref{thm:EE-student} below concerns a more specialized class of Studentized t-statistic because establishing valid Edgeworth expansions is considerably harder when dealing with Studentization.

The idea of employing alternative (more general) asymptotic approximation frameworks that do not enforce asymptotic linearity for two-step semiparametric estimators has also featured in other context such as partially linear series-based, many covariates and many instrument estimation as well as certain network estimation settings \citep{Cattaneo-Jansson-Newey_2018_ET,Cattaneo-Jansson-Newey_2018_JASA,Matsushita-Otsu_2021_Biometrika}, as well as other non-linear two-step semiparametric settings \citep{Cattaneo-Crump-Jansson_2013_JASA,Cattaneo-Jansson_2018_ECMA,Cattaneo-Jansson-Ma_2019_RESTUD}. While our theoretical developments and results focus specifically on the case of kernel-based DWAD estimation, their main conceptual conclusions can be extrapolated to those settings as well. The main takeaway is that employing alternative asymptotic frameworks can deliver improved inference with smaller higher-order distributional approximation errors, thereby offering more robust inference procedures in finite samples. 

The paper continues as follows. Section \ref{sec: Setup} introduces the setup and main assumptions. Section \ref{sec: First-order Distribution Theory} reviews the classical first-order distributional approximation based on asymptotic linearity and the more general small bandwidth distributional approximation, along with their corresponding choices of standard error formulas. Section \ref{sec: Higher-order Distribution Theory} presents the main results of our paper. Section \ref{sec: Conclusion} concludes. The appendix is organized in three parts: Appendix \ref{App A: General EE Standarized} provides a self-contained generic Edgeworth expansion for second-order U-statistics, which may be of independent technical interest, Appendix \ref{App A: Proof for The Standardized Case} gives the proof of Theorem \ref{thm:EE-standard} (standardized t-statistic), and Appendix \ref{App A: Proof for The Studentized Case} gives the proof of Theorem \ref{thm:EE-student} (Studentized t-statistic).

% % % % % % % % % % % % % % 
% % % % % % % % % % % % % % 
\section{Setup and Assumptions}\label{sec: Setup}

Suppose $Z_i=(Y_i, X_i')'$, $i = 1,\dots,n$, is a random sample from the distribution of the random vector $Z=(Y, X')'$, where $Y$ is an outcome variable of interest and $X$ takes value on $\R^d$ with Lebesgue density $f$. We consider the density weighted average derivative of the regression function $g(X) = \E[Y|X]$ given by
\begin{equation*}
    \theta := \E[f(X)\dot{g}(X)],
\end{equation*}
where for any function $a$ we define $\dot{a}(x):= \frac{\partial}{\partial x}a(x)$. To save notation, we also define $e(X) := f(X)g(X)$ and $v(X) := \E[Y^2|X]$. We impose the following conditions on the underlying data generating process. Let $\|\cdot\|$ be the Euclidean norm.

\begin{assumption}\label{A:DGP} $ $
\begin{enumerate}[(a)]
    \item $\E[|Y|^p]<\infty$, for some $p\geq 3$.
    \item $\Sigma := \E[\psi(Z)\psi(Z)']$ is positive definite, where $\psi(Z) := 2\big[\dot{e}(X) -Y\dot{f}(X) - \theta \big]$.
    %\item The underlying measure of $(Y;X')'$ can be written as $\mu_Y \times \mu_X$, where $\mu_Y$ and $\mu_X$ are the Lebesgue measure on $\R$ and $\R^d$ respectively; \textcolor{red}{I don't understand the point of this assumption. Do we really need it?}
    \item $f$ is $(S+1)$ times differentiable, and $f$ and its $(S+1)$ derivatives are bounded, for $2S>d+2$;
    \item $g$ is $(S+1)$ times differentiable and its first three derivatives are bounded;
    \item $e$ and its first $(S+1)$ derivatives are bounded;
    \item $v$ is twice diferentiable, and its first two derivatives are bounded, and $v\dot{f}$ and $\E[|Y|^3|X] f(X)$ are bounded;
    \item $f$, $gf$, $\dot{g}f$ and $vf$ vanish on the boundaries of their convex supports;
    \item Cram√©r Condition:  $\sup\limits_{\nu\in\R^d:\|v\|=1}\limsup\limits_{|t|\to\infty}|\E \exp(\iota t\ell_1/\bar{\sigma}_\nu)|<1$ where $\bar{\sigma}_\nu:=\nu'\Sigma \nu$.
\end{enumerate}
\end{assumption}

Under Assumption \ref{A:DGP} and using integration by parts, the DWAD vector can be expressed as
\begin{equation*}
    \theta = -2\E[Y \dot{f}(X)],
\end{equation*}
which motivates the celebrated plug-in analog estimator of \citet{Powell-Stock-Stoker_1989_ECMA} given by
\begin{equation*}
    \widehat{\theta} = -2\frac{1}{n}\sum_{i=1}^{n} Y_{i} \widehat{\dot{f}}_{i}(X_{i}), \qquad 
    \widehat{f}_{i}(x) = \frac{1}{n-1}\sum_{j=1,j\neq i}^{n} \frac{1}{h^{d}} K\left(\frac{X_{j}-x}{h}\right),
\end{equation*}%
where $\widehat{f}_{i}(\cdot)$ is a ``leave-one-out'' kernel density estimator for kernel function $K:\mathbb{R}^{d}\rightarrow \mathbb{R}$ and positive vanishing (bandwidth) sequence $h$. For the kernel function, we impose the following conditions.

\begin{assumption}\label{A:kernel} $ $
\begin{enumerate}[(a)]
    \item $K$ is even, differentiable, and $\dot{K}$ is bounded;
    \item $\int_{\R^d} \dot{K}(u)\dot{K}(u)' \mathrm{d}u $ is positive definite;
    \item For some $P\geq 2$, 
    \[\int_{\R^d} |K(u)|(1+\|u\|^P) \mathrm{d}u + \int_{\R^d} \|\dot{K}(u)\|(1+\|u\|^2) \mathrm{d}u<\infty\]    
    and
    \[\int_{\R^d} u^a K(u) \mathrm{d}u =\begin{cases} 1, &\text{if } [a] =  0,\\
    0, &\text{if } 0<[a] <P\\
    \mu_a < \infty, &\text{if } [a] =P,
    \end{cases}\]
    where $a\in\Z^d_+$ is a multi-index.\footnote{We employ standard multi-index notation. For $a := (a_1,\dots,a_d)$ we have (i) $[a] := a_1+\dots +a_d$, (ii) $a! := a_1!\dots a_d!$, (iii) $x^a := x_1^{a_1}\dots x_d^{a_d}$ for $x\in\R^d$ and (iv) $q^{(a)}(x)= \frac{\partial^{[a]}q}{\partial^{a_1} x_1\dots \partial^{a_d} x_d}$ for smooth enough $q:\R^d\to\R$.}
\end{enumerate}
\end{assumption}

The estimator $\widehat{\theta}$ can be expressed as a second-order U-statistic with $n$-varying kernel:
\begin{equation}\label{eq:estimator}
    \widehat{\theta} = \binom{n}{2}^{-1}\sum_{i<j}^n U_{ij}, \qquad U_{ij} = -\frac{1}{h^{d+1}}\dot{K}\left(\frac{X_i-X_j}{h}\right)(Y_i-Y_j),
\end{equation}
where $\sum_{i<j}^n$ is shorthand notation for $\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}$.

% % % % % % % % % % % % % % 
% % % % % % % % % % % % % % 
\section{First-order Theory}\label{sec: First-order Distribution Theory}

Before presenting our main results concerning the higher-order distributional properties of different statistics based on $\widehat{\theta}$, we overview conventional and alternative asymptotic distributional approximations, and the variance estimation methods proposed in the literature emerging from those distinct approximation frameworks. Limits are taken as $h\to0$ and $n\to\infty$ unless otherwise noted.

\subsection{Distributional Approximation}

In a landmark contribution, \citet{Powell-Stock-Stoker_1989_ECMA} studied the first-order large sample distributional properties of $\widehat{\theta}$. They showed that, under appropriate restrictions on $h$ and $K$, the estimator $\widehat{\theta}$ is asymptotically linear with (efficient) influence function $\psi(z)$, and thus with semiparametric (efficient) asymptotic variance $\Sigma$. More precisely, \citet{Powell-Stock-Stoker_1989_ECMA} showed that if Assumptions \ref{A:DGP} and \ref{A:kernel} hold,
and if $nh^{2\min(P,S)}\to 0$ and $nh^{d+2}\to \infty $, then
\begin{equation}\label{eq:AL-Distribution}
    \sqrt{n}(\widehat{\theta} - \theta) = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \psi(Z_i) + o_\P(1)
    \rightsquigarrow \mathcal{N}(0, \Sigma).
\end{equation}
This result follows from the $U$-statistic representation in \eqref{eq:estimator} and its Hoeffding decomposition, which gives $\widehat{\theta} = \E[U_{ij}] + \bar{L} + \bar{Q}$, where
\[\bar{L} = \frac{1}{n}\sum_{i=1}^{n} L_i, \qquad L_i = 2(\E[U_{ij}|Z_i] - \E[U_{ij}]),\]
and
\[\bar{Q} = \binom{n}{2}^{-1}\sum_{i<j}^n Q_{ij}, \qquad Q_{ij} = U_{ij} - \E[U_{ij}|Z_i] - \E[U_{ij}|Z_j] + \E[U_{ij}],\]
both mean zero random vectors. Because $\E[U_{ij}] = \theta + O(h^{\min(P,S)})$ and $\bar{Q} = O_\P(n^{-1}h^{-(d+2)/2})$, it follows that
\[\sqrt{n}(\widehat{\theta} - \theta) = \frac{1}{\sqrt{n}}\sum_{i=1}^{n} \Big( \E[U_{ij}|Z_i] - \E[U_{ij}]\Big) + O_\P\Big(\sqrt{n}h^{\min(P,S)} + \frac{1}{\sqrt{n h^{d+2}}}\Big),
\]
from which the asymptotic linear representation based on the (efficient) influence function in \eqref{eq:AL-Distribution} is established upon noting that $\E[\|\bar{L}- \sum_{i=1}^{n} \psi(Z_i)/n\|^2]=O(n^{-1}h)$.

Conceptually, the Hoeffding decomposition and subsequent analysis of each of its terms shows that the estimator admits a bilinear form representation in general, which then is reduced to a sample average approximation by assuming a bandwidth sequence and kernel shape that makes both the misspecification error (smoothing bias) and the variability introduced by $\bar{Q}$ (``quadratic term'' term) negligible in large samples. As a result, provided that such tuning parameter choices are feasible, the estimator will be asymptotically linear.

Asymptotic linearity of a semiparametric estimator has several distinct features that may be considered attractive from a theoretical point of view \citep{Newey_1994_ECMA}. In particular, it is a necessary condition for semiparametric efficiency and it leads to a limiting distribution that is invariant to the choice of the first-step nonparametric estimator entering the two-step semiparametric procedure. However, insisting on asymptotic linearity may also have its drawbacks because it requires several potentially strong assumptions and leads to a large sample theory that may not accurately represent the finite sample behavior of the statistic. In the case of $\widehat{\theta}$, asymptotic linearity requires $P>2$ unless $d=1$, thereby forcing restrictive smoothness conditions ($S\geq P$) and the use of higher-order kernels or similar debiasing techniques \citep[see, e.g.,][and references therein]{Chernozhukov-etal_2022_ECMA}. In addition, classical asymptotic linear theory (whenever valid) leads to a limiting experiment which is invariant to the particular choices of smoothing ($K$) and bandwidth ($h$) tuning parameters involved in the construction of the estimator, and therefore it is unable to ``adapt'' to changes in those choices. As a result, asymptotically linear large sample distribution theory is silent with respect to the impact that tuning parameter choices may have on the finite sample behavior of the two-step semiparametric statistic.

To address the aforementioned limitations with classical asymptotic distribution theory, \citet{Cattaneo-Crump-Jansson_2014a_ET} proposed a more general distributional approximation for kernel-based DWAD estimators that accommodates but does not enforces asymptotic linearity. The core idea is to characterize the joint asymptotic distributional features of both the linear ($\bar{L}$) and quadratic ($\bar{Q}$) terms jointly, and in the process develop an alternative first-order asymptotic theory that accommodates weaker assumptions than those imposed in the classical asymptotically linear distribution theory. Formally, if Assumptions \ref{A:DGP} and \ref{A:kernel} hold,
and if $\min( nh^{d+2},1) nh^{2\min(P,S) }\to 0$ and $n^{2}h^{d}\to \infty$, then
\begin{equation}\label{eq:SB-Distribution}
    (\V[\widehat{\theta}])^{-1/2}(\widehat{\theta}-\theta)\rightsquigarrow\mathcal{N}(0,I),
\end{equation}
where
\[\V[\widehat{\theta}] = \V[\bar{L}] + \V[\bar{Q}], \qquad
  \V[\bar{L}] = \frac{1}{n} \big[ \Sigma + o(1) \big], \qquad
  \V[\bar{Q}] = \binom{n}{2}^{-1} h^{-d-2} \big[ \Delta + o(1) \big],
\]
and $\Delta = 2\E[v({X})f(X)] \int_{\mathbb{R}^{d}}\dot{K}(u) \dot{K}(u)'\text{d}u$.

This more general distributional approximation was developed explicitly in an attempt to better characterize the finite sample behavior of $\widehat{\theta}$. The result in \eqref{eq:SB-Distribution} shows that the conditions on the bandwidth sequence may be
considerably weakened without invalidating the limiting Gaussian distribution, albeit the asymptotic variance formula may change. Importantly, if $nh^{d+2} $ is bounded then $\widehat{\theta}$ is no longer asymptotically linear and its limiting distribution will cease to be invariant with respect to the underlying preliminary nonparametric estimator. In particular, if $nh^{d+2}\to c >0$ then $\widehat{\theta}$ is root-$n$ consistency but not asymptotically linear. In addition, because the bandwidth is allowed to be ``smaller'' than usual, the bias of the estimator is controlled in a different way, removing the need for higher-order kernels. Interestingly, \eqref{eq:SB-Distribution} allows for the point estimator to not even be consistent for $\theta$, for sufficiently small bandwidth sequences.

Beyond the aforementioned technical considerations, the result in \eqref{eq:SB-Distribution} can conceptually be interpreted as a more refined first-order distributional approximation for the standarized statistics $(\V[\widehat{\theta}])^{-1/2}(\widehat{\theta}-\theta)$, which by relying on a quadratic approximation (i.e., capturing the stochastic contributions of both $\bar{L}$ and $\bar{Q}$) it is expected to offer a ``better'' distributional approximation. The idea of standarizing a U-statistic by the joint variance of the linear and quadratic terms underlying its Hoeffding decomposition can be traced back to the original paper of \citet[p. 307]{Hoeffding_1948_IMS}. Furthermore, the asymptotic distribution theory proposed by \citet{Cattaneo-Crump-Jansson_2014a_ET} can be viewed as highlighting the well known trade-off between robustness and efficiency in two-step semiparametric settings: $\widehat{\theta}$ is semiparametric efficient if and only if $nh^{d+2}\to\infty$, while it seems possible to construct more robust inference procedures under considerably weaker conditions that would not be semiparametric efficient. Simulation evidence reported in \citet{Cattaneo-Crump-Jansson_2010_JASA,Cattaneo-Crump-Jansson_2014a_ET,Cattaneo-Crump-Jansson_2014b_ET} corroborated those conceptual interpretations numerically, but no formal justification is available in the literature. Theorem \ref{thm:EE-standard} below will offer the first theoretical result in the literature highlighting specific robustness features of the distributional approximation in \eqref{eq:SB-Distribution} by showing that such approximation has a demonstrably smaller higher-order distributional approximation error.  


\subsection{Variance Estimation}

Based on the asymptotically linear distributional approximation in \eqref{eq:AL-Distribution}, \citet{Powell-Stock-Stoker_1989_ECMA} also proposed the following variance estimator
\begin{equation*}
    \widehat{\Sigma} = \frac{1}{n}\sum_{i=1}^{n}\widehat{L}_{i}\widehat{L}_{i}', \qquad
    \widehat{L}_{i} = 2\Big[ \frac{1}{n-1}\sum_{j=1,j\neq i}^{n} U_{ij} -\widehat{\theta} \Big],
\end{equation*}
and proved its consistency (i.e., $\widehat{\Sigma}\to_\P \Sigma$) under the same bandwidth sequences ($nh^{2\min(P,S)}\to 0$ and $nh^{d+2}\to \infty $) required for asymptotic linearity. This result justifies employing the Studentized statistic
\begin{equation}\label{eq:T-stat-AL}
    \widehat{\Sigma}^{-1/2}\sqrt{n}(\widehat{\theta}-\theta)\rightsquigarrow\mathcal{N}(0,I)
\end{equation}
for inferences purposes, that is, to construct a confidence interval for $\mathbf{\theta}$ and smooth trasformations thereof, or to carry out statistical hypothesis testing in the usual way.

However, motivated by their alternative asymptotic approximation, \citet{Cattaneo-Crump-Jansson_2014a_ET} showed that
\begin{equation*}
    \frac{1}{n}\widehat{\Sigma}=\frac{1}{n}[\Sigma + o_\P(1) ] + 2\binom{n}{2}^{-1} h^{-d-2}[\Delta+o_\P(1)],
\end{equation*}
which implies that the consistency result $\widehat{\Sigma}\to_\P \Sigma$ is valid if and only if $nh^{d+2}\to \infty$; otherwise, $\widehat{\Sigma}$ is in general asymptotically upwards biased relative to $\V[\widehat{\theta}]$ in \eqref{eq:SB-Distribution}. Because $\widehat{\Sigma}$ is asymptotically equivalent to the jackknife variance estimator of $\widehat{\theta}$, \citet{Cattaneo-Crump-Jansson_2014b_ET} also noted that the asymptotic bias of $\widehat{\Sigma}$ is a result of a more generic phenomena underlying jackknife variance estimators studied in \citet{Efron-Stein_1981_AoS}. See also \citet{Matsushita-Otsu_2021_Biometrika} for related discussion. 

To conduct asymptotically valid inference under the more general small bandwidth asymptotic regime, \citet{Cattaneo-Crump-Jansson_2014a_ET} proposed several ``debiased'' variance estimators, including the following
\begin{equation*}
    \widehat{V} = \frac{1}{n} \widehat{\Sigma} - \binom{n}{2}^{-1}h^{-d-2}\widehat{\Delta},\qquad
    \widehat{\Delta} = h^{d+2}\binom{n}{2}^{-1}\sum_{i=1}^{n-1}\sum_{j=i+1}^{n} U_{ij} U_{ij}',
\end{equation*}
%with
%\begin{equation*}
%    \widehat{\Delta} = h^{d+2}\binom{n}{2}^{-1}\sum_{i=1}^{n-1}\sum_{j=i+1}^{n} \widehat{Q}_{ij} \widehat{Q}_{ij}',\qquad
%    \widehat{Q}_{ij} = U_{ij} - \frac{1}{2}\Big( \widehat{L}_{i} + \widehat{L}_{j} \Big) -\mathbf{\widehat{\theta}},
%\end{equation*}
and show that $\widehat{\Delta}\to_\P \Delta$ under the same bandwidth sequences ($nh^{2\min(P,S)}\to 0$ and $n^{2}h^{d}\to \infty$) required for \eqref{eq:SB-Distribution} to hold. The estimator $\widehat{\Delta}$ is asymptotically equivalent to the debiasing procedure proposed in \citet{Efron-Stein_1981_AoS}. This result justifies employing the Studentized statistic
\begin{equation}\label{eq:T-stat-SB}
    \widehat{V}^{-1/2}(\widehat{\theta}-\theta)\rightsquigarrow\mathcal{N}(0,I)
\end{equation}
for more ``robust'' inferences purposes relative to those constructed using \eqref{eq:T-stat-AL}.

Heuristically, robustness manifests in two distinct ways. First, the underlying Gaussian distributional approximation holds under weaker bandwidth restrictions and does not require asymptotic linearity, thereby making the limiting distribution explicitly depend on tuning parameter choices. Second, the new standard error formula $\widehat{V}$ is derived from the more general small bandwidth approximation and make explicit the contribution of terms regarded as higher-order by classical large sample distributional approximations.

While not reproduced here to conserve space, the in-depth Monte Carlo evidence reported in \citet{Cattaneo-Crump-Jansson_2010_JASA,Cattaneo-Crump-Jansson_2014a_ET,Cattaneo-Crump-Jansson_2014b_ET} also showed that employing inference procedures based on \eqref{eq:T-stat-SB} lead to large improvements in terms of ``robustness'' to bandwidth choice and other tuning inputs, when compared to classical asymptotically linear inference procedures based on \eqref{eq:T-stat-AL}. Theorem \ref{thm:EE-student} below will study those two feasible statistics and show formally that the distributional approximation \eqref{eq:T-stat-SB} has demonstrably smaller higher-order errors than the distributional approximation \eqref{eq:T-stat-AL}. 

\section{Higher-order Distribution Theory}\label{sec: Higher-order Distribution Theory}

We present Edgeworth expansions for scalar standarized and studentized statistics based on $\widehat{\theta}_\nu - \theta_\nu$ with $\widehat{\theta}_\nu:=\nu'\widehat{\theta}$ and $\theta_\nu:=\nu'\theta$, where $\nu\in\R^d$ is a fixed non-random vector. Considering scalar statistics substantially simplify the developments and proofs without affecting the main conceptual and theoretical takeaways. The sequence $\Scale$ will first be non-random, thereby allowing us to investigate the role of classical distributional approximations based on asymptotic linearity vis-\`a-vis the more general distributional approximations based on small bandwidth asymptotics for standarized statistics. Then, the sequence $\Scale$ will be taken to be random based on the two alternative variance estimators introduced in the previous section, thereby allowing us to investigate the role of variance estimation on the performance of distributional approximations for Studentized statistics.

\subsection{Distributional Approximation}

Our first theorem offers a valid Edgeworth expansion for the sampling distribution function
\begin{equation*}
    F_\Scale(t) := \P\left[\frac{\widehat{\theta}_\nu - \theta_\nu}{\Scale} \leq t\right], \qquad t\in\R,
\end{equation*}
with precise characterization of the first three cummulants determining the leading errors in distributional approximation of the Studentized statistic. Define the following key quantities:
\begin{align*}
    \beta &:= 2(-1)^P\sum_{[k] = P}\frac{\mu_k}{k!}\E\Big[g(X) \frac{\partial^{k}}{\partial X^{k}}\nu'\dot{f}(X)\Big], \qquad \sigma^2 := \V[\widehat{\theta}_\nu],\\
    \kappa_1 &:= \E[\nu'\psi(Z)^3], \qquad
    \kappa_2 := 4\E[\delta(Z)\dot{\eta}(Z)] - 8 \E[\delta(Z)^2]\theta_\nu + 4\theta_\nu^3,
\end{align*}
where $\delta(Z) := \nu'\psi(Z)/2+\theta_\nu$ and $\eta(Z_2) = \lim_{n\to\infty} \E[\delta(Z_1)\nu'U_{12}|Z_2]$.

\begin{thm}[Standardized] \label{thm:EE-standard}
    Suppose Assumptions \ref{A:DGP} and \ref{A:kernel} hold. If $\sqrt{n}h^P\to 0$ and $nh^{d+2}\to\infty$, then for any positive non-random sequence $\Scale$ such that $\Scale/\sigma\to 1$,
    \[\sup_{t\in \R}\big|F_\Scale(t)- G_\Scale(t)\big| = O(R_n) + o(n^{-1/2})\]
    with
    \[G_\Scale(t) := \Phi(t) -\phi(t)\Big\{\frac{\beta}{\Scale}h^P + \Big(\frac{\sigma^2}{\Scale^2} -1\Big) + \frac{\kappa_1+\kappa_2}{6n^2\Scale^3}(t^2-1)\Big\},\]
    and $R_n := nh^{2P} + \Big(\frac{(\log n)^{3}}{nh^{d+2}}\Big)^{3/2} + \frac{h^{d/3+1}}{nh^{d+2}} + \Big(\frac{h^{d/9+2/3}}{nh^{d+2}}\Big)^{3/2}$, where $\Phi$ and $\phi$ are the c.d.f. and p.d.f. of a standard Gaussian distribution. Furthermore, if $\frac{(\log n)^{3}}{nh^{d+2}}\to 0$, then $R_n = o\left(\sqrt{n} h^P + \tfrac{1}{nh^{d+2}}\right)$.
\end{thm}
This theorem is proven by verifying the high-level conditions of a result in Appendix \ref{App A: General EE Standarized} establishing a valid Edgeworth Expansion for a generic class of U-statistics with $n$-varying kernels, which may be of independent theoretical interest. Specifically, Theorem \ref{thm:general} and its corollary \ref{coro:general} improve on \citet{Jing-Wang_2003_AOS} by allowing for $n$-varying kernels under more general condition suitable for the semiparametric problem of interest herein. Theorem \ref{thm:EE-standard} also improves on \citet[Theorem 1]{Nishiyama-Robinson_2000_ECMA} in two respects: (i) it allows for a generic standardization scheme $\Scale$ instead of their specific choice $\sqrt{\nu'\Sigma\nu/n}$; and (ii) it presents a valid Edgeworth expansion with precise error rates with respect to the bandwidth. These improvements enable us to compare the two different distributional approximations of interest, \eqref{eq:AL-Distribution} vs. \eqref{eq:SB-Distribution}.

The main conclusion in Theorem \ref{thm:EE-standard} follows the expected logic underlying Edgeworth Expansions: $\frac{\beta}{\Scale}h^P$, $\tfrac{\sigma^2}{\Scale^2} - 1$ and $\frac{\kappa_1+\kappa_2}{6n^2\Scale^3}$ capture, respectively, the standardized bias, variance and higher moments of the statistic. Inspection of these terms lead to interesting implications for large sample distribution theory, in particular leading to a sharp contrast between distribution theory based on asymptotic linear representations vis-\`a-vis alternative asymptotics, each with either fixed-bandwidth or leading asymptotic variance standardization. More specifically, we can consider four distinct standarization schemes: from first-order asymptotic linear theory \eqref{eq:AL-Distribution} we have
\[\Scale_\mathtt{AL}^2 := \V[\nu'\bar{L}] = \frac{1}{n}\V[\nu'L_i] \qquad\text{and}\qquad
  \breve{\Scale}_\mathtt{AL}^2 := \frac{1}{n}\nu'\Sigma\nu,\]
while from small bandwidth distribution theory \eqref{eq:SB-Distribution} we have
\[\Scale_\mathtt{SB}^2 := \V[\widehat{\theta}_\nu] = \sigma^2 \qquad\text{and}\qquad
  \breve{\Scale}_\mathtt{SB}^2 := \frac{1}{n}\nu'\Sigma\nu + \binom{n}{2}^{-1} h^{-d-2} \nu'\Delta\nu.\]
The standardizations $\Scale_\mathtt{AL}$ and $\Scale_\mathtt{SB}$ correspond to those constructed using the pre-asymptotic variance of the point estimator, each justified according to the asymptotic regime considered (asymptotic linear and small bandwidth, respectively). In contrast, the standardizations $\breve{\Scale}_\mathtt{AL}$ and $\breve{\Scale}_\mathtt{SB}$ correspond to employing the leading term only in the large sample approximation of the pre-asymptotic variance of the point estimator, again keeping only those terms that are justified by the asymptotic regime considered. That is, $\Scale_\mathtt{AL} = \breve{\Scale}_\mathtt{AL}+o(n^{-1})$ and $\Scale_\mathtt{SB} = \breve{\Scale}_\mathtt{SB}+o(n^{-1})$ under the assumptions of Theorem \ref{thm:EE-standard}. For comparison, \citet[Theorem 1]{Nishiyama-Robinson_2000_ECMA} used $\breve{\Scale}_\mathtt{AL}$.

Employing Theorem \ref{thm:EE-standard} we can now compare the different approaches to standardization and their associated errors generated in the distributional approximation. Firstly, it is easy to see that employing $\breve{\Scale}_\mathtt{AL}$ and $\breve{\Scale}_\mathtt{SB}$ will generate larger distributional approximation errors relative to their pre-asymptotic counterparts, $\Scale_\mathtt{AL}$ and $\Scale_\mathtt{SB}$, respectively. See the proof in the appendix for exact rates, which are not reproduced here to conserve space. The main conceptual message is that one should always employ variance formulas that capture the full variability of the statistic whenever possible, as opposed to employing those that capture only the leading variability in large samples. See \citet{Calonico-Cattaneo-Farrell_2018_JASA,Calonico-Cattaneo-Farrell_2022_Bernoulli} for closely related results in the context of nonparametric kernel-based density and local polynomial regression estimation and inference.

Secondly, and more importantly for our purposes, Theorem \ref{thm:EE-standard} shows that even if the full finite-sample variance of the point estimator is captured for standardization purposes, it is still crucial to incorporate the variability of both the linear and quadratic terms. More precisely, setting $\Scale=\Scale_\mathtt{AL}$ then $\tfrac{\sigma^2}{\Scale^2} - 1 = O(n^{-1}h^{-d-2})$, while setting $\Scale=\Scale_\mathtt{SB}$ implies that $\tfrac{\sigma^2}{\Scale^2} - 1 = 0$. As a consequence, our first main result shows that employing the pre-asymptotic variance of the statistic, which is naturally justified by the more general asymptotic distributional approximation \eqref{eq:SB-Distribution}, leads to the smallest error in the distributional approximation of the sampling distribution of the standardized statistic. This result thus provides theory-based evidence in favor of employing small bandwidth asymptotics for kernel-based DWAD methods whenever the goal is to minimize errors of inference procedures relying on large sample Gaussian approximations.

The methodological implications of our first theoretical result can be illustrated by analyzing the coverage error of standardized confidence intervals. According to Theorem \ref{thm:EE-standard}, for any $\alpha\in(0,1)$, a $100(1-\alpha)\%$ two-sided confidence interval based on asymptotic linearity satisfy 
\begin{equation*}
    \P\Big[ \theta_\nu \in \big[ \widehat{\theta}_\nu \pm \Phi_{1-\alpha/2} \Scale_\mathtt{AL}\big] \Big]
    = 1-\alpha + \frac{\mathfrak{K}_\mathtt{AL}}{nh^{d+2}} 
      + o\big(\sqrt{n}h^{P} + n^{-1}h^{-d-2} + n^{-1/2}\big),
\end{equation*}
where $\Phi_{\alpha} = \Phi^{-1}(\alpha)$, and $\mathfrak{K}_\mathtt{AL} = 2 \Phi_{1-\alpha/2} \phi(1-\alpha/2) n^{-1}h^{-d-2} (\sigma^2 / \Scale_\mathtt{AL}^2 - 1) = O(1+h^2)$, with the exact form of the leading terms described in the appendix. On the other hand, under the conditions in Theorem \ref{thm:EE-standard}, a $100(1-\alpha)\%$ two-sided confidence intervals based on small bandwidth asymptotics satisfy
\begin{equation*}
    \P\Big[ \theta_\nu \in \big[ \widehat{\theta}_\nu \pm \Phi_{1-\alpha/2} \Scale_\mathtt{SB}\big] \Big]
    = 1-\alpha 
      + o\big(\sqrt{n}h^{P} + n^{-1}h^{-d-2} + n^{-1/2}\big),
\end{equation*}
implying a smaller coverage error distortion in large samples.

The above coverage error comparison is conceptually useful, but it does not directly translate to practice because the confidence intervals are infeasible. To complement the results in this section, we consider next the implications of constructing variance estimators and hence study feasible (Studentized) inference procedures.

\subsection{Variance Estimation}

We study the role of Studentization and thus obtain valid Edgeworth expansion for the sampling distribution functions
\begin{equation*}
    F_\mathtt{AL}(t) := \P\left[\frac{\widehat{\theta}_\nu - \theta_\nu}{\widehat{\Scale}_\mathtt{AL}}\leq t\right],\qquad
    \widehat{\Scale}_\mathtt{AL} := \frac{1}{n}\nu'\widehat{\Sigma}\nu
\end{equation*}
and
\begin{equation*}
    F_\mathtt{SB}(t) := \P\left[\frac{\widehat{\theta}_\nu - \theta_\nu}{\widehat{\Scale}_\mathtt{SB}}\leq t\right],\qquad
    \widehat{\Scale}_\mathtt{SB} := \frac{1}{n}\nu'\widehat{\Sigma}\nu - \binom{n}{2}^{-1}h^{-d-2}\nu'\widehat{\Delta}\nu.
\end{equation*}
Crucially, the estimators $\widehat{\Sigma}$ and $\widehat{\Delta}$ target the total variability $n\V[\bar{L}]=\V[L_i]$ and $\binom{n}{2}h^{d+2}\V[\bar{Q}]=h^{d+2}\V[Q_{ij}]$, respectively, and not just their leading quantities $\Sigma$ and $\Delta$. Therefore, in light of the results reported in the previous section, we do not explicitly consider na\"ive plug-in estimators of $\breve{\Scale}_\mathtt{AL}$ and $\breve{\Scale}_\mathtt{SBA}$ such as $\frac{2}{n^2} \sum_{i=1}^n (\nu'[\widehat{\dot{e}}(X_i) -y\widehat{\dot{f}}(X_i) - \widehat{\theta}])^2$ for the former, where $\widehat{\dot{e}}(x)$ and $\widehat{\dot{f}}(x)$ are plug-in nonparametric estimators of $\dot{e}(x)$ and $\dot{f}(x)$, respectively. These alternative Studentization schemes will lead to larger higher-order distributional approximation errors when compared to $\widehat{\Scale}_\mathtt{AL}$ and $\widehat{\Scale}_\mathtt{SB}$.

\begin{thm}[Studentized]\label{thm:EE-student}
    Suppose Assumptions \ref{A:DGP} and \ref{A:kernel} hold with $p\geq8$. If $\sqrt{n}h^P\to 0$ and $nh^{d+2}/(\log n)^9\to\infty$, then
    \[\sup_{t\in \R}\big|F_\mathtt{AL}(t)- G_\mathtt{AL}(t)\big| = o(r_n)\]
    with
    \[G_\mathtt{AL}(t) := \Phi(t) -\phi(t)\Big\{\frac{\sqrt{n}h^P\beta}{\nu'\Sigma\nu} - \frac{1}{nh^{d+2}} \frac{\nu'\Delta\nu}{\nu'\Sigma\nu} t - \frac{1}{\sqrt{n} 6(\nu'\Sigma\nu)^3}\Big[\kappa_1 (2t^2+1) + \kappa_2 (t^2+1)\Big]\Big\},\]
    and
    \[\sup_{t\in \R}\big|F_\mathtt{SB}(t)- G_\mathtt{SB}(t)\big| = o(r_n)\]
    with
    \[G_\mathtt{SB}(t) := \Phi(t) -\phi(t)\Big\{\frac{\sqrt{n}h^P\beta}{\nu'\Sigma\nu} - \frac{1}{\sqrt{n} 6(\nu'\Sigma\nu)^3}\Big[\kappa_1 (2t^2+1) + \kappa_2 (t^2+1)\Big]\Big\},\]
    where $r_n := \sqrt{n}h^{P} + n^{-1}h^{-d-2} + n^{-1/2}$
\end{thm}

This theorem shows that employing Studentization based on small bandwidth asymptotics offers demonstrable improvements in terms of distributional approximations for the resulting feasible \textit{t}-test. The main practical implication of our second result can again be illustrated by analyzing the coverage error of Studentized confidence intervals. According to Theorem \ref{thm:EE-student}, and as it was the case for stdentized confidence intervals, a $100(1-\alpha)\%$ two-sided confidence intervals based on asymptotic linearity satisfy
\begin{equation*}
    \P\Big[ \theta_\nu \in \big[ \widehat{\theta}_\nu \pm \Phi_{1-\alpha/2} \widehat{\Scale}_\mathtt{AL}\big] \Big]
    = 1-\alpha + \frac{1}{nh^{d+2}} 2 \Phi_{1-\alpha/2} \phi(1-\alpha/2) \frac{\nu'\Delta\nu}{\nu'\Sigma\nu} + o(r_n),
\end{equation*}
while, under the conditions in Theorem \ref{thm:EE-student}, a $100(1-\alpha)\%$ two-sided confidence intervals based on small bandwidth asymptotics satisfy
\begin{equation*}
    \P\Big[ \theta_\nu \in \big[ \widehat{\theta}_\nu \pm \Phi_{1-\alpha/2} \widehat{\Scale}_\mathtt{SB}\big] \Big]
    = 1-\alpha + o(r_n),
\end{equation*}
implying a smaller coverage error distortion in large samples. This result provides a theoretical justification to the simulation evidence reported in \citet{Cattaneo-Crump-Jansson_2014a_ET,Cattaneo-Crump-Jansson_2014b_ET,Cattaneo-Crump-Jansson_2010_JASA} where feasible confidence intervals based on small bandwidth asymptotics were shown to offer better finite sample performance in terms of coverage error than their counterparts based classical asymptotic linear approximations.

\section{Conclusion}\label{sec: Conclusion}

Employing Edgeworth expansions, we study the higher-order properties of two alternative first-order distributional approximations and their associated inference procedures (e.g., confidence intervals) for the kernel-based DWAD estimator of \citet{Powell-Stock-Stoker_1989_ECMA}. We showed that small bandwidth asymptotics not only give demonstrable better distributional approximations than asymptotic linear approximations, but also justify employing a variance estimator for Studentization purposes that also improves the distributional approximation. The main take away from our results is that in two-step semiparametric settings and related problems, alternative asymptotic approximations that capture higher-order terms ignored by classic asymptotic linear approximation can deliver better distributional approximations and, by implication, better inference procedures with improved performance in finite samples.

While beyond the scope of this paper, it would be of interest to develop analogous Edgeworth expansions for non-linear two-step semiparamtric procedures developed using alternative asymptotic approximations and resampling methods \citep{Cattaneo-Crump-Jansson_2013_JASA,Cattaneo-Jansson_2018_ECMA,Cattaneo-Jansson-Ma_2019_RESTUD}. For the special case of kernel-based DWAD estimators (a linear two-step kernle-based semiparametric estimator), \citet{Nishiyama-Robinson_2005_ECMA} present results that could be contrasted with those obtained under under small bandwidth asymptotics \citep{Cattaneo-Crump-Jansson_2014b_ET}. We relegate such developments for future research due to the substantial amount of additional technical work required.


%\newpage
\appendix

\numberwithin{equation}{section}
\numberwithin{assumption}{section}
\numberwithin{lem}{section}
\numberwithin{thm}{section}
\numberwithin{coro}{section}


\section{Edgeworth Expansion for Second-Order U-Statistic}\label{App A: General EE Standarized}

Consider the sequence of maps $(u_n: \R^d\times\R^d \to\R, n\in \N)$ where $u:=u_n$ is symmetric in terms of the permutation of its two arguments for every $n\in\N$. Given a random sample $Z_1,\dots, Z_n$ for $n\geq 2$ of the random variable $Z$ taking values on $\R^d$, the object of interest in the second order U-statistics with an $n$-varying kernel given by
\begin{equation}\label{E:U-statistic}
    \bar{U} :=\binom{n}{2}^{-1}\sum_{1\leq i< j\leq n}^n u(Z_i,Z_j).
\end{equation}
We drop the subscript $n$ to simplify notation. By the Hoeffding decomposition,
\[\frac{\bar{U}-\Center}{\Scale} = B + L + Q,\]
where $B:= (\E u(Z_1,Z_2)-\Center)/\Scale$, $L:= \frac{1}{\Scale n}\sum_{i=1}^n\ell_i$ and $Q:= \frac{1}{\Scale}\binom{n}{2}^{-1}\sum_{1\leq i< j\leq n}^n q_{ij}$, where $\ell_i:=\ell(Z_i)$ and $q_{ij}:=q(Z_i,Z_j)$ with $\ell(Z_1) :=  2[\E u(Z_1,Z_2|Z_1) - \E u(Z_1,Z_2)]$ and $q(Z_1,Z_2) := u(Z_1,Z_2) -\ell(Z_1)/2 - \ell(Z_2)/2 - \E u(Z_1,Z_2)$. Given the decomposition above,
\begin{equation}\label{E:variance_decomposition}
    \sigma^2:= \V[\bar{U}]= \frac{1}{n} \sigma_{\ell}^2 + \binom{n}{2}^{-1}\sigma_{q}^2,
\end{equation}
where $\sigma_{\ell}^2:=\E \ell^2_1$ and $\sigma_{q}^2:=\E q^2_{12}$.

We establish a valid third-order Edgeworth expansion for the the sampling distribution of the centered and standardized version of $\bar{U}$:
\begin{equation}
    F(t):=\P\left[\frac{\bar{U}-\Center}{\Scale}\leq t\right],\qquad t\in\R,
\end{equation}
where $\Center\in\R$ and $\Scale>0$ are non-random.

\begin{thm}\label{thm:general} Let the following conditions hold:
    \begin{enumerate}[(a)]
        \item $\E\big[(\ell_1/\sigma_\ell)^3\big]= O(1)$ and $\E[|q_{12}|]^{2+\delta}<\infty$, and $\sigma_\ell>0$ 

        \item $\frac{\sigma_{q}}{\sqrt{n}\sigma_{\ell}} \to 0$ and $\frac{\sigma}{\Scale}\to 1$.

        \item $\limsup\limits_{n\to\infty}\limsup\limits_{|t|\to\infty}|\E \exp(\iota t\ell_1/\sigma_\ell)|<1$.

    \end{enumerate}
    Then, $\sup_{t\in\R}\left|F(t) - G(t)\right|=O(\mathcal{E})+ o(n^{-1/2})$ where $G$ is the distribution function with characteristic function
\begin{align*}
\chi_{G}(t):= e^{\iota tB-\tfrac{t^2}{2}}\left[ 1 + \sum_{j=2}^9 \left(\iota t\right)^j\gamma_j \right],
\end{align*}
with $\iota := \sqrt{-1}$,
\begin{align*}
    \gamma_2&=\tfrac{1}{2}\left(\tfrac{\sigma^2}{\Scale^2}-1\right),\quad \gamma_3= \tfrac{1}{6\Scale^3n^2}\left[\E \ell_1^3 + 6\E\ell_1\ell_2q_{12} \right],\quad \gamma_4 = \tfrac{1}{4\Scale^2}\left(\tfrac{\sigma^2}
{\Scale^2}-1\right)\binom{n}{2}^{-1} \sigma_q^2\\
    \gamma_5 &= \tfrac{1}{12 n^2\Scale^5}\left[\binom{n}{2}^{-1}(\E\ell_1^3 )\sigma_q^2 + 6\Scale^2\left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\E\ell_1\ell_2q_{12}\right] \\
    \gamma_6 &= \tfrac{1}{6\Scale^6n^4}\left[(\E\ell_1^3)\E\ell_1\ell_2q_{12} + 12\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^ 2\right],\quad \gamma_7=0,\\
\gamma_8 &=\tfrac{1}{4\Scale^6n^4}\left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^2, \quad
\gamma_9 = \tfrac{1}{12\Scale^9n^6}\binom{n}{2}^{-2}\binom{n}{4} \E\ell_1^3\left[\E\ell_1\ell_2q_{12}\right]^ 2,
\end{align*}
    and
    \begin{align*}
    \mathcal{E} &:=\left(\tfrac{\log n}{ n^{3/2}\sigma_\ell}\right)^{2+\delta}\Pi_{2+\delta}(n) +\left(\tfrac{(\log n)^{\frac{4+\delta}{2+\delta}}\sigma_q^2}{n\sigma_\ell^2}\right)^{\tfrac{2+\delta}{2}}+\left(\tfrac{\log n}{n\sigma_\ell}\right)^{2+\delta}\Pi_{2+\delta}(\log n)\\
    &\qquad + \tfrac{1}{\sigma_\ell^4 n }\E|\ell_1^2\ell_2q_{12}| + \tfrac{1}{\sigma_\ell^5n^{3/2}}\E|\ell_1^2\ell_2^2q_{12}| + \tfrac{1}{\sigma_\ell^2 n^2}\E|\ell_1 q_{12}^2| + \tfrac{1}{\sigma_\ell^5 n^{3/2}}\E |\ell_1\ell_2\ell_3q_{13}q_{23}| \\ &\qquad + \tfrac{1}{\sigma_\ell^7n^{3/2}}(\E\ell_1\ell_2q_{12})(\E|\ell_1^2\ell_2q_{12}|)+ \tfrac{1}{\sigma_\ell^8n^2}(\E\ell_1\ell_2q_{12})(\E|\ell_1^2\ell_2^2q_{12}|),
    \end{align*}
    with $\Pi_{2+\delta}(m) :=\E|\sum_{i=1}^{[m]-1}\sum_{j=i+1}^n q_{ij}|^{2+\delta}$ for real $m>1$ and $[\cdot]$ denoting the floor operator.
\end{thm}

% DO NOT DELETE FOR NOW
% DUMMY
%     \begin{align*}
%     \chi_G(t)
%     &:=e^{itB-\tfrac{t^2}{2}}\left[1 + \frac{(\iota t)^2}{2}\left(\tfrac{\sigma^2}{\Scale^2 n}-1\right) +\frac{(\iota t)^3}{6\Scale^3n^2}\E \ell_1^3 \right] \\
%     &\qquad \times\Big[1 - \frac{4\iota t}{\Scale^3 n^3}\E\ell_1q_{12}^2 +\frac{(\iota t)^3}{\Scale^3n^2}\E \ell_1\ell_2q_{12}\\
%     &\qquad \qquad + (\iota t)^4 \left(\frac{2\E\ell_1^2\ell_2q_{12} + 4\E\ell_1\ell_2q_{13}q_{23}}{2\Scale^4n^3}  
%                    + \frac{\sigma_q^2 \left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)}{2\Scale^2 n^2} \right)\\
%     &\qquad\qquad  + (\iota t)^5\left(\frac{(\E\ell_1^2\ell_2q_{13}q_{23}) + (\sigma_q^2\E \ell_1^3)}{\Scale^5n^4}
%                    -\frac{\big(\E\ell_1\ell_2q_{12}\big)\big(\E\ell_1^2\ell_2q_{12} \big)}{\Scale^7n^5} \right)\\
%     &\qquad\qquad  +\frac{(\iota t)^6}{2\Scale^6 n^4} (\E\ell_1\ell_2q_{12})^2 \Big],
%     \end{align*}

\begin{coro}\label{coro:general}
    Let the assumptions of Theorem \ref{thm:general} hold. If $B\to 0$, then
    \[\sup_{t\in\R}\left|F(t) - G(t)\right| = O\left( B^2 + \mathcal{E} \right) + o(n^ {-1/2}),\]
    with 
\[
\chi_{G}(t):= e^{-\tfrac{t^2}{2}}\left[ 1 + B\iota t  + \sum_{j=2}^9 \left(\tfrac{\iota t}{\Scale}\right)^j\gamma_j \right].
\]
%  
\end{coro}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{align*}
%     \chi_G(t)&=e^{-\tfrac{t^2}{2}}\left[1 +\iota tB  +\frac{(\iota t)^2}{2} \left(\frac{\sigma^2}{\Scale^2 }-1\right)+ \frac{(\iota t)^3}{6\Scale^3n^2}\E \ell_1^3 \right] \\
%     &\qquad \times\left[1 - \frac{4\iota t}{\Scale^3 n^3}\E\ell_1q_{12}^2  +\frac{(\iota t)^3}{\Scale^3n^2}\E \ell_1\ell_2q_{12}  +\frac{(\iota t)^4}{\Scale^4n^3}\left(\E\ell_1^2\ell_2q_{12} + 2\E\ell_1\ell_2q_{13}q_{23} \right) \right.\\
%     &\qquad\qquad \left.  + (\iota t)^5\left(\frac{\E\ell_1^2\ell_2q_{13}q_{23}}{\Scale^5n^4}  -\frac{\big(\E\ell_1\ell_2q_{12}\big)\big(\E\ell_1^2\ell_2q_{12} \big)}{\Scale^7n^5} \right)+\frac{(\iota t)^6}{2\Scale^6 n^4} (\E\ell_1\ell_2q_{12})^2\right].
%     \end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{remark}
    Lemma \ref{L:Q_Lp} below gives the following simpler bound
    \[\Pi_{2+\delta}(m) \ls  (n m\sigma_q^2)^{(2+\delta)/2}\lor mn^{1+\delta/2}\E\big[(\E (q_{12}^2|Z_1))^{1+\delta/2}\big]\lor nm\E|q_{12}|^{2+\delta},\]
    where $\ls$ denotes bounded up to a fixed constant, and $a\lor b=\max\{a,b\}$.
\end{remark} 

\begin{remark}
    We can invert the characteristic function above to obtain a close form for $F$ using the fact that for non-negative integer $k$, $\frac{1}{2\pi}\int_\R \exp{(-\iota tx-t^2/2)} (\iota t)^{k} \d t = H_k(x)\phi(x)$, where $H_k(x)$ is the $k$-th order Hermite polynomial (e.g., $H_0(k)=1$, $H_1(x) = x$, $H_2(x) = x^2-1$, $H_3(x) = x^3 - 3x$). Therefore, the distribution function of $\chi_G(t)$ from Corollary \ref{coro:general} is
    \[G(x) = \Phi(x) - \phi(x)\left[\sum_{j=1}^9 \gamma_j H_{j-1}(x)\right].\]
\end{remark}

\begin{remark}
    To compare to \citet{Jing-Wang_2003_AOS}, let $u(\cdot,\cdot)$ not dependent on $n$, $\Center=\E u(Z_1,Z_2)$, $\Scale^2 =\sigma_\ell^2/n$, and $\E|q_{12}|^{2+\delta}$ bounded. Then, $\mathcal{E}=o(n^{-1/2})$ and $\chi_G(t)=\exp(-t^2/2)\left(1 -\tfrac{\iota \kappa_3t^3}{6\sqrt{n}} \right) + o(n^{-1/2})$, giving
    \[G(x) = \Phi(x) - \phi(x)\frac{1}{6\sqrt{n}}\left[\E\left(\tfrac{\ell_i}{\sigma_\ell}\right)^3 +\frac{6\E \ell_1\ell_2q_{12}}{\sigma_\ell^3}\right](x^2-1).\]
\end{remark}

\subsection{Proof of Theorem \ref{thm:general}}

Let $\chi_F$ denote the characteristic function $F$ and $g$ be the density of $G$.  Using the well-known ``smoothing inequality'' \citep{Bhattacharya-Rao1976_book,Hall1992_book}, we write
\[
 \rho(F,G)  \leq\frac{1}{\pi}\left[ \int_{-\upsilon}^\upsilon\left |\frac{\chi_F(t)-\chi_G(t)}{t}\right|\d t +\frac{24\sup_{x\in\R}|g(x)|}{\upsilon}\right],\qquad \upsilon>0
\]
where $\rho$ is the Kolmogorov distance. We set $v=\sqrt{n}\log n$ and split the range of integration into ``low'' frequencies  and ``high'' frequencies. By the triangle inequality,
\begin{equation}\label{E:inter_bond}
 \rho(F,G) \ls I_1 + I_2 + I_3 + I_4 +\tfrac{1}{\sqrt{n}\log n},
\end{equation}
where
\begin{align*}
I_1&:=\int_{|t|\leq\log n}\left |\frac{\chi_F(t)-\chi_G(t)}{t}\right|\d t, \qquad
I_2:=\int_{\log n<|t|\leq c \sqrt{n}}\left |\frac{\chi_F(t)}{t}\right|\d t, \\
I_3&:=\int_{c \sqrt{n}<|t|\leq \sqrt{n}\log n}\left |\frac{\chi_F(t)}{t}\right|\d t, \qquad
I_4:=\int_{|t|>\log n}\left |\frac{\chi_G(t)}{t}\right|\d t;
\end{align*}
Moreover, $c>0$ is a  fixed constant to be specified later. 

We now bound each of these integrals in turn. We use extensively the fact hat 
 
 \begin{equation}\label{E:exp_expansion}
\left|\exp(\iota x) - \sum_{j=0}^2\frac{(\iota x)^j}{j!} \right|\leq  |x|^{2+\delta}\quad , \quad \forall \delta\in[0,1].
\end{equation}
Also, define for $\psi(t) := \E \exp(\iota t\ell_1)$ for $t\in\R$ where $\sigma_\ell$ is positive by Assumption (a).

\subsubsection*{Bound for $I_1$}

We start by decomposing $\chi_F(t) =\E\exp\big[\iota t (\frac{\bar{U}-\theta}{\Scale})\big]=\exp(\iota t b)\chi_{L+Q}(t)$ where $\chi_{L+Q}(t) := \E\exp (\iota tL) \exp (\iota tQ)$. Use  \eqref{E:exp_expansion} to expand the second exponential in $\chi_{L+Q}(t)$ to write
\begin{equation}\label{E:first_expansion}
\chi_{L+Q}(t) = \E\exp (\iota tL)[1 + \iota tQ -\tfrac{1}{2}(tQ)^2 + O((tQ)^{2+\delta})].
\end{equation}
Since $\ell_1,\dots,\ell_n$ is a i.i.d sequence (for a given $n\geq 2$), the first term in \eqref{E:first_expansion} can be written as
\begin{align*}
\E\exp (\iota tL) = \E\exp \left(\frac{\iota t}{\Scale n}\sum_{i=1}^n \ell_i\right)= \E\prod_{i=1}^n\exp \left(\frac{\iota t\ell_i}{\Scale n}\right)= \prod_{i=1}^n \E\exp \left(\frac{\iota t\ell_i}{\Scale n}\right)= \psi^n\left(\frac{t}{\Scale n}\right).
\end{align*}
For the second term in \eqref{E:first_expansion}, we have
\begin{align*}
\E\exp (\iota tL)\iota tQ &= \frac{\iota t}{\Scale}\binom{n}{2}^{-1}\sum_{i<j}\E\prod_{k=1}^n\exp \left(\frac{\iota t}{\Scale n}\ell_k\right)q_{ij}\\
&= \frac{\iota t}{\Scale}\binom{n}{2}^{-1}\sum_{i<j}\E\prod_{k\neq i,j}^n\exp \left(\frac{\iota t}{\Scale n}\ell_k\right)\exp\left(\frac{\iota t}{\Scale n}(\ell_i+\ell_j)\right)q_{ij}\\
&= \frac{\iota t}{\Scale}\binom{n}{2}^{-1}\sum_{i<j}\prod_{k\neq i,j}^n\E\exp \left(\frac{\iota t}{\Scale n}\ell_k\right)\E\exp\left(\frac{\iota t}{\Scale n}(\ell_i+\ell_j)\right)q_{ij}\\
&= \frac{\iota t}{\Scale}\psi^{n-2}\left(\tfrac{t}{\Scale n}\right)\E\exp\left(\frac{\iota t}{\Scale n}(\ell_1+\ell_2)\right)q_{12}.
\end{align*}
Similarly, for the third term in \eqref{E:first_expansion}, we use
\begin{align*}
\E\exp (\iota tL)(\iota tQ)^2 &= \left[\frac{\iota t}{\Scale}\binom{n}{2}^{-1}\right]^{2}\times\left[\sum_{i<j}\E\prod_{k=1}^n\exp \left(\frac{\iota t}{\Scale n}\ell_k\right)q_{ij}^2\right. \\
&\qquad +\sum_{i<j=k<l}\E\prod_{m\neq i,j,l}^n\exp \left(\frac{\iota t}{\Scale n}\ell_m\right)q_{ij}q_{jl}\\
&\qquad  \left.+\sum_{i<j<k<l}\E\prod_{m\neq i,j,k,l}^n\exp \left(\frac{\iota t}{\Scale n}\ell_m\right)q_{ij}q_{kl}\right]\\
&= \left[\frac{\iota t}{\Scale}\binom{n}{2}^{-1}\right]^{2}\times\left[\psi^{n-2}\left(\tfrac{t}{\Scale n}\right)\binom{n}{2}\E\exp\left(\frac{\iota t}{\Scale n}(\ell_1 +\ell_2)\right)q_{12}^2\right.\\
&\qquad +\psi^{n-3}\left(\tfrac{t}{\Scale n}\right)\binom{n}{3}\E\exp\left(\frac{\iota t}{\Scale n}(\ell_1 +\ell_2+\ell_3)\right)q_{12}q_{23}\\
&\qquad  \left.+\psi^{n-4}\left(\tfrac{t}{\Scale n}\right)\binom{n}{4}\left(\E\exp\left(\frac{\iota t}{\Scale n}(\ell_1 +\ell_2)\right)q_{12}\right)^2\right].
\end{align*}

For the last in \eqref{E:first_expansion}, we have
\[|\E\exp (\iota tL)(tQ)^{2+\delta}|\leq \E|tQ|^{2+\delta}
  = \left[\frac{|t|}{\Scale}\binom{n}{2}^{-1}\right]^{2+\delta}\E\Big|\sum_{i<j}q_{ij} \Big|^{2+\delta}
  = O\left( \left(\frac{|t|}{\Scale n^2}\right)^{2+\delta}\Pi_{2+\delta}(n)\right).
\]

Using the last four displays, we simplify \eqref{E:first_expansion} to
\begin{align}\label{E:second_expansion}
\chi_{L+Q}(t) &= \psi^n\left(\tfrac{t}{\Scale n}\right) \nonumber\\
&\qquad +\psi^{n-2}\left(\tfrac{t}{\Scale n}\right)\left[\frac{\iota t}{\Scale}\E \exp(\tfrac{\iota t}{\Scale n}(\ell_1+\ell_2))q_{12} +\frac{(it)^2}{2\Scale^2} \binom{n}{2}^{-1}\E \exp(\tfrac{\iota t}{\Scale n}(\ell_1+\ell_2))q_{12}^ 2\right] \nonumber\\
&\qquad + \tfrac{1}{2}\left[\tfrac{\iota t}{\Scale}\binom{n}{2}^{-1}\right]^{2}\psi^{n-3}\left(\tfrac{t}{\Scale n}\right)\binom{n}{3}\E \exp(\tfrac{\iota t}{\Scale n}(\ell_1+\ell_2+\ell_3))q_{13}q_{23} \nonumber\\
&\qquad + \tfrac{1}{2}\left[\tfrac{\iota t}{\Scale}\binom{n}{2}^{-1}\right]^{2}\psi^{n-4}\left(\tfrac{t}{\Scale n}\right)\binom{n}{4} \left[\E \exp(\tfrac{\iota t}{\Scale n}(\ell_1+\ell_2))q_{12}\right]^ 2 \nonumber\\
&\qquad  +  O\left[ \left(\tfrac{|t|}{\Scale n^2}\right)^{2+\delta}\Pi_{2+\delta}(n)\right].
\end{align}

We now expand the exponentials inside the expectation and collect terms. For notation brevity, write $a:=\tfrac{\iota t}{\Scale n}$. For the first one, we have
\begin{align*}
 \E \exp(a(\ell_1 + \ell_2))q_{12} &=\E \big(\exp(a\ell_1)-1\big)\big(\exp(a\ell_2)-1\big)q_{12}\\
&=\E\left[\big(\exp(a\ell_1)-1-a\ell_1\big)\big(\exp(a\ell_2)-1-a\ell_2\big)q_{12}\right.\\
&\qquad\left. + a\ell_1\big(\exp(a\ell_2)-1-a\ell_2\big)q_{12} + a\ell_2\big(\exp(a\ell_1)-1-a\ell_1\big)q_{12} + a^2\ell_1\ell_2q_{12}\right]\\
& = a^2\E\ell_1\ell_2q_{12} + O\left(|a|^3\E|\ell_1^2\ell_2q_{12}| + |a|^4\E|\ell_1^2\ell_2^2q_{12}|\right),
\end{align*}
for the second term we have
\begin{align*}
\E \exp(a(\ell_1+\ell_2))q_{12}^2 &= \sigma_q^2 +  \E \big[\exp(a(\ell_1+\ell_2))-1\big]q_{12}^2 = \sigma_q^2 +  O(|a|\E|\ell_1q_{12}^2|),
\end{align*}
and for the third term we have
\begin{align*}
\E \prod_{i=1}^3\exp(a\ell_i)q_{13}q_{23} &= \E \prod_{i=1}^3\big(\exp(a\ell_i)-1\big)q_{13}q_{23}= O(|a|^3 \E |\ell_1\ell_2\ell_3q_{13}q_{23}|).
\end{align*}

Plugging the above expansions back into \eqref{E:second_expansion} yields
\begin{align}\label{E:third_expansion}
\chi_{L+Q}(t) &= \psi^n\left(\tfrac{t}{\Scale n}\right) + \psi^{n-2}\left(\tfrac{t}{\Scale n}\right)\left[\frac{(\iota t)^3}{\Scale^3 n^2}\E\ell_1\ell_2q_{12}  +\frac{(it)^2}{2\Scale^2} \binom{n}{2}^{-1}\sigma_q^2\right] \nonumber\\
&\qquad +\psi^{n-4}\left(\tfrac{t}{\Scale n}\right) \tfrac{1}{2}\tfrac{(\iota t)^6}{\Scale^6n^4}\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^ 2 \nonumber\\
&\qquad  +  O\left(\psi^{n-2}\left(\tfrac{t}{\Scale n}\right)\left[\tfrac{t^4}{\Scale^4n^3}\E|\ell_1^2\ell_2q_{12}| + \tfrac{|t|^5}{\Scale^5n^4}\E|\ell_1^2\ell_2^2q_{12}| + \tfrac{|t|^3}{\Scale^2 n^3}\E|\ell_1 q_{12}^2|\right]\right)\nonumber\\
&\qquad  +  O\left(\psi^{n-3}\left(\tfrac{t}{\Scale n}\right)\tfrac{|t|^5}{\Scale^5 n^4}\E |\ell_1\ell_2\ell_3q_{13}q_{23}|\right)\nonumber\\
&\qquad  +  O\left(\psi^{n-4}\left(\tfrac{t}{\Scale n}\right)\left[\tfrac{|t|^7}{\Scale^7n^5}(\E\ell_1\ell_2q_{12})(\E|\ell_1^2\ell_2q_{12}|)+ \tfrac{|t|^8}{\Scale^8n^6}(\E\ell_1\ell_2q_{12})(\E|\ell_1^2\ell_2^2q_{12}|)\right]\right)\nonumber\\
&\qquad  +  O\left[ \left(\tfrac{|t|}{\Scale n^2}\right)^{2+\delta}\Pi_{2+\delta}(n)\right].
\end{align}


From the Edgeworth expansion theory for sum off i.i.d random variables \citep{Bhattacharya-Rao1976_book,Hall1992_book}, we have for $|t|\leq \delta^*\sqrt{n}$ for some small enough $\delta^*>0$ 
\[\psi^n\left(\frac{t}{\sigma_\ell \sqrt{n}}\right) = \exp\left[-\tfrac{1}{2}t^2\right]\left[1 -\frac{\iota t^3}{6\sqrt{n}}\E \left(\frac{\ell_1}{\sigma_\ell}\right)^3 \right] + o\left(\frac{(|t|^3 + t^6)}{\sqrt{n}}\exp(-t^2/4)\right).\]
Let $\alpha_k:=\frac{\sigma_\ell\sqrt{n-k}}{\Scale n}$ for $k\in\{0,2,3,4\}$. Since $\alpha_k\asymp 1$ by assumption, where $\asymp$ denotes proportional up to a fixed finite positive constant, we obtain
\begin{align*}
    \psi^{n-k}\left(\frac{t}{\Scale n}\right) &=\psi^{n-k}\left(\frac{\alpha_k t}{\sigma_\ell \sqrt{n-k}}\right)\\
    &= \exp\left[-\tfrac{1}{2}\left(\alpha_k t\right)^2\right]\left[1 -\frac{\iota (\alpha_k t)^3}{6\sqrt{n-k}}\E \left(\frac{\ell_1}{\sigma_\ell}\right)^3 \right]  + o\left(\frac{(|t|^3 + t^6)}{\sqrt{n}}\exp(-(\alpha_k t)^2/4)\right).
\end{align*}
A first-order Taylor expansion yields 
\[\exp(-(\alpha_k t)^2/2) = \exp(-t^2/2)\left[1 - (\alpha^2_k-1)\tfrac{t^2}{2} + O(p(t) (\alpha^2_k-1)^2)\right],\]
and plugging it back in the previous expression, we have
\begin{align*}
    \psi^{n-k}\left(\frac{t}{\Scale n}\right) 
    &= \exp\left(-\tfrac{t^2}{2}\right)\left[1 - (\alpha^2_k-1)\frac{t^2}{2} -\frac{\iota (\alpha_k t)^3}{6\sqrt{n-k}}\E \left(\frac{\ell_1}{\sigma_\ell}\right)^3 \right]  \\
    &\qquad + O\left( (\alpha_k^2 -1)^2p(t)\exp{(-t^2/2)}\right) + o\left[\frac{(|t|^3 + t^6)}{\sqrt{n}}\exp(-(\alpha_k t)^2/4)\right].
\end{align*}
Use the fact that $\alpha_k^2 =\alpha_0^2(1-k/n)=\left(\frac{\sigma_\ell}{\Scale \sqrt{n}} \right)^2 +O(n^{-1})$ to conclude that
\begin{align}\label{E:iid_EE}
    \psi^{n-k}\left(\frac{t}{\Scale n}\right) 
    &= \exp\left(-\tfrac{t^2}{2}\right)\left[1 - \left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\frac{t^2}{2} -\frac{\iota t^3}{6\Scale^3n^2}\E \ell_1^3 \right] \nonumber \\
    &\qquad + O\left( \left(\tfrac{\sigma_\ell^2}{\Scale^2 n} -1\right)^2p(t)\exp{(-t^2/2)}\right) + o\left(\frac{(|t|^3 + t^6)}{\sqrt{n}}\exp(- t^2/4)\right),
\end{align}
for $|t|\leq \delta^*\sqrt{n}$.

Combine \eqref{E:third_expansion} and \eqref{E:iid_EE} to conclude that, for $|t|\leq \delta^*\sqrt{n}$,
\begin{align}\label{E:L+Q_expansion}
    \chi_{L+Q}(t)
    &=  \exp\left(-\tfrac{t^2}{2}\right)\left[1 - \left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\frac{t^2}{2} -\frac{\iota t^3}{6\Scale^3n^2}\E \ell_1^3 \right] \nonumber\\
    &\qquad \times\left[1 + \frac{(it)^2}{2\Scale^2} \binom{n}{2}^{-1}\sigma_q^2 + \frac{(\iota t)^3}{\Scale^3 n^2}\E\ell_1\ell_2q_{12} + \tfrac{1}{2}\tfrac{(\iota t)^6}{\Scale^6n^4}\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^ 2\right]\nonumber\\
    &\qquad + O\left( \exp\left(-\tfrac{t^2}{2}\right)\left[1 +\left(\tfrac{\sigma_\ell^2}{\Scale^2 n} -1\right) t^2 + \left(\tfrac{\sigma_\ell^2}{\Scale^2 n} -1\right)^2 p(|t|) +\tfrac{|t|^3}{\sqrt{n}} \right] R(t)\right)\nonumber\\
    &\qquad + o\left( \exp\left(-\tfrac{t^2}{4}\right)\left[ \tfrac{|t|^3 + t^6}{\sqrt{n}} \right] R(t)\right)\nonumber\\
    &\qquad + O\left(\left(\tfrac{|t|}{\Scale n^2}\right)^{2+\delta}\Pi_{2+\delta}(n)\right),
\end{align}
where
\begin{align*}
    R(t):=\tfrac{t^4}{\Scale^4n^3}\E|\ell_1^2\ell_2q_{12}| + \tfrac{|t|^5}{\Scale^5n^4}\E|\ell_1^2\ell_2^2q_{12}| + \tfrac{|t|^3}{\Scale^2 n^3}\E|\ell_1 q_{12}^2| + \tfrac{|t|^5}{\Scale^5 n^4}\E |\ell_1\ell_2\ell_3q_{13}q_{23}| \\+ \tfrac{|t|^7}{\Scale^7n^5}(\E\ell_1\ell_2q_{12})(\E|\ell_1^2\ell_2q_{12}|)+ \tfrac{|t|^8}{\Scale^8n^6}(\E\ell_1\ell_2q_{12})(\E|\ell_1^2\ell_2^2q_{12}|).
\end{align*}

After some rearrangement, the first term in \eqref{E:L+Q_expansion} becomes
\begin{equation*}
\widetilde{\chi}_{L+Q}(t):= \exp\left(-\tfrac{t^2}{2}\right)P(t)  =  \exp\left(-\tfrac{t^2}{2}\right)\left[1 + \sum_{j=2}^9 \left(\tfrac{\iota t}{\Scale}\right)^j\gamma_j \right],
\end{equation*}
where 
\begin{align*}
    P(t)&:=1 + \tfrac{(\iota t)^2}{2}\left(\tfrac{\sigma^2}{\Scale^2}-1\right) + \tfrac{(\iota t)^3}{6\Scale^3n^2}\left[\E \ell_1^3 + 6\E\ell_1\ell_2q_{12} \right] + \tfrac{(\iota t)^4}{4\Scale^2}\left(\tfrac{\sigma^2}
{\Scale^2}-1\right)\binom{n}{2}^{-1} \sigma_q^2\\
    &\qquad + \tfrac{(\iota t)^5}{12\Scale^5 n^2}\left[\binom{n}{2}^{-1}(\E\ell_1^3 )\sigma_q^2 + 6\Scale^2\left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\E\ell_1\ell_2q_{12}\right] \\
    &\qquad + \tfrac{(\iota t)^6}{6\Scale^6n^4}\left[(\E\ell_1^3)\E\ell_1\ell_2q_{12} + 12\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^ 2\right]\\
&\qquad + \tfrac{1}{4}\tfrac{(\iota t)^8}{\Scale^6n^4}\left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^2\\
&\qquad + \tfrac{1}{12}\tfrac{(\iota t)^9}{\Scale^9n^6}\binom{n}{2}^{-2}\binom{n}{4} \E\ell_1^3\left[\E\ell_1\ell_2q_{12}\right]^ 2.
\end{align*}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DO NOT delete these intermediate steps in case we need to recheck it !!!!

% polynomial
% \begin{align*}
%     P(t)&:=1 + \tfrac{(\iota t)^2}{2}\left(\tfrac{\sigma^2}{\Scale^2}-1\right) + \tfrac{(\iota t)^3}{6\Scale^3n^2}\E \ell_1^3 + \tfrac{(it)^5}{12\Scale^5 n^2}\binom{n}{2}^{-1}(\E\ell_1^3 )\sigma_q^2 + \tfrac{(\iota t)^4}{4\Scale^2}\left(\tfrac{\sigma^2}
% {\Scale^2}-1\right)\binom{n}{2}^{-1} \sigma_q^2\\
% &\qquad +\tfrac{(\iota t)^3}{\Scale^3 n^2}\E\ell_1\ell_2q_{12} + \tfrac{(\iota t)^5}{2\Scale^3 n^2}\left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\E\ell_1\ell_2q_{12} + \tfrac{(\iota t)^6}{6\Scale^6n^4}(\E\ell_1^3)\E\ell_1\ell_2q_{12}\\
% &\qquad +\tfrac{1}{2}\tfrac{(\iota t)^6}{\Scale^6n^4}\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^ 2 + \tfrac{1}{4}\tfrac{(\iota t)^8}{\Scale^6n^4}\left(\tfrac{\sigma_\ell^2}{\Scale^2 n}-1\right)\binom{n}{2}^{-2}\binom{n}{4} \left[\E\ell_1\ell_2q_{12}\right]^2\\
% &\qquad + \tfrac{1}{12}\tfrac{(\iota t)^9}{\Scale^9n^6}\binom{n}{2}^{-2}\binom{n}{4} \E\ell_1^3\left[\E\ell_1\ell_2q_{12}\right]^ 2.
% \end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since $\widetilde{\chi}_{L+Q}(t) = \exp(-\iota tb)\chi_G(t)$, we have under Assumption (a) and (b)
\begin{align*}
 |\chi_F(t)-\chi_G(t)| =  O\left(\exp\left(-\tfrac{t^2}{4}\right)R(t) + \left(\tfrac{|t|}{\Scale n^2}\right)^{2+\delta}\Pi_{2+\delta}(n)\right).
\end{align*}
Therefore,
\begin{align*}
I_1 &= O\left(\int_{|t|\leq\log n}|t|^{-1}\exp(-t^2/4)R(t)\d t + \frac{\Pi_{2+\delta}(n)}{(\Scale n^2)^{2+\delta}}\int_{|t|\leq\log n} |t|^{1+\delta}\d t\right) \\
& =   O\left(R(1) + \left(\frac{\log n}{\Scale n^2}\right)^{2+\delta}\Pi_{2+\delta}(n)\right).
\end{align*}
\subsubsection*{Bound for $I_2$}

For $1\leq m <n$, define $Q_m :=\tfrac{1}{\Scale}\binom{n}{2}^{-1}\sum_{i=1}^m\sum_{j=i+1}^n q_{ij}$. Using \eqref{E:exp_expansion}  we can write
\begin{align*}
|\chi_F(t)|=|\chi_{L+Q}(t)|\leq \left| \E	\exp(\iota t(L+Q-Q_m))\sum_{k=0}^2\frac{(itQ_m)^k}{k!})\right|+	|t|^{2+\delta}\E|Q_{m}|^{2+\delta}.
\end{align*}
Exploiting the fact that $Q - Q_m$ is only a function of $X_{m+1},\dots, X_n$, we have
\[|\E	\exp(\iota t(L + Q-Q_m)|\leq |\psi\left(\tfrac{t}{\Scale n}\right)|^{m}. \]
For the second term
\begin{align*}
|\E	\exp(\iota t(T-Q_m)Q_m|& =\frac{1}{\Scale}\binom{n}{2}^{-1}\left|\sum_{i=1}^m\sum_{j=i+1}^n \E	\exp(\iota t(L + Q-Q_m)q_{ij}\right|\\
&\ls \tfrac{1}{\Scale n^2}|\psi\left(\tfrac{t}{\Scale n}\right)|^{m-2}mn\E|q_{12}|.
\end{align*}
Similarly,  using the fact that 
\[\left[\Scale\binom{n}{2} Q_m\right]^2 =\sum_{i=1}^m\sum_{j=i+1}^nq_{ij}^2, +\sum_{i=1}^m\sum_{j=i+1}^n \sum_{\substack{k=1,\\k\neq i,j}}^m q_{ij}q_{jk}+ \sum_{i=1}^m\sum_{j=i+1}^n \sum_{\substack{k=1,\\k\neq i,j}}^m\sum_{\substack{l=k+1,\\l\neq i,j}}^nq_{ij}q_{kl}, \]
we conclude for $k\in\{0,1,2\}$, 
\begin{align*}
|\E	\exp(\iota t(T-Q_m))Q_m^k| \ls |\psi\left(\tfrac{t}{\Scale n}\right)|^{m-2k}\left(\frac{mn}{\Scale}\binom{n}{2}^{-1}\right)^k \E|q_{12}|^k.
\end{align*}
Finally, using the fact that $\Scale = O (\sigma_\ell/\sqrt{n})$ by Assumption (b) and combining the last displays,
\begin{equation}\label{E:large_deviation_bound}
|\chi_F(t)|\ls\sum_{k=0}^2 \left(\frac{|t|m}{\sqrt{n}}\right)^k |\psi\left(\tfrac{t}{\Scale n}\right)|^{m-2k} \E\left|\tfrac{q_{12}}{\sigma_\ell}\right|^k + |t|^{2+\delta}\E|Q_{m}|^{2+\delta},
\end{equation}
for $1\leq m<n$ and $\delta\in [0,1]$.

By the triangle inequality followed by \eqref{E:exp_expansion}, we have
\[|\psi\left(\tfrac{t}{\Scale n}\right)|-| 1 - \tfrac{t^2}{2(\Scale n)^2}\sigma_\ell^2|\leq |\psi\left(\tfrac{t}{\Scale n}\right) - (1 - \tfrac{t^2}{2(\Scale n)^2}\sigma_\ell^2)|\leq \tfrac{1}{6}\tfrac{|t|^3}{(\Scale n)^3}\E|\ell_1|^3.\]
For $|t|\leq \tfrac{\sqrt{2}\Scale n}{\sigma_\ell}$ we have $| 1 - \tfrac{t^2}{2(\Scale n)^2}\sigma_\ell^2| =  1 - \tfrac{t^2}{2(\Scale n)^2}\sigma_\ell^2$ hence
\[
|\psi\left(\tfrac{t}{\Scale n}\right)| \leq  1 - \tfrac{t^2}{2(\Scale n)^2}\sigma_\ell^2 + \tfrac{1}{6}\tfrac{|t|^3}{(\Scale n)^3}\E|\ell_1|^3;\qquad |t|\leq \tfrac{\sqrt{2}\Scale n}{\sigma_\ell}.
\]
Assumption (b) together with \eqref{E:variance_decomposition} implies that $\tfrac{\sigma_\ell}{\sqrt{n}\Scale}\to 1$ as $n\to\infty$. Then, we can find a $N_1\in\N$ such that $\sqrt{5/6}\leq \tfrac{\sigma_\ell}{\sqrt{n}\Scale} \leq (6/5)^{1/3}$ for $n\geq N_1$. Also, Assumption (a) implies the existence of a constant $C>0$ and $N_2\in\N$ such that $\E|\ell_1/\sigma_\ell|^3\leq C$ for $n\geq N_2$. Then, for $|t|\leq c\sqrt{n}$ where $c:= (\sqrt{2}/(6/5)^{1/3})\land (5/(12C))$ and $n\geq N_0:=N_1\lor N_2$, we have
\begin{align}\label{E:linear_interm}
|\psi\left(\tfrac{t}{\Scale n}\right)| &\leq 1 - \frac{t^2}{n}\left[\frac{1}{2}\left(\frac{\sigma_\ell}{\Scale \sqrt{n}}\right)^2 - \frac{|t|\E|\ell_1/\sigma_\ell|^3}{6\sqrt{n}}\left(\frac{\sigma_\ell}{\Scale \sqrt{n}}\right)^3\right] \leq 1 - \frac{t^2}{3n}\leq \exp(-\tfrac{t^2}{3n}).
\end{align}

For $\log n <|t|\leq c\sqrt{n}$, set $m=[\tfrac{15 n\log n}{t^2}]+1=O(n)$, then plug in \eqref{E:linear_interm} to conclude that $|\psi\left(\tfrac{t}{\Scale n}\right)|^{m-2k} \ls\exp(-\tfrac{t^2m}{3n})\ls n^{-5}$. Combining this last bound with \eqref{E:large_deviation_bound}, we obtain
\begin{equation}\label{E:cf_interm}
|\chi_F(t)|\ls\sum_{k=0}^2 \frac{|t|^k}{n^{5-k}} \E\left|\tfrac{q_{12}}{\sigma_\ell}\right|^k  + |t|^{2+\delta}\E|Q_{m}|^{2+\delta},
\end{equation}
for $|t|\leq c\sqrt{n}$  and $n\geq N_0$. Then,
\begin{align*}
    I_2
    &\ls\sum_{k=0}^2  \frac{1}{n^{5-k-k/2}} \frac{\E|q_{12}|^k}{\sigma_\ell^k}  + 	\left(\frac{\sqrt{n\log n}}{n}\right)^{2+\delta}\left(\tfrac{\sigma_q}{\sigma_\ell}\right)^{2+\delta}\log n \\
    &\ls \frac{1}{n} \frac{\sigma_q^2}{n\sigma_\ell^2}  + 	\log n\left(\frac{(\log n)\sigma_q^2}{n\sigma_\ell^2}\right)^{\tfrac{2+\delta}{2}}.
\end{align*} 
 
Therefore, since $\tfrac{\sigma_q^2}{n\sigma_\ell^2} = o(1)$  by Assumption (b), we conclude
\begin{align*}
I_2 = o(n^{-1}) + O\left(\left[\frac{(\log n)^{\frac{4+\delta}{2+\delta}}\sigma_q^2}{n\sigma_\ell^2}\right]^{\tfrac{2+\delta}{2}}\right).
\end{align*}


\subsubsection*{Bound for $I_3$ and $I_4$}

Under Assumption (c),  for sufficient large $n$, we may find a $b>0$ such that for $|t|>c\sqrt{n}$
\[|\psi\left(\tfrac{t}{\Scale n}\right)|\leq 1-b<\exp(-b),\]
where $c>0$ is define just before  \eqref{E:linear_interm}. 
Set $m=[\tfrac{4\log n}{b}] + 1$, then $nm\ls n \log n$ and $|\psi\left(\tfrac{t}{\Scale n}\right)^{m-s}|\ls n^{-4}$ for sufficient large $n$ and $s\in\{1,3,4,5\}$. Use these upper bounds on \eqref{E:large_deviation_bound} to conclude that
\begin{equation} \label{E:cf_high}
|\chi_F(t)|\ls  n^{-4}\left(1 + |t|\log n \E|q_{12}/\sigma_\ell| + t^2(\log n)^2\tfrac{\sigma_q^2}{\sigma_\ell^2} \right) +  	|t|^{2+\delta}(n\log n)^{1+\delta/2}\E|q_{12}|^{2+\delta},
\end{equation}
for sufficient large $n$ and $|t|>c\sqrt{n}$. Then,
\begin{align*}
I_3&= o(n^{-1/2}) + O\left((n\log n)^{1+\delta/2}\E|q_{12}|^{2+\delta} \int_{c\sqrt{n}\leq |t|\leq \sqrt{n}\log n} |t|^{1+\delta} dt\right)\\
& =  o(n^{-1/2}) + O\left(\left[\frac{\log n}{n}\right]^{2+\delta}\Pi_{2+\delta}(\log n)\right).
\end{align*}
Finally,
\begin{align*}
   I_4&=\int_{|t|>\log n}|t|^{-1}\exp(-\tfrac{t^2}{2})\left|1 +\sum_{j=2}^9\left(\tfrac{it}{\Scale}\right)^j\gamma_j \right|\d t\\
      &\leq C \int_{t>\log n}t^{-1}\exp(-\tfrac{t^2}{2})\d t +  \sum_{j=2}^9\frac{|\gamma_j|}{\Scale^j}\int_{t>\log n}t^{j-1}\exp(-\tfrac{t^2}{2}) \d t,
\end{align*}
where the first integral is $o(n^{-1})$ and the second is $o(1)$. Therefore,
\[I_4 = o\left(n^{-1} + \sum_{j=2}^9\frac{|\gamma_j|}{\Scale^j}\right).\]

\bigskip
The proof is complete. \qed

\subsection{Auxiliary Lemmas}

\begin{lem}\label{L:Q_moment} Let $n\geq 2$, $1\leq l\leq m< n$ and $p\geq 2$ then
\[
\E\left|\sum_{i=l}^{m}\sum_{j=i+1}^{n}q_{ij}\right|^p\leq C_p (n-l)^{p/2}\max_{l<j\leq n}\E\left|\sum_{i=l}^{(m\land j)-1}q_{ij}\right|^p\leq  K_p\left[(n-l)(m-l)\right]^{p/2}\E|q_{12}|^p,
\]
where $C_p:=\big[8(p-1)(1\lor 2^{p-3})\big]^p$ and $K_p$ is a constant only depending on $p$.
\end{lem}
\begin{proof}
    The double summation on the left-hand side can be written as $\sum_{j=l+1}^{n}\xi_j$ where $\xi_{j}:=\sum_{i=l}^{(m\land j)-1}q_{ij}$. Notice that $\{\xi_j,\m{F}_j\}$ is m.d.s when $\m{F}_j$ is the $\sigma$-algebra generated by $\{X_1,\dots, X_j\}$ for $j\geq 1$ and $\m{F}_0$ is trivial.  Then by \citet{DFJ_1968} followed by a trivial bound
    \[\E\left|\sum_{i=l}^{m}\sum_{j=i+1}^{n}q_{ij}\right|^p =\E\left|\sum_{j=l+1}^{n}\xi_j\right|^p \leq C_p(n-l)^{p/2-1}\sum_{j=l+1}^{n}\E|\xi_j|^p\leq C_p (n-l)^{p/2}\max_{l<j\leq n}\E\left|\xi_j\right|^p.
    \]
\end{proof}

\begin{lem}\label{L:Q_Lp} For $p\in[2,\infty)$ there exist a constant $C_p$ only depending on $p$ such that for $\m{S}\subseteq \{(i,j): 1\leq i<j\leq n\}$
\begin{align*}
\E\left|\sum_{\m{S}} q_{ij}\right|^p&\leq C_{p}\left\{|\m{S}|^{p/2}\big[\E q_{12}^2 \big]^{p/2} \lor s\E\big[(\E (q_{12}^2|Z_1))^{p/2}\big] \lor   |\m{S}|\E|q_{12}|^p\right\},
\end{align*}
where $|\m{S}|$ denotes the cardinality of the set $\m{S}$, $s:=s_i\lor s_j$ with $s_i := \sum_{(i,\cdot)\in\m{S}}\left(\sum_{(\cdot,j)\in \m{S}} 1\right)^{p/2}$ and $s_j := \sum_{(\cdot,j)\in\m{S}}\left(\sum_{(i,\cdot)\in \m{S}} 1\right)^{p/2}$.
\end{lem}
\begin{proof}
    Combining Proposition 2.1 with expression (2.18) in \citet{gine-latala-zinn_2000}, we obtain the inequality above for the \emph{decoupled version} of $q_{ij}$, defined as $\widetilde{q}_{ij}:=q(Z_i^{(1)},Z_j^{(2)})$ where $Z_i^{(j)}:1\leq i\leq  n$, $1\leq j\leq 2$ are i.i.d. Finally, we can apply the decoupling inequalities in \citet{de1995decoupling} to obtain the result at the expense of increasing the constant without altering the order of the upper bound. For further details, see section 2.5 in \citet{gine-latala-zinn_2000}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:EE-standard} (Standardized Edgeworth Expansion)}\label{App A: Proof for The Standardized Case}

We apply Corollary \ref{coro:general} with $u(Z_i,Z_j) = \nu'U_{ij}$ in \eqref{eq:estimator} and $\delta =1$. We assume throughout that Assumptions \ref{A:DGP} and \ref{A:kernel} hold. Condition (a) in Theorem \ref{thm:general} is verified by direct calculations as in \citet{Cattaneo-Crump-Jansson_2010_JASA,Cattaneo-Crump-Jansson_2014a_ET,Cattaneo-Crump-Jansson_2014b_ET}. Condition (b) in Theorem \ref{thm:general} is verified because \eqref{E:variance_decomposition} gives $\sigma^2 = \frac{1}{n} \V[\nu'L_i] + \binom{n}{2}^{-1} \V[\nu'Q_{i,j}]$, which implies
\[\sigma_\ell^2 = \nu'\Sigma\nu + O(h^P) \qquad\text{and}\qquad
  \sigma_q^2 = \frac{1}{h^{d+2}}[\nu'\Delta\nu + h^2 \nu'\mathcal{V}\nu] + o(h^{-d}),\]
with $\mathcal{V}$ given in \citet{Cattaneo-Crump-Jansson_2010_JASA}. These results imply $\sigma_q^2 = o(n\sigma_\ell^2)$ if (and only if) $nh^{d+2}\to\infty$. Therefore, we take $\Scale \asymp \sigma \asymp 1/\sqrt{n}$. Condition (c) in Theorem \ref{thm:general} holds by assumption.

The additional condition $B\to0$ in Corollary \ref{coro:general} holds if (and only if, when $\beta\neq 0$) $\sqrt{n}h^P\to 0$. To see this, using integration by parts, $\E[U_{12}|Z_1] = \int_{\mathbb{R}^d} \nu'\dot{e}(X_1+uh) K(u)\text{d}u - Y_1 \int_{\mathbb{R}^d}\nu'\dot{f}(X_1+uh) K(u)\text{d} u$. Then, repeated Taylor series expansions and integration by parts give $\E[u(Z_1,Z_2)|Z_1] = \delta(Z_1) + h^P(-1)^P\sum_{[k]=P}\frac{\mu_k}{k!} \delta^{(1+k)}(z)+ o(h^P)$. In turn, this result implies that $\E[u(Z_1,Z_2)] = \theta_\nu + h^P \beta + o(h^P)$. As a consequence, $B = (\E[\widehat{\theta}_\nu] - \theta_\nu)/\Scale = h^P\beta/\Scale + o(\sqrt{n}h^P)$. See \citet{Cattaneo-Crump-Jansson_2010_JASA,Cattaneo-Crump-Jansson_2014a_ET,Cattaneo-Crump-Jansson_2014b_ET} for details.

Law of iterated expectations, integration by parts, and Taylor series expansions give
\[\E[\ell_1^3] = \kappa_{1} + O(h^P).\]
Proceeding analogously, because $\E[\ell_1\ell_2q_{12}] = \E[\ell_2\E[\ell_1q_{12}|Z_2]]=\E[\ell_2\ell_1U_{12}]$ and $\E[\ell_2\ell_1U_{12}] = 4\E[\E[U_{12}|Z_1]\E[U_{12}|Z_2]U_{12}] - 8 \E[U_{12}]\E[\E[U_{12}|Z_1]^2] + 4\E[U_{12}]^3$, we have $\E[\E[U_{12}|Z_1]\E[U_{12}|Z_2]U_{12}] = \E[\delta(Z)\dot{\eta}(Z)] + O(h^P)$, $\E[\E[U_{12}|Z_1]^2] = \E[\delta(Z_1)^2] + O(h^P)$, and $\E[U_{12}] = \theta + O(h^P)$ and $\E[U_{12}]^3 = \theta^3 + O(h^P)$. Collecting these results, we verify
\[\E[\ell_1\ell_2q_{12}]= \kappa_{2} + O(h^P).\]
These results imply $\gamma_3=O(n^{-2})$, $\gamma_4=O(n^{-3}h^{-d-2})$, $\gamma_5=O(n^{-2})$, $\gamma_6=O(n^{-1})$, $\gamma_8=O(n^{-3})$, $\gamma_9=O(n^{-7/2})$.

It remains to bound $\mathcal{E}$. First, by standard results $\E|q_{12}|^3=O(h^{2d+3})$, so
\[\Pi_3(m) =O\Big(\frac{(mn)^{3/2}}{h^{(3/2)(d+2)}}\lor \frac{m n^{3/2}}{h^{(3/2)(d+2)}}\lor \frac{mn}{h^{2d+3}}\Big)=O\Big(\frac{mn}{h^{d+2}}\Big)^{3/2}.\]
Second, using the results in \citet[Supplemental Appendix]{Cattaneo-Crump-Jansson_2014b_ET}, we have 
$\E|\ell_1^2\ell_2q_{12}|=O(h^{-2d/3-1})$, $\E|\ell_1^2\ell_2^2 q_{12}|=O(h^{-2d/3-1})$, $\E|\ell_1q_{12}^2| = O(h^{-d-2})$, $\E |\ell_1\ell_2\ell_3q_{13}q_{23}|=O(h^{-4d/3-2})$. Thus, collecting all the bounds, we verify:
\begin{equation*}
    \mathcal{E} 
     = O\Big(\Big(\frac{(\log n)^{3}}{nh^{d+2}}\Big)^{3/2} + \frac{h^{d/3+1}}{nh^{d+2}} + \Big(\frac{h^{d/9+2/3}}{nh^{d+2}}\Big)^{3/2}\Big)
     = o\Big(\frac{1}{nh^{d+2}}\Big)
\end{equation*}

This completes the proof.\qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem \ref{thm:EE-student} (Studentized Edgeworth Expansion)}\label{App A: Proof for The Studentized Case}

For any estimated scale $\widehat{\Scale}$ and nonrandom centering $\Scale$, we have
\[\frac{\widehat{\theta}_\nu - \theta_\nu}{\widehat{\Scale}}
  = \frac{\widehat{\theta}_\nu - \theta_\nu}{\Scale}
    \Big[ 1 - \frac{\widehat{\Scale}^2 - \Scale^2}{2\Scale^2} 
            + \frac{\widehat{\Scale}+2\Scale^2}{2\Scale^2 \widehat{\Scale}}
              \frac{(\widehat{\Scale}^2 - \Scale^2)^2}{\widehat{\Scale}^2 + \Scale^2}\Big].
\]
Recall that $\widehat{\theta}_\nu$ is a second-order U-statistic satisfying the H-decomposition $(\widehat{\theta}_\nu - \theta_\nu)/\Scale = B + \bar{L}/\Scale + \bar{Q}/\Scale$. Using standard results for Edgeworth expansions \citep{Bhattacharya-Rao1976_book,Hall1992_book},
\begin{equation*}
    \sup_{t\in\R}\Big|\P\Big[\tfrac{\widehat{\theta}_\nu - \theta_\nu}{\widehat{\Scale}} \leq t \Big] - G(t) \Big| \leq E + \mathcal{R}_1 + \mathcal{R}_2 + \mathcal{R}_3 + O\Big(\frac{r_n}{\log n}\Big),
\end{equation*}
where
\begin{equation*}
    E := \sup_{t\in\R}\left|\P\left[
    \Big(1 - \frac{\widehat{\Scale}^2 - \Scale^2}{2\Scale^2} \Big)
    \Big(\nu'\bar{L}/\Scale + \nu'\bar{Q}/\Scale\Big) + B \leq t \right]
    - G(t)\right|,
\end{equation*}
$B= \nu'(\E[U_{12}]-\Center)/\Scale$, $G$ denoting a distribution function later to be set to either $G_\mathtt{AL}$ or $G_\mathtt{SB}$ as appropriate, and
\begin{align*}
    \mathcal{R}_1 &:= \P\left[\left|\frac{\widehat{\Scale}+2\Scale^2}{2\Scale^2 \widehat{\Scale}}\right|
                    \frac{(\widehat{\Scale}^2 - \Scale^2)^2}{\widehat{\Scale}^2 + \Scale^2}  > C \frac{r_n}{(\log n)^2} \right],\\
    \mathcal{R}_2 &:= \P\left[\left|\frac{\widehat{\theta}_\nu - \theta_\nu}{\Scale} \right| > C \log n \right],\\
    \mathcal{R}_3 &:= \P\left[\left|\frac{\widehat{\Scale}-\Scale}{\Scale} B \right| > C \frac{\sqrt{n}h^P}{\log n} \right],
\end{align*}
with $C$ denoting a generic constant, which can take different values in different places. The term $E$ will give the Edgeworth expansion upon setting $\widehat{\Scale}$ and $\Scale$ appropriately, while the terms $\mathcal{R}_1$--$\mathcal{R}_3$ capture higher-order remainders. 


\subsection*{Variance Estimators}

The estimators $\widehat{\Scale}_\mathtt{AL}^2$ and $\widehat{\Scale}_\mathtt{SB}^2$ are linear combinations of U-statistics as follows:
\begin{equation*}
    \widehat{\Scale}_\mathtt{AL}^2 = \frac{1}{n} \nu'\widehat{\Sigma}\nu
    = 2\binom{n}{2}^{-1} \bar{W}_1 + \frac{4}{n}\frac{n-2}{n-1} \bar{W}_2 - \frac{4}{n}\widehat{\theta}_\nu^2
\end{equation*}
and
%\begin{align*}
%    \binom{n}{2}^{-1}h^{-d-2}\nu'\widehat{\Delta}\nu
%    &= \Big(1+\frac{2(n-2)}{(n-1)^2}\Big) \binom{n}{2}^{-1}\bar{W}_1 + \frac{2(n^2-6n+8)}{(n-1)^2} \binom{n}{2}^{-1}\bar{W}_2\\
%    &\qquad - \binom{n}{2}^{-1}\bar{U}^{2} + \frac{2(n-2)(n-3)}{(n-1)^{2}} \binom{n}{2}^{-1}\bar{W}_3,
%\end{align*}
\begin{equation*}
    \binom{n}{2}^{-1}h^{-d-2}\nu'\widehat{\Delta}\nu = \binom{n}{2}^{-1}\bar{W}_1
\end{equation*}
with
\begin{align*}
    \widehat{\theta}_\nu &= \binom{n}{2}^{-1}\sum_{i<j} (\nu'U_{ij}),\qquad 
    \bar{W}_1 = \binom{n}{2}^{-1}\sum_{i<j} (\nu'U_{ij})^2,\\
    \bar{W}_2 &= \binom{n}{3}^{-1}\sum_{i<j<k} W_{ijk}, \qquad W_{ijk} = \frac{(\nu'U_{ij})(\nu'U_{ik})+(\nu'U_{ij})(\nu'U_{h,jk})+(\nu'U_{ik})(\nu'U_{h,jk})}{3}.
%    \bar{W}_3 &= \binom{n}{4}^{-1}\sum_{i<j<k<l} W_{ijkl}, \qquad W_{ijkl} = \frac{(\nu'U_{h,ij})(\nu'U_{h,kl})+(\nu'U_{h,ik})(\nu'U_{h,jl})+(\nu'U_{h,il})(\nu'U_{h,jk})}{3}.
\end{align*}
See Lemmas 3.1.1 and 3.1.2 in the Supplemental Appendix of \citet{Cattaneo-Crump-Jansson_2014b_ET} for a proof. Thus, for $\mathfrak{c}\in\mathbb{R}$, we consider the following generic (debiased when $\mathfrak{c}=1$) Studentization:
\begin{equation*}
    \widehat{\Scale}_\mathfrak{c}^2 := (2-\mathfrak{c}) \binom{n}{2}^{-1} \bar{W}_1 + \frac{4}{n} [1+o(n^{-1})] \bar{W}_2 - \frac{4}{n} \widehat{\theta}_\nu^2.
\end{equation*}
In particular, $\widehat{\Scale}_\mathtt{AL}^2=\widehat{\Scale}_0^2$ and $\widehat{\Scale}_\mathtt{SB}^2=\widehat{\Scale}_1^2$. The centering considered in the literature is $\widehat{\Scale}_\mathfrak{c}^2$ is
\begin{equation*}
    \Scale_\mathfrak{c}^2 := \mathfrak{c} \binom{n}{2}^{-1} \E[\bar{W}_1] + \frac{4}{n} \E[\bar{W}_2] - \frac{4}{n} (\E[\widehat{\theta}_\nu])^2,
\end{equation*}
which implies that $\Scale_0^2 = \Scale_\mathtt{AL}^2$ and $\Scale_1^2 = \Scale_\mathtt{SB}^2 + o(n^{-1})$.

The underlying U-statistics have the following mean square convergence rates:
\begin{align*}
    \E[(\widehat{\theta}_\nu   - \E[\widehat{\theta}_\nu])^2]             &= O(n^{-1}          + n^{-2}h^{-d-2}),\\
    \E[(\bar{W}_1 - \E[(\nu'U_{12})^2])^2]         &= O(n^{-1}h^{-2d-4} + n^{-2}h^{-3d-4}),\\
    \E[(\bar{W}_2 - \E[(\E[\nu'U_{12}|Z_1])^2])^2] &= O(n^{-1}          + n^{-2}h^{-d-4}  + n^{-3}h^{-2d-4}),
%    \E[(\bar{W}_3 - (\E[\nu'U_{12}])^2)^2]         &= O(n^{-1}          + n^{-2}h^{-d-2}  + n^{-4}h^{-2d-4}).
\end{align*}
The proof is given in \citet[Supplemental Appendix]{Cattaneo-Crump-Jansson_2014b_ET}: see Lemma 3.1.3 for the first two results, and Lemma 3.1.4 for the third result. %and Lemma 3.1.5 for the last result.
(Note that while the statement of those lemmas gives convergence rates in probability, the proof mean square convergence rates.) Therefore,
\begin{align*}
    \E\Big[(\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2)^2 \Big] 
    &\leq C n^{-4} \E[(\bar{W}_1 - \E[(\nu'U_{12})^2])^2] + C n^{-2} \E[(\bar{W}_2 - \E[(\E[\nu'U_{12}|Z_1])^2])^2]\\
    & \qquad + C n^{-2} \E[(\bar{U} - \E[\nu'U_{12}])^2] %+ C n^{-4} \E[(\bar{W}_3 - (\E[\nu'U_{12}])^2)^2]
    \\
    &= O(n^{-3} + n^{-4}h^{-d-4}).
\end{align*}
Similar long calculations as in \citet[Supplemental Appendix]{Cattaneo-Crump-Jansson_2014b_ET} show that:
\begin{align*}
    \E[(\widehat{\theta}_\nu   - \E[\widehat{\theta}_\nu])^4]             &= O(n^{-2}          + n^{-4} h^{-d-4}  + n^{-5} h^{-2d-4} + n^{-6} h^{-3d-4}),\\
    \E[(\bar{W}_1 - \E[(\nu'U_{12})^2])^4]         &= O(n^{-2}h^{-4d-8} + n^{-4} h^{-6d-8} + n^{-5} h^{-6d-8} + n^{-6} h^{-7d-8}),\\
    \E[(\bar{W}_2 - \E[(\E[\nu'U_{12}|Z_1])^2])^4] &= O(n^{-2}          + n^{-4} h^{-d-8}  + n^{-5} h^{-2d-8} + n^{-6} h^{-3d-8}),
    %\E[(\bar{W}_3 - (\E[\nu'U_{12}])^2)^4]         &= O( TO BE DONE IF NEEDED),
\end{align*}
which gives
\begin{align*}
    \E\Big[(\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2)^4 \Big] 
    &\leq C n^{-8} \E[(\bar{W}_1 - \E[(\nu'U_{12})^2])^4] + C n^{-4} \E[(\bar{W}_2 - \E[(\E[\nu'U_{12}|Z_1])^2])^4]\\
    & \qquad + C n^{-4} \E[(\bar{U} - \E[\nu'U_{12}])^4]\\
    &= O(n^{-6} + n^{-8} h^{-d-8}).
\end{align*}

\bigskip
Consequently, for the remainder of the proof we set $\widehat{\Scale}^2=\widehat{\Scale}_\mathfrak{c}^2$ and $\Scale^2=\Scale_\mathfrak{c}^2$.

\subsection*{Bounds for $\mathcal{R}_1$--$\mathcal{R}_3$}

For $n$ large enough, and using Markov inequality,
\begin{align*}
    \mathcal{R}_1 &\leq \P\Big[(\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2)^2 > \frac{C r_n}{n^2(\log n)^2} \Big] + o(r_n)
        \leq C n^4 (\log n)^{4}r_n^{-2} \E\Big[ (\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2)^4 \Big] + o(r_n)\\
        & = n^5 (\log n)^{4} O(n^{-6} + n^{-8} h^{-d-8}) + o(r_n) = o(r_n).
\end{align*}

Using Theorem \ref{thm:general} and Corollary \ref{coro:general}, it follows that a valid Edgeworth expansion holds for $\frac{\widehat{\theta}_\nu - \theta_\nu}{\Scale_\mathfrak{c}}$, which implies that
\begin{align*}
    \mathcal{R}_2
    &= 1 - \P\left[\frac{\widehat{\theta}_\nu - \theta_\nu}{\Scale} \leq C \log n \right] + \P\left[\frac{\widehat{\theta}_\nu - \theta_\nu}{\Scale} \leq -C \log n \right]\\
    &= 1 - \Phi(C \log(n)) + \Phi(-C \log(n)) + C\frac{\phi(\log n) \log n}{nh^{d+2}}  + o(r_n) = o(r_n),
\end{align*}
by properties of the Gaussian distribution.

Finally, Markov inequality implies
\begin{equation*}
    \mathcal{R}_3 \leq C n (\log n)^2 \E\Big[ (\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2)^2 \Big] 
        = n (\log n)^2  O(n^{-3} + n^{-4}h^{-d-4}) = o(r_n).
\end{equation*}

\bigskip
Therefore, $\mathcal{R}_1+\mathcal{R}_2+\mathcal{R}_3=o(r_n)$.

\subsection*{Expansion for $E$}

We consider $E=\rho(\breve{F}_\mathfrak{c},G_\mathfrak{c})$, where
\[\breve{F}_\mathfrak{c}(t) := \P\left[\Big(1 - \frac{\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2}{2\Scale_\mathfrak{c}^2} \Big)
    \Big(\frac{\nu'\bar{L}}{\Scale_\mathfrak{c}} + \frac{\nu'\bar{Q}}{\Scale_\mathfrak{c}}\Big) + B_\mathfrak{c} \leq t\right], \qquad
  B_\mathfrak{c} := \frac{\E[\widehat{\theta}_\nu]-\theta_\nu}{\Scale_\mathfrak{c}},
\]
and 
\[G_\mathfrak{c}(t) := \Phi(t) -\phi(t)\Big\{\frac{\sqrt{n}h^P\beta}{\nu'\Sigma\nu} - \frac{1-\mathfrak{c}}{nh^{d+2}} \frac{\nu'\Delta\nu}{\nu'\Sigma\nu} t - \frac{1}{\sqrt{n} 6(\nu'\Sigma\nu)^3}\Big[\kappa_1 (2t^2+1) + \kappa_2 (t^2+1)\Big]\Big\}.\]
Recall that, in particular, $\mathfrak{c}=0$ corresponds to $\mathtt{AL}$ implementation and $\mathfrak{c}=1$ corresponds to $\mathtt{SB}$ implementation (i.e., $G_\mathtt{AL}(t)=G_0(t)$ and $G_\mathtt{SB}(t)=G_1(t)$). Then, applying the smoothing inequality as in Theorem \ref{thm:general},
\begin{equation*}
    \rho(\breve{F}_\mathfrak{c},G_\mathfrak{c}) \ls \breve{I}_1 + \breve{I}_2 + \breve{I}_3 + \breve{I}_4 +\tfrac{1}{\sqrt{n}\log n},
\end{equation*}
where 
\begin{align*}
\breve{I}_1&:=\int_{|t|\leq\log n}\left |\frac{\chi_{\breve{F}_\mathfrak{c}}(t)-\chi_{G_\mathfrak{c}}(t)}{t}\right|\d t, \qquad
\breve{I}_2:=\int_{\log n<|t|\leq c \sqrt{n}}\left |\frac{\chi_{\breve{F}_\mathfrak{c}}(t)}{t}\right|\d t, \\
\breve{I}_3&:=\int_{c \sqrt{n}<|t|\leq \sqrt{n}\log n}\left |\frac{\chi_{\breve{F}_\mathfrak{c}}(t)}{t}\right|\d t, \qquad
\breve{I}_4:=\int_{|t|>\log n}\left |\frac{\chi_{G_\mathfrak{c}}(t)}{t}\right|\d t.
\end{align*}

The last three integrals above can be upper bounded following the same arguments used in the proof of Theorem \ref{thm:general} to conclude that $\breve{I}_2 + \breve{I}_3 + \breve{I}_4=o(\sqrt{n}h^P + n^{-1}h^{-d-2} + n^{-1/2})$. The first integral, $\breve{I}_1$, is analyzed by expanding $\chi_{\breve{F}_\mathfrak{c}}(t)$ by generalizing the proof of Theorem \ref{thm:general} to account for the contribution from Studentization to the sampling distribution of the linearized version of the statistic ($\breve{F}_\mathfrak{c}$).

First, by \eqref{E:exp_expansion} we write
\begin{equation}\label{E:expansion_stundent_bias}
\chi_{\breve{F}_\mathfrak{c}}(t)=\exp (\iota t B_\mathfrak{c})\E\exp(\iota t \widetilde{U}_\mathfrak{c}) = \left[1+ \iota t B_\mathfrak{c} + O(t^2B_\mathfrak{c}^2)\right]\E\exp( \iota t \widetilde{U}_\mathfrak{c}),
\end{equation}
 where
\[\widetilde{U}_\mathfrak{c}
  = \left(1 - \frac{\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2}{2\Scale_\mathfrak{c}^2} \right)\left(\frac{\nu'\bar{L}}{\Scale_\mathfrak{c}} + \frac{\nu'\bar{Q}}{\Scale_\mathfrak{c}}\right) \]

From \eqref{E:altdecompstudent} below, we have
\begin{equation}\label{E:decompostion_AL}
    - \frac{\widehat{\Scale}_\mathfrak{c}^2 - \Scale_\mathfrak{c}^2}{2\Scale_\mathfrak{c}^2} = H_\mathfrak{c} + \mathcal{T}_\mathfrak{c}
\end{equation}
with
\begin{align*}
    H_\mathfrak{c}
      &:= -\binom{n}{2}^{-1} \frac{1-\mathfrak{c}}{2\vartheta_\mathfrak{c}^2} \E[q_{12}^2] - \frac{1}{2n\vartheta_\mathfrak{c}^2} \frac{1}{n}\sum_{i=1}^n \Big\{ \big(\ell_i^2 - \E[\ell_i^2]\big) + 4 \E[\ell_i q_{ij}|Z_i] \Big\}\\
      & \qquad - \frac{2}{n\vartheta_\mathfrak{c}^2} \binom{n}{2}^{-1} \sum_{i<j} \E[q_{ij}q_{ik}|Z_j,Z_k],
\end{align*}
where we define $\ell_i := \nu'L_i$ and $q_{ij} := \nu'Q_{ij}$, and $\mathcal{T}_\mathfrak{c}:= -\mathcal{V}_\mathfrak{c}/(2\vartheta_\mathfrak{c}^2)$ with $\mathcal{V}_\mathfrak{c}$ is given in \eqref{E:altdecompstudent}. Next, applying \eqref{E:exp_expansion} repeatedly, we are left with
\begin{equation}\label{E:expansion_student_1}
    \E\exp( \iota t \widetilde{U}_\mathfrak{c})
      = \E\exp\left[\iota t \left(\tfrac{\nu'\bar{L}}{\Scale_\mathfrak{c}} + \tfrac{\nu'\bar{Q}}{\Scale_\mathfrak{c}}\right)\right] 
        + \iota t\E H_\mathfrak{c} \tfrac{\nu'\bar{L}}{\Scale_\mathfrak{c}}\exp(\iota t\tfrac{\nu'\bar{L}}{\Scale_\mathfrak{c}} ) 
        + O\left(\mathcal{E}_1(t)\right),
\end{equation}
where $\mathcal{E}_1(t)=|t|\E|\mathcal{T}_\mathfrak{c} (\nu'\bar{L}) + (H_\mathfrak{c} + \mathcal{T}_\mathfrak{c})(\nu'\bar{Q})| + t^2(\E(H_\mathfrak{c} (\nu'\bar{L}))^2 + \E|H_\mathfrak{c} (\nu'\bar{L}) (\nu'\bar{Q})|$. The first term was expanded in the proof of Theorem \ref{thm:general}, as it corresponds to the standardized version of the statistic. The second term can be expanded analogously (see, e.g., Appendix B-(a) in \citet{Nishiyama-Robinson_2001_ChBook}): 
\begin{align}\label{E:extra_term_expasion}
    &\E \left[H_\mathfrak{c}\tfrac{\nu'\bar{L}}{\Scale_\mathfrak{c}}\exp(\iota t \nu'\bar{L}/\Scale_\mathfrak{c})\right]\\
    & = -[\psi\big(\tfrac{t}{n\Scale_\mathfrak{c}}\big)]^{n-1}\left[ \frac{1-\mathfrak{c}}{2} \frac{\iota t }{\vartheta_\mathfrak{c}^2} \binom{n}{2}^{-1}\E[q_{12}^2] 
          + O\left(\tfrac{|t|}{n^{2}h^{d+2}} + \tfrac{t^2}{n^{3/2}h^{d+2}} + \tfrac{|t|h^P}{h^{d+2}}\right)\right]\nonumber\\
    &\qquad -[\psi\big(\tfrac{t}{n\Scale_\mathfrak{c}}\big)]^{n-1}\left[\frac{1}{2\Scale_\mathfrak{c}^3 n^2}(\E\ell_1^3 + 4\E\ell_1\ell_2 q_{12})         
          + O\left(\tfrac{|t|}{n} \right)\right]\nonumber\\
   &\qquad -[\psi\big(\tfrac{t}{n\Scale_\mathfrak{c}}\big)]^{n-2}\left[\frac{(\iota t)^2}{2\Scale_\mathfrak{c}^3 n^2}(\E\ell_1^3 + 4\E\ell_1\ell_2 q_{12}) +O\left(\tfrac{t^2 + |t|^3}{n} +\tfrac{t^4}{n^{3/2}} \right)\right] \nonumber\\
      &\qquad -[\psi\big(\tfrac{t}{n\Scale_\mathfrak{c}}\big)]^{n-3}\left[O\left(\tfrac{|t|^3 + |t|}{n} +\tfrac{t^2}{n^{3/2}h^{d+2}}  +\tfrac{t^6}{n^{3}h^{d+2}} + \tfrac{|t|^5}{n^{5/2}h^{d+2}} +\tfrac{t^4 + |t|^3}{n^{2}h^{d+2}} \right)\right].
\end{align}
Combine \eqref{E:iid_EE}, \eqref{E:L+Q_expansion}, \eqref{E:expansion_student_1}, and \eqref{E:extra_term_expasion} to obtain
\begin{align}\label{E:expansion_studdent_2}
     \E\exp( \iota t \widetilde{U}_\mathfrak{c})
     &= \exp\left(-\tfrac{t^2}{2}\right)\Big[1 + \frac{(\iota t)^2}{2}\left(\tfrac{\E[\ell_1^2]}{\Scale_\mathfrak{c}^2 n}-1\right) +\frac{(\iota t)^3}{6\Scale_\mathfrak{c}^3n^2}\E \ell_1^3  + O(\mathcal{E}_2(t)) + o(\mathcal{E}_3(t))\Big] \nonumber \\
    &\qquad \times\Big[1 + \frac{(\iota t)^2}{2\Scale_\mathfrak{c}^2} \binom{n}{2}^{-1}\E [q_{12}^2] + \frac{(\iota t)^3}{\Scale_\mathfrak{c}^3 n^2}\E\ell_1\ell_2q_{12} \nonumber\\
    &\qquad\qquad -\frac{1-\mathfrak{c}}{2} \frac{(\iota t)^2}{\vartheta_\mathfrak{c}^2} \binom{n}{2}^{-1}\E [q_{12}^2] - \left(\frac{\iota t + (\iota t)^3}{\vartheta_\mathfrak{c}^3n^2}\right)\left(\frac{\E\ell_1^3}{2} + 2\E\ell_1\ell_2 q_{12}\right) + O(\mathcal{E}_4(t))\Big]\nonumber\\
    &\qquad + O(\mathcal{E}_1(t)),
\end{align}
where $\mathcal{E}_2(t)$ and $\mathcal{E}_3(t)$ are the last two rates appearing in \eqref{E:iid_EE} respectively. Also, proceeding as in \citet{Nishiyama-Robinson_2001_ChBook},
\begin{align*}
    \mathcal{E}_4(t) = o\left(\frac{t^2 + t^{10}}{nh^{d+2}} + \frac{t^2  + t^6}{\sqrt{n}}\right).
\end{align*}
Combine \eqref{E:expansion_studdent_2} with \eqref{E:expansion_stundent_bias} and expand the product to obtain
\begin{equation*}
     \chi_{\breve{F}_\mathfrak{c}}(t)= \exp\left(-\tfrac{t^2}{2}\right)\left[1 + \sum_{j=1}^3(\iota t)^j\breve{\gamma}_{\mathfrak{c},j}\right]+ O(\mathcal{E}_5(t)),
\end{equation*}
where
\begin{align*}
    \breve{\gamma}_{\mathfrak{c},1} &:= \left( \frac{\beta h^P}{\vartheta_\mathfrak{c}} - \frac{\E\ell_1^3/2 + 2\E\ell_1\ell_2 q_{12}}{\vartheta_\mathfrak{c}^3n^2}\right),\\
    \breve{\gamma}_{\mathfrak{c},2} &:= -(1-\mathfrak{c}) \binom{n}{2}^{-1}\frac{\E q_{12}^2}{2\vartheta_\mathfrak{c}^2},\\
    \breve{\gamma}_{\mathfrak{c},3} &:= -\frac{1}{6n^2\vartheta_\mathfrak{c}^3}(2\E\ell_1^3 + 6\E\ell_1\ell_2 q_{12}),
\end{align*}
and
\begin{align*}
\mathcal{E}_{5}(t)&:=\left[e^{-t^2/2}\tfrac{|t|^3}{\sqrt{n}}+o(n^{-1/2}(t^6 + |t|^3)e^{-t^2/4})\right]\left[\tfrac{t^2}{n^2h^{d+2}} + \tfrac{|t|^3 + |t|}{\sqrt{n}} + \mathcal{E}_{4}(t)\right]\\
&\qquad + e^{-t^2/2}\left[|t|\sqrt{n}h^P+t^2h^{2P} + |t|\sqrt{n}h^{2P}\right]\left[\tfrac{t^2}{n^2h^{d+2}} + \tfrac{|t|^3 + |t|}{\sqrt{n}} + \mathcal{E}_{4}(t)\right]\\
&\qquad +(|t|\sqrt{n}h^P + t^2nh^{2P})\left[e^{-t^2/2}\tfrac{|t|^3}{\sqrt{n}}+o(n^{-1/2}(t^6 + |t|^3)e^{-t^2/4})\right]\\
&\qquad +(|t|\sqrt{n}h^P + t^2nh^{2P})\left[e^{-t^2/2}\tfrac{|t|^3}{\sqrt{n}}+o(n^{-1/2}(t^6 + |t|^3)e^{-t^2/4})\right]\left[\tfrac{t^2}{n^2h^{d+2}} + \tfrac{|t|+|t|^3}{\sqrt{n}}+ \mathcal{E}_{4}(t)\right]\\
&\qquad +(|t|+t^2\sqrt{n}h^P + |t|^3nh^{2P})\big(\E|\mathcal{T}_\mathfrak{c}\bar{L}| + \E|(H_\mathfrak{c}+\mathcal{T}_\mathfrak{c})\bar{Q} |\big)\\
&\qquad +(t^2+|t|^3\sqrt{n}h^P + t^4 nh^{2P})\big(\E(H_\mathfrak{c}\bar{L})^2 + \E|H_\mathfrak{c}\bar{L}\bar{Q}|\big).
\end{align*}

We showed in the proof of Theorem \ref{thm:EE-standard} that $\E\ell_1^3 = \kappa_1 + O(h^P)=o(1)$ and $\E\ell_1\ell_2q_{12} = \kappa_2 + O(h^P)=o(1)$, and hence
\begin{align*}
     \chi_{\breve{F}_\mathfrak{c}}(t)
     &= \exp\Big(-\tfrac{t^2}{2}\Big)\left[1 + \iota t\left(\tfrac{\beta h^P}{\vartheta_\mathfrak{c}} - \tfrac{\kappa_1/2 + 2 \kappa_2}{\vartheta_\mathfrak{c}^3n^2}\right) - (\iota t)^2\binom{n}{2}^{-1}\frac{\E q_{12}^2}{2\vartheta_\mathfrak{c}^2} -\tfrac{(\iota t)^3}{6n^2\vartheta_\mathfrak{c}^3}(2\kappa_1 + 6\kappa_2)\right]\\
     &\qquad+ O(\mathcal{E}_5(t)) +o\left(\exp\left(-\tfrac{t^2}{2}\right)\tfrac{|t|+|t|^3}{\sqrt{n}}\right).
\end{align*}
Note that the first term is the characteristic function of $G$. Finally, we bound the moments appearing in $\mathcal{E}_5(t)$: $\E|\mathcal{T}_\mathfrak{c}\bar{L}|$, $\E|(H_\mathfrak{c}+\mathcal{T}_\mathfrak{c})\bar{Q}|$, $\E(H_\mathfrak{c}\bar{L})^2$, and $\E|H_\mathfrak{c}\bar{L}\bar{Q}|$. Holder's inequality combined with the theorem assumptions give
\begin{align*}
    \E|\mathcal{T}_\mathfrak{c}\bar{L}| &\leq \sqrt{\E|\mathcal{T}_\mathfrak{c}|^2\E|\bar{L}|^2} = O(n^{-1}h^{-(d+2)/2})\\
    \E|(H_\mathfrak{c}+\mathcal{T}_\mathfrak{c})\bar{Q}| &\leq \sqrt{\E|H_\mathfrak{c}+\mathcal{T}_\mathfrak{c}|^2\E|\bar{Q}|^2}= O\big((n^{-1/2} + n^{-1}h^{-d-2})(n^{-1/2}h^{-d/2-1})\big)\\
    \E(H_\mathfrak{c}\bar{L})^2 & = O(n^{-1} + n^{-2}h^{-2d-4})\\
    \E|H_\mathfrak{c}\bar{L}\bar{Q}| &= O\big((n^{-1/2} + n^{-1}h^{-d-2})(n^{-1/2}h^{-d/2 - 1})\big).
\end{align*}

Therefore, if $(\log n)^9/(nh^{d+2})\to 0$,
\[\breve{I}_1 := \int_{|t|\leq \log n} \frac{|\chi_{\breve{F}_\mathfrak{c}}(t) - \chi_{G_\mathfrak{c}}(t)|}{|t|} = o(\sqrt{n}h^P + {n^{-1}h^{-d-2}} + n^{-1/2}).\]

The proof is finalized.\qed

\subsection{Alternative Decomposition of $\widehat{\Scale}_\mathfrak{c}$}

Let $u_{ij}= \nu'U_{ij}$ and following \citet{Callaert-Veraverbeke_1981_AOS} with $S_N^2$ given in the their main Theorem, we have
\begin{align*}
    S_N^2
    &:= \frac{n^2(n-1)}{(n-2)^2}\widehat{\Scale}_\mathtt{AL}^2 
    = \frac{4(n-1)}{(n-2)^2} \sum_{i=1}^n (\nu'\widehat{L}_i/2)^2\\
    &= \frac{8}{(n-1)(n-2)^2}\left[\sum_{i<j} (u_{ij}-\E u_{12})^2 +\sum_{i=1}^n\sum_{j<k,j\neq i} (u_{ij}-\E u_{12})(u_{ik}-\E u_{12})\right]\\
    &\qquad -\frac{4n(n-1)}{(n-2)^2}(\widehat{\theta} - \E u_{12}).
\end{align*}
Define $g_i := \E[\ell_j q_{ij}|Z_i]$ and use the fact that $u_{ij}-\E u_{12} = \ell_i/2 + \ell_j/2 + q_{ij}$ to further decompose
\begin{align}\label{E:AL_decomp}
    S_N^2 &= \frac{1}{n}\sum_{i=1}^n\ell_i^2 + 4 g_i
            - \binom{n}{2}^{-1}\sum_{i<j}^n\ell_i\ell_j
            + 2\binom{n}{2}^{-1}\sum_{i<j}^n\big[(\ell_i + \ell_j)q_{ij} - g_i - g_j\big]\nonumber\\
    &\qquad -\frac{4}{n}\binom{n-1}{2}^{-1}\sum_{i=1}^n\ell_i\sum_{j<k, j\neq i}^nq_{jk}
            + \frac{4}{n-2}\binom{n-1}{2}^{-1}\sum_{i=1}^n\sum_{j<k, j\neq i}^nq_{ij}q_{ik}\nonumber\\
    &\qquad - \frac{4n(n-1)}{(n-2)^2}\left[\binom{n}{2}^{-1}\sum_{i<j} q_{ij}\right]^2
            + \frac{4n}{(n-2)^2}\binom{n}{2}^{-1}\sum_{i<j} q_{ij}^2.
\end{align}

The first term, we center on its expectation
\[\frac{1}{n}\sum_{i=1}^n\ell_i^2 + 4 g_i = \E[\ell_1^2] + \frac{1}{n}\sum_{i=1}^n\big(\ell_i^2 - \E[\ell_1^2]\big) + 4 g_i.\]
Define $\varphi_{ij}:=\E[ q_{ki}q_{kj}|Z_i,Z_j]$, and for the fifth term we write
\begin{equation*}
    \frac{4}{n-2}\binom{n-1}{2}^{-1}\sum_{i=1}^n\sum_{j<k, j\neq i}^nq_{ij}q_{ik}= 4\binom{n-1}{2}^{-1}\sum_{i<j}\varphi_{ij} + \frac{4}{n-2}\binom{n-1}{2}^{-1}\sum_{i=1}^n\sum_{j<k, j\neq i}^n(q_{ij}q_{ik}-\varphi_{jk}).
\end{equation*}
Finally, for the last term
\begin{align*}
\frac{4n}{(n-2)^2}\binom{n}{2}^{-1}\sum_{i<j} q_{ij}^2&= \frac{4n}{(n-2)^2}\binom{n}{2}^{-1}\sum_{i<j} \big[q_{ij}^2 - \varphi_{ii} - \varphi_{jj} + \E[q_{12}]^2]\\
&\qquad + \frac{8}{(n-2)^2}\sum_{i=1}^n \big(\varphi_{ii}-\E[q_{12}^2]\big) + \frac{4n}{(n-2)^2}\E[q_{12}^2]
\end{align*}

Plug the last three displays back into \eqref{E:AL_decomp}
to conclude
\begin{align*}
    S_N^2 &= \E[\ell_1^2] +  \frac{4n}{(n-2)^2}\E [q_{12}^2] + \frac{1}{n}\sum_{i=1}^n\big[(\ell_i^2-\E[\ell_1^2]) + 4 g_i\big] + 4\binom{n-1}{2}^{-1}\sum_{i<j}\varphi_{ij} +\mathcal{S}_\mathfrak{c},
\end{align*}
where $\mathcal{S}_\mathfrak{c}$ collection the remaining terms.

Next, we have
\begin{align*}
    \binom{n}{2}^{-1} \sum_{i<j} u_{ij}^2
    &= \binom{n}{2}^{-1} \sum_{i<j} (q_{ij} - \ell_i/2 - \ell_j/2 - \E u_{12})^2\\
    &= \E q_{12}^2 + \binom{n}{2}^{-1} \sum_{i<j} (q_{ij}^2 - \E q_{ij}^2) - \binom{n}{2}^{-1} \sum_{i=1}^n (\ell_i/2)^2  - (\E u_{12})^2\\
    &\qquad - \binom{n}{2}^{-1} \sum_{i<j} q_{ij} (\ell_i+\ell_j) + 2 \E u_{12} \binom{n}{2}^{-1} \sum_{i<j} (q_{ij} + \ell_i/2 + \ell_j/2)\\
    &\qquad + \frac{1}{2} \binom{n}{2}^{-1} \sum_{i<j} \ell_i \ell_j\\
    &= \E q_{12}^2 + \mathcal{Q}_\mathfrak{c},
\end{align*}
where $\mathcal{Q}_\mathfrak{c}$ is by definition.

We have
\begin{align*}
    \widehat{\Scale}_\mathfrak{c} 
    &= \widehat{\Scale}_\mathtt{AL}^2 - \mathfrak{c} \binom{n}{2}^{-1} h^{-d-2} \nu'\widehat{\Delta}\nu\\
    &= \frac{(n-2)^2}{n^2(n-1)} S_N^2 - \mathfrak{c} \binom{n}{2}^{-1} \left[\binom{n}{2}^{-1} \sum_{i<j} u_{ij}^2 \right]\\
    &= (2-\mathfrak{c}) \binom{n}{2}^{-1} \E [q_{12}^2] + \frac{1+o(1)}{n}\E[\ell_1^2]  + \frac{1+o(1)}{n^2}\sum_{i=1}^n\big[(\ell_i^2-\E[\ell_1^2]) + 4 g_i\big]\\
    &\qquad + \frac{1+o(1)}{n} 4 \binom{n}{2}^{-1} \sum_{i<j}\varphi_{ij}
    + \frac{1+o(1)}{n}\mathcal{S}_\mathfrak{c} + \mathfrak{c} \binom{n}{2}^{-1} \mathcal{Q}_\mathfrak{c},
\end{align*}
which gives the following simplified expression for the class of Studentizations:
\begin{equation}\label{E:altdecompstudent}
    \widehat{\Scale}_\mathfrak{c} 
    = (2-\mathfrak{c}) \binom{n}{2}^{-1} \E [q_{12}^2] + \frac{1}{n}\E[\ell_1^2]  + \frac{1}{n^2}\sum_{i=1}^n\big[(\ell_i^2-\E[\ell_1^2]) + 4 g_i\big] + \frac{1}{n} 4 \binom{n}{2}^{-1} \sum_{i<j}\varphi_{ij} + \mathcal{V}_\mathfrak{c},
\end{equation}
where $\mathcal{V}_\mathfrak{c}$ is by definition.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{References} %Remove the asterisk to get References in the TOC, with a numbered section.
%\small
%\singlespacing
\begingroup
\renewcommand{\section}[2]{}	%this removes the standard 'References' header that BibTeX puts in.
\bibliography{CFJM_2022_Powell--bib}
\bibliographystyle{jasa}
\endgroup



\end{document}
